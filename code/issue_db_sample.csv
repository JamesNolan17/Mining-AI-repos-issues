,Title,Name_Repo,State,Assignees,Proposed_By,Closed_By,Date_Created,Date_Closed,Num_Comment,Label_Issue,Identity_Repo,Identity_Global,Body,Sentiment_Title,Sentiment_Body
0,Upstream Build Break - 20191030,tensorflow/addons,closed,,seanpmorgan,seanpmorgan,2019-10-30 15:20:05,2019-11-04 23:23:06,2.0,bug#build,649.0,514757335.0,"Just for visibility:
https://github.com/tensorflow/tensorflow/issues/33852",neutral,neutral
1,the version of pytorch,ShichenLiu/CondenseNet,open,,cambridgeinch,,2019-04-11 10:25:55,,1.0,,22.0,431947090.0,"There are several problems of train-loader from torch.vision using my torch version (1.0.1), so could you show the requirements.txt or a latest code version using torch 1.0.1 ?",neutral,neutral
2,Can't reproduce Text BERT and Image-Grid validation set evaluations from paper,facebookresearch/mmf,closed,,shivgodhia,shivgodhia,2021-01-18 03:49:13,2021-04-03 18:13:52,6.0,,737.0,787891721.0,"I have an issue reproducing the baselines in the Hateful Memes Paper. Specifically I am trying to get the baselines for Text BERT but I am also not able to get the baselines for Image-Grid
## Instructions To Reproduce the Issue:

I ran this exact command for Text BERT, and ran it twice and got the same result
```
mmf_run config=mmf/projects/hateful_memes/configs/unimodal/bert.yaml model=unimodal_text dataset=hateful_memes run_type=val checkpoint.resume_zoo=unimodal_text.hateful_memes.bert checkpoint.resume_pretrained=False
```

And this command for Image-Grid:
```
mmf_run config=mmf/projects/hateful_memes/configs/unimodal/image.yaml model=unimodal_image dataset=hateful_memes run_type=val checkpoint.resume_zoo=unimodal_image.hateful_memes.images checkpoint.resume_pretrained=False
```

Log of TEXT Bert validation evaluation I observed:
```
/Users/shiv/Library/Mobile Documents/com~apple~CloudDocs/Dissertation/hateful-memes/env/lib/python3.8/site-packages/mmf/utils/configuration.py:535: UserWarning: Device specified is 'cuda' but cuda is not present. Switching to CPU version.
  warnings.warn(
2021-01-18T11:38:43 | mmf.utils.configuration: Overriding option config to mmf/projects/hateful_memes/configs/unimodal/bert.yaml
2021-01-18T11:38:43 | mmf.utils.configuration: Overriding option model to unimodal_text
2021-01-18T11:38:43 | mmf.utils.configuration: Overriding option datasets to hateful_memes
2021-01-18T11:38:43 | mmf.utils.configuration: Overriding option run_type to val
2021-01-18T11:38:43 | mmf.utils.configuration: Overriding option checkpoint.resume_zoo to unimodal_text.hateful_memes.bert
2021-01-18T11:38:43 | mmf.utils.configuration: Overriding option checkpoint.resume_pretrained to False
2021-01-18T11:38:43 | mmf: Logging to: ./save/train.log
2021-01-18T11:38:43 | mmf_cli.run: Namespace(config_override=None, local_rank=None, opts=['config=mmf/projects/hateful_memes/configs/unimodal/bert.yaml', 'model=unimodal_text', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=unimodal_text.hateful_memes.bert', 'checkpoint.resume_pretrained=False'])
2021-01-18T11:38:44 | mmf_cli.run: Torch version: 1.6.0
2021-01-18T11:38:44 | mmf_cli.run: Using seed 43511271
2021-01-18T11:38:44 | mmf.trainers.mmf_trainer: Loading datasets
2021-01-18T11:38:50 | mmf.trainers.mmf_trainer: Loading model
2021-01-18T11:38:55 | mmf.trainers.mmf_trainer: Loading optimizer
2021-01-18T11:38:55 | mmf.trainers.mmf_trainer: Loading metrics
2021-01-18T11:38:55 | mmf.utils.checkpoint: Loading checkpoint
WARNING 2021-01-18T11:38:57 | mmf: Key data_parallel is not present in registry, returning default value of None
WARNING 2021-01-18T11:38:57 | mmf: Key distributed is not present in registry, returning default value of None
WARNING 2021-01-18T11:38:57 | mmf: Key data_parallel is not present in registry, returning default value of None
WARNING 2021-01-18T11:38:57 | mmf: Key distributed is not present in registry, returning default value of None
WARNING 2021-01-18T11:38:58 | mmf.utils.checkpoint: Missing keys ['base.encoder.embeddings.position_ids'] in the checkpoint.
If this is not your checkpoint, please open up an issue on MMF GitHub. 
Unexpected keys if any: []
WARNING 2021-01-18T11:38:58 | py.warnings: /Users/shiv/Library/Mobile Documents/com~apple~CloudDocs/Dissertation/hateful-memes/env/lib/python3.8/site-packages/mmf/utils/checkpoint.py:291: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.
  warnings.warn(

WARNING 2021-01-18T11:38:58 | py.warnings: /Users/shiv/Library/Mobile Documents/com~apple~CloudDocs/Dissertation/hateful-memes/env/lib/python3.8/site-packages/mmf/utils/checkpoint.py:291: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.
  warnings.warn(

WARNING 2021-01-18T11:38:58 | py.warnings: /Users/shiv/Library/Mobile Documents/com~apple~CloudDocs/Dissertation/hateful-memes/env/lib/python3.8/site-packages/mmf/utils/checkpoint.py:334: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.
  warnings.warn(

WARNING 2021-01-18T11:38:58 | py.warnings: /Users/shiv/Library/Mobile Documents/com~apple~CloudDocs/Dissertation/hateful-memes/env/lib/python3.8/site-packages/mmf/utils/checkpoint.py:334: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.
  warnings.warn(

2021-01-18T11:38:58 | mmf.utils.checkpoint: Checkpoint loaded.
2021-01-18T11:38:58 | mmf.utils.checkpoint: Current num updates: 0
2021-01-18T11:38:58 | mmf.utils.checkpoint: Current iteration: 0
2021-01-18T11:38:58 | mmf.utils.checkpoint: Current epoch: 0
2021-01-18T11:38:58 | mmf.trainers.mmf_trainer: ===== Model =====
2021-01-18T11:38:58 | mmf.trainers.mmf_trainer: UnimodalText(
  (base): UnimodalBase(
    (encoder): BertModelJit(
      (embeddings): BertEmbeddingsJit(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (classifier): MLPClassifer(
    (layers): ModuleList(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Dropout(p=0.5, inplace=False)
      (4): Linear(in_features=768, out_features=768, bias=True)
      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (6): ReLU()
      (7): Dropout(p=0.5, inplace=False)
      (8): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
2021-01-18T11:38:58 | mmf.utils.general: Total Parameters: 110668034. Trained Parameters: 110668034
2021-01-18T11:38:58 | mmf.trainers.mmf_trainer: Starting inference on val set
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:26<00:00, 17.23s/it]
2021-01-18T11:40:24 | mmf.trainers.callbacks.logistics: val/hateful_memes/cross_entropy: 0.7017, val/total_loss: 0.7017, val/hateful_memes/accuracy: 0.6167, val/hateful_memes/binary_f1: 0.4069, val/hateful_memes/roc_auc: 0.6119
2021-01-18T11:40:24 | mmf.trainers.callbacks.logistics: Finished run in 01m 29s 250ms
```

## Expected behavior:

For Text BERT, I expected Validation Accuracy to be 58.26%, and AUROC to be 64.65%. But I seem to have gotten 61.67% and 61.19% respectively. A similar error is gotten for Image-Grid:


```
val/hateful_memes/cross_entropy: 0.7144, val/total_loss: 0.7144, val/hateful_memes/accuracy: 0.6185, val/hateful_memes/binary_f1: 0.2256, val/hateful_memes/roc_auc: 0.5781
```


## Environment:

Provide your environment information using the following command:
```
Collecting environment information...
PyTorch version: 1.6.0
Is debug build: No
CUDA used to build PyTorch: None

OS: Mac OSX 11.1
GCC version: Could not collect
CMake version: version 3.19.2

Python version: 3.8
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.6.0
[pip3] torchtext==0.5.0
[pip3] torchvision==0.7.0
[conda] Could not collect
```

",neutral,negative
3,cannot converge use tf.nn.l2_normalize,williamleif/GraphSAGE,open,,stevechoris,,2019-03-22 07:43:39,,1.0,,77.0,424083252.0,"I try graphsage with my own code and data.
it works well when not use tf.nn.l2_normalize after encoder and before decoder. but when i use tf.nn.l2_normalize to normlize the output vector to length of 1, the model just cannot converge and the loss does not decrease. what is the cause of this problem.",neutral,negative
4,Adding a regression head to regress a new feature,open-mmlab/mmdetection,open,jshilong,SamihaSara,,2021-06-11 02:59:18,,4.0,community help wanted#reimplementation,5337.0,918156264.0,"I am using **mmdetection project** for disease detection. I have converted my dataset in COCO Format and added a new field in the annotation file, called ""severity_score""(value ranges from 0 to 1). This I want to feed along with bounding box coordinates and class labels. I have run the following command to run **mmdetection's** FasterRCNN and FCOS models:

python tools/train.py configs/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco.py
and
python tools/train.py configs/faster_rcnn/faster_rcnn_r101_fpn_2x_coco.py

Now I am trying to feed my dataset to regress a new feature in the annotation file. I understand I need to modify 
https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/roi_heads/bbox_heads/bbox_head.py.
But **mmdetection** seems a big project I am not quite getting to feed a new field from ground truth and add a new regression head which files should I look into. Any suggestion regarding this would be greatly appreciated.
Also, for a beginner, what kind of model would be convenient i.e. anchor free or anchor models? I would like this feature in the existing code, without registering new heads/modules if possible.


@hellock please if you have some time, I would be thankful for your help. 

",negative,positive
5,Question about the lane width,nutonomy/nuscenes-devkit,closed,,LuanDi1996,holger-motional,2020-06-03 09:18:31,2020-06-05 05:01:44,5.0,,409.0,629835608.0,"Hi Team

I am wondering if there some information about the lane width, and if I have a token, how can I know it is lane or lane connector? 

Thanks.",neutral,positive
6,Do you have Tensorflow implementation?,Alibaba-MIIL/ASL,closed,,guotong1988,guotong1988,2022-03-25 02:25:00,2022-03-28 01:46:12,1.0,,84.0,1180266738.0,Thank you very much!,neutral,positive
7,Pretrained model on Kinetics with input of Optical Flow,MIT-HAN-LAB/temporal-shift-module,closed,,misunusc,tonylins,2019-12-28 23:19:00,2020-01-11 07:29:51,3.0,,48.0,543291926.0,"Hi Dr Lin,
Did you train TSM on Kinetics dataset using optical flow as the input modality?
If so, could you please release the pre-trainded model on Kinetics with the input of optical flow?
Thank you!",neutral,positive
8,No module named 'cluster',xu-ji/IIC,closed,,shlokk,xu-ji,2019-03-30 20:44:58,2019-04-03 18:46:46,3.0,,1.0,427326318.0,"Hi, I'm getting import issues. Also, I have changed code to code1 as it was causing some dependency issues.
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m code1.scripts.segmentation.segmentation_twohead --mode IID --dataset Coco164kCuratedFew --dataset_root /scratch/local/ssd/xuji/COCO/CocoStuff164k --model_ind 714 --arch SegmentationNet10aTwoHead --num_epochs 4800 --lr 0.0001 --lamb_A 1.0 --lamb_B 1.0 --num_sub_heads 1 --batch_sz 120 --num_dataloaders 1 --use_coarse_labels --output_k_A 15 --output_k_B 3 --gt_k 3 --pre_scale_all --pre_scale_factor 0.33 --input_sz 128 --half_T_side_sparse_min 0 --half_T_side_sparse_max 0 --half_T_side_dense 10 --include_rgb  --coco_164k_curated_version 6 --use_uncollapsed_loss --batchnorm_track > gnoded2_gpu0123_m714.out
Traceback (most recent call last):
  File ""/vulcan/scratch/shlok/Ana/envs/pytorch/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/vulcan/scratch/shlok/Ana/envs/pytorch/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/vulcan/scratch/shlok/IIC/code1/scripts/segmentation/segmentation_twohead.py"", line 18, in <module>
    import code1.archs as archs
  File ""/vulcan/scratch/shlok/IIC/code1/archs/__init__.py"", line 1, in <module>
    from cluster import *
ImportError: No module named 'cluster'",neutral,neutral
9,Is it possible to expose the exp map to the Python API via a logsignature_to_signature function?,patrick-kidger/signatory,open,,Mithrillion,,2022-08-30 09:20:48,,2.0,,48.0,1355447546.0,"Somehow, I found myself needing to map a logsignature back to the full signature (where the logsignature can be in either the tensor algebra or a compressed form). It appears the relevant operations such as exp map already exist in the C++ code, so I wonder how difficult it would be to implement a `logsignature_to_signature` function.

Currently, I have tried two options. The `iisignature` package do offer a mapping from logsignature to signature, but this requires casting data from PyTorch to numpy and back, and potentially also moving data between devices, so it works, but is not ideal.

Alternatively, I could implement the truncated expansion of the exp map directly in PyTorch, following the template in `iisignature` and also bottler's [free-lie-algebra-py](https://github.com/bottler/free-lie-algebra-py), using the `exp(x)=1+x+x/2(x+x/3(x+...))` trick. However, this is significantly slower than computing via `iisignature`.

Therefore I guess my best bet is to expose the exp map within `signatory`'s C++ code to the Python API. However, I am not familiar with PyTorch in C++ and am not sure how much effort it would take to try to implement a `logsignature_to_signature` function.

Is it possible that such a functionality can be implemented fairly quickly? And is it possible for `logsignature_to_signature` to eventually be included in `signatory`?

Many thanks!",neutral,negative
10,How to train,hufu6371/DORN,open,,boyob,,2019-04-06 12:55:49,,0.0,,29.0,430031154.0,Please provide some instructions to train.,neutral,neutral
11,encoding issue?,rsennrich/subword-nmt,closed,,kmario23,rsennrich,2018-09-05 17:33:44,2018-09-06 05:27:37,2.0,,63.0,357331031.0,"Hi,
  I have been using this code for sometime but lately I've been getting this issue. 

`no pair has frequency >= 2. Stopping`

What's strange about this is that it works with some input files whereas it fails with the above message for some files (although these are exactly in the same format as required)

What exactly is the problem? Also, any ideas about how to fix this? I have already tried the solution suggested in #29 but that didn't help much.

thanks!",neutral,negative
12,Age recognition Errors when Mtcnn model fails to locate the face,deepinsight/insightface,open,,zylxadz,,2018-11-22 03:23:39,,1.0,,449.0,383366968.0,"when running insightface/gender-age/test.py,the following error happens.
It is caused by Mtcnn model failing to locate the face.Does anyone have the same problem?Appreciate your reply.Thanks a lot.

File ""/home/ndir/.local/lib/python2.7/site-packages/mxnet/module/module.py"", line 606, in forward
    new_data_shapes = tuple(i.shape for i in data_batch.data)
AttributeError: 'NoneType' object has no attribute 'data'

environment：
cuda9.0+python2.7+mxnet-cu90",negative,negative
13,How to get AP for mask and box separately,matterport/Mask_RCNN,open,,mminakshi,,2020-03-03 19:08:54,,17.0,,2025.0,574879237.0,,neutral,neutral
14,render birds eye view video,nutonomy/nuscenes-devkit,closed,,xli4217,holger-motional,2020-04-23 19:01:04,2020-04-24 00:36:57,1.0,,368.0,605783793.0,"Hi, I was wondering if there is support to render top down view video to visualize trajectory prediction results for example. Thank you. ",neutral,positive
15,How can ap values be visualized during VOC dataset training and testing,open-mmlab/mmdetection,closed,ZwwWayne,gaobo25,ZwwWayne,2022-10-09 02:22:00,2022-10-09 07:21:34,3.0,feature request,8973.0,1402122772.0,"### What is the problem this feature will solve?

How can ap values be visualized during VOC dataset training and testing

### What is the feature you are proposing to solve the problem?

How can ap values be visualized during VOC dataset training and testing

### What alternatives have you considered?

_No response_",neutral,positive
16,Missing parts in fused point clouds,bertjiazheng/Structured3D,closed,,ywyue,ywyue,2022-05-19 22:28:35,2022-06-13 09:03:12,11.0,question,26.0,1242394661.0,"Hi,

Thanks for the great work! I am trying to generate point cloud from the dataset. I follow this [script](https://gist.github.com/bertjiazheng/c43066678c9070448d7076291d0d30d9) and generate the point cloud for scene_00000 based on the Structured3D_perspective_full_00 dataset. However, there are some missing parts obviously. Do you think this is normal or is there a way to generate a more complete point cloud? Thanks in advance!

![image](https://user-images.githubusercontent.com/40747438/169414713-59b6d578-fb7a-4d38-9b2d-42ca3ea22d84.png)

Best,
Yue



",neutral,positive
17,Question about deepfill V2,JiahuiYu/generative_inpainting,closed,,erhuodaosi,erhuodaosi,2019-06-17 03:08:12,2019-06-20 02:24:14,0.0,,276.0,456721976.0,"Wonderful work!
It is so awesome a project!
I have read DeepFillv2 release #62 carefully,but I still have some questions about deepfillv2.
Why we use sketch channel(user guide)?Given a broken picture,if we use HED to generate the edge flist,and training it with edge flist.That's equal to know the broken area in advance.So what is the meaning to use sketch channel?By changing the broken area in a user guide way to generate the picture we want?How do you do in this sketch channel?Do you just put the original picture,mask and edge flist as input and train?
I would very appreciate if you could help me,thank you very much!
Best wishes to you!",neutral,positive
18,FileNotFoundError: [Errno 2] File b'../../../../data/raw/event-story-cluster/event_story_cluster.txt' does not exist,BangLiu/ArticlePairMatching,open,,empty-id,,2019-12-19 10:41:36,,11.0,,18.0,540230333.0,"<img width=""1112"" alt=""image"" src=""https://user-images.githubusercontent.com/56990007/71167145-1e2aa380-228f-11ea-8edc-e7fcfc58a02f.png"">
",neutral,neutral
19,demo.py issue,Zhongdao/Towards-Realtime-MOT,closed,,pacearthur,pacearthur,2019-12-15 23:39:38,2019-12-22 10:02:36,1.0,,63.0,538124144.0,"Hi,

Can someone help me about this issue ?

![Capture](https://user-images.githubusercontent.com/33637884/70871147-5df64000-1f9c-11ea-8242-86146168bdf2.PNG)

**First :**
""Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx""

**And also:**
""[image2 @ 0000021d85b49580] Could find no file with path 'results\frame/%05d.jpg' and index in the range 0-4 results\frame/%05d.jpg: No such file or directory""


Indeed I don't have any NVIDIA GPU on my computer but I thought that it will be possible to run the demo.py script
",neutral,neutral
20,Loosen the restriction on ConfigSpace requirements,automl/nasbench301,closed,,Neonkraft,Neonkraft,2022-10-05 09:54:01,2022-10-05 10:11:10,1.0,,17.0,1397536797.0,"Hi,

The version of ConfigSpace required by the nasbench301 is set to exactly `0.4.12`. This clashes with NASLib, which requires at least `0.4.17`.

I've raised #16 to fix this issue.

Best
Arjun



",neutral,neutral
21,verification on my own data,deepinsight/insightface,closed,,cici-tan,nttstar,2018-06-12 09:57:04,2018-06-28 05:54:15,8.0,,252.0,331518825.0,"I generated my own data using lfw2pack.py, and image size is 160x160.
But when I run verification.py, it popped out this error...

""expected [3,160,160], got [3,112,112]""

Traceback (most recent call last):
  File ""verification.py"", line 566, in <module>
    data_set = load_bin(path, image_size)
  File ""verification.py"", line 196, in load_bin
    data_list[flip][i][:] = img
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.py"", line 437, in __setitem__
    self._set_nd_basic_indexing(key, value)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.py"", line 691, in _set_nd_basic_indexing
    value.copyto(self)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.py"", line 1876, in copyto
    return _internal._copyto(self, out=other)
  File ""<string>"", line 25, in _copyto
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/_ctypes/ndarray.py"", line 92, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 146, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:54:06] src/operator/nn/./../tensor/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [3,160,160], got [3,112,112]",neutral,positive
22,Evaluation ,snakeztc/NeuralDialog-CVAE,open,,LeiCaiwsu,,2018-04-10 23:53:28,,0.0,,11.0,313127774.0,"Hi, could you provide the evaluation code to reproduce the results of the paper?",neutral,neutral
23,mmf pytest failed,facebookresearch/mmf,closed,,AbSaifi,apsdehal,2020-06-07 15:54:41,2020-06-07 20:27:09,1.0,,300.0,633528438.0,"Steps : 
1. Latest source from git clone https://github.com/facebookresearch/mmf.git --> Success
2. Go to mmf source folder --> Success
3. pip install --editable . --> Success
4. pytest ./tests/ --> Fail

Result --> 8 tests failed, 57 passed, 18 warnings and 2 error. Problem can be seen majorly as
AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'
E       AssertionError: '0ms' != '000ms'
E       - 0ms
E       + 000ms
E       ? ++
 

O/p of pytest :

pytest ./tests/
============================================================================================== test session starts ===============================================================================================
platform win32 -- Python 3.7.4, pytest-5.2.1, py-1.8.0, pluggy-0.13.0
rootdir: F:\Python\DD_HM\mmf
plugins: arraydiff-0.3, doctestplus-0.4.0, openfiles-0.4.0, remotedata-0.3.2
collected 67 items

tests\common\test_sample.py ...                                                                                                                                                                             [  4%]
tests\configs\test_configs_for_keys.py ..                                                                                                                                                                   [  7%]
tests\configs\test_zoo_urls.py .                                                                                                                                                                            [  8%]
tests\datasets\test_base_dataset.py .                                                                                                                                                                       [ 10%]
tests\datasets\test_processors.py ...                                                                                                                                                                       [ 14%]
tests\models\test_cnn_lstm.py .                                                                                                                                                                             [ 16%]
tests\models\interfaces\test_interfaces.py F                                                                                                                                                                [ 17%]
tests\modules\test_fusions.py ..........                                                                                                                                                                    [ 32%]
tests\modules\test_layers.py .....                                                                                                                                                                          [ 40%]
tests\modules\test_losses.py ..                                                                                                                                                                             [ 43%]
tests\modules\test_metrics.py ..........                                                                                                                                                                    [ 58%]
tests\utils\test_checkpoint.py FFFFF..                                                                                                                                                                      [ 68%]
tests\utils\test_configuration.py .                                                                                                                                                                         [ 70%]
tests\utils\test_distributed.py .                                                                                                                                                                           [ 71%]
tests\utils\test_download.py ...                                                                                                                                                                            [ 76%]
tests\utils\test_file_io.py ....                                                                                                                                                                            [ 82%]
tests\utils\test_general.py ..                                                                                                                                                                              [ 85%]
tests\utils\test_logger.py EE                                                                                                                                                                               [ 88%]
tests\utils\test_text.py .....                                                                                                                                                                              [ 95%]
tests\utils\test_timer.py F.F                                                                                                                                                                               [100%]

===================================================================================================== ERRORS =====================================================================================================
__________________________________________________________________________________ ERROR at setup of TestLogger.test_log_writer __________________________________________________________________________________

cls = <class 'tests.utils.test_logger.TestLogger'>

    @classmethod
    def setUpClass(cls) -> None:
        cls._tmpdir = tempfile.mkdtemp()
        args = argparse.Namespace()
        args.opts = [f""env.save_dir={cls._tmpdir}"", f""model=cnn_lstm"", f""dataset=clevr""]
        args.config_override = None
        configuration = Configuration(args)
        configuration.freeze()
        cls.config = configuration.get_config()
        registry.register(""config"", cls.config)
>       cls.writer = Logger(cls.config)

tests\utils\test_logger.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\logger.py:19: in __init__
    self._is_master = is_master()
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
--------------------------------------------------------------------------------------------- Captured stdout setup ----------------------------------------------------------------------------------------------
Overriding option env.save_dir to C:\Users\ABDURR~1\AppData\Local\Temp\tmpjpb0tpp3
Overriding option model to cnn_lstm
Overriding option datasets to clevr
_________________________________________________________________________________ ERROR at setup of TestLogger.test_logger_files _________________________________________________________________________________

cls = <class 'tests.utils.test_logger.TestLogger'>

    @classmethod
    def setUpClass(cls) -> None:
        cls._tmpdir = tempfile.mkdtemp()
        args = argparse.Namespace()
        args.opts = [f""env.save_dir={cls._tmpdir}"", f""model=cnn_lstm"", f""dataset=clevr""]
        args.config_override = None
        configuration = Configuration(args)
        configuration.freeze()
        cls.config = configuration.get_config()
        registry.register(""config"", cls.config)
>       cls.writer = Logger(cls.config)

tests\utils\test_logger.py:31:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\logger.py:19: in __init__
    self._is_master = is_master()
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
==================================================================================================== FAILURES ====================================================================================================
___________________________________________________________________________________ TestModelInterfaces.test_mmbt_hm_interface ___________________________________________________________________________________

self = <test_interfaces.TestModelInterfaces testMethod=test_mmbt_hm_interface>

    @skip_if_no_network
    def test_mmbt_hm_interface(self):
>       model = MMBT.from_pretrained(""mmbt.hateful_memes.images"")

tests\models\interfaces\test_interfaces.py:14:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\models\mmbt.py:514: in from_pretrained
    model = super().from_pretrained(model_name, *args, **kwargs)
mmf\models\base_model.py:196: in from_pretrained
    output = load_pretrained_model(model_name, *args, **kwargs)
mmf\utils\checkpoint.py:40: in load_pretrained_model
    download_path = download_pretrained_model(model_name_or_path, *args, **kwargs)
mmf\utils\download.py:378: in download_pretrained_model
    if is_master():
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
_____________________________________________________________________________ TestUtilsCheckpoint.test_finalize_and_restore_from_it ______________________________________________________________________________

self = <tests.utils.test_checkpoint.TestUtilsCheckpoint testMethod=test_finalize_and_restore_from_it>

    def test_finalize_and_restore_from_it(self):
        with mock_env_with_temp():
            checkpoint = Checkpoint(self.trainer)
            self._init_early_stopping(checkpoint)
            original_model = deepcopy(self.trainer.model)
            self._do_a_pass()
            model_1500 = deepcopy(self.trainer.model)
>           checkpoint.save(1500)

tests\utils\test_checkpoint.py:218:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\checkpoint.py:358: in save
    if not is_master():
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
_______________________________________________________________________________ TestUtilsCheckpoint.test_finalize_and_resume_file ________________________________________________________________________________

self = <tests.utils.test_checkpoint.TestUtilsCheckpoint testMethod=test_finalize_and_resume_file>

    def test_finalize_and_resume_file(self):
        with mock_env_with_temp() as d:
            checkpoint = Checkpoint(self.trainer)
            self._init_early_stopping(checkpoint)
            self._do_a_pass()
>           checkpoint.finalize()

tests\utils\test_checkpoint.py:273:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\checkpoint.py:415: in finalize
    if is_master():
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
____________________________________________________________________________________ TestUtilsCheckpoint.test_pretrained_load ____________________________________________________________________________________

self = <tests.utils.test_checkpoint.TestUtilsCheckpoint testMethod=test_pretrained_load>

    def test_pretrained_load(self):
        with mock_env_with_temp() as d:
            checkpoint = Checkpoint(self.trainer)
            self._init_early_stopping(checkpoint)
            self._do_a_pass()
            original_model = deepcopy(self.trainer.model)
            # Test with zoo now
            ret_load_pretrained_zoo = {
                ""config"": self.config.model_config,
                ""checkpoint"": deepcopy(self.trainer.model.state_dict()),
                ""full_config"": self.config,
            }

>           checkpoint.save(2000)

tests\utils\test_checkpoint.py:471:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\checkpoint.py:358: in save
    if not is_master():
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
________________________________________________________________________________________ TestUtilsCheckpoint.test_resets _________________________________________________________________________________________

self = <tests.utils.test_checkpoint.TestUtilsCheckpoint testMethod=test_resets>

    def test_resets(self):
        with mock_env_with_temp():
            checkpoint = Checkpoint(self.trainer)
            self._init_early_stopping(checkpoint)
            self._do_a_pass()

            original_optimizer = deepcopy(self.trainer.optimizer)
            original_model = deepcopy(self.trainer.model)

            self.trainer.current_epoch = 3
>           checkpoint.save(2000, update_best=True)

tests\utils\test_checkpoint.py:316:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\checkpoint.py:358: in save
    if not is_master():
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
_______________________________________________________________________________ TestUtilsCheckpoint.test_save_and_load_state_dict ________________________________________________________________________________

self = <tests.utils.test_checkpoint.TestUtilsCheckpoint testMethod=test_save_and_load_state_dict>

    def test_save_and_load_state_dict(self):
        with mock_env_with_temp() as d:
            checkpoint = Checkpoint(self.trainer)
            self._init_early_stopping(checkpoint)
            self._do_a_pass()
            # Test normal case
>           checkpoint.save(1500)

tests\utils\test_checkpoint.py:105:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
mmf\utils\checkpoint.py:358: in save
    if not is_master():
mmf\utils\distributed.py:39: in is_master
    return get_rank() == 0
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def get_rank():
>       if not dist.is_nccl_available():
E       AttributeError: module 'torch.distributed' has no attribute 'is_nccl_available'

mmf\utils\distributed.py:31: AttributeError
________________________________________________________________________________________ TestUtilsTimer.test_get_current _________________________________________________________________________________________

self = <tests.utils.test_timer.TestUtilsTimer testMethod=test_get_current>

    def test_get_current(self):
        timer = Timer()
        expected = ""000ms""

>       self.assertEqual(timer.get_current(), expected)
E       AssertionError: '0ms' != '000ms'
E       - 0ms
E       + 000ms
E       ? ++

tests\utils\test_timer.py:13: AssertionError
___________________________________________________________________________________________ TestUtilsTimer.test_reset ____________________________________________________________________________________________

self = <tests.utils.test_timer.TestUtilsTimer testMethod=test_reset>

    def test_reset(self):
        timer = Timer()
        time.sleep(2)
        timer.reset()
        expected = ""000ms""

>       self.assertEqual(timer.get_current(), expected)
E       AssertionError: '0ms' != '000ms'
E       - 0ms
E       + 000ms
E       ? ++

tests\utils\test_timer.py:21: AssertionError
================================================================================================ warnings summary ================================================================================================
C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\win32\lib\pywintypes.py:2
  C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\win32\lib\pywintypes.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
    import imp, sys, os

tests\utils\test_model.py:8
  F:\Python\DD_HM\mmf\tests\utils\test_model.py:8: PytestCollectionWarning: cannot collect test class 'TestDecoderModel' because it has a __init__ constructor (from: tests/utils/test_model.py)
    class TestDecoderModel(nn.Module):

tests\utils\test_model.py:8
  F:\Python\DD_HM\mmf\tests\utils\test_model.py:8: PytestCollectionWarning: cannot collect test class 'TestDecoderModel' because it has a __init__ constructor (from: tests/utils/test_text.py)
    class TestDecoderModel(nn.Module):

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_dataset_configs_for_keys
  F:\Python\DD_HM\mmf\tests\configs\test_configs_for_keys.py:51: UserWarning: Dataset vqa2_ocr has no default configuration defined. Skipping it. Make sure it is intentional
    ).format(builder_key)

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_model_configs_for_keys
  F:\Python\DD_HM\mmf\tests\configs\test_configs_for_keys.py:26: UserWarning: Model multihead has no default configuration defined. Skipping it. Make sure it is intentional
    ).format(model_key)

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_model_configs_for_keys
  F:\Python\DD_HM\mmf\tests\configs\test_configs_for_keys.py:26: UserWarning: Model top_down_bottom_up has no default configuration defined. Skipping it. Make sure it is intentional
    ).format(model_key)

tests/modules/test_metrics.py::TestModuleMetrics::test_caption_bleu4
  C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly
    regargs, varargs, varkwargs, defaults, formatvalue=lambda value: """"

tests/modules/test_metrics.py::TestModuleMetrics::test_caption_bleu4
  C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\lm\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Sequence, defaultdict

tests/modules/test_metrics.py::TestModuleMetrics::test_caption_bleu4
  C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\nltk\lm\vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Counter, Iterable

tests/modules/test_metrics.py::TestModuleMetrics::test_macro_f1
  C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
    'precision', 'predicted', average, warn_for)

tests/modules/test_metrics.py::TestModuleMetrics::test_multilabel_macro_f1
  C:\Users\Abdurrabb\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\metrics\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
    'recall', 'true', average, warn_for)

tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_finalize_and_restore_from_it
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_finalize_and_resume_file
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_pretrained_load
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_resets
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_save_and_load_state_dict
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_zoo_load
  F:\Python\DD_HM\mmf\mmf\models\base_model.py:162: UserWarning: 'losses' already present in model output. No calculation will be done in base model.
    ""'losses' already present in model output. ""

tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_zoo_load
  F:\Python\DD_HM\mmf\mmf\utils\checkpoint.py:224: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.
    ""'optimizer' key is not present in the ""

-- Docs: https://docs.pytest.org/en/latest/warnings.html
========================================================================= 8 failed, 57 passed, 18 warnings, 2 error in 269.61s (0:04:29) =========================================================================


",neutral,negative
24,Error while training on own dataset: ValueError: Criteria used for early stopping (val/hateful_memes/roc_auc) is not present in meter,facebookresearch/mmf,closed,,kLabille,kLabille,2021-01-28 18:01:31,2021-01-28 21:41:04,2.0,,754.0,796229638.0,"## ❓ Questions and Help
Hi,

I was able to add my own dataset to mmf, however, when I try to train the unimodal image (Image Grid) model on it, I run into an error when evaluating the first epoch:

```
2021-01-28T11:33:06 | mmf.trainers.core.training_loop: Evaluation time. Running on full validation set...
....
ValueError: Criteria used for early stopping (val/hateful_memes/roc_auc) is not present in meter.
```
Any help is appreciated. 
Thank you",neutral,positive
25,training is so slow,open-mmlab/mmdetection,closed,jshilong,olfa-koubaa,jshilong,2021-05-27 14:50:25,2021-06-29 05:25:07,8.0,community help wanted,5237.0,903874156.0,"I tried training the mask rcnn with both detectron2 and mmdetection and the training speed gap is huge 

mmdetection offers a lot of preimplemented models that I want to run my tests with but I'm struggling with the fact that is so slow 

I found some people who encountered the same problem but it's not clear as to what's causing it or what is the solution",neutral,neutral
26,Quenstion about get_L2norm_loss_self_drive,jihanyang/AFN,open,,kevinbro96,,2020-08-18 15:05:21,,2.0,,13.0,681116342.0,"Hi there. I wonder why we should the L2 norm the  feature 'x'  first?
The L2 norm of feature 'x' will always be 1,  and the true norm of feature 'x' is lost.
",neutral,positive
27,About the rotation of test_set in MPII,Microsoft/human-pose-estimation.pytorch,open,,hcy226,,2022-04-11 12:01:37,,0.0,,176.0,1199870120.0,How can I get the correct PCK if I rotate the test_set in MPII? How to deal with the gt.mat？,neutral,neutral
28,RuntimeError: The size of tensor a (47) must match the size of tensor b (48) at non-singleton dimension 2,open-mmlab/mmdetection,closed,,ghost,,2019-08-08 03:37:23,2019-08-08 03:52:21,4.0,,1147.0,478233642.0,"my dataset get_item is:
`    def __getitem__(self, idx):

        # ---------
        #  Image
        # ---------
        gt_labels = []
        gt_bboxes = []
        img_path = self.img_files[idx % len(self.img_files)].rstrip()
        ori_img = transforms.ToTensor()(Image.open(img_path).convert('RGB'))
        img = mmcv.imread(osp.join(img_path))
        self.img_transform = ImageTransform(
            size_divisor=self.size_divisor, **self.img_norm_cfg)
        # apply transforms
        flip = True if np.random.rand() < self.flip_ratio else False
        # randomly sample a scale
        img_scale = random_scale(self.img_scales, self.multiscale_mode)

        img, img_shape, pad_shape, scale_factor = self.img_transform(
            img, img_scale, flip, keep_ratio=self.resize_keep_ratio)
        _ , h , w = ori_img.shape
        ori_shape = (h,w, 3)
        img_meta = dict(
            ori_shape=ori_shape,
            img_shape=img_shape,
            pad_shape=pad_shape,
            scale_factor=scale_factor,
            flip=flip)
        # ---------
        #  Label
        # ---------
        label_path = self.label_files[idx % len(self.img_files)].rstrip()
        if os.path.exists(label_path):
            boxes = np.loadtxt(label_path).reshape(-1, 5)
            gt_labels = boxes[:, 0]
            gt_bboxes = boxes[:,1:]
        if len(gt_labels) == 0 and self.skip_img_without_anno:
            print(""no annotationis img is "",img_path)
            return None
        gt_bboxes = self.bbox_transform(gt_bboxes, img_shape, scale_factor,
            flip)
        imgs = to_tensor(img)
        data = dict(
            img=DC(to_tensor(img), stack=True),
            img_meta=DC(img_meta, cpu_only=True),
            gt_bboxes=DC(to_tensor(gt_bboxes)),
            gt_labels = DC(to_tensor(gt_labels))
            )
        print(""the img size "",imgs.size())
        return data`

the img size  torch.Size([3, 750, 1333]) ,the batch_size is 10
 I have printed the laterals .size in fpn.py:

> /mmdetection/mmdet/models/necks/fpn.py(114)forward()
    113             laterals[i - 1] += F.interpolate(
--> 114                 laterals[i], scale_factor=2, mode='nearest')
    115 

ipdb> p laterals[0].size(),laterals[1].size()                                                                                                                                    
(torch.Size([2, 256, 188, 334]), torch.Size([2, 256, 94, 167]))
ipdb> p laterals[2].size(),laterals[3].size()                                                                                                                                    
(torch.Size([2, 256, 47, 84]), torch.Size([2, 256, 24, 42]))

the error happens in fpn.py:
`                                                                   
RuntimeError: The size of tensor a (47) must match the size of tensor b (48) at non-singleton dimension 2`

why ? I use the htc_without_semantic_r50_fpn_1x.py",neutral,neutral
29,A resize_ error,junyanz/BicycleGAN,closed,,JaheimLee,junyanz,2018-01-29 08:39:38,2018-01-31 22:59:40,1.0,,11.0,292323309.0,"Hello! When I run the test_edges2handbags.sh, I met an error. What's the problem?

model [BiCycleGANModel] was created
Loading model bicycle_gan
process input image 000/010
Traceback (most recent call last):
  File ""./test.py"", line 41, in <module>
    model.set_input(data)
  File ""/home/jaheimlee/gitrep/BicycleGAN/models/base_model.py"", line 193, in set_input
    self.input_A.resize_(input_A.size()).copy_(input_A)
RuntimeError: calling resize_ on a tensor that has non-resizable storage. Clone it first or create a new tensor instead.
",neutral,neutral
30,run forward without loading network,seung-lab/znn-release,closed,,jingpengw,jingpengw,2015-10-12 18:19:08,2015-10-13 18:36:07,1.0,,29.0,111027751.0,"to test the initalization, it would be better to be able to run forward pass using an initialized network without loading existing .h5 network.
",neutral,neutral
31,"CORNER NET: AssertionError: RandomCenterCropPad needs the input image of dtype np.float32, please set ""to_float32=True"" in ""LoadImageFromFile"" pipeline",open-mmlab/mmdetection,closed,,ravising-h,hellock,2020-08-19 11:58:19,2020-08-26 17:16:57,12.0,,3581.0,681782593.0,"

**Checklist**
- [x] I have searched related issues but cannot get the expected help. 
- [x]  The bug has not been fixed in the latest version. 

**Describe the bug**
I have trained cornernet model and 
`AssertionError: RandomCenterCropPad needs the input image of dtype np.float32, please set ""to_float32=True"" in ""LoadImageFromFile"" pipeline` and this error shows while inference. (to_float is True in config)

**Reproduction**

```
img = mmcv.imread('/content/drive/My Drive/R-FCN/data/coco/JPEGImages/renamed_4210.jpg')

model.cfg = cfg
result = inference_detector(model, img)
show_result_pyplot(model, img, result)
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?
3. What dataset did you use?
XML DATASET (CUSTOM  only added class) 

**Environment**

```
2020-08-17 18:09:55,706 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.6.9 (default, Jul 17 2020, 12:50:27) [GCC 8.4.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GPU 0: Tesla K80
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.5.1+cu101
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.5 Product Build 20190808 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.6.1+cu101
OpenCV: 4.1.2
MMCV: 1.0.5
MMDetection: 2.3.0+6acf6be
MMDetection Compiler: GCC 7.5
MMDetection CUDA Compiler: 10.1
------------------------------------------------------------

2020-08-17 18:09:55,707 - mmdet - INFO - Distributed training: False
2020-08-17 18:09:56,095 - mmdet - INFO - Config:
checkpoint_config = dict(interval=10)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
model = dict(
    type='CornerNet',
    backbone=dict(
        type='HourglassNet',
        downsample_times=5,
        num_stacks=2,
        stage_channels=[256, 256, 384, 384, 384, 512],
        stage_blocks=[2, 2, 2, 2, 2, 4],
        norm_cfg=dict(type='BN', requires_grad=True)),
    neck=None,
    bbox_head=dict(
        type='CornerHead',
        num_classes=1,
        in_channels=256,
        num_feat_levels=2,
        corner_emb_channels=1,
        loss_heatmap=dict(
            type='GaussianFocalLoss', alpha=2.0, gamma=4.0, loss_weight=1),
        loss_embedding=dict(
            type='AssociativeEmbeddingLoss',
            pull_weight=0.25,
            push_weight=0.25),
        loss_offset=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1)))
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile', to_float32=True),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='PhotoMetricDistortion',
        brightness_delta=32,
        contrast_range=(0.5, 1.5),
        saturation_range=(0.5, 1.5),
        hue_delta=18),
    dict(
        type='RandomCenterCropPad',
        crop_size=(511, 511),
        ratios=(0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3),
        test_mode=False,
        test_pad_mode=None,
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Resize', img_scale=(511, 511), keep_ratio=False),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile', to_float32=True),
    dict(
        type='MultiScaleFlipAug',
        scale_factor=1.0,
        flip=True,
        transforms=[
            dict(type='Resize'),
            dict(
                type='RandomCenterCropPad',
                crop_size=None,
                ratios=None,
                border=None,
                test_mode=True,
                test_pad_mode=['logical_or', 127],
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(
                type='Collect',
                keys=['img'],
                meta_keys=('filename', 'ori_shape', 'img_shape', 'pad_shape',
                           'scale_factor', 'flip', 'img_norm_cfg', 'border'))
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        ann_file='/content/drive/My Drive/R-FCN/data/coco/train_dent.txt',
        img_prefix='/content/drive/My Drive/R-FCN/data/coco/',
        pipeline=[
            dict(type='LoadImageFromFile', to_float32=True),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='PhotoMetricDistortion',
                brightness_delta=32,
                contrast_range=(0.5, 1.5),
                saturation_range=(0.5, 1.5),
                hue_delta=18),
            dict(
                type='RandomCenterCropPad',
                crop_size=(511, 511),
                ratios=(0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3),
                test_mode=False,
                test_pad_mode=None,
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Resize', img_scale=(511, 511), keep_ratio=False),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='MyDataset',
        ann_file='/content/drive/My Drive/R-FCN/data/coco/test_dent.txt',
        img_prefix='/content/drive/My Drive/R-FCN/data/coco/',
        pipeline=[
            dict(type='LoadImageFromFile', to_float32=True),
            dict(
                type='MultiScaleFlipAug',
                scale_factor=1.0,
                flip=True,
                transforms=[
                    dict(type='Resize'),
                    dict(
                        type='RandomCenterCropPad',
                        crop_size=None,
                        ratios=None,
                        border=None,
                        test_mode=True,
                        test_pad_mode=['logical_or', 127],
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(
                        type='Collect',
                        keys=['img'],
                        meta_keys=('filename', 'ori_shape', 'img_shape',
                                   'pad_shape', 'scale_factor', 'flip',
                                   'img_norm_cfg', 'border'))
                ])
        ]),
    test=dict(
        type='MyDataset',
        ann_file='/content/drive/My Drive/R-FCN/data/coco/test.txt',
        img_prefix='/content/drive/My Drive/R-FCN/data/coco/',
        pipeline=[
            dict(type='LoadImageFromFile', to_float32=True),
            dict(
                type='MultiScaleFlipAug',
                scale_factor=1.0,
                flip=True,
                transforms=[
                    dict(type='Resize'),
                    dict(
                        type='RandomCenterCropPad',
                        crop_size=None,
                        ratios=None,
                        border=None,
                        test_mode=True,
                        test_pad_mode=['logical_or', 127],
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(
                        type='Collect',
                        keys=['img'],
                        meta_keys=('filename', 'ori_shape', 'img_shape',
                                   'pad_shape', 'scale_factor', 'flip',
                                   'img_norm_cfg', 'border'))
                ])
        ]))
train_cfg = None
test_cfg = dict(
    corner_topk=100,
    local_maximum_kernel=3,
    distance_threshold=0.5,
    score_thr=0.05,
    max_per_img=100,
    nms_cfg=dict(type='soft_nms', iou_threshold=0.5, method='gaussian'))
optimizer = dict(type='Adam', lr=0.0005)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    step=[180])
total_epochs = 11
dataset_type = 'MyDataset'
data_root = '/content/drive/My Drive/R-FCN/data/coco/'
evaluation = dict(interval=12, metric='bbox')
work_dir = './work_dirs/cornernet_hourglass104_mstest_10x5_210e_coco'
gpu_ids = range(0, 1)

2020-08-17 18:10:03,484 - mmdet - INFO - Start running, host: root@ead17a162f1a, work_dir: /content/mmdetection/work_dirs/cornernet_hourglass104_mstest_10x5_210e_coco
2020-08-17 18:10:03,484 - mmdet - INFO - workflow: [('train', 1)], max: 11 epochs
2020-08-17 18:15:00,067 - mmdet - INFO - Epoch [1][50/375]	lr: 1.993e-04, eta: 6:42:50, time: 5.931, data_time: 0.079, memory: 10550, det_loss: 1892.8123, off_loss: 19.1445, pull_loss: 696.7372, push_loss: 0.0067, loss: 2608.7008, grad_norm: 373508.4643
2020-08-17 18:19:53,047 - mmdet - INFO - Epoch [1][100/375]	lr: 2.327e-04, eta: 6:35:29, time: 5.860, data_time: 0.009, memory: 10550, det_loss: 54.5773, off_loss: 15.0010, pull_loss: 534.7616, push_loss: 0.0219, loss: 604.3618, grad_norm: 29908.6013
2020-08-17 18:24:46,235 - mmdet - INFO - Epoch [1][150/375]	lr: 2.660e-04, eta: 6:29:52, time: 5.864, data_time: 0.009, memory: 10550, det_loss: 46.9078, off_loss: 12.4506, pull_loss: 1293.7027, push_loss: 0.0590, loss: 1353.1201, grad_norm: 62648.4193
2020-08-17 18:29:39,138 - mmdet - INFO - Epoch [1][200/375]	lr: 2.993e-04, eta: 6:24:31, time: 5.858, data_time: 0.009, memory: 10550, det_loss: 30.5830, off_loss: 8.5783, pull_loss: 938.9489, push_loss: 0.1025, loss: 978.2128, grad_norm: 54262.1950
2020-08-17 18:34:32,058 - mmdet - INFO - Epoch [1][250/375]	lr: 3.327e-04, eta: 6:19:22, time: 5.858, data_time: 0.009, memory: 10550, det_loss: 74.8030, off_loss: 7.5447, pull_loss: 2966.4483, push_loss: 0.2229, loss: 3049.0189, grad_norm: 33954.0571
2020-08-17 18:39:24,955 - mmdet - INFO - Epoch [1][300/375]	lr: 3.660e-04, eta: 6:14:18, time: 5.858, data_time: 0.009, memory: 10550, det_loss: 26.4749, off_loss: 5.6188, pull_loss: 140.9628, push_loss: 0.1617, loss: 173.2182, grad_norm: 5608.6533
2020-08-17 18:44:17,955 - mmdet - INFO - Epoch [1][350/375]	lr: 3.993e-04, eta: 6:09:18, time: 5.860, data_time: 0.009, memory: 10550, det_loss: 16.7364, off_loss: 3.1022, pull_loss: 34.8091, push_loss: 0.2154, loss: 54.8631, grad_norm: 1957.3584

```
Training continued
**Error traceback**
If applicable, paste the error trackback here.
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-12-1d152aae2563> in <module>()
      2 
      3 model.cfg = cfg
----> 4 result = inference_detector(model, img)
      5 show_result_pyplot(model, img, result)

4 frames
/content/mmdetection/mmdet/apis/inference.py in inference_detector(model, img)
     94     # prepare data
     95     data = dict(img=img)
---> 96     data = test_pipeline(data)
     97     data = collate([data], samples_per_gpu=1)
     98     if next(model.parameters()).is_cuda:

/content/mmdetection/mmdet/datasets/pipelines/compose.py in __call__(self, data)
     38 
     39         for t in self.transforms:
---> 40             data = t(data)
     41             if data is None:
     42                 return None

/content/mmdetection/mmdet/datasets/pipelines/test_time_aug.py in __call__(self, results)
    103                 _results['flip'] = flip
    104                 _results['flip_direction'] = direction
--> 105                 data = self.transforms(_results)
    106                 aug_data.append(data)
    107         # list of dict to dict of list

/content/mmdetection/mmdet/datasets/pipelines/compose.py in __call__(self, data)
     38 
     39         for t in self.transforms:
---> 40             data = t(data)
     41             if data is None:
     42                 return None

/content/mmdetection/mmdet/datasets/pipelines/transforms.py in __call__(self, results)
   1511         img = results['img']
   1512         assert img.dtype == np.float32, (
-> 1513             'RandomCenterCropPad needs the input image of dtype np.float32,'
   1514             ' please set ""to_float32=True"" in ""LoadImageFromFile"" pipeline')
   1515         h, w, c = img.shape

AssertionError: RandomCenterCropPad needs the input image of dtype np.float32, please set ""to_float32=True"" in ""LoadImageFromFile"" pipeline
```

",positive,positive
32,Training on low res images,matterport/Mask_RCNN,closed,,JonathanCMitchell,waleedka,2018-03-12 17:49:39,2018-03-29 22:39:31,20.0,,321.0,304475566.0,"Hi there, what would I need to change in the config / loss function in order to train this on lower resolution images? I would like to train on (256, 256, 3) images instead of (1024, 1024, 3) images. When I changed
```
IMAGE_MIN_DIM = 800 //4
IMAGE_MAX_DIM=800//4
USE_MINI_MASK=False
```
This seemed to work, but I started to get nan's for my rpn loss value. Any suggestions on what I might need to change in order to train on lower resolution images? (I am hoping that just by changing the IMAGE_MAX_DIM the code will actually resize my images for me). If that's not the case please let me know.

Thanks,
Edited 03/13/18 ===
```
   RES_SCALE_FACTOR = 4
    RPN_ANCHOR_SCALES = (32//RES_SCALE_FACTOR, 64//RES_SCALE_FACTOR, 128//RES_SCALE_FACTOR, 256//RES_SCALE_FACTOR, 512//RES_SCALE_FACTOR)
    IMAGE_MIN_DIM = 800 //RES_SCALE_FACTOR
    IMAGE_MAX_DIM = 1024 //RES_SCALE_FACTOR
    USE_MINI_MASK=False
```",neutral,positive
33,BatchNormalization is trainable even though it should not,matterport/Mask_RCNN,open,,b-nils,,2022-02-23 12:08:35,,0.0,,2780.0,1148010714.0,"**Current behavior**
After having set `TRAIN_BN = False` in [config.py](https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/config.py#L208), and having called [set_trainable()](https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L2201) with [`layer_regex[""all""]`](https://github.com/matterport/Mask_RCNN/blob/3deaec5d902d16e1daf56b62d5971d428dc920bc/mrcnn/model.py#L2313), `BatchNormalization` layers are trainable again.

**Expected behavior**
`BatchNormalization` layers should persist non-trainable if this was set at program startup.

Am I missing something here?  
Am I expected to simply exclude `BatchNormalization` layers from the `layer_regex`?

Thanks 👍🏽 ",neutral,negative
34,Video for ECCV workshop,Arthur151/ROMP,closed,,FishWoWater,Arthur151,2020-12-07 11:06:10,2020-12-07 11:16:56,8.0,,14.0,758409037.0,Can you share the video / ppt for your presentation at eccv 2020 workshop? I am also interested in solutions from other teams. But I can not find related materials on the internet.,neutral,neutral
35,Question about box_in_image,nutonomy/nuscenes-devkit,closed,,s-ryosky,whyekit-motional,2022-08-31 12:46:17,2022-08-31 13:37:16,1.0,,814.0,1357286498.0,"Why are you repeating similar actions with different thresholds?

https://github.com/nutonomy/nuscenes-devkit/blob/28765b8477dbd3331bacd922fada867c2c4db1d7/python-sdk/nuscenes/utils/geometry_utils.py#L73-L75",neutral,neutral
36,Question about validation,JiahuiYu/generative_inpainting,closed,,rcogrann,JiahuiYu,2019-06-11 08:40:01,2019-06-11 10:47:19,1.0,,273.0,454555544.0,"Hello @JiahuiYu, first thank you for your work,
I have a question about the content of the ""raw_incomplete_predicted_complete"" data in tensorboard. I assume that when you train your model with validation data (VAL=true), it shows the result of inpainting on validation images , but if you don't have validation data, does it show the result of inpainting on training images ?

 ",neutral,neutral
37,Training Time,wbw520/scouter,open,,EgeD,,2021-10-09 12:42:16,,0.0,,7.0,1021699836.0,"Hi, 
Initially, it's a great work congratulations. Secondly, How much does your training take and what was the data amount for the imageNet's first 100 classes ?

Thanks in advance",neutral,positive
38,Investigate why rrelu_test is flaky,tensorflow/addons,closed,,seanpmorgan,seanpmorgan,2019-12-21 15:50:07,2020-01-05 21:36:29,2.0,bug#help wanted#good first issue#activations#test-cases,802.0,541340716.0,"Example log:
https://source.cloud.google.com/results/invocations/2490d700-0837-4468-916a-0306b1714d87/targets/tensorflow_addons%2Fubuntu%2Fgpu%2Fpy3%2Fpresubmit/log

",neutral,neutral
39,feature extraction in THUMOS14,wzmsltw/BSN-boundary-sensitive-network.pytorch,open,,dreamedrainbow,,2021-07-12 06:52:49,,0.0,,30.0,941730758.0,"Thx for the excellent work for the community! I have two confusions hoping to be answered:
1. When reading the paper, I found the 2 stream network released in NIPS was used. But the TSN is used, when I read the code here. So what do we use?
2. I found that the process is different between THUMOS14 and ActivityNet in [Feature extraction questions #14](https://github.com/wzmsltw/BSN-boundary-sensitive-network.pytorch/issues/14#issuecomment-524163172). Could you sent me the code on THUMOS14 please? My email is  rainbowdream1991@gmail.com",neutral,positive
40,About Loading ResNet50 Pretrained Weights,open-mmlab/mmdetection,closed,hhaAndroid,SJLeo,hhaAndroid,2021-08-10 07:03:44,2021-08-30 09:40:23,3.0,,5849.0,964660955.0,"I trained a ResNet50 weight with Top1 acc of 78.86%. However, when I loaded it into the backbone in Faster RCNN. I achieved worse performance than the weight of torchvsion://resnet50.pth (Top1 acc 76.1%). I only modified the backbone pretrained path of faster_rcnn_r50_fpn_1x_coco.py
",neutral,neutral
41,How long did you take to train resnet100? ,deepinsight/insightface,open,,Gabit07,,2019-06-07 02:34:23,,30.0,,728.0,453314230.0,"1 NVIDIA TITAN Xp, MS-1M_v2, batch_size : 64
How long does it take to train a resnet100 model with environment and configuration?",neutral,neutral
42,Pytorch Deep CFR example uses wrong value for Policy Network layers,deepmind/open_spiel,closed,,newmanne,OpenSpiel,2021-08-13 18:28:26,2021-08-16 08:17:24,1.0,fixed,694.0,970625395.0,"The policy network seems to be created using the advantage network parameters and the policy network layer parameters appear to be unused. 

https://github.com/deepmind/open_spiel/blob/1506d1c7b14c0f7dfcc091c41824c5a0dbe1971e/open_spiel/python/pytorch/deep_cfr.py#L269",negative,neutral
43,AP is very low during the training,open-mmlab/mmdetection,closed,,sunnyisabaster,sunnyisabaster,2020-07-29 20:18:15,2020-08-03 19:25:18,5.0,reimplementation,3434.0,668123432.0,"**Notice**

There are several common situations in the reimplementation issues as below
1. Reimplement a model in the model zoo using the provided configs
2. Reimplement a model in the model zoo on other dataset (e.g., custom datasets)
3. Reimplement a custom model but all the components are implemented in MMDetection
4. Reimplement a custom model with new modules implemented by yourself

There are several things to do for different cases as below.
- For case 1 & 3, please follow the steps in the following sections thus we could help to quick identify the issue.
- For case 2 & 4, please understand that we are not able to do much help here because we usually do not know the full code and the users should be responsible to the code they write.
- One suggestion for case 2 & 4 is that the users should first check whether the bug lies in the self-implemented code or the original code. For example, users can first make sure that the same model runs well on supported datasets. If you still need help, please describe what you have done and what you obtain in the issue, and follow the steps in the following sections and try as clear as possible so that we can better help you.

**Checklist**
1. I have searched related issues but cannot get the expected help.
2. The issue has not been fixed in the latest version.

**Describe the issue**

The ap is very low during the training.

**Reproduction**
1. What command or script did you run?
```
python tools/train.py ./configs/cascade_rcnn/cascade_rcnn_x101_64x4d_fpn_20e_coco.py
```
2. What config dir you run?
```
model = dict(
    type='CascadeRCNN',
    pretrained='open-mmlab://resnext101_64x4d',
    backbone=dict(
        type='ResNeXt',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        groups=64,
        base_width=4),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=1,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=1,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=1,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]))
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.7,
                min_pos_iou=0.7,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ])
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.5),
        max_per_img=100))
dataset_type = 'MyDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(720, 1280), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(720, 1280),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='MyDataset',
        ann_file='data/coco/annotations/instances_train2017.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(720, 1280), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='MyDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(720, 1280),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='MyDataset',
        ann_file='data/coco/annotations/instances_test2017.json',
        img_prefix='data/coco/test2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(720, 1280),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(metric=['bbox'])
optimizer = dict(type='SGD', lr=1e-05, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[16, 19])
total_epochs = 20
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'checkpoints/1.pth'
resume_from = None
workflow = [('train', 1), ('val', 1)]
work_dir = './work_dirs/cascade_rcnn_x101_64x4d_fpn_20e_coco'
gpu_ids = range(0, 1)
```
3. Did you make any modifications on the code or config? Did you understand what you have modified?
4. What dataset did you use?

**Environment**

2020-07-29 15:13:25,212 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.7 (default, Mar 23 2020, 22:36:06) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GPU 0,1: GeForce GTX 1080 Ti
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.6.0a0+82fd1c8
OpenCV: 4.3.0
MMCV: 1.0.2
MMDetection: 2.3.0rc0+d613f21
MMDetection Compiler: GCC 7.3
MMDetection CUDA Compiler: 10.1

**Results**
# I don't know why the ap was keeping 0 and -1 at this step
If applicable, paste the related results here, e.g., what you expect and what you get.
```
2020-07-29 19:44:11,810 - mmdet - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=2.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=16.20s).
Accumulating evaluation results...
DONE (t=1.87s).
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.035
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.008
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.192
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.051
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.177
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.188
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.188
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
2020-07-30 19:16:07,328 - mmdet - INFO - Epoch [3][4490/4490]	lr: 1.250e-03, bbox_mAP: 0.0130, bbox_mAP_50: 0.0350, bbox_mAP_75: 0.0080, bbox_mAP_s: 0.1920, bbox_mAP_m: -1.0000, bbox_mAP_l: -1.0000, bbox_mAP_copypaste: 0.013 0.035 0.008 0.192 -1.000 -1.000
------------------------------
[   0 8980]
------------------------------
2020-07-30 19:17:07,265 - mmdet - INFO - Epoch [4][50/4490]	lr: 1.250e-03, eta: 23:52:23, time: 1.198, data_time: 0.052, memory: 9096, loss_rpn_cls: 0.0051, loss_rpn_bbox: 0.0027, s0.loss_cls: 0.0536, s0.acc: 97.9102, s0.loss_bbox: 0.0202, s1.loss_cls: 0.0323, s1.acc: 97.4316, s1.loss_bbox: 0.0259, s2.loss_cls: 0.0179, s2.acc: 97.0547, s2.loss_bbox: 0.0206, loss: 0.1783
2020-07-30 19:18:05,698 - mmdet - INFO - Epoch [4][100/4490]	lr: 1.250e-03, eta: 23:51:38, time: 1.169, data_time: 0.009, memory: 9096, loss_rpn_cls: 0.0042, loss_rpn_bbox: 0.0039, s0.loss_cls: 0.0489, s0.acc: 97.9336, s0.loss_bbox: 0.0199, s1.loss_cls: 0.0304, s1.acc: 97.4414, s1.loss_bbox: 0.0265, s2.loss_cls: 0.0169, s2.acc: 97.1211, s2.loss_bbox: 0.0210, loss: 0.1717
```
**dataset sample**
```
{
    ""info"": ""spytensor created"", 
    ""license"": [""license""], 
    ""annotations"": [{""segmentation"": [1278.5, 1.5, 1253.5, 41.5, 1214.5, 88.5, 1165.5, 180.5, 1147.5, 218.5, 1171.5, 230.5, 1171.5, 250.5, 1205.5, 247.5, 1226.5, 234.5, 1197.5, 237.5, 1238.5, 226.5, 1280.5, 217.5], ""area"": 1.0, ""iscrowd"": 0, ""image_id"": 0, ""bbox"": [1147.0, 1.0, 1280.0, 250.0], ""category_id"": 0, ""id"": 24}, {""segmentation"": [1277.5, 2.5, 1238.5, 31.5, 1181.5, 77.5, 1139.5, 68.5, 1088.5, 75.5, 1041.5, 104.5, 1003.5, 109.5, 940.5, 152.5, 880.5, 217.5, 807.5, 297.5, 783.5, 348.5, 807.5, 361.5, 806.5, 378.5, 837.5, 380.5, 857.5, 367.5, 919.5, 364.5, 942.5, 352.5, 1018.5, 336.5, 1076.5, 323.5, 1152.5, 292.5, 1212.5, 248.5, 1260.5, 207.5, 1261.5, 179.5, 1257.5, 173.5, 1277.5, 172.5], ""area"": 1.0, ""iscrowd"": 0, ""image_id"": 1, ""bbox"": [783.0, 2.0, 1277.0, 380.0], ""category_id"": 0, ""id"": 50}], 
    ""images"": [{""file_name"": ""aja-helo-1H000314_2017-12-01_0000/499.jpg"", ""height"": 720, ""width"": 1280, ""id"": 0}, {""file_name"": ""aja-helo-1H000314_2017-12-01_0000/508.jpg"", ""height"": 720, ""width"": 1280, ""id"": 1}], 
    ""categories"": [{""id"": 0, ""name"": ""fish""}]
}
```
**Issue fix**

If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
",negative,positive
44,issue when loading a checkpoint file,clovaai/tunit,closed,,anish9,anish9,2020-06-30 17:37:46,2020-07-02 06:59:53,2.0,,8.0,648372539.0,"Hi, I used the following code load the checkpoint:

G = DataParallel(Generator(img_size=512,sty_dim=128))
C = DataParallel(GuidingNet(128))
load_file = ""./ologs/GAN_20200630-174700/model_42.ckpt""
checkpoint = torch.load(load_file, map_location=""cuda:0"")
G.load_state_dict(checkpoint[""G_EMA_state_dict""]) #G_EMA_state_dict
C.load_state_dict(checkpoint['C_EMA_state_dict'])
G.to(device=""cuda"")
C.to(device=""cuda"")
G.eval()
C.eval()
c_src = G.cnt_encoder(x_src)
s_ref = C.moco(x_ref)
x_res = G.decode(c_src, s_ref)

i am getting the following error:
`
AttributeError: 'DataParallel' object has no attribute 'cnt_encoder'`

since i don't have great experience with pytorch i am not able to figure how to actually load without dataparallel. let me know how i can solve and run the inference part,
Thanks",neutral,positive
45,Add Keras implementation for ProximalAdagradOptimizer,tensorflow/addons,closed,WindQAQ,yhliang2018,WindQAQ,2019-10-10 21:08:43,2020-07-12 09:33:32,0.0,optimizers#Feature Request,591.0,505512413.0,"**Describe the feature and the current behavior/state.**
ProximalAdagradOptimizer is currently implemented in TF 1.x as [`tf.train.ProximalAdagradOptimizer`.](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/training/proximal_adagrad.py)
As its API is deprecated in TF 2.0 and users are encouraged to use `tf.keras.optimizers.*` in TF 2.0, we need an equivalent Keras-version implementation for it. Besides, `ProximalAdagradOptimizer` is supported in [CannedEstimatorV1,](https://github.com/tensorflow/estimator/blob/3a35a2b01a5831cf7b22fa6226e7547d2e9d0465/tensorflow_estimator/python/estimator/canned/dnn_linear_combined.py#L438) but not yet supported in the V2 version due to the lack of its Keras version. It should be prioritized to fill the feature gap asap.

**Relevant information**
- Are you willing to contribute it (yes/no): 
Sorry but I don't have the bandwith now. 
- Are you willing to maintain it going forward? (yes/no):
Sure, I can help on this.
- Is there a relevant academic paper? (if so, where):
Yes. Check this [paper](http://papers.nips.cc/paper/3793-efficient-learning-using-forward-backward-splitting.pdf).
- Is there already an implementation in another framework? (if so, where):
Yes. It's already implemented in tensorflow [here](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/training/proximal_adagrad.py).
- Was it part of tf.contrib? (if so, where):
No.

**Which API type would this fall under (layer, metric, optimizer, etc.)**
optimizer
**Who will benefit with this feature?**
All users that use `tf.train.ProximalAdagradOptimizer` in 1.x and want to migrate to 2.0.
**Any other info.**
",neutral,negative
46,Where to get the mot17.train? ,Zhongdao/Towards-Realtime-MOT,open,,vickyliuliu,,2020-07-15 07:49:02,,1.0,,176.0,657140197.0,"Hello, I'm wondering how to get the ./data/mot17.train file mentioned in .../src/lib/cfg/data.json? It seems it does not exist in the MOT17 dataset? Or do I just need to produce one by myself? 
Thank you~ ",neutral,neutral
47,Question: Is there any information that can be used to identify Open Spiel supports?,deepmind/open_spiel,closed,,Hidetomi,lanctot,2019-09-15 06:34:12,2019-09-17 03:40:09,2.0,,55.0,493701720.0,"I am not familiar with game classification.

For example, I want to add tetris.
But I don't know if Tetris applies to the classification described in [Open Spiel supports](https://github.com/Hidetomi/open_spiel/blob/master/docs/intro.md#what-is-openspiel). Is there any information that can be used to identify it?

Thank you.",neutral,neutral
48,Confusing about the Deepfillv2 training d_loss,JiahuiYu/generative_inpainting,closed,,ewrfcas,JiahuiYu,2020-09-03 03:37:13,2020-11-25 15:29:04,1.0,,467.0,691607056.0,"Sorry to bother you but I have some problems while reimplement training the deepfillv2 model.
The d_loss is converged to 1.0 which means that the discriminator output the same value for both real and fake samples.
Should I train the netD for few times (maybe 5?) after once train the netG?
For some reason I can't use the tensorboard. Here is the manual loss pic.
![image](https://user-images.githubusercontent.com/12042259/92068440-80a1e080-edd9-11ea-8d5b-5ac92a184c20.png)
![image](https://user-images.githubusercontent.com/12042259/92068445-87305800-edd9-11ea-9ba6-331968e16a7d.png)
![image](https://user-images.githubusercontent.com/12042259/92068460-92838380-edd9-11ea-8efb-9fe1a71293e5.png)

The model results are not good
![image](https://user-images.githubusercontent.com/12042259/92068533-c2328b80-edd9-11ea-9736-3c4f469c3377.png)


",neutral,negative
49,How to optimize hyper parameters using some frame works such as optuna?,open-mmlab/mmdetection,closed,BIGWangYuDong,AstroYuta,BIGWangYuDong,2021-09-24 08:19:45,2021-10-26 03:50:51,4.0,,6158.0,1006208202.0,"Hi forks! Thanks for developing this inspiring project! 

I am currently working on instance segmentations using Mask R-CNN or Cascade R-CNN, and very new to this field. Note that my project is trying to identify thousands (>2000) rocks in an image.

I would like to optimize hyper parameters by using frameworks such as optuna (https://github.com/optuna/optuna), but I cannot figure out how to implement those frameworks. Especially, how do you get losses (ex. val/loss) or accuracy in each epoch?

My provisional ways to optimize according to some documents (ex. https://medium.com/pytorch/using-optuna-to-optimize-pytorch-hyperparameters-990607385e36) are:

**1. edit hyper parameters in config files using trial class objects of optuna**
In /mmdetection/configs/_base_/models/mask_rcnn_r50_fpn.py
##### before
```
...
rpn_proposal=dict(
            nms_pre=4000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0)
...
```

##### after
```
...
rpn_proposal=dict(
            nms_pre=trial.suggest_uniform('nms_pre', 1000, 4000), <= Changed
            max_per_img=trial.suggest_uniform('max_per_img', 1000, 4000), <= Changed
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0)
```

**2. Return loss values or accuracy in each epoch (How??)**
In tools/train.py

```
def objective(trial): <= Changed from train method
...
    train_detector(
            model,
            datasets,
            cfg,
            distributed=distributed,
            validate=(not args.no_validate),
            timestamp=timestamp,
            meta=meta)
    return accuracy <= Added.
...

```

**3. Run the trials and obtain best hyper parameters**
In tools/train.py

```
import optuna
...
if __name__ == '__main__':
    study = optuna.create_study(direction=""maximize"")
    study.optimize(objective, n_trials=100)
```


At last, my questions are:
1. Is it possible to get loss values or accuracy in each epoch? Where can i get them?
2. If you have any advice to the usage of optuna, please let me know.
2. If you know some frameworks for optimizing hyper parameters working very well with MMDetection rather than optuna, please let me know.

Thanks!",neutral,negative
50,git clone https://github.com/facebookresearch/mmf.git ,facebookresearch/mmf,closed,,sandeepmte,vedanuj,2021-01-20 15:47:40,2021-01-21 01:43:09,3.0,,743.0,790098437.0,"## 🐛 Bug

(vqa_v1) F:\Pythia_V1\~\mmf\mmf_cli>python run.py --datasets TextVQA --model lorra --config \configs/vqa/textvqa/lorra.yaml
Traceback (most recent call last):
  File ""run.py"", line 9, in <module>
    from mmf.common.registry import registry
ModuleNotFoundError: No module named 'mmf.common'
",neutral,neutral
51,Did anyone try to train the mobilefacenet with mobilenetv3?,deepinsight/insightface,closed,,Royzon,Royzon,2019-06-20 07:49:29,2019-07-08 09:37:12,1.0,,751.0,458407193.0,"I tryed to train mobilefacenet with [mobilenetv3](https://github.com/xwu6614555/MobileNetV3-Mxnet), but got very worse result as bellow:

INFO:root:Epoch[0] Batch [0-20]	Speed: 1383.73 samples/sec	acc=0.000000	lossvalue=45.870661
INFO:root:Epoch[0] Batch [20-40]	Speed: 1478.24 samples/sec	acc=0.000000	lossvalue=45.791344
INFO:root:Epoch[0] Batch [40-60]	Speed: 1529.15 samples/sec	acc=0.000000	lossvalue=45.693285
INFO:root:Epoch[0] Batch [60-80]	Speed: 1486.48 samples/sec	acc=0.000000	lossvalue=45.641450
INFO:root:Epoch[0] Batch [80-100]	Speed: 1319.35 samples/sec	acc=0.000000	lossvalue=45.618647
INFO:root:Epoch[0] Batch [100-120]	Speed: 1443.19 samples/sec	acc=0.000000	lossvalue=45.513809
INFO:root:Epoch[0] Batch [120-140]	Speed: 1423.21 samples/sec	acc=0.000000	lossvalue=45.537901
INFO:root:Epoch[0] Batch [140-160]	Speed: 1412.56 samples/sec	acc=0.000000	lossvalue=45.422717
INFO:root:Epoch[0] Batch [160-180]	Speed: 1368.57 samples/sec	acc=0.000000	lossvalue=45.396835
INFO:root:Epoch[0] Batch [180-200]	Speed: 1403.32 samples/sec	acc=0.000000	lossvalue=45.292634
INFO:root:Epoch[0] Batch [200-220]	Speed: 1396.86 samples/sec	acc=0.000000	lossvalue=45.342935
INFO:root:Epoch[0] Batch [220-240]	Speed: 1398.27 samples/sec	acc=0.000000	lossvalue=45.262554
INFO:root:Epoch[0] Batch [240-260]	Speed: 1414.64 samples/sec	acc=0.000000	lossvalue=45.187489
INFO:root:Epoch[0] Batch [260-280]	Speed: 1410.47 samples/sec	acc=0.000000	lossvalue=45.190232
INFO:root:Epoch[0] Batch [280-300]	Speed: 1422.95 samples/sec	acc=0.000000	lossvalue=45.187069
INFO:root:Epoch[0] Batch [300-320]	Speed: 1353.78 samples/sec	acc=0.000000	lossvalue=45.118768
INFO:root:Epoch[0] Batch [320-340]	Speed: 1449.97 samples/sec	acc=0.000000	lossvalue=45.086011
INFO:root:Epoch[0] Batch [340-360]	Speed: 1469.77 samples/sec	acc=0.000000	lossvalue=45.082504
INFO:root:Epoch[0] Batch [360-380]	Speed: 1439.81 samples/sec	acc=0.000000	lossvalue=45.032984
INFO:root:Epoch[0] Batch [380-400]	Speed: 1420.65 samples/sec	acc=0.000000	lossvalue=44.992775
INFO:root:Epoch[0] Batch [400-420]	Speed: 1443.83 samples/sec	acc=0.000000	lossvalue=44.951703
INFO:root:Epoch[0] Batch [420-440]	Speed: 1448.53 samples/sec	acc=0.000000	lossvalue=44.939882
INFO:root:Epoch[0] Batch [440-460]	Speed: 1398.47 samples/sec	acc=0.000000	lossvalue=44.914947
INFO:root:Epoch[0] Batch [460-480]	Speed: 1302.95 samples/sec	acc=0.000000	lossvalue=44.818283
INFO:root:Epoch[0] Batch [480-500]	Speed: 1448.03 samples/sec	acc=0.000000	lossvalue=44.843665
INFO:root:Epoch[0] Batch [500-520]	Speed: 1446.03 samples/sec	acc=0.000000	lossvalue=44.770267
INFO:root:Epoch[0] Batch [520-540]	Speed: 1407.06 samples/sec	acc=0.000000	lossvalue=44.787757
INFO:root:Epoch[0] Batch [540-560]	Speed: 1410.15 samples/sec	acc=0.000000	lossvalue=44.759510
INFO:root:Epoch[0] Batch [560-580]	Speed: 1414.18 samples/sec	acc=0.000000	lossvalue=44.713844
INFO:root:Epoch[0] Batch [580-600]	Speed: 1397.30 samples/sec	acc=0.000000	lossvalue=44.694967
INFO:root:Epoch[0] Batch [600-620]	Speed: 1432.31 samples/sec	acc=0.000000	lossvalue=44.672683
INFO:root:Epoch[0] Batch [620-640]	Speed: 1428.93 samples/sec	acc=0.000000	lossvalue=44.670642
INFO:root:Epoch[0] Batch [640-660]	Speed: 1398.66 samples/sec	acc=0.000000	lossvalue=44.618105
INFO:root:Epoch[0] Batch [660-680]	Speed: 1450.98 samples/sec	acc=0.000000	lossvalue=44.593677
INFO:root:Epoch[0] Batch [680-700]	Speed: 1445.77 samples/sec	acc=0.000000	lossvalue=44.566131
INFO:root:Epoch[0] Batch [700-720]	Speed: 1443.75 samples/sec	acc=0.000000	lossvalue=44.497654
INFO:root:Epoch[0] Batch [720-740]	Speed: 1429.67 samples/sec	acc=0.000000	lossvalue=44.470155
INFO:root:Epoch[0] Batch [740-760]	Speed: 1442.88 samples/sec	acc=0.000000	lossvalue=44.450265
INFO:root:Epoch[0] Batch [760-780]	Speed: 1444.47 samples/sec	acc=0.000000	lossvalue=44.425858
INFO:root:Epoch[0] Batch [780-800]	Speed: 1438.24 samples/sec	acc=0.000000	lossvalue=44.400564
INFO:root:Epoch[0] Batch [800-820]	Speed: 1435.24 samples/sec	acc=0.000000	lossvalue=44.400841
INFO:root:Epoch[0] Batch [820-840]	Speed: 1419.75 samples/sec	acc=0.000000	lossvalue=44.402317
INFO:root:Epoch[0] Batch [840-860]	Speed: 1436.97 samples/sec	acc=0.000000	lossvalue=44.341323
INFO:root:Epoch[0] Batch [860-880]	Speed: 1414.36 samples/sec	acc=0.000000	lossvalue=44.396828
INFO:root:Epoch[0] Batch [880-900]	Speed: 1430.75 samples/sec	acc=0.000000	lossvalue=44.332490
INFO:root:Epoch[0] Batch [900-920]	Speed: 1417.48 samples/sec	acc=0.000000	lossvalue=44.267582
INFO:root:Epoch[0] Batch [920-940]	Speed: 1432.93 samples/sec	acc=0.000000	lossvalue=44.277461
INFO:root:Epoch[0] Batch [940-960]	Speed: 1415.93 samples/sec	acc=0.000000	lossvalue=44.248853
INFO:root:Epoch[0] Batch [960-980]	Speed: 1417.95 samples/sec	acc=0.000000	lossvalue=44.262166
lr-batch-epoch: 0.01 999 0
INFO:root:Epoch[0] Batch [980-1000]	Speed: 1331.41 samples/sec	acc=0.000000	lossvalue=44.207125
INFO:root:Epoch[0] Batch [1000-1020]	Speed: 1421.38 samples/sec	acc=0.000000	lossvalue=44.183258
INFO:root:Epoch[0] Batch [1020-1040]	Speed: 1436.71 samples/sec	acc=0.000000	lossvalue=44.153931
INFO:root:Epoch[0] Batch [1040-1060]	Speed: 1440.20 samples/sec	acc=0.000000	lossvalue=44.121399
INFO:root:Epoch[0] Batch [1060-1080]	Speed: 1409.65 samples/sec	acc=0.000000	lossvalue=44.097820
INFO:root:Epoch[0] Batch [1080-1100]	Speed: 1393.39 samples/sec	acc=0.000000	lossvalue=44.112846
INFO:root:Epoch[0] Batch [1100-1120]	Speed: 1451.69 samples/sec	acc=0.000000	lossvalue=44.098074
INFO:root:Epoch[0] Batch [1120-1140]	Speed: 1444.11 samples/sec	acc=0.000000	lossvalue=44.076516
INFO:root:Epoch[0] Batch [1140-1160]	Speed: 1482.61 samples/sec	acc=0.000000	lossvalue=43.990565
INFO:root:Epoch[0] Batch [1160-1180]	Speed: 1465.92 samples/sec	acc=0.000000	lossvalue=44.019992
INFO:root:Epoch[0] Batch [1180-1200]	Speed: 1442.89 samples/sec	acc=0.000000	lossvalue=44.019296
INFO:root:Epoch[0] Batch [1200-1220]	Speed: 1450.66 samples/sec	acc=0.000000	lossvalue=43.938944
INFO:root:Epoch[0] Batch [1220-1240]	Speed: 1453.08 samples/sec	acc=0.000000	lossvalue=43.946642
INFO:root:Epoch[0] Batch [1240-1260]	Speed: 1416.95 samples/sec	acc=0.000000	lossvalue=43.960813
INFO:root:Epoch[0] Batch [1260-1280]	Speed: 1438.48 samples/sec	acc=0.000000	lossvalue=43.953425
INFO:root:Epoch[0] Batch [1280-1300]	Speed: 1449.19 samples/sec	acc=0.000000	lossvalue=43.938370
INFO:root:Epoch[0] Batch [1300-1320]	Speed: 1434.35 samples/sec	acc=0.000000	lossvalue=43.881000
INFO:root:Epoch[0] Batch [1320-1340]	Speed: 1431.79 samples/sec	acc=0.000000	lossvalue=43.851503
INFO:root:Epoch[0] Batch [1340-1360]	Speed: 1440.55 samples/sec	acc=0.000000	lossvalue=43.858247
INFO:root:Epoch[0] Batch [1360-1380]	Speed: 1429.69 samples/sec	acc=0.000000	lossvalue=43.866306
INFO:root:Epoch[0] Batch [1380-1400]	Speed: 1431.58 samples/sec	acc=0.000000	lossvalue=43.823151
INFO:root:Epoch[0] Batch [1400-1420]	Speed: 1469.46 samples/sec	acc=0.000000	lossvalue=43.824927
INFO:root:Epoch[0] Batch [1420-1440]	Speed: 1457.58 samples/sec	acc=0.000000	lossvalue=43.790226
INFO:root:Epoch[0] Batch [1440-1460]	Speed: 1438.75 samples/sec	acc=0.000000	lossvalue=43.781603
INFO:root:Epoch[0] Batch [1460-1480]	Speed: 1426.49 samples/sec	acc=0.000000	lossvalue=43.761189
INFO:root:Epoch[0] Batch [1480-1500]	Speed: 1436.99 samples/sec	acc=0.000000	lossvalue=43.762128
INFO:root:Epoch[0] Batch [1500-1520]	Speed: 1441.87 samples/sec	acc=0.000000	lossvalue=43.748407
INFO:root:Epoch[0] Batch [1520-1540]	Speed: 1429.04 samples/sec	acc=0.000000	lossvalue=43.719254
INFO:root:Epoch[0] Batch [1540-1560]	Speed: 1428.21 samples/sec	acc=0.000000	lossvalue=43.722699
INFO:root:Epoch[0] Batch [1560-1580]	Speed: 1336.43 samples/sec	acc=0.000000	lossvalue=43.743402
INFO:root:Epoch[0] Batch [1580-1600]	Speed: 1458.24 samples/sec	acc=0.000000	lossvalue=43.732825
INFO:root:Epoch[0] Batch [1600-1620]	Speed: 1424.19 samples/sec	acc=0.000000	lossvalue=43.661644
INFO:root:Epoch[0] Batch [1620-1640]	Speed: 1449.55 samples/sec	acc=0.000000	lossvalue=43.642130
INFO:root:Epoch[0] Batch [1640-1660]	Speed: 1485.67 samples/sec	acc=0.000000	lossvalue=43.653942
INFO:root:Epoch[0] Batch [1660-1680]	Speed: 1438.97 samples/sec	acc=0.000000	lossvalue=43.629233
INFO:root:Epoch[0] Batch [1680-1700]	Speed: 1462.65 samples/sec	acc=0.000000	lossvalue=43.580198
INFO:root:Epoch[0] Batch [1700-1720]	Speed: 1416.80 samples/sec	acc=0.000000	lossvalue=43.591219
INFO:root:Epoch[0] Batch [1720-1740]	Speed: 1450.70 samples/sec	acc=0.000000	lossvalue=43.565578
INFO:root:Epoch[0] Batch [1740-1760]	Speed: 1443.97 samples/sec	acc=0.000000	lossvalue=43.544191
INFO:root:Epoch[0] Batch [1760-1780]	Speed: 1431.65 samples/sec	acc=0.000000	lossvalue=43.568670
INFO:root:Epoch[0] Batch [1780-1800]	Speed: 1459.77 samples/sec	acc=0.000000	lossvalue=43.550752
INFO:root:Epoch[0] Batch [1800-1820]	Speed: 1444.76 samples/sec	acc=0.000000	lossvalue=43.517789
INFO:root:Epoch[0] Batch [1820-1840]	Speed: 1446.74 samples/sec	acc=0.000000	lossvalue=43.470806
INFO:root:Epoch[0] Batch [1840-1860]	Speed: 1446.13 samples/sec	acc=0.000000	lossvalue=43.482472
INFO:root:Epoch[0] Batch [1860-1880]	Speed: 1437.97 samples/sec	acc=0.000000	lossvalue=43.475338
INFO:root:Epoch[0] Batch [1880-1900]	Speed: 1465.48 samples/sec	acc=0.000000	lossvalue=43.460360
INFO:root:Epoch[0] Batch [1900-1920]	Speed: 1432.71 samples/sec	acc=0.000000	lossvalue=43.460346
INFO:root:Epoch[0] Batch [1920-1940]	Speed: 1393.56 samples/sec	acc=0.000000	lossvalue=43.419805
INFO:root:Epoch[0] Batch [1940-1960]	Speed: 1433.58 samples/sec	acc=0.000000	lossvalue=43.436627
INFO:root:Epoch[0] Batch [1960-1980]	Speed: 1412.13 samples/sec	acc=0.000000	lossvalue=43.436714
lr-batch-epoch: 0.01 1999 0
testing verification..
(12000, 512)
infer time 4.767922
[agedb_30][2000]XNorm: 5.186779
[agedb_30][2000]Accuracy-Flip: 0.56133+-0.01785
testing verification..
(12000, 512)
infer time 4.882558
[calfw][2000]XNorm: 5.326550
[calfw][2000]Accuracy-Flip: 0.50433+-0.01352
testing verification..
(14000, 512)
infer time 5.835709
[cfp_ff][2000]XNorm: 24.351639
[cfp_ff][2000]Accuracy-Flip: 0.64257+-0.02092
testing verification..
(14000, 512)
infer time 6.510527
[cfp_fp][2000]XNorm: 21.560500
[cfp_fp][2000]Accuracy-Flip: 0.60300+-0.02718
testing verification..
(12000, 512)
infer time 5.20483
[cplfw][2000]XNorm: 8.356117
[cplfw][2000]Accuracy-Flip: 0.50817+-0.01463
testing verification..
(12000, 512)
infer time 5.349741
[lfw][2000]XNorm: 3.610347
[lfw][2000]Accuracy-Flip: 0.52283+-0.02500
saving 1
INFO:root:Saved checkpoint to ""./models/m3-arcface-emore/model-0001.params""
[2000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [1980-2000]	Speed: 170.20 samples/sec	acc=0.000000	lossvalue=43.404378
INFO:root:Epoch[0] Batch [2000-2020]	Speed: 1397.33 samples/sec	acc=0.000000	lossvalue=43.363487
INFO:root:Epoch[0] Batch [2020-2040]	Speed: 1397.93 samples/sec	acc=0.000000	lossvalue=43.367393
INFO:root:Epoch[0] Batch [2040-2060]	Speed: 1376.78 samples/sec	acc=0.000000	lossvalue=43.385328
INFO:root:Epoch[0] Batch [2060-2080]	Speed: 1391.53 samples/sec	acc=0.000000	lossvalue=43.348934
INFO:root:Epoch[0] Batch [2080-2100]	Speed: 1403.09 samples/sec	acc=0.000000	lossvalue=43.293539
INFO:root:Epoch[0] Batch [2100-2120]	Speed: 1359.96 samples/sec	acc=0.000000	lossvalue=43.275719
INFO:root:Epoch[0] Batch [2120-2140]	Speed: 1335.08 samples/sec	acc=0.000000	lossvalue=43.333215
INFO:root:Epoch[0] Batch [2140-2160]	Speed: 1447.66 samples/sec	acc=0.000000	lossvalue=43.351860
INFO:root:Epoch[0] Batch [2160-2180]	Speed: 1438.23 samples/sec	acc=0.000000	lossvalue=43.268195
INFO:root:Epoch[0] Batch [2180-2200]	Speed: 1302.31 samples/sec	acc=0.000000	lossvalue=43.260030
INFO:root:Epoch[0] Batch [2200-2220]	Speed: 1428.12 samples/sec	acc=0.000000	lossvalue=43.272679
INFO:root:Epoch[0] Batch [2220-2240]	Speed: 1384.74 samples/sec	acc=0.000000	lossvalue=43.243468
INFO:root:Epoch[0] Batch [2240-2260]	Speed: 1449.10 samples/sec	acc=0.000000	lossvalue=43.249479
INFO:root:Epoch[0] Batch [2260-2280]	Speed: 1418.01 samples/sec	acc=0.000000	lossvalue=43.220001
INFO:root:Epoch[0] Batch [2280-2300]	Speed: 1416.75 samples/sec	acc=0.000000	lossvalue=43.239038
INFO:root:Epoch[0] Batch [2300-2320]	Speed: 1452.63 samples/sec	acc=0.000000	lossvalue=43.182532
INFO:root:Epoch[0] Batch [2320-2340]	Speed: 1425.29 samples/sec	acc=0.000000	lossvalue=43.180995
INFO:root:Epoch[0] Batch [2340-2360]	Speed: 1440.57 samples/sec	acc=0.000000	lossvalue=43.188975
INFO:root:Epoch[0] Batch [2360-2380]	Speed: 1434.21 samples/sec	acc=0.000000	lossvalue=43.196234
INFO:root:Epoch[0] Batch [2380-2400]	Speed: 1423.04 samples/sec	acc=0.000000	lossvalue=43.177007
INFO:root:Epoch[0] Batch [2400-2420]	Speed: 1407.31 samples/sec	acc=0.000000	lossvalue=43.145509
INFO:root:Epoch[0] Batch [2420-2440]	Speed: 1417.49 samples/sec	acc=0.000000	lossvalue=43.158761
INFO:root:Epoch[0] Batch [2440-2460]	Speed: 1426.18 samples/sec	acc=0.000000	lossvalue=43.141473
INFO:root:Epoch[0] Batch [2460-2480]	Speed: 1435.73 samples/sec	acc=0.000000	lossvalue=43.126241
INFO:root:Epoch[0] Batch [2480-2500]	Speed: 1432.84 samples/sec	acc=0.000000	lossvalue=43.093768
INFO:root:Epoch[0] Batch [2500-2520]	Speed: 1422.08 samples/sec	acc=0.000000	lossvalue=43.118095
INFO:root:Epoch[0] Batch [2520-2540]	Speed: 1454.36 samples/sec	acc=0.000000	lossvalue=43.107464
INFO:root:Epoch[0] Batch [2540-2560]	Speed: 1429.15 samples/sec	acc=0.000000	lossvalue=43.098206
INFO:root:Epoch[0] Batch [2560-2580]	Speed: 1425.54 samples/sec	acc=0.000000	lossvalue=43.083347
INFO:root:Epoch[0] Batch [2580-2600]	Speed: 1455.07 samples/sec	acc=0.000000	lossvalue=43.080341
INFO:root:Epoch[0] Batch [2600-2620]	Speed: 1451.78 samples/sec	acc=0.000000	lossvalue=43.062382
INFO:root:Epoch[0] Batch [2620-2640]	Speed: 1443.80 samples/sec	acc=0.000000	lossvalue=43.049131
INFO:root:Epoch[0] Batch [2640-2660]	Speed: 1406.09 samples/sec	acc=0.000000	lossvalue=43.024845
INFO:root:Epoch[0] Batch [2660-2680]	Speed: 1452.47 samples/sec	acc=0.000000	lossvalue=43.027570
INFO:root:Epoch[0] Batch [2680-2700]	Speed: 1407.33 samples/sec	acc=0.000000	lossvalue=43.035068
INFO:root:Epoch[0] Batch [2700-2720]	Speed: 1433.38 samples/sec	acc=0.000000	lossvalue=43.018409
INFO:root:Epoch[0] Batch [2720-2740]	Speed: 1445.41 samples/sec	acc=0.000000	lossvalue=43.000950
INFO:root:Epoch[0] Batch [2740-2760]	Speed: 1476.70 samples/sec	acc=0.000000	lossvalue=43.001233
INFO:root:Epoch[0] Batch [2760-2780]	Speed: 1418.75 samples/sec	acc=0.000000	lossvalue=42.993690
INFO:root:Epoch[0] Batch [2780-2800]	Speed: 1439.07 samples/sec	acc=0.000000	lossvalue=42.977153
INFO:root:Epoch[0] Batch [2800-2820]	Speed: 1406.74 samples/sec	acc=0.000000	lossvalue=42.964536
INFO:root:Epoch[0] Batch [2820-2840]	Speed: 1308.39 samples/sec	acc=0.000000	lossvalue=42.955763
INFO:root:Epoch[0] Batch [2840-2860]	Speed: 1418.14 samples/sec	acc=0.000000	lossvalue=42.965248
INFO:root:Epoch[0] Batch [2860-2880]	Speed: 1420.41 samples/sec	acc=0.000000	lossvalue=42.951956
INFO:root:Epoch[0] Batch [2880-2900]	Speed: 1416.45 samples/sec	acc=0.000000	lossvalue=42.900782
INFO:root:Epoch[0] Batch [2900-2920]	Speed: 1437.10 samples/sec	acc=0.000000	lossvalue=42.941202
INFO:root:Epoch[0] Batch [2920-2940]	Speed: 1427.38 samples/sec	acc=0.000000	lossvalue=42.905976
INFO:root:Epoch[0] Batch [2940-2960]	Speed: 1423.14 samples/sec	acc=0.000000	lossvalue=42.923396
INFO:root:Epoch[0] Batch [2960-2980]	Speed: 1432.20 samples/sec	acc=0.000000	lossvalue=42.925973
lr-batch-epoch: 0.01 2999 0
INFO:root:Epoch[0] Batch [2980-3000]	Speed: 1417.04 samples/sec	acc=0.000000	lossvalue=42.919900
INFO:root:Epoch[0] Batch [3000-3020]	Speed: 1436.58 samples/sec	acc=0.000000	lossvalue=42.874327
INFO:root:Epoch[0] Batch [3020-3040]	Speed: 1416.77 samples/sec	acc=0.000000	lossvalue=42.899130
INFO:root:Epoch[0] Batch [3040-3060]	Speed: 1426.80 samples/sec	acc=0.000000	lossvalue=42.854612
INFO:root:Epoch[0] Batch [3060-3080]	Speed: 1407.18 samples/sec	acc=0.000000	lossvalue=42.905168
INFO:root:Epoch[0] Batch [3080-3100]	Speed: 1414.27 samples/sec	acc=0.000000	lossvalue=42.866004
INFO:root:Epoch[0] Batch [3100-3120]	Speed: 1429.14 samples/sec	acc=0.000000	lossvalue=42.863892
INFO:root:Epoch[0] Batch [3120-3140]	Speed: 1426.98 samples/sec	acc=0.000000	lossvalue=42.844164
INFO:root:Epoch[0] Batch [3140-3160]	Speed: 1429.76 samples/sec	acc=0.000000	lossvalue=42.820837
INFO:root:Epoch[0] Batch [3160-3180]	Speed: 1434.44 samples/sec	acc=0.000000	lossvalue=42.846137
INFO:root:Epoch[0] Batch [3180-3200]	Speed: 1427.84 samples/sec	acc=0.000000	lossvalue=42.812166
INFO:root:Epoch[0] Batch [3200-3220]	Speed: 1436.46 samples/sec	acc=0.000000	lossvalue=42.801479
INFO:root:Epoch[0] Batch [3220-3240]	Speed: 1444.55 samples/sec	acc=0.000000	lossvalue=42.782662
INFO:root:Epoch[0] Batch [3240-3260]	Speed: 1418.60 samples/sec	acc=0.000000	lossvalue=42.834113
INFO:root:Epoch[0] Batch [3260-3280]	Speed: 1423.13 samples/sec	acc=0.000000	lossvalue=42.797234
INFO:root:Epoch[0] Batch [3280-3300]	Speed: 1453.12 samples/sec	acc=0.000000	lossvalue=42.787975
INFO:root:Epoch[0] Batch [3300-3320]	Speed: 1449.47 samples/sec	acc=0.000000	lossvalue=42.788614
INFO:root:Epoch[0] Batch [3320-3340]	Speed: 1428.52 samples/sec	acc=0.000000	lossvalue=42.770773
INFO:root:Epoch[0] Batch [3340-3360]	Speed: 1427.01 samples/sec	acc=0.000000	lossvalue=42.791105
INFO:root:Epoch[0] Batch [3360-3380]	Speed: 1414.08 samples/sec	acc=0.000000	lossvalue=42.767718
INFO:root:Epoch[0] Batch [3380-3400]	Speed: 1426.79 samples/sec	acc=0.000000	lossvalue=42.723938
INFO:root:Epoch[0] Batch [3400-3420]	Speed: 1428.09 samples/sec	acc=0.000000	lossvalue=42.720111
INFO:root:Epoch[0] Batch [3420-3440]	Speed: 1426.12 samples/sec	acc=0.000000	lossvalue=42.728325
INFO:root:Epoch[0] Batch [3440-3460]	Speed: 1321.42 samples/sec	acc=0.000000	lossvalue=42.722214
INFO:root:Epoch[0] Batch [3460-3480]	Speed: 1430.87 samples/sec	acc=0.000000	lossvalue=42.717883
INFO:root:Epoch[0] Batch [3480-3500]	Speed: 1423.54 samples/sec	acc=0.000000	lossvalue=42.723384
INFO:root:Epoch[0] Batch [3500-3520]	Speed: 1442.60 samples/sec	acc=0.000000	lossvalue=42.691322
INFO:root:Epoch[0] Batch [3520-3540]	Speed: 1427.28 samples/sec	acc=0.000000	lossvalue=42.682272
INFO:root:Epoch[0] Batch [3540-3560]	Speed: 1432.55 samples/sec	acc=0.000000	lossvalue=42.702149
INFO:root:Epoch[0] Batch [3560-3580]	Speed: 1425.38 samples/sec	acc=0.000000	lossvalue=42.702578
INFO:root:Epoch[0] Batch [3580-3600]	Speed: 1437.43 samples/sec	acc=0.000000	lossvalue=42.698026
INFO:root:Epoch[0] Batch [3600-3620]	Speed: 1400.61 samples/sec	acc=0.000000	lossvalue=42.677364
INFO:root:Epoch[0] Batch [3620-3640]	Speed: 1442.46 samples/sec	acc=0.000000	lossvalue=42.661396
INFO:root:Epoch[0] Batch [3640-3660]	Speed: 1437.35 samples/sec	acc=0.000000	lossvalue=42.672045
INFO:root:Epoch[0] Batch [3660-3680]	Speed: 1434.27 samples/sec	acc=0.000000	lossvalue=42.653893
INFO:root:Epoch[0] Batch [3680-3700]	Speed: 1416.24 samples/sec	acc=0.000000	lossvalue=42.647855
INFO:root:Epoch[0] Batch [3700-3720]	Speed: 1434.07 samples/sec	acc=0.000000	lossvalue=42.636478
INFO:root:Epoch[0] Batch [3720-3740]	Speed: 1451.74 samples/sec	acc=0.000000	lossvalue=42.657539
INFO:root:Epoch[0] Batch [3740-3760]	Speed: 1410.02 samples/sec	acc=0.000000	lossvalue=42.635114
INFO:root:Epoch[0] Batch [3760-3780]	Speed: 1439.91 samples/sec	acc=0.000000	lossvalue=42.629244
INFO:root:Epoch[0] Batch [3780-3800]	Speed: 1418.50 samples/sec	acc=0.000000	lossvalue=42.636886
INFO:root:Epoch[0] Batch [3800-3820]	Speed: 1404.63 samples/sec	acc=0.000000	lossvalue=42.611131
INFO:root:Epoch[0] Batch [3820-3840]	Speed: 1428.15 samples/sec	acc=0.000000	lossvalue=42.616557
INFO:root:Epoch[0] Batch [3840-3860]	Speed: 1406.44 samples/sec	acc=0.000000	lossvalue=42.608893
INFO:root:Epoch[0] Batch [3860-3880]	Speed: 1448.66 samples/sec	acc=0.000000	lossvalue=42.591905
INFO:root:Epoch[0] Batch [3880-3900]	Speed: 1437.10 samples/sec	acc=0.000000	lossvalue=42.607033
INFO:root:Epoch[0] Batch [3900-3920]	Speed: 1434.37 samples/sec	acc=0.000000	lossvalue=42.613058
INFO:root:Epoch[0] Batch [3920-3940]	Speed: 1468.91 samples/sec	acc=0.000000	lossvalue=42.591647
INFO:root:Epoch[0] Batch [3940-3960]	Speed: 1430.64 samples/sec	acc=0.000000	lossvalue=42.566539
INFO:root:Epoch[0] Batch [3960-3980]	Speed: 1442.96 samples/sec	acc=0.000000	lossvalue=42.585085
lr-batch-epoch: 0.01 3999 0
testing verification..
(12000, 512)
infer time 5.147187
[agedb_30][4000]XNorm: 2.132027
[agedb_30][4000]Accuracy-Flip: 0.51183+-0.01613
testing verification..
(12000, 512)
infer time 5.289282
[calfw][4000]XNorm: 3.467386
[calfw][4000]Accuracy-Flip: 0.50417+-0.01009
testing verification..
(14000, 512)
infer time 5.898726
[cfp_ff][4000]XNorm: 30.462818
[cfp_ff][4000]Accuracy-Flip: 0.66143+-0.02125
testing verification..
(14000, 512)
infer time 5.747373
[cfp_fp][4000]XNorm: 24.980808
[cfp_fp][4000]Accuracy-Flip: 0.61129+-0.02457
testing verification..
(12000, 512)
infer time 4.717469
[cplfw][4000]XNorm: 6.243488
[cplfw][4000]Accuracy-Flip: 0.50100+-0.00854
testing verification..
(12000, 512)
infer time 5.074133
[lfw][4000]XNorm: 2.288231
[lfw][4000]Accuracy-Flip: 0.49783+-0.01726
[4000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [3980-4000]	Speed: 175.71 samples/sec	acc=0.000000	lossvalue=42.550051
INFO:root:Epoch[0] Batch [4000-4020]	Speed: 1393.38 samples/sec	acc=0.000000	lossvalue=42.574287
INFO:root:Epoch[0] Batch [4020-4040]	Speed: 1423.98 samples/sec	acc=0.000000	lossvalue=42.553419
INFO:root:Epoch[0] Batch [4040-4060]	Speed: 1394.03 samples/sec	acc=0.000000	lossvalue=42.549029
INFO:root:Epoch[0] Batch [4060-4080]	Speed: 1423.38 samples/sec	acc=0.000000	lossvalue=42.536241
INFO:root:Epoch[0] Batch [4080-4100]	Speed: 1474.36 samples/sec	acc=0.000000	lossvalue=42.540261
INFO:root:Epoch[0] Batch [4100-4120]	Speed: 1302.93 samples/sec	acc=0.000000	lossvalue=42.547042
INFO:root:Epoch[0] Batch [4120-4140]	Speed: 1429.10 samples/sec	acc=0.000000	lossvalue=42.525436
INFO:root:Epoch[0] Batch [4140-4160]	Speed: 1427.53 samples/sec	acc=0.000000	lossvalue=42.532377
INFO:root:Epoch[0] Batch [4160-4180]	Speed: 1383.11 samples/sec	acc=0.000000	lossvalue=42.500257
INFO:root:Epoch[0] Batch [4180-4200]	Speed: 1432.20 samples/sec	acc=0.000000	lossvalue=42.516144
INFO:root:Epoch[0] Batch [4200-4220]	Speed: 1420.97 samples/sec	acc=0.000000	lossvalue=42.509694
INFO:root:Epoch[0] Batch [4220-4240]	Speed: 1421.21 samples/sec	acc=0.000000	lossvalue=42.520906
INFO:root:Epoch[0] Batch [4240-4260]	Speed: 1432.03 samples/sec	acc=0.000000	lossvalue=42.508407
INFO:root:Epoch[0] Batch [4260-4280]	Speed: 1442.09 samples/sec	acc=0.000000	lossvalue=42.490495
INFO:root:Epoch[0] Batch [4280-4300]	Speed: 1439.79 samples/sec	acc=0.000000	lossvalue=42.510910
INFO:root:Epoch[0] Batch [4300-4320]	Speed: 1427.76 samples/sec	acc=0.000000	lossvalue=42.469509
INFO:root:Epoch[0] Batch [4320-4340]	Speed: 1443.76 samples/sec	acc=0.000000	lossvalue=42.482943
INFO:root:Epoch[0] Batch [4340-4360]	Speed: 1442.26 samples/sec	acc=0.000000	lossvalue=42.456209
INFO:root:Epoch[0] Batch [4360-4380]	Speed: 1433.37 samples/sec	acc=0.000000	lossvalue=42.468422
INFO:root:Epoch[0] Batch [4380-4400]	Speed: 1436.84 samples/sec	acc=0.000000	lossvalue=42.462881
INFO:root:Epoch[0] Batch [4400-4420]	Speed: 1446.74 samples/sec	acc=0.000000	lossvalue=42.482299
INFO:root:Epoch[0] Batch [4420-4440]	Speed: 1467.64 samples/sec	acc=0.000000	lossvalue=42.459066
INFO:root:Epoch[0] Batch [4440-4460]	Speed: 1461.00 samples/sec	acc=0.000000	lossvalue=42.462655
INFO:root:Epoch[0] Batch [4460-4480]	Speed: 1415.87 samples/sec	acc=0.000000	lossvalue=42.460793
INFO:root:Epoch[0] Batch [4480-4500]	Speed: 1410.21 samples/sec	acc=0.000000	lossvalue=42.469575
INFO:root:Epoch[0] Batch [4500-4520]	Speed: 1430.43 samples/sec	acc=0.000000	lossvalue=42.429813
INFO:root:Epoch[0] Batch [4520-4540]	Speed: 1437.68 samples/sec	acc=0.000000	lossvalue=42.460910
INFO:root:Epoch[0] Batch [4540-4560]	Speed: 1415.48 samples/sec	acc=0.000000	lossvalue=42.440271
INFO:root:Epoch[0] Batch [4560-4580]	Speed: 1444.00 samples/sec	acc=0.000000	lossvalue=42.425996
INFO:root:Epoch[0] Batch [4580-4600]	Speed: 1445.10 samples/sec	acc=0.000000	lossvalue=42.444479
INFO:root:Epoch[0] Batch [4600-4620]	Speed: 1429.36 samples/sec	acc=0.000000	lossvalue=42.441484
INFO:root:Epoch[0] Batch [4620-4640]	Speed: 1441.15 samples/sec	acc=0.000000	lossvalue=42.407234
INFO:root:Epoch[0] Batch [4640-4660]	Speed: 1455.54 samples/sec	acc=0.000000	lossvalue=42.419878
INFO:root:Epoch[0] Batch [4660-4680]	Speed: 1428.00 samples/sec	acc=0.000000	lossvalue=42.431764
INFO:root:Epoch[0] Batch [4680-4700]	Speed: 1422.00 samples/sec	acc=0.000000	lossvalue=42.408870
INFO:root:Epoch[0] Batch [4700-4720]	Speed: 1419.59 samples/sec	acc=0.000000	lossvalue=42.403528
INFO:root:Epoch[0] Batch [4720-4740]	Speed: 1422.40 samples/sec	acc=0.000000	lossvalue=42.374300
INFO:root:Epoch[0] Batch [4740-4760]	Speed: 1328.92 samples/sec	acc=0.000000	lossvalue=42.394442
INFO:root:Epoch[0] Batch [4760-4780]	Speed: 1454.65 samples/sec	acc=0.000000	lossvalue=42.384118
INFO:root:Epoch[0] Batch [4780-4800]	Speed: 1428.79 samples/sec	acc=0.000000	lossvalue=42.393840
INFO:root:Epoch[0] Batch [4800-4820]	Speed: 1432.17 samples/sec	acc=0.000000	lossvalue=42.386306
INFO:root:Epoch[0] Batch [4820-4840]	Speed: 1430.07 samples/sec	acc=0.000000	lossvalue=42.375308
INFO:root:Epoch[0] Batch [4840-4860]	Speed: 1424.42 samples/sec	acc=0.000000	lossvalue=42.365072
INFO:root:Epoch[0] Batch [4860-4880]	Speed: 1433.53 samples/sec	acc=0.000000	lossvalue=42.370385
INFO:root:Epoch[0] Batch [4880-4900]	Speed: 1423.51 samples/sec	acc=0.000000	lossvalue=42.375198
INFO:root:Epoch[0] Batch [4900-4920]	Speed: 1428.65 samples/sec	acc=0.000000	lossvalue=42.384885
INFO:root:Epoch[0] Batch [4920-4940]	Speed: 1375.04 samples/sec	acc=0.000000	lossvalue=42.364101
INFO:root:Epoch[0] Batch [4940-4960]	Speed: 1418.07 samples/sec	acc=0.000000	lossvalue=42.331564
INFO:root:Epoch[0] Batch [4960-4980]	Speed: 1411.97 samples/sec	acc=0.000000	lossvalue=42.336429
lr-batch-epoch: 0.01 4999 0
INFO:root:Epoch[0] Batch [4980-5000]	Speed: 1383.41 samples/sec	acc=0.000000	lossvalue=42.342953
INFO:root:Epoch[0] Batch [5000-5020]	Speed: 1418.45 samples/sec	acc=0.000000	lossvalue=42.353621
INFO:root:Epoch[0] Batch [5020-5040]	Speed: 1447.26 samples/sec	acc=0.000000	lossvalue=42.346309
INFO:root:Epoch[0] Batch [5040-5060]	Speed: 1432.50 samples/sec	acc=0.000000	lossvalue=42.341695
INFO:root:Epoch[0] Batch [5060-5080]	Speed: 1440.10 samples/sec	acc=0.000000	lossvalue=42.317080
INFO:root:Epoch[0] Batch [5080-5100]	Speed: 1408.21 samples/sec	acc=0.000000	lossvalue=42.341804
INFO:root:Epoch[0] Batch [5100-5120]	Speed: 1439.04 samples/sec	acc=0.000000	lossvalue=42.332448
INFO:root:Epoch[0] Batch [5120-5140]	Speed: 1437.54 samples/sec	acc=0.000000	lossvalue=42.317316
INFO:root:Epoch[0] Batch [5140-5160]	Speed: 1448.88 samples/sec	acc=0.000000	lossvalue=42.329248
INFO:root:Epoch[0] Batch [5160-5180]	Speed: 1424.91 samples/sec	acc=0.000000	lossvalue=42.299067
INFO:root:Epoch[0] Batch [5180-5200]	Speed: 1415.99 samples/sec	acc=0.000000	lossvalue=42.319560
INFO:root:Epoch[0] Batch [5200-5220]	Speed: 1422.27 samples/sec	acc=0.000000	lossvalue=42.309711
INFO:root:Epoch[0] Batch [5220-5240]	Speed: 1427.98 samples/sec	acc=0.000000	lossvalue=42.298055
INFO:root:Epoch[0] Batch [5240-5260]	Speed: 1440.10 samples/sec	acc=0.000000	lossvalue=42.315811
INFO:root:Epoch[0] Batch [5260-5280]	Speed: 1437.36 samples/sec	acc=0.000000	lossvalue=42.300466
INFO:root:Epoch[0] Batch [5280-5300]	Speed: 1412.81 samples/sec	acc=0.000000	lossvalue=42.294637
INFO:root:Epoch[0] Batch [5300-5320]	Speed: 1393.58 samples/sec	acc=0.000000	lossvalue=42.288443
INFO:root:Epoch[0] Batch [5320-5340]	Speed: 1440.76 samples/sec	acc=0.000000	lossvalue=42.292585
INFO:root:Epoch[0] Batch [5340-5360]	Speed: 1420.52 samples/sec	acc=0.000000	lossvalue=42.291715
INFO:root:Epoch[0] Batch [5360-5380]	Speed: 1458.70 samples/sec	acc=0.000000	lossvalue=42.288405
INFO:root:Epoch[0] Batch [5380-5400]	Speed: 1438.32 samples/sec	acc=0.000000	lossvalue=42.278516
INFO:root:Epoch[0] Batch [5400-5420]	Speed: 1439.55 samples/sec	acc=0.000000	lossvalue=42.273524
INFO:root:Epoch[0] Batch [5420-5440]	Speed: 1303.37 samples/sec	acc=0.000000	lossvalue=42.284446
INFO:root:Epoch[0] Batch [5440-5460]	Speed: 1450.37 samples/sec	acc=0.000000	lossvalue=42.264119
INFO:root:Epoch[0] Batch [5460-5480]	Speed: 1416.76 samples/sec	acc=0.000000	lossvalue=42.270426
INFO:root:Epoch[0] Batch [5480-5500]	Speed: 1444.06 samples/sec	acc=0.000000	lossvalue=42.281589
INFO:root:Epoch[0] Batch [5500-5520]	Speed: 1461.23 samples/sec	acc=0.000000	lossvalue=42.248047
INFO:root:Epoch[0] Batch [5520-5540]	Speed: 1419.07 samples/sec	acc=0.000000	lossvalue=42.268784
INFO:root:Epoch[0] Batch [5540-5560]	Speed: 1425.21 samples/sec	acc=0.000000	lossvalue=42.271940
INFO:root:Epoch[0] Batch [5560-5580]	Speed: 1421.12 samples/sec	acc=0.000000	lossvalue=42.264134
INFO:root:Epoch[0] Batch [5580-5600]	Speed: 1430.31 samples/sec	acc=0.000000	lossvalue=42.274274
INFO:root:Epoch[0] Batch [5600-5620]	Speed: 1429.46 samples/sec	acc=0.000000	lossvalue=42.264286
INFO:root:Epoch[0] Batch [5620-5640]	Speed: 1420.31 samples/sec	acc=0.000000	lossvalue=42.248168
INFO:root:Epoch[0] Batch [5640-5660]	Speed: 1445.08 samples/sec	acc=0.000000	lossvalue=42.239975
INFO:root:Epoch[0] Batch [5660-5680]	Speed: 1424.62 samples/sec	acc=0.000000	lossvalue=42.237265
INFO:root:Epoch[0] Batch [5680-5700]	Speed: 1432.27 samples/sec	acc=0.000000	lossvalue=42.241098
INFO:root:Epoch[0] Batch [5700-5720]	Speed: 1400.27 samples/sec	acc=0.000000	lossvalue=42.227811
INFO:root:Epoch[0] Batch [5720-5740]	Speed: 1416.32 samples/sec	acc=0.000000	lossvalue=42.244482
INFO:root:Epoch[0] Batch [5740-5760]	Speed: 1406.91 samples/sec	acc=0.000000	lossvalue=42.220995
INFO:root:Epoch[0] Batch [5760-5780]	Speed: 1413.25 samples/sec	acc=0.000000	lossvalue=42.223339
INFO:root:Epoch[0] Batch [5780-5800]	Speed: 1433.31 samples/sec	acc=0.000000	lossvalue=42.230837
INFO:root:Epoch[0] Batch [5800-5820]	Speed: 1419.77 samples/sec	acc=0.000000	lossvalue=42.208267
INFO:root:Epoch[0] Batch [5820-5840]	Speed: 1434.48 samples/sec	acc=0.000000	lossvalue=42.216342
INFO:root:Epoch[0] Batch [5840-5860]	Speed: 1445.68 samples/sec	acc=0.000000	lossvalue=42.207249
INFO:root:Epoch[0] Batch [5860-5880]	Speed: 1412.29 samples/sec	acc=0.000000	lossvalue=42.231215
INFO:root:Epoch[0] Batch [5880-5900]	Speed: 1489.47 samples/sec	acc=0.000000	lossvalue=42.206901
INFO:root:Epoch[0] Batch [5900-5920]	Speed: 1431.93 samples/sec	acc=0.000000	lossvalue=42.200964
INFO:root:Epoch[0] Batch [5920-5940]	Speed: 1412.74 samples/sec	acc=0.000000	lossvalue=42.202295
INFO:root:Epoch[0] Batch [5940-5960]	Speed: 1399.04 samples/sec	acc=0.000000	lossvalue=42.190144
INFO:root:Epoch[0] Batch [5960-5980]	Speed: 1423.30 samples/sec	acc=0.000000	lossvalue=42.184068
lr-batch-epoch: 0.01 5999 0
testing verification..
(12000, 512)
infer time 5.024747
[agedb_30][6000]XNorm: 1.423212
[agedb_30][6000]Accuracy-Flip: 0.50133+-0.00323
testing verification..
(12000, 512)
infer time 5.24066
[calfw][6000]XNorm: 2.490709
[calfw][6000]Accuracy-Flip: 0.50133+-0.00702
testing verification..
(14000, 512)
infer time 5.529946
[cfp_ff][6000]XNorm: 24.229985
[cfp_ff][6000]Accuracy-Flip: 0.65329+-0.01836
testing verification..
(14000, 512)
infer time 6.060891
[cfp_fp][6000]XNorm: 19.544963
[cfp_fp][6000]Accuracy-Flip: 0.59671+-0.02189
testing verification..
(12000, 512)
infer time 5.416972
[cplfw][6000]XNorm: 5.137669
[cplfw][6000]Accuracy-Flip: 0.50150+-0.00529
testing verification..
(12000, 512)
infer time 4.913089
[lfw][6000]XNorm: 1.665831
[lfw][6000]Accuracy-Flip: 0.50117+-0.01229
[6000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [5980-6000]	Speed: 171.20 samples/sec	acc=0.000000	lossvalue=42.196841
INFO:root:Epoch[0] Batch [6000-6020]	Speed: 1382.05 samples/sec	acc=0.000000	lossvalue=42.192054
INFO:root:Epoch[0] Batch [6020-6040]	Speed: 1452.38 samples/sec	acc=0.000000	lossvalue=42.191133
INFO:root:Epoch[0] Batch [6040-6060]	Speed: 1376.12 samples/sec	acc=0.000000	lossvalue=42.189343
INFO:root:Epoch[0] Batch [6060-6080]	Speed: 1399.28 samples/sec	acc=0.000000	lossvalue=42.204668
INFO:root:Epoch[0] Batch [6080-6100]	Speed: 1326.18 samples/sec	acc=0.000000	lossvalue=42.176307
INFO:root:Epoch[0] Batch [6100-6120]	Speed: 1462.26 samples/sec	acc=0.000000	lossvalue=42.181716
INFO:root:Epoch[0] Batch [6120-6140]	Speed: 1434.85 samples/sec	acc=0.000000	lossvalue=42.196211
INFO:root:Epoch[0] Batch [6140-6160]	Speed: 1455.70 samples/sec	acc=0.000000	lossvalue=42.194986
INFO:root:Epoch[0] Batch [6160-6180]	Speed: 1426.39 samples/sec	acc=0.000000	lossvalue=42.171385
INFO:root:Epoch[0] Batch [6180-6200]	Speed: 1453.21 samples/sec	acc=0.000000	lossvalue=42.172093
INFO:root:Epoch[0] Batch [6200-6220]	Speed: 1408.12 samples/sec	acc=0.000000	lossvalue=42.170577
INFO:root:Epoch[0] Batch [6220-6240]	Speed: 1417.95 samples/sec	acc=0.000000	lossvalue=42.170736
INFO:root:Epoch[0] Batch [6240-6260]	Speed: 1447.37 samples/sec	acc=0.000000	lossvalue=42.161931
INFO:root:Epoch[0] Batch [6260-6280]	Speed: 1444.26 samples/sec	acc=0.000000	lossvalue=42.145989
INFO:root:Epoch[0] Batch [6280-6300]	Speed: 1437.50 samples/sec	acc=0.000000	lossvalue=42.140790
INFO:root:Epoch[0] Batch [6300-6320]	Speed: 1441.20 samples/sec	acc=0.000000	lossvalue=42.174975
INFO:root:Epoch[0] Batch [6320-6340]	Speed: 1441.32 samples/sec	acc=0.000000	lossvalue=42.161075
INFO:root:Epoch[0] Batch [6340-6360]	Speed: 1453.63 samples/sec	acc=0.000000	lossvalue=42.162563
INFO:root:Epoch[0] Batch [6360-6380]	Speed: 1398.34 samples/sec	acc=0.000000	lossvalue=42.150429
INFO:root:Epoch[0] Batch [6380-6400]	Speed: 1378.18 samples/sec	acc=0.000000	lossvalue=42.140183
INFO:root:Epoch[0] Batch [6400-6420]	Speed: 1433.25 samples/sec	acc=0.000000	lossvalue=42.154503
INFO:root:Epoch[0] Batch [6420-6440]	Speed: 1433.36 samples/sec	acc=0.000000	lossvalue=42.139214
INFO:root:Epoch[0] Batch [6440-6460]	Speed: 1450.16 samples/sec	acc=0.000000	lossvalue=42.139873
INFO:root:Epoch[0] Batch [6460-6480]	Speed: 1441.93 samples/sec	acc=0.000000	lossvalue=42.146629
INFO:root:Epoch[0] Batch [6480-6500]	Speed: 1439.58 samples/sec	acc=0.000000	lossvalue=42.147348
INFO:root:Epoch[0] Batch [6500-6520]	Speed: 1438.32 samples/sec	acc=0.000000	lossvalue=42.136663
INFO:root:Epoch[0] Batch [6520-6540]	Speed: 1450.35 samples/sec	acc=0.000000	lossvalue=42.134079
INFO:root:Epoch[0] Batch [6540-6560]	Speed: 1426.14 samples/sec	acc=0.000000	lossvalue=42.124281
INFO:root:Epoch[0] Batch [6560-6580]	Speed: 1409.75 samples/sec	acc=0.000000	lossvalue=42.134493
INFO:root:Epoch[0] Batch [6580-6600]	Speed: 1412.57 samples/sec	acc=0.000000	lossvalue=42.128955
INFO:root:Epoch[0] Batch [6600-6620]	Speed: 1416.73 samples/sec	acc=0.000000	lossvalue=42.117061
INFO:root:Epoch[0] Batch [6620-6640]	Speed: 1362.93 samples/sec	acc=0.000000	lossvalue=42.129056
INFO:root:Epoch[0] Batch [6640-6660]	Speed: 1398.75 samples/sec	acc=0.000000	lossvalue=42.124024
INFO:root:Epoch[0] Batch [6660-6680]	Speed: 1404.41 samples/sec	acc=0.000000	lossvalue=42.118924
INFO:root:Epoch[0] Batch [6680-6700]	Speed: 1365.18 samples/sec	acc=0.000000	lossvalue=42.114451
INFO:root:Epoch[0] Batch [6700-6720]	Speed: 1424.69 samples/sec	acc=0.000000	lossvalue=42.115270
INFO:root:Epoch[0] Batch [6720-6740]	Speed: 1421.34 samples/sec	acc=0.000000	lossvalue=42.112736
INFO:root:Epoch[0] Batch [6740-6760]	Speed: 1276.32 samples/sec	acc=0.000000	lossvalue=42.098629
INFO:root:Epoch[0] Batch [6760-6780]	Speed: 1403.91 samples/sec	acc=0.000000	lossvalue=42.105654
INFO:root:Epoch[0] Batch [6780-6800]	Speed: 1427.84 samples/sec	acc=0.000000	lossvalue=42.103990
INFO:root:Epoch[0] Batch [6800-6820]	Speed: 1462.64 samples/sec	acc=0.000000	lossvalue=42.106331
INFO:root:Epoch[0] Batch [6820-6840]	Speed: 1406.41 samples/sec	acc=0.000000	lossvalue=42.113586
INFO:root:Epoch[0] Batch [6840-6860]	Speed: 1377.89 samples/sec	acc=0.000000	lossvalue=42.103058
INFO:root:Epoch[0] Batch [6860-6880]	Speed: 1355.97 samples/sec	acc=0.000000	lossvalue=42.112700
INFO:root:Epoch[0] Batch [6880-6900]	Speed: 1325.65 samples/sec	acc=0.000000	lossvalue=42.097591
INFO:root:Epoch[0] Batch [6900-6920]	Speed: 1411.97 samples/sec	acc=0.000000	lossvalue=42.098501
INFO:root:Epoch[0] Batch [6920-6940]	Speed: 1423.06 samples/sec	acc=0.000000	lossvalue=42.089276
INFO:root:Epoch[0] Batch [6940-6960]	Speed: 1427.09 samples/sec	acc=0.000000	lossvalue=42.080213
INFO:root:Epoch[0] Batch [6960-6980]	Speed: 1432.16 samples/sec	acc=0.000000	lossvalue=42.089613
lr-batch-epoch: 0.01 6999 0
INFO:root:Epoch[0] Batch [6980-7000]	Speed: 1400.62 samples/sec	acc=0.000000	lossvalue=42.088815
INFO:root:Epoch[0] Batch [7000-7020]	Speed: 1439.06 samples/sec	acc=0.000000	lossvalue=42.087424
INFO:root:Epoch[0] Batch [7020-7040]	Speed: 1440.75 samples/sec	acc=0.000000	lossvalue=42.067699
INFO:root:Epoch[0] Batch [7040-7060]	Speed: 1429.19 samples/sec	acc=0.000000	lossvalue=42.094941
INFO:root:Epoch[0] Batch [7060-7080]	Speed: 1444.85 samples/sec	acc=0.000000	lossvalue=42.085171
INFO:root:Epoch[0] Batch [7080-7100]	Speed: 1424.62 samples/sec	acc=0.000000	lossvalue=42.083311
INFO:root:Epoch[0] Batch [7100-7120]	Speed: 1450.79 samples/sec	acc=0.000000	lossvalue=42.086273
INFO:root:Epoch[0] Batch [7120-7140]	Speed: 1427.95 samples/sec	acc=0.000000	lossvalue=42.081083
INFO:root:Epoch[0] Batch [7140-7160]	Speed: 1420.23 samples/sec	acc=0.000000	lossvalue=42.083924
INFO:root:Epoch[0] Batch [7160-7180]	Speed: 1427.76 samples/sec	acc=0.000000	lossvalue=42.082238
INFO:root:Epoch[0] Batch [7180-7200]	Speed: 1439.70 samples/sec	acc=0.000000	lossvalue=42.081499
INFO:root:Epoch[0] Batch [7200-7220]	Speed: 1431.10 samples/sec	acc=0.000000	lossvalue=42.065490
INFO:root:Epoch[0] Batch [7220-7240]	Speed: 1443.44 samples/sec	acc=0.000000	lossvalue=42.083921
INFO:root:Epoch[0] Batch [7240-7260]	Speed: 1420.57 samples/sec	acc=0.000000	lossvalue=42.065946
INFO:root:Epoch[0] Batch [7260-7280]	Speed: 1406.95 samples/sec	acc=0.000000	lossvalue=42.072768
INFO:root:Epoch[0] Batch [7280-7300]	Speed: 1415.92 samples/sec	acc=0.000000	lossvalue=42.065378
INFO:root:Epoch[0] Batch [7300-7320]	Speed: 1415.89 samples/sec	acc=0.000000	lossvalue=42.062989
INFO:root:Epoch[0] Batch [7320-7340]	Speed: 1419.06 samples/sec	acc=0.000000	lossvalue=42.064022
INFO:root:Epoch[0] Batch [7340-7360]	Speed: 1422.13 samples/sec	acc=0.000000	lossvalue=42.051758
INFO:root:Epoch[0] Batch [7360-7380]	Speed: 1427.27 samples/sec	acc=0.000000	lossvalue=42.051090
INFO:root:Epoch[0] Batch [7380-7400]	Speed: 1425.62 samples/sec	acc=0.000000	lossvalue=42.050246
INFO:root:Epoch[0] Batch [7400-7420]	Speed: 1305.61 samples/sec	acc=0.000000	lossvalue=42.062048
INFO:root:Epoch[0] Batch [7420-7440]	Speed: 1435.37 samples/sec	acc=0.000000	lossvalue=42.061152
INFO:root:Epoch[0] Batch [7440-7460]	Speed: 1410.85 samples/sec	acc=0.000000	lossvalue=42.054612
INFO:root:Epoch[0] Batch [7460-7480]	Speed: 1425.80 samples/sec	acc=0.000000	lossvalue=42.047012
INFO:root:Epoch[0] Batch [7480-7500]	Speed: 1452.10 samples/sec	acc=0.000000	lossvalue=42.059667
INFO:root:Epoch[0] Batch [7500-7520]	Speed: 1425.86 samples/sec	acc=0.000000	lossvalue=42.026051
INFO:root:Epoch[0] Batch [7520-7540]	Speed: 1431.03 samples/sec	acc=0.000000	lossvalue=42.045337
INFO:root:Epoch[0] Batch [7540-7560]	Speed: 1443.19 samples/sec	acc=0.000000	lossvalue=42.038288
INFO:root:Epoch[0] Batch [7560-7580]	Speed: 1435.81 samples/sec	acc=0.000000	lossvalue=42.030659
INFO:root:Epoch[0] Batch [7580-7600]	Speed: 1448.49 samples/sec	acc=0.000000	lossvalue=42.039278
INFO:root:Epoch[0] Batch [7600-7620]	Speed: 1422.49 samples/sec	acc=0.000000	lossvalue=42.040024
INFO:root:Epoch[0] Batch [7620-7640]	Speed: 1420.79 samples/sec	acc=0.000000	lossvalue=42.033022
INFO:root:Epoch[0] Batch [7640-7660]	Speed: 1426.24 samples/sec	acc=0.000000	lossvalue=42.032648
INFO:root:Epoch[0] Batch [7660-7680]	Speed: 1427.59 samples/sec	acc=0.000000	lossvalue=42.040214
INFO:root:Epoch[0] Batch [7680-7700]	Speed: 1436.16 samples/sec	acc=0.000000	lossvalue=42.040748
INFO:root:Epoch[0] Batch [7700-7720]	Speed: 1447.00 samples/sec	acc=0.000000	lossvalue=42.030937
INFO:root:Epoch[0] Batch [7720-7740]	Speed: 1427.96 samples/sec	acc=0.000000	lossvalue=42.029444
INFO:root:Epoch[0] Batch [7740-7760]	Speed: 1433.70 samples/sec	acc=0.000000	lossvalue=42.039563
INFO:root:Epoch[0] Batch [7760-7780]	Speed: 1425.40 samples/sec	acc=0.000000	lossvalue=42.038318
INFO:root:Epoch[0] Batch [7780-7800]	Speed: 1433.88 samples/sec	acc=0.000000	lossvalue=42.022294
INFO:root:Epoch[0] Batch [7800-7820]	Speed: 1425.88 samples/sec	acc=0.000000	lossvalue=42.020742
INFO:root:Epoch[0] Batch [7820-7840]	Speed: 1430.61 samples/sec	acc=0.000000	lossvalue=42.027658
INFO:root:Epoch[0] Batch [7840-7860]	Speed: 1433.35 samples/sec	acc=0.000000	lossvalue=42.034516
INFO:root:Epoch[0] Batch [7860-7880]	Speed: 1421.92 samples/sec	acc=0.000000	lossvalue=42.010682
INFO:root:Epoch[0] Batch [7880-7900]	Speed: 1409.32 samples/sec	acc=0.000000	lossvalue=42.016182
INFO:root:Epoch[0] Batch [7900-7920]	Speed: 1424.05 samples/sec	acc=0.000000	lossvalue=42.024273
INFO:root:Epoch[0] Batch [7920-7940]	Speed: 1416.98 samples/sec	acc=0.000000	lossvalue=42.014298
INFO:root:Epoch[0] Batch [7940-7960]	Speed: 1411.33 samples/sec	acc=0.000000	lossvalue=42.031139
INFO:root:Epoch[0] Batch [7960-7980]	Speed: 1382.00 samples/sec	acc=0.000000	lossvalue=42.025514
lr-batch-epoch: 0.01 7999 0
testing verification..
(12000, 512)
infer time 4.986866
[agedb_30][8000]XNorm: 1.027080
[agedb_30][8000]Accuracy-Flip: 0.50067+-0.00186
testing verification..
(12000, 512)
infer time 4.825368
[calfw][8000]XNorm: 1.632914
[calfw][8000]Accuracy-Flip: 0.50217+-0.00454
testing verification..
(14000, 512)
infer time 5.582666
[cfp_ff][8000]XNorm: 16.623992
[cfp_ff][8000]Accuracy-Flip: 0.60443+-0.01392
testing verification..
(14000, 512)
infer time 5.578
[cfp_fp][8000]XNorm: 13.326273
[cfp_fp][8000]Accuracy-Flip: 0.56386+-0.02096
testing verification..
(12000, 512)
infer time 4.811976
[cplfw][8000]XNorm: 3.476282
[cplfw][8000]Accuracy-Flip: 0.49917+-0.00664
testing verification..
(12000, 512)
infer time 4.507722
[lfw][8000]XNorm: 1.115357
[lfw][8000]Accuracy-Flip: 0.50383+-0.00817
[8000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [7980-8000]	Speed: 182.05 samples/sec	acc=0.000000	lossvalue=42.002839
INFO:root:Epoch[0] Batch [8000-8020]	Speed: 1343.79 samples/sec	acc=0.000000	lossvalue=42.023796
INFO:root:Epoch[0] Batch [8020-8040]	Speed: 1391.27 samples/sec	acc=0.000000	lossvalue=42.021088
INFO:root:Epoch[0] Batch [8040-8060]	Speed: 1406.34 samples/sec	acc=0.000000	lossvalue=42.018094
INFO:root:Epoch[0] Batch [8060-8080]	Speed: 1416.11 samples/sec	acc=0.000000	lossvalue=42.013804
INFO:root:Epoch[0] Batch [8080-8100]	Speed: 1414.52 samples/sec	acc=0.000000	lossvalue=42.021646
INFO:root:Epoch[0] Batch [8100-8120]	Speed: 1410.66 samples/sec	acc=0.000000	lossvalue=42.005379
INFO:root:Epoch[0] Batch [8120-8140]	Speed: 1335.76 samples/sec	acc=0.000000	lossvalue=42.003079
INFO:root:Epoch[0] Batch [8140-8160]	Speed: 1399.16 samples/sec	acc=0.000000	lossvalue=42.000332
INFO:root:Epoch[0] Batch [8160-8180]	Speed: 1414.74 samples/sec	acc=0.000000	lossvalue=42.009885
INFO:root:Epoch[0] Batch [8180-8200]	Speed: 1415.45 samples/sec	acc=0.000000	lossvalue=42.003193
INFO:root:Epoch[0] Batch [8200-8220]	Speed: 1432.53 samples/sec	acc=0.000000	lossvalue=41.998407
INFO:root:Epoch[0] Batch [8220-8240]	Speed: 1407.10 samples/sec	acc=0.000000	lossvalue=42.007038
INFO:root:Epoch[0] Batch [8240-8260]	Speed: 1438.36 samples/sec	acc=0.000000	lossvalue=42.010870
INFO:root:Epoch[0] Batch [8260-8280]	Speed: 1432.62 samples/sec	acc=0.000000	lossvalue=41.998699
INFO:root:Epoch[0] Batch [8280-8300]	Speed: 1418.51 samples/sec	acc=0.000000	lossvalue=41.996247
INFO:root:Epoch[0] Batch [8300-8320]	Speed: 1432.23 samples/sec	acc=0.000000	lossvalue=41.997199
INFO:root:Epoch[0] Batch [8320-8340]	Speed: 1463.53 samples/sec	acc=0.000000	lossvalue=41.997478
INFO:root:Epoch[0] Batch [8340-8360]	Speed: 1436.30 samples/sec	acc=0.000000	lossvalue=42.006742
INFO:root:Epoch[0] Batch [8360-8380]	Speed: 1439.45 samples/sec	acc=0.000000	lossvalue=41.987122
INFO:root:Epoch[0] Batch [8380-8400]	Speed: 1414.07 samples/sec	acc=0.000000	lossvalue=41.988386
INFO:root:Epoch[0] Batch [8400-8420]	Speed: 1425.56 samples/sec	acc=0.000000	lossvalue=41.987975
INFO:root:Epoch[0] Batch [8420-8440]	Speed: 1429.08 samples/sec	acc=0.000000	lossvalue=41.991180
INFO:root:Epoch[0] Batch [8440-8460]	Speed: 1430.65 samples/sec	acc=0.000000	lossvalue=42.001754
INFO:root:Epoch[0] Batch [8460-8480]	Speed: 1435.57 samples/sec	acc=0.000000	lossvalue=41.991315
INFO:root:Epoch[0] Batch [8480-8500]	Speed: 1432.98 samples/sec	acc=0.000000	lossvalue=41.982089
INFO:root:Epoch[0] Batch [8500-8520]	Speed: 1424.44 samples/sec	acc=0.000000	lossvalue=41.990520
INFO:root:Epoch[0] Batch [8520-8540]	Speed: 1431.28 samples/sec	acc=0.000000	lossvalue=41.995154
INFO:root:Epoch[0] Batch [8540-8560]	Speed: 1426.16 samples/sec	acc=0.000000	lossvalue=41.987347
INFO:root:Epoch[0] Batch [8560-8580]	Speed: 1426.23 samples/sec	acc=0.000000	lossvalue=41.980704
INFO:root:Epoch[0] Batch [8580-8600]	Speed: 1428.86 samples/sec	acc=0.000000	lossvalue=41.987747
INFO:root:Epoch[0] Batch [8600-8620]	Speed: 1438.03 samples/sec	acc=0.000000	lossvalue=41.981680
INFO:root:Epoch[0] Batch [8620-8640]	Speed: 1434.76 samples/sec	acc=0.000000	lossvalue=41.995870
INFO:root:Epoch[0] Batch [8640-8660]	Speed: 1395.83 samples/sec	acc=0.000000	lossvalue=41.980906
INFO:root:Epoch[0] Batch [8660-8680]	Speed: 1436.40 samples/sec	acc=0.000000	lossvalue=41.985622
INFO:root:Epoch[0] Batch [8680-8700]	Speed: 1427.66 samples/sec	acc=0.000000	lossvalue=41.980081
INFO:root:Epoch[0] Batch [8700-8720]	Speed: 1433.52 samples/sec	acc=0.000000	lossvalue=41.981155
INFO:root:Epoch[0] Batch [8720-8740]	Speed: 1407.18 samples/sec	acc=0.000000	lossvalue=41.984562
INFO:root:Epoch[0] Batch [8740-8760]	Speed: 1436.53 samples/sec	acc=0.000000	lossvalue=41.978110
INFO:root:Epoch[0] Batch [8760-8780]	Speed: 1411.15 samples/sec	acc=0.000000	lossvalue=41.976267
INFO:root:Epoch[0] Batch [8780-8800]	Speed: 1425.90 samples/sec	acc=0.000000	lossvalue=41.980106
INFO:root:Epoch[0] Batch [8800-8820]	Speed: 1316.30 samples/sec	acc=0.000000	lossvalue=41.981414
INFO:root:Epoch[0] Batch [8820-8840]	Speed: 1412.42 samples/sec	acc=0.000000	lossvalue=41.977699
INFO:root:Epoch[0] Batch [8840-8860]	Speed: 1441.54 samples/sec	acc=0.000000	lossvalue=41.966282
INFO:root:Epoch[0] Batch [8860-8880]	Speed: 1441.60 samples/sec	acc=0.000000	lossvalue=41.969540
INFO:root:Epoch[0] Batch [8880-8900]	Speed: 1410.46 samples/sec	acc=0.000000	lossvalue=41.974941
INFO:root:Epoch[0] Batch [8900-8920]	Speed: 1416.21 samples/sec	acc=0.000000	lossvalue=41.972329
INFO:root:Epoch[0] Batch [8920-8940]	Speed: 1423.41 samples/sec	acc=0.000000	lossvalue=41.982397
INFO:root:Epoch[0] Batch [8940-8960]	Speed: 1433.85 samples/sec	acc=0.000000	lossvalue=41.960446
INFO:root:Epoch[0] Batch [8960-8980]	Speed: 1419.24 samples/sec	acc=0.000000	lossvalue=41.965259
lr-batch-epoch: 0.01 8999 0
INFO:root:Epoch[0] Batch [8980-9000]	Speed: 1429.51 samples/sec	acc=0.000000	lossvalue=41.963624
INFO:root:Epoch[0] Batch [9000-9020]	Speed: 1421.19 samples/sec	acc=0.000000	lossvalue=41.975398
INFO:root:Epoch[0] Batch [9020-9040]	Speed: 1434.94 samples/sec	acc=0.000000	lossvalue=41.967802
INFO:root:Epoch[0] Batch [9040-9060]	Speed: 1441.51 samples/sec	acc=0.000000	lossvalue=41.958901
INFO:root:Epoch[0] Batch [9060-9080]	Speed: 1404.33 samples/sec	acc=0.000000	lossvalue=41.964572
INFO:root:Epoch[0] Batch [9080-9100]	Speed: 1446.24 samples/sec	acc=0.000000	lossvalue=41.959388
INFO:root:Epoch[0] Batch [9100-9120]	Speed: 1422.98 samples/sec	acc=0.000000	lossvalue=41.962714
INFO:root:Epoch[0] Batch [9120-9140]	Speed: 1414.02 samples/sec	acc=0.000000	lossvalue=41.957292
INFO:root:Epoch[0] Batch [9140-9160]	Speed: 1442.95 samples/sec	acc=0.000000	lossvalue=41.947298
INFO:root:Epoch[0] Batch [9160-9180]	Speed: 1428.41 samples/sec	acc=0.000000	lossvalue=41.957963
INFO:root:Epoch[0] Batch [9180-9200]	Speed: 1423.07 samples/sec	acc=0.000000	lossvalue=41.949533
INFO:root:Epoch[0] Batch [9200-9220]	Speed: 1425.92 samples/sec	acc=0.000000	lossvalue=41.955536
INFO:root:Epoch[0] Batch [9220-9240]	Speed: 1425.39 samples/sec	acc=0.000000	lossvalue=41.955172
INFO:root:Epoch[0] Batch [9240-9260]	Speed: 1422.23 samples/sec	acc=0.000000	lossvalue=41.948323
INFO:root:Epoch[0] Batch [9260-9280]	Speed: 1440.76 samples/sec	acc=0.000000	lossvalue=41.963384
INFO:root:Epoch[0] Batch [9280-9300]	Speed: 1426.57 samples/sec	acc=0.000000	lossvalue=41.951702
INFO:root:Epoch[0] Batch [9300-9320]	Speed: 1433.89 samples/sec	acc=0.000000	lossvalue=41.956513
INFO:root:Epoch[0] Batch [9320-9340]	Speed: 1425.13 samples/sec	acc=0.000000	lossvalue=41.953564
INFO:root:Epoch[0] Batch [9340-9360]	Speed: 1370.22 samples/sec	acc=0.000000	lossvalue=41.964537
INFO:root:Epoch[0] Batch [9360-9380]	Speed: 1365.07 samples/sec	acc=0.000000	lossvalue=41.946557
INFO:root:Epoch[0] Batch [9380-9400]	Speed: 1390.25 samples/sec	acc=0.000000	lossvalue=41.948429

(eog:35032): EOG-WARNING **: Failed to open file '/home/rongy/.cache/thumbnails/normal/e0f3196ef3a20c0b96b730474c7231ee.png': No such file or directory
INFO:root:Epoch[0] Batch [9400-9420]	Speed: 1423.46 samples/sec	acc=0.000000	lossvalue=41.953710
INFO:root:Epoch[0] Batch [9420-9440]	Speed: 1409.61 samples/sec	acc=0.000000	lossvalue=41.943771
INFO:root:Epoch[0] Batch [9440-9460]	Speed: 1404.73 samples/sec	acc=0.000000	lossvalue=41.947741
INFO:root:Epoch[0] Batch [9460-9480]	Speed: 1410.33 samples/sec	acc=0.000000	lossvalue=41.959211
INFO:root:Epoch[0] Batch [9480-9500]	Speed: 1436.10 samples/sec	acc=0.000000	lossvalue=41.964881
INFO:root:Epoch[0] Batch [9500-9520]	Speed: 1284.75 samples/sec	acc=0.000000	lossvalue=41.940122
INFO:root:Epoch[0] Batch [9520-9540]	Speed: 1416.94 samples/sec	acc=0.000000	lossvalue=41.946654
INFO:root:Epoch[0] Batch [9540-9560]	Speed: 1409.22 samples/sec	acc=0.000000	lossvalue=41.953679
INFO:root:Epoch[0] Batch [9560-9580]	Speed: 1430.54 samples/sec	acc=0.000000	lossvalue=41.946638
INFO:root:Epoch[0] Batch [9580-9600]	Speed: 1421.10 samples/sec	acc=0.000000	lossvalue=41.945858
INFO:root:Epoch[0] Batch [9600-9620]	Speed: 1457.82 samples/sec	acc=0.000000	lossvalue=41.945140
INFO:root:Epoch[0] Batch [9620-9640]	Speed: 1428.59 samples/sec	acc=0.000000	lossvalue=41.937235
INFO:root:Epoch[0] Batch [9640-9660]	Speed: 1420.10 samples/sec	acc=0.000000	lossvalue=41.946729
INFO:root:Epoch[0] Batch [9660-9680]	Speed: 1377.31 samples/sec	acc=0.000000	lossvalue=41.943734
INFO:root:Epoch[0] Batch [9680-9700]	Speed: 1460.06 samples/sec	acc=0.000000	lossvalue=41.942536
INFO:root:Epoch[0] Batch [9700-9720]	Speed: 1453.38 samples/sec	acc=0.000000	lossvalue=41.944737
INFO:root:Epoch[0] Batch [9720-9740]	Speed: 1440.01 samples/sec	acc=0.000000	lossvalue=41.938507
INFO:root:Epoch[0] Batch [9740-9760]	Speed: 1427.13 samples/sec	acc=0.000000	lossvalue=41.932979
INFO:root:Epoch[0] Batch [9760-9780]	Speed: 1446.75 samples/sec	acc=0.000000	lossvalue=41.941228
INFO:root:Epoch[0] Batch [9780-9800]	Speed: 1423.12 samples/sec	acc=0.000000	lossvalue=41.939507
INFO:root:Epoch[0] Batch [9800-9820]	Speed: 1456.51 samples/sec	acc=0.000000	lossvalue=41.925296
INFO:root:Epoch[0] Batch [9820-9840]	Speed: 1456.65 samples/sec	acc=0.000000	lossvalue=41.934135
INFO:root:Epoch[0] Batch [9840-9860]	Speed: 1428.55 samples/sec	acc=0.000000	lossvalue=41.936381
INFO:root:Epoch[0] Batch [9860-9880]	Speed: 1419.47 samples/sec	acc=0.000000	lossvalue=41.939161
INFO:root:Epoch[0] Batch [9880-9900]	Speed: 1462.09 samples/sec	acc=0.000000	lossvalue=41.936261
INFO:root:Epoch[0] Batch [9900-9920]	Speed: 1442.82 samples/sec	acc=0.000000	lossvalue=41.929150
INFO:root:Epoch[0] Batch [9920-9940]	Speed: 1444.01 samples/sec	acc=0.000000	lossvalue=41.932265
INFO:root:Epoch[0] Batch [9940-9960]	Speed: 1419.85 samples/sec	acc=0.000000	lossvalue=41.929162
INFO:root:Epoch[0] Batch [9960-9980]	Speed: 1436.27 samples/sec	acc=0.000000	lossvalue=41.931037
lr-batch-epoch: 0.01 9999 0
testing verification..
(12000, 512)
infer time 4.598723
[agedb_30][10000]XNorm: 0.798147
[agedb_30][10000]Accuracy-Flip: 0.49950+-0.00150
testing verification..
(12000, 512)
infer time 4.744518
[calfw][10000]XNorm: 1.327497
[calfw][10000]Accuracy-Flip: 0.50333+-0.00325
testing verification..
(14000, 512)
infer time 5.152224
[cfp_ff][10000]XNorm: 18.945766
[cfp_ff][10000]Accuracy-Flip: 0.58400+-0.01557
testing verification..
(14000, 512)
infer time 5.904056
[cfp_fp][10000]XNorm: 14.005116
[cfp_fp][10000]Accuracy-Flip: 0.54114+-0.01367
testing verification..
(12000, 512)
infer time 5.123472
[cplfw][10000]XNorm: 3.457998
[cplfw][10000]Accuracy-Flip: 0.50133+-0.00799
testing verification..
(12000, 512)
infer time 4.997448
[lfw][10000]XNorm: 0.884298
[lfw][10000]Accuracy-Flip: 0.50283+-0.00753
[10000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [9980-10000]	Speed: 179.89 samples/sec	acc=0.000000	lossvalue=41.932265
INFO:root:Epoch[0] Batch [10000-10020]	Speed: 1352.16 samples/sec	acc=0.000000	lossvalue=41.930890
INFO:root:Epoch[0] Batch [10020-10040]	Speed: 1403.24 samples/sec	acc=0.000000	lossvalue=41.936999
INFO:root:Epoch[0] Batch [10040-10060]	Speed: 1352.08 samples/sec	acc=0.000000	lossvalue=41.921464
INFO:root:Epoch[0] Batch [10060-10080]	Speed: 1213.13 samples/sec	acc=0.000000	lossvalue=41.930905
INFO:root:Epoch[0] Batch [10080-10100]	Speed: 1293.58 samples/sec	acc=0.000000	lossvalue=41.927600
INFO:root:Epoch[0] Batch [10100-10120]	Speed: 1389.86 samples/sec	acc=0.000000	lossvalue=41.936483
INFO:root:Epoch[0] Batch [10120-10140]	Speed: 1375.93 samples/sec	acc=0.000000	lossvalue=41.926200
INFO:root:Epoch[0] Batch [10140-10160]	Speed: 1390.30 samples/sec	acc=0.000000	lossvalue=41.931243
^CTraceback (most recent call last):
  File ""train.py"", line 447, in <module>
    
  File ""train.py"", line 444, in main
    
  File ""train.py"", line 439, in train_net
    
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 539, in fit
    self.update_metric(eval_metric, data_batch.label)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/module/module.py"", line 775, in update_metric
    self._exec_group.update_metric(eval_metric, labels, pre_sliced)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 639, in update_metric
    eval_metric.update_dict(labels_, preds)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/metric.py"", line 304, in update_dict
    metric.update_dict(labels, preds)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/metric.py"", line 132, in update_dict
",neutral,neutral
52,s3dis,yangyanli/PointCNN,closed,,mohuamuyu,mohuamuyu,2018-03-18 09:31:41,2018-03-18 10:22:47,2.0,,20.0,306225595.0,"1. How to use eval_s3dis.py? 
We split each room into several small blocks for testing, right? 
so, do we need to combine the prediction of each block to get the prediction of the whole room? 
since the groud-truth-files used in  ""eval_s3dis.py"" is about the whole room.
if so, can you provide the script for combining the predictions of each room blocks?

2. In s3dis_split.py.
we first ""split-to-blocks"" and then ""reblock"", 
so is there any chance the number of points in the blocks still less than ""block_min_pnum""
after the ""reblock"" operation?
of course, I think it does not matter, so you can choose to ignore this question.

3. when using ""eval_shapenet_seg.py"" in the shapenet part segmentation task, 
we have to change the following command :
python3 eval_shapenet_seg.py -g ../../data/shapenet_partseg/test_label -p ../../data/shapenet_partseg/test_data_pred_10
into: 
python3 eval_shapenet_seg.py -g ../../data/shapenet_partseg/test_label -p ../../data/shapenet_partseg/test_data_pred_10 **-a**
right?

Thank you for your great work!",neutral,positive
53,"An error  always occurs when I using faster rcnn ,cascade-rcnn or cascade-maskrcnn...",open-mmlab/mmdetection,closed,,JiaquanYe,JiaquanYe,2019-08-19 09:34:35,2019-08-19 12:10:36,2.0,,1214.0,482205478.0,"Thanks for your error report and we appreciate it a lot.

**Checklist**
1. I have searched related issues but cannot get the expected help.
2. The bug has not been fixed in the latest version.

**Describe the bug**
I want to change my own dataset and it has 20 classes, so I have create mydataset.py and modify some dataset setting according to GET_STARTED.md. 
But I have met a strange situation, I have modify num_class=21 in  faster-rcnn config.py, it raise an error : RuntimeError: copy_if failed to synchronize: device-side assert triggered.
When I change the num_class=81, it is ok and no error.
And I check my json file(my own dataset) in 'data/coco/annotations/instances_train2017.json', the classes is 20.
The most strange is, when I use config file 'retinanet_x101_64x4d_fpn_1x.py' to a retinanet model, and I also change the num classes to 21, it is OK and no error.
So, I think there are maybe some bug in faster-rcnn config?
I have train the model in a single GPU,no server,  RTX2080Ti, CUDA9.0, cudnn 7.3.1.


**Reproduction**
1. What command or script did you run?
python tools/train.py configs/.../faster_rcnn_r50_fpn_1x.py

A placeholder for the command.
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?
I have only modify the dataset part. I create mydataset.py and inherit 'class CocoDataset' like this:
from .coco import CocoDataset
from .registry import DATASETS

@DATASETS.register_module
class GDDataset(CocoDataset):

    CLASSES = ('1', '2', '3', '4', '5', '6', '7', \
               '8', '9', '10', '11', '12', '13', '14', \
               '15', '16', '17', '18', '19', '20')

and add GDDataset to __all__  of __init__.py.


3. What dataset did you use?
My own dataset with COCO format.


**Environment**
 - OS: [e.g., Ubuntu 18.04]
 - GCC [e.g., 5.5.0]
 - PyTorch version [e.g., 1.2.0]
- How you installed PyTorch [conda]
- GPU model [2080 Ti]
- CUDA and CUDNN version 
CUDA 9.0 and CUDNN 7.3.1
- [optional] Other information that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**
If applicable, paste the error trackback here.
```
2019-08-19 17:11:32,384 - INFO - Distributed training: False
2019-08-19 17:11:32,616 - INFO - load model from: torchvision://resnet50
2019-08-19 17:11:32,744 - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

loading annotations into memory...
Done (t=0.08s)
creating index...
index created!
2019-08-19 17:11:35,154 - INFO - Start running, host: lab404@LAB404, work_dir: /home/lab404/mmdetection/work_dirs/tianchi_gd/faster_rcnn_r50_fpn_1x
2019-08-19 17:11:35,154 - INFO - workflow: [('train', 1)], max: 12 epochs
/opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [512,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
Traceback (most recent call last):
  File ""tools/train.py"", line 108, in <module>
    main()
  File ""tools/train.py"", line 104, in main
    logger=logger)
  File ""/home/lab404/mmdetection/mmdet/apis/train.py"", line 60, in train_detector
    _non_dist_train(model, dataset, cfg, validate=validate)
  File ""/home/lab404/mmdetection/mmdet/apis/train.py"", line 221, in _non_dist_train
    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
  File ""/home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/runner.py"", line 358, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/runner.py"", line 264, in train
    self.model, data_batch, train_mode=True, **kwargs)
  File ""/home/lab404/mmdetection/mmdet/apis/train.py"", line 38, in batch_processor
    losses = model(**data)
  File ""/home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 150, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/lab404/mmdetection/mmdet/core/fp16/decorators.py"", line 49, in new_func
    return old_func(*args, **kwargs)
  File ""/home/lab404/mmdetection/mmdet/models/detectors/base.py"", line 86, in forward
    return self.forward_train(img, img_meta, **kwargs)
  File ""/home/lab404/mmdetection/mmdet/models/detectors/two_stage.py"", line 183, in forward_train
    *bbox_targets)
  File ""/home/lab404/mmdetection/mmdet/core/fp16/decorators.py"", line 127, in new_func
    return old_func(*args, **kwargs)
  File ""/home/lab404/mmdetection/mmdet/models/bbox_heads/bbox_head.py"", line 123, in loss
    4)[pos_inds, labels[pos_inds]]
RuntimeError: copy_if failed to synchronize: device-side assert triggered
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered (insert_events at /opt/conda/conda-bld/pytorch_1565272271120/work/c10/cuda/CUDACachingAllocator.cpp:569)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7fe07b870e37 in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x12e14 (0x7fe07baa8e14 in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)
frame #2: <unknown function> + 0x165bf (0x7fe07baac5bf in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::TensorImpl::release_resources() + 0x74 (0x7fe07b85bfa4 in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #4: <unknown function> + 0x1bb014 (0x7fe0ac95c014 in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x40142b (0x7fe0acba242b in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #6: <unknown function> + 0x401461 (0x7fe0acba2461 in /home/lab404/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #26: __libc_start_main + 0xe7 (0x7fe0ba61cb97 in /lib/x86_64-linux-gnu/libc.so.6)

Aborted (core dumped)
```
",neutral,negative
54, data['category_id'] = self.cat_ids[label]  IndexError: list index out of range,open-mmlab/mmdetection,closed,jshilong,aishangmaxiaoming,jshilong,2021-05-13 08:27:16,2021-06-29 05:27:39,5.0,,5152.0,890843553.0,"<img width=""1211"" alt=""image"" src=""https://user-images.githubusercontent.com/67897601/118099628-10d40300-b408-11eb-9770-2e5f773ddcdf.png"">
",neutral,neutral
55,"assert hasattr(value, 'numel') and value.numel() == 1",open-mmlab/mmdetection,closed,Czm369,vansin,vansin,2022-08-29 18:40:44,2022-08-30 03:28:10,1.0,,8664.0,1354725252.0,"```shell

---------------iou_thr: 0.95---------------
08/30 02:37:24 - mmengine - INFO - 
+-------+-----+------+--------+-------+
| class | gts | dets | recall | ap    |
+-------+-----+------+--------+-------+
| table | 449 | 3880 | 0.004  | 0.000 |
+-------+-----+------+--------+-------+
| mAP   |     |      |        | 0.000 |
+-------+-----+------+--------+-------+
Traceback (most recent call last):
  File ""/home/elaine/miniconda3/envs/mmdet-yolo/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/elaine/miniconda3/envs/mmdet-yolo/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/elaine/.vscode/extensions/ms-python.python-2022.12.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py"", line 39, in <module>
    cli.main()
  File ""/home/elaine/.vscode/extensions/ms-python.python-2022.12.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 430, in main
    run()
  File ""/home/elaine/.vscode/extensions/ms-python.python-2022.12.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py"", line 284, in run_file
    runpy.run_path(target, run_name=""__main__"")
  File ""/home/elaine/.vscode/extensions/ms-python.python-2022.12.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File ""/home/elaine/.vscode/extensions/ms-python.python-2022.12.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File ""/home/elaine/.vscode/extensions/ms-python.python-2022.12.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py"", line 124, in _run_code
    exec(code, run_globals)
  File ""tools/train.py"", line 120, in <module>
    main()
  File ""tools/train.py"", line 116, in main
    runner.train()
  File ""/project/openmmlab2/mmengine/mmengine/runner/runner.py"", line 1631, in train
    model = self.train_loop.run()  # type: ignore
  File ""/project/openmmlab2/mmengine/mmengine/runner/loops.py"", line 94, in run
    self.runner.val_loop.run()
  File ""/project/openmmlab2/mmengine/mmengine/runner/loops.py"", line 347, in run
    self.runner.call_hook('after_val_epoch', metrics=metrics)
  File ""/project/openmmlab2/mmengine/mmengine/runner/runner.py"", line 1693, in call_hook
    getattr(hook, fn_name)(self, **kwargs)
  File ""/project/openmmlab2/mmengine/mmengine/hooks/runtime_info_hook.py"", line 115, in after_val_epoch
    runner.message_hub.update_scalar(f'val/{key}', value)
  File ""/project/openmmlab2/mmengine/mmengine/logging/message_hub.py"", line 139, in update_scalar
    checked_value = self._get_valid_value(value)
  File ""/project/openmmlab2/mmengine/mmengine/logging/message_hub.py"", line 338, in _get_valid_value
    assert hasattr(value, 'numel') and value.numel() == 1
AssertionError
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:           acc █▇▁▆▆█
wandb:     data_time ▂█▁▁▁▁
wandb:         epoch ▁▁▁▁▁▁
wandb:          loss █▂▃▂▁▁
wandb:     loss_bbox ▁▇█▅▅▄
wandb:      loss_cls █▂▄▂▁▁
wandb: loss_rpn_bbox █▃██▁▂
wandb:  loss_rpn_cls █▁▁▁▁▁
wandb:            lr ▁▂▄▅▇█
wandb:          time ▁█▁▁▃▁
wandb: 
wandb: Run summary:
wandb:           acc 98.24219
wandb:     data_time 0.0028
wandb:         epoch 1
wandb:          loss 0.1954
wandb:     loss_bbox 0.06718
wandb:      loss_cls 0.07565
wandb: loss_rpn_bbox 0.03021
wandb:  loss_rpn_cls 0.02236
wandb:            lr 0.01199
wandb:          time 0.33906
wandb: 
wandb: Synced true-totem-21: http://localhost:8080/vansin/mmdetection-tools/runs/2gczk992
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./work_dirs/faster-rcnn_r50_fpn_1x_coco/20220830_023329/vis_data/wandb/run-20220830_023351-2gczk992/logs

```",neutral,neutral
56,errors while running krisp code,facebookresearch/mmf,open,,jiny419,,2021-06-22 07:46:40,,5.0,,980.0,926934824.0,"## ❓ Questions and Help

Hi !

I am running KRISP project code in mmf. 
But I discovered some errors. 

1) torch-sparse module is missed in requirements.txt of krisp project.
2) when I installed torch-sparse suitable for cuda version 10.2, I got the error below 

2021-06-22T16:32:10 | mmf.utils.configuration: Overriding option config to ./projects/krisp/configs/krisp/okvqa/train_val.yaml
2021-06-22T16:32:10 | mmf.utils.configuration: Overriding option run_type to train_val
2021-06-22T16:32:10 | mmf.utils.configuration: Overriding option datasets to okvqa
2021-06-22T16:32:10 | mmf.utils.configuration: Overriding option model to krisp
2021-06-22T16:32:14 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:14 | mmf.utils.distributed: Distributed Init (Rank 3): tcp://localhost:12572
2021-06-22T16:32:14 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:14 | mmf.utils.distributed: Distributed Init (Rank 4): tcp://localhost:12572
2021-06-22T16:32:15 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:15 | mmf.utils.distributed: Distributed Init (Rank 1): tcp://localhost:12572
2021-06-22T16:32:15 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:15 | mmf.utils.distributed: Distributed Init (Rank 0): tcp://localhost:12572
2021-06-22T16:32:15 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:15 | mmf.utils.distributed: Distributed Init (Rank 2): tcp://localhost:12572
2021-06-22T16:32:15 | root: Added key: store_based_barrier_key:1 to store for rank: 2
2021-06-22T16:32:15 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:15 | mmf.utils.distributed: Distributed Init (Rank 5): tcp://localhost:12572
2021-06-22T16:32:15 | root: Added key: store_based_barrier_key:1 to store for rank: 5
2021-06-22T16:32:15 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:15 | mmf.utils.distributed: Distributed Init (Rank 7): tcp://localhost:12572
2021-06-22T16:32:15 | root: Added key: store_based_barrier_key:1 to store for rank: 7
2021-06-22T16:32:15 | mmf.utils.distributed: XLA Mode:False
2021-06-22T16:32:15 | mmf.utils.distributed: Distributed Init (Rank 6): tcp://localhost:12572
2021-06-22T16:32:15 | root: Added key: store_based_barrier_key:1 to store for rank: 6
2021-06-22T16:32:15 | root: Added key: store_based_barrier_key:1 to store for rank: 3
2021-06-22T16:32:15 | root: Added key: store_based_barrier_key:1 to store for rank: 4
2021-06-22T16:32:16 | root: Added key: store_based_barrier_key:1 to store for rank: 1
2021-06-22T16:32:16 | root: Added key: store_based_barrier_key:1 to store for rank: 0
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 0
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 2
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 5
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 3
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 6
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 7
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 4
2021-06-22T16:32:16 | mmf.utils.distributed: Initialized Host 4eb3a36d858c as Rank 1
2021-06-22T16:32:21 | mmf: Logging to: ./save/train.log
2021-06-22T16:32:21 | mmf_cli.run: Namespace(config_override=None, local_rank=None, opts=['config=./projects/krisp/configs/krisp/okvqa/train_val.yaml', 'run_type=train_val', 'dataset=okvqa', 'model=krisp'])
2021-06-22T16:32:21 | mmf_cli.run: Torch version: 1.8.1+cu102
2021-06-22T16:32:21 | mmf.utils.general: CUDA Device 0 is: GeForce RTX 2080 Ti
2021-06-22T16:32:21 | mmf_cli.run: Using seed 21664516
2021-06-22T16:32:21 | mmf.trainers.mmf_trainer: Loading datasets
okvqa/defaults/annotations/annotations/graph_vocab/graph_vocab.pth.tar
/home/aimaster/.cache/torch/mmf/data
2021-06-22T16:32:27 | mmf.datasets.multi_datamodule: Multitasking disabled by default for single dataset training
2021-06-22T16:32:27 | mmf.datasets.multi_datamodule: Multitasking disabled by default for single dataset training
2021-06-22T16:32:27 | mmf.datasets.multi_datamodule: Multitasking disabled by default for single dataset training
2021-06-22T16:32:27 | mmf.trainers.mmf_trainer: Loading model
Import error with KRISP dependencies. Fix dependencies if you want to use KRISP
Traceback (most recent call last):
  File ""/home/aimaster/anaconda3/envs/mmf/bin/mmf_run"", line 33, in <module>
    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf_cli/run.py"", line 129, in run
    nprocs=config.distributed.world_size,
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 188, in start_processes
    while not context.join():
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 59, in _wrap
    fn(i, *args)
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf_cli/run.py"", line 66, in distributed_main
    main(configuration, init_distributed=True, predict=predict)
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf_cli/run.py"", line 52, in main
    trainer.load()
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/trainers/mmf_trainer.py"", line 42, in load
    super().load()
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/trainers/base_trainer.py"", line 33, in load
    self.load_model()
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/trainers/mmf_trainer.py"", line 96, in load_model
    self.model = build_model(attributes)
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/utils/build.py"", line 87, in build_model
    model = model_class(config)
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/models/krisp.py"", line 39, in __init__
    self.build()
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/models/krisp.py"", line 75, in build
    from projects.krisp.graphnetwork_module import GraphNetworkModule
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/projects/krisp/graphnetwork_module.py"", line 21, in <module>
    from torch_geometric.nn import BatchNorm, GCNConv, RGCNConv, SAGEConv
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch_geometric/__init__.py"", line 5, in <module>
    import torch_geometric.data
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch_geometric/data/__init__.py"", line 1, in <module>
    from .data import Data
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch_geometric/data/data.py"", line 8, in <module>
    from torch_sparse import coalesce, SparseTensor
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch_sparse/__init__.py"", line 36, in <module>
    from .storage import SparseStorage  # noqa
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch_sparse/storage.py"", line 21, in <module>
    class SparseStorage(object):
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/_script.py"", line 974, in script
    _compile_and_register_class(obj, _rcb, qualified_name)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/_script.py"", line 67, in _compile_and_register_class
    torch._C._jit_script_class_compile(qualified_name, ast, defaults, rcb)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/_recursive.py"", line 757, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/_script.py"", line 990, in script
    qualified_name, ast, _rcb, get_default_args(obj)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/_recursive.py"", line 757, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/_script.py"", line 986, in script
    ast = get_jit_def(obj, obj.__name__)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/frontend.py"", line 271, in get_jit_def
    return build_def(ctx, fn_def, type_line, def_name, self_name=self_name)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/frontend.py"", line 293, in build_def
    param_list = build_param_list(ctx, py_def.args, self_name)
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/jit/frontend.py"", line 316, in build_param_list
    raise NotSupportedError(ctx_range, _vararg_kwarg_err)
torch.jit.frontend.NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:
  File ""/home/aimaster/lab_storage/jinyeong/VQA/mmf/mmf/utils/distributed.py"", line 340
    def warn(*args, **kwargs):
                     ~~~~~~~ <--- HERE
        force = kwargs.pop(""force"", False)
        if is_master or force:
'get_layout' is being compiled since it was called from 'SparseStorage.set_value'
  File ""/home/aimaster/anaconda3/envs/mmf/lib/python3.7/site-packages/torch_sparse/storage.py"", line 210
                  layout: Optional[str] = None):
        if value is not None:
            if get_layout(layout) == 'csc':
               ~~~~~~~~~~~~~~~~~ <--- HERE
                value = value[self.csc2csr()]
            value = value.contiguous()

",neutral,negative
57,Time cost increases,uber/sbnet,open,,IwakuraRein,,2021-08-01 08:52:51,,1.0,,28.0,957434294.0,"Hi. Thanks for the codes and the detailed instruction.

I implemented sparse convolution into my encoder:
```python
with tf.variable_scope('featureEncoder'):
	auxiShape = (self.inputShape[0], self.inputShape[1], self.inputShape[2], 7)
	featureShape = (self.inputShape[0], self.inputShape[1], self.inputShape[2], 32)
	blockSize = 8
	blockStride = (8,8)
	blockOffset = (0,0)
	blockCount = (self.divup(self.inputShape[1], blockStride[0]), self.divup(self.inputShape[2], blockStride[1]))
	inBlockParams = { ""dynamic_bsize"": (blockSize, blockSize), ""dynamic_boffset"": blockOffset, ""dynamic_bstride"": blockStride }
	outBlockParams = { ""dynamic_bsize"": (blockSize, blockSize), ""dynamic_boffset"": blockOffset, ""dynamic_bstride"": blockStride }
	
	if not self.training:
		indices = sbnet_module.reduce_mask(self.mask, blockCount, tol=0.1, **inBlockParams)
	
		# stack active overlapping tiles to batch dimension
		stack = sbnet_module.sparse_gather(
			auxi, indices.bin_counts, indices.active_block_indices, transpose=False, **inBlockParams)
	else:
		stack = auxi
	# perform dense convolution on a sparse stack of tiles
	stack = self.conv_layer2(stack, 7, 32, name='1')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='2')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='3')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='4')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='5')
	stack = tf.nn.leaky_relu(stack)

	# write/scatter the tiles back on top of original tensor
	# note that the output tensor is reduced by 1 on each side due to 'VALID' convolution
	if not self.training:
		feature=sbnet_module.sparse_scatter(
			stack, indices.bin_counts, indices.active_block_indices,
			self.lastFeature, transpose=False, add=False, atomic=False, **outBlockParams)
		feature.set_shape(featureShape)
	else:
		feature=stack
```

`self.training` is set `False` when training and `True` when testing. Variable `mask` is generated outside the network and fed in via `tf.placeholder`. So does `self.lastFeature`.

I tried to measure the inference time with timeline:
```python
feed_dict = {model.source: src, model.target: tgt, model.batch_size:src_hdr.shape[0], model.mask:Mask, model.feature:Feature}
denoised_1_bd, Feature = sess.run([model.fake_image, model.feature], feed_dict, options=run_options, run_metadata=run_metadata)
tl = timeline.Timeline(run_metadata.step_stats)
ctf = tl.generate_chrome_trace_format(show_memory=True)
with open(os.path.join(errorlog_dir, 'timeline.json'),'w') as wd:
	wd.write(ctf)
```

![timeline](https://user-images.githubusercontent.com/28486541/127764964-e58c3a77-afdc-49ba-831d-d2d4f2ee0edc.png)

However, I can't find time records of layers under 'featureEncoder'. And there are two bars captioned unknown, the second of which is strangely long. Some Pooling and LeakyRelu‘s time is also strange, costing nearly 2ms.

![unknown](https://user-images.githubusercontent.com/28486541/127765056-52b29203-e8ef-40d5-8809-5bd9684499c5.png)

I wonder how I can get the proper time measurement. Thanks.

**My Environment**
TensorFlow Version: 1.15.0
Operating System: Ubuntu 16.04
Python Version: 3.6.13
CUDA Version: 10.0
CUDNN Version: 7.6.4
GPU Type: RTX 2080ti
Nvidia Driver Version: 460.67",neutral,negative
58,jmlr training code vs description in arxiv paper,deepinsight/insightface,open,,park-sungjoon,,2022-09-21 05:47:45,,3.0,,2112.0,1380353948.0,"Thanks for sharing your excellent work!
I am looking through your code, and there seem to be some differences with the paper shared on arxiv (or I misunderstood something).
Are the data preparation, training, and configuration files the same as those used to train the pretrained-model you share?
Some examples:
1. in `rec_builder.py` line 79, `cfg.input_size = 512`. My understanding is that this results in image with size 512, not 256 as described in the paper
2. in `train.py` line 245, we have `iter_loss.backward()`, but `iter_loss = dloss['Loss']`, which does not include the ""bone_losses"". I.e. We only have L_vert + L_land in Eq. 4 in the paper on arxiv.

Thanks in advance.",neutral,negative
59,Add option to disable non-max supression.,open-mmlab/mmdetection,closed,ZwwWayne,Erotemic,hhaAndroid,2020-12-01 20:05:12,2021-04-09 01:51:06,3.0,enhancement,4214.0,754686882.0,"**Describe the feature**

It would be nice if there was a simple configuration to disable non-max suppression. This should be fairly simple to add to `multiclass_nms`. In the `nms_cfg` simply accept a key called `""enabled""`, which defaults to `True` and if `False`  then just modify the format so it agrees with the normal output of that function: 

```python
    enabled = nms_cfg.get('enabled', True)
    if enabled:
        dets, keep = batched_nms(bboxes, scores, labels, nms_cfg)
    else:
        # Keep the output format the same as if nms was called
        keep = torch.arange(scores.shape[0])
        dets = torch.cat((bboxes[keep], scores[keep].reshape(-1, 1)), dim=1)
```

This will not break existing models, but it will allow for new models to have this feature.

**Motivation**

It is inconvenient when I want to use mmdet to build larger models, but I'm forced to execute its nms step. I find myself monkey patching it out frequently. I'd like to avoid this and simply set a flag that disables it. 
",neutral,negative
60,F1 score fails for multiclass classification,namisan/mt-dnn,closed,namisan,mariamedp,namisan,2020-03-11 13:13:53,2020-04-01 06:56:04,2.0,,154.0,579254688.0,"calc_metrics fails when trying to use F1 as a metric in multiclass classification:

```
Traceback (most recent call last):
  ...
  File ""<path>\data_utils\metrics.py"", line 18, in compute_f1
    return 100.0 * f1_score(labels, predicts)
  File ""<path>\lib\site-packages\sklearn\metrics\_classification.py"", line 1099, in f1_score    
    zero_division=zero_division)
  File ""<path>\lib\site-packages\sklearn\metrics\_classification.py"", line 1226, in fbeta_score 
    zero_division=zero_division)
  File ""<path>\lib\site-packages\sklearn\metrics\_classification.py"", line 1484, in precision_recall_fscore_support
    pos_label)
  File ""<path>\lib\site-packages\sklearn\metrics\_classification.py"", line 1316, in _check_set_wise_labels
    % (y_type, average_options))
ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].
```

The equivalent to what accuracy_score does in multiclass would be:
```
100.0 * f1_score(labels, predicts, average='macro')
```
but I'm not sure if that's the desired behaviour or it would be better to add extra metrics F1macro, F1micro, etc.
",negative,negative
61,Mask R-CNN for single instance mask detection,matterport/Mask_RCNN,open,,HaroldTaeter,,2020-08-04 17:00:07,,0.0,,2302.0,672940786.0,"Hi everybody. 

I'm using Mask R-CNN on videos that contain a single object moving. I'm first dividing the video into frames and then treating each frame independently. In every frame, I have a single object belonging to a single class. Primarily, I want to detect the object in each frame and, secondarily, I want to retrieve the mask around that object as accurately as possible in every frame. This is already working more or less but I'd like to know how to tune the model's parameters in order to improve my results. 

In particular, I have questions about the following parameters:
- **DETECTION_MAX_INSTANCES**: should this be set to 1 ?
- **DETECTION_MIN_CONFIDENCE**: originally 0.7, should this be set to a lower value ? (to increase the chance of detecting the object)
- **RPN_NMS_THRESHOLD**: originally 0.7, should this be increased ? (to increase the chance of detecting the object)
- **LOSS_WEIGHTS = {
        ""rpn_class_loss"": 1.,
        ""rpn_bbox_loss"": 1.,
        ""mrcnn_class_loss"": 1.,
        ""mrcnn_bbox_loss"": 1.,
        ""mrcnn_mask_loss"": 1.
    }**: should I change something here ? For example first training with a higher rpn_bbox_loss and a higher mrcnn_bbox_loss and then training subsequently with a higher mrcnn_mask_loss.

Additionally, is there any pre-processing or post-processing step that could be useful in this context ? 

Any other suggestion on how to improve the accuracy of the process is of course welcome.

Thanks !",neutral,neutral
62,Loss with x2,JiahuiYu/generative_inpainting,closed,,GeorgeKaspar,JiahuiYu,2018-07-18 13:54:08,2018-07-18 16:34:05,5.0,,94.0,342338860.0,"Why did you use ae_loss and l1_loss with x2
Is it smooth results?",neutral,neutral
63,Mean and std for Dataset,TRI-ML/packnet-sfm,closed,,MingYang-buaa,MingYang-buaa,2020-09-13 13:16:03,2020-09-14 07:03:51,2.0,,76.0,700568239.0,"Hey, where there mean and std for the KITTI datasets? I find that the DepthResNet using mean and std for image as:
`
x = (input_image - 0.45) / 0.225
`
But I cannot find  such Normalization in PackNet.",neutral,neutral
64,Converting 3D bounding Box in LiDAR coordinate to 3D bounding Box in Camera coordinate,nutonomy/nuscenes-devkit,closed,,konyul,whyekit-motional,2022-07-28 01:55:59,2022-07-28 08:25:07,1.0,,798.0,1320292430.0,"I want to project 3D bounding box in lidar coordinate to image. However, I see there is a code that projects 3D bounding box in camera coordinate to image. Is there any codes that can convert 3D bounding Box in LiDAR coordinate to 3D bounding Box in Camera coordinate?",neutral,neutral
65,Can you creat clever business platform?,google-research/google-research,open,,adosha111,,2020-12-12 14:21:08,,0.0,,492.0,763883424.0,,neutral,neutral
66,Annotation information reading,nutonomy/nuscenes-devkit,closed,,Majiawei,holger-motional,2021-06-29 13:06:25,2021-06-29 13:48:25,1.0,,615.0,932643375.0,"How can I read 2D annotation information of front camera image?
From nuscenes to 3D annotation conversion or 2D annotation in nuimage",neutral,neutral
67,How to read fnames for each batch by setting return_fnames=True on data.data_from_fnames(),JiahuiYu/generative_inpainting,closed,,jelujelu,JiahuiYu,2020-07-10 01:05:43,2020-08-13 12:48:00,1.0,,456.0,654438233.0,"Hi,
I want to apply different mask for each image within the same batch using the annotated facial landmarks.
If I understand correctly, I need to first read the fnames within each batch by setting `return_fnames=True` when creating the data object.

The data pipeline returns me `<tf.Tensor 'FIFOQueueV2_DequeueMany:1' shape=(16,) dtype=string>`, however I have trouble with viewing/reading this tensor using tf.run(). What would be the correct way to obtain the filenames for each batch?

Thanks,",neutral,positive
68,【POEM】Finished training suddenly and all assert_file_exists output not found,google-research/google-research,closed,,Brave-peng,Brave-peng,2020-11-03 13:16:46,2020-11-04 05:13:35,3.0,,449.0,735311058.0,"I try to sh 'run.sh', and it seems that I meet all experience requirements, but I find the system Finished training suddenly and all assert_file_exists outputs 'not found'.

 I can't understand what happens, Can someone help me? It's my first time to ask questions in English, this may make my question unclear, and I will try my best to think about anybody's response, thanks!



This is my log:

> Building wheels for collected packages: wrapt, termcolor
  Building wheel for wrapt (setup.py) ... done
  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19552 sha256=9bb47c22b3213eae33687d520405a6ad1af358742738563bf5677d2c120cdccf
  Stored in directory: /home/zp/.cache/pip/wheels/4c/8d/0e/ecb228daca7bc2ae7f9a9d713ef75fce4a083de089869418b5
  Building wheel for termcolor (setup.py) ... done
  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=747aa37b47651ca7e68b8a56cac12cbf58b0b9ea7bca97f9540358d75237ea5f
  Stored in directory: /home/zp/.cache/pip/wheels/22/6d/f1/cee5814a13ba0c8ecd5ae67093238282c258bfdd6881b6b638
Successfully built wrapt termcolor
Installing collected packages: six, absl-py, numpy, protobuf, zipp, importlib-metadata, markdown, certifi, chardet, urllib3, idna, requests, grpcio, tensorboard-plugin-wit, werkzeug, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, h5py, google-pasta, keras-preprocessing, wrapt, gast, opt-einsum, termcolor, tensorflow-estimator, astunparse, tensorflow, decorator, dm-tree, cloudpickle, tensorflow-probability, tf-slim
Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 cloudpickle-1.6.0 decorator-4.4.2 dm-tree-0.1.5 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 idna-2.10 importlib-metadata-2.0.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 tensorflow-probability-0.11.1 termcolor-1.1.0 tf-slim-1.1.0 urllib3-1.25.11 werkzeug-1.0.1 wrapt-1.12.1 zipp-1.2.0
+ TRAIN_DIR=/tmp/e3d/train
+ mkdir -p /tmp/e3d/train
+ python3 -m poem.train --alsologtostderr --input_table=poem/testdata/tfe-2.tfrecords --train_log_dir=/tmp/e3d/train --batch_size=4 --num_steps=5 --input_shuffle_buffer_size=10 --summarize_percentiles=false
2020-11-03 12:27:16.959669: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/ops/linalg/linear_operator_full_matrix.py:149: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.
Instructions for updating:
Do not pass `graph_parents`.  They will  no longer be used.
W1103 12:27:18.325209 140324728178496 deprecation.py:506] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/ops/linalg/linear_operator_full_matrix.py:149: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.
Instructions for updating:
Do not pass `graph_parents`.  They will  no longer be used.
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/poem/core/models.py:97: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).
W1103 12:27:18.362956 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/poem/core/models.py:97: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W1103 12:27:18.365832 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py:458: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W1103 12:27:18.705586 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py:458: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I1103 12:27:18.933850 140324728178496 pipeline_utils.py:308] Resume latest training checkpoint in: /tmp/e3d/train.
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1103 12:27:19.484864 140324728178496 deprecation.py:506] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tf_slim/learning.py:734: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
W1103 12:27:19.663510 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tf_slim/learning.py:734: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2020-11-03 12:27:19.780182: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-03 12:27:19.801271: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3696000000 Hz
2020-11-03 12:27:19.802094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8bb7aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-03 12:27:19.802106: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-03 12:27:19.834374: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-11-03 12:27:19.927968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 12:27:19.928318: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8bce8b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-03 12:27:19.928330: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-11-03 12:27:19.928462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 12:27:19.928775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-11-03 12:27:19.928809: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-03 12:27:20.214316: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-11-03 12:27:20.218626: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-03 12:27:20.219405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-03 12:27:20.224505: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-11-03 12:27:20.227276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-11-03 12:27:20.227897: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64::/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/usr/local/gcc-5.3.0/lib:/usr/local/gcc-5.3.0/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64
2020-11-03 12:27:20.227946: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-11-03 12:27:20.227998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-03 12:27:20.228025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-11-03 12:27:20.228047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
INFO:tensorflow:Restoring parameters from /tmp/e3d/train/model.ckpt-00000006
I1103 12:27:20.233745 140324728178496 saver.py:1293] Restoring parameters from /tmp/e3d/train/model.ckpt-00000006
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
W1103 12:27:20.375499 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
I1103 12:27:20.376783 140324728178496 session_manager.py:505] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I1103 12:27:20.393647 140324728178496 session_manager.py:508] Done running local_init_op.
INFO:tensorflow:Starting Session.
I1103 12:27:21.270997 140324728178496 learning.py:746] Starting Session.
INFO:tensorflow:Saving checkpoint to path /tmp/e3d/train/model.ckpt
I1103 12:27:21.287984 140318586431232 supervisor.py:1117] Saving checkpoint to path /tmp/e3d/train/model.ckpt
INFO:tensorflow:Starting Queues.
I1103 12:27:21.288069 140324728178496 learning.py:760] Starting Queues.
INFO:tensorflow:Recording summary at step 6.
I1103 12:27:21.492430 140318569645824 supervisor.py:1050] Recording summary at step 6.
INFO:tensorflow:global_step/sec: 0
I1103 12:27:21.823515 140318578038528 supervisor.py:1099] global_step/sec: 0
INFO:tensorflow:Stopping Training.
I1103 12:27:21.906329 140324728178496 learning.py:769] Stopping Training.
INFO:tensorflow:Finished training! Saving model to disk.
I1103 12:27:21.906489 140324728178496 learning.py:777] Finished training! Saving model to disk.
/home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/summary/writer/writer.py:387: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.
  warnings.warn(""Attempting to use a closed FileWriter. ""
+ assert_file_exists /tmp/e3d/train/all_flags.train.json
+ [[ ! -f /tmp/e3d/train/all_flags.train.json ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/graph.pbtxt
+ [[ ! -f /tmp/e3d/train/graph.pbtxt ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/model.ckpt-00000005.data-00000-of-00001
+ [[ ! -f /tmp/e3d/train/model.ckpt-00000005.data-00000-of-00001 ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/model.ckpt-00000005.meta
+ [[ ! -f /tmp/e3d/train/model.ckpt-00000005.meta ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/model.ckpt-00000005.index
+ [[ ! -f /tmp/e3d/train/model.ckpt-00000005.index ]]
script.sh: 37: script.sh: [[: not found
",neutral,negative
69,Bug - self.easy_margin not defined in arcloss,deepinsight/insightface,closed,,nsriniva03,anxiangsir,2022-01-24 15:26:49,2022-02-03 12:25:20,2.0,,1893.0,1112793435.0,"In Arcface class, self.easy_margin is not defined.  Variable easy_margin must be passed as input during initialization
",neutral,neutral
70,y_true not cast to one_hot if needed in f1_score metric,tensorflow/addons,open,Aditya-Jha2002,BlueskyFR,,2022-02-14 23:31:53,,1.0,,2665.0,1137973766.0,"Hi!

As per [this line](https://github.com/tensorflow/addons/blob/8cec33fcaaf1cf90aec7bdd55a0fcdbb251ce5c2/tensorflow_addons/metrics/f_scores.py#L167), if `y_true` is not under the `one_hot` form (i.e. an integer), the call will fail, so it must be converted to `one_hot` encoding if needed before.",neutral,negative
71,Does the loss will be averaged by batchsize (especially in distributed training)?,open-mmlab/mmdetection,closed,yhcao6,tswc,ZwwWayne,2021-03-26 02:17:27,2022-07-24 13:18:22,1.0,,4834.0,841528089.0,"I have not found related code in mmdetection and mmcv, could someone help？",neutral,neutral
72,Build binaries for python3.9,tensorflow/addons,closed,,seanpmorgan,seanpmorgan,2020-10-06 01:51:40,2021-05-14 17:28:09,7.0,build,2188.0,715276737.0,"**Describe the feature and the current behavior/state.**

Py39 is out but we need to wait for core TF to support it first. We should quickly follow.

",neutral,neutral
73,Cannot find the eval results in runner.log_buffer.output ,open-mmlab/mmdetection,closed,RangiLyu,FrancescoSaverioZuppichini,RangiLyu,2021-09-30 12:31:50,2022-10-25 03:33:46,1.0,,6203.0,1012120952.0,"Hi guys, 

I hope you are doing great. I am trying to get the `eval_results` from my custom hook but even if they are set (https://github.com/open-mmlab/mmcv/blob/8cac7c25ee5bc199d6e4059297ef2fa92d9c069c/mmcv/runner/hooks/evaluation.py#L327) in `runner.log_buffer.output` I am not able to find them. I have tried with any time of callbacks in the hooks APIs (after_iter, after_epoch, after_val_epoch, etc) but the `log_buffer` is always empty

```
@HOOKS.register_module()
class EarlyStopping(Hook):
    def after_val_epoch(self, runner):
        print(runner.log_buffer.output)

```

Output:

```
OrderedDict()
```

What is the correct way to properly fetch the eval results?

Thank you",neutral,positive
74,Slow training,matterport/Mask_RCNN,open,,ganav,,2019-08-05 09:59:45,,3.0,,1678.0,476769006.0,"I have gtx1070 video card and i installed everything using requirements.txt, however it is very slow 
when i train the shapes sample. it takes almost a half minute per image.
So i guess it is using CPU. then how it is using CPU when i installed everything using requirements.txt? So i checked the text file but there was no character ""-gpu"" in the text file. then i reinstalled using ""pip install tensorflow-gpu"". But getting errors related to versions that they dont match to each other. So im totally stuck. 

1. Why is it so slow?
2. is it really using CPU? (coz mostly cmd shows how much memory we have when train starts but not showing anything)
3. how to reinstall or correct installations back? should i format my pc or any simple way?
help",neutral,negative
75,Corrupted LMDB in VQA2,facebookresearch/mmf,closed,,woojeongjin,woojeongjin,2020-10-19 04:41:12,2020-10-19 18:57:31,5.0,,635.0,724252995.0,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
Hello,
The VQA2 dataset was downloaded automatically, but I faced the following error.

`  File ""/home/woojeong2/miniconda3/envs/mmf2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/woojeong2/miniconda3/envs/mmf2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/woojeong2/miniconda3/envs/mmf2/lib/python3.7/site-packages/torch/utils/data/dataset.py"", line 207, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File ""/home/woojeong2/mmf/mmf/datasets/builders/vqa2/dataset.py"", line 53, in __getitem__
    return self.load_item(idx)
  File ""/home/woojeong2/mmf/mmf/datasets/builders/vqa2/dataset.py"", line 90, in load_item
    features = self.features_db[idx]
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 91, in __getitem__
    return self.get(image_info)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 99, in get
    return self.from_path(feature_path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 107, in from_path
    features, infos = self._get_image_features_and_info(path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 80, in _get_image_features_and_info
    image_feats, infos = self._read_features_and_info(feat_file)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 65, in _read_features_and_info
    feature, info = feature_reader.read(feat_file)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/readers/feature_readers.py"", line 95, in read
    return self.feat_reader.read(image_feat_path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/readers/feature_readers.py"", line 157, in read
    image_info = self._load(image_feat_path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/readers/feature_readers.py"", line 240, in _load
    image_info = pickle.loads(txn.get(self.image_ids[img_id_idx]))
lmdb.CorruptedError: mdb_get: MDB_CORRUPTED: Located page was wrong type` 


Sometimes it showed a different error as follows:
`  File ""/home/woojeong2/miniconda3/envs/mmf2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/woojeong2/miniconda3/envs/mmf2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/woojeong2/miniconda3/envs/mmf2/lib/python3.7/site-packages/torch/utils/data/dataset.py"", line 207, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File ""/home/woojeong2/mmf/mmf/datasets/builders/vqa2/dataset.py"", line 53, in __getitem__
    return self.load_item(idx)
  File ""/home/woojeong2/mmf/mmf/datasets/builders/vqa2/dataset.py"", line 90, in load_item
    features = self.features_db[idx]
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 91, in __getitem__
    return self.get(image_info)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 99, in get
    return self.from_path(feature_path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 107, in from_path
    features, infos = self._get_image_features_and_info(path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 80, in _get_image_features_and_info
    image_feats, infos = self._read_features_and_info(feat_file)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/features_database.py"", line 65, in _read_features_and_info
    feature, info = feature_reader.read(feat_file)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/readers/feature_readers.py"", line 95, in read
    return self.feat_reader.read(image_feat_path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/readers/feature_readers.py"", line 157, in read
    image_info = self._load(image_feat_path)
  File ""/home/woojeong2/mmf/mmf/datasets/databases/readers/feature_readers.py"", line 240, in _load
    image_info = pickle.loads(txn.get(self.image_ids[img_id_idx]))
_pickle.UnpicklingError: invalid load key, '\x00'.`

## Command
CUDA_VISIBLE_DEVICES=0 mmf_run config=projects/visual_bert/configs/vqa2/defaults.yaml model=visual_bert dataset=vqa2 checkpoint.resume_zoo=visual_bert.finetuned.vqa2.from_coco_train  run_type=val training.batch_size=40 

## To Reproduce

Steps to reproduce the behavior:

<!-- If you were running a command, post the exact command that you were running -->

1. run the command on the vqa2 dataset.
2.
3.

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from the
environment collection script from PyTorch
(or fill out the checklist below manually).

You can run the script with:
```
# For security purposes, please check the contents of collect_env.py before running it.
python -m torch.utils.collect_env
```

 - PyTorch Version (e.g., 1.0): 1.6.0
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): conda
 - Build command you used (if compiling from source): pip install --editable .
 - Python version: 3.7.9
 - CUDA/cuDNN version: Cuda version 10.1
 - GPU models and configuration: Qudro RTX 8000
 - Any other relevant information:

## Additional context
This might be an issue on my side because our machine has a file system issue now. I want to confirm if this also occurs to others.
<!-- Add any other context about the problem here. -->
",negative,positive
76,What's the relation betwen IMAGE_SIZE and HEATMAP_SIZE,Microsoft/human-pose-estimation.pytorch,closed,,ArchNew,ArchNew,2018-09-29 03:17:36,2019-01-02 15:07:28,0.0,,32.0,365091362.0,My cards (Nvidia Titan XP) is unable to run on 384x288 (out of memory). I want to run on something less than 384x288 but more than 256x192. Thanks!,neutral,positive
77,Error while loading multi-channel model,Wensi-Tang/OS-CNN,closed,,Karlheinzniebuhr,Karlheinzniebuhr,2022-05-27 00:06:42,2022-05-30 21:00:34,5.0,,11.0,1250206118.0,"Ok, we are almost there. Getting this error while loading a trained multi-channel model. I integrated to the best of my ability the code you showed us for the BTC pipeline. 
But the load-model pipeline differs from the training pipeline so I'm not sure what I'm missing.
[Check the complete notebook to reproduce](https://colab.research.google.com/drive/1r36hRSYqIIV0XEg1sw_KafUv4hLTH12C?usp=sharing)

```
TypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:
 * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
 * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)
```",neutral,negative
78,"no issue,sorry",matterport/Mask_RCNN,closed,,AlexSG18,AlexSG18,2020-03-13 13:08:03,2020-03-13 13:11:59,0.0,,2046.0,580589910.0,,neutral,neutral
79,torch serve unregistered model,open-mmlab/mmdetection,closed,hhaAndroid,FirasKedidi,FirasKedidi,2021-07-05 15:35:04,2021-07-18 08:11:52,4.0,,5538.0,937200840.0,"Hello,
i want to use torch serve to deploy my model . Wen running torchserve --start i get this error that the model is unregistered. 

![image](https://user-images.githubusercontent.com/58661767/124494570-eae73100-ddae-11eb-9899-22794a4fbf23.png)


Can you help me please.",neutral,negative
80,training parameter,DmitryUlyanov/texture_nets,open,,suke27,,2018-03-19 11:45:02,,0.0,,89.0,306429094.0,"I found default transfer parameter is 
cmd:option('-content_weight', 1)
cmd:option('-style_weight', 1)
cmd:option('-tv_weight', 0, 'Total variation weight.')

seems not same with common option(Gatys set 1000:1), is it your pretrained option? and do you try transfer result with and without tv loss?",neutral,neutral
81,add 'Masked-attention Mask Transformer for Universal Image Segmentation' to mmdet,open-mmlab/mmdetection,closed,BIGWangYuDong,chhluo,chhluo,2021-12-09 16:07:47,2022-04-08 13:27:14,1.0,enhancement,6748.0,1075744574.0,"**add 'Masked-attention Mask Transformer for Universal Image Segmentation' to mmdet**

**Motivation**
This algorithm called mask2former is an improved version of maskformer, and the results in the paper are still very good.

**Related resources**
http://arxiv.org/abs/2112.01527


**Additional context**
I want to reimplement it, add it to mmdet.",neutral,positive
82,Simplified Causal/Unidirectional Fast Attention,google-research/google-research,open,,jackd,,2022-12-01 01:24:48,,0.0,,1406.0,1470502463.0,"I've been playing around with the [causal attention performer implementation](https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py) and believe I've found a much simpler representation that leads to more straight-forward implementation (no explicit loops/scans or custom gradients). Performance is comparable, though compilation/tracing time is orders of magnitude faster. I'd be happy to put a PR in with changes, but I realize these types of improvements aren't necessarily a high priority so thought I'd check here first to see if there's demand for it here first.

TL;DR: the implementations below, compared to the tensorflow implementation in this repo:
- is way less code (~5 lines of logic for numerator/denominator, compared to ~50)
- jit-compiles faster (thanks to no explicit python loops)
- gives the same results in roughly the same time using basic dimensions

Implementations/benchmarks/tests are available [here](https://github.com/jackd/simple-fast-attention). Relevant parts included below and a notebook is [here](https://colab.research.google.com/drive/1Fk7LWy87LzZ1usDDVhllIYxUNpoZelTh?usp=sharing).

## Implementations

```python
def causal_numerator(qs: tf.Tensor, ks: tf.Tensor, vs: tf.Tensor):
    """"""Computes not-normalized FAVOR causal attention A_{masked}V.

    Args:
      qs: query_prime tensor of the shape [L,B,H,M].
      ks: key_prime tensor of the shape [L,B,H,M].
      vs: value tensor of the shape [L,B,H,D].

    Returns:
      Not-normalized FAVOR causal attention A_{masked}V.
    """"""
    # rhs = tf.einsum('lbhm,lbhd->lbhdm', ks, vs)
    rhs = tf.expand_dims(ks, axis=-2) * tf.expand_dims(vs, axis=-1)  # [L,B,H,D,M]
    rhs = tf.cumsum(rhs, axis=0)
    # return tf.einsum('lbhm,lbhdm->lbhd', qs, rhs)
    return tf.linalg.matvec(rhs, qs)
```

```python
def causal_denominator(qs, ks):
    """"""Computes FAVOR normalizer in causal attention.

    Args:
      qs: query_prime tensor of the shape [L,B,H,M].
      ks: key_prime tensor of the shape [L,B,H,M].

    Returns:
      FAVOR normalizer in causal attention.
    """"""
    rhs = tf.cumsum(ks, axis=0)
    return tf.einsum(""lbhm,lbhm->lbh"", qs, rhs)
```

## Theory

The task we consider is to compute the noncausal numerator $N$, where

$N = \left[(Q K^T) \circ L\right] V$

where $Q$, $K$ and $V$ are the query, key and value matrices used in fast attention, $L$ is a lower triangular matrix with values of $1$ on and below the diagonal and $\circ$ is the _Hadamard product_ (elementwise product). Noting that $Q$ and $K$ are low-rank (that's the whole point of performers/FAVOR), we can use the following handy dandy property of Hadamard products ([Property 1](http://pi.math.cornell.edu/~ajt/presentations/HadamardProduct.pdf)):

$\left[A \circ \sum_j \mathbf{u}_j \mathbf{Pv}_j^T\right]x = \sum_j D(\mathbf{u}_j) A D(\mathbf{v}_j) \mathbf{x}$

where $D(\mathbf{z})$ is the diagonal matrix with diagonal values $\mathbf{z}$. This means we can express our fast causal attention output as

$N = \sum_m D(\mathbf{q}_m) L D(\mathbf{k}_m) V$

where $\mathbf{q}_m$ and $\mathbf{k}_m$ are the $m^\text{th}$ columns of Q and K respectively.

Note it is neither efficient nor necessary to compute any of the new matrices above. $D(\mathbf{k}_m) Z$ is just the scaling of rows of $Z$ by $\mathbf{k}_m$, while $L Z$ is the cumulative sum of $Z$ on the leading dimension. This results in a significantly simpler tensorflow implementation without the need to implement custom gradients or use python loops.

## Benchmarks

Results using google-benchmark are given below. `v0` is the original, `v1` is the one discussed above. `warmup_time` is the time for the first run, which I'm using as a proxy for compilation time. Results were generated on a fairly old laptop with an Nvidia 1050Ti. Script to generate available [here](https://github.com/jackd/simple-fast-attention/blob/main/gbenchmark.py).

```txt
--------------------------------------------------------------
Benchmark                    Time             CPU   Iterations
--------------------------------------------------------------
v0_forward-cpu         5403096 ns       364764 ns         1000
v1_forward-cpu         5419832 ns       365650 ns         1000
v0_backward-cpu         268558 ns       238634 ns         2896
v1_backward-cpu         267089 ns       235842 ns         2937
v0_forward-gpu          288531 ns       241580 ns         2874
v1_forward-gpu          285695 ns       238078 ns         2908
v0_backward-gpu         268220 ns       237309 ns         2869
v1_backward-gpu         268324 ns       240429 ns         2751
v0_forward-cpu-jit      299143 ns       271613 ns         2516
v1_forward-cpu-jit      291873 ns       269618 ns         2538
v0_backward-cpu-jit     303150 ns       275359 ns         2483
v1_backward-cpu-jit     303948 ns       276806 ns         2482
v0_forward-gpu-jit      278147 ns       277842 ns         2450
v1_forward-gpu-jit      276128 ns       275956 ns         2523
v0_backward-gpu-jit     256809 ns       256798 ns         2706
v1_backward-gpu-jit     252543 ns       252537 ns         2769

Warmup time for v0_forward-cpu: 6.56445574760437
Warmup time for v1_forward-cpu: 0.1015627384185791
Warmup time for v0_backward-cpu: 22.0670325756073
Warmup time for v1_backward-cpu: 0.08140373229980469
Warmup time for v0_forward-gpu: 6.233572244644165
Warmup time for v1_forward-gpu: 0.028412342071533203
Warmup time for v0_backward-gpu: 22.226712226867676
Warmup time for v1_backward-gpu: 0.051419734954833984
Warmup time for v0_forward-cpu-jit: 6.481787443161011
Warmup time for v1_forward-cpu-jit: 0.05790424346923828
Warmup time for v0_backward-cpu-jit: 24.72081184387207
Warmup time for v1_backward-cpu-jit: 0.09151363372802734
Warmup time for v0_forward-gpu-jit: 8.328083515167236
Warmup time for v1_forward-gpu-jit: 0.08592033386230469
Warmup time for v0_backward-gpu-jit: 24.7033634185791
Warmup time for v1_backward-gpu-jit: 0.12377095222473145
```",neutral,positive
83,TabNet. Unable to reproduce the 99% accuracy on Poker Hand dataset.,google-research/google-research,closed,,mgrankin,mgrankin,2020-02-03 07:04:54,2020-02-07 06:27:48,8.0,,191.0,558885187.0,"We're trying to reproduce the results from a TabNet paper. There is a remarkable 99% accuracy on Poker Hand dataset. We can't reproduce the result, we've got only 54%. Can you share the code, so we can check what's wrong with our setup?
",neutral,negative
84,Can you make the landmark annotations on the validation set of WIDER FACE public?,deepinsight/insightface,open,,hdjsjyl,,2019-08-13 15:01:39,,0.0,,853.0,480208175.0,Can you make the landmark annotations on the validation set of WIDER FACE public?,neutral,neutral
85,The problem of package SDK,open-mmlab/mmdetection,closed,,linquanxu,linquanxu,2019-06-19 04:47:55,2019-07-01 02:23:12,5.0,,828.0,457804702.0,"hi,Thank you for contributing such a great project. I want to use mmdetection to package SDK(such 
 as .dll, .so etc.) for detecting, how to finish it?",neutral,positive
86,the training process don't go on my own dataset,open-mmlab/mmdetection,closed,v-qjqs,lujiazho,v-qjqs,2021-01-13 08:38:42,2021-01-18 01:38:31,3.0,,4438.0,784903689.0,"When I trained the Faster R-CNN model on my own dataset GWHD, I modify the Dataset Register Class:


And I modify the configuration, then I run it:
![image](https://user-images.githubusercontent.com/68451409/104426902-88f6fd00-55bd-11eb-8794-f5f48351675d.png)

It seems that it was running, but actually, I have waited for a long time. It didn't have any progression and there is no any error message showed, the cell is still running. How could that be? What would cause this to happen and can anyone help me?",neutral,negative
87,Run GNNExplainer,divelab/DIG,closed,,paulamartingonzalez,Nate1874,2021-05-20 09:57:39,2021-05-31 01:38:48,8.0,xgraph,24.0,896573687.0,"I am trying to run the GNN Explainer code. Although I have used the install.batch to generate the conda environment, I am having issues with the packages. Namely with tap (in the line ""from tap import Tap"") . I tried installing it myself and I can import tap but doesn't seem to find anything called Tap on it. Could you guide me here please?

Also, I was going through the example and I was wondering which settings should I change to run it on the graph level predictions. Is there an example for that? And, is it possible to get the feature importances and the important subgraphs? I didn't see any outputs in the explain functions.

Thanks in advance!
",neutral,positive
88,No enough gpus for dedicated usage.,JiahuiYu/generative_inpainting,closed,,KangSH9776,JiahuiYu,2018-05-25 06:45:55,2018-05-25 17:58:46,3.0,,47.0,326400545.0,"Hello, I have a question and I am writing. 
I want to 'test' the 'canyon_input.png' image in the 'examples' folder.
I also want to 'train' the 'places2' dataset
Look for your github and save the files to be stored in 'model_logs'
I wrote an error below the article saying that an error occurred in 'neuralgym'.
Your answers I'll wait.

<Error
Traceback (most recent call last):
  File ""test.py"", line 23, in <module>
    ng.get_gpus(1)
  File ""/home/pmi/Documents/Songhee/songhee/lib/python3.5/site-packages/neuralgym/utils/gpus.py"", line 70, in get_gpus
    ' [(gpu id: num of processes)]: {}'.format(sorted_gpus))
SystemError: No enough gpus for dedicated usage. [(gpu id: num of processes)]: [(0, 3)]
",neutral,neutral
89,Pretrained models,ShichenLiu/CondenseNet,closed,,ArsenLuca,ShichenLiu,2018-08-24 10:22:40,2018-09-29 17:59:32,2.0,,18.0,353728023.0,Would you please share the pretrained model in Baidu disk or Google Drive? I could not access Drop Box.,neutral,positive
90,Config Day2Night | Split Datasets,mingyuliutw/UNIT,closed,,solesensei,mingyuliutw,2018-12-23 23:56:41,2021-03-03 17:32:21,7.0,,96.0,393789394.0,"Hi, there, i'm trying to train dataset and stucked with some learning problems

**Example** of 10000 iterations day 2 night (train)
![image](https://user-images.githubusercontent.com/24857057/50388395-345a5e00-0726-11e9-95ab-60b9b549572c.png)
**Example** of 10000 iterations night 2 day (train)
![image](https://user-images.githubusercontent.com/24857057/50388408-7edbda80-0726-11e9-9de0-8a257573d44e.png)

I think that it's `unit_day2night.yaml` configuration problem. Can you share yours config file?

---

Another option, that it's **dataset's split** issue.
I used **50/50** for train and test. And both divided into two equal domains A and B.

",neutral,negative
91,K-Fold Cross Validation in MMDet,open-mmlab/mmdetection,open,Czm369,hllkya,,2022-08-29 14:23:54,,1.0,,8662.0,1354394283.0,"Can I implement K-Fold cross validation on my dataset using MMDetection toolbox, is this supported at current version of the toolbox? Are there anyways to implement it?",neutral,neutral
92,Question about the reprojected distance loss function,TRI-ML/packnet-sfm,open,,zxiaomzxm,,2021-10-13 08:03:11,,0.0,,183.0,1024923007.0,"Hi!
I have reimplement the reprojected distance loss in my own framework, 
but I found it hard to converge, because my initial pose is close to identity, thus in this scenario, the reprojected loss term is close to 0, right？Thus this scenario is a local minimal for this loss, it backpropagate to the pose directly, makes the photomatic loss diverged, and I can not get a good depth map in this setting.
Am I missing something? ",negative,negative
93,How to show evaluation value of every category?,open-mmlab/mmdetection,closed,,yuyijie1995,yuyijie1995,2020-01-01 06:22:39,2020-01-02 03:17:18,1.0,,1903.0,544317343.0,"Using the default mode,this evaluation value shows like this:
Average Precision  (AP) @[ IoU=0.50:0.95 \| area=   all \| maxDets=100 ] = 0.500
--
Average Precision  (AP) @[ IoU=0.50      \| area=   all \| maxDets=100 ] = 0.775
Average Precision  (AP) @[ IoU=0.75      \| area=   all \| maxDets=100 ] = 0.503
Average Precision  (AP) @[ IoU=0.50:0.95 \| area= small \| maxDets=100 ] = 0.312
Average Precision  (AP) @[ IoU=0.50:0.95 \| area=medium \| maxDets=100 ] = 0.410
Average Precision  (AP) @[ IoU=0.50:0.95 \| area= large \| maxDets=100 ] = 0.559
Average Recall     (AR) @[ IoU=0.50:0.95 \| area=   all \| maxDets=  1 ] = 0.516
Average Recall     (AR) @[ IoU=0.50:0.95 \| area=   all \| maxDets= 10 ] = 0.611
Average Recall     (AR) @[ IoU=0.50:0.95 \| area=   all \| maxDets=100 ] = 0.617
Average Recall     (AR) @[ IoU=0.50:0.95 \| area= small \| maxDets=100 ] = 0.428
Average Recall     (AR) @[ IoU=0.50:0.95 \| area=medium \| maxDets=100 ] = 0.562
Average Recall     (AR) @[ IoU=0.50:0.95 \| area= large \| maxDets=100 ] = 0.658
I want to get the every category's P and R . Is there a method to get the result except changing the code in coco_util.py ?
",positive,positive
94,RGB and optical flow image numbers do not match in some classes,feichtenhofer/twostreamfusion,closed,,Hongbo-Miao,Hongbo-Miao,2021-05-07 03:36:37,2021-05-09 22:54:58,1.0,,58.0,878399586.0,"RGB and optical flow image numbers do not match in some classes. The most matches.

After unzipping these two

- HMDB51 RGB: [part1](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/hmdb51_jpegs_256.zip)
- HMDB51 Flow: [part1](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/hmdb51_tvl1_flow.zip)

For example, `50_FIRST_DATES_kick_f_cm_np1_ba_med_19`, it has 1 more RGB image than optical flow image.

- `jpegs_256`
    - `50_FIRST_DATES_kick_f_cm_np1_ba_med_19` (48 images)
        - `frame000001.jpg`
        - `frame000002.jpg`
        - ...
    - ...
- `tvl1_flow`
    - `u`
        - `50_FIRST_DATES_kick_f_cm_np1_ba_med_19` (47 images)
            - `frame000001.jpg`
            - `frame000002.jpg`
            - ...
        - ...
    - `v`
        - `50_FIRST_DATES_kick_f_cm_np1_ba_med_19` (47 images)
            - `frame000001.jpg`
            - `frame000002.jpg`
            - ...
        - ...",neutral,negative
95,a runtime error after a few hundred iterations,bowenliu16/rl_graph_generation,closed,,bowenliu16,bowenliu16,2018-04-13 22:21:55,2018-04-20 23:42:06,1.0,,1.0,314266834.0,"Here is the error log

```
[10:{'node':` <gym.core.Space object at 0x7effce1cd6a0>, 'adj': <gym.core.Space object at 0x7effce1cd438>}

WARN: Could not seed environment <MoleculeEnv<molecule-v0>>

ob_adj (?, 3, ?, ?) ob_node (?, 1, ?, 10)

logits_first (?, ?) logits_second (?, ?) logits_edge (?, 3)

ac_edge (?,)

ob_adj (?, 3, ?, ?) ob_node (?, 1, ?, 10)

logits_first (?, ?) logits_second (?, ?) logits_edge (?, 3)

ac_edge (?,)

[10:54:02] Explicit valence for atom # 2 O, 3, is greater than permitted

[10:54:02] Explicit valence for atom # 4 N, 4, is greater than permitted

[10:54:02] Explicit valence for atom # 1 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 5 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 4 N, 4, is greater than permitted

[10:54:02] Explicit valence for atom # 1 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 8 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 9 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 22 O, 3, is greater than permitted

[10:54:02] Explicit valence for atom # 7 N, 4, is greater than permitted

[10:54:02] Explicit valence for atom # 1 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 20 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 2 O, 3, is greater than permitted

[10:54:02] Explicit valence for atom # 0 Br, 2, is greater than permitted

[10:54:02] Explicit valence for atom # 1 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 14 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 20 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 14 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 2 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 3 O, 3, is greater than permitted

[10:54:02] Explicit valence for atom # 8 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 8 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 8 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 11 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 19 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 2 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 24 O, 3, is greater than permitted

[10:54:02] Explicit valence for atom # 0 N, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 30 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 2 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 30 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 7 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 19 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 2 N, 4, is greater than permitted

[10:54:02] Explicit valence for atom # 6 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 5 N, 4, is greater than permitted

[10:54:02] Explicit valence for atom # 2 N, 4, is greater than permitted

[10:54:02] Explicit valence for atom # 7 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 16 O, 3, is greater than permitted

[10:54:02] Explicit valence for atom # 3 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 1 C, 6, is greater than permitted

[10:54:02] Explicit valence for atom # 13 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 27 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 22 C, 5, is greater than permitted

[10:54:02] Explicit valence for atom # 14 N, 5, is greater than permitted

/home/bowen/anaconda3/envs/rl_graph_generation_apr_11_2018/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.

  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""

Traceback (most recent call last):

  File ""run_molecule.py"", line 86, in main

    train(args,args.env, num_timesteps=args.num_timesteps, seed=args.seed,writer=writer)

  File ""run_molecule.py"", line 46, in train

    schedule='linear', writer=writer

  File ""/home/bowen/pycharm_deployment_directory/rl_graph_generation/rl-baselines/baselines/ppo1/pposgd_simple_gcn.py"", line 254, in learn

    seg = seg_gen.__next__()

  File ""/home/bowen/pycharm_deployment_directory/rl_graph_generation/rl-baselines/baselines/ppo1/pposgd_simple_gcn.py"", line 74, in traj_segment_generator

    info = env.get_info()

  File ""/home/bowen/pycharm_deployment_directory/rl_graph_generation/gym-molecule/gym_molecule/envs/molecule.py"", line 281, in get_info

    info['reward_sa'] = calculateScore(m) * self.sa_ratio  # lower better

  File ""/home/bowen/pycharm_deployment_directory/rl_graph_generation/gym-molecule/gym_molecule/envs/sascorer.py"", line 59, in calculateScore

    2)  #<- 2 is the *radius* of the circular fingerprint

Boost.Python.ArgumentError: Python argument types in

    rdkit.Chem.rdMolDescriptors.GetMorganFingerprint(NoneType, int)

did not match C++ signature:

    GetMorganFingerprint(RDKit::ROMol mol, int radius, boost::python::api::object invariants=[], boost::python::api::object fromAtoms=[], bool useChirality=False, bool useBondTypes=True, bool useFeatures=False, bool useCounts=True, boost::python::api::object bitInfo=None)



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File ""run_molecule.py"", line 93, in <module>

    main()

  File ""run_molecule.py"", line 88, in main

    writer.export_scalars_to_json(""./all_scalars.json"")

AttributeError: 'NoneType' object has no attribute 'export_scalars_to_json'
```",neutral,neutral
96,Installation error: use of overloaded operator '<<' is ambiguous on Mac ,deepmind/open_spiel,closed,,TheoCabannes,OpenSpiel,2021-05-04 15:07:02,2021-05-10 09:34:44,9.0,,577.0,875559624.0,"Hey,

I am trying to compile the code and I have the following error:
```In file included from /Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/game_parameters.h:23:
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:111:7: error: use of overloaded operator '<<' is ambiguous (with
      operand types 'std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >' and 'const nullptr_t')
  out << arg;
  ~~~ ^  ~~~
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, nullptr_t>' requested here
  SpielStrOut(out, std::forward<Args>(args)...);
  ^
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char [13], nullptr_t &>'
      requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >,
      open_spiel::algorithms::MDPNode *, char const (&)[13], nullptr_t &>' requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char [4],
      open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char [7], char const (&)[4],
      open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: (skipping 1 context in backtrace; use
      -ftemplate-backtrace-limit=0 to see all)
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char [2], char const (&)[17],
      char const (&)[7], char const (&)[4], open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, int, char const (&)[2], char
      const (&)[17], char const (&)[7], char const (&)[4], open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:117:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char [2], int, char const
      (&)[2], char const (&)[17], char const (&)[7], char const (&)[4], open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:131:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrOut<std::__1::basic_ostringstream<char, std::__1::char_traits<char>, std::__1::allocator<char> >, char [128], char const (&)[2],
      int, char const (&)[2], char const (&)[17], char const (&)[7], char const (&)[4], open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
  SpielStrOut(out, std::forward<Args>(args)...);
  ^
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/algorithms/tabular_best_response_mdp.cc:42:3: note: in instantiation of function template
      specialization 'open_spiel::internal::SpielStrCat<char const (&)[128], char const (&)[2], int, char const (&)[2], char const (&)[17], char const (&)[7], char const (&)[4],
      open_spiel::algorithms::MDPNode *&, char const (&)[13], nullptr_t &>' requested here
  SPIEL_CHECK_NE(child, nullptr);
  ^
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:252:30: note: expanded from macro 'SPIEL_CHECK_NE'
#define SPIEL_CHECK_NE(x, y) SPIEL_CHECK_OP(x, !=, y)
                             ^
/Users/theophile/Documents/research/Markov model/open_spiel_clean/open_spiel/open_spiel/../open_spiel/spiel_utils.h:220:57: note: expanded from macro 'SPIEL_CHECK_OP'
      open_spiel::SpielFatalError(open_spiel::internal::SpielStrCat( \
                                                        ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:194:20: note: candidate function
    basic_ostream& operator<<(basic_ostream& (*__pf)(basic_ostream&))
                   ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:198:20: note: candidate function
    basic_ostream& operator<<(basic_ios<char_type, traits_type>&
                   ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:203:20: note: candidate function
    basic_ostream& operator<<(ios_base& (*__pf)(ios_base&))
                   ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:218:20: note: candidate function
    basic_ostream& operator<<(const void* __p);
                   ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:219:20: note: candidate function
    basic_ostream& operator<<(basic_streambuf<char_type, traits_type>* __sb);
                   ^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:862:1: note: candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const char* __str)
^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:816:1: note: candidate function [with _CharT = char, _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<_CharT, _Traits>& __os, const char* __strn)
^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:869:1: note: candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const signed char* __str)
^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:877:1: note: candidate function [with _Traits = std::__1::char_traits<char>]
operator<<(basic_ostream<char, _Traits>& __os, const unsigned char* __str)
^
/Library/Developer/CommandLineTools/usr/bin/../include/c++/v1/ostream:206:20: note: candidate function
    basic_ostream& operator<<(bool __n);
                   ^
```

My computer is using MacOS BigSur 11.3. With cmake version 3.15.3 and Apple clang version 11.0.3 (clang-1103.0.32.59). I am running the code from a conda environment (with conda 4.8.3).
I get this error when downloading the git and running ```python setup.py install```. I also get this error when running ```CXX=/usr/bin/clang++ ./open_spiel/scripts/build_and_run_tests.sh --virtualenv=false``` after running ```sh install.sh``` (and the same with --vertualenv=true).

Thank you a lot for your help!
",neutral,positive
97,"tfa.image.gaussian_filter2d doesn't support tensor inputs for ""filter_shape"" and ""sigma"" args",tensorflow/addons,open,,charles92,,2021-12-03 01:47:38,,3.0,,2617.0,1070137284.0,"# Issue

The `tfa.image.gaussian_filter2d` function doesn't seem to support tensors for its `filter_shape` and `sigma` arguments. Instead it only accepts `int`s and `float`s respectively, severely limiting the use case.

https://github.com/tensorflow/addons/blob/8cec33fcaaf1cf90aec7bdd55a0fcdbb251ce5c2/tensorflow_addons/image/filters.py#L221-L222

In graph mode, when you pass a `tf.Tensor`-typed `sigma` into the function, it will emit the following error:

```
    File "".../tensorflow_addons/image/filters.py"", line 269, in gaussian_filter2d  *
        if any(s < 0 for s in sigma):

    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed:
      AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
```

# Request

Add support for tensor-typed `filter_shape` and `sigma` arguments. They are useful when the arguments can't be determined statically (e.g., when a random `sigma` is needed for data augmentation).

The bulk of the implementation seems to support this already. It may simply require rewriting some of the sanity checking code, like the one below:

https://github.com/tensorflow/addons/blob/8cec33fcaaf1cf90aec7bdd55a0fcdbb251ce5c2/tensorflow_addons/image/filters.py#L262-L263",neutral,negative
98,How to draw pr curve? Thanks,open-mmlab/mmdetection,closed,,guanbin1994,hhaAndroid,2018-10-30 04:01:22,2021-03-31 07:55:08,2.0,community help wanted,85.0,375310908.0,"I have finished test procedure and get a .pkl file.

If I want to draw a pr curve based on my  .pkl file, what should I do?
Thanks!",positive,positive
99,Question about training the generator,facebookresearch/low-shot-shrink-hallucinate,open,,uridabomb,,2018-08-24 10:23:18,,0.0,,7.0,353728215.0,"Hi, I'm trying to use the method on a completely different dataset. My implementation is in Keras and I built the generator model such that I can view the two losses (rms loss and cls loss) during the training phase. I trained for 10 epochs and I get suspicious results:

1. The classification loss/accuracy does not change at all (is it because I don't change the weights of the classifier?) and the accuracy is *very* small: `~0.0818` (the classifier achieved `~0.7` accuracy on a test set for trained categories).
2. The rms loss/accuracy improves - from `~0.6390` accuracy in the first epoch to `~0.7245` in the last epoch while I used `λ=1`.
3. The rms loss/accuracy converges to `~0.6286` in the second epoch (and does not change at all aftwards) while I used `λ=10` (as you did).

Is there something I should be worried about those results? 
Do you have any enlightenments?

Thanks!",neutral,negative
100,f score is -1,jiesutd/NCRFpp,closed,,udion,jiesutd,2018-06-06 01:13:10,2018-06-06 08:05:50,3.0,,22.0,329676563.0,"In the file `demo.train.config` I changed the iterations to 100 and batch_size to 32, dev and test scores are almost always -1. (Note this is on the sample_dataset that you have provided with the embeddings that you have provided) ",neutral,neutral
101,Code to reproduce NYU RGBD results / input pipeline,HobbitLong/CMC,open,,meyerjo,,2019-08-09 06:45:00,,4.0,,9.0,478824345.0,"Hi,
thanks for your repo.
It would be nice if you could provide the code / the input pipeline which you used to run the NYU RGB-D experiments as well (similar to #4 ). To me it is not entirely clear, how you added the different modalities.
Best,",neutral,positive
102,CRF decode saved model fails to load in TF2.5+,tensorflow/addons,closed,,seanpmorgan,bhack,2021-05-13 23:50:30,2021-09-28 19:33:03,7.0,help wanted#crf,2476.0,891466321.0,"**System information**
- TensorFlow version and how it was installed (source or binary): TF2.5+
- TensorFlow-Addons version and how it was installed (source or binary): TFA 0.13
- Python version: Any

**Describe the bug**
CRF Decode fails serialization in TF2.5+

Possibly related:
https://github.com/tensorflow/tensorflow/pull/45534

**Code to reproduce the issue**
Remove pytest skip in `test_crf_decode_save_load`

**Other info / logs**
```
=========================================================================================== FAILURES ============================================================================================
________________________________________________________________________________ test_crf_decode_save_load[cpu] _________________________________________________________________________________

tmpdir = local('/tmp/pytest-of-root/pytest-3/test_crf_decode_save_load_cpu_0')

    @pytest.mark.skipif(
        tf.__version__[:3] == ""2.4"",
        reason=""CRF Decode doesn't work in TF2.4, the issue was fixed in TF core, but didn't make the release"",
    )
    def test_crf_decode_save_load(tmpdir):
        tf.keras.backend.clear_session()
        input_tensor = tf.keras.Input(shape=(10, 3), dtype=tf.float32, name=""input_tensor"")
        seq_len = tf.keras.Input(shape=(), dtype=tf.int32, name=""seq_len"")
        transition = tf.constant([[1, 1, 0], [0, 1, 1], [1, 0, 1]], dtype=tf.float32)

        output = tf.multiply(input_tensor, tf.constant(1.0))
        decoded, _ = text.crf_decode(input_tensor, transition, seq_len)

        model = tf.keras.Model(
            inputs=[input_tensor, seq_len], outputs=[output, decoded], name=""example_model""
        )
        model.compile(optimizer=""Adam"")

        x_data = {
            ""input_tensor"": np.random.random_sample((5, 10, 3)).astype(dtype=np.float32),
            ""seq_len"": np.array([10] * 5, dtype=np.int32),
        }
        y_data = {""tf.math.multiply"": np.random.randint(0, 3, (5, 10))}

        model.fit(x_data, y_data)
        model.predict(
            {
                ""input_tensor"": tf.expand_dims(x_data[""input_tensor""][0], 0),
                ""seq_len"": np.array([10]),
            }
        )

        temp_dir = str(tmpdir.mkdir(""model""))
        tf.saved_model.save(model, temp_dir)

        tf.keras.backend.clear_session()
>       model = tf.keras.models.load_model(
            temp_dir,
            custom_objects={""CrfDecodeForwardRnnCell"": text.crf.CrfDecodeForwardRnnCell},
        )

tensorflow_addons/text/tests/crf_test.py:536:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py:206: in load_model
    return saved_model_load.load(filepath, compile, options)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py:155: in load
    keras_loader.finalize_objects()
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py:626: in finalize_objects
    self._reconstruct_all_models()
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py:645: in _reconstruct_all_models
    self._reconstruct_model(model_id, model, layers)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py:691: in _reconstruct_model
    created_layers) = functional_lib.reconstruct_from_config(
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:1289: in reconstruct_from_config
    process_node(layer, node_data)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:1237: in process_node
    output_tensors = layer(input_tensors, **kwargs)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:969: in __call__
    return self._functional_construction_call(inputs, args, kwargs,
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1107: in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:840: in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:880: in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py:1363: in _call_wrapper
    return self._call_wrapper(*args, **kwargs)
/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py:1395: in _call_wrapper
    result = self.function(*args, **kwargs)
/usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: in wrapper
    return target(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

fn = '_scan_fn', elems = <tf.Tensor 'Placeholder:0' shape=(9, None, 3) dtype=int32>, initializer = <tf.Tensor 'Placeholder_1:0' shape=(None, 1) dtype=int32>, parallel_iterations = 10
back_prop = True, swap_memory = False, infer_shape = True, reverse = False, name = None

    @tf_export(v1=[""scan""])
    @dispatch.add_dispatch_support
    def scan(fn,
             elems,
             initializer=None,
             parallel_iterations=10,
             back_prop=True,
             swap_memory=False,
             infer_shape=True,
             reverse=False,
             name=None):
      """"""scan on the list of tensors unpacked from `elems` on dimension 0.

      See also `tf.map_fn`.

      The simplest version of `scan` repeatedly applies the callable `fn` to a
      sequence of elements from first to last. The elements are made of the tensors
      unpacked from `elems` on dimension 0. The callable fn takes two tensors as
      arguments. The first argument is the accumulated value computed from the
      preceding invocation of fn, and the second is the value at the current
      position of `elems`. If `initializer` is None, `elems` must contain at least
      one element, and its first element is used as the initializer.

      Suppose that `elems` is unpacked into `values`, a list of tensors. The shape
      of the result tensor is `[len(values)] + fn(initializer, values[0]).shape`.
      If reverse=True, it's fn(initializer, values[-1]).shape.

      This method also allows multi-arity `elems` and accumulator.  If `elems`
      is a (possibly nested) list or tuple of tensors, then each of these tensors
      must have a matching first (unpack) dimension.  The second argument of
      `fn` must match the structure of `elems`.

      If no `initializer` is provided, the output structure and dtypes of `fn`
      are assumed to be the same as its input; and in this case, the first
      argument of `fn` must match the structure of `elems`.

      If an `initializer` is provided, then the output of `fn` must have the same
      structure as `initializer`; and the first argument of `fn` must match
      this structure.

      For example, if `elems` is `(t1, [t2, t3])` and `initializer` is
      `[i1, i2]` then an appropriate signature for `fn` in `python2` is:
      `fn = lambda (acc_p1, acc_p2), (t1, [t2, t3]):` and `fn` must return a list,
      `[acc_n1, acc_n2]`.  An alternative correct signature for `fn`, and the
       one that works in `python3`, is:
      `fn = lambda a, t:`, where `a` and `t` correspond to the input tuples.

      Args:
        fn: The callable to be performed.  It accepts two arguments.  The first will
          have the same structure as `initializer` if one is provided, otherwise it
          will have the same structure as `elems`.  The second will have the same
          (possibly nested) structure as `elems`.  Its output must have the same
          structure as `initializer` if one is provided, otherwise it must have the
          same structure as `elems`.
        elems: A tensor or (possibly nested) sequence of tensors, each of which will
          be unpacked along their first dimension.  The nested sequence of the
          resulting slices will be the first argument to `fn`.
        initializer: (optional) A tensor or (possibly nested) sequence of tensors,
          initial value for the accumulator, and the expected output type of `fn`.
        parallel_iterations: (optional) The number of iterations allowed to run in
          parallel.
        back_prop: (optional) True enables support for back propagation.
        swap_memory: (optional) True enables GPU-CPU memory swapping.
        infer_shape: (optional) False disables tests for consistent output shapes.
        reverse: (optional) True scans the tensor last to first (instead of first to
          last).
        name: (optional) Name prefix for the returned tensors.

      Returns:
        A tensor or (possibly nested) sequence of tensors.  Each tensor packs the
        results of applying `fn` to tensors unpacked from `elems` along the first
        dimension, and the previous accumulator value(s), from first to last (or
        last to first, if `reverse=True`).

      Raises:
        TypeError: if `fn` is not callable or the structure of the output of
          `fn` and `initializer` do not match.
        ValueError: if the lengths of the output of `fn` and `initializer`
          do not match.

      Examples:
        ```python
        elems = np.array([1, 2, 3, 4, 5, 6])
        sum = scan(lambda a, x: a + x, elems)
        # sum == [1, 3, 6, 10, 15, 21]
        sum = scan(lambda a, x: a + x, elems, reverse=True)
        # sum == [21, 20, 18, 15, 11, 6]
        ```

        ```python
        elems = np.array([1, 2, 3, 4, 5, 6])
        initializer = np.array(0)
        sum_one = scan(
            lambda a, x: x[0] - x[1] + a, (elems + 1, elems), initializer)
        # sum_one == [1, 2, 3, 4, 5, 6]
        ```

        ```python
        elems = np.array([1, 0, 0, 0, 0, 0])
        initializer = (np.array(0), np.array(1))
        fibonaccis = scan(lambda a, _: (a[1], a[0] + a[1]), elems, initializer)
        # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])
        ```
      """"""
      if not callable(fn):
>       raise TypeError(""fn must be callable."")
E       TypeError: fn must be callable.

/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py:544: TypeError
```",positive,neutral
103,TypeError: string indices must be integers,matterport/Mask_RCNN,open,,AidaSilva,,2021-07-06 19:03:57,,0.0,,2624.0,938178401.0,"<__main__.CustomDataset object at 0x000001F4241B0208>
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-8ee06627f1ef> in <module>
     37             ""mrcnn_bbox"", ""mrcnn_mask""])
     38 
---> 39 train(model)

<ipython-input-4-8ee06627f1ef> in train(model)
      4     dataset_train = CustomDataset()
      5     print(dataset_train)
----> 6     dataset_train.load_custom(""C:/Users/Owner/Mask_RCNN_foun/dataset"", ""train"")
      7     dataset_train.prepare()
      8 

<ipython-input-3-3b9c51fede60> in load_custom(self, dataset_dir, subset)
     43             # the outline of each object instance. There are stores in the
     44             # shape_attributes (see json format above)
---> 45             polygons = [r['shape_attributes'] for r in a['regions']]
     46             objects = [s['region_attributes']['name'] for s in a['regions']]
     47             print(""objects:"",objects)

<ipython-input-3-3b9c51fede60> in <listcomp>(.0)
     43             # the outline of each object instance. There are stores in the
     44             # shape_attributes (see json format above)
---> 45             polygons = [r['shape_attributes'] for r in a['regions']]
     46             objects = [s['region_attributes']['name'] for s in a['regions']]
     47             print(""objects:"",objects)",neutral,neutral
104,No shuffle layer when training condensenet?,ShichenLiu/CondenseNet,closed,,hiyijian,ShichenLiu,2017-12-18 10:22:40,2017-12-19 17:52:09,4.0,,7.0,282828674.0,"Dear @ShichenLiu ,
  I did not found any shuffle layer related stuff in models.condensenet, which use layers.LearnedGroupConv as LGC. However, the paper says we should use it clearly. Is it a mismatch? 
  Thanks",neutral,neutral
105,ImageSize,DmitryUlyanov/texture_nets,closed,,DavieHR,DmitryUlyanov,2016-05-22 02:24:52,2016-08-12 10:39:20,2.0,,9.0,156131236.0,"Hi, I found that your stylization_process couldn't adaptive to different resolution,just using the resolution 800*800,etc. I fixed it. 
",neutral,neutral
106,"Hello, I want to ask where is the supplementary information mentioned in the paper？",lmb-freiburg/hand3d,closed,,qqq1mmm,zimmerm,2019-03-05 08:07:19,2019-03-23 06:35:43,1.0,,23.0,417167307.0,,neutral,neutral
107,list of object detection and instance segmentation model list,open-mmlab/mmdetection,closed,,Suzan009,RangiLyu,2022-01-18 17:52:25,2022-01-27 08:04:43,1.0,enhancement#Doc,7030.0,1107198784.0,"The list here https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo, how would I know which one is for object detection and which one is for instance segmentation? ",neutral,neutral
108,"RuntimeError: expand(torch.cuda.FloatTensor{[256, 100, 100]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (3)",open-mmlab/mmdetection,closed,,Youggie,hellock,2019-12-18 09:28:49,2019-12-18 13:28:49,1.0,,1831.0,539558696.0,"When you multiply two tensors, you get this error. How do you solve it?Does anyone have any good advice?Thank you very much",neutral,neutral
109,"Please, I'm trying to make an API, I want to put the loss value in a variable and send it to the client at each step",matterport/Mask_RCNN,open,,izamkarsp,,2020-08-23 02:06:27,,0.0,,2333.0,684099693.0,"Please, I'm trying to make an API, I want to put the loss value in a variable and send it to the client at each step",positive,positive
110,How to train model for multiple classes in Colab?,matterport/Mask_RCNN,closed,,Ruchikamodgil,Ruchikamodgil,2021-02-13 23:12:41,2021-02-24 20:40:28,0.0,,2481.0,807848159.0,,neutral,neutral
111,Incorrect Predictions,reactive-systems/deepltl,closed,,watakandai,realChrisHahn2,2022-09-28 17:29:12,2022-09-30 19:07:56,2.0,,2.0,1389715438.0,"Hi,

I tried running the code you provided and followed the procedures in Readme. 
```bash
python -m deepltl.train.train_transformer --problem='ltl' --ds-name='ltl-35' --epochs=5
python -m deepltl.train.train_transformer --problem='ltl' --ds-name='ltl-50-test' --test
```

These are the results I got during the evaluation phase.
Solutions seemed to be mostly incorrect. Do you have an explanation for this? 
Do you have any idea what went wrong?

```
INCORRECT 1
input : Xb & Xe U d & !X(Xc & Xd & c U !d & (b & X((d & Xe) U (e & !Xd & Xa U !c))) U !(!c U c))
output: d; b; cycle{1}
target: d; b & !c & d; cycle{1}

INCORRECT 4
input : (X(Xb & !XXa U !(a & !e & !Xb U !a)) U !(b U c)) U (!b & X(d U !(1 U !X!a)))
output: !b; !b; cycle{!b}
target: !b; 1; cycle{!a}

INCORRECT 5
input : !X((!c U Xc) U 1 U !(c U !d)) & !(!c U !(!b & d)) U X(Xa U (d & a U b))
output: 1; b & !d; cycle{!d}
target: 1; b & c & d; cycle{!d}

INCORRECT 6
input : !((b & e U c) U 1 U a) & a U (!(b & e & XXc) & !(!X(!a & b) U (X!d & c U b)))
output: !a & !b & !c | !a & !c & !e; cycle{!a}
target: !a & !b & c; !a & !b & c & !d; cycle{!a & !b & c}

INCORRECT 8
input : !X(d & X(X(!a & Xe) U !X(!e U 1 U !e))) & X(!Xc & X!c & Xd U (Xe & !XXXe U !a))
output: 1; !d; !c & e; !c; cycle{1}
target: 1; !a & !d; !c & e; cycle{1}

INCORRECT 9
input : X((X!XX!(!b & c) U XX(d U c U !e)) U !X(!X!(a & e) & (Xc & Xd) U !e) U XXX(Xa U d))
output: 1; c; cycle{1}
target: 1; 1; 1; 1; d; cycle{1}

INCORRECT 10
input : X(X(!c & X!(a & e)) & !X(!(!a & !Xc & !(a & X!a) U X(a & e & Xa)) U d))
output: 1; 1; !a & !c; !a & !c | !c & !e; cycle{1}
target: 1; 1; !c & !d; !a & !d | !d & !e; cycle{!d}

INCORRECT 11
input : !(Xa & XX((!Xc & Xa) U X!b U X!(Xc U !d))) U X(a & b & c) U X(1 U XX(d U e))
output: 1; 1; 1; 1; !c; cycle{1}
target: 1; 1; 1; e; cycle{1}
```",neutral,negative
112,About self.ind,XudongLinthu/context-gated-convolution,closed,,MPhone1000,XudongLinthu,2020-11-24 08:30:09,2020-12-02 22:17:28,2.0,,1.0,749492460.0,"At line 19 in `context-gated-convolution/image-classification/models/layers.py`,
```
if kernel_size == 1 or True:
    self.ind = True
```
`self.ind` is always True and the layer works the same as traditional convolution.

Is `or True` redundant?",neutral,negative
113,no code for DeepGCN+FLAG on ogbg-molpcba and ogbg-code,devnkong/FLAG,closed,,lipingcoding,lipingcoding,2020-11-20 06:54:30,2020-11-20 07:58:32,2.0,,2.0,747227347.0,no code for DeepGCN+FLAG on ogbg-molpcba and ogbg-code,neutral,neutral
114,How to get gt_bboxes and gt_labels annotations during testing?,open-mmlab/mmdetection,closed,ZwwWayne,ghost,hhaAndroid,2020-08-30 03:47:01,2021-04-09 02:36:19,3.0,enhancement,3653.0,688646986.0,"This is useful in some ablation studies. For example, to verify the upper bound performance of bbox regression branch, we can replace the anchors with `gt_bboxes`.

Since in testing mode the detector doesn't receive `gt_bboxes` and `gt_labels`, I tried to add `return_loss=True` in <https://github.com/open-mmlab/mmdetection/blob/f93c00fd05b50cd3903c964599efb7eb9fd354ea/mmdet/apis/test.py#L26> and input the annotations in the same line, but the detector with `training mode` only returns losses and doesn't return detection results as I expected. I have to modify the detector to output detection results in `training mode`.

Is there cheaper way of feeding annotations to the detector during testing?

",neutral,negative
115,Failed to compile  sigmoid_focal_loss_cuda.cu   Under   win10,open-mmlab/mmdetection,closed,,nothingws,ZwwWayne,2020-06-23 08:40:41,2021-01-24 15:43:50,1.0,windows,3111.0,643646295.0,"Failed to compile               # make_cuda_ext(
            #     name='sigmoid_focal_loss_ext',
            #     module='mmdet.ops.sigmoid_focal_loss',
            #     sources=['src/sigmoid_focal_loss_ext.cpp'],
            #     sources_cuda=['src/cuda/sigmoid_focal_loss_cuda.cu']),

vs2017 cuda10.1 pytorch1.3",neutral,neutral
116,how to test ?,MIT-HAN-LAB/temporal-shift-module,closed,,ucasiggcas,tonylins,2019-12-20 09:41:13,2019-12-26 00:26:31,5.0,,42.0,540894284.0,"hi,dear
Do not want test on the txt file's images
Just want to test other images or videos ,so how to ?
pls supply the file
### kinetics/labels/val_videofolder.txt
or tell me what's on it ?

thx",neutral,neutral
117,can't use CBN to train a resnet model from scratch？ask for help,Howal/Cross-iterationBatchNorm,open,,nkyle04,,2022-04-15 03:01:13,,0.0,,8.0,1205201698.0,"I was using a resent18 model from torchvision，and try to replace all bn by CBN. But I can't train it at all, the accuracy is always very little, such as 0.01%

I also use an pre trained resnet18 model that has been trained for 40 epochs, and continue train with CBN replaced for some epochs, and the accuracy is also dropped immediately.
 
Is there any details I had missed？Please give me some help, Thanks.",neutral,positive
118,How to make transfer learned model only predict my own labels,open-mmlab/mmdetection,closed,,Magsun,hellock,2020-01-15 13:45:38,2020-02-02 05:24:37,5.0,,1987.0,550195631.0,"您好，我在试着用mmdetection框架中的一个预训练模型来迁移学习自己的类别，在训练的时候我修改了训练的配置，能够正常训练，但是在预测的时候偶尔模型会预测出不在训练类别中的标签。经过查看coco数据集的类别，推测可能是模型输出了coco预训练过的类别。
请问在哪里修改配置让模型只输出预训练过的类别？
非常感谢！",neutral,neutral
119,HD-Map Filter,nutonomy/nuscenes-devkit,closed,,Ckerrr,holger-motional,2019-11-26 17:37:26,2019-11-27 08:22:24,2.0,,260.0,528875427.0,"Does nuscenes-devkit have the API to apply ""HD-map"" filter? Eg. filter the objects or pointcloud inside the drivable area. 

If not, does the map possibly suppot the functionality in nature? Thanks!",neutral,positive
120,FBetaScore is missing sample_weight parameter in update_state function,tensorflow/addons,closed,,PhilipMay,Squadrick,2019-08-06 11:22:05,2019-08-30 13:07:02,3.0,metrics,396.0,477322712.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see https://gist.github.com/PhilipMay/8897cd3980b4367f22cee7a0f8cb2ee5#file-poc_f1_bug-ipynb
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- TensorFlow installed from (source or binary): binary (pypi with pip)
- TensorFlow version (use command below): tensorflow  2.0.0b1
- TensorFlow Addons installed from (source, PyPi): pypi
- TensorFlow Addons version: tfa-nightly 0.5.0.dev20190804
- Python version and type (eg. Anaconda Python, Stock Python as in Mac, or homebrew installed Python etc): conda python 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- Is GPU used? (yes/no): no
- GPU model (if used): -

**Describe the bug**
Using `F1Score`from metrics with tf.keras does not work. See exception here:
https://gist.github.com/PhilipMay/8897cd3980b4367f22cee7a0f8cb2ee5#file-poc_f1_bug-ipynb

**Describe the expected behavior**
Should calculate the f1 metric

**Code to reproduce the issue**
see: https://gist.github.com/PhilipMay/8897cd3980b4367f22cee7a0f8cb2ee5#file-poc_f1_bug-ipynb

**Solution**
FBetaScore is missing sample_weight parameter in update_state function:
https://github.com/tensorflow/addons/blob/dab18121fa4253cf05320d41962bfbf1803e1f1b/tensorflow_addons/metrics/f_scores.py#L178

This would be a workaround for the problem:
`def update_state(self, y_true, y_pred, sample_weight=None):`

Although this way sample_weight is not considered in metric calculation.

Please note that all `update_state`functions from `tf.keras.metrics` do contain the `sample_weight` parameter: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/metrics.py

If you want me to I can prepare a PR.

Same problem should be here:
- https://github.com/tensorflow/addons/blob/master/tensorflow_addons/metrics/multilabel_confusion_matrix.py#L100
- https://github.com/tensorflow/addons/blob/master/tensorflow_addons/metrics/r_square.py#L54

**Other info / logs**
```
$ pip list
Package              Version             
-------------------- --------------------
absl-py              0.7.1               
appnope              0.1.0               
astor                0.8.0               
attrs                19.1.0              
backcall             0.1.0               
bleach               3.1.0               
certifi              2019.6.16           
cycler               0.10.0              
decorator            4.4.0               
defusedxml           0.6.0               
entrypoints          0.3                 
future               0.17.1              
gast                 0.2.2               
google-pasta         0.1.7               
grpcio               1.22.0              
h5py                 2.9.0               
hyperopt             0.1.2               
imbalanced-learn     0.5.0               
imblearn             0.0                 
ipykernel            5.1.1               
ipython              7.7.0               
ipython-genutils     0.2.0               
ipywidgets           7.5.1               
jedi                 0.14.1              
Jinja2               2.10.1              
joblib               0.13.2              
jsonschema           3.0.2               
jupyter              1.0.0               
jupyter-client       5.3.1               
jupyter-console      6.0.0               
jupyter-core         4.5.0               
Keras                2.2.4               
Keras-Applications   1.0.8               
keras-metrics        1.1.0               
Keras-Preprocessing  1.1.0               
kiwisolver           1.1.0               
lightgbm             2.2.3               
Markdown             3.1.1               
MarkupSafe           1.1.1               
matplotlib           3.1.1               
mistune              0.8.4               
nbconvert            5.5.0               
nbformat             4.4.0               
networkx             2.3                 
notebook             6.0.0               
numpy                1.17.0              
pandas               0.25.0              
pandocfilters        1.4.2               
parso                0.5.1               
pexpect              4.7.0               
pickleshare          0.7.5               
pip                  19.1.1              
prometheus-client    0.7.1               
prompt-toolkit       2.0.9               
protobuf             3.9.0               
ptyprocess           0.6.0               
Pygments             2.4.2               
pymongo              3.8.0               
pyparsing            2.4.2               
pyrsistent           0.15.4              
python-dateutil      2.8.0               
pytz                 2019.2              
PyYAML               5.1.2               
pyzmq                18.0.2              
qtconsole            4.5.2               
scikit-learn         0.21.3              
scipy                1.3.0               
Send2Trash           1.5.0               
setuptools           41.0.1              
six                  1.12.0              
tb-nightly           1.14.0a20190603     
tensorflow           2.0.0b1             
termcolor            1.1.0               
terminado            0.8.2               
testpath             0.4.2               
tf-estimator-nightly 1.14.0.dev2019060501
tfa-nightly          0.5.0.dev20190804   
tornado              6.0.3               
tqdm                 4.32.2              
traitlets            4.3.2               
wcwidth              0.1.7               
webencodings         0.5.1               
Werkzeug             0.15.5              
wheel                0.33.4              
widgetsnbextension   3.5.1               
wrapt                1.11.2              
xgboost              0.90    
```",neutral,negative
121,meta_vid.mat,feichtenhofer/detect-track,open,,xjtuwh,,2018-10-11 00:27:42,,2.0,,35.0,368905361.0,Hello~ How can I get teh meta_vid.mat which used by imdb_from_ilsvrc15vid.m?,neutral,neutral
122,Batch inference for inference_detector,open-mmlab/mmdetection,closed,,Thevakumar-Luheerathan,Thevakumar-Luheerathan,2022-01-31 02:34:07,2022-01-31 10:59:24,1.0,,7106.0,1118884597.0,"Does the mmdetection supports batch inference for inference_detector? If how can I do that. I could not able to find it in documentation.
Batch processing is a must in my case. ",neutral,neutral
123,Problem while loading yolox-tiny model parameters,open-mmlab/mmdetection,closed,BIGWangYuDong,reclusezz,reclusezz,2021-09-24 06:02:14,2021-09-26 07:47:54,8.0,reimplementation,6156.0,1006111442.0,"The model and loaded state dict do not match exactly

unexpected key in source state_dict: ema_backbone_stem_conv_conv_weight, ema_backbone_stem_conv_bn_weight, ema_backbone_stem_conv_bn_bias, ema_backbone_stem_conv_bn_running_mean, ema_backbone_stem_conv_bn_running_var, ema_backbone_stem_conv_bn_num_batches_tracked, ema_backbone_stage1_0_conv_weight, ema_backbone_stage1_0_bn_weight, ema_backbone_stage1_0_bn_bias, ema_backbone_stage1_0_bn_running_mean, ema_backbone_stage1_0_bn_running_var, ema_backbone_stage1_0_bn_num_batches_tracked, ema_backbone_stage1_1_main_conv_conv_weight, ema_backbone_stage1_1_main_conv_bn_weight, ema_backbone_stage1_1_main_conv_bn_bias, ema_backbone_stage1_1_main_conv_bn_running_mean, ema_backbone_stage1_1_main_conv_bn_running_var, ema_backbone_stage1_1_main_conv_bn_num_batches_tracked, ema_backbone_stage1_1_short_conv_conv_weight, ema_backbone_stage1_1_short_conv_bn_weight, ema_backbone_stage1_1_short_conv_bn_bias, ema_backbone_stage1_1_short_conv_bn_running_mean, ema_backbone_stage1_1_short_conv_bn_running_var, ... ...






When I load trained yolox-tiny model during testing phase, I meet the problem. How to solve it and do it influence the result?",neutral,neutral
124,Visualize augmented images and masks,matterport/Mask_RCNN,closed,,jatinrastogi,jatinrastogi,2021-07-12 14:31:42,2021-07-14 02:40:26,3.0,,2631.0,942127427.0,"So I was trying to save the augmented images and masks on them but I ran through some error when I tried to save masks, As mask gives a boolean array so I tried to apply it on the image like this 
![Screenshot from 2021-07-12 20-03-57](https://user-images.githubusercontent.com/67838197/125306118-8eef5000-e34c-11eb-9748-c8d6baaa2925.png)


But then I face type error
`TypeError:can't multiply sequence of non-int type float`
Then I replaced 0.5*color[c]*255 with color[c]*255
But then I faced this error
`operands could not be broadcast together shapes (1024,1024) (765,)`",neutral,negative
125,Custom training dataset format,TRI-ML/packnet-sfm,open,,UditSinghParihar,,2021-11-02 08:50:54,,1.0,,186.0,1042034673.0,"Hello Sir,
   Thanks for providing the code for packnet-sfm.

I am currently trying to train on custom Carla dataset. My yaml configuration file looks like:

```
checkpoint:
    filepath: '/workspace/packnet-sfm/results/checkpoints'
    save_top_k: 5

save:
    folder: '/workspace/packnet-sfm/results'

model:
    name: 'SelfSupModel'
    optimizer:
        name: 'Adam'
        depth:
            lr: 0.0002
        pose:
            lr: 0.0002
    scheduler:
        name: 'StepLR'
        step_size: 30
        gamma: 0.5
    depth_net:
        name: 'PackNet01'
        version: '1A'
    pose_net:
        name: 'PoseNet'
        version: ''
    params:
        crop: 'garg'
        min_depth: 0.0
        max_depth: 80.0

datasets:
    augmentation:
        image_shape: (256, 320)
    train:
        batch_size: 2
        dataset: ['Image']
        path: ['/data/datasets/carla/Town01_short/carla_test/train']
        split: ['{:04}']
        repeat: [1]
    validation:
        dataset: ['Image']
        path: ['/data/datasets/carla/Town01_short/carla_test/val']
        split: ['{:04}']  
    test:
        dataset: ['Image']
        path: ['/data/datasets/carla/Town01_short/carla_test/val']
        split: ['{:04}']
```

My directory structure looks like:
```
.
├── data_splits
├── train
└── val
```

However during training, my training-loss is non-zero, while my validation-loss is zero, I suspect [validation-loss](https://github.com/TRI-ML/packnet-sfm/blob/master/packnet_sfm/trainers/horovod_trainer.py#L65) being zero might be due to wrong data loading, could you help me help out what is the correct directory structure for using [image_dataset.py](https://github.com/TRI-ML/packnet-sfm/blob/master/packnet_sfm/datasets/image_dataset.py) and what should be `path` and `split` field in the config file?
My issue is similar to [this issue](https://github.com/TRI-ML/packnet-sfm/issues/96#issuecomment-762436108) 
",neutral,negative
126,bbox_mAP_s: -1.0000,open-mmlab/mmdetection,closed,jbwang1997,champagne-yellow,jbwang1997,2022-06-01 07:20:44,2022-06-06 03:08:11,1.0,,8106.0,1255204542.0,"On my own dataset, the bbox_mAP_s is -1.0000 regardless of the method.",neutral,neutral
127,bitflips always has value -1,tancik/StegaStamp,closed,,Jackqu,tancik,2019-04-22 11:05:05,2019-04-22 17:55:46,1.0,,2.0,435690236.0,"When I run the code to my own video
python detector.py \
  --detector_model detector_models/stegastamp_detector \
  --decoder_model saved_models/stegastamp_pretrained \
  --video test_vid.mp4
I found bitflips always has value -1
The following is what I did：
step 1: Hide message into image lena and verified the message can be decoded
step 2: Print the image with message inside
step 3: Record a video of the printed image
And then, I found the bitflips always has value -1, could you help me with this problem. Thank you.
The image and video can be find here.[https://drive.google.com/open?id=1XGXWCN78kDvr6IEtTyIQKB6JJ9jXvTok]",neutral,positive
128,The AP value of the coco testing set is -1.000.,open-mmlab/mmdetection,closed,hhaAndroid,rsj007,hhaAndroid,2021-01-12 12:23:29,2021-04-09 02:01:55,4.0,reimplementation,4435.0,784189552.0,"#I am reimplementing cascade_rcnn_fpn_1x_coco.py in the coco dataset using the provided configs. After training, I want to test the model in the coco test dataset. I change the ann file and test dataset path in the test dict of the config. However the APs are all -1.000. I don't know what's the problem.
The result is below.
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = -1.000
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
{'bbox_mAP': -1.0, 'bbox_mAP_50': -1.0, 'bbox_mAP_75': -1.0, 'bbox_mAP_s': -1.0, 'bbox_mAP_m': -1.0, 'bbox_mAP_l': -1.0, 'bbox_mAP_copypaste': '-1.000 -1.000 -1.000 -1.000 -1.000 -1.000'}

The command I used for test is ""`CUDA_VISIBLE_DEVICES=5,6,7,8 bash mmdetection/tools/dist_test.sh mmdetection/work_dirs/cascade_rcnn_r50_fpn_1x_coco/cascade_rcnn_r50_fpn_1x_coco.py mmdetection/work_dirs/cascade_rcnn_r50_fpn_1x_coco/epoch_1.pth 4 --eval bbox`"".

My config is below.
```
model = dict(
    type='CascadeRCNN',
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]))
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.7,
                min_pos_iou=0.7,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ])
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.5),
        max_per_img=100))
dataset_type = 'CocoDataset'
data_root = '/data/sj_data/data/coco'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file='/data/sj_data/data/coco/annotations/instances_train2017.json',
        img_prefix='/data/sj_data/data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='/data/sj_data/data/coco/annotations/instances_val2017.json',
        img_prefix='/data/sj_data/data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='/data/sj_data/data/coco/annotations/image_info_test-dev2017.json',
        img_prefix='/data/sj_data/data/coco/test2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric='bbox')
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
total_epochs = 12
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
work_dir = './work_dirs/cascade_rcnn_r50_fpn_1x_coco'
gpu_ids = range(0, 1)

```


My environment is below.
`Python: 3.7.9 (default, Aug 31 2020, 12:42:55) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7,8,9: GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda-10.0
NVCC: Cuda compilation tools, release 10.0, V10.0.130
GCC: gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.4.0
MMCV: 1.2.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.6.0+`
",neutral,positive
129,Training on new data,jiesutd/NCRFpp,closed,,junaidamk,jiesutd,2019-04-27 13:13:16,2019-04-28 01:54:57,3.0,,110.0,437942387.0,https://github.com/jiesutd/NCRFpp/issues/104#issuecomment-486535264,neutral,neutral
130,训练一段时间后，f1为-1,jiesutd/NCRFpp,closed,,Xzeffort,Xzeffort,2021-11-04 07:45:37,2021-11-05 01:55:18,2.0,,181.0,1044442088.0,,neutral,neutral
131,Faster RCNN-std bettwen rpn and bbox,open-mmlab/mmdetection,closed,,abcxs,abcxs,2020-08-07 05:08:23,2020-08-14 07:03:34,2.0,,3508.0,674761272.0,"In Faster RCNN, the STD used by RPN head is [1, 1, 1, 1], while that used by bbox head is [0.1,0.1,0.2,0.2]
1. Why are the settings of RPN and Bbox head different?
2. Why does the STD of Bbox head need to be set like [0.1, 0.1, 0.2, 0.2]",neutral,neutral
132,CRF parameters used in the ablation study of SegFix paper,openseg-group/openseg.pytorch,closed,,jfzhuang,PkuRainBow,2021-02-26 09:26:00,2021-02-27 06:36:07,1.0,,53.0,817189437.0,"Thanks for your excellent work. I want to reproduce your experimental results in the comparison with DenseCRF in the SegFix paper. However, I can not find an appropriate group of parameters. It will be very helpful if you can provide the parameters used in your experiments. Thanks a lot!",neutral,positive
133,Question about flow-based propagation.,Microsoft/human-pose-estimation.pytorch,open,,ybai62868,,2019-02-13 06:00:06,,0.0,,88.0,409641031.0,"Hi, I recently try to re-implement the flow-based propagation section in your paper.
I use the FlowNet2S you mentioned, and I get the flow information between two adjacent frames on posetrack2018 val dataset. The question is the inference size must be divisible by 64. Therefore, the shape of flow is not corresponding to the shape of images. In order to get the same results reported, I wonder how to solve this problem and the inference time for ""With Joint Propagation"" on posetrack2018 val dataset.
Thanks a lot!",neutral,positive
134,Can anybody know how to deploy MTCNN by TVM Framework?,deepinsight/insightface,open,,MirrorYuAI,,2019-04-20 08:34:24,,0.0,,639.0,435371381.0,"I want to deploy MTCNN based on TVM, Can anyone give me some advices? thx a lot~",neutral,neutral
135,Not able to use linux redirection for interactive mode,percyliang/sempre,closed,,chunyangx,yonatansito,2014-05-19 20:05:03,2014-06-21 15:05:21,1.0,,22.0,33831784.0,"Hello, 
I execute the following command:

./sempre @mode=interact          @domain=webquestions          @sparqlserver=localhost:8890/sparql          @cacheserver=local @load=15 @executeTopOnly=0 < Lundi_test.txt 

However, inside Master class, the runInteractivePrompt() method will always read the line as null, I put the details in:

http://stackoverflow.com/questions/23742284/java-readline-null

Do you know where is the problem, please?
",neutral,positive
136,Potential bug of detection target ,matterport/Mask_RCNN,open,,zb1439,,2019-07-09 12:35:57,,0.0,,1622.0,465756065.0,"In model.py line 563-571
```python
# Assign positive ROIs to GT boxes.
    positive_overlaps = tf.gather(overlaps, positive_indices)
    roi_gt_box_assignment = tf.cond(
        tf.greater(tf.shape(positive_overlaps)[1], 0),
        true_fn = lambda: tf.argmax(positive_overlaps, axis=1),
        false_fn = lambda: tf.cast(tf.constant([]),tf.int64)
    )
    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)
    roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)
```
Tensor positive_overlaps could have a shape of [positive_count, MAX_GT_INSTANCES], which makes the tf condition always be true. This might be subtle as RPN might hardly produce proposals matching no ground truth.",neutral,positive
137,How to get more accurate results in satellite imagery ??,matterport/Mask_RCNN,open,,geomaticsbetul,,2021-04-02 17:42:05,,0.0,,2524.0,849358197.0,"Hello everyone! I'm trying to classify satellite images. I have 5 classes in total. My mAP value is like 0.5. My loss value is not going below 0.7.
Do you have any suggestions to better them and get more accurate results?

please help 
thanks. ",neutral,positive
138,IPM corner points,SeokjuLee/VPGNet,open,,guozixunnicolas,,2019-04-05 06:56:02,,0.0,,40.0,429609386.0,"Hi seokju,

I have similar problem like the picture below
![visual](https://user-images.githubusercontent.com/44756093/55608964-277fe880-57b2-11e9-8d2e-57d476e775ff.png)
Even though I tried SCNN but I believe the postprocessing step is similar.
below is my result
![hw_curve55_result](https://user-images.githubusercontent.com/44756093/55609048-5ac27780-57b2-11e9-8c98-c0af16421d71.jpg)
the model confuse itself to see points from different lanes. Thus I want to cluster all points again using IPM to seperate the points near VP.

May i know which 4 src points did you use in cv2.getperspective() function. Since the prediction of mine has a large variety, I have some trouble finding these 4 points which encloses all points found.
Could you share some insight how you find these 4 points ?

Best regards,

ZX",neutral,positive
139,Finished processing dependencies for mmdet==0.6rc0+unknown,open-mmlab/mmdetection,closed,,Hesene,hellock,2019-03-07 06:16:30,2019-03-24 02:27:47,1.0,,369.0,418146832.0,"Thank you for your sharing.
I meet some problem,Why is my mmdet version is =0.6rc0+unknown，what is the ''unknow''means?
and when I run the test.py, it will appear
 ‘’ImportError: /home/vvv/jh/mmdetectionV1/mmdet/ops/dcn/deform_conv_cuda.cpython-36m-x86_64-linux-gnu.so: undefined symbol: __cudaPopCallConfiguration ‘’ 
My environment is 
ubuntu18.04, cuda10, cudnn7.4.1, torch1.0 
Thank you for your answer！！！
",neutral,positive
140,Help running retina code,deepinsight/insightface,open,,ThanhPham1987,,2019-11-23 16:03:43,,1.0,,994.0,527574785.0,"Hi authors,
When I run this code by command line: CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --prefix ./model/retina --network resnet. I am facing the issue like this:
Traceback (most recent call last):
  File ""train.py"", line 367, in <module>
    main()
  File ""train.py"", line 350, in main
    args = parse_args()
  File ""train.py"", line 344, in parse_args
    parser.add_argument('--wd', help='weight decay', default=default.wd, type=float)
AttributeError: 'EasyDict' object has no attribute 'wd'
Please give me some advice???
Best regards,
PeterPham
",neutral,positive
141,Labels for annotations,open-mmlab/mmdetection,closed,,dhananjaisharma10,dhananjaisharma10,2019-08-16 13:38:23,2019-08-21 20:17:14,2.0,,1204.0,481613949.0,"Hi!

I have a trivial question. Should the labels for annotations begin from 0 or 1? Please let me know.

Thanks!",neutral,positive
142,Theano scan error while training model,tuzhaopeng/NMT-Coverage,closed,,chaitanyamalaviya,tuzhaopeng,2018-01-05 09:57:45,2021-03-09 07:02:46,4.0,,3.0,286241920.0,"Hi, 

I was trying to train a model using the following command on a custom dataset:
`python train.py --state german-data.py `
I tried this with theano version 0.8.2 and 0.9 as well.
I faced the following error:
```
Traceback (most recent call last):
  File ""train.py"", line 100, in <module>
    main()
  File ""train.py"", line 81, in main
    enc_dec.build()
  File ""/projects/tir1/users/cmalaviy/NMT-Coverage/experiments/nmt/encdec.py"", line 1864, in build
    c=self.sampling_c)
  File ""/projects/tir1/users/cmalaviy/NMT-Coverage/experiments/nmt/encdec.py"", line 1725, in build_sampler
    name=""{}_sampler_scan"".format(self.prefix))
  File ""/projects/tir1/users/cmalaviy/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan.py"", line 1071, in scan
    scan_outs = local_op(*scan_inputs)
  File ""/projects/tir1/users/cmalaviy/anaconda2/lib/python2.7/site-packages/theano/gof/op.py"", line 615, in __call__
    node = self.make_node(*inputs, **kwargs)
  File ""/projects/tir1/users/cmalaviy/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.py"", line 578, in make_node
    inner_sitsot_out.type.ndim))
ValueError: When compiling the inner function of scan (the function called by scan in each of its iterations) the following error has been encountered: The initial state (`outputs_info` in scan nomenclature) of variable IncSubtensor{Set;:int64:}.0 (argument number 4) has 4 dimension(s), while the corresponding variable in the result of the inner function of scan (`fn`) has 1 dimension(s) (it should be one less than the initial state). For example, if the inner function of scan returns a vector of size d and scan uses the values of the previous time-step, then the initial state in scan should be a matrix of shape (1, d). The first dimension of this matrix corresponds to the number of previous time-steps that scan uses in each of its iterations. In order to solve this issue if the two varialbe currently have the same dimensionality, you can increase the dimensionality of the variable in the initial state of scan by using dimshuffle or shape_padleft. 
```
Would appreciate any help.

Thanks!",neutral,positive
143,Evaluate Tracking Results on a few scenes ONLY,nutonomy/nuscenes-devkit,closed,,rmooreRD,whyekit-motional,2022-01-28 20:56:43,2022-01-31 03:27:01,3.0,,713.0,1117840781.0,"I am trying to evaluate my tracker results using the nuscenes 'evaluate.py' script. I am using the v1.0-trainval version but I am only running my tracker on the first scene (of the pointPillars-val.json). As a result, I am getting an error (AssertionError: Samples in split don't match samples in predicted tracks.) when running the evaluate script. I believe this is because the script is trying to evaluate over the entire data set while I am only inputting tracking results for the first scene. How can I modify the 'evaluate.py' script to evaluate over a select number of scenes ONLY? If I am mistaken, what might the issue be? Thanks in advance!",neutral,negative
144,Training questions,Arthur151/ROMP,open,,ekuznetsov139,,2022-10-10 16:43:24,,1.0,,353.0,1403451329.0,"I'm trying to reproduce BEV training results, and I have a few questions.
1. The source tree references a ""posetrack"" dataset. It is not mentioned in the paper, its official web site is dead (as in, the domain name does not even resolve), and last activity in its maintainers' Twitter account was two years ago. For now, I've deleted it from the configs, but is there any other place where I can download it?
2. The paper says that training was done on 4x V100; was that 16 GB or 32 GB? I am currently training on 2x 1080Ti (12 GB each), and I can only go up to batch size 18 without running out of GPU memory. I assume that you can do 64 because you have twice as many GPUs (so, your 64 is 16 per GPU)? Should I adjust the learning rate to compensate?
3. The paper describes a two-step training strategy: ""We first learn monocular 3D pose and shape estimation for 120 epochs on basic training datasets. Then we add the weak annotations of RH to training samples and train for 120 epochs."" Does the v6_train.sh script correspond to the first step or the second step?
4. How can I tell if it is training correctly? What kind of ""Losses"" should I see after one or two epochs? How long did it take to train?
5. Do I understand correctly that the model assumes that all humans are seen at fairly low field-of-view angles (the paper mentions 60 degrees)? I tried the pretrained checkpoint on some wide-angle photos with 90 degree FOV and the results weren't very satisfactory.
6. Is the ""SMIL infant template"" the same one that's available for download on the AGORA web site? (I've managed to import that one into Blender and to have a look at it, but I can't figure out how to do the same with your SMPLA_NEUTRAL.pth.) If it is, are you aware of its defects? Most notably, its hands are clenched into fists rather than flat, lips and eyeballs are messed up, and feet are way too small (I'd say they need to be about 50% larger.)",neutral,negative
145,我想要自定义NMS，请问如何实现,open-mmlab/mmdetection,closed,jbwang1997,liuzhihengg,jbwang1997,2022-03-02 16:46:10,2022-03-08 13:45:19,4.0,reimplementation,7305.0,1157440538.0,如题,neutral,neutral
146,what it means of result in inference code? ,open-mmlab/mmdetection,closed,hhaAndroid,leesangjoon1,hhaAndroid,2022-05-10 13:29:27,2022-05-17 09:26:29,1.0,community help wanted,7956.0,1231165512.0,"from mmdet.apis import init_detector, inference_detector, show_result_pyplot
import mmcv

config_file = '/home/sangjoon/realmmdetection/mmdetection/configs/swin/cascade_rcnn_swin_base_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_coco.py'
checkpoint_file = '/media/sensor-lab/sensor-lab/sangjoon/20220504_mmdet_swint/white2/best_bbox_mAP_epoch_160.pth'

model = init_detector(config_file, checkpoint_file, device='cuda:0')


img = '/home/sangjoon/realmmdetection/mmdetection/test_result_0504/aedes53_white.jpg'  # or img = mmcv.imread(img), which will only load it once
result = inference_detector(model, img)
print(result)
show_result_pyplot(model, img, result,score_thr=0.5, out_file='result8.jpg')


I want to know the result so I print it. but the result is printed like this.

load checkpoint from local path: /media/sensor-lab/sensor-lab/sangjoon/20220504_mmdet_swint/white2/best_bbox_mAP_epoch_160.pth
------------------------------------------------------------------------------------
[array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([[2.58407300e+03, 2.10433667e+03, 3.23126855e+03, 2.76259326e+03,
        9.98115301e-01],
       [3.21808154e+03, 1.76806506e+03, 3.61496802e+03, 2.25606348e+03,
        9.97614503e-01],
       [3.22050952e+03, 1.49609100e+02, 3.89842480e+03, 8.01991577e+02,
        9.97450292e-01],
       [2.02563428e+03, 1.20052258e+03, 2.30296191e+03, 1.50188538e+03,
        9.94220138e-01],
       [2.19514844e+03, 6.42477844e+02, 2.47515991e+03, 8.50240723e+02,
        8.96067619e-01],
       [2.69194385e+03, 6.62032593e+02, 3.01585229e+03, 1.12459119e+03,
        8.37097347e-01],
       [2.95001685e+03, 6.77757874e+02, 3.29493457e+03, 1.00104956e+03,
        6.01028860e-01],
       [2.01086804e+03, 1.75742285e+03, 2.15133838e+03, 1.95265588e+03,
        4.15365756e-01],
       [2.68181787e+03, 6.46592285e+02, 3.30367603e+03, 1.07376648e+03,
        1.10488474e-01]], dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([[2.0033392e+03, 1.7577284e+03, 2.1579434e+03, 1.9553176e+03,
        2.6439449e-01]], dtype=float32), array([], shape=(0, 5), dtype=float32), array([[2.68923120e+03, 6.41427246e+02, 3.05115796e+03, 1.12726489e+03,
        1.50827914e-01],
       [2.94998779e+03, 6.77278931e+02, 3.29742944e+03, 1.00485815e+03,
        5.37463166e-02],
       [2.19628638e+03, 6.43043640e+02, 2.47625830e+03, 8.52549561e+02,
        5.07708751e-02]], dtype=float32), array([], shape=(0, 5), dtype=float32)]

what is means of the numbers in numpy array??? 

and how does it match to box coordinates and class name? ",neutral,neutral
147,Why use aspect ratio as the basis for grouping？,open-mmlab/mmdetection,closed,,ttandqq,ttandqq,2019-10-15 06:50:02,2019-10-20 08:12:23,2.0,,1548.0,507042603.0,"When do group sampler, I noticed that aspect ratio is basis for grouping, why?",neutral,neutral
148,Same outputs for multiple  results,lyndonzheng/TFill,closed,,Jackieqfh143,Jackieqfh143,2022-09-11 08:57:56,2022-09-19 05:32:04,2.0,,9.0,1368879623.0,"When testing the model on CelebA-HQ dataset, we got exactly same outputs for multiple  results. Is there anything wrong? ",neutral,negative
149,How can I make random box into a specific box for each image? ,JiahuiYu/generative_inpainting,closed,,dreamay,JiahuiYu,2018-08-16 07:01:57,2018-08-16 15:31:39,2.0,,115.0,351079811.0,"I want to inpaint a special box in some images,and I get the location.I can change the function of random_bbox,but should  I change codes about the section of batch_data.I am confused.I need your help.Thank you. @JiahuiYu ",neutral,neutral
150,[Feature Request] TensorboardVisBackend Images,open-mmlab/mmdetection,open,,austinmw,,2022-10-18 18:32:22,,2.0,feature request#v-3.x,9070.0,1413669535.0,"### What is the problem this feature will solve?

Visualizing outputs makes debugging training and improving performance much easier

### What is the feature you are proposing to solve the problem?

I'd love an option for the `TensorboardVisBackend` to display a couple batches of validation (and maybe also training) predictions each epoch. For example:

![image](https://user-images.githubusercontent.com/12224358/196514787-aa84f930-d9e8-4296-8ccb-6326e223b9d4.png)

Another possible feature would be to display these both before and after postprocessing is applied.

### What alternatives have you considered?

Did not see a feature or request for this already, but please let me know if I missed it! People could write custom hooks, but this feature seems useful to enough people that maybe it's a good idea to include directly.",neutral,positive
151,how to training with inception-resnet-v2,deepinsight/insightface,closed,,shenmanmiao,nttstar,2018-04-18 05:56:20,2018-08-27 15:08:29,0.0,,179.0,315340761.0,"@nttstar, Thanks for sharing this nice work. I am trying to train with Inception-ResNet-v2, howerer the GPU memory consumes so much.  Could  you please tell me the details about training Inception-ResNet-V2, such as GPU version 、GPU numbers、batch_size per GPU.",neutral,positive
152,when the multi-test for the cascade models?,open-mmlab/mmdetection,closed,,runzeer,hellock,2019-04-17 09:07:45,2019-04-21 21:04:07,1.0,,512.0,434172701.0,,neutral,neutral
153,add center loss problem,deepinsight/insightface,closed,,IrvingShu,nttstar,2018-04-20 05:48:27,2018-08-27 15:09:25,4.0,,183.0,316135798.0,"1. add losstype=8 //centerloss.
problem: 
mxnet.base.MXNetError: [13:38:41] src/io/image_io.cc:186: Check failed: inputs[00
].ctx().dev_mask() == Context::kCPU (2 vs. 1) Only supports cpu input",negative,negative
154,bash install has problem,junyanz/VON,open,,humaolin,,2019-10-06 11:09:32,,1.0,,21.0,503085919.0,"Add -gencode to match all the GPU architectures you have.
Check 'https://en.wikipedia.org/wiki/CUDA#GPUs_supported' for list of architecture.
Check 'http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html' for GPU compilation based on architecture.
nvcc -c -o vtn_cuda_kernel_generic.cu.o vtn_cuda_kernel_generic.cu -x cu -Xcompiler -fPIC -std=c++11 -I /usr/local/anaconda3/lib/python3.7/site-packages/torch/lib/include/TH -I /usr/local/anaconda3/lib/python3.7/site-packages/torch/lib/include -I /usr/local/anaconda3/lib/python3.7/site-packages/torch/lib/include/THC -I /home/humaolin/papercode/VON/render_module/vtn/vtn/src         -gencode arch=compute_30,code=sm_30         -gencode arch=compute_35,code=sm_35         -gencode arch=compute_52,code=sm_52         -gencode arch=compute_61,code=sm_61 
vtn_cuda_kernel_generic.cu:1:10: fatal error: THC.h: No such file or directory
 #include <THC.h>
          ^~~~~~~
compilation terminated.
",neutral,positive
155,How do i create a new model from my epoch.pth checkpoint file and config file?,open-mmlab/mmdetection,closed,hhaAndroid,Enish258,hhaAndroid,2021-06-14 20:02:33,2021-08-02 03:13:00,10.0,,5354.0,920732065.0,"I have a checkpoint file that I created by train_detector.
![Screenshot (449)](https://user-images.githubusercontent.com/64497088/121950960-ef07cc00-cd77-11eb-8c06-f6d8bf1d4e79.png).
After that, let's say I have to create a new model.So what exactly is the procedure for that?
Like should I create an init_detector using my saved epoch checkpoint file +the original model cfg?
I did it this way.
![Screenshot (451)](https://user-images.githubusercontent.com/64497088/121951940-35a9f600-cd79-11eb-8911-a1391c5ce2f9.png)

![Screenshot (450)](https://user-images.githubusercontent.com/64497088/121951671-d77d1300-cd78-11eb-9f66-52b529fa6c63.png)
But it keeps giving a warning saying that class names are not saved in checkpoints.And when i do inference using the model,it keeps outputting the original classes itself instead of the trained classes.
![Screenshot (452)](https://user-images.githubusercontent.com/64497088/121952157-815c9f80-cd79-11eb-9774-50743b4bd50a.png)
Please help me.
Thank you.

",neutral,positive
156,problem occured in  hrnet_backbone.py,openseg-group/openseg.pytorch,closed,,daixiaolei623,PkuRainBow,2020-07-15 13:21:50,2020-07-18 16:08:48,12.0,,25.0,657345847.0,"Dear Author,

Thank you for your excellent work, but some errors are reported for backbones.

    checkpoint names:
    checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth


    commands:
    (for HRNet-W48:)
    python -u main.py --configs configs/cityscapes/H_48_D_4.json --drop_last y --backbone hrnet48 --model_name hrnet_w48_ocr --checkpoints_name hrnet_w48_ocr_1 --phase test --gpu 0 --resume ./checkpoints/cityscapes/hrnet_w48_ocr_1_latest.pth --loss_type fs_auxce_loss --test_dir input_images --out_dir output_images


Error messages:

2020-07-15 21:00:10,470 INFO    [module_runner.py, 44] BN Type is inplace_abn.
Traceback (most recent call last):
  File ""main.py"", line 214, in <module>
    model = Tester(configer)    
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/segmentor/tester.py"", line 69, in __init__
    self._init_model()
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/segmentor/tester.py"", line 72, in _init_model
    self.seg_net = self.model_manager.semantic_segmentor()
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/model_manager.py"", line 81, in semantic_segmentor
    model = SEG_MODEL_DICT[model_name](self.configer)
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/nets/hrnet.py"", line 105, in __init__
    self.backbone = BackboneSelector(configer).get_backbone()
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/backbones/backbone_selector.py"", line 34, in get_backbone
    model = HRNetBackbone(self.configer)(**params)
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/backbones/hrnet/hrnet_backbone.py"", line 598, in __call__
    bn_momentum=0.1)
  File ""/home/dai/code/semantic_segmentation/9/openseg.pytorch-master/lib/models/backbones/hrnet/hrnet_backbone.py"", line 307, in __init__
    self.bn1 = ModuleHelper.BatchNorm2d(bn_type=bn_type)(64, momentum=bn_momentum)
TypeError: 'NoneType' object is not callable




Could you please tell me what is wrong? thank you.
",neutral,positive
157,Emotion Transfer,NVIDIA/flowtron,open,,letrongan,,2021-09-20 16:12:52,,1.0,,131.0,1001148916.0,"Hi everyone, I'm a new member of the group. Glad to have read the detailed instructions in the README and the previous discussions. I completed the voice training after 3 steps:
- Step 1: train with tacotron2
- Step 2: train flowtron with n_flows=1 and warmstart with tacotron2 model
- Step 3: continue with n_flows=2

The results are quite good, the sound is easy to understand (with me and some my friends). 
I'm doing the emotional transfer(style_transfer) but the quality is very poor (The sound doesn't sound natural, doesn't feel right). 
I have tried with both the flowtron_ljs model for English and my model for Vietnamese.

Can someone who has completed this process give me some suggestions?
Thanks a lot.

@rafaelvalle  Please help me and give me some advice.",neutral,negative
158,Rotating Anchor Boxes,matterport/Mask_RCNN,open,,YashRunwal,,2020-09-09 08:09:44,,4.0,,2362.0,696568735.0,"Hello fellow mask_rcnn users,

I am using Mask_RCNN to perform instance segmentation. An I am using labelme as a labelling tool. 

From the implementations, I can see that the bounding boxes are all straight. Is there a way to change the class
to accommodate the rotation of the bounding boxes? (Please see the figure below)

![image](https://user-images.githubusercontent.com/56769818/92572063-33fc5080-f284-11ea-8798-c06598de40ab.png)

This is important because the objects I want to detect should have the rotated anchor boxes, so that I can later determine the orientation, whether it is left or right. 

I would appreciate any help I can get.

Regards,
Yash Runwal. 
",neutral,neutral
159,Google Colab [Errno 2] No such file or directory: 'property',deepinsight/insightface,open,,ansh103,,2020-01-14 06:07:57,,1.0,,1046.0,549354168.0,"I'm running Python 2.7 in Google Colab and downloaded the pre-trained model as well from the model zoo. While running the verification.py file I'm getting the following error:- 

`IOErrorTraceback (most recent call last)
/content/insightface/src/eval/verification.py in <module>()
    518   args = parser.parse_args()
    519 
--> 520   prop = face_image.load_property(args.data_dir)
    521   image_size = prop.image_size
    522   print('image_size', image_size)

/content/insightface/src/eval/../common/face_image.py in load_property(data_dir)
      8 def load_property(data_dir):
      9   prop = edict()
---> 10   for line in open(os.path.join(data_dir, 'property')):
     11     vec = line.strip().split(',')
     12     assert len(vec)==3

IOError: [Errno 2] No such file or directory: 'property'`

I'm new to Insightface. Can anyone help me out or tell me what additional info is needed to solve the problem? ",neutral,negative
160,BUG REPORT for mmdetect/core/anchor/anchor_target.py,open-mmlab/mmdetection,closed,,xfguo-ucas,hellock,2019-01-20 07:04:49,2019-02-13 08:35:36,2.0,,275.0,401069392.0,"**RuntimeError: Expected object of type torch.cuda.FloatTensor but found type torch.cuda.LongTensor for argument  'other'**

When i change num_class in configs/retinanet_r101_fpn_1x.py from 81 to 2. the above error occurs.  This error seems to be caused by focal loss. In mmdetect/core/anchor/anchor_target.py line 150, if label_channels equal 1, labels will not be converted to support focal loss.
",neutral,negative
161,TypeError: 'NoneType' object has no attribute '__getitem__',deepinsight/insightface,closed,,xinxliu,xinxliu,2019-03-11 14:00:44,2019-03-12 10:57:45,1.0,,592.0,419484784.0,"Platform information:

- Python 2.7
- MXNet 1.4

Hi, I met the following error when I ran the `train.py` under `insightface/recognition`. I am sure I use the **recommended dataset MS1M-ArcFace** from the  [baidu source](https://pan.baidu.com/s/1S6LJZGdqcZRle1vlcMzHOQ).

> Traceback (most recent call last):
  File ""train.py"", line 366, in <module>
    main()
  File ""train.py"", line 363, in main
    train_net(args)
  File ""train.py"", line 231, in train_net
    images_filter        = config.data_images_filter,
  File ""/home/liuxx/work/insightface/recognition/image_iter.py"", line 42, in __init__
    header, _ = recordio.unpack(s)
  File ""/home/liuxx/anaconda2/envs/mxnet/lib/python2.7/site-packages/mxnet/recordio.py"", line 416, in unpack
    header = IRHeader(*struct.unpack(_IR_FORMAT, s[:_IR_SIZE]))
TypeError: 'NoneType' object has no attribute '__getitem__'

Then I manually load the dataset:
`>>> imgrec = mx.recordio.MXIndexedRecordIO('datasets/faces_emore/train.idx', 'datasets/faces_emore/train.rec', 'r')`
 `>>> s = imgrec.read_idx(0)`
`>>>s is None`
`True`

The result of `s is None` is **True**. Hence is there something wrong with the dataset?  
",neutral,negative
162,How to configure config so that the model is only segment without detection,open-mmlab/mmdetection,closed,,abing222,abing222,2019-07-06 01:03:01,2019-07-06 08:22:54,0.0,,936.0,464803527.0,"for example, mask rcnn or cascade mask rcnn? Thank you",neutral,neutral
163,Same color of instances(the color should be different for each instance) while visualizing instance masks on test data for the model trained on cascade mask rcnn,open-mmlab/mmdetection,closed,ZwwWayne,abubakar2141731,ZwwWayne,2020-09-24 00:51:58,2020-10-03 07:06:28,1.0,,3827.0,707771915.0,"HOw to visualize different color for each instance mask, using mmdetection, please explain? right now i am getting the same color for each instance belonging to the same class in an image while testing the images.",neutral,positive
164,Arcface PyTorch Backbones and Inference,deepinsight/insightface,closed,,CengizhanYurdakul,CengizhanYurdakul,2021-09-08 09:10:45,2021-11-25 08:07:23,1.0,,1745.0,990914853.0,"Hello and thanks for great works,
My first question was while examining the pytorch implementation of Arcface, I saw the r2060 backbone in the table. But I could not find the weights of this model in the link. Haven't you posted yet or did I miss something?

![Screenshot from 2021-09-08 11-40-49](https://user-images.githubusercontent.com/53911245/132479994-91c066f5-c0c2-47db-9207-2972a6e76b95.png)
My second question is, I couldn't see any alignment process while inference. Did you use a different alignment template or is the old template still up to date? (Here I am assuming that you have trained pytorch from scratch and have not made any conversions from mxnet)
![Screenshot from 2021-09-08 12-05-14](https://user-images.githubusercontent.com/53911245/132480814-695053a0-3e5c-4b09-825c-cb778d30e72d.png)",neutral,positive
165,Granularity,irwinherrmann/stochastic-gates,open,,KrishnaTarun,,2022-05-02 16:32:50,,0.0,,1.0,1223092774.0,What is the purpose of having granularity as the input to model design?,neutral,neutral
166,"While training the  Fashion Segmentation model, is there  a way to train only bbox detection and turn off segmentation",open-mmlab/mmdetection,closed,ZwwWayne,kartikwar,ZwwWayne,2020-08-02 09:52:13,2020-10-12 07:41:40,5.0,usage,3462.0,671586838.0,"Hi I have a fashion dataset that I want to train. The issue is I have only bbox coordinates with me. I dont want to train segmentation . I am using the configuration deepfashion/mask_rcnn_r50_fpn_15e_deepfashion.py. I am able to train by providing random segmentation points. But the issue is this would impact loss and hence the model (since model is also taking loss for segmentation predictions) so this would affect my detection results as well for the trained model. Is there any way in ehich I can turn off the segmentation mask pipeline in this model. Also if not are there any other alternatives to achieve this ?

Thanks",neutral,negative
167,What kinds of planes are available?,bertjiazheng/Structured3D,closed,,GuardianWang,GuardianWang,2019-09-09 12:56:44,2019-09-10 06:54:46,2.0,question,2.0,491079691.0,"Thank you for your outstanding work!

I have a question about available plane labels. I notice that in Figure 1 (b) Planes of your paper, several walls are labeled. So I wonder have you just labeled walls, or your dataset actually provides planar labels of all kinds of objects?

![360截图17571122107150157](https://user-images.githubusercontent.com/31812793/64532496-3318a200-d344-11e9-8489-7d32020e0840.jpg)

Thank you.",neutral,positive
168,I add a new neck but not work,open-mmlab/mmdetection,closed,hhaAndroid,Atlantisming,Atlantisming,2022-06-14 04:33:19,2022-10-26 02:01:35,1.0,,8181.0,1270235992.0,"**Describe the bug**
I try to use the convnext to replace the resnet50 in the DETR, and as the output shape is not the same. I add a new FCN neck but it not work.
**Reproduction**

1. What command or script did you run?
Here's I run the command
`python mmdetection/tools/test.py mmdetection/configs/detr/detr_convnext_1x1_100e_coco.py mmdetection/checkpoints/convnext_tiny_1k_224_ema.pth --eval proposal`

and this is the configs file.
```_base_ = [
    '../_base_/datasets/coco_detection.py', '../_base_/default_runtime.py'
]

custom_imports = dict(imports=['mmcls.models'], allow_failed_imports=False)
checkpoint_file = 'checkpoints/convnext_tiny_1k_224_ema.pth'  # noqa

model = dict(
    type='DETR',
    pretrained = checkpoint_file,
    backbone = dict(
        type='mmcls.ConvNeXt',
        arch='tiny',
        out_indices=[3,0,1,2],
        drop_path_rate=0.4,
        layer_scale_init_value=1.0,
        gap_before_final_norm=False,
        init_cfg=dict(
            type='Pretrained', checkpoint=checkpoint_file,
            prefix='backbone.')),
    neck=dict(
        type='ConvNeck',
        in_channels=96,
        out_channels=256,
        kernel_size = 7,
        stride = 7),
    bbox_head=dict(
        type='DETRHead',
        num_classes=80,
        in_channels=2048,
        transformer=dict(
            type='Transformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1)
                    ],
                    feedforward_channels=2048,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DetrTransformerDecoder',
                return_intermediate=True,
                num_layers=6,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=dict(
                        type='MultiheadAttention',
                        embed_dims=256,
                        num_heads=8,
                        dropout=0.1),
                    feedforward_channels=2048,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')),
            )),
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        loss_cls=dict(
            type='CrossEntropyLoss',
            bg_cls_weight=0.1,
            use_sigmoid=False,
            loss_weight=1.0,
            class_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0)),
    # training and testing settings
    train_cfg=dict(
        assigner=dict(
            type='HungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=1.),
            reg_cost=dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),
            iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))),
    test_cfg=dict(max_per_img=100))
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
# train_pipeline, NOTE the img_scale and the Pad's size_divisor is different
# from the default setting in mmdet.
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='AutoAugment',
        policies=[[
            dict(
                type='Resize',
                img_scale=[(480, 1333), (512, 1333), (544, 1333), (576, 1333),
                           (608, 1333), (640, 1333), (672, 1333), (704, 1333),
                           (736, 1333), (768, 1333), (800, 1333)],
                multiscale_mode='value',
                keep_ratio=True)
        ],
                  [
                      dict(
                          type='Resize',
                          img_scale=[(400, 1333), (500, 1333), (600, 1333)],
                          multiscale_mode='value',
                          keep_ratio=True),
                      dict(
                          type='RandomCrop',
                          crop_type='absolute_range',
                          crop_size=(384, 600),
                          allow_negative_crop=True),
                      dict(
                          type='Resize',
                          img_scale=[(480, 1333), (512, 1333), (544, 1333),
                                     (576, 1333), (608, 1333), (640, 1333),
                                     (672, 1333), (704, 1333), (736, 1333),
                                     (768, 1333), (800, 1333)],
                          multiscale_mode='value',
                          override=True,
                          keep_ratio=True)
                  ]]),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=1),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
# test_pipeline, NOTE the Pad's size_divisor is different from the default
# setting (size_divisor=32). While there is little effect on the performance
# whether we use the default setting or use size_divisor=1.
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=1),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(pipeline=train_pipeline),
    val=dict(pipeline=test_pipeline),
    test=dict(pipeline=test_pipeline))
# optimizer
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    weight_decay=0.0001,
    paramwise_cfg=dict(
        custom_keys={'backbone': dict(lr_mult=0.1, decay_mult=1.0)}))
optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))
# learning policy
lr_config = dict(policy='step', step=[100])
runner = dict(type='EpochBasedRunner', max_epochs=150)
```
I assert that I add this class ConvNeck to the __init__.py
```import torch.nn as nn
from ..builder import NECKS

@NECKS.register_module()
class ConvNeck(nn.Module):

    def __init__(self,
                in_channels = 96,
                out_channels = 256 ,
                kernel_size = 7,
                stride = 7):
        self.conv = nn.Conv2d(96,256,7,7)

    def forward(self, inputs):
        # implementation is ignored
        x = self.conv(x)
        x.flatten(2,3)
        return x
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?
I created self.neck in the class DETR, __init__ function in the detr.py located :/root/mmdetection/mmdet/models/detectors/detr.py

4. What dataset did you use?
I used coco2017 dataset
**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
```
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100S-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.24
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.7.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.8.1
OpenCV: 4.6.0
MMCV: 1.5.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.0
MMDetection: 2.25.0+
```

**Error traceback**
If applicable, paste the error trackback here.

```none
RuntimeError: Given groups=1, weight of size [256, 2048, 1, 1], expected input[1, 96, 200, 299] to have 2048 channels, but got 96 channels instead
```
the error is as the same as before I add the neck.

Thanks for your help.
**Bug fix**
I found the DETR class can inherit from class SingleStageDetection, and it have the neck property.
",neutral,negative
169,How to add multiple key value pairs in --eval-options,open-mmlab/mmdetection,closed,,Chenliang-Gu,yhcao6,2020-09-22 09:42:46,2020-09-24 10:02:30,4.0,,3814.0,706228484.0,"```
parser.add_argument(
        '--eval-options',
        nargs='+',
        action=DictAction,
        help='custom options for evaluation, the key-value pair in xxx=yyy '
        'format will be kwargs for dataset.evaluate() function')
```
I want to add multiple key value pairs of --eval-options to the command line：
""classwise=True"" ""iou_thrs=[0.1]""
So how to add these multiple key value pairs in command line?",neutral,positive
170,About n_segment,liu-zhy/temporal-adaptive-module,open,,Kimsure,,2022-02-10 15:02:46,,2.0,,9.0,1130245456.0,n_segment是视频序列中帧的个数，但如果不确定视频序列帧数应该怎么办呢，这里不能自适应调整嘛？,neutral,neutral
171,Stage 2 Output images are all black,xthan/VITON,open,,Neuroforge,,2018-05-10 01:45:23,,23.0,,9.0,321781195.0,"Hello,

I am running this code.

Stage 1 works fine.

Stage 2 runs but outputs black images. I am using Octave and not Matlab to run shape_context_warp.m. Is the output of this available online?

Regards,

Daniel",neutral,positive
172,mxnet.base.MXNetError: mutex lock failed: Invalid argument,deepinsight/insightface,open,,CachCheng,,2020-01-11 09:06:57,,4.0,,1044.0,548404373.0,"mxnet.base.MXNetError: [16:59:51] /Users/travis/build/dmlc/mxnet-distro/mxnet-build/3rdparty/tvm/nnvm/include/nnvm/graph.h:263: Check failed: it != attrs.end(): Cannot find attribute shape in the graph
Stack trace:
  [bt] (0) 1   libmxnet.so                         0x0000001c19046929 mxnet::op::NDArrayOpProp::~NDArrayOpProp() + 4473
  [bt] (1) 2   libmxnet.so                         0x0000001c19045d19 mxnet::op::NDArrayOpProp::~NDArrayOpProp() + 1385
  [bt] (2) 3   libmxnet.so                         0x0000001c1a55df53 mxnet::exec::GraphExecutor::Backward(std::__1::vector<mxnet::NDArray, std::__1::allocator<mxnet::NDArray> > const&, bool) + 4275
  [bt] (3) 4   libmxnet.so                         0x0000001c1a55b30c std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, mxnet::NDArray, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, mxnet::NDArray> > >::~unordered_map() + 1724
  [bt] (4) 5   libmxnet.so                         0x0000001c1a4f256e MXExecutorForward + 46
  [bt] (5) 6   libffi.6.dylib                      0x0000000102420884 ffi_call_unix64 + 76
  [bt] (6) 7   ???                                 0x00007000028506e0 0x0 + 123145344583392",negative,neutral
173,tws_stream failed to run colab: output_02_inference,google-research/google-research,closed,,wangtz,wangtz,2021-01-19 12:29:12,2021-01-25 07:47:59,4.0,,531.0,788982668.0,"Failed in block 25, source:
===
```
# create model with flag's parameters
model_non_stream_batch = models.MODELS[flags.model_name](flags)

# load model's weights
weights_name = 'best_weights'
model_non_stream_batch.load_weights(os.path.join(train_dir, weights_name))
```

Error message:
--------------------------------------------------------------------------
```AttributeError                           Traceback (most recent call last)
<ipython-input-25-a170e05f648d> in <module>
      1 # create model with flag's parameters
----> 2 model_non_stream_batch = models.MODELS[flags.model_name](flags)
      3 
      4 # load model's weights
      5 weights_name = 'best_weights'

~/google-research/kws_streaming/colab/google-research/kws_streaming/models/svdf.py in model(flags)
    104     # it is a self contained model, user need to feed raw audio only
    105     net = speech_features.SpeechFeatures(
--> 106         speech_features.SpeechFeatures.get_params(flags))(
    107             net)
    108 

~/google-research/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py in get_params(flags)
    274     """"""
    275 
--> 276     if flags.time_shift_ms != 0.0 and flags.sp_time_shift_ms != 0.0:
    277       raise ValueError('both time_shift_ms and sp_time_shift_ms are set '
    278                        'only one parameter should be used: '

AttributeError: 'Namespace' object has no attribute 'sp_time_shift_ms'
```

Temporary fix: 
===
```
flags.sp_time_shift_ms = .0
flags.sp_resample = .0
```

Remaining problem
===
--------------------------------------------------------------------------
```AttributeError                           Traceback (most recent call last)
<ipython-input-30-a170e05f648d> in <module>
      1 # create model with flag's parameters
----> 2 model_non_stream_batch = models.MODELS[flags.model_name](flags)
      3 
      4 # load model's weights
      5 weights_name = 'best_weights'

~/google-research/kws_streaming/colab/google-research/kws_streaming/models/svdf.py in model(flags)
    104     # it is a self contained model, user need to feed raw audio only
    105     net = speech_features.SpeechFeatures(
--> 106         speech_features.SpeechFeatures.get_params(flags))(
    107             net)
    108 

~/google-research/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py in get_params(flags)
    327             flags.use_tf_fft,
    328         'use_spec_cutout':
--> 329             flags.use_spec_cutout,
    330         'spec_cutout_masks_number':
    331             flags.spec_cutout_masks_number,

AttributeError: 'Namespace' object has no attribute 'use_spec_cutout'```

Not sure how to solve it.",neutral,negative
174,Create Draw precision-recall curve,matterport/Mask_RCNN,open,,hungdaica77,,2020-04-03 15:57:03,,1.0,,2084.0,593481178.0,"![image](https://user-images.githubusercontent.com/15230555/78380503-2ae4c480-75fe-11ea-8fbb-f84f9760cc54.png)
![image](https://user-images.githubusercontent.com/15230555/78380581-464fcf80-75fe-11ea-80ba-dfad02b4a5d8.png)

 visualize.display_instances run ok but AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,  r['rois'], r['class_ids'], r['scores'], r['masks']) not running. help me",neutral,neutral
175,Question about facescrub dataset clean in your paper.,deepinsight/insightface,closed,,WeicongChen,nttstar,2018-03-26 15:39:24,2018-03-27 10:50:15,4.0,,135.0,308625705.0,"In your paper, your said:

> We manually clean the FaceScrub dataset and finally find 605 noisy face images. During testing, we change the noisy face to another right face, which can increase the identification accuracy by about 1%. 

So I'd like to know if your  83.27% identity result on megaface is tested using the cleaned facescrub list  or official list. Because in my test on megaface using official list, I got only 79.65% identity accuracy. Thank you! @nttstar ",neutral,neutral
176,crf,bplank/bilstm-aux,closed,,bourne-3,bplank,2020-11-15 07:38:29,2020-11-17 08:40:54,1.0,,19.0,743202920.0,‘the option of running a CRF has been added’---- how to run the crf version by the  common?,neutral,neutral
177,Defining Layer_regex for New backbone for training,matterport/Mask_RCNN,open,,suresh-s,,2020-12-24 05:17:09,,0.0,,2448.0,774187404.0,"I trying to use new Backbone with Mask_rcnn code base instead of Resnet 101.  I am defining ""layer_regex"" for new Backbone for training.

I have included only Conv2D, BatchNormalization and Activation in ""layer_regex"" for training. I have not included Pooling layer, Lambda layer and Concatenate layers in ""layer_regex"" for training. 

Should i include Pooling layer, Lambda layer and Concatenate layers in the layer_regex ? if not included, what will be the impact in training?",neutral,neutral
178,Can the model classify correctly classes with the object size ratio vary significantly,matterport/Mask_RCNN,open,,hoangphucITJP,,2019-06-25 07:19:38,,0.0,,1584.0,460254595.0,"As the title said.
For example, my data contain some classes. Objects in those classes have the width/height ratio vary significantly (0.1 to 10 for example). Given that situation, can the model classify those objects as the same class efficiently?",neutral,neutral
179,ATSS assigner error when training in small image size:,open-mmlab/mmdetection,closed,,chuong98,chuong98,2020-07-29 05:27:52,2020-07-29 21:02:10,1.0,,3428.0,667559946.0,"Thanks for your error report and we appreciate it a lot.

**Checklist**
1. I have searched related issues but cannot get the expected help.
2. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

**Reproduction**
1. What command or script did you run?
```
A placeholder for the command.
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?
I change the image size to smaller size (320,320) for mobile application:
```python
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(320, 320), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
```
3. What dataset did you use?
coco
**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
2. You may add addition that may be helpful for locating the problem, such as
    - How you installed PyTorch [e.g., pip, conda, source]
    - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**
If applicable, paste the error trackback here.
```
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!

For small size, some time the image does not have enough positive sample at high stage. Thus. topk=9 sometime fails. 
In your code, line 122,123 of file: `mmdet/core/bbox/assigners/atss_assigner.py`, replace:
```python
_, topk_idxs_per_level = distances_per_level.topk(
                self.topk, dim=0, largest=False)
``` 
to
```python
_, topk_idxs_per_level = distances_per_level.topk(
    min(self.topk, distances_per_level.shape[0]), dim=0, largest=False)
```
It will pass the error.",neutral,positive
180,Could you provide the devkit version that support the Teaser dataset (v0.1) ?,nutonomy/nuscenes-devkit,closed,,PhyllisH,oscar-nutonomy,2019-04-17 14:49:29,2019-04-17 18:28:52,1.0,,121.0,434326095.0,"Since I want to load a mini version of the full dataset, however, the 'version='v1.0-mini'' setting in the tutorial does not work.",neutral,neutral
181,Batch normalization,gpapamak/maf,closed,,ikostrikov,gpapamak,2018-08-29 15:17:52,2018-08-29 22:30:29,1.0,question,6.0,355210599.0,"Hi!

Thanks for sharing amazing work!

I'm trying to port your code to PyTorch (for further use in my research).

I have a question regarding your implementation of Batch Norm. As you mention in the paper, it's implemented using global batch statistics. Could you please provide pointers to the lines where it is implemented exactly? My knowledge of Theano is a little bit rusty.
",neutral,positive
182,Error when trying to train num_layer > 1,snakeztc/NeuralDialog-CVAE,closed,,dimeldo,snakeztc,2017-12-07 15:39:35,2018-03-12 14:51:25,2.0,,5.0,280175350.0,"Hi,

When I set:

```
cxt_cell_size = 150  # context encoder hidden size
sent_cell_size = 75  # utterance encoder hidden size
dec_cell_size = 100  # response decoder hidden size
num_layer = 4  # number of context RNN layers
```
I get the following error:

`ValueError: Trying to share variable model/contextRNN/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel, but specified shape (300, 300) and found shape (302, 300).`",neutral,negative
183,The question for kitti dataset,TRI-ML/packnet-sfm,open,,Youyi7,,2022-02-10 04:20:52,,2.0,,211.0,1129429543.0,"Hi, for the training list you provide in [eigen_train](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/splits/KITTI/eigen_train_files.txt), the corresponding depth data in the first line seems not exist

> 2011_09_28/2011_09_28_drive_0001_sync/image_02/data/0000000105.png 2011_09_28/2011_09_28_drive_0001_sync/image_03/data/0000000105.png

I could only find image data in [KITTI_raw](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/datasets/KITTI_raw.tar.gz), while there is no corresponding depth data in [KITTI_depth](https://tri-ml-public.s3.amazonaws.com/github/packnet-sfm/depth_maps/KITTI_raw_groundtruth.tar.gz)

Do I make some mistake in your code?

Thank you very much!",neutral,positive
184,How can I plot proposed bounding boxes produced by RPN?,matterport/Mask_RCNN,closed,,uzunper,uzunper,2020-05-22 10:57:27,2020-05-29 13:18:38,2.0,,2198.0,623127932.0,"Hi,
I am using your Mask R CNN code for nucleus detection. I am not training the model, just using the code for detection. I would like to plot regions proposed by RPN algorithm. How can I do this? Also I could not get the idea very well. In the test step, is it proposing different regions for each image? Or is it using learned regions from training?",neutral,neutral
185,error: command '/usr/local/cuda/bin/nvcc' failed with exit status 1,open-mmlab/mmdetection,closed,,chenjun2hao,chenjun2hao,2019-08-29 03:39:50,2019-08-29 06:27:50,4.0,,1294.0,486718303.0,"when install mmdetection, i meet this issue.
the detail information is:
`cc1plus: 警告：command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
/usr/local/cuda/bin/nvcc -I/home/yangna/yangna/tool/anaconda2/envs/torch110/lib/python3.6/site-packages/torch/include -I/home/yangna/yangna/tool/anaconda2/envs/torch110/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/home/yangna/yangna/tool/anaconda2/envs/torch110/lib/python3.6/site-packages/torch/include/TH -I/home/yangna/yangna/tool/anaconda2/envs/torch110/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/yangna/yangna/tool/anaconda2/envs/torch110/include/python3.6m -c mmdet/ops/nms/src/nms_kernel.cu -o build/temp.linux-x86_64-3.6/mmdet/ops/nms/src/nms_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=nms_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
/usr/local/gcc-6.3/include/c++/6.3.0/tuple: In instantiation of ‘static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]’:
/usr/local/gcc-6.3/include/c++/6.3.0/tuple:626:248:   required by substitution of ‘template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]’
/home/yangna/yangna/tool/anaconda2/envs/torch110/lib/python3.6/site-packages/torch/include/ATen/core/TensorMethods.h:1181:57:   required from here
/usr/local/gcc-6.3/include/c++/6.3.0/tuple:483:67: 错误：展开‘std::is_constructible<_Elements, _UElements&&>’时参数包长度不匹配
       return __and_<is_constructible<_Elements, _UElements&&>...>::value;
                                                                   ^~~~~
/usr/local/gcc-6.3/include/c++/6.3.0/tuple:484:1: 错误：body of constexpr function ‘static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]’ not a return-statement
     }
 ^
/usr/local/gcc-6.3/include/c++/6.3.0/tuple: In instantiation of ‘static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]’:
/usr/local/gcc-6.3/include/c++/6.3.0/tuple:626:362:   required by substitution of ‘template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]’
/home/yangna/yangna/tool/anaconda2/envs/torch110/lib/python3.6/site-packages/torch/include/ATen/core/TensorMethods.h:1181:57:   required from here
/usr/local/gcc-6.3/include/c++/6.3.0/tuple:489:65: 错误：展开‘std::is_convertible<_UElements&&, _Elements>’时参数包长度不匹配
       return __and_<is_convertible<_UElements&&, _Elements>...>::value;
error: command '/usr/local/cuda/bin/nvcc' failed with exit status 1
`
my Environment：
pytorch1.1.0
cuda9.0
gcc6.3",negative,neutral
186,Calculating Hyperbolicity bug in delta.py?,leymir/hyperbolic-image-embeddings,closed,,ajbanks,leymir,2021-08-07 10:20:20,2022-01-12 13:09:00,1.0,,12.0,963202343.0,"I have tried to reproduce your hyperbolicity experiments with the cifar10 dataset, using the code in delta.py. I have stumbled across a potential error. In the paper you calculate the relative Hyperbolicity of a dataset by doing the following calculation:

`δrel(X) = 2δ(X) / diam(X)`

However, in the delta.py δ(X) is not multiplied by 2, giving incorrect results:
`delta_rel = delta_hyp(distmat) / diam`

When I multiply by 2 it seems to work and I can reproduce the cifar10 hyperbolicity score:

`delta_rel = (2 * delta_hyp(distmat)) / diam`

Is this change correct?

",neutral,negative
187,Convergence Quality check - Run Validation after every Epoch,matterport/Mask_RCNN,open,,suresh-s,,2020-12-24 06:11:39,,0.0,,2449.0,774207197.0,"I want to access the quality of convergence after each epoch or after every 5 epoch.   It helps me to correct/debug the Architecture immediately after finding out convergence quality is not optimal.  I don't want to wait for 5 days then find out validation is not satisfactory.  It will waste the time.

How should i run validation after every 5 epoch?  Could you suggest me?",neutral,negative
188,"During verifing the trained model, I got TypeError",open-mmlab/mmdetection,closed,,BaeSukkyoung,ZwwWayne,2022-02-14 08:19:52,2022-03-13 07:44:11,1.0,,7153.0,1136939125.0,"I use same code given by mmdetection like under.

config = 'cascade_mask_rcnn_x101_64x4d_fpn_1x_for_crack_road_crack_210517.py'
checkpoint = 'cascade_mask_rcnn_x101_64x4d_fpn_1x_for_crack_road_crack_210517/epoch_30.pth'

device = 'cuda:0'
model = init_detector(config, checkpoint, device=device)

And I run the code, I get the error like underline.

TypeError: __init__() got an unexpected keyword argument 'num_stages'

What's the problem ?
",neutral,positive
189,Where can i find generate_edge.py?,deepinsight/insightface,open,,xevolesi,,2022-05-10 08:33:58,,6.0,,2001.0,1230808795.0,"Hi, could you please tell me where can i find `generate_edge.py`? I think that your face-parsing approach greatly meets my requirements. I looked inside `parsing` directory but i was not able to find this scirpt.

Thank you!",neutral,positive
190,通过lst生成rec,deepinsight/insightface,closed,,zhaowwenzhong,nttstar,2018-06-27 01:59:31,2018-06-28 05:54:58,3.0,,265.0,336050599.0,"我利用mxnet\trunk\tools\im2rec.py生成lst文件
再利用这lst文件，通过insightface\trunk\src\data\face2rec2.py 生成rec文件
是否要修改这些地方，insightface\trunk\src\train_softmax.py读取数据。
1、insightface\trunk\src\common\face_preprocess.py
def parse_lst_line(line):
  vec = line.strip().split(""\t"")
  assert len(vec)>=3
  aligned = int(vec[0])
  image_path = vec[1]
  label = int(vec[2])
  bbox = None
  landmark = None
。。。
修改为：
def parse_lst_line(line):
  vec = line.strip().split(""\t"")
  assert len(vec)>=3
  aligned = True#int(vec[0])
  image_path = vec[2]
  label = int(float(vec[1]))
  bbox = None
  landmark = None

2、insightface\trunk\src\data\face2rec2.py

def read_list(path_in):
    path_ = path_in[:-4]        #获取文件所在目录
    with open(path_in) as fin:
        identities = []
        last = [-1, -1]
        _id = 1
        while True:
            line = fin.readline()
            if not line:
                break
            item = edict()
            item.flag = 0
            item.image_path, label, item.bbox, item.landmark, item.aligned = face_preprocess.parse_lst_line(line)
            item.image_path = os.path.join(path_, item.image_path)             #图片路径
            if not os.path.exists( item.image_path):
                continue
            if not item.aligned and item.landmark is None:
              #print('ignore line', line)
              continue
            item.id = _id
            item.label = [label, item.aligned]  => item.label = label        #label
  ",neutral,positive
191,Colab demo fails,facebookresearch/mmf,closed,,LanceNorskog,apsdehal,2019-05-26 05:23:07,2019-05-30 00:28:26,6.0,,67.0,448529840.0,"The Colab demo fails. I believe the problem is that the CUDA version install now on Colab is not what this version of PyTorch uses. I noticed that in general the Colab Python package versions are ahead of what this demo uses.

If you want to have a permanently working Colab demo, I suspect that you need to _nuke & pave_ the standard Colab runtime. That is, remove everything installed by PIP, the CUDA runtime, whatever else you can think of, and install from the repos. For example, the Python packages need:
!pip freeze > /tmp/all_packages.txt
!pip uninstall -r /tmp/all_packages.txt

Also, the demo demands GPU and will not work in CPU-only mode. I have not tried the TPU runtime, but suspect it will also not work.

Stack trace:

/content/pythia/pythia/.vector_cache/glove.6B.zip: 862MB [01:03, 13.5MB/s]                           
100%|█████████▉| 399163/400000 [00:50<00:00, 7829.19it/s] 
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-9-e64f71f583fa> in <module>()
----> 1 demo = PythiaDemo()

8 frames
<ipython-input-8-036a011f5715> in __init__(self)
     40   def __init__(self):
     41     self._init_processors()
---> 42     self.pythia_model = self._build_pythia_model()
     43     self.detection_model = self._build_detection_model()
     44     self.resnet_model = self._build_resnet_model()

<ipython-input-8-036a011f5715> in _build_pythia_model(self)
     82 
     83     model.load_state_dict(state_dict)
---> 84     model.to(""cuda"")
     85     model.eval()
     86 

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)
    379             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
    380 
--> 381         return self._apply(convert)
    382 
    383     def register_backward_hook(self, hook):

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)
    185     def _apply(self, fn):
    186         for module in self.children():
--> 187             module._apply(fn)
    188 
    189         for param in self._parameters.values():

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)
    185     def _apply(self, fn):
    186         for module in self.children():
--> 187             module._apply(fn)
    188 
    189         for param in self._parameters.values():

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)
    185     def _apply(self, fn):
    186         for module in self.children():
--> 187             module._apply(fn)
    188 
    189         for param in self._parameters.values():

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in _apply(self, fn)
    185     def _apply(self, fn):
    186         for module in self.children():
--> 187             module._apply(fn)
    188 
    189         for param in self._parameters.values():

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py in _apply(self, fn)
    115     def _apply(self, fn):
    116         ret = super(RNNBase, self)._apply(fn)
--> 117         self.flatten_parameters()
    118         return ret
    119 

/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py in flatten_parameters(self)
    111                     all_weights, (4 if self.bias else 2),
    112                     self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,
--> 113                     self.batch_first, bool(self.bidirectional))
    114 
    115     def _apply(self, fn):

RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",neutral,negative
192,"warnings.warn('show==False and out_file is not specified, only",open-mmlab/mmdetection,closed,,wangning7149,hellock,2020-06-04 04:51:24,2020-06-04 05:58:28,2.0,,2893.0,630509247.0,"已经训练好模型了，我在测试一张图片时，用命令： python demo/image_demo.py demo/03001-0002.jpg configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py epoch_32.pth --device cpu
出现了错误：

/home/wn/projects/mmdetection-2.0.0/mmdet/apis/inference.py:95: UserWarning: We set use_torchvision=True in CPU mode.
  warnings.warn('We set use_torchvision=True in CPU mode.')
/opt/conda/conda-bld/pytorch_1587428091666/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of addcmul is deprecated:
        addcmul(Tensor input, Number value, Tensor tensor1, Tensor tensor2, *, Tensor out)
Consider using one of the following signatures instead:
        addcmul(Tensor input, Tensor tensor1, Tensor tensor2, *, Number value, Tensor out)
/home/wn/projects/mmdetection-2.0.0/mmdet/models/detectors/base.py:233: UserWarning: show==False and out_file is not specified, only result image will be returned
  warnings.warn('show==False and out_file is not specified, only '

请问怎么解决啊？

",neutral,positive
193,how to make my own binary file for verification sets,deepinsight/insightface,closed,,BUAA-21Li,nttstar,2018-06-20 07:04:06,2018-06-20 09:22:31,1.0,,258.0,333948531.0,"I have a database and wanna make it as verification sets during training network.Could you tell me how to make it  .bin file? just as lfw.bin,  cfp_ff.bin, cfp_fp.bin, agedb_30.bin.THANKS!",neutral,neutral
194,population support,facebookresearch/EGG,closed,"mbaroni
robertodessi
eugene-kharitonov",mbaroni,robertodessi,2020-11-13 13:36:41,2021-02-01 10:48:53,1.0,major-new-functionality#upcoming,151.0,742454949.0,"Support the possibility of playing games with multiple senders and receivers.
",positive,positive
195,RPN Predictions fail,matterport/Mask_RCNN,open,,ghost,,2020-02-21 03:50:42,,0.0,,2011.0,568719094.0,"Help! I'm begginer with AI. I run the DFG trafic sign dataset with COCO format, run the inpect_model, and i dound when RPN Prediction failed. How can i fix it?
![Capture1](https://user-images.githubusercontent.com/42407667/75002703-e2cb7180-5497-11ea-9502-627a9160ec42.PNG)
![Capture2](https://user-images.githubusercontent.com/42407667/75002718-eeb73380-5497-11ea-8d24-960a99e82e7b.PNG)
![Capture3](https://user-images.githubusercontent.com/42407667/75002722-efe86080-5497-11ea-85ca-a41ba14c9f87.PNG)
",negative,negative
196,Question about the gradient equation,hufu6371/DORN,open,,dongda118,,2020-01-16 08:12:46,,0.0,,38.0,550643695.0,"Hi, dear all,
     when I tried to train the net, i met a problem about the gradient  calculation with Equation(4). I tried to write the LossLayer, but I don't know how to write the backward. which layer's output is the x(w,h)  in the equation? 

Thanks,",neutral,neutral
197,Five facial landmarks are missing in annotations ,deepinsight/insightface,open,,look4pritam,,2019-11-10 06:30:13,,0.0,,973.0,520561057.0,"Face bounding boxes are present but five facial landmarks are missing in annotations in download  [link](https://www.dropbox.com/s/7j70r3eeepe4r2g/retinaface_gt_v1.1.zip?dl=0)
Are you going to release ground truth for five facial landmarks for WIDER Face, which you have used in paper?",neutral,neutral
198,how to obtain models/waveglow_256channels_v4.pt for the Inference demo?,NVIDIA/flowtron,closed,,erekgit,erekgit,2020-05-15 01:11:07,2020-05-15 05:57:42,7.0,,3.0,618627956.0,"I'm trying to locate the approprate waveglow_256channels_v4.pt for flowtron inference demo.  i googled and found a colab notebook and tried using it:

""%%bash
wget -N  -q https://raw.githubusercontent.com/yhgon/colab_utils/master/gfile.py
python gfile.py -u 'https://drive.google.com/open?id=1ZesPPyRRKloltRIuRnGZ2LIUEuMSVjkI' -f 'mellotron_libritts.pt'
python gfile.py -u 'https://drive.google.com/open?id=1Rm5rV5XaWWiUbIpg5385l5sh68z2bVOE' -f 'waveglow_256channels_v4.pt'""

---
Results:

python inference.py -c config.json -f models/flowtron_ljs.pt -w models/waveglow_256channels_v4.pt -t ""It is well know that deep generative models have a deep latent space!"" -i 0

---

Loaded checkpoint 'models/flowtron_ljs.pt')
Number of speakers : 1
Traceback (most recent call last):
  File ""inference.py"", line 122, in <module>
    args.n_frames, args.sigma, args.seed)
  File ""inference.py"", line 80, in infer
    audio = waveglow.infer(mels.half(), sigma=0.8).float()
  File ""tacotron2/waveglow\glow.py"", line 276, in infer
    output = self.WN[k]((audio_0, spect))
  File ""F:\Users\Erik.DESKTOP-E5E1V83\anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""tacotron2/waveglow\glow.py"", line 161, in forward
    self.cond_layers[i](spect),
  File ""F:\Users\Erik.DESKTOP-E5E1V83\anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 594, in __getattr__
    type(self).__name__, name))
AttributeError: 'WN' object has no attribute 'cond_layers'
",neutral,neutral
199,validation too slow,open-mmlab/mmdetection,closed,jbwang1997,yyccphus,yyccphus,2022-10-12 07:38:06,2022-10-12 16:10:07,2.0,,9005.0,1405724124.0,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 💬 Describe the reimplementation questions

I am trying to finetuning mask rcnn in my custom dataset with single GPU, but I found that it takes about 20mins to training and  75mins to validation(0.3~0.5 tasks/s), validation process was too slow, my logs are provided as below, is there any problem of my set up?
```
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29069683_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0
PyTorch: 1.8.2+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.2+cu111
OpenCV: 4.5.5
MMCV: 1.4.8
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.24.1+
------------------------------------------------------------

2022-10-12 13:06:32,263 - mmdet - INFO - Distributed training: False
2022-10-12 13:06:32,568 - mmdet - INFO - Config:
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=1,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=1,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=6,
    workers_per_gpu=6,
    train=dict(
        type='CocoDataset',
        ann_file='dataset/train/annotations.json',
        img_prefix='dataset/train/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ],
        classes=('car', )),
    val=dict(
        type='CocoDataset',
        ann_file='dataset/val/annotations.json',
        img_prefix='dataset/val/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('car', )),
    test=dict(
        type='CocoDataset',
        ann_file='dataset/test/annotations.json',
        img_prefix='dataset/test/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('car', )))
evaluation = dict(metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.002, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=50,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 8)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=8)
classes = ('car', )
fp16 = dict(loss_scale=512.0)
work_dir = './work_dirs/mask_rcnn_r50_fpn_1x_uvsd'
auto_resume = False
gpu_ids = [2]

2022-10-12 13:06:32,569 - mmdet - INFO - Set random seed to 1547918333, deterministic: False
2022-10-12 13:06:33,072 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}
2022-10-12 13:06:33,433 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2022-10-12 13:06:33,478 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2022-10-12 13:06:33,490 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

neck.lateral_convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.lateral_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.lateral_convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.lateral_convs.3.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

rpn_head.rpn_conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_head.rpn_conv.bias - torch.Size([256]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_head.rpn_cls.weight - torch.Size([3, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_head.rpn_cls.bias - torch.Size([3]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_head.rpn_reg.weight - torch.Size([12, 256, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

rpn_head.rpn_reg.bias - torch.Size([12]): 
NormalInit: mean=0, std=0.01, bias=0 

roi_head.bbox_head.fc_cls.weight - torch.Size([2, 1024]): 
NormalInit: mean=0, std=0.01, bias=0 

roi_head.bbox_head.fc_cls.bias - torch.Size([2]): 
NormalInit: mean=0, std=0.01, bias=0 

roi_head.bbox_head.fc_reg.weight - torch.Size([4, 1024]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.fc_reg.bias - torch.Size([4]): 
NormalInit: mean=0, std=0.001, bias=0 

roi_head.bbox_head.shared_fcs.0.weight - torch.Size([1024, 12544]): 
XavierInit: gain=1, distribution=uniform, bias=0 

roi_head.bbox_head.shared_fcs.0.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

roi_head.bbox_head.shared_fcs.1.weight - torch.Size([1024, 1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

roi_head.bbox_head.shared_fcs.1.bias - torch.Size([1024]): 
XavierInit: gain=1, distribution=uniform, bias=0 

roi_head.mask_head.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

roi_head.mask_head.convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

roi_head.mask_head.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

roi_head.mask_head.convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

roi_head.mask_head.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

roi_head.mask_head.convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

roi_head.mask_head.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

roi_head.mask_head.convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of MaskRCNN  

roi_head.mask_head.upsample.weight - torch.Size([256, 256, 2, 2]): 
Initialized by user-defined `init_weights` in FCNMaskHead  

roi_head.mask_head.upsample.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in FCNMaskHead  

roi_head.mask_head.conv_logits.weight - torch.Size([1, 256, 1, 1]): 
Initialized by user-defined `init_weights` in FCNMaskHead  

roi_head.mask_head.conv_logits.bias - torch.Size([1]): 
Initialized by user-defined `init_weights` in FCNMaskHead  
2022-10-12 13:06:46,591 - mmdet - INFO - Automatic scaling of learning rate (LR) has been disabled.
2022-10-12 13:06:47,500 - mmdet - INFO - Start running, host: user@user-Super-Server, work_dir: /home/user/yyc/UAV_mmdetection/work_dirs/mask_rcnn_r50_fpn_1x_uvsd
2022-10-12 13:06:47,500 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-10-12 13:06:47,501 - mmdet - INFO - workflow: [('train', 8)], max: 12 epochs
2022-10-12 13:06:47,502 - mmdet - INFO - Checkpoints will be saved to /home/user/mmdetection/work_dirs/mask_rcnn_r50_fpn_1x_**** by HardDiskBackend.
```

### Environment

```
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29069683_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0
PyTorch: 1.8.2+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.2+cu111
OpenCV: 4.5.5
MMCV: 1.4.8
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.24.1+
```

### Expected results

_No response_

### Additional information

_No response_",neutral,neutral
200,How to get Polygon and bounding box as Mask-RCNN outputs?,open-mmlab/mmdetection,closed,RangiLyu,sarmientoj24,RangiLyu,2022-08-11 16:29:49,2022-08-12 06:53:51,1.0,,8546.0,1336205023.0,,neutral,neutral
201,ModuleNotFoundError: No module named 'tf_lib',guochengqian/PU-GCN,closed,,GabauerDavid,guochengqian,2021-04-19 13:12:15,2021-04-19 13:37:06,3.0,,2.0,861295455.0,"Dear Guochenq Qian,

I have set up the environment and installed all necessary packages however I get the following error:
ModuleNotFoundError: No module named 'tf_lib'
I looked through the whole repository but could not find the tf_lib package which should include all methods.
Could it be that this script has not been uploaded yet?

Kind regards,
David",neutral,neutral
202, ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.,matterport/Mask_RCNN,closed,,mlsy123,mlsy123,2021-12-09 08:25:19,2021-12-16 00:13:51,2.0,,2737.0,1075288286.0,"I have ran into an error while running this code below from . Any help would be appreciated

# Code
%Create model object in inference mode.
model = modellib.MaskRCNN(mode=""inference"", model_dir=MODEL_DIR, config=config)
% Load weights trained on MS-COCO
model.load_weights(COCO_MODEL_PATH, by_name=True)

# Error
ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.

I have tried this method but I'm still getting the error
 
![image](https://user-images.githubusercontent.com/94586300/145360017-ff8db264-bb8d-45fc-b3b8-d35d16aa7500.png)
",positive,positive
203,How to find the  clean images of DND/NC12/Nam,GuoShi28/CBDNet,open,,xiaojuan123,,2019-12-23 02:49:46,,1.0,,35.0,541546703.0,"don't have the clean  images of DND/NC12/Nam,how to calculate the PSNR value.",neutral,neutral
204,about sigmoid activation use in model:bbox_head,open-mmlab/mmdetection,closed,,dfgan,hhaAndroid,2019-08-08 02:30:27,2021-04-09 01:49:33,1.0,,1145.0,478219736.0,"I use the sigmoid instead of the softmax in bbox_head.loss_cls, But the ap is 0 when i test , the  loss keeps decreasing when i training. Did I tuning parameter is wrong?",neutral,negative
205,Error for VFNet pretrained checkpoint file,open-mmlab/mmdetection,closed,ZwwWayne,seriousran,ZwwWayne,2021-02-14 03:29:07,2021-02-16 06:32:53,1.0,,4627.0,807879932.0,"```
import torch, torchvision
import mmdet

from mmdet.apis import inference_detector, init_detector, show_result_pyplot

config = 'configs/vfnet/vfnet_x101_32x4d_fpn_mdconv_c3-c5_mstrain_2x_coco.py'
checkpoint = 'weights//vfnet_x101_64x4d_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-b5f6da5e.pth'

model = init_detector(config, checkpoint, device='cuda:1')
```

```
Error: The model and loaded state dict do not match exactly

size mismatch for backbone.layer1.0.conv1.weight: copying a param with shape torch.Size([256, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).
...
```
I would appreciate it if you make sure that the config and checkpoint files match.

I've downloaded weights from here: https://github.com/open-mmlab/mmdetection/tree/master/configs/vfnet

<hr>

**Environment**

sys.platform: linux
Python: 3.8.5 (default, Sep  4 2020, 07:30:14) [GCC 7.3.0]
CUDA available: True
GPU 0,1: Quadro RTX 6000
CUDA_HOME: /usr
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.7.1
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.8.2
OpenCV: 4.4.0
MMCV: 1.2.6
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.0
MMDetection: 2.9.0+4680122",neutral,positive
206,Problem with run sota/lm1b.sh.,kimiyoung/transformer-xl,open,,BogdanDidenko,,2019-02-21 11:38:29,,1.0,,40.0,412881787.0,"1. python data_utils.py receive `--dataset=enwik8` but dataset name - `lm1b`
https://github.com/kimiyoung/transformer-xl/blob/master/tf/sota/lm1b.sh#L30
2 If I change `dataset` parameter to `lm1b` and run `sh sota/lm1b.sh` I have this error:
```
Assign requires shapes of both tensors to match. lhs shape= [153470,20] rhs shape= [153472,20]
	 [[node save/Assign_5 (defined at /Users/path/transformer-xl/tf/train_gpu.py:419) ]]
```
",neutral,negative
207,Beginner Question: Adding more functions to Sempre,percyliang/sempre,closed,,SouLeo,SouLeo,2019-12-02 21:57:02,2019-12-13 02:57:32,4.0,,197.0,531565069.0,"Hello, I'm new to Sempre, and I want to try and construct a grammar for robotic tasks. I'd like to add additional functions such as 

move
turn
rotate
pause
stop
continue

in addition to the basic arithmetic formula made available to us. I was wondering, what is the best way to go about doing that? 

Thank you!",neutral,positive
208,pretrained  model,JiahuiYu/generative_inpainting,closed,,Haoru,Haoru,2020-03-24 05:20:44,2020-03-24 05:21:03,0.0,,416.0,586700526.0,,neutral,neutral
209,Session Management Error,open-mmlab/mmdetection,closed,,FrankXinqi,hellock,2019-10-31 12:59:14,2020-04-24 05:59:47,2.0,,1611.0,515413872.0,"Hi, I try to run the first example, as
`python tools/test.py configs/faster_rcnn_r50_fpn_1x.py checkpoints/faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth --show`
But, get an session management error, as
`loading annotations into memory...`
`Done (t=0.60s)`
`creating index...`
`index created!`
`[                              ] 0/5000, elapsed: 0s, ETA:Qt: Session management error: None of the authentication protocols specified are supported`",neutral,negative
210,About the Generative Prepend Models,facebookresearch/EmpatheticDialogues,closed,,yuboxie,EricMichaelSmith,2020-01-30 17:13:28,2020-04-10 12:18:45,3.0,,22.0,557635682.0,"Hi,

I was wondering if the generative prepend models (EmoPrepend and TopicPrepend) involve any pre-trained BERT weights? From my understanding, it seems that you first trained the prepend models on Reddit and then fine-tuned them on ED, right? And for prepend models, you only experimented with 4-layer transformers but not 5-layer (denoted as ""Large"" in the paper)?

Another related question would be, when you trained the prepend models on Reddit, you still predicted the labels based on the input context and prepended them in the front, am I correct?

Thanks for your time!
Yubo",neutral,positive
211,compile cphoc.c ,facebookresearch/mmf,open,,linxi1158,,2021-07-12 12:41:07,,3.0,,1011.0,942020519.0,"## ❓ Questions and Help
How to compile cphoc.c file?

 ~/Documents/code/mmf-master/mmf/utils/phoc/src  gcc -c -fPIC `python-config --cflags` cphoc.c
cphoc.c:49:53: warning: format string is not a string literal (potentially insecure)
      [-Wformat-security]
            return PyErr_Format(PyExc_RuntimeError, error_msg);
                                                    ^~~~~~~~~
cphoc.c:49:53: note: treat the string as an argument to avoid this
            return PyErr_Format(PyExc_RuntimeError, error_msg);
                                                    ^
                                                    ""%s"",
cphoc.c:31:13: warning: implicit conversion loses integer precision: 'unsigned long' to
      'int' [-Wshorten-64-to-32]
    int n = strlen(word);
        ~   ^~~~~~~~~~~~
cphoc.c:135:5: error: use of undeclared identifier 'PyModuleDef_HEAD_INIT'
    PyModuleDef_HEAD_INIT,
    ^
cphoc.c:134:27: error: variable has incomplete type 'struct PyModuleDef'
static struct PyModuleDef cphoc = {
                          ^
cphoc.c:134:15: note: forward declaration of 'struct PyModuleDef'
static struct PyModuleDef cphoc = {
              ^
cphoc.c:145:12: error: implicit declaration of function 'PyModule_Create' is invalid in
      C99 [-Werror,-Wimplicit-function-declaration]
    return PyModule_Create(&cphoc);
           ^
cphoc.c:145:12: note: did you mean '_PyModule_Clear'?
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7/moduleobject.h:19:18: note:
      '_PyModule_Clear' declared here
PyAPI_FUNC(void) _PyModule_Clear(PyObject *);
                 ^
2 warnings and 3 errors generated.
 ✘  ~/Documents/code/mmf-master/mmf/utils/phoc/src  ls
cphoc.c
 ~/Documents/code/mmf-master/mmf/utils/phoc/src  ls
cphoc.c
 ~/Documents/code/mmf-master/mmf/utils/phoc/src  vi cphoc.c",neutral,negative
212,Distributed training will hang if log_vars has different length among GPUs,open-mmlab/mmdetection,closed,"RangiLyu
hhaAndroid
ZwwWayne",fingertap,hhaAndroid,2021-11-13 03:15:34,2021-12-29 01:35:11,5.0,bug,6495.0,1052529762.0,"Similar to the [issue in mmseg](https://github.com/open-mmlab/mmsegmentation/issues/1030). Here I just quote:

> In the `_parse_log` function of the `mmseg.segmentors.base.BaseSegmentor`, it attempts to synchronize the loss values among all GPUs. What happened is that in this loop ([line 194](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/segmentors/base.py#L194)):
>
> ```python
> for loss_name, loss_value in log_vars.items():
>    # reduce loss when distributed training
>    if dist.is_available() and dist.is_initialized():
>        loss_value = loss_value.data.clone()
>        dist.all_reduce(loss_value.div_(dist.get_world_size()))
>    log_vars[loss_name] = loss_value.item()
>```
>
>One GPU A does not have a `""roi_acc""` as `loss_name` (suppose it is the last key in `log_vars`). Then this GPU A thinks it has done all its work, and jump out of the loop. Other GPUs with the last `""roi_acc""` will try to call `torch.distributed.all_reduce`, which infinitely waits for the reply from GPU A.

This bug is hard to debug, as it just gets blocked without error messages.",neutral,negative
213,the log of train_completion.py,hansen7/OcCo,open,,zshyang,,2022-03-30 15:54:46,,0.0,,30.0,1186665919.0,"Hi @hansen7 ,
Thanks for sharing this great work with us. 
I have a question regarding the output log during pretraining. The detail is [here](https://gist.github.com/zshyang/08fb47c7f4aa2835da25e6e98349cf51).
The question is, it seems that the loss did not drop too much or its range is varied from 0.03 to 0.04. Did this happen to your pretraining as well?
But visually, I could see some improvements in the `plots` folder.
Best regards",neutral,positive
214,tensorflow,matterport/Mask_RCNN,open,,LY-happy,,2021-11-13 05:53:31,,4.0,,2724.0,1052553639.0,"ModuleNotFoundError: No module named 'tensorflow'
![ff7c28a77d7c98e93e60821e71c1350](https://user-images.githubusercontent.com/90297716/141607587-b33f3234-2b2e-40f5-ad2a-12edc1004355.png)
![eed5804707f041a3475e14392a94951](https://user-images.githubusercontent.com/90297716/141607593-236ab86c-cbfd-4f02-b361-0b5828872cae.png)

but my keras is 2.6.0 tensorflow-gpu is 2.6.0
thanks a lot
",neutral,positive
215,How to continue train pretrained models r100?,deepinsight/insightface,open,,Miracle-doctor,,2020-05-30 18:38:14,,0.0,,1141.0,627793101.0,I need continue trains trained model. How Can I do this?,neutral,neutral
216,Multiple image augmentation for training dataset,matterport/Mask_RCNN,open,,ronykalfarisi,,2018-07-11 17:38:24,,35.0,,768.0,340339836.0,"Dear All and @waleedka ,
I've been using this repo to detect and create masking for crack damage on bridge structures. My training dataset has 850 images and overall I got decent result. As you can see in two images below, the model can detect and segment horizontal and vertical crack well. But it fails in detecting diagonal crack. So, I'm thinking that I could solve this problem if I have more images.
![5](https://user-images.githubusercontent.com/33510059/42589310-9353a67c-850e-11e8-9a2f-7f489202d994.png)
![0](https://user-images.githubusercontent.com/33510059/42589232-53e02fc4-850e-11e8-9c08-e29f679872cf.png)
I noticed that we could use image augmentation in the training stage using the keyword option _augmentation_ as follow

    model.train(dataset_train, dataset_val,
                learning_rate=config.LEARNING_RATE,
                epochs=30,
                layers='all', 
                augmentation = imgaug.augmenters.Sequential([ 
                    imgaug.augmenters.Fliplr(1), 
                    imgaug.augmenters.Flipud(1), 
                    imgaug.augmenters.Affine(rotate=(-45, 45)), 
                    imgaug.augmenters.Affine(rotate=(-90, 90)), 
                    imgaug.augmenters.Affine(scale=(0.5, 1.5))]))

However, from what I understand, this augmentation is applied consecutively to each image. In other words, for each image, the augmentation apply flip LR, and then followed by flip UD, then followed by rotation of -45 and 45, then followed by another rotation of -90 and 90, and lastly followed by scaling with factor 0.5 and 1.5. 

So, my question is, **Is there a way to apply each augmentation separately for each image?** What I meant by this is, I want each augmentation to generate one extra data (and mask) alongside with the original. If this can be achieved, the augmentation will generate 6x total images when I apply 5 image augmentation making the whole dataset contains 5100 images. 

Thank you and I really appreciate the helps.",neutral,neutral
217,how to calculate the flops & params of 'DCNs',open-mmlab/mmdetection,closed,chhluo,Lausen-Ng,Lausen-Ng,2022-03-19 12:27:40,2022-03-20 11:18:27,2.0,awaiting response,7463.0,1174254581.0,"Here, I want to know how to calculate the flops & params of 'DCNs'?
There is an official file 'get_flops.py' to help calculate the flops of the model. However, it does not support the *deformable convolution*.
Does anyone know the solution? Thanks very much!!!!",neutral,neutral
218,roi的问题,DetectionTeamUCAS/R2CNN-Plus-Plus_Tensorflow,open,,ZhengHeZhe,,2020-06-19 02:17:14,,1.0,,71.0,641655605.0,"你好 我想问下，我在训练的时候出现这个问题，基本检测不到目标，看了生成的anchor是没有问题的，但是却出现了这个问题，你知道这是怎么回事吗？改怎么修改呀
![捕获f](https://user-images.githubusercontent.com/52985137/85089750-fdc7ab80-b215-11ea-8498-02f270c7c2ec.jpg)

",neutral,neutral
219,Error when unzip the nuscenes dataset,nutonomy/nuscenes-devkit,closed,,BoPang1996,whyekit-motional,2022-07-22 06:32:06,2022-07-22 06:50:27,4.0,,791.0,1314415236.0,"Hi, thanks for the dataset.

I have trouble when unzipping the v1.0-trainvalxx_blobs.tgz files. I have tried to unzip on Ubuntu with ""tar -xf"" and unzip on Windows with WinRAR, but they all say the file is not complete. I have checked the md5 of all the blobs. They are different from the mentioned ones but the size of the files are the same. I have tried to download the data with wget for 3 times. The results are all the same. Do I do something wrong?
 
![截图](https://user-images.githubusercontent.com/17980676/180377490-7d57e092-2a88-4e86-977e-b6b68ba9187d.PNG)
![截图2](https://user-images.githubusercontent.com/17980676/180377512-80314635-604b-4dfe-9a76-c30b81f7bdbd.PNG)
 ",neutral,positive
220,Do you use all the boxes got from selective search?,gkioxari/RstarCNN,closed,,KnightOfTheMoonlight,KnightOfTheMoonlight,2016-06-03 08:51:13,2016-06-16 02:21:38,1.0,,13.0,158320227.0,"Recently, I found the first boxes got from selective search is the image itself. So have you deleted the first box from selective search, or just use all the boxes got from selective search?
",neutral,neutral
221,Did you provide a method to let use train the model initialized from the model zoo pretrained parameters?,open-mmlab/mmdetection,closed,,CoinCheung,CoinCheung,2019-07-23 08:51:00,2019-07-24 09:31:22,2.0,,1044.0,471566173.0,,neutral,neutral
222,YOLOX TypeError: CocoDataset: _init_() got an unexpected keyword argument 'dataset',open-mmlab/mmdetection,closed,hhaAndroid,UygarUsta,ZwwWayne,2021-11-29 17:21:51,2021-12-16 14:27:26,2.0,,6629.0,1066300857.0,"I am trying to train a YOLOX model with default configuration settings.I have only changed the number of classes according to my dataset and dataset paths inside the cfg.You can see my configuration below ;

optimizer = dict(
    type='SGD',
    lr=0.0025,
    momentum=0.9,
    weight_decay=0.0005,
    nesterov=True,
    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='YOLOX',
    warmup=None,
    by_epoch=False,
    warmup_by_epoch=True,
    warmup_ratio=1,
    warmup_iters=5,
    num_last_epochs=15,
    min_lr_ratio=0.05)
runner = dict(type='EpochBasedRunner', max_epochs=5)
checkpoint_config = dict(interval=5)
log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [
    dict(type='YOLOXModeSwitchHook', num_last_epochs=15, priority=48),
    dict(
        type='SyncRandomSizeHook',
        ratio_range=(10, 20),
        img_scale=(640, 640),
        priority=48),
    dict(type='SyncNormHook', num_last_epochs=15, interval=10, priority=48),
    dict(type='ExpMomentumEMAHook', resume_from=None, priority=49)
]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = './yolox_tiny_8x8_300e_coco_20210806_234250-4ff3b67e.pth'
resume_from = None
workflow = [('train', 1)]
model = dict(
    type='YOLOX',
    backbone=dict(type='CSPDarknet', deepen_factor=0.33, widen_factor=0.375),
    neck=dict(
        type='YOLOXPAFPN',
        in_channels=[96, 192, 384],
        out_channels=96,
        num_csp_blocks=1),
    bbox_head=dict(
        type='YOLOXHead', num_classes=3, in_channels=96, feat_channels=96),
    train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),
    test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))
data_root = 'data/coco/'
dataset_type = 'CocoDataset'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
img_scale = (640, 640)
train_pipeline = [
    dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
    dict(
        type='RandomAffine',
        scaling_ratio_range=(0.5, 1.5),
        border=(-320, -320)),
    dict(
        type='PhotoMetricDistortion',
        brightness_delta=32,
        contrast_range=(0.5, 1.5),
        saturation_range=(0.5, 1.5),
        hue_delta=18),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Resize', keep_ratio=True),
    dict(type='Pad', pad_to_square=True, pad_val=114.0),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
train_dataset = dict(
    type='MultiImageMixDataset',
    dataset=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_train2017.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile', to_float32=True),
            dict(type='LoadAnnotations', with_bbox=True)
        ],
        filter_empty_gt=False),
    pipeline=[
        dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
        dict(
            type='RandomAffine',
            scaling_ratio_range=(0.5, 1.5),
            border=(-320, -320)),
        dict(
            type='PhotoMetricDistortion',
            brightness_delta=32,
            contrast_range=(0.5, 1.5),
            saturation_range=(0.5, 1.5),
            hue_delta=18),
        dict(type='RandomFlip', flip_ratio=0.5),
        dict(type='Resize', keep_ratio=True),
        dict(type='Pad', pad_to_square=True, pad_val=114.0),
        dict(
            type='Normalize',
            mean=[123.675, 116.28, 103.53],
            std=[58.395, 57.12, 57.375],
            to_rgb=True),
        dict(type='DefaultFormatBundle'),
        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
    ],
    dynamic_scale=(640, 640))
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(416, 416),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Pad', size=(416, 416), pad_val=114.0),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        dataset=dict(
            type='CocoDataset',
            ann_file=
            '/home/ai/Downloads/detr/Hands_Arm/annotations/instances_train2017.json',
            img_prefix='/home/ai/Downloads/detr/Hands_Arm/train2017',
            pipeline=[
                dict(type='LoadImageFromFile', to_float32=True),
                dict(type='LoadAnnotations', with_bbox=True)
            ],
            filter_empty_gt=False),
        pipeline=[
            dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
            dict(
                type='RandomAffine',
                scaling_ratio_range=(0.5, 1.5),
                border=(-320, -320)),
            dict(
                type='PhotoMetricDistortion',
                brightness_delta=32,
                contrast_range=(0.5, 1.5),
                saturation_range=(0.5, 1.5),
                hue_delta=18),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='Resize', keep_ratio=True),
            dict(type='Pad', pad_to_square=True, pad_val=114.0),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        dynamic_scale=(640, 640),
        img_prefix='/home/ai/Downloads/detr/Hands_Arm/train2017',
        classes=('Arm', 'EmptyHand', 'FullHand'),
        ann_file=
        '/home/ai/Downloads/detr/Hands_Arm/annotations/instances_train2017.json'
    ),
    val=dict(
        type='CocoDataset',
        ann_file=
        '/home/ai/Downloads/detr/Hands_Arm/annotations/instances_val2017.json',
        img_prefix='/home/ai/Downloads/detr/Hands_Arm/val2017',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(416, 416),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(type='Pad', size=(416, 416), pad_val=114.0),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='DefaultFormatBundle'),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Arm', 'EmptyHand', 'FullHand')),
    test=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(416, 416),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(type='Pad', size=(416, 416), pad_val=114.0),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='DefaultFormatBundle'),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
interval = 10
evaluation = dict(interval=5, metric='bbox')
classes = ('Arm', 'EmptyHand', 'FullHand')
seed = 0
gpu_ids = range(0, 1)
work_dir = './yolox_checkpoints'
total_epochs = 5

Thanks in advance !",neutral,positive
223,Seems unable to utilize multiple GPUs,DeepGraphLearning/GNN-QE,open,,slnxyr,,2022-10-13 05:28:33,,1.0,,4.0,1407165997.0,"Hi there.

I have tried running this code on one of my machine with two RTX3090 GPUs (GPU memory 24GB for each)

python -m torch.distributed.launch --nproc_per_node=2 script/run.py -c config/fb15k237.yaml --gpus [0,1]

However, I encountered the error saying that incompatible function arguments.

![image](https://user-images.githubusercontent.com/53633782/195509065-08d118bf-1053-4ca9-bc74-3d4a6d29340a.png)

![image](https://user-images.githubusercontent.com/53633782/195509188-965b3308-151c-4013-be48-d9e03391f34f.png)

Could you help me solve this problem? Is there something wrong with my setups? 

",neutral,negative
224,Add content preservation evaluation metric,vineetjohn/linguistic-style-transfer,closed,vineetjohn,vineetjohn,vineetjohn,2018-03-29 21:55:38,2018-03-30 16:20:18,0.0,enhancement,37.0,309916366.0,,neutral,neutral
225,AffineTransform,tensorflow/addons,closed,,fisakhan,seanpmorgan,2019-09-10 16:02:44,2019-09-10 23:48:04,1.0,,496.0,491765134.0,"**Describe the feature and the current behavior/state.**

**Relevant information**
- Are you willing to contribute it (yes/no):
- Are you willing to maintain it going forward? (yes/no):
- Is there a relevant academic paper? (if so, where):yes
- Is there already an implementation in another framework? (if so, where):https://en.wikipedia.org/wiki/Affine_transformation
- Was it part of tf.contrib? (if so, where):tf.keras.preprocessing.image.apply_affine_transform

**Which API type would this fall under (layer, metric, optimizer, etc.)**

**Who will benefit with this feature?**

**Any other info.**
",neutral,neutral
226,ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory,open-mmlab/mmdetection,closed,v-qjqs,Chan-Sun,Chan-Sun,2020-12-16 11:14:54,2020-12-24 02:20:03,6.0,,4312.0,768735725.0,"I was trying to run /demo/image_demo.py to valid installation. but got errors as follow:

>Traceback (most recent call last):
>File ""image_demo.py"", line 3, in <module>
>from mmdet.apis import inference_detector, init_detector, show_result_pyplot
>File ""/home/hustwen/mmdetection/mmdet/apis/__init__.py"", line 1, in <module>
>from .inference import (async_inference_detector, inference_detector,
>File ""/home/hustwen/mmdetection/mmdet/apis/inference.py"", line 7, in <module>
>from mmcv.ops import RoIPool
>File ""/home/hustwen/anaconda3/envs/mmdet/lib/python3.7/site-packages/mmcv/ops/__init__.py"", line 1, in <module>
>from .bbox import bbox_overlaps
>File ""/home/hustwen/anaconda3/envs/mmdet/lib/python3.7/site-packages/mmcv/ops/bbox.py"", line 3, in <module>
>ext_module = ext_loader.load_ext('_ext', ['bbox_overlaps'])
>File ""/home/hustwen/anaconda3/envs/mmdet/lib/python3.7/site-packages/mmcv/utils/ext_loader.py"", line 11, in load_ext
>ext = importlib.import_module('mmcv.' + name)
>File ""/home/hustwen/anaconda3/envs/mmdet/lib/python3.7/importlib/__init__.py"", line 127, in import_module
>return _bootstrap._gcd_import(name[level:], package, level)
>ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory

I have upgrade CUDA from 9.0 to 10.1, and rewrite the environment PATH.

Pytorch(pip install from pytorch.org)、CUDA、mmcv、mmdet and gcc version (on Ubuntu 16.04): 

> pytorch                   1.7.1            py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch
> torchaudio              0.7.2           py37                                                         pytorch
> torchvision              0.8.2           py37_cu101                                            pytorch

> nvcc: NVIDIA (R) Cuda compiler driver
> Copyright (c) 2005-2019 NVIDIA Corporation
> Built on Sun_Jul_28_19:07:16_PDT_2019
> Cuda compilation tools, release 10.1, V10.1.243

> mmcv-full                 1.2.1                     <pip>

> mmdet __version__ = '2.7.0'

> gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12)",neutral,positive
227,[Question] The bidirectional cycle-consistency losses advantage compared to pix2pix?,junyanz/BicycleGAN,closed,,lxj616,lxj616,2018-02-02 03:52:38,2018-02-02 04:41:29,2.0,,12.0,293765427.0,"Hi, I am really shocked by the awesome work you guys achieved, but yet I get confused about ""How BicycleGAN's result differs from pix2pix's result and cyclegan's""

1. Does BicycleGAN give better result at edge2shoes than pix2pix with same training sets and epochs ?
2. Does BicycleGAN trains faster using bidirectional cycle-consistency losses ?
3. In what condition BicycleGAN is better than ... others , and when does not ?",negative,negative
228,fastBPE step in embed.py fails if no tokenization,facebookresearch/LASER,open,,bricksdont,,2020-01-27 15:30:45,,0.0,,121.0,555652007.0,"If there is no tokenization with `embed.py`, running the script fails:

```
cat input.de | python tools/laser/source/embed.py --encoder tools/laser/models/bilstm.93langs.2018-12-26.pt --bpe-codes tools/laser/models/93langs.fcodes --output embedded.de --verbose
 - Encoder: loading /net/cephfs/scratch/mathmu/laser-contra/tools/laser/models/bilstm.93langs.2018-12-26.pt
 - fast BPE: processing
Loading codes from /net/cephfs/scratch/mathmu/laser-contra/tools/laser/models/93langs.fvocab ...
fast: fastBPE/fastBPE.hpp:455: void fastBPE::readCodes(const char*, std::unordered_map<std::pair<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >, unsigned int, fastBPE::pair_hash>&, std::unordered_map<std::__cxx11::basic_string<char>, std::pair<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> > >&): Assertion `splits.size() == 3' failed.
Aborted (core dumped)
```

This is due to the fact that the [input file is `sys.stdin` before tokenization](https://github.com/facebookresearch/LASER/blob/8b053348af22a0038db495616023a7341a4a614f/source/embed.py#L366). The tokenizing step can handle input from STDIN, but the fastBPE step tries to execute the following command:

    ./fast applybpe [TWO SPACES HERE] /tmp/tmpgh9sgiy_/bpe \
        tools/laser/models/93langs.fcodes \
        tools/laser/models/93langs.fvocab

While the general recipe is

    ./fast applybpe output input codes vocab

The temporary output file `/tmp/tmpgh9sgiy_/bpe` is mistaken for the codes file, which makes this assertion fail.",neutral,negative
229,Question: Can we use LASER for sentence entailment?,facebookresearch/LASER,closed,,rajasen2019,hoschwenk,2019-02-07 09:33:26,2019-02-11 12:18:29,5.0,,34.0,407608853.0,"I tried sentence embedding example, and I observed that it doesn't understand sentences with opposite meaning. 

Is there any way to tune it or use different models?",neutral,neutral
230,"What format is used when extracting features, RGB or BGR?",ChenRocks/UNITER,closed,,VisualJoyce,VisualJoyce,2021-06-22 16:31:48,2021-06-23 16:10:37,1.0,,77.0,927429266.0,"In the feature extraction code, I notice that although `Image` is used to get `img`, but `im` read from `cv2` is used later. It's confusing that `img = Image.open(im_file).convert('RGB')` is used, but no  `cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB)`.
```python
    try:
        img = Image.open(im_file).convert('RGB')
    except Exception as e:
        print(e)
        print(""Corrupted image failed with Image.open: %s"" % (im_file_name))
        return corrupted_im_return

    try:
        im = cv2.imread(im_file)
        if im is None:
            print(""Corrupted image failed with cv2: %s"" % (im_file))
            return corrupted_im_return
    except Exception as e:
        print(e)
        print(""Corrupted image failed with cv2: %s"" % (im_file))
        return corrupted_im_return

    try:
        print(""Processing image_file: %s."" % (im_file_name))
        scores, boxes, attr_scores, rel_scores = im_detect(net, im)
        # Keep the original boxes
        # don't worry about the regresssion bbox outputs
        rois = net.blobs['rois'].data.copy()
        # unscale back to raw image space
        blobs, im_scales = _get_blobs(im, None)

        cls_boxes = rois[:, 1:5] / im_scales[0]

        cls_prob = net.blobs['cls_prob'].data
        pool5 = net.blobs['pool5_flat'].data
    except:
        print(""Got exception when processing image_file: %s."" % (im_file))
        return corrupted_im_return
```

So, is RGB or BGR that we need to use for this step?",neutral,negative
231,Data Format,ChenRocks/fast_abs_rl,closed,,Scagin,ChenRocks,2018-06-06 06:15:58,2018-07-14 04:07:48,2.0,,1.0,329724667.0,I read the source code and found that the data format is JSON. How it is specifically constructed? I want to use my own data. What do I need to do?,neutral,neutral
232,Benchmark Evaluation,Arthur151/ROMP,closed,,zhLawliet,zhLawliet,2021-09-26 07:12:04,2021-09-26 07:46:19,2.0,,77.0,1007302898.0,"你好，执行Benchmark Evaluation的代码的时候，输出的数值和git上不一样，差异很大，不知道你这边有没有确认这版代码：
cd ROMP
CUDA_VISIBLE_DEVICES=0 python romp/lib/evaluation/collect_3DPW_results.py --configs_yml=configs/eval_3dpw_challenge.yml
我这边执行的结果：
![image](https://user-images.githubusercontent.com/17424385/134797439-b79120c0-22cd-410d-b81f-087a5b177070.png)

我自己进行了一下排查：
代码模型的加载正确性验证：试了一下这版代码的下面两个执行脚本，效果是OK的。
sh scripts/image.sh
sh scripts/video.sh

未公布训练代码之前的 commit 7400b34d1d69112625a2165c78b5c754943651c0
Benchmark Evaluation，数值也是ok的。但是当前这个 commit 248660608d2de85d635122a1bb75ef4988b9947b的执行结果
不对，不知道哪里是不是有问题。
",neutral,neutral
233,Unable to install nuscenes-devkit via pip,nutonomy/nuscenes-devkit,closed,,mtshikomba,holger-motional,2019-11-09 10:53:56,2019-11-10 03:31:20,7.0,,253.0,520388120.0,"I had issues installing openCV via pip so i was forced to install it via the alternative (by building my own), when i do a `pip install -r setup/requirements.txt` i get the error below: 

```
ERROR: Could not find a version that satisfies the requirement opencv-python (from versions: none)
ERROR: No matching distribution found for opencv-python
```

I was forced to removed it from my requirements file and everything else installed fine. However when i do `pip install nuscenes-devkit` the same error above shows up.

How can I bypass this? ",neutral,negative
234,RuntimeError: Sizes of tensors must match except in dimension 3. Got 348 and 347,SeungjunNah/DeepDeblur-PyTorch,closed,,ziyadn,SeungjunNah,2020-06-19 02:42:54,2020-07-22 04:26:43,3.0,,3.0,641663760.0,"Helo @SeungjunNah , Thanks for your great works on this great repository.

But I have some problem here, I have tried to run demo from your repo (I have python 3.8.3), Using the following steps:

1. I clone your repositories, create conda environtment install pytorch 1.5.0 using `conda install -c pytorch pytorch`
2. Install all the requirements, download pretrained model
3. Run using the following command line  `python main.py --demo true --save_dir GOPRO_L1_amp --demo_input_dir INPUT_DIR --demo_output_dir OUTPUT_DIR`

then occurs this errors:
![following Error](https://user-images.githubusercontent.com/39609060/85090549-9a3a7f80-b20f-11ea-9793-98d1a3a43f2a.png)

after I forcing the dimension using some modification line in [MSResNet.py](https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/model/MSResNet.py#L59) like this:
 
```python
input_s = input_pyramid[-1]
for s in scales:    # [2, 1, 0]
      output_pyramid[s] = self.body_models[s](input_s)
      if s > 0:
         up_feat = self.conv_end_models[s](output_pyramid[s])
         bound3 = input_pyramid[s-1].size()[3]
         bound2 = input_pyramid[s-1].size()[2]
         input_s = torch.cat((input_pyramid[s-1], up_feat[:, :, :bound2, :bound3]), 1)
```

I can run the demo scripts, but got the different result with yours as the following: 
![sample22](https://user-images.githubusercontent.com/39609060/85090971-9a874a80-b210-11ea-8899-a20ce20d3460.png)

The first image (top position) is my result, and the second image (bottom position) is yours. Mine is a little bit blurred than yours, can you figure out why? and can you handdle my errors? thank you for your reply 

",neutral,positive
235,Jetson AGX Xavier 0.15.0 build requires std=c++14 in the crosstool chain configuration,tensorflow/addons,closed,,girgink,bhack,2021-11-20 07:39:49,2021-11-28 11:29:47,4.0,,2612.0,1059086070.0,"**System information**
- Jetson AGX Xavier (ARMv8) custom L4T based on Ubuntu 20.04
- TensorFlow 2.7.0 (source)
- TensorFlow-Addons 0.15.0 (source)
- Python 3.8
- Is GPU used? Yes

**Describe the bug**
Because Jetson AGX Xavier has CUDA 10.2, the system includes `build_deps/toolchains/gpu/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl`, which only defines rules for C++11. However TF 2.7.0 uses C++14 (e.g. in `tensorflow/core/framework/tensor_types.h`, `MaybeWith32BitIndexingImpl<Eigen::GpuDevice>.operator()`). Therefore build fails.

**Code to reproduce the issue**

```
TF_NEED_CUDA=""1"" \
TF_CUDA_VERSION=""10.2"" \
TF_CUDNN_VERSION=""8"" \
CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" \
python ./configure.py

bazel build build_pip_pkg
```

**Other info / logs**

Replacing all `c++11` with `c++14` in `build_deps/toolchains/gpu/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` solved the issue. Just in case, I also replaced all `c++11` with `c++14` in `build_deps/toolchains/gpu/crosstool/cc_toolchain_config.bzl.tpl`. I see that CUDA 11 configuration files include C++14 rules. It can be good to add them also other CUDA configurations.
",neutral,negative
236,How to define the load_mask according to my json format ?,matterport/Mask_RCNN,closed,,hamzanaeem1999,hamzanaeem1999,2021-06-22 07:29:30,2021-06-23 10:57:08,0.0,,2609.0,926921696.0,"I have the annotations in the below format , I am getting some errors . How to define the load mask and my json looks like :


{""_via_settings"":{""ui"":{""annotation_editor_height"":25,""annotation_editor_fontsize"":0.8,""leftsidebar_width"":18,""image_grid"":{""img_height"":80,""rshape_fill"":""none"",""rshape_fill_opacity"":0.3,""rshape_stroke"":""yellow"",""rshape_stroke_width"":2,""show_region_shape"":true,""show_image_policy"":""all""},""image"":{""region_label"":""__via_region_id__"",""region_color"":""__via_default_region_color__"",""region_label_font"":""10px Sans"",""on_image_annotation_editor_placement"":""NEAR_REGION""}},""core"":{""buffer_size"":18,""filepath"":{},""default_filepath"":""""},""project"":{""name"":""Validation_Annotations_New""}},""_via_img_metadata"":{""19Apple.jpg4222"":{""filename"":""19Apple.jpg"",""size"":4222,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":31,""y"":43,""width"":162,""height"":154},""region_attributes"":{""class"":{""apple"":true}}}],""file_attributes"":{}},""19_banana.jpg4849"":{""filename"":""19_banana.jpg"",""size"":4849,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":27,""y"":31,""width"":207,""height"":125},""region_attributes"":{""class"":{""banana"":true}}}],""file_attributes"":{}},""20Apple.jpg3368"":{""filename"":""20Apple.jpg"",""size"":3368,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":91,""y"":38,""width"":116,""height"":130},""region_attributes"":{""class"":{""apple"":true}}}],""file_attributes"":{}},""21Apple.jpg36817"":{""filename"":""21Apple.jpg"",""size"":36817,""regions"":[{""shape_attributes"":{""name"":""rect"",""x"":113,""y"":62,""width"":177,""height"":177},""region_attributes"":{""class"":{""apple"":true}}}],""file_attributes"":{}}},""_via_attributes"":{""region"":{""class"":{""type"":""checkbox"",""description"":"""",""options"":{""apple"":"""",""banana"":"""",""\""\"""":""""},""default_options"":{}}},""file"":{}}}
",neutral,neutral
237,Data Generator - reduction operation minimum which has no identity,matterport/Mask_RCNN,open,,ilkarman,,2017-11-02 17:50:02,,3.0,,7.0,270747215.0,"I'm playing around with the shapes data-set; I have increased the settings by 4:

```
IMAGE_MIN_DIM = 4*128
IMAGE_MAX_DIM = 4*128
RPN_ANCHOR_SCALES = (4*8, 4*16, 4*32, 4*64, 4*128)  # anchor side in pixels
TRAIN_ROIS_PER_IMAGE = 4*32
```

And also generate 100x more samples. However I get errors like this that stop execution:

> ValueError                                Traceback (most recent call last)
<ipython-input-9-2101606c7d8e> in <module>()
      6             learning_rate=config.LEARNING_RATE,
      7             epochs=1,
----> 8             layers='heads')
/home/iliauk/Mask_RCNN/model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers)
   2072             ""steps_per_epoch"": self.config.STEPS_PER_EPOCH,
   2073             ""callbacks"": callbacks,
-> 2074             ""validation_data"": next(val_generator),
   2075             ""validation_steps"": self.config.VALIDATION_STPES,
   2076             ""max_queue_size"": 100,
/home/iliauk/Mask_RCNN/model.py in data_generator(dataset, config, shuffle, augment, random_rois, batch_size, detection_targets)
   1525             image_id = image_ids[image_index]
   1526             image, image_meta, gt_boxes, gt_masks = \
-> 1527                 load_image_gt(dataset, config, image_id, augment=augment, use_mini_mask=config.USE_MINI_MASK)
   1528 
   1529             # Skip images that have no instances. This can happen in cases
/home/iliauk/Mask_RCNN/model.py in load_image_gt(dataset, config, image_id, augment, use_mini_mask)
   1150     # Resize masks to smaller size to reduce memory usage
   1151     if use_mini_mask:
-> 1152         mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)
   1153 
   1154     # Image meta data
/home/iliauk/Mask_RCNN/utils.py in minimize_mask(bbox, mask, mini_shape)
    433         y1, x1, y2, x2 = bbox[i][:4]
    434         m = m[y1:y2, x1:x2]
--> 435         m = scipy.misc.imresize(m.astype(float), mini_shape, interp='bilinear')
    436         mini_mask[:, :, i] = np.where(m >= 128, 1, 0)
    437     return mini_mask
/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)
     99             """"""`arrayrange` is deprecated, use `arange` instead!""""""
    100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)
--> 101             return func(*args, **kwds)
    102 
    103         newfunc = _set_function_name(newfunc, old_name)
/anaconda/envs/py35/lib/python3.5/site-packages/scipy/misc/pilutil.py in imresize(arr, size, interp, mode)
    552 
    553     """"""
--> 554     im = toimage(arr, mode=mode)
    555     ts = type(size)
    556     if issubdtype(ts, numpy.signedinteger):
/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)
     99             """"""`arrayrange` is deprecated, use `arange` instead!""""""
    100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)
--> 101             return func(*args, **kwds)
    102 
    103         newfunc = _set_function_name(newfunc, old_name)
/anaconda/envs/py35/lib/python3.5/site-packages/scipy/misc/pilutil.py in toimage(arr, high, low, cmin, cmax, pal, mode, channel_axis)
    334         if mode in [None, 'L', 'P']:
    335             bytedata = bytescale(data, high=high, low=low,
--> 336                                  cmin=cmin, cmax=cmax)
    337             image = Image.frombytes('L', shape, bytedata.tostring())
    338             if pal is not None:
/anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/utils.py in newfunc(*args, **kwds)
     99             """"""`arrayrange` is deprecated, use `arange` instead!""""""
    100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)
--> 101             return func(*args, **kwds)
    102 
    103         newfunc = _set_function_name(newfunc, old_name)
/anaconda/envs/py35/lib/python3.5/site-packages/scipy/misc/pilutil.py in bytescale(data, cmin, cmax, high, low)
     91 
     92     if cmin is None:
---> 93         cmin = data.min()
     94     if cmax is None:
     95         cmax = data.max()
/anaconda/envs/py35/lib/python3.5/site-packages/numpy/core/_methods.py in _amin(a, axis, out, keepdims)
     27 
     28 def _amin(a, axis=None, out=None, keepdims=False):
---> 29     return umr_minimum(a, axis, None, out, keepdims)
     30 
     31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False):
ValueError: zero-size array to reduction operation minimum which has no identity",neutral,neutral
238,Any plans for adding support for YOLOv5？,open-mmlab/mmdetection,closed,yhcao6,Wulingtian,yhcao6,2020-12-12 00:56:30,2020-12-14 03:02:21,1.0,,4288.0,763124633.0,Hi! May I ask that is there any plan for implementing YOLOv5? Thank you!,neutral,positive
239,question about Gaussion kernel size and CAR useage during training ROMP,Arthur151/ROMP,closed,,ChangjianLi,Arthur151,2022-07-18 04:30:29,2022-11-28 00:52:27,3.0,,309.0,1307391266.0,"作者大大你好
I have a question about the training codes. I have read through the codes and I couldn't find the methods of Gaussion kernel size and CAR calculation used for the training. Did I missed some part lol ",neutral,positive
240,The para Temperature of DWA ,lorenmt/mtan,closed,,ykeivn,lorenmt,2021-09-17 13:48:25,2021-12-11 11:17:46,6.0,,49.0,999374866.0,您好，我想问一下联合多任务损失函数中DWA的Temperature的取值范围在多少合适呢，依据什么来选择得到文章中的2呢？（loss值的大小？），先提前感谢您的回答,neutral,neutral
241,make core failed,percyliang/sempre,open,,ghost,,2015-06-11 09:53:02,,8.0,,57.0,87285922.0,"Hello,

Thank you very much for sharing with us this great project.

I have some difficulties with installation process.

When I run 'make core' if fails with a number of error messages

cd src/edu/stanford/nlp/sempre && ant compile
Buildfile: .../sempre/src/edu/stanford/nlp/sempre/build.xml

compile:
    [javac] Compiling 119 source files to .../sempre/classes
    [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.antlr.v4.runtime.misc.NullUsageProcessor' less than -source '1.8'
    [javac] .../sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:4: error: package fig.basic does not exist
    [javac] import fig.basic.LogInfo;
    [javac]                 ^
    [javac] .../sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:5: error: package fig.basic does not exist
    [javac] import fig.basic.MapUtils;
    [javac]                 ^
    [javac] .../sempre/src/edu/stanford/nlp/sempre/AbstractReinforcementParserState.java:6: error: package fig.basic does not exist
    [javac] import fig.basic.StopWatchSet;
    [javac]                 ^

First I tried to solve the problem with  'fig'. I've added all possible 'CLASSPATH' values. 

'export CLASSPATH=$CLASSPATH:$FIG_HOME/fig.jar'

But it doesn't help. I don't understand what's the problem. Thank you.

java-1.8.0-openjdk-1.8.0.45-30.b13.el7_1.x86_64
Apache Ant(TM) version 1.9.4 compiled on April 29 2014
ruby 2.0.0p598 (2014-11-13) [x86_64-linux]
",neutral,negative
242,How to add the parameter gt_bboxes_ignore in model,open-mmlab/mmdetection,open,,liujiawen-jpg,,2021-08-20 14:24:18,,2.0,usage,5927.0,975658963.0,"I am using maxiouassigner but I need gt_bboxes_ignore parameter when I use it, I don't know how to pass this parameter to forward_trian().",neutral,neutral
243,Feature Bounding Boxes don't match to image,facebookresearch/mmf,closed,,PaulOpu,PaulOpu,2019-11-08 10:14:32,2019-11-08 15:01:18,2.0,,196.0,519928175.0,"## ❓ Questions and Help
I extracted the bounding boxes of the resnet152 features from the `pythia/data/open_images/detectron_fix_100/fc6/train/{image_name}_info.npy` file and tried to visualize the bounding boxes on the image. 


```
boxes = np.zeros((100,5))
boxes[:,:-1] = image_dic[""boxes""]
boxes[:,4] = image_dic[""cls_scores""]

from pythia.legacy.tools.visualize_bbox import vis_detections
img = load_image_from_url(orig_image_url)
vis_detections(img,class_name,[],boxes,thresh=0.8)
```

1e4fcfbd0bb6e1e1
![image](https://user-images.githubusercontent.com/22904585/68468204-d63c4980-0217-11ea-83bd-26066fd22230.png)


Unfortunately, the mapped objects didn't seem to match with the image (at least I don't see a connection). That happens for the original image (flickr_original_url) and the cropped one (flickr_300k_url), validated with many images. 

Is there something I do wrong? Thank you for your help!",neutral,negative
244,Is it possible to keep the old version of model_data.zip?,Arthur151/ROMP,closed,,jiangwei221,jiangwei221,2022-08-04 04:40:22,2022-08-04 09:31:08,4.0,,326.0,1328057852.0,"I saw model_data.zip has changed on 21 Jun 2022. See: https://github.com/Arthur151/ROMP/releases/tag/v1.1
Is it possible to have an old copy of it?
",neutral,neutral
245,Which tf version should I use in google-research/tft?,google-research/google-research,open,,jiminbot20,,2021-11-19 04:29:13,,1.0,,900.0,1058119649.0,Which exact tensorflow version should I use? Minor errors appear due to the version issue. There's no info in _google-research/tft/requirements.txt_,neutral,neutral
246,speech rate,NVIDIA/flowtron,open,,evelynyhc,,2021-03-24 08:04:58,,0.0,,109.0,839461709.0,"hello, i trained the model with LJspeech, and i want to know how to control the speech rate when i inference? is there any parameter control it?",neutral,neutral
247,How to implement the improvement of build_fpn_mask_graph? ,matterport/Mask_RCNN,open,,John1231983,,2018-03-09 16:34:23,,2.0,,317.0,303900775.0,"Hello all, I have a plane to implement an improvement of mask segmentation _branch. It is described as following:

>We further create a short path from layer conv3 to a fc layer. There are two 3×3 convolutional layers where the second shrinks channels to half to reduce computational overhead. The mask size we use is
28 × 28 so that the fc layer produces a 784 × 1 × 1 vector. This vector is reshaped to the same spatial size as the mask predicted by FCN. To obtain the final mask prediction, mask of each class from FCN and foreground/background prediction from fc are added. 
![screenshot from 2018-03-10 01-14-40](https://user-images.githubusercontent.com/24875971/37217283-79c2da9e-2400-11e8-9052-39c634034259.png)

The below code shows my implementation that I want to ask two questions:
1. What is the number of feature of `mrcnn_mask_conv4_fc` and `mrcnn_mask_conv5_fc`. The paper said that **""two 3×3 convolutional layers where the second shrinks channels""**. Is it 128?
2. What is the reshape size (after fully connected layer). How can we fill the `?` in the ` x_fc = K.reshape(x_fc, (-1, ?, ?, ?, ?)))`?
3. I have run the code but it shows some error likes below. I guess have some problem with `Add ` function
```
  File ""/home/john/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""/home/john/.local/lib/python3.5/site-packages/keras/engine/topology.py"", line 1732, in __init__
    build_map_of_graph(x, finished_nodes, nodes_in_progress)
  File ""/home/john/.local/lib/python3.5/site-packages/keras/engine/topology.py"", line 1722, in build_map_of_graph
    layer, node_index, tensor_index)
  File ""/home/john/.local/lib/python3.5/site-packages/keras/engine/topology.py"", line 1722, in build_map_of_graph
    layer, node_index, tensor_index)
  File ""/home/john/.local/lib/python3.5/site-packages/keras/engine/topology.py"", line 1693, in build_map_of_graph
    layer, node_index, tensor_index = tensor._keras_history
AttributeError: 'Tensor' object has no attribute '_keras_history'
```
This is my implementation
```
    x = KL.TimeDistributed(BatchNorm(axis=3),
                           name='mrcnn_mask_bn3')(x)
    x = KL.Activation('relu')(x)
    x_classify = x
    ...
    # Add fc _branch
    x_fc = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=""same""),
                                  name=""mrcnn_mask_conv4_fc"")(x_conv3)
    x_fc = KL.Activation('relu')(x_fc)
    x_fc = KL.TimeDistributed(KL.Conv2D(128, (3, 3), padding=""same""),
                                  name=""mrcnn_mask_conv5_fc"")(x_fc)
    x_fc= KL.TimeDistributed(KL.Dense(num_classes, activation='linear'),
                                  name='mrcnn_bbox_fc')(x_fc)

    x_fc = K.reshape(x_fc, (-1, ?, ?, ?, ?)))
    x_fc = KL.TimeDistributed(KL.Add(name=""mrcnn_bbox_fc_deconv_add"")([x_fc, x_classify]))
    x_fc = KL.TimeDistributed(KL.Conv2D(num_classes, (1, 1), strides=1, activation=""sigmoid""),
                           name=""mrcnn_mask"")(x_fc)
     return x_fc
```
",neutral,positive
248,Fix support for models with one output (PyTorch implementation),CuriousAI/mean-teacher,open,,tarvaina,,2018-02-21 12:18:29,,0.0,,8.0,298948284.0,The PyTorch code currently crashes if the model has only one output. See comments on issue #7 .,neutral,neutral
249,Stain Normalization ,open-mmlab/mmdetection,closed,,fanxiaochen-123,hhaAndroid,2021-11-13 10:25:31,2022-03-10 09:08:43,2.0,reimplementation,6498.0,1052648970.0,"Is there any staining normalization in MMDetection?
",neutral,neutral
250,Modify config file when train with custom dataset,open-mmlab/mmdetection,closed,,neoyang0620,hellock,2020-01-06 02:49:02,2020-01-20 14:38:44,3.0,,1924.0,545512113.0,"Hi,

I built a custom dataset based on COCO format. And added a new my_dataset.py under mmdet/datasets and added from .my_dataset import MyDataset in __init__.py. 

Then, I modified the  configuration file:

dataset_type = 'MyDataset'
data_root = 'path/to/my/dataset'
data = dict{}

During the training, the accuracy is almost 100. However, in evaluation, the mAP is almost 0. I don't know what's wrong with my setting. Did i forget to change any details?

Best,
Neo
",neutral,positive
251,Training on single speaker (male) hindi dataset - unable to attend (flow=1),NVIDIA/flowtron,open,,astricks,,2020-08-03 16:14:40,,8.0,,53.0,672187890.0,"Continuing the conversation from this somewhat related issue - https://github.com/NVIDIA/flowtron/issues/39 - but opening a new issue since I my model is unable to attend even with 1 flow.

My issue some also somewhat similar to https://github.com/NVIDIA/flowtron/issues/41 - I have now trained my model for 790,000 steps. Validation loss seems to have hit a minima at around 360k steps at which point attention was biased, and further training made attention vanish and validation loss slowly increase.

Attached below are attention plots for steps=215k, 360k, 790k; and the validation loss.
<img width=""732"" alt=""attn215k"" src=""https://user-images.githubusercontent.com/1024213/89203172-1ef41980-d582-11ea-952c-57efe6b616fa.png"">
<img width=""692"" alt=""Attn360k"" src=""https://user-images.githubusercontent.com/1024213/89203137-0c79e000-d582-11ea-90a7-4ff8a26535d5.png"">
<img width=""741"" alt=""attn790k"" src=""https://user-images.githubusercontent.com/1024213/89203190-25829100-d582-11ea-8638-d1629aab61f3.png"">
<img width=""392"" alt=""valLoss"" src=""https://user-images.githubusercontent.com/1024213/89203208-2ddacc00-d582-11ea-9807-ca44460e6d8f.png"">

I am wondering how to proceed. The options I'm considering are:
1. increase flow=2 and warmstart with checkpoint 790k.
2. increase flow=2 and warmstart with checkpoint 360k. I deleted that checkpoint to save some diskspace and am now regretting it - I'll have to restart training and get to 360k steps again.
3. Train a new tacotron2 model using my hindi dataset and warmstart flowtron using that.

@rafaelvalle Would love some advice at this point.

Also attaching some inference files below. The speech is senseless though.
![sid0_sigma0 5_attnlayer0](https://user-images.githubusercontent.com/1024213/89203567-c5d8b580-d582-11ea-8622-2b61c1526b4e.png)
[sid0_sigma0.5.wav.zip](https://github.com/NVIDIA/flowtron/files/5017062/sid0_sigma0.5.wav.zip)

",neutral,positive
252,Imbalance and noise handling,mp2893/retain,open,,dhyoon0527,,2020-09-16 14:11:17,,2.0,,9.0,702800793.0,"Hello Ed,

First of all, thank you for this amazing work. I was wondering if you could answer the following questions, it'd be really appreciated.

1. How do you think RETAIN handles with class-imbalance situation? I personally saw it handled well in my dataset, but couldn't follow why it worked great.

2. Patients can have common medical codes like hypertension or diabetes, which can be regarded as noise in terms of predicting other disease. Can RETAIN capture such codes with less weight even though they appear a lot?

Best Regards,",neutral,positive
253,Useful Conda Environment for Mask_RCNN Project,matterport/Mask_RCNN,open,,Atotem,,2022-04-07 16:43:47,,1.0,,2801.0,1196310183.0,"Hi! After several tries I finally obtained a useful conda enviorment to run at least the demo from the samples folder. I had to set the enviorment to a python 3.6 version and install tensorflow and keras correctly. I hope this information is useful to you, i wasted a lot of time setting up my environment. Cheers!

P.D.: Change the document suffix from .txt to .yml
[environment.txt](https://github.com/matterport/Mask_RCNN/files/8444996/environment.txt)

",positive,positive
254,train speed too slow ,deepinsight/insightface,closed,,Edwardmark,nttstar,2018-07-16 08:37:32,2018-09-21 07:04:11,10.0,,291.0,341431891.0,"I try to finetune   LResNet50E-IR model using glint Asian dataset, I run the following command:  
CUDA_VISIBLE_DEVICES='0,1,2' python -u train_softmax.py \
                                --network r50 \
                                --loss-type 0 \
                                --lr 0.005 \
                                --per-batch-size 64 \
                                --data-dir /mntML/dongbin/datasets/faces_asian_112x112 \
                                --pretrained ../models/model-r50-am-lfw/model,0000 \
                                --prefix ""$PREFIX""
I use mxnet_cu90, and I use 1080Ti x 3，the speed is so slow, the log is as follows:

gpu num: 3
num_layers 50
image_size [112, 112]
num_classes 93979
Called with argument: Namespace(batch_size=192, beta=1000.0, beta_freeze=0, beta_min=5.0, bn_mom=0.9, ckpt=1, ctx_num=3, cutoff=0, data_dir='/mntML/dongbin/datasets/faces_asian_112x112', easy_margin=0, emb_size=512, end_epoch=100000, fc7_wd_mult=1.0, gamma=0.12, image_channel=3, image_h=112, image_w=112, loss_type=0, lr=0.005, lr_steps='', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, max_steps=0, mom=0.9, network='r50', num_classes=93979, num_layers=50, per_batch_size=64, power=1.0, prefix='../model-r50-asian/model-asian', pretrained='../models/model-r50-am-lfw/model,0000', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
loading ['../models/model-r50-am-lfw/model', '0000']
[07:58:36] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
init resnet 50
0 1 E 3 prelu
INFO:root:loading recordio /mntML/dongbin/datasets/faces_asian_112x112/train.rec...
header0 label [2830147. 2924126.]
id2range 93979
2830146
rand_mirror 1
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [106666, 160000, 213333]
call reset()
/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:490: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.333333333333 vs. 0.00520833333333). Is this intended?
  optimizer_params=optimizer_params)
INFO:root:Epoch[0] Batch [20]	Speed: 29.62 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [40]	Speed: 36.43 samples/sec	acc=0.006510
INFO:root:Epoch[0] Batch [60]	Speed: 41.90 samples/sec	acc=0.040104
INFO:root:Epoch[0] Batch [80]	Speed: 55.39 samples/sec	acc=0.102344
INFO:root:Epoch[0] Batch [100]	Speed: 76.50 samples/sec	acc=0.155469
INFO:root:Epoch[0] Batch [120]	Speed: 70.54 samples/sec	acc=0.182031
INFO:root:Epoch[0] Batch [140]	Speed: 72.65 samples/sec	acc=0.203125  


Anybody knows why?",neutral,positive
255,how to preprocess image using pytorch only but mmcv ？,open-mmlab/mmdetection,closed,RunningLeon,alexiycv,ZwwWayne,2021-04-16 09:29:22,2022-03-13 13:57:35,1.0,community discussion,4988.0,859641845.0,"i have converted the mmdet trained model into onnx, how to  preprocess image to feed to the onnx model using pytorch transform or using onnx c++ preprocessing? ",neutral,neutral
256,Train a model to detect person(only one class) using COCO dataset,open-mmlab/mmdetection,closed,,LifeIsSoSolong,LifeIsSoSolong,2019-08-27 10:43:56,2019-08-29 12:07:48,4.0,,1281.0,485732486.0,"Thanks for your contributions~
I want to train a faster_rcnn detector to detect ""person""(only one class) using original coco datasets, and I counter some problems.
My train process are:
0.I can train and test original base model normally.

1. create mmdet/datasets/my_dataset.py and write:

```
from .coco import CocoDataset
from .registry import DATASETS

@DATASETS.register_module
class MyDataset(CocoDataset):
    CLASSES = ('person')
```
2.In mmdet/datasets/__init__.py""
add:
`from .my_dataset import MyDataset`

3.copy faster_rnn cfg (checkpoints/faster_rcnn_r50_fpn_1x.py) to a new dir, rename(faster_rcnn_r50_fpn_1x_person.py) and only change two places:
1)in model->bbox_head->numclasses 
`num_classes=81 => num_classes = 2`
2)in dataset setting 
`dataset_type = 'CocoDataset' => dataset_type = 'MyDataset'`

4. train
python tools/train.py configs/faster_rcnn_r50_fpn_1x_person.py --work_dir zkk_workdir/

then, I counter error:

```
2019-08-27 18:17:28,585 - INFO - Distributed training: False
2019-08-27 18:17:28,832 - INFO - load model from: torchvision://resnet50
2019-08-27 18:17:29,095 - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

FasterRCNN(....)
loading annotations into memory...
Done (t=9.56s)
creating index...
index created!
2019-08-27 18:17:41,537 - INFO - Start running, host: kaikai@kaikai, work_dir: /home/kaikai/anaconda3/envs/open-mmlab/mmdetection/zkk_workdir
2019-08-27 18:17:41,537 - INFO - workflow: [('train', 1)], max: 12 epochs
THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THC/THCTensorMathCompare.cuh line=82 error=59 : device-side assert triggered
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [512,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [0,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [1,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [2,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [3,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [4,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [5,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [6,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [7,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
Traceback (most recent call last):
  File ""tools/train.py"", line 126, in <module>
    main()
  File ""tools/train.py"", line 122, in main
    logger=logger)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/apis/train.py"", line 60, in train_detector
    _non_dist_train(model, dataset, cfg, validate=validate)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/apis/train.py"", line 221, in _non_dist_train
    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/runner.py"", line 358, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/runner.py"", line 264, in train
    self.model, data_batch, train_mode=True, **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/apis/train.py"", line 38, in batch_processor
    losses = model(**data)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 150, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/core/fp16/decorators.py"", line 49, in new_func
    return old_func(*args, **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/models/detectors/base.py"", line 86, in forward
    return self.forward_train(img, img_meta, **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/models/detectors/two_stage.py"", line 183, in forward_train
    *bbox_targets)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/core/fp16/decorators.py"", line 127, in new_func
    return old_func(*args, **kwargs)
  File ""/home/kaikai/anaconda3/envs/open-mmlab/mmdetection/mmdet/models/bbox_heads/bbox_head.py"", line 118, in loss
    pos_inds = labels > 0
RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/THC/THCTensorMathCompare.cuh:82
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered (insert_events at /opt/conda/conda-bld/pytorch_1556653114079/work/c10/cuda/CUDACachingAllocator.cpp:564)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7f908f03bdc5 in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x14792 (0x7f908bf19792 in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)
frame #2: c10::TensorImpl::release_resources() + 0x50 (0x7f908f02b640 in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #3: <unknown function> + 0x3067fb (0x7f908c6387fb in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #4: <unknown function> + 0x14019b (0x7f90be05819b in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3bfc84 (0x7f90be2d7c84 in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #6: <unknown function> + 0x3bfcd1 (0x7f90be2d7cd1 in /home/kaikai/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #26: __libc_start_main + 0xf0 (0x7f90cd6be830 in /lib/x86_64-linux-gnu/libc.so.6)

已放弃 (核心已转储)
```
Maby I loss some steps when I training a new model for person using coco datasets, But I don't know how to figure it . 

Who can give me some advice.

Thanks for anyone who saw this issue.



",neutral,positive
257,Online normalization layer,tensorflow/addons,closed,,DanielWicz,AakashKumarNain,2021-07-24 14:16:07,2021-07-24 18:06:42,1.0,,2532.0,952083699.0,"**Describe the feature and the current behavior/state.**
Online normalization layer allows for a normalization in the case, when input distribution changes significantly over time (RNN, online learning, GANs), and thus techniques like batch normalization hurt more than help.



**Relevant information**
- Are you willing to contribute it (yes/no):
*If you wish to contribute, then read the requirements for new contributions in [`CONTRIBUTING.md`](https://github.com/tensorflow/addons/blob/master/CONTRIBUTING.md#requirements-for-new-contributions-to-the-repository)*
- Are you willing to maintain it going forward? (yes/no): No
- Is there a relevant academic paper? (if so, where): https://arxiv.org/pdf/1905.05894.pdf (Accepted on NeurIPS 2019)
- Does the relavent academic paper exceed 50 citations? (yes/no): No, it's fresh and has 21
- Is there already an implementation in another framework? (if so, where): There is already an implementation for tensorflow: https://github.com/Cerebras/online-normalization together with a CPU and a GPU kernels, but does not work with the newest versions of TF (I think it works up to ~2.3).
- Was it part of tf.contrib? (if so, where): I don't know

**Which API type would this fall under (layer, metric, optimizer, etc.)**
Layer

**Who will benefit with this feature?**
People who use: Online learning, RNN, GANs

",neutral,negative
258,Bug in WeightNormalization for Bidirectional RNN,tensorflow/addons,open,,zzw922cn,,2021-07-29 13:06:37,,0.0,,2536.0,955823904.0,"**code**
tf.keras.layers.Bidirectional(tfa.layers.WeightNormalization(tf.keras.layers.GRU(units_gru, return_sequences=True)), merge_mode='concat') 

**problem**
In Weight Normalization, there is no ""go_backwards"" argument, but Bidirectional needs such argument, so this line code met errors as following:
```
 File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py"", line 422, in __init__
    layer, go_backwards=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py"", line 481, in _recreate_layer_from_config
    config['go_backwards'] = not config['go_backwards']
KeyError: 'go_backwards'
```",neutral,positive
259,how to train 2d106det？,deepinsight/insightface,open,,intjun,,2022-06-15 09:57:24,,1.0,,2031.0,1271980565.0,"if want train 2d106det,how to train 2d106det？",neutral,neutral
260,SIL Value update,junhyukoh/self-imitation-learning,closed,,boscotsang,boscotsang,2018-10-15 02:10:11,2019-06-28 07:04:03,2.0,,3.0,369972618.0,"In the paper, sil value loss is defined as 0.5 * max(0, (R-V))^2. Howerver in the code, the value loss is defined as below
`self.vf_loss = tf.reduce_sum(self.W * v_estimate * tf.stop_gradient(delta)) / self.num_samples`
which means that the value loss is 0.5 * V * clip((V-R), -5, 0).
What's the advantage of this implementation. Thanks
",neutral,negative
261,pretrained model,srxdev0619/Latent_Convolutional_Models,open,,agapiR,,2019-06-04 21:04:14,,0.0,,1.0,452207435.0,Great work! Can we have the parameters of the pretrained generator g_theta? Thank you very much.,neutral,positive
262,KeyError during training with my own dataset,matterport/Mask_RCNN,closed,,cvKDean,cvKDean,2019-02-19 18:57:57,2019-02-20 16:53:34,7.0,,1294.0,412074994.0,"Good day, I am trying to follow the [Color Splash Example](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon) via this [blog post](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46) where I used my own class, which detects roofs in the image. I just replaced the `balloon` class with `roof` so the configuration has only 2 classes (including the background). I annotated my own images using VIA v.2.0.5. However during training, I am getting the following error:

`Epoch 1/30
ERROR:root:Error processing image {'id': '0030_0040.png', 'source': 'roof', 'path': 'roxas-mask-rcnn/val/0030_0040.png', 'width': 255, 'height': 260, 'polygons': [{'name': 'polygon', 'all_points_x': [7, 42, 90, 14, 1, 1, 19], 'all_points_y': [107, 89, 175, 213, 186, 144, 135]}, {'name': 'polygon', 'all_points_x': [185, 231, 253, 254, 220], 'all_points_y': [142, 117, 157, 183, 202]}, {'name': 'polygon', 'all_points_x': [82, 119, 135, 92], 'all_points_y': [220, 202, 226, 250]}]}
Traceback (most recent call last):
  File ""/home/deankarlo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1709, in data_generator
    use_mini_mask=config.USE_MINI_MASK)
  File ""/home/deankarlo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1275, in load_image_gt
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][""source""]]
KeyError: 'roof'`

Why is it looking for a key named `roof`? Is it supposed to look for `source` instead? Any help is appreciated. Thank you!",neutral,neutral
263,"tensorflow.keras.layers TextVectorization: adapt() with output_mode=""tf_idf"" (GPU only) throws InvalidArgumentError: INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string ",google-research/google-research,open,,jmbo1190,,2022-01-17 18:04:57,,1.0,,947.0,1106136417.0,"**Behaviour observed with**:
* Python 3.7 Tensorflow 2.6, Py 3.7 TF 2.7, Py 3.8 TF 2.7, Py 3.9.9 TF 2.7.0, 
* GPUs: 
    * GeForce GTX 1660 SUPER, compute capability: 7.5, 5944MiB, CUDA 11.2
    * Tesla M60, compute capability: 5.2, 8129MiB, CUDA 11.5

**Error Message**:
```
InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
	 [[map/while/loop_body_control/_21/_51]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_adapt_step_1288]

Function call stack:
adapt_step -> adapt_step
```
**ReprEx**:
```
import os, pathlib
import tensorflow as tf
from tensorflow import keras

base_dir = pathlib.Path(""RepRex"")
os.makedirs(base_dir / ""neg"", exist_ok = True)
os.makedirs(base_dir / ""pos"", exist_ok = True)

with open(base_dir / ""pos"" / ""sample1.txt"", ""w"") as f:
    f.write(""The food was excellent.\n"")

with open(base_dir / ""neg"" / ""sample1.txt"", ""w"") as f:
    f.write(""The wine was awful, we will never go to that place again.\n"")


batch_size = 2
ds = keras.utils.text_dataset_from_directory(""RepRex"", batch_size=batch_size)
text_only_ds = ds.map(lambda x, y: x)   # Prepare a dataset that only yields raw text inputs (no labels).

text_vectorization = keras.layers.TextVectorization(max_tokens=20000, output_mode=""tf_idf"")
print(""Running adapt() with 'tf_idf' on CPU"") # ok
with tf.device('/cpu:0'):
    text_vectorization.adapt(text_only_ds)  
    
text_vectorization2 = keras.layers.TextVectorization(max_tokens=20000, output_mode=""count"")
print(""Running adapt() with 'count' on GPU"") # ok
with tf.device('/gpu:0'):
    text_vectorization2.adapt(text_only_ds)  

text_vectorization3 = keras.layers.TextVectorization(max_tokens=20000, output_mode= ""multi_hot"")
print(""Running adapt() with 'multi_hot' on GPU"") # ok
with tf.device('/gpu:0'):
    text_vectorization3.adapt(text_only_ds)  

text_vectorization4 = keras.layers.TextVectorization(max_tokens=20000, output_mode=""tf_idf"")
print(""Running adapt() with 'tf_idf' on GPU"") # error
with tf.device('/gpu:0'):
    text_vectorization4.adapt(text_only_ds)  
 
```

** ReprEx output**:
```
Found 2 files belonging to 2 classes.
Running adapt() with 'tf_idf' on CPU
Running adapt() with 'count' on GPU
WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe5d80cec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Running adapt() with 'multi_hot' on GPU
WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe5d80ceaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Running adapt() with 'tf_idf' on GPU
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/tmp/ipykernel_610484/3457753437.py in <module>
     37 print(""Running adapt() with 'tf_idf' on GPU"")
     38 with tf.device('/gpu:0'):
---> 39     text_vectorization4.adapt(text_only_ds)
     40 

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py in adapt(self, data, batch_size, steps)
    242       with data_handler.catch_stop_iteration():
    243         for _ in data_handler.steps():
--> 244           self._adapt_function(iterator)
    245           if data_handler.should_sync:
    246             context.async_wait()

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     56   try:
     57     ctx.ensure_initialized()
---> 58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     59                                         inputs, attrs, num_outputs)
     60   except core._NotOkStatusException as e:

InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
	 [[map/while/loop_body_control/_21/_51]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_adapt_step_1288]

Function call stack:
adapt_step -> adapt_step
```",neutral,positive
264,SchNet: How to get graph embedding instead of prediction,divelab/DIG,closed,,LanceKnight,limei0307,2022-07-19 06:00:11,2022-07-20 21:42:23,2.0,3dgraph,122.0,1309019572.0,"Hello, it seems the output of SchNet is of dimension 1, which is the prediction dimension. Is it possible to change to output the graph embedding instead of the predicton? This way it's more flexible for the users to put whatever downstream classifier after the embedding and have a control of how many prediction dimension there can be (multi-task potentially)",neutral,neutral
265,实测感觉有点慢,Zhongdao/Towards-Realtime-MOT,open,,hushunda,,2020-05-12 08:40:02,,3.0,,153.0,616471760.0,"在rtx2080ti上,测试JDE-576x320模型,使用的是摄像头读取的图像,但是速度只有8fps.
还有什么可以加速的吗?

`def track(opt):

    cfg_dict = parse_model_cfg(opt.cfg)
    opt.img_size = [int(cfg_dict[0]['width']), int(cfg_dict[0]['height'])]

    # run tracking
    timer = Timer()
    accs = []
    n_frame = 0
    tracker = JDETracker(opt, frame_rate=30)
    cap= cv2.VideoCapture(3)
    cap.set(3,1920)
    cap.set(4,1080)
    # 目标尺度处理
    vw,vh = [int(cfg_dict[0]['width']), int(cfg_dict[0]['height'])]
    wa, ha = float(cap.get(3)) / vw, float(cap.get(4)) / vh
    a=min(wa,ha)
    w,h=int(vw * a), int(vh * a)
    cv2.namedWindow('show',0)
    while 1:
        timer.tic()
        ret,img0 = cap.read()
        if ret:
            img = cv2.resize(img0, (w, h))
            img, _, _, _ = letterbox(img, height=vh, width=vw)
            # Normalize RGB
            img = img[:, :, ::-1].transpose(2, 0, 1)
            img = np.ascontiguousarray(img, dtype=np.float32)
            img /= 255.0
            blob = torch.from_numpy(img).cuda().unsqueeze(0)
            online_targets = tracker.update(blob, img0)
            if len(online_targets)>0:
                print('ok')
            print(len(online_targets))
            online_tlwhs = []
            online_ids = []
            for t in online_targets:
                tlwh = t.tlwh
                tid = t.track_id
                vertical = tlwh[2] / tlwh[3] > 1.6
                if tlwh[2] * tlwh[3] > opt.min_box_area and not vertical:
                    online_tlwhs.append(tlwh)
                    online_ids.append(tid)
            timer.toc()
            online_im = vis.plot_tracking(img0, online_tlwhs, online_ids, frame_id=1,
                                              fps=1. / timer.average_time)
            cv2.imshow('show', online_im)
            cv2.waitKey(1)`",neutral,neutral
266,train.log should be replaced by <run_type>.log,facebookresearch/mmf,open,,netw0rkf10w,,2020-10-21 14:52:02,,2.0,needs discussion#triaged,653.0,726567553.0,"## 🚀 Feature

`train.log` should be replaced by `<run_type>.log`, i.e. when `train.log` when `run_type=train` and `val.log` when `run_type=val`.

## Motivation

I launched a training (on a Slurm server), which produced a log file at `save/train.log`. After the training has finished, I launched an evaluation of the trained model using (of course) the same configuration file: my training logs `save/train.log` were replaced by the evaluation logs, which is clearly not desirable.",neutral,neutral
267,Age detection ,deepinsight/insightface,open,,bhatiyaarp,,2021-10-18 07:31:04,,0.0,,1790.0,1028733272.0,"Hi, how can we work on age estimation and gender detection",neutral,neutral
268,Dysfunctional link in readme,facebookresearch/mmf,closed,,steph-en-m,facebook-github-bot,2020-07-16 18:06:13,2020-07-17 05:55:03,2.0,,417.0,658430853.0,The link to the hateful memes tutorial in the readme is faulty. The page it redirects to displays a `This page does not exist yet..` message.,neutral,negative
269,day-night,junyanz/BicycleGAN,closed,,cuimiao187561,junyanz,2017-12-10 14:16:54,2018-01-17 00:54:34,1.0,,4.0,280812447.0,"hello junyanz
Can you share day-night for me?
thank you very much",neutral,positive
270,How to use knowledge distillation on a custom dataset?,namisan/mt-dnn,closed,,IoSylar,IoSylar,2021-06-09 12:05:28,2021-08-04 19:27:25,0.0,,217.0,916125387.0,"Hello ,thanks for sharing of your work. I' m using MT-DNN on a custom datasets. I'm following the tutorials so I did single task learning and MTL, but I fail to do KD. I don't know if  it is necessary a processing of data or others previous steps. I 've simply used 
`!python train.py --task_def tutorials/tutorial_task_def.yml --data_dir tutorials/bert_base_uncased/  --init_checkpoint=""mt_dnn_models/mt_dnn_kd_large_cased.pt"" --train_datasets MyTask --test_datasets MytTask --epochs=1 --batch_size=1`
 Later, I have had an error:
 `RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`.  I'm using Google Colab and I've tried differents batch size. ",neutral,negative
271,What is the ground truth for noise level map when JPEG compression is considered?,GuoShi28/CBDNet,closed,,deng-wei,deng-wei,2019-01-08 09:39:30,2019-01-09 03:20:45,2.0,,13.0,396821850.0,"What is the ground truth for noise level map when JPEG compression is considered?
Is it still \sigma_c + L\sigma_s ?
Thank you",positive,positive
272,Prgram failed when I follow the instruction in GETTING_STARTED.md,openseg-group/openseg.pytorch,closed,,jiady1990,hsfzxjy,2021-05-05 11:19:14,2021-05-05 13:16:43,1.0,,61.0,876344199.0,"Thank you for open-source the project.
I follow the instruction in GETTING_STARTED.md but the program break down when the log shows ""[data_helper.py, 120] Target keys: ['labelmap']"".
[
![Screenshot from 2021-05-05 19-18-02](https://user-images.githubusercontent.com/14902230/117133295-a81cd300-add6-11eb-9ec9-17e6add8ae54.png)
](url)
",neutral,neutral
273,Feature extraction questions,wzmsltw/BSN-boundary-sensitive-network.pytorch,closed,,cinjon,wzmsltw,2019-08-22 20:35:46,2019-08-29 11:58:23,27.0,,14.0,484204068.0,"Hi there, I am trying to train this on another dataset and am getting a bit stuck trying to figure out how exactly you extracted the features using TSN.

I am using the mmaction repository (which is what the authors of the TSN library you suggest ... suggest using) and the approach in that repository is to oversample by first computing the crops and flips and then run that through the model. 

I noticed in #3 that you said you don't remember if you used oversample or not. Has that changed by any chance? It would save me a lot of time if you can remember that.

Also, I noticed that the size of the features in the provided CSV were each of size 400. That seems small given that the TSN outputs features of size 1024 out of the box. Is there some other setting you used to get size 400 features?

Thanks for your help.",neutral,positive
274,关于ISA 代码,openseg-group/openseg.pytorch,closed,,swjtulinxi,PkuRainBow,2020-09-04 08:13:16,2020-09-04 12:25:26,1.0,,34.0,692874339.0,,neutral,neutral
275,demo报错,open-mmlab/mmdetection,closed,"AronLin
BIGWangYuDong",bingo789,AronLin,2021-09-07 14:53:47,2021-09-22 03:25:29,4.0,bug,6057.0,990071934.0,"### 运行命令

`python demo/image_demo.py demo/demo.jpg  configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py  checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth --device=cpu`

### 报错
```
Traceback (most recent call last):
  File ""demo/image_demo.py"", line 50, in <module>
    main(args)
  File ""demo/image_demo.py"", line 32, in main
    show_result_pyplot(model, args.img, result, score_thr=args.score_thr)
  File ""/Users/bingo/opt/anaconda3/envs/ai/lib/python3.7/site-packages/mmdet/apis/inference.py"", line 241, in show_result_pyplot
    text_color=(72, 101, 241))
  File ""/Users/bingo/opt/anaconda3/envs/ai/lib/python3.7/site-packages/mmdet/models/detectors/base.py"", line 343, in show_result
    out_file=out_file)
  File ""/Users/bingo/opt/anaconda3/envs/ai/lib/python3.7/site-packages/mmdet/core/visualization/image.py"", line 172, in imshow_det_bboxes
    img_rgba = buffer.reshape(height, width, 4)
ValueError: cannot reshape array of size 1090560 into shape (427,640,4)
```
### 版本信息
`mmdet: 2.16.0`
",neutral,neutral
276,MovingAverage optimizer swap_weights is not very usable ,tensorflow/addons,closed,,marksandler2,bhack,2020-08-20 23:30:55,2020-10-08 21:45:28,2.0,optimizers,2107.0,683155758.0,"**System information**
Any

**Describe the bug**

If one calls `optimizer.swap_weights` without calling `optimizer.shadow_copy` it crashes with cryptic error message
 (of _average_weights not existing) - despite successfully optimizing for earlier steps (and presumably creating average variables - just not setting up _average_weight!). 

However, ` shadow_copy` requires a `var_list`, which is not something that is necessarily available at the call site that needs eed to swap_weights, which leads to implementation details leaking over the place. 

**Code to reproduce the issue**
```python
optimizer = tf.keras.optimizers.RMSprop(
       learning_rate=lr_schedule, rho=0.9, momentum=0.9, epsilon=0.001)
optimizer = tfa.optimizers.MovingAverage(
        optimizer, average_decay=FLAGS.moving_average_decay)

run_training(optimizer) # trainng loop code (that is not aware that optimizer is MovingAverage optimizer
# now I want to swap weights to run evaluation:
optimizer.swap_weights()  # crashes because shadow_copy was never created. 
run_eval()
```

**Other info / logs**

It would be much better if _create_slots() would also setup _average_weights, and_model_weights,
 so that swap_weights could be called as soon as slots are created. Here is a simple change that makes it workable again:

```python
  def _create_slots(self, var_list):
    self._optimizer._create_slots(var_list=var_list)  # pylint: disable=protected-access
    for var in var_list:
      self.add_slot(var, ""average"", var.read_value()) 
    ###### new code
    self._average_weights = [
        self.get_slot(var, ""average"") for var in var_list
    ]
    self._model_weights = var_list
    ###### end new code
```
Optional: delete shadow_copy function

",neutral,negative
277,Batch Size and Adam Optimizer,yangyanli/PointCNN,closed,burui11087,rruixxu,burui11087,2019-03-14 05:11:17,2019-03-28 07:27:08,2.0,,152.0,420834609.0,"Hi, 
I am reading your tensorflow code, but I found the batch size used in the code is different from that declared in your NIPS paper. In the code, batch size is 128 for ModelNet40, however, it becomes 16 in your paper. What is the real batch size?

I also found that epsilon in Adam optimizer is 0.01, but epsilon for Adam is usually very small like 1e-7 or 1e-8. The purpose of epsilon is to prevent from dividing by zero, isn't it? Is it possible or are there other motivations for you to set epsilon so large?

Thank you!",neutral,positive
278,very low mAP when training from scratch and using bn,open-mmlab/mmdetection,closed,,ZQ-zz,ZwwWayne,2019-09-29 13:58:09,2022-07-08 11:55:20,4.0,community help wanted,1468.0,499921546.0,"I got bad result when training from scratch and using bn or syncBN.
I think my config is right for bn:
'norm_cfg = dict(type='BN', requires_grad=True)'
'norm_eval = False'
and change the norm_cfg in backbone, neck and head
case 1:
coco dataset from scratch
I train fcos, and change input size to (512, 512), batch size can up to 28 for single gpu, after 74epoch get only 18.9; but when using pretrained model and gn, can get mAP about 35.7.
case 2: 
own dataset from scratch
when I train my own dataset, after 10epoch, gn can get mAP about 21.7 and bn can only get mAP 2.0, the two config is the same except for bn and gn and batch size is 16 for one gpu.

I think when using bn and batch size is big enough, training from scratch can get similar result with pretrained result.  It seems there is some thing wrong with bn ？ Could you reproduce any model without pretrained weights and using bn or syncBN？ I have tried fcos，retinaNet，r50 and hrnet... in mmdetection, but all failed",neutral,positive
279,Train model using 3+ channels ,matterport/Mask_RCNN,closed,,olgaliak,waleedka,2018-03-08 23:25:54,2018-06-07 22:24:48,13.0,,314.0,303669893.0,"Hello!
I'd like to use Mask RCNN for satellite imagery analysis.  In addition to  aerial view (RGB input), I want model to learn from hillshade view (thus learn topography) as well as Indexes data (NWDI -- Normalized Difference Water Index, EVI --Enhanced Vegetation Index).
Could you please provide guidance how to update the code to support N channels (instead of 3)?
Also ideally I'd love  to use pretrained weights when possible rather start completely from scratch all the time.
Thanks a lot!",neutral,positive
280,Saving past training,percyliang/sempre,closed,,hjw9673,ppasupat,2016-06-28 06:57:52,2016-09-27 21:09:45,5.0,,101.0,162615533.0,"Do we have to train every single time we want to test in ""tables mode""?
Or is there a way to test without training (using training data from last time?)
If so, how can I do it?

Thank you.
",neutral,neutral
281,Reduce Image size,Zhongdao/Towards-Realtime-MOT,closed,,kikirizki,kikirizki,2019-10-10 05:49:47,2019-10-11 02:46:58,2.0,,14.0,505050105.0,Can I reduce image size without retrain the model?,neutral,neutral
282,How to retrieve mAP score for my own data set?,MahyarNajibi/SNIPER,closed,,bfialkoff,bfialkoff,2019-05-06 14:10:59,2019-05-15 07:52:21,1.0,,132.0,440719427.0,"I see that in the paper and in the readme the mAP score is published for the pascal and coco datasets, but I can't see how to generate the scores for my own dataset.",neutral,neutral
283,About the category list of ImageNet-100 subset,HobbitLong/CMC,closed,,szq0214,HobbitLong,2019-11-29 21:48:20,2019-12-03 05:23:30,6.0,,21.0,530494164.0,"Hi @HobbitLong, thanks for your great work and also sharing the code. I guess the ImageNet-100 is not a conventional subset so I wonder if you can share the list since we also don't have enough resources to run on the full ImageNet ==.",neutral,positive
284,What's the correct structure for polygons in annotations? ,matterport/Mask_RCNN,closed,,deveshdatwani,deveshdatwani,2021-12-11 18:39:50,2021-12-11 18:40:07,0.0,,2740.0,1077615257.0,,neutral,neutral
285,test on MOT20,Zhongdao/Towards-Realtime-MOT,closed,,MCQCQM,MCQCQM,2020-05-16 10:27:14,2020-05-16 13:59:39,0.0,,155.0,619437883.0,"when testing on MOT20 train dataset,there is an error:
Traceback (most recent call last):
  File ""track20.py"", line 173, in <module>
    save_videos=opt.save_videos)
  File ""track20.py"", line 98, in main
    dataloader = datasets.LoadImages(osp.join(data_root, seq, 'img1'), opt.img_size)  #todo MOT20时没有file属性
  File ""G:\programming practice\Towards-Realtime-MOT\utils\datasets.py"", line 25, in __init__
    self.nF = len(self.files)  # number of image files
AttributeError: 'LoadImages' object has no attribute 'files'
how to save this?",neutral,neutral
286,"WARNING:root:You are using the default load_mask(), maybe you need to define your own one.",matterport/Mask_RCNN,open,,AidaSilva,,2021-07-06 21:32:55,,8.0,,2625.0,938276581.0,"WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
Epoch 1/10
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.
WARNING:root:You are using the default load_mask(), maybe you need to define your own one.


I have this error when I am trying to costum MaskRCNN on my own dataset.
Is there anyone has faced the same issue and got the solution",neutral,neutral
287,Why add noise to GCN network？,nnaisense/deep-iterative-surface-normal-estimation,open,,nickucas123,,2021-01-08 08:05:43,,0.0,,2.0,781931851.0,Why add noise to GCN network，in line 57  'cov = cov + torch.diag(noise).cuda()',neutral,neutral
288,Question about data augmentation and memory bank,HobbitLong/CMC,open,,talshef,,2020-03-25 09:27:57,,0.0,,51.0,587558837.0,"Hi, Thanks a lot for sharing this great code.
I have a question about data augmentation and the memory bank. If we use data augmentation, the features in the memory bank are not update for this issue. Especially for the positive examples which we using from the memory bank.
Have you thought about it?",neutral,positive
289,Keras implementation of the temporal shift,MIT-HAN-LAB/temporal-shift-module,closed,,hammacktony,hammacktony,2019-10-10 15:51:57,2019-10-21 23:29:26,0.0,,5.0,505360983.0,Anyway you can release a Keras version of the Temporal Shift in `ops/temporal_shift.py`. I am looking at implementing this in a custom layer to use with my project.,neutral,neutral
290,A new question,tancik/StegaStamp,closed,,tongshiwen,tongshiwen,2019-07-02 14:54:26,2019-07-08 07:11:57,1.0,,6.0,463276631.0,"Have you implemented this algorithm in other deep learning frameworks, such as pytorch, caffe ?",neutral,neutral
291,Training Loss Doesn't Improve for CIFAR10,HobbitLong/CMC,closed,,QiyaoWei,QiyaoWei,2021-01-31 14:19:28,2021-03-04 08:15:07,0.0,,62.0,797722032.0,"Dear authors,

Hope this question finds you well. As I was extending CMC to CIFAR10, I noticed that the model doesn't seem to be training, and l_p and ab_p numbers don't seem to be changing either. This is strange, as I never changed the training code. Please see below for a screenshot of this problem. What might be a cause for this? I am aware of #50, but this problem persists even after tuning the softmax and sampling parameters.

<img width=""566"" alt=""Screen Shot 2021-01-31 at 11 49 54 AM"" src=""https://user-images.githubusercontent.com/36057290/106386919-41240280-6412-11eb-9173-bb1bbf96d7a3.png"">
",negative,positive
292,Why the value function seems different from that in the paper?,mlii/mfrl,closed,,woaipichuli,mlii,2018-06-26 03:13:07,2018-07-04 09:58:46,7.0,,1.0,335642349.0,"I found the value function in the code is a Q table and is defined as (Q = np.zeros((n_agents, dim_Q_state, n_actions))). 
However, the Q function in the paper is defined as (state,action1,action2). Why they are different?",neutral,neutral
293,May I ask whether this project is for image stitching? I look forward to your answer,JirongZhang/DeepHomography,open,,csdyang,,2020-09-02 08:31:03,,2.0,,21.0,690827902.0,,neutral,neutral
294,pretrained models,carolineec/EverybodyDanceNow,open,,ak9250,,2020-09-20 17:30:28,,5.0,,1.0,705150436.0,will the pretrained models be released?,neutral,neutral
295,DETECTION_MAX_INSTANCES - detections of up to 1000 objects per image,matterport/Mask_RCNN,open,,velaia,,2019-11-25 23:17:10,,3.0,,1884.0,528393250.0,"Hi all, thanks for this great research and publishing it here! And thanks to many others for their great questions and answers. ❤

I'm working on a model that detects metal bars in bundles of bars. I'm using the Mask R-CNN implementation that comes with Detectron2 (the parameter there is called DETECTIONS_PER_IMAGE I think) and face the issue that the number of bars detected peaks at a certain threshold. 
First the detections stopped at 100.
Then I found the above mentioned parameter and adjusted it to 1000. Now the number of detections peaks at about 469. I've adjusted the SCORE_THRESH_TEST to 0.5 but the maximum number seems to be fixed. Because the objects to detect can be pretty small I've also adjusted the ANCHOR_GENERATOR sizes to include [16] in the (RPN) ANCHOR_GENERATOR section of the model.
What else can I do? Adjust RESNETS.OUT_FEATURES to not include ""res5""? What's the detection limit that comes with the architecture (if any)?

This is an image showing the detections:
![image](https://user-images.githubusercontent.com/1515904/69585833-1a359980-0fe1-11ea-8c95-96dc912c5cac.png)",neutral,positive
296,Avg cost function,lorenmt/mtan,closed,,vesaalexandru95,lorenmt,2021-02-08 05:36:41,2021-02-10 02:22:36,2.0,,37.0,803206449.0,"Hello ! 
Can you explain please why did you initialize the avg cost function with  24?
eg :      avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)
 ",neutral,neutral
297,the output images' format should be changed from CHW to HWC,anuragranj/cc,open,,nightmaredimple,,2019-06-13 20:01:32,,8.0,,10.0,455927343.0,"Tensors in pytorch are formatted in CHW(BCHW) by default, so if you wanna output the results of depth,flow and mask, you should change them into HWC format.
such as:
 test_flow.py line 180
>row1_viz_im = Image.fromarray((255*row1_viz).astype('uint8'))
row2_viz_im = Image.fromarray((row2_viz).astype('uint8'))

this will raise TypeError(""Cannot handle this data type"")
you should transpose/permute the format into HWC like this below:

>row1_viz_im = Image.fromarray((255*row1_viz).astype('uint8').transpose((1,2,0)))
row2_viz_im = Image.fromarray((row2_viz).astype('uint8').transpose((1,2,0)))
",neutral,neutral
298,how to add reid,timmeinhardt/trackformer,open,,liu0623,,2021-09-29 02:23:01,,11.0,,20.0,1010405666.0,"hi
I want to add some reid network.But I do not know how to do it .could you help me? or give me some advice.
Thank you!",neutral,positive
299,IMAGE_RESIZE_MODE='none' not work?,matterport/Mask_RCNN,open,,world4jason,,2018-06-17 20:21:36,,2.0,,689.0,333089948.0,"the size of my training data is 1024x2048
and what i set in the Config class is like this

        class DataConfig(Config):
            IMAGE_RESIZE_MODE = 'none'
            IMAGE_MIN_DIM = 1024
            IMAGE_MAX_DIM = 2048

it shows that the image shape is 2048x2048
IMAGE_RESIZE_MODE              none
IMAGE_SHAPE                    [2048 2048    3]

and what in config.py is

        if self.IMAGE_RESIZE_MODE == ""crop"":
            self.IMAGE_SHAPE = np.array([self.IMAGE_MIN_DIM, self.IMAGE_MIN_DIM, 3])
        else:
            self.IMAGE_SHAPE = np.array([self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 3])

so this means i always get a square image?

=====================================

BTW is it suggested to use square images for training ?

",neutral,positive
300,Batch_size expression,matterport/Mask_RCNN,open,,HAMZARaouia,,2019-10-25 16:17:05,,1.0,,1824.0,512603099.0,"In keras, 
**batch_size = [ train set size / steps_per_epoch ]** 

In the configuration,
**batch_size = GPU_count * Images_Per_GPU**

Given the fact that the steps_per epoch is a parameter to change, these two expression give different results. 

Any ideas about which one to hold ?",neutral,neutral
301,Do you plan to provide dataset part code for training imagenet vid and det?,open-mmlab/mmdetection,closed,,TyroneLi,ZwwWayne,2019-10-14 12:08:58,2021-07-20 02:53:22,2.0,enhancement#community help wanted,1542.0,506612694.0,As what I mention above.,neutral,neutral
302,Config files and pretrained weights for Path Aggregation Network (PANet/PAFPN),open-mmlab/mmdetection,closed,v-qjqs,bpmsilva,ZwwWayne,2020-10-29 23:35:40,2020-11-05 07:05:40,1.0,,4030.0,732742184.0,"**Describe the feature**
I would like to suggest the incorporation of config files and pre-trained weights for the Path Aggregation Network (PANet/PAFPN).

**Motivation**
The YOLO v4 explores the same concepts of PANet. Also, I had good results with the PANet architecture in a recent benchmark. However, I had to use [this repo](https://github.com/ShuLiu1993/PANet), which is based on [detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch). The [detectron](https://github.com/facebookresearch/Detectron) and the [detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch) are deprecated, and I had a lot of trouble to use PANet due to compatibility issues. The mmdetection repo includes config files and pre-trained weights for the PANet, but only for detection (no instance segmentation). Also, the backbone is only the ResNet-50 (is there a limitation to use deeper backbones?).

**Related resources**
I link [ShuLiu1993/PANet](https://github.com/ShuLiu1993/PANet), but I think the repo comes from someone from the Multimedia Laboratory, CUHK. So, I don't think it is something new to you guys.

Thanks for your attention.

",neutral,positive
303,Backbone with DCNv2 cannot implement training procedure,open-mmlab/mmdetection,closed,jbwang1997,gsygsy96,gsygsy96,2022-08-03 04:30:01,2022-08-04 10:00:05,3.0,,8478.0,1326663262.0,"I found that  models using resnet101 with DCNV2 cannot reproduce same gradient  after first backward with same config file, even with fix random seed and turn cudnn.deterministic==True.
Here is part of my config file:

> model = dict(type='DebugFrame',
>              debug = True,
>              backbone=dict(type='ResNet',
>                             depth=101,
>                             num_stages=4,
>                             out_indices=(0, 1, 2, 3),
>                             frozen_stages=0,
>                             norm_cfg=dict(type='BN2d', requires_grad=False),
>                             norm_eval=True,
>                             style='caffe',
>                             dcn=dict(type='DCNv2', deform_groups=1, fallback_on_stride=False),
>                             stage_with_dcn=(False, False, True, True)))
> optimizer = dict(type='SGD', lr=0.00005, momentum=0.)
> optimizer_config = dict()

Note that to debug this problem , I build a model called'DebugFrame', whose structure is shown below:

> @DETECTORS.register_module()
> class DebugFrame(BaseDetector):
>     def __init__(self,
>                  debug = True,
>                  backbone= None,
>                  *args,
>                  **kwargs):
>         super(DebugFrame, self).__init__()
>         self.debug = debug
>         self.backbone = build_backbone(backbone)
>         self.loss_type = torch.nn.L1Loss()
>         self.count = 0
> 
>     def init_weights(self):
>         pass
> 
>     def forward(self, return_loss=True, **kwargs):
> 
>         if return_loss:
>             return self.forward_train(**kwargs)
>         else:
>             return self.forward_test(**kwargs)
> 
>     def forward_train(self, img, *arg, **kwargs):
>         if self.debug:
>             save_dict = dict(backbone=dict(params=dict(),
>                                            grad=dict()))
> 
>             for k, v in self.backbone.named_parameters():
>                 save_dict['backbone']['params'][k] = v.clone().detach().cpu()
>                 if self.count == 0:
>                     save_dict['backbone']['grad'][k] = None
>                 else:
>                     save_dict['backbone']['grad'][k] = v.grad.clone().detach().cpu() if v.grad is not None else None
>             torch.save(save_dict, '/data1/CODE/detr_3d_sr/'
>                                   'work_dir/debug_backbone/backbone_%d.pth' % self.count)
>             print('BB saved')
>             self.count+=1
> 
>         img = rearrange(img, 'b n c h w -> (b n) c h w')
>         feats  = self.backbone(img)
>         losses = dict()
>         for i, f in enumerate(feats):
>             cur_gt = torch.ones_like(f)
>             losses[str(i)+'_loss'] = self.loss_type(cur_gt, f)*0.1
>         return losses
> 
> 
>     def aug_test(self, *args, **kwargs):
>         pass
> 
>     def extract_feat(self, *args, **kwargs):
>         pass
> 
>     def simple_test(self, *args, **kwargs):
>         pass


I checked gradient after the first backward(save_dict['backbone']['grad']) and the gradient varies firstly at layer 'layer4.2.conv1.weight', which receives gradient right after the most bottem DCN layer.

Samely, the gradient variation problem disappears when I shut down the DCN, with config '  stage_with_dcn=(False, False, False, False)))'
",neutral,positive
304,About caffe-style model,Microsoft/human-pose-estimation.pytorch,open,,ZhongXiaoFang,,2019-09-09 12:02:26,,0.0,,127.0,491053748.0,"Hi, thank you for your well done jobs. I want test the caffe-style model on val2017. where I can download  the caffe-style model you have trained ,such as ""256x192_pose_resnet_50_caffe "".

Thank you for your help",neutral,positive
305,What is the raw data?,jiesutd/LatticeLSTM,closed,,MrRace,jiesutd,2018-07-09 11:17:38,2018-07-09 11:24:56,3.0,,11.0,339407735.0,"(1)We can feed three kinds of parameter:""train""，""test"" and ""decode"" to the main.py. In ""train"" step you have use ""dev"" set to choose best mode and save it. It seems that you use the ""test"" data to print the model'performance each iteration. Am I right? When status=""test"",you also use the ""dev"" data and ""test"" data to show the model'performance, but your have used  them during trainning stage. Is that OK？
(2)In the main.py you mention ""raw"" data when status argument is ""decode"".Where to get the ""raw"" data?",neutral,positive
306,Relation between Prediction challenge and images from camera,nutonomy/nuscenes-devkit,closed,,ranran9991,holger-motional,2021-05-23 09:47:41,2021-05-24 10:09:58,1.0,,599.0,898979886.0,"Hey,
I'm trying to understand the relation between the prediction challenge and the cameras on the vehicle

In the prediction challenge, we're trying to predict the trajectories of some agent in the scene. 
Is this agent, the same agent that is taking pictures? or is it some other vehicle?
If they are not the same one, is the agent on which we predict trajectories present in the images? can we use them for the prediction challenge?",neutral,neutral
307,all_points_y error when training or generating data,matterport/Mask_RCNN,open,,mia2mia,,2018-04-30 20:25:56,,10.0,,503.0,319023588.0,"I am trying to train Mask RCNN on my own data but I get the errors below when training or loading the mini mask or using the modellib.data_generator.

Anyone else had this problem?

ERROR:root:Error processing image {'id': 'location-494-270.jpg', 'source': 'flowers', 'path': '/home/ubuntu/ssl/notebooks/Mask_RCNN/Mask_RCNN/datasets/flowers/train/file2.jpg', 'width': 640, 'height': 400, 'polygons': [{'name': 'rect', 'x': 301, 'y': 178, 'width': 40, 'height': 93}]}
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1692, in data_generator
    use_mini_mask=config.USE_MINI_MASK)
  File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py"", line 1207, in load_image_gt
    mask, class_ids = dataset.load_mask(image_id)
  File ""flowers.py"", line 158, in load_mask
    rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])
KeyError: 'all_points_y'Epoch 1/30
",neutral,neutral
308,About dataset identity,deepinsight/insightface,closed,,dajidali010,nttstar,2018-05-22 08:01:53,2018-09-18 03:03:16,0.0,,225.0,325181113.0," As you say, the emore dataset is still largely based on ms1m, do you add new identity besides ms1m? Can you public the names? Thanks a lot.
 best ",neutral,positive
309,Regarding training loop.,anuragranj/cc,closed,,ezorfa,ezorfa,2020-01-22 14:10:26,2020-01-22 14:17:45,0.0,,22.0,553566234.0,"Hi @anuragranj !

It is not clear from the train.py that how you were able to train like the training algorithm mentioned in the paper. How did you do that?

Thankyou!",neutral,neutral
310,ImportError: No module named 'vtkOpenGLKitPython',princeton-vl/DeepV2D,open,,ghoshaw,,2020-09-10 09:01:30,,0.0,,27.0,697643957.0,"Hi, I run the demo as README. I install the vtk and I am sure that 'vtkOpenGLKitPython.so' is in /usr/local/lib/python3.5/dist-packages/vtk/ folder. 
but still got the error as mentioned above. ",neutral,negative
311,"How to understand the NN part, e.g., qnet.cpp",Hanjun-Dai/graph_comb_opt,open,,tianshangaga,,2019-05-07 12:55:48,,5.0,,13.0,441217488.0,"I find it is not easy to understand how does the nn works, e.g., the func 'QNet::SetupGraphInput()' and ' QNet::BuildNet()', so can you give more detailed instructions about the code? thanks a lot!",neutral,positive
312,CUDA out of memory ,open-mmlab/mmdetection,closed,"ZwwWayne
BIGWangYuDong",alaa-shubbak,BIGWangYuDong,2021-08-14 15:26:25,2022-08-10 08:25:08,28.0,enhancement,5889.0,970933525.0,"Dear All , 

I have a question reagrd the problem of CUDA out of the memory. 

I try to train a CentriapetalNet on my custom dataset , which has the same format of COCO dataset. 
this is the config file : https://github.com/open-mmlab/mmdetection/tree/master/configs/centripetalnet
I got such error massage on CUDA memory ,  I changed the  following code 
from 6*3 to 16*4 

```
data = dict(
    samples_per_gpu=6,
    workers_per_gpu=3,
```
I noticed that changing such batch size help in reduce the CUDA memory needs during the training. 
but it is still not working , I did not understand the theory behind . also  I really confused what should I do to continues the training. 

I read this document : 
https://mmdetection.readthedocs.io/en/latest/faq.html , the part of CUDA memory

and i did not know how to change those three items described there. 

any help will be highly appreciated.  




![cuda out of memory](https://user-images.githubusercontent.com/10245810/129450800-b52d63ee-05c7-4b4d-9a11-78803c0a96cb.JPG)
",neutral,negative
313,always zero_mean function?,wujian16/Cornell-MOE,open,,shalijiang,,2019-04-05 16:13:59,,0.0,,70.0,429833192.0,"seems by default the prior mean function of the GP is zero, which is inappropriate if, e.g., the target function is always positive. 
Is there a way to specify a constant mean or other mean functions?",neutral,neutral
314,Google Colab installation ModuleNotFoundError,matterport/Mask_RCNN,closed,,jeremyimmanuel,jeremyimmanuel,2019-08-01 07:53:29,2019-08-11 22:00:33,5.0,,1673.0,475530108.0,"Hello, 

I'm trying to use this module in Google Colab.
So I successfully installed the module with the following commands.

` !git clone https://github.com/matterport/Mask_RCNN.git`
`!pip install -r 'Mask_RCNN/requirements.txt' `
`!cd Mask_RCNN ; python setup.py install`

Then `!pip show mask-rcnn` works, but when I tried to import mrcnn it said 'No Module Found'
![image](https://user-images.githubusercontent.com/35509768/62275370-a4387180-b3f6-11e9-81bb-e6a3159c8916.png)

Can someone help me out with this problem? Thank you so much.",neutral,positive
315,How did you do the bbox ensemble ?,MahyarNajibi/SNIPER,open,,PacteraOliver,,2018-06-28 22:06:45,,3.0,question,21.0,336796476.0,"Hi,

Thank you for your repo and paper. This is an awesome work!

I want to ask that in your paper [An Analysis of Scale Invariance in Object Detection – SNIP](https://www.cs.umd.edu/~bharat/crsnip.pdf)

You mentioned an ensemble operation. You said that:
> Since proposals are shared across all networks, we
average the scores and box-predictions for each RoI. During
flipping we average the detection scores and bounding
box predictions. 

How to average box predictions for each RoI? 

How did you implemented in your code?

Thank you!",neutral,positive
316,"butd demo results  ""a close up of a white and white photo""",facebookresearch/mmf,closed,,mnbv7581,apsdehal,2021-01-13 01:59:39,2021-01-19 06:44:52,3.0,,729.0,784724161.0,"## ❓ Questions and Help

[https://colab.research.google.com/drive/1Z9fsh10rFtgWe4uy8nvU4mQmqdokdIRR?usp=sharing](url)

We implemented butd demo source by modifying the colab demo source. Using maskrcnnn_benchmark api, extract feature from COCO image and create a sample class. After setting with input data, the prediction progressed. Successful execution but learned by coco dataset Even though it is a model weight, the result is always ""a close up of a white and white photo"". by any chance Is there anything I missed?

```python

 def predict(self, url, question):
    with torch.no_grad():

      detectron_features, image = self.get_detectron_features(url)

      sample = Sample()

      tokens=[]
      ref_list = []
      for _ in range(0,52):
          tokens.append(0)
          
      tokens = torch.tensor(tokens)
      ref_list.append(tokens)

      sample.answers=torch.stack(ref_list)[: 5]
      sample.image_feature_0 = detectron_features
      #sample.image_feature_0 = resnet_features
      sample.image_info_0 = Sample({
          ""max_features"": torch.tensor(100, dtype=torch.long)
      })
      #sample.image_info_1 = Sample({
      #    ""max_features"": torch.tensor(196, dtype=torch.long)
      #})

      sample.dataset_name = 'coco'
      sample.dataset_type = 'val'

      sample_list = SampleList([sample])
      sample_list = sample_list.to(""cuda"")

      captions = self.butd_model(sample_list)[""captions""]
      captions = captions[0].type(torch.int32).tolist()
      captions = self.caption_processor(captions)

    gc.collect()
    torch.cuda.empty_cache()
    
    return captions

```
",neutral,positive
317,训练了20多轮，此时lr=0.000001，还是会Nan,deepinsight/insightface,open,,EdwardVincentMa,,2019-11-13 02:51:15,,4.0,,977.0,521924469.0,"INFO:root:Epoch[24] Batch [520-540]     Speed: 1727.27 samples/sec      acc=0.983984    lossvalue=2.195642
INFO:root:Epoch[24] Batch [540-560]     Speed: 1288.51 samples/sec      acc=0.983203    lossvalue=2.237722
INFO:root:Epoch[24] Batch [560-580]     Speed: 1296.01 samples/sec      acc=0.983594    lossvalue=2.236295
INFO:root:Epoch[24] Batch [580-600]     Speed: 1339.65 samples/sec      acc=0.982617    lossvalue=2.273337
INFO:root:Epoch[24] Batch [600-620]     Speed: 2007.56 samples/sec      acc=0.983398    lossvalue=2.299879
INFO:root:Epoch[24] Batch [620-640]     Speed: 1343.99 samples/sec      acc=0.983789    lossvalue=2.221828
INFO:root:Epoch[24] Batch [640-660]     Speed: 1290.73 samples/sec      acc=0.983984    lossvalue=2.208789
INFO:root:Epoch[24] Batch [660-680]     Speed: 1371.16 samples/sec      acc=0.984375    lossvalue=2.210849
INFO:root:Epoch[24] Batch [680-700]     Speed: 2012.76 samples/sec      acc=0.983984    lossvalue=2.210747
INFO:root:Epoch[24] Batch [700-720]     Speed: 1319.51 samples/sec      acc=0.984180    lossvalue=2.203272
INFO:root:Epoch[24] Batch [720-740]     Speed: 1287.94 samples/sec      acc=0.984180    lossvalue=2.205525
INFO:root:Epoch[24] Batch [740-760]     Speed: 1382.29 samples/sec      acc=0.983984    lossvalue=2.197704
INFO:root:Epoch[24] Batch [760-780]     Speed: 2022.44 samples/sec      acc=0.983594    lossvalue=2.228923
INFO:root:Epoch[24] Batch [780-800]     Speed: 1306.10 samples/sec      acc=0.983984    lossvalue=2.219678
INFO:root:Epoch[24] Batch [800-820]     Speed: 1290.28 samples/sec      acc=0.984180    lossvalue=2.214591
INFO:root:Epoch[24] Batch [820-840]     Speed: 1295.13 samples/sec      acc=0.442383    lossvalue=nan
INFO:root:Epoch[24] Batch [840-860]     Speed: 1689.81 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [860-880]     Speed: 1493.23 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [880-900]     Speed: 1303.18 samples/sec      acc=0.000000    lossvalue=nan

INFO:root:Epoch[24] Batch [1760-1780]   Speed: 1374.06 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1780-1800]   Speed: 1300.23 samples/sec      acc=0.000195    lossvalue=nan
INFO:root:Epoch[24] Batch [1800-1820]   Speed: 1303.87 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1820-1840]   Speed: 1606.54 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1840-1860]   Speed: 1593.05 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1860-1880]   Speed: 1305.64 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1880-1900]   Speed: 1301.60 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Train-acc=0.000000
INFO:root:Epoch[24] Train-lossvalue=nan
INFO:root:Epoch[24] Time cost=363.141
call reset()
INFO:root:Epoch[25] Batch [0-20]        Speed: 1585.96 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[25] Batch [20-40]       Speed: 1304.34 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[25] Batch [40-60]       Speed: 1302.62 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[25] Batch [60-80]       Speed: 1786.20 samples/sec      acc=0.000195    lossvalue=nan
lr-batch-epoch: 1e-05 99 25
testing verification..
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 369, in train_net
    epoch_end_callback = epoch_cb )
  File ""/software/python-3.6/lib/python3.6/site-packages/mxnet/module/base_module.py"", line 553, in fit
    callback(batch_end_params)
  File ""train.py"", line 305, in _batch_callback
    acc_list = ver_test(mbatch)
  File ""train.py"", line 274, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, args.batch_size, 10, None, None)
  File ""eval/verification.py"", line 272, in test
    embeddings = sklearn.preprocessing.normalize(embeddings)
  File ""/software/python-3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py"", line 1614, in normalize
    estimator='the normalize function', dtype=FLOAT_DTYPES)
  File ""/software/python-3.6/lib/python3.6/site-packages/sklearn/utils/validation.py"", line 542, in check_array
    allow_nan=force_all_finite == 'allow-nan')
  File ""/software/python-3.6/lib/python3.6/site-packages/sklearn/utils/validation.py"", line 56, in _assert_all_finite
    raise ValueError(msg_err.format(type_err, X.dtype))
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
",neutral,neutral
318,How to draw the Recall-IoU curve?,matterport/Mask_RCNN,open,,mengsuma,,2019-11-17 07:42:06,,0.0,,1863.0,523950213.0,,neutral,neutral
319,can you share the dataset of eurocity person ?,hasanirtiza/Pedestron,closed,,tangsipeng,tangsipeng,2020-06-26 02:57:36,2020-06-26 13:18:47,1.0,,48.0,645971518.0,"can you share the dataset of eurocity person ?

",neutral,neutral
320,Evaluation on S3DIS takes so long,yangyanli/PointCNN,open,,linhaojia13,,2020-04-17 11:33:04,,0.0,,222.0,601909568.0,"Hi, thank you for your teams' persistent maintennance for this issue community!

For evaluating the trained model, I follow the Readme to run the script '/test_s3dis.sh'. However, I find it takes several hours, which completely disobeys my expectation about fast evaluation for a trained model. I think fast evaluation is very important for develop a desirable neural networks, not excepting PointCNN. How can I conduct a fast evaluation for the trained model on S3DIS?",neutral,positive
321,Encounter error when trying to use ResNet-50 to replace VGG-19 as the loss network,DmitryUlyanov/texture_nets,open,,michaelhuang74,,2017-03-11 05:20:40,,2.0,,67.0,213506754.0,"I tried to replace the VGG-19 with ReSNet-50 for the loss network used in the training.
I downloaded the prototxt and caffemodel files from https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777, which is given on https://github.com/KaimingHe/deep-residual-networks

Then I tried to launch the training as following:
 th train.lua -style_image style/face_bottle_o.png -style_size 512 -image_size 256 -style_layers res2b_relu,res3b_relu,res4b_relu,res5b_relu -content_layers res4b_relu -style_weight 20 -content_weight 1 -proto_file data/pretrained/ResNet-50-deploy.prototxt -model_file data/pretrained/ResNet-50-model.caffemodel -checkpoints_path checkpoint/ -checkpoints_name face_bottle_o.s512i256.sL2b3b4b5b.cL4b.sw20.cw1.ResNet50.4x -num_iterations 10000 -batch_size 4 -save_every 2000

I encountered the following errors.
torch.display not found. unable to plot
Using TV loss with weight       1e-06
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 26:26: Message type ""caffe.LayerParameter"" has no field named ""batch_norm_param"".
Successfully loaded data/pretrained/ResNet-50-model.caffemodel
warning: module 'bn_conv1 [type BatchNorm]' not found
warning: module 'scale_conv1 [type Scale]' not found
warning: module 'pool1_pool1_0_split [type Split]' not found
warning: module 'bn2a_branch1 [type BatchNorm]' not found
warning: module 'scale2a_branch1 [type Scale]' not found
warning: module 'bn2a_branch2a [type BatchNorm]' not found
warning: module 'scale2a_branch2a [type Scale]' not found
..........(similar warning messages)
warning: module 'bn5c_branch2b [type BatchNorm]' not found
warning: module 'scale5c_branch2b [type Scale]' not found
warning: module 'bn5c_branch2c [type BatchNorm]' not found
warning: module 'scale5c_branch2c [type Scale]' not found
warning: module 'res5c [type Eltwise]' not found
conv1: 64 3 7 7
res2a_branch1: 256 64 1 1
Segmentation fault (core dumped)

Any idea how to resolve this issue? Thanks.
",negative,neutral
322,RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.,open-mmlab/mmdetection,closed,yhcao6,tankche1,tankche1,2020-02-19 22:05:58,2020-02-20 16:14:58,2.0,,2117.0,567877068.0,"**Checklist**
1. I have searched related issues but cannot get the expected help.
2. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

**Reproduction**
1. What command or script did you run?
```
./tools/dist_train.sh configs/pascal_voc/fast_rcnn_r50_fpn_1x_voc0712.py 4
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?

Yes. I load selective search bounding boxes and change the num_classes from 81 to 21.(from coco to pascal voc)
Here is the config:
```
# model settings
model = dict(
    type='FastRCNN',
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    bbox_roi_extractor=dict(
        type='SingleRoIExtractor',
        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),
        out_channels=256,
        featmap_strides=[4, 8, 16, 32]),
    bbox_head=dict(
        type='SharedFCBBoxHead',
        num_fcs=2,
        in_channels=256,
        fc_out_channels=1024,
        roi_feat_size=7,
        num_classes=21,#####
        target_means=[0., 0., 0., 0.],
        target_stds=[0.1, 0.1, 0.2, 0.2],
        reg_class_agnostic=False,
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))
# model training and testing settings
train_cfg = dict(
    rcnn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.5,
            min_pos_iou=0.5,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=512,
            pos_fraction=0.25,
            neg_pos_ub=-1,
            add_gt_as_proposals=True),
        pos_weight=-1,
        debug=False))
test_cfg = dict(
    rcnn=dict(
        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))
# dataset settings
dataset_type = 'VOCDataset'
data_root = 'data/VOCdevkit/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)

train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadProposals', num_max_proposals=2000),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1000, 600), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'proposals', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadProposals', num_max_proposals=2000), ###
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1000, 600),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img', 'proposals']),
        ])
]
data = dict(
    imgs_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='RepeatDataset',
        times=4,##
        dataset=dict(
            type=dataset_type,
            ann_file=[
                data_root + 'VOC2007/ImageSets/Main/trainval.txt',
                data_root + 'VOC2012/ImageSets/Main/trainval.txt'
            ],
            proposal_file=[
                data_root + 'voc_2007_trainval.pkl',
                data_root + 'voc_2012_trainval.pkl'
            ],
            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],
            pipeline=train_pipeline)),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',
        img_prefix=data_root + 'VOC2007/',
        proposal_file=data_root + 'voc_2007_test.pkl',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',
        img_prefix=data_root + 'VOC2007/',
        proposal_file=data_root + 'voc_2007_test.pkl',
        pipeline=test_pipeline))
evaluation = dict(interval=1, metric='mAP')
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
# learning policy
lr_config = dict(policy='step', step=[3])
checkpoint_config = dict(interval=1)
# yapf:disable
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook')
    ])
# yapf:enable
# runtime settings
total_epochs = 6##
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/fast_rcnn_r50_fpn_1x_voc0712'
load_from = None
resume_from = None
workflow = [('train', 1)]

```

3. What dataset did you use?
pascal voc 07+12

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment infomation and paste it here.

sys.platform: linux
Python: 3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /cm/shared/apps/cuda100/10.0.130
NVCC: Cuda compilation tools, release 10.0, V10.0.130
GPU 0,1,2,3: GeForce GTX TITAN X
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)
PyTorch: 1.1.0
PyTorch compiling details: PyTorch built with:
  - GCC 4.9
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.18.1 (Git Hash 7de7e5d02bf687f971e7668963649728356e0c20)
  - OpenMP 201307 (a.k.a. OpenMP 4.0)
  - NNPACK is enabled
  - CUDA Runtime 10.0
  - NVCC architecture flags: -gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_50,code=compute_50
  - CuDNN 7.5.1
  - Magma 2.5.0
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS=  -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -W
no-strict-aliasing -Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math, DISABLE_NUMA=1, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, USE_CUDA=True, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_M
PI=OFF, USE_NCCL=True, USE_NNPACK=True, USE_OPENMP=ON,

TorchVision: 0.3.0
OpenCV: 4.2.0
MMCV: 0.3.1
MMDetection: 1.0.0+2afa063
MMDetection Compiler: GCC 6.1
MMDetection CUDA Compiler: 10.0

2. You may add addition that may be helpful for locating the problem, such as
    - How you installed PyTorch [e.g., pip, conda, source]
    - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**
If applicable, paste the error trackback here.
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing its output (the return value of `forward`). You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.Distributed
DataParallel`. If you already have this argument set, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /opt/conda/cond
a-bld/pytorch_1556653114079/work/torch/csrc/distributed/c10d/reducer.cpp:408)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x2aaaf74b1dc5 in /home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: c10d::Reducer::prepare_for_backward(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) + 0x5ff (0x2aaac82fabbf in /home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #2: <unknown function> + 0x6cb6c8 (0x2aaac82f06c8 in /home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #3: <unknown function> + 0x12d07a (0x2aaac7d5207a in /home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #4: _PyMethodDef_RawFastCallKeywords + 0x264 (0x5555556b5114 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #5: _PyCFunction_FastCallKeywords + 0x21 (0x5555556b5231 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #6: _PyEval_EvalFrameDefault + 0x52cf (0x555555719e8f in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #7: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #8: _PyFunction_FastCallDict + 0x400 (0x55555566fa30 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #9: _PyObject_Call_Prepend + 0x63 (0x55555568a943 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #10: PyObject_Call + 0x6e (0x55555567db9e in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #11: _PyEval_EvalFrameDefault + 0x1e35 (0x5555557169f5 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #12: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #13: _PyFunction_FastCallDict + 0x400 (0x55555566fa30 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #14: _PyObject_Call_Prepend + 0x63 (0x55555568a943 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #15: <unknown function> + 0x17512a (0x5555556c912a in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #16: PyObject_Call + 0x6e (0x55555567db9e in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #17: _PyEval_EvalFrameDefault + 0x1e35 (0x5555557169f5 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #18: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #19: _PyFunction_FastCallDict + 0x400 (0x55555566fa30 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #20: _PyEval_EvalFrameDefault + 0x1e35 (0x5555557169f5 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #21: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #22: _PyFunction_FastCallDict + 0x1d5 (0x55555566f805 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #23: _PyObject_Call_Prepend + 0x63 (0x55555568a943 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #24: PyObject_Call + 0x6e (0x55555567db9e in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #25: _PyEval_EvalFrameDefault + 0x1e35 (0x5555557169f5 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #26: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #27: _PyFunction_FastCallKeywords + 0x387 (0x5555556b4917 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #28: _PyEval_EvalFrameDefault + 0x6a0 (0x555555715260 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #29: _PyEval_EvalCodeWithName + 0xc30 (0x55555566f030 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #30: _PyFunction_FastCallKeywords + 0x387 (0x5555556b4917 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x14e6 (0x5555557160a6 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #32: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #33: _PyFunction_FastCallKeywords + 0x387 (0x5555556b4917 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #34: _PyEval_EvalFrameDefault + 0x14e6 (0x5555557160a6 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #35: _PyFunction_FastCallKeywords + 0xfb (0x5555556b468b in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x416 (0x555555714fd6 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #37: _PyEval_EvalCodeWithName + 0x2f9 (0x55555566e6f9 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #38: PyEval_EvalCodeEx + 0x44 (0x55555566f5f4 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #39: PyEval_EvalCode + 0x1c (0x55555566f61c in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #40: <unknown function> + 0x21c974 (0x555555770974 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #41: PyRun_FileExFlags + 0xa1 (0x55555577acf1 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #42: PyRun_SimpleFileExFlags + 0x1c3 (0x55555577aee3 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #43: <unknown function> + 0x227f95 (0x55555577bf95 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #44: _Py_UnixMain + 0x3c (0x55555577c0bc in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)
frame #45: __libc_start_main + 0xf5 (0x2aaaaaf0d3d5 in /lib64/libc.so.6)
frame #46: <unknown function> + 0x1d0990 (0x555555724990 in /home/zitianchen/anaconda2/envs/open-mmlab/bin/python)

```
Traceback (most recent call last):
  File ""./tools/train.py"", line 141, in <module>
    main()
  File ""./tools/train.py"", line 137, in main
    meta=meta)
  File ""/home/zitianchen/code/mmdetection/mmdet/apis/train.py"", line 102, in train_detector
    meta=meta)
  File ""/home/zitianchen/code/mmdetection/mmdet/apis/train.py"", line 251, in _dist_train
    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
  File ""/home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/runner.py"", line 371, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/runner.py"", line 275, in train
    self.model, data_batch, train_mode=True, **kwargs)
  File ""/home/zitianchen/code/mmdetection/mmdet/apis/train.py"", line 75, in batch_processor
    losses = model(**data)
  File ""/home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/zitianchen/anaconda2/envs/open-mmlab/lib/python3.7/site-packages/torch/nn/parallel/distributed.py"", line 392, in forward
    self.reducer.prepare_for_backward([])
```

**Bug fix**
This error happened after 4 epochs and 1500 iterations. I think it is because there are unused parameters in loss calculation. Any solution to figure it out? init_dist() does not have keyword argument find_unused_parameters=True. My code can correctly run using only one GPU(./tools/train.py). I try to print out some variables, it seems that there is nothing strange with the input.

I wonder if there is a way to print out unused_parameters.

BTW, I have written another detection model that facing this issue in the first 100 iterations.  

Some related reference:
https://github.com/ultralytics/yolov3/issues/331

https://github.com/pytorch/pytorch/issues/22049
",neutral,negative
323,How can I visualize output from different layers and visualize them to catch issues and odd patterns during training?,matterport/Mask_RCNN,open,,AI-ML-Enthusiast,,2019-12-26 10:13:10,,0.0,,1933.0,542509180.0,"Thanks for wonderful repository. I inspect model using your provided notebook, such as inspect_model.ipynb.
Now I would like to visualize output from different layers during training time?
Is there any suggestion please",neutral,positive
324,Memory Leak when Training US-Net,JiahuiYu/slimmable_networks,closed,,VectorYoung,JiahuiYu,2019-07-15 18:26:35,2019-08-05 18:08:36,1.0,,20.0,468270916.0,"Hi @JiahuiYu ,
I am trying to train US-MobileNet, but scale up to [1.0, 2.0]. However, I get the 'CUDA out of memory' error. During training, the memory varies between 1000MB to 11000MB, but after some iterations, it suddenly got 'CUDA out of memory'. I got the same issue when training US-ResNet. But it is fine with US-MobileNet_[0.25, 1]. 

One thing weird is that when I fix the 4 width(e.g. [1.0, 1.5, 1.7, 2.0], whatever, just like Slimmable Network), I won't have the memory issue. And the memory is fixed about 4200MB. 

I am guessing some tensors or graphs are not freed. But I don't know how to debug it. Do you have the same issue?",neutral,neutral
325," AttributeError: 'module' object has no attribute 'v1' , keep_dims is deprecated, use keepdims instead",matterport/Mask_RCNN,open,,subbulakshmisubha,,2019-11-12 13:42:24,,0.0,,1857.0,521558625.0,"![1](https://user-images.githubusercontent.com/15358611/68676534-8a144080-055a-11ea-8ba7-b81e8f29042b.PNG)

I am trying to train mask-rcnn with the coco data set. my TensorFlow version is 1.3 and TensorFlow- GPU version is 1.9.0. I would like to know how to fix this issue.  I have changed the keep_dims variable to keepdims already in the model file, in spite of that I get this.",neutral,negative
326,Will open mmengine source code ？,open-mmlab/mmdetection,closed,hhaAndroid,FDInSky,hhaAndroid,2022-08-23 05:59:59,2022-10-19 03:10:16,0.0,,8618.0,1347381428.0,,neutral,neutral
327,loss bbx is nan when train from scratch,open-mmlab/mmdetection,closed,ZwwWayne,Hlings,ZwwWayne,2021-07-10 13:09:48,2022-03-20 16:03:48,3.0,,5581.0,941247452.0,I want to train faster rcnn in VOC dataset which just has one person class. Model configs and datasets configs have been changerd correct. But just loss_bbx will be nan while training from scratch. I can't deal with this problem by chaning learning rate or batchsize. Thanks.,negative,positive
328,question about joint bpe vocab size,rsennrich/subword-nmt,closed,,zrlhk,rsennrich,2019-09-16 01:25:19,2019-09-22 08:56:51,1.0,,79.0,493812475.0,"Hi, If I use joint bpe; for example: 
Training with joint bpe model and set the bpe vocab size is 64000;and in network vocabulary:
1.for source vocab 35000 + target vocab 35000 (slightly larger than 32000+32000)
2.for source vocab 70000 + target vocab 70000 (slightly larger than 64000+64000)

so which is right? how to set the size of bpe size and network vocabulary size?
",neutral,neutral
329,Google Landmark Federated split is missing files,google-research/google-research,open,,marcociccone,,2022-01-17 12:46:04,,2.0,,946.0,1105812743.0,"Hi!
I've started working with the gldv2 dataset for FL using the split provided [here](https://github.com/google-research/google-research/tree/master/federated_vision_datasets).
In particular, I'm using the [dataset loader](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/gldv2/load_data) from tensorflow federated.

I've noticed that roughly a 10% of images are missing: [this](https://github.com/tensorflow/federated/blob/da2ca219e75e3317c43e10a83d07b635aad9cfd2/tensorflow_federated/python/simulation/datasets/gldv2.py#L108) exception is raised when creating the clients tfrecords.
It looks that some filenames in the federated splits are not matching filenames from the original dataset. 

I've collected the missing files in a [json file](https://drive.google.com/file/d/1HbMKi8LrWCwldhl7H4DuZmEJcLBSavgq/view?usp=sharing) that you can check (list of dict - one element per client - with fields `user_id`, `total`, `found`, `missing`, `missing_files`)

@hang-qi I'm keeping bugging you sorry :)",neutral,negative
330,tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle ,matterport/Mask_RCNN,open,,TangLaoDA,,2019-08-16 02:18:31,,0.0,,1690.0,481412222.0,"when I use the image size 1024X800 to train, batch_size = 1, after about 3000 steps,program stopped unexpectedly and raise information:""tensorflow/core/common_runtime/bfc_allocator.cc:380] Check failed: h != kInvalidChunkHandle "",but when i observe memory usage on GTX1070TI,it occupies about 5400M of video memory,there is about 2000M free memory.I try to change the image size to 128X64,it occupies more memory,but the program does not stop unexpectedly.This phenomenon is very strange。",neutral,neutral
331,[Question] Error when converting yolact to ONNX,open-mmlab/mmdetection,open,Czm369,gggaugau,,2022-05-16 09:24:14,,2.0,,7995.0,1236870481.0,"Hello. May I ask if converting yolact to ONNX/TRT is currently available? I tried to convert using tools/deployment/pytorch2onnx.py but it seems to be errors. 
Please help me, thanks a lot in advance.
```
python tools/deployment/pytorch2onnx.py configs/yolact/yolact_r50_1x8_coco.py /home/gggggg/projects/yolact_weights/r50_1x8_coco/2104.pth --output-file test.onnx --input-img /home/gggggg/projects/test.jpg
mmdetection/tools/deployment/pytorch2onnx.py:299: UserWarning: Arguments like `--mean`, `--std`, `--dataset` would be         parsed directly from config file and are deprecated and         will be removed in future releases.
  warnings.warn('Arguments like `--mean`, `--std`, `--dataset` would be \
/home/gggggg/miniconda3/lib/python3.9/site-packages/mmcv/onnx/symbolic.py:481: UserWarning: DeprecationWarning: This function will be deprecated in future. Welcome to use the unified model deployment toolbox MMDeploy: https://github.com/open-mmlab/mmdeploy
  warnings.warn(msg)
load checkpoint from local path: /home/gggggg/projects/yolact_weights/r50_1x8_coco/2104.pth
[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.
/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py:415: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  nms_pre_tensor = torch.tensor(
/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py:436: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert cls_score.size()[-2:] == bbox_pred.size()[-2:]
/home/gggggg/mmdetection/./mmdet/core/export/onnx_helper.py:63: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if k <= 0 or size <= 0:
/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py:457: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if nms_pre > 0:
Traceback (most recent call last):
  File ""/home/gggggg/mmdetection/tools/deployment/pytorch2onnx.py"", line 335, in <module>
    pytorch2onnx(
  File ""/home/gggggg/mmdetection/tools/deployment/pytorch2onnx.py"", line 91, in pytorch2onnx
    torch.onnx.export(
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/__init__.py"", line 305, in export
    return utils.export(model, args, f, export_params, verbose, training,
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 118, in export
    _export(model, args, f, export_params, verbose, training, input_names, output_names,
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 719, in _export
    _model_to_graph(model, args, verbose, input_names,
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 499, in _model_to_graph
    graph, params, torch_out, module = _create_jit_graph(model, args)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 440, in _create_jit_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 391, in _trace_and_get_graph_from_model
    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py"", line 1166, in _get_trace_graph
    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py"", line 127, in forward
    graph, out = torch._C._create_graph_by_tracing(
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py"", line 118, in wrapper
    outs.append(self.inner(*trace_inputs))
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1098, in _slow_forward
    result = self.forward(*input, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 109, in new_func
    return old_func(*args, **kwargs)
  File ""/home/gggggg/mmdetection/./mmdet/models/detectors/base.py"", line 169, in forward
    return self.onnx_export(img[0], img_metas[0])
  File ""/home/gggggg/mmdetection/./mmdet/models/detectors/single_stage.py"", line 168, in onnx_export
    det_bboxes, det_labels = self.bbox_head.onnx_export(
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 197, in new_func
    return old_func(*args, **kwargs)
  File ""/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py"", line 460, in onnx_export
    nms_pre_score = (nms_pre_score * score_factors[..., None])
RuntimeError: The size of tensor a (45600) must match the size of tensor b (1459200) at non-singleton dimension 1
```",neutral,positive
332,AttributeError: 'NoneType' object has no attribute 'squeeze',open-mmlab/mmdetection,closed,v-qjqs,mhyeonsoo,v-qjqs,2020-09-15 07:41:14,2020-10-13 05:54:10,3.0,,3767.0,701693265.0,"
**Bug Description**
I am trying to implement DetectoRS with own custom dataset, and I am getting an error which is 
_AttributeError: 'NoneType' object has no attribute 'squeeze'_

**Reproduction**
1. What command or script did you run?
```
python tools/train_gs.py configs/detectors/detectors_htc_r50_1x_gs.py
```

The config I am using is below.
```
dataset_type = 'myDataset'
data_root = '/shared/data/mydata/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[1.0, 1.0, 1.0], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(760, 510), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[1.0, 1.0, 1.0],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(760, 510),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[1.0, 1.0, 1.0],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='myDataset',
        ann_file='/shared/data/mydata/train/train_1cls.json',
        img_prefix='/shared/data/mydata/train/result_image/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(760, 510), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[1.0, 1.0, 1.0],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='myDataset',
        ann_file='/shared/data/mydata/test/train_1cls.json',
        img_prefix='/shared/data/mydata/test/result_image/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(760, 510),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[1.0, 1.0, 1.0],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='myDataset',
        ann_file='/shared/data/mydata/test/train_1cls.json',
        img_prefix='/shared/data/mydata/test/result_image/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(760, 510),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[1.0, 1.0, 1.0],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
total_epochs = 12
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
model = dict(
    type='HybridTaskCascade',
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='DetectoRS_ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        conv_cfg=dict(type='ConvAWS'),
        sac=dict(type='SAC', use_deform=True),
        stage_with_sac=(False, True, True, True),
        output_img=True),
    neck=dict(
        type='RFP',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5,
        rfp_steps=2,
        aspp_out_channels=64,
        aspp_dilations=(1, 3, 6, 1),
        rfp_backbone=dict(
            rfp_inplanes=256,
            type='DetectoRS_ResNet',
            depth=50,
            num_stages=4,
            out_indices=(0, 1, 2, 3),
            frozen_stages=1,
            norm_cfg=dict(type='BN', requires_grad=True),
            norm_eval=True,
            conv_cfg=dict(type='ConvAWS'),
            sac=dict(type='SAC', use_deform=True),
            stage_with_sac=(False, True, True, True),
            pretrained='torchvision://resnet50',
            style='pytorch')),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='HybridTaskCascadeRoIHead',
        interleaved=True,
        mask_info_flow=True,
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ],
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=[
            dict(
                type='HTCMaskHead',
                with_conv_res=False,
                num_convs=4,
                in_channels=256,
                conv_out_channels=256,
                num_classes=80,
                loss_mask=dict(
                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
            dict(
                type='HTCMaskHead',
                num_convs=4,
                in_channels=256,
                conv_out_channels=256,
                num_classes=80,
                loss_mask=dict(
                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),
            dict(
                type='HTCMaskHead',
                num_convs=4,
                in_channels=256,
                conv_out_channels=256,
                num_classes=80,
                loss_mask=dict(
                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))
        ],
        semantic_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[8]),
        semantic_head=dict(
            type='FusedSemanticHead',
            num_ins=5,
            fusion_level=1,
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=183,
            ignore_label=255,
            loss_weight=0.2)))
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.7,
                min_pos_iou=0.7,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ])
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.5),
        max_per_img=100))
work_dir = './work_dirs/detectors_htc_r50_1x_gs'
gpu_ids = range(0, 1)

```
3. What dataset did you use?

I am using my own dataset which has coco format

**Environment**
```
Python: 3.7.7 (default, Mar 23 2020, 22:36:06) [GCC 7.3.0]
CUDA available: True
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-32GB
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF,

TorchVision: 0.6.0a0+82fd1c8
OpenCV: 4.4.0
MMCV: 1.1.2
MMDetection: 2.4.0+11b1ef8
MMDetection Compiler: GCC 7.3
MMDetection CUDA Compiler: 10.1
```

**Error traceback**
If applicable, paste the error trackback here.
```
Traceback (most recent call last):
  File ""tools/train_gs.py"", line 182, in <module>
    main()
  File ""tools/train_gs.py"", line 178, in main
    meta=meta)
  File ""/shared/hsmoon/mmdetection_official/mmdet/apis/train.py"", line 143, in train_detector
    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 122, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 32, in train
    **kwargs)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py"", line 67, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/shared/hsmoon/mmdetection_official/mmdet/models/detectors/base.py"", line 234, in train_step
    losses = self(**data)
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/shared/hsmoon/mmdetection_official/mmdet/core/fp16/decorators.py"", line 51, in new_func
    return old_func(*args, **kwargs)
  File ""/shared/hsmoon/mmdetection_official/mmdet/models/detectors/base.py"", line 168, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/shared/hsmoon/mmdetection_official/mmdet/models/detectors/two_stage.py"", line 164, in forward_train
    **kwargs)
  File ""/shared/hsmoon/mmdetection_official/mmdet/models/roi_heads/htc_roi_head.py"", line 256, in forward_train
    loss_seg = self.semantic_head.loss(semantic_pred, gt_semantic_seg)
  File ""/shared/hsmoon/mmdetection_official/mmdet/core/fp16/decorators.py"", line 131, in new_func
    return old_func(*args, **kwargs)
  File ""/shared/hsmoon/mmdetection_official/mmdet/models/roi_heads/mask_heads/fused_semantic_head.py"", line 105, in loss
    labels = labels.squeeze(1).long()
AttributeError: 'NoneType' object has no attribute 'squeeze'
```

When I debugged, I could see that pred_mask in fused_semantic_head.py has values which are not 0, but the labels was None.
Could anyone suggest me the solution that I can try with?

Thank you!",neutral,negative
333,Add validation that classifies of the generated sentence,vineetjohn/linguistic-style-transfer,closed,vineetjohn,vineetjohn,vineetjohn,2018-03-07 22:15:44,2018-03-10 15:14:58,1.0,enhancement,16.0,303280280.0,,neutral,neutral
334,Why use relu here?,JiahuiYu/generative_inpainting,closed,,cmyyy,JiahuiYu,2019-04-20 13:36:18,2019-04-20 14:11:22,1.0,,250.0,435400052.0,https://github.com/JiahuiYu/generative_inpainting/blob/06cd62cfca8c10c349b451fa33d9cbb786bfaa20/inpaint_model.py#L96,neutral,neutral
335,loss goes up ,JiahuiYu/generative_inpainting,closed,,123liluky,JiahuiYu,2018-09-17 08:06:32,2018-09-17 12:50:58,1.0,,133.0,360760248.0,"I trained my 1883 images about 380 epochs with parameters like below:
#parameters
DATASET: 'celebahq'
RANDOM_CROP: False
VAL: False
LOG_DIR: 'logs'
MODEL_RESTORE: 'a'  #

GAN: 'wgan_gp'  # 'dcgan', 'lsgan', 'wgan_gp', 'one_wgan_gp'
PRETRAIN_COARSE_NETWORK: False # unused
GAN_LOSS_ALPHA: 0.001  # dcgan: 0.0008, wgan: 0.0005, onegan: 0.001
WGAN_GP_LAMBDA: 10
COARSE_L1_ALPHA: 0.5
L1_LOSS_ALPHA: 0.5
AE_LOSS_ALPHA: 0.5
GAN_WITH_MASK: False
DISCOUNTED_MASK: True
RANDOM_SEED: False
PADDING: 'SAME'

#training
NUM_GPUS: 1
GPU_ID: 3  # -1 indicate select any available one, otherwise select gpu ID, e.g. [0,1,3]
TRAIN_SPE: 940
MAX_ITERS: 400000
VIZ_MAX_OUT: 10
GRADS_SUMMARY: False
GRADIENT_CLIP: False
GRADIENT_CLIP_VALUE: 0.1
VAL_PSTEPS: 50

#data
DATA_FLIST:
  celebahq: [
    'training_data/train_shuffled.flist',
    ''  # VAL=False 
  ]

STATIC_VIEW_SIZE: 30
IMG_SHAPES: [224, 224, 3]
HEIGHT: 224     
WIDTH: 224   
MAX_DELTA_HEIGHT: 0
MAX_DELTA_WIDTH: 0
BATCH_SIZE: 2
VERTICAL_MARGIN: 0
HORIZONTAL_MARGIN: 0

#loss
AE_LOSS: True
L1_LOSS: True
GLOBAL_DCGAN_LOSS_ALPHA: 1.
GLOBAL_WGAN_LOSS_ALPHA: 1.

#loss legacy
LOAD_VGG_MODEL: False
VGG_MODEL_FILE: data/model_zoo/vgg16.npz
FEATURE_LOSS: False
GRAMS_LOSS: False
TV_LOSS: False
TV_LOSS_ALPHA: 0.
FEATURE_LOSS_ALPHA: 0.01
GRAMS_LOSS_ALPHA: 50 # unused
SPATIAL_DISCOUNTING_GAMMA: 0.9
I found loss kept going up.
![image](https://user-images.githubusercontent.com/20832778/45611513-37dcaa00-ba92-11e8-9acb-7e166892a7ce.png)
And imges with bigger masks were inpainted unsatisfing.
![image](https://user-images.githubusercontent.com/20832778/45611807-50998f80-ba93-11e8-8802-ec3861b0bfac.png)
![image](https://user-images.githubusercontent.com/20832778/45611865-99e9df00-ba93-11e8-854d-35566e56ff46.png)
Could you please give me some help?",negative,neutral
336,Implementing problems in Rotated YOLOX,open-mmlab/mmdetection,open,Czm369,liuyanyi,,2022-07-25 14:03:17,,1.0,reimplementation,8427.0,1316889093.0,"I am implementing Rotated YOLOX for MMRotate in https://github.com/open-mmlab/mmrotate/pull/409, SimOTA Assigner has CUDA Error while training.

Compared with mmdet, only `get_in_gt_and_in_center_info` and `bbox_overlaps` is different to support rotated detection. After set `CUDA_LAUNCH_BLOCKING=1`, the error log shows that error may cause by binary_cross_entropy. It's werid because there is no error when training with fp16. Is there any suggestion to debug that?

Error info:
```
./aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [58,0,0], thread: [63,0,0] Assertion `input_val >= zero && input_val <= one` failed.
Traceback (most recent call last):
  File ""/miniconda3/lib/python3.9/site-packages/mmdet/core/bbox/assigners/sim_ota_assigner.py"", line 67, in assign
    assign_result = self._assign(pred_scores, priors, decoded_bboxes,
  File ""/workspace/mmrotate/mmrotate/core/bbox/assigners/r_sim_ota_assinger.py"", line 85, in _assign
    F.binary_cross_entropy(
  File ""/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py"", line 3065, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: CUDA error: device-side assert triggered
```",neutral,negative
337,"What's the best accuracy of  mobilefacenet  on lfw, cfp_fp, agedb_30?",deepinsight/insightface,closed,,Wisgon,nttstar,2018-05-11 00:12:21,2018-05-17 17:06:46,3.0,,206.0,322127332.0,"I have only two GPU of 1070ti, so my command is `CUDA_VISIBLE_DEVICES='0,1' python -u train_softmax.py --network y1 --loss-type 4 --margin-s 128 --margin-m 0.5 --per-batch-size 182 --emb-size 128 --ckpt 2 --data-dir ../datasets/faces_ms1m_112x112 --wd 0.00004 --fc7-wd-mult 10.0 --prefix ../mobilenet/model-mobilefacenet-182`
My final accuracy is lfw: 0.98967,     agedb_30: 0.911,     cfp_fp: 0.87471.  And the speed of training is about 900sample/sec.
Is the accuracy normal or too low?",neutral,neutral
338,question about flip test,Microsoft/human-pose-estimation.pytorch,open,,Liz66666,,2018-12-19 10:10:09,,6.0,,69.0,392527185.0,"Hi, thanks for your code!
But I have a question about flip test. I don't know why here you do a shift on output_flipped.
The comments says that ""feature is not aligned, shift flipped heatmap for higher accuracy"", what's the meaning of this opreration? 
https://github.com/Microsoft/human-pose-estimation.pytorch/blob/8ed745798439f247c85c57392428320d4c553654/lib/core/function.py#L121-L125",neutral,positive
339,Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.,matterport/Mask_RCNN,open,,LyndonZV,,2020-03-20 22:25:07,,0.0,,2058.0,585357538.0,"When I tried to run the demo.ipynb, turns out this error.

#Process
ipython 
%run demo.ipython
AttributeError: '_NamespacePath' object has no attribute 'sort'

then I tried to update pip3

pip3 install --upgrade pip
pip3 install --upgrade setuptools
ipython 
%run demo.ipython
Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.",negative,negative
340,U^2-Net (U square net) in mmdetection,open-mmlab/mmdetection,closed,,muhammadabdullah34907,hellock,2020-06-26 08:13:15,2020-06-28 06:59:53,2.0,,3131.0,646091732.0,"Hi @ZwwWayne 
Any plan to add this method in future. I believe it's best feature for smooth masking of objects. 

https://github.com/NathanUA/U-2-Net
U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection",neutral,positive
341,Unable to download the dataset from Baidu,deepinsight/insightface,closed,,ghost,,2018-01-25 03:23:39,2018-01-25 12:17:38,4.0,,12.0,291438085.0,"Hi,
Is it possible to share the dataset somewhere else?",neutral,neutral
342,关于non-local 和 RGA连接权重的区别,microsoft/Relation-Aware-Global-Attention-Networks,open,,bigmoking,,2020-07-15 01:56:06,,1.0,,5.0,657003140.0,"hi 您好：
最近在精读您的文章，但是一直有一个疑问，暂时没有想通，RGA如何学得基于位置变化的一个连接权重？看了下non-local,主要区别是一个是直接加，在这里进行了一个stack操作，还望大佬解惑！",neutral,neutral
343,where to download the Paris StreetView Dataset,pathak22/context-encoder,closed,,micklexqg,pathak22,2017-12-19 12:10:38,2018-02-17 07:31:59,184.0,,24.0,283206489.0,"Hi, I want to train it by myself,but i don't know how to download the datasets.
can you give me a link? any related url are appreciated. ",neutral,positive
344,How to evaluate with COCO style using CustomDataset?,open-mmlab/mmdetection,closed,,xuw080,hellock,2018-12-06 18:32:53,2019-10-26 14:00:55,13.0,,145.0,388342098.0,"I prepared my custom dataset using CustomDataset object, however, when I was testing my model on custom datasets, this error will appear:

Traceback (most recent call last):
  File ""tools/test.py"", line 124, in <module>
    main()
  File ""tools/test.py"", line 112, in main
    results2json(dataset, outputs, result_file)
  File ""/home/Xwang/anaconda3/envs/mmdetection/lib/python3.7/site-packages/mmdet-0.5.4+65a2e5e-py3.7.egg/mmdet/core/evaluation/coco_utils.py"", line 142, in results2json
    json_results = det2json(dataset, results)
  File ""/home/Xwang/anaconda3/envs/mmdetection/lib/python3.7/site-packages/mmdet-0.5.4+65a2e5e-py3.7.egg/mmdet/core/evaluation/coco_utils.py"", line 106, in det2json
    img_id = dataset.img_ids[idx]
AttributeError: 'CustomDataset' object has no attribute 'img_ids'

If we can not use coco_utils for testing, due to CustomDataset doesn't have img_ids. How do we test CustomDatasets with coco style evaluation methods?

Thanks",neutral,positive
345,subclass or specific class training COCO Dataset,open-mmlab/mmdetection,closed,ZwwWayne,muhammadabdullah34907,muhammadabdullah34907,2020-06-21 14:32:15,2020-06-25 07:21:29,13.0,,3094.0,642574506.0,"Hi Everyone.

I want to train only 1 class i.e person out of all classes in COCO dataset. I have made changes to config files like below:
Is this correct that it will load only person class ? also, i am using load_from parameter to further fine tune. does this parameter cause any trouble ?

```
model = dict(
    type='MaskRCNN',
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', out_size=7, sample_num=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=1,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', out_size=14, sample_num=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=1,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=-1,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.5,
            min_pos_iou=0.5,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=512,
            pos_fraction=0.25,
            neg_pos_ub=-1,
            add_gt_as_proposals=True),
        mask_size=28,
        pos_weight=-1,
        debug=False))
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='nms', iou_thr=0.5),
        max_per_img=100,
        mask_thr_binary=0.7))
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]   
classes = ('person')
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        classes=classes,
        ann_file='data/coco/annotations/instances_train2017.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='CocoDataset',
        classes=classes,
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        classes=classes,
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
total_epochs = 12
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'home/user/mmdetection/checkpoints/mask_rcnn_r50_fpn_1x_coco_20200205-d4b0c5d6.pth'
resume_from = None
workflow = [('train', 1)]
work_dir = './work_dirs/mask_rcnn_r50_fpn_1x_coco'
gpu_ids = range(0, 1)

```",neutral,positive
346,RuntimeError: NCCL error,HikariTJU/LD,closed,,eeric,HikariTJU,2021-04-26 03:35:31,2021-04-27 08:42:39,12.0,,2.0,867222857.0,"./tools/dist_train.sh configs/ld/ld_gflv1_r101_r50_fpn_coco_1x.py 8

RuntimeError: NCCL error  in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, invalid usage, NCCL version 2.7.8",neutral,negative
347,CUDNN is not installed in CUDA directory by default,DmitryUlyanov/texture_nets,open,,shubhvachher,,2018-05-02 21:23:29,,0.0,,91.0,319717292.0,"I'm new to torch and have a fresh install just to test out fast style transfer.

Installing cudnn from deb packages installs it in `/usr/lib/x86_64-linux-gnu/` as referenced [here](https://github.com/NVIDIA/nvidia-docker/issues/172) and in this comment [here](https://github.com/NVIDIA/nvidia-docker/issues/168#issuecomment-238924455).

I think support for that in this code is missing. I would add it myself but am quite new to torch and so adding an issue.
```
$ th train.lua -data ./datasets/ -style_image ./data/textures/862.png -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2
/home/user/torch/install/bin/luajit: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: /home/user/torch/install/share/lua/5.1/cudnn/ffi.lua:1603: 'libcudnn (R5) not found in library path.
Please install CuDNN from https://developer.nvidia.com/cuDNN
Then make sure files named as libcudnn.so.5 or libcudnn.5.dylib are placed in
your library load path (for example /usr/local/lib , or manually add a path to LD_LIBRARY_PATH)

Alternatively, set the path to libcudnn.so.5 or libcudnn.5.dylib
to the environment variable CUDNN_PATH and rerun torch.
For example: export CUDNN_PATH=""/usr/local/cuda/lib64/libcudnn.so.5""

stack traceback:
	[C]: in function 'error'
	...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	train.lua:5: in main chunk
	[C]: in function 'dofile'
	...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50

```
Solution being : 
`export CUDNN_PATH=""/usr/lib/x86_64-linux-gnu/libcudnn.so.5""`",neutral,positive
348,Python 3 support,wujian16/Cornell-MOE,closed,,gokceneraslan,wujian16,2017-11-07 15:24:01,2019-04-30 01:24:17,3.0,enhancement#help wanted,27.0,271870750.0,It would be nice to support Python 3 by default. Can you shortly write down the things required for a Python 3 port? Is it trivial via `futurize` or `six` packages?,positive,positive
349,About hidden layer dimension change,timmeinhardt/trackformer,open,,davidyang180,,2022-09-02 04:40:04,,1.0,,58.0,1359695337.0,"Hi! I observed by debugging the code, why when using the multi-frame training strategy, the hidden layer dimension should be changed to 288, which does not seem to be mentioned in the paper.",neutral,neutral
350,"mAP, dets, etc. and all other metrics are 0",open-mmlab/mmdetection,closed,BIGWangYuDong,sarmientoj24,hhaAndroid,2022-03-04 14:37:26,2022-04-08 09:25:47,11.0,awaiting response,7321.0,1159715472.0,"My dataset is a custom CocoDataset. But I also tried using a CustomDataset as a wrapper for the CocoDataset. I am having problems as to my mAP and dets are all 0. Any idea why?

For context, my dataset is dental xrays

## CONFIG
```
Config:
model = dict(
    type='FasterRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=2,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.2),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.1),
            max_per_img=100)))
dataset_type = 'BIPADataset'
data_root = 'dataset/bipa'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Pad', size_divisor=32),
    dict(
        type='Albu',
        transforms=[
            dict(
                type='ShiftScaleRotate',
                shift_limit=0.0625,
                scale_limit=0.2,
                rotate_limit=45,
                interpolation=1,
                p=0.25),
            dict(
                type='RandomBrightnessContrast',
                brightness_limit=[-0.15, 0.15],
                contrast_limit=[-0.15, 0.15],
                p=0.7),
            dict(
                type='ImageCompression',
                quality_lower=85,
                quality_upper=95,
                p=0.2),
            dict(
                type='OneOf',
                transforms=[
                    dict(type='Blur', blur_limit=3, p=1.0),
                    dict(type='MedianBlur', blur_limit=3, p=1.0)
                ],
                p=0.25),
            dict(
                type='OneOf',
                transforms=[
                    dict(type='Sharpen', p=1.0),
                    dict(type='Emboss', p=1.0)
                ],
                p=0.25),
            dict(
                type='GaussNoise',
                var_limit=[20.0, 80.0],
                per_channel=False,
                p=0.3),
            dict(type='RandomRotate90', p=0.3),
            dict(type='Transpose', p=0.2)
        ],
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=['gt_labels'],
            min_visibility=0.1,
            filter_lost_elements=True),
        keymap=dict(img='image', gt_bboxes='bboxes'),
        update_pad_shape=False,
        skip_img_without_anno=True),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=4,
    train=dict(
        type='BIPADataset',
        ann_file='annotations/train.json',
        img_prefix='train2017',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='Pad', size_divisor=32),
            dict(
                type='Albu',
                transforms=[
                    dict(
                        type='ShiftScaleRotate',
                        shift_limit=0.0625,
                        scale_limit=0.2,
                        rotate_limit=45,
                        interpolation=1,
                        p=0.25),
                    dict(
                        type='RandomBrightnessContrast',
                        brightness_limit=[-0.15, 0.15],
                        contrast_limit=[-0.15, 0.15],
                        p=0.7),
                    dict(
                        type='ImageCompression',
                        quality_lower=85,
                        quality_upper=95,
                        p=0.2),
                    dict(
                        type='OneOf',
                        transforms=[
                            dict(type='Blur', blur_limit=3, p=1.0),
                            dict(type='MedianBlur', blur_limit=3, p=1.0)
                        ],
                        p=0.25),
                    dict(
                        type='OneOf',
                        transforms=[
                            dict(type='Sharpen', p=1.0),
                            dict(type='Emboss', p=1.0)
                        ],
                        p=0.25),
                    dict(
                        type='GaussNoise',
                        var_limit=[20.0, 80.0],
                        per_channel=False,
                        p=0.3),
                    dict(type='RandomRotate90', p=0.3),
                    dict(type='Transpose', p=0.2)
                ],
                bbox_params=dict(
                    type='BboxParams',
                    format='pascal_voc',
                    label_fields=['gt_labels'],
                    min_visibility=0.1,
                    filter_lost_elements=True),
                keymap=dict(img='image', gt_bboxes='bboxes'),
                update_pad_shape=False,
                skip_img_without_anno=True),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        data_root='dataset/bipa',
        classes=('cavity', 'pa')),
    val=dict(
        type='BIPADataset',
        ann_file='annotations/val.json',
        img_prefix='val2017',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='dataset/bipa',
        classes=('cavity', 'pa')),
    test=dict(
        type='BIPADataset',
        ann_file='annotations/val.json',
        img_prefix='val2017',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='dataset/bipa',
        classes=('cavity', 'pa')))
evaluation = dict(interval=1, metric='mAP')
checkpoint_config = dict(interval=20)
log_config = dict(interval=10, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup=None,
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
albu_train_transforms = [
    dict(
        type='ShiftScaleRotate',
        shift_limit=0.0625,
        scale_limit=0.2,
        rotate_limit=45,
        interpolation=1,
        p=0.25),
    dict(
        type='RandomBrightnessContrast',
        brightness_limit=[-0.15, 0.15],
        contrast_limit=[-0.15, 0.15],
        p=0.7),
    dict(type='ImageCompression', quality_lower=85, quality_upper=95, p=0.2),
    dict(
        type='OneOf',
        transforms=[
            dict(type='Blur', blur_limit=3, p=1.0),
            dict(type='MedianBlur', blur_limit=3, p=1.0)
        ],
        p=0.25),
    dict(
        type='OneOf',
        transforms=[dict(type='Sharpen', p=1.0),
                    dict(type='Emboss', p=1.0)],
        p=0.25),
    dict(type='GaussNoise', var_limit=[20.0, 80.0], per_channel=False, p=0.3),
    dict(type='RandomRotate90', p=0.3),
    dict(type='Transpose', p=0.2)
]
classes = ('cavity', 'pa')
work_dir = './logs'
seed = 0
gpu_ids = range(0, 1)
```

## Some results
```
2022-03-04 21:18:47,752 - mmdet - INFO - Epoch [1][10/88]	lr: 2.500e-03, eta: 0:11:33, time: 0.663, data_time: 0.243, memory: 4150, loss_rpn_cls: 0.6046, loss_rpn_bbox: 0.0207, loss_cls: 0.6186, acc: 86.9725, loss_bbox: 0.0029, loss: 1.2468
2022-03-04 21:18:51,920 - mmdet - INFO - Epoch [1][20/88]	lr: 2.500e-03, eta: 0:09:19, time: 0.417, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.2628, loss_rpn_bbox: 0.0273, loss_cls: 0.1441, acc: 99.3993, loss_bbox: 0.0070, loss: 0.4412
2022-03-04 21:18:55,970 - mmdet - INFO - Epoch [1][30/88]	lr: 2.500e-03, eta: 0:08:28, time: 0.406, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1565, loss_rpn_bbox: 0.0293, loss_cls: 0.1773, acc: 99.1720, loss_bbox: 0.0096, loss: 0.3728
2022-03-04 21:19:00,116 - mmdet - INFO - Epoch [1][40/88]	lr: 2.500e-03, eta: 0:08:02, time: 0.413, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1515, loss_rpn_bbox: 0.0207, loss_cls: 0.1260, acc: 99.1560, loss_bbox: 0.0066, loss: 0.3048
2022-03-04 21:19:04,368 - mmdet - INFO - Epoch [1][50/88]	lr: 2.500e-03, eta: 0:07:47, time: 0.426, data_time: 0.017, memory: 4150, loss_rpn_cls: 0.1256, loss_rpn_bbox: 0.0155, loss_cls: 0.0605, acc: 99.3428, loss_bbox: 0.0047, loss: 0.2062
2022-03-04 21:19:08,423 - mmdet - INFO - Epoch [1][60/88]	lr: 2.500e-03, eta: 0:07:33, time: 0.406, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1229, loss_rpn_bbox: 0.0181, loss_cls: 0.0697, acc: 99.1772, loss_bbox: 0.0063, loss: 0.2170
2022-03-04 21:19:12,333 - mmdet - INFO - Epoch [1][70/88]	lr: 2.500e-03, eta: 0:07:19, time: 0.391, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1123, loss_rpn_bbox: 0.0144, loss_cls: 0.0641, acc: 99.2437, loss_bbox: 0.0044, loss: 0.1952
2022-03-04 21:19:16,410 - mmdet - INFO - Epoch [1][80/88]	lr: 2.500e-03, eta: 0:07:10, time: 0.407, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1253, loss_rpn_bbox: 0.0155, loss_cls: 0.0638, acc: 99.2192, loss_bbox: 0.0047, loss: 0.2093
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 351/351, 18.7 task/s, elapsed: 19s, ETA:     0s
---------------iou_thr: 0.5---------------
2022-03-04 21:19:38,893 - mmdet - INFO - 
+--------+-----+------+--------+-------+
| class  | gts | dets | recall | ap    |
+--------+-----+------+--------+-------+
| cavity | 490 | 694  | 0.000  | 0.000 |
| pa     | 172 | 713  | 0.000  | 0.000 |
+--------+-----+------+--------+-------+
| mAP    |     |      |        | 0.000 |
+--------+-----+------+--------+-------+
2022-03-04 21:19:38,996 - mmdet - INFO - Epoch(val) [1][351]	AP50: 0.0000, mAP: 0.0000
2022-03-04 21:19:45,616 - mmdet - INFO - Epoch [2][10/88]	lr: 2.500e-03, eta: 0:06:48, time: 0.652, data_time: 0.241, memory: 4150, loss_rpn_cls: 0.1151, loss_rpn_bbox: 0.0129, loss_cls: 0.0537, acc: 99.3276, loss_bbox: 0.0039, loss: 0.1856
2022-03-04 21:19:49,711 - mmdet - INFO - Epoch [2][20/88]	lr: 2.500e-03, eta: 0:06:43, time: 0.410, data_time: 0.016, memory: 4150, loss_rpn_cls: 0.1234, loss_rpn_bbox: 0.0192, loss_cls: 0.0569, acc: 99.2179, loss_bbox: 0.0049, loss: 0.2043
2022-03-04 21:19:53,953 - mmdet - INFO - Epoch [2][30/88]	lr: 2.500e-03, eta: 0:06:38, time: 0.424, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1300, loss_rpn_bbox: 0.0191, loss_cls: 0.0476, acc: 99.3787, loss_bbox: 0.0040, loss: 0.2007
2022-03-04 21:19:58,358 - mmdet - INFO - Epoch [2][40/88]	lr: 2.500e-03, eta: 0:06:35, time: 0.439, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1152, loss_rpn_bbox: 0.0147, loss_cls: 0.0500, acc: 99.3385, loss_bbox: 0.0042, loss: 0.1841
2022-03-04 21:20:03,647 - mmdet - INFO - Epoch [2][50/88]	lr: 2.500e-03, eta: 0:06:38, time: 0.529, data_time: 0.016, memory: 4150, loss_rpn_cls: 0.1326, loss_rpn_bbox: 0.0196, loss_cls: 0.0530, acc: 99.2578, loss_bbox: 0.0042, loss: 0.2094
2022-03-04 21:20:08,866 - mmdet - INFO - Epoch [2][60/88]	lr: 2.500e-03, eta: 0:06:39, time: 0.523, data_time: 0.016, memory: 4150, loss_rpn_cls: 0.1151, loss_rpn_bbox: 0.0163, loss_cls: 0.0474, acc: 99.3412, loss_bbox: 0.0041, loss: 0.1829
2022-03-04 21:20:16,842 - mmdet - INFO - Epoch [2][70/88]	lr: 2.500e-03, eta: 0:06:54, time: 0.788, data_time: 0.015, memory: 4150, loss_rpn_cls: 0.1186, loss_rpn_bbox: 0.0174, loss_cls: 0.0514, acc: 99.2953, loss_bbox: 0.0052, loss: 0.1926
2022-03-04 21:20:26,973 - mmdet - INFO - Epoch [2][80/88]	lr: 2.500e-03, eta: 0:07:19, time: 1.013, data_time: 0.025, memory: 4150, loss_rpn_cls: 0.1377, loss_rpn_bbox: 0.0211, loss_cls: 0.0541, acc: 99.2310, loss_bbox: 0.0049, loss: 0.2179
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 351/351, 6.7 task/s, elapsed: 52s, ETA:     0s
---------------iou_thr: 0.5---------------
2022-03-04 21:21:28,909 - mmdet - INFO - 
+--------+-----+------+--------+-------+
| class  | gts | dets | recall | ap    |
+--------+-----+------+--------+-------+
| cavity | 490 | 686  | 0.000  | 0.000 |
| pa     | 172 | 696  | 0.000  | 0.000 |
+--------+-----+------+--------+-------+
| mAP    |     |      |        | 0.000 |
+--------+-----+------+--------+-------+
2022-03-04 21:21:29,035 - mmdet - INFO - Epoch(val) [2][351]	AP50: 0.0000, mAP: 0.0000
2022-03-04 21:21:40,744 - mmdet - INFO - Epoch [3][10/88]	lr: 2.500e-03, eta: 0:07:22, time: 1.154, data_time: 0.251, memory: 4150, loss_rpn_cls: 0.1221, loss_rpn_bbox: 0.0174, loss_cls: 0.0472, acc: 99.3110, loss_bbox: 0.0023, loss: 0.1890
2022-03-04 21:21:50,115 - mmdet - INFO - Epoch [3][20/88]	lr: 2.500e-03, eta: 0:07:36, time: 0.935, data_time: 0.026, memory: 4150, loss_rpn_cls: 0.1162, loss_rpn_bbox: 0.0156, loss_cls: 0.0484, acc: 99.3057, loss_bbox: 0.0053, loss: 0.1856
2022-03-04 21:21:59,574 - mmdet - INFO - Epoch [3][30/88]	lr: 2.500e-03, eta: 0:07:48, time: 0.947, data_time: 0.026, memory: 4150, loss_rpn_cls: 0.1194, loss_rpn_bbox: 0.0184, loss_cls: 0.0473, acc: 99.3047, loss_bbox: 0.0070, loss: 0.1921
2022-03-04 21:22:08,808 - mmdet - INFO - Epoch [3][40/88]	lr: 2.500e-03, eta: 0:07:57, time: 0.924, data_time: 0.024, memory: 4150, loss_rpn_cls: 0.1386, loss_rpn_bbox: 0.0235, loss_cls: 0.0507, acc: 99.2411, loss_bbox: 0.0066, loss: 0.2193
2022-03-04 21:22:17,994 - mmdet - INFO - Epoch [3][50/88]	lr: 2.500e-03, eta: 0:08:04, time: 0.918, data_time: 0.023, memory: 4150, loss_rpn_cls: 0.1277, loss_rpn_bbox: 0.0199, loss_cls: 0.0524, acc: 99.2042, loss_bbox: 0.0063, loss: 0.2062
2022-03-04 21:22:27,251 - mmdet - INFO - Epoch [3][60/88]	lr: 2.500e-03, eta: 0:08:10, time: 0.925, data_time: 0.025, memory: 4150, loss_rpn_cls: 0.1124, loss_rpn_bbox: 0.0151, loss_cls: 0.0519, acc: 99.2095, loss_bbox: 0.0082, loss: 0.1877
2022-03-04 21:22:36,481 - mmdet - INFO - Epoch [3][70/88]	lr: 2.500e-03, eta: 0:08:15, time: 0.923, data_time: 0.025, memory: 4150, loss_rpn_cls: 0.1107, loss_rpn_bbox: 0.0137, loss_cls: 0.0492, acc: 99.2561, loss_bbox: 0.0045, loss: 0.1781
2022-03-04 21:22:45,755 - mmdet - INFO - Epoch [3][80/88]	lr: 2.500e-03, eta: 0:08:18, time: 0.927, data_time: 0.025, memory: 4150, loss_rpn_cls: 0.1037, loss_rpn_bbox: 0.0146, loss_cls: 0.0375, acc: 99.4686, loss_bbox: 0.0023, loss: 0.1581
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 351/351, 8.1 task/s, elapsed: 43s, ETA:     0s
---------------iou_thr: 0.5---------------
2022-03-04 21:23:36,839 - mmdet - INFO - 
+--------+-----+------+--------+-------+
| class  | gts | dets | recall | ap    |
+--------+-----+------+--------+-------+
| cavity | 490 | 627  | 0.000  | 0.000 |
| pa     | 172 | 642  | 0.000  | 0.000 |
+--------+-----+------+--------+-------+
| mAP    |     |      |        | 0.000 |
+--------+-----+------+--------+-------+
2022-03-04 21:23:36,949 - mmdet - INFO - Epoch(val) [3][351]	AP50: 0.0000, mAP: 0.0000
2022-03-04 21:23:48,465 - mmdet - INFO - Epoch [4][10/88]	lr: 2.500e-03, eta: 0:08:07, time: 1.132, data_time: 0.248, memory: 4150, loss_rpn_cls: 0.1304, loss_rpn_bbox: 0.0173, loss_cls: 0.0491, acc: 99.2145, loss_bbox: 0.0048, loss: 0.2017
2022-03-04 21:23:57,147 - mmdet - INFO - Epoch [4][20/88]	lr: 2.500e-03, eta: 0:08:08, time: 0.872, data_time: 0.026, memory: 4150, loss_rpn_cls: 0.1131, loss_rpn_bbox: 0.0151, loss_cls: 0.0408, acc: 99.3862, loss_bbox: 0.0028, loss: 0.1718
2022-03-04 21:24:04,628 - mmdet - INFO - Epoch [4][30/88]	lr: 2.500e-03, eta: 0:08:05, time: 0.748, data_time: 0.022, memory: 4150, loss_rpn_cls: 0.1386, loss_rpn_bbox: 0.0231, loss_cls: 0.0507, acc: 99.2047, loss_bbox: 0.0053, loss: 0.2177
2022-03-04 21:24:11,722 - mmdet - INFO - Epoch [4][40/88]	lr: 2.500e-03, eta: 0:08:00, time: 0.709, data_time: 0.022, memory: 4150, loss_rpn_cls: 0.1062, loss_rpn_bbox: 0.0152, loss_cls: 0.0460, acc: 99.3127, loss_bbox: 0.0073, loss: 0.1747
2022-03-04 21:24:19,158 - mmdet - INFO - Epoch [4][50/88]	lr: 2.500e-03, eta: 0:07:56, time: 0.744, data_time: 0.022, memory: 4150, loss_rpn_cls: 0.1103, loss_rpn_bbox: 0.0139, loss_cls: 0.0435, acc: 99.3448, loss_bbox: 0.0054, loss: 0.1731
2022-03-04 21:24:26,233 - mmdet - INFO - Epoch [4][60/88]	lr: 2.500e-03, eta: 0:07:51, time: 0.708, data_time: 0.021, memory: 4150, loss_rpn_cls: 0.1100, loss_rpn_bbox: 0.0158, loss_cls: 0.0382, acc: 99.4258, loss_bbox: 0.0040, loss: 0.1680
2022-03-04 21:24:33,200 - mmdet - INFO - Epoch [4][70/88]	lr: 2.500e-03, eta: 0:07:46, time: 0.697, data_time: 0.021, memory: 4150, loss_rpn_cls: 0.1064, loss_rpn_bbox: 0.0158, loss_cls: 0.0449, acc: 99.3171, loss_bbox: 0.0066, loss: 0.1738
2022-03-04 21:24:40,105 - mmdet - INFO - Epoch [4][80/88]	lr: 2.500e-03, eta: 0:07:40, time: 0.691, data_time: 0.021, memory: 4150, loss_rpn_cls: 0.1024, loss_rpn_bbox: 0.0165, loss_cls: 0.0422, acc: 99.3595, loss_bbox: 0.0053, loss: 0.1664
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 351/351, 11.3 task/s, elapsed: 31s, ETA:     0s
---------------iou_thr: 0.5---------------
2022-03-04 21:25:16,887 - mmdet - INFO - 
+--------+-----+------+--------+-------+
| class  | gts | dets | recall | ap    |
+--------+-----+------+--------+-------+
| cavity | 490 | 1    | 0.000  | 0.000 |
| pa     | 172 | 1    | 0.000  | 0.000 |
+--------+-----+------+--------+-------+
| mAP    |     |      |        | 0.000 |
+--------+-----+------+--------+-------+
2022-03-04 21:25:16,998 - mmdet - INFO - Epoch(val) [4][351]	AP50: 0.0000, mAP: 0.0000
2022-03-04 21:25:26,499 - mmdet - INFO - Epoch [5][10/88]	lr: 2.500e-03, eta: 0:07:24, time: 0.935, data_time: 0.247, memory: 4150, loss_rpn_cls: 0.1406, loss_rpn_bbox: 0.0196, loss_cls: 0.0434, acc: 99.3253, loss_bbox: 0.0051, loss: 0.2086
2022-03-04 21:25:33,520 - mmdet - INFO - Epoch [5][20/88]	lr: 2.500e-03, eta: 0:07:19, time: 0.702, data_time: 0.022, memory: 4150, loss_rpn_cls: 0.1178, loss_rpn_bbox: 0.0188, loss_cls: 0.0399, acc: 99.3976, loss_bbox: 0.0028, loss: 0.1793
2022-03-04 21:25:40,410 - mmdet - INFO - Epoch [5][30/88]	lr: 2.500e-03, eta: 0:07:13, time: 0.689, data_time: 0.022, memory: 4150, loss_rpn_cls: 0.1123, loss_rpn_bbox: 0.0166, loss_cls: 0.0431, acc: 99.3181, loss_bbox: 0.0043, loss: 0.1764
2022-03-04 21:25:47,383 - mmdet - INFO - Epoch [5][40/88]	lr: 2.500e-03, eta: 0:07:08, time: 0.698, data_time: 0.022, memory: 4150, loss_rpn_cls: 0.0969, loss_rpn_bbox: 0.0118, loss_cls: 0.0419, acc: 99.3701, loss_bbox: 0.0062, loss: 0.1568
2022-03-04 21:25:54,425 - mmdet - INFO - Epoch [5][50/88]	lr: 2.500e-03, eta: 0:07:02, time: 0.704, data_time: 0.021, memory: 4150, loss_rpn_cls: 0.1298, loss_rpn_bbox: 0.0213, loss_cls: 0.0415, acc: 99.3547, loss_bbox: 0.0054, loss: 0.1980
2022-03-04 21:26:01,477 - mmdet - INFO - Epoch [5][60/88]	lr: 2.500e-03, eta: 0:06:57, time: 0.705, data_time: 0.021, memory: 4150, loss_rpn_cls: 0.1163, loss_rpn_bbox: 0.0149, loss_cls: 0.0462, acc: 99.2868, loss_bbox: 0.0058, loss: 0.1832
2022-03-04 21:26:08,455 - mmdet - INFO - Epoch [5][70/88]	lr: 2.500e-03, eta: 0:06:51, time: 0.696, data_time: 0.021, memory: 4150, loss_rpn_cls: 0.1050, loss_rpn_bbox: 0.0146, loss_cls: 0.0430, acc: 99.3356, loss_bbox: 0.0069, loss: 0.1696
2022-03-04 21:26:15,400 - mmdet - INFO - Epoch [5][80/88]	lr: 2.500e-03, eta: 0:06:45, time: 0.696, data_time: 0.023, memory: 4150, loss_rpn_cls: 0.1199, loss_rpn_bbox: 0.0176, loss_cls: 0.0446, acc: 99.3069, loss_bbox: 0.0037, loss: 0.1858
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 351/351, 11.6 task/s, elapsed: 30s, ETA:     0s
---------------iou_thr: 0.5---------------
2022-03-04 21:26:51,395 - mmdet - INFO - 
+--------+-----+------+--------+-------+
| class  | gts | dets | recall | ap    |
+--------+-----+------+--------+-------+
| cavity | 490 | 0    | 0.000  | 0.000 |
| pa     | 172 | 0    | 0.000  | 0.000 |
+--------+-----+------+--------+-------+
| mAP    |     |      |        | 0.000 |
+--------+-----+------+--------+-------+
```",neutral,negative
351,想问一下论文里的东西,jiesutd/LatticeLSTM,closed,,yujh123,jiesutd,2019-10-23 01:34:06,2019-10-23 01:35:12,1.0,,96.0,511018061.0,"您好！
想请教一下论文里面的Lattice LSTM模型有提到是用的双向还是单向LSTM吗？希望您解答，谢谢。",neutral,neutral
352,Deprecate GELU ,tensorflow/addons,closed,,seanpmorgan,seanpmorgan,2020-07-16 00:43:00,2020-08-03 01:49:33,2.0,help wanted#activations,2005.0,657753229.0,"Per https://github.com/tensorflow/community/pull/252, oonce  https://github.com/tensorflow/tensorflow/pull/41178 merges we'll need to deprecate our GELU for versions of TensorFlow that include it within core.",neutral,neutral
353,Installing svae: any instructions?,mattjj/svae,open,,finmod,,2016-04-05 12:06:14,,8.0,,1.0,145968007.0,"Any instructions on how to install svae? A similar to autograd and simple install like 

pip install svae or pip install https://github.com/mattjj/svae/archive/master.zip

would be appreciated.  

Thanks
",neutral,positive
354,ImportError: No module named nltk.corpus,cyvius96/adgpm,closed,,joanconquerlee,yinboc,2019-10-14 03:23:24,2019-10-14 20:22:46,1.0,,11.0,506430087.0,"root@aca4066e21cd:/tmp/pycharm_project_546/materials# python make_induced_graph.py
Traceback (most recent call last):
  File ""make_induced_graph.py"", line 4, in <module>
    from nltk.corpus import wordnet as wn
ImportError: No module named nltk.corpus
",neutral,neutral
355,pad_val not used in class Pad when pad to a fixed size,open-mmlab/mmdetection,closed,,lhcalibur,hellock,2020-02-12 08:01:37,2020-02-13 03:48:31,1.0,,2083.0,563821901.0,"in mmdet/datasets/pipelines/transforms.py **class Pad**
```python
    def _pad_img(self, results):
        if self.size is not None:
            padded_img = mmcv.impad(results['img'], self.size)
```
self.pad_val is not pass to mmcv.impad",neutral,neutral
356,"Moving from AWS to local inference, which GPU?",matterport/Mask_RCNN,open,,mehditlili,,2019-01-30 09:43:27,,3.0,,1262.0,404683381.0,"Hi everybody,
I have been using mask rcnn for a while now running on an AWS EC2 instance (p2.xlarge). This instance came with a Tesla k80 with half the memory (12Gb). It could run inference on 320x240 rgb images in 200ms. Now I am looking to have my own inference machine locally and am thinking about buying a GPU to replace the K80. I couldn't find any benchmarking for maskrcnn. This [blog post](http://timdettmers.com/2018/11/05/which-gpu-for-deep-learning/) was helpful but I wanted to ask you guys about your experience. Would an RTX 2070 be able to handle inference in 200ms, or even lower? I can't run inference on more than one image as I am getting data from a camera so my batch size in inference is limited to 1. Also do you have any experience with 16bit floats for maskrcnn?",neutral,neutral
357,Feature request: Rectangular images,mingyuliutw/UNIT,closed,,slundell,mingyuliutw,2017-11-10 17:33:52,2017-11-10 17:45:59,1.0,,19.0,273006745.0,"As far as I understand, in the current version only image sets consisting of images where width == height are properly supported. Current workaround would be to pre- and post-scale aspect ratios or add padding. Niether is ideal.",neutral,neutral
358,S3DIS point number,yangyanli/PointCNN,closed,,CZ-Wu,yangyanli,2018-11-30 01:40:50,2018-12-04 00:16:19,2.0,,119.0,386001082.0,请问s3dis分割你们用了多少个点的？好像s3dis_x8_2048_fps.py并没有点数的设置，你们的点数是是在生成数据集的时候确定的吗？你们是用8192个点的吗？,neutral,neutral
359,How to use batchsize in onnxruntime? ,deepinsight/insightface,open,,C-Jaewon,,2021-05-12 07:29:15,,0.0,,1506.0,889768686.0,"Hi, thank you for the good work.

How to use batchsize in onnxruntime? ",neutral,positive
360,Bugs when the labels is empty during inference.,open-mmlab/mmdetection,closed,,bo-miao,hellock,2020-05-12 12:22:02,2020-05-20 07:41:26,1.0,,2696.0,616612533.0,"Hi,

I noticed that if labels is empty during inference, there will have a bug in max(labels) as shown below.

============================
mmdet/models/detectors/base.py
row:204
raw:
            color_masks = [
                np.random.randint(0, 256, (1, 3), dtype=np.uint8)
                for _ in range(max(labels) + 1)
            ]
fixed:
            max_labels = max(labels) if len(labels) > 0 else 0
            color_masks = [
                np.random.randint(0, 256, (1, 3), dtype=np.uint8)
                for _ in range(max_labels + 1)
            ]

",neutral,neutral
361,How can I config the output of picture? If model predict both mask and box but I only want mask.,open-mmlab/mmdetection,closed,BIGWangYuDong,peace20162,BIGWangYuDong,2021-06-21 07:04:32,2021-07-08 05:13:28,1.0,,5404.0,925896467.0,"I try to use DetectoRS  model for mask my data but it also make box on my picture. I want to remove background with this model.
",neutral,neutral
362,When appear *SparqlExecutor.execute* in log?,percyliang/sempre,closed,,SaraMitchell,SaraMitchell,2015-05-19 19:22:48,2015-05-19 20:15:48,3.0,,49.0,78218252.0,"Hello all

I am really confused in something
When I run this command that is said in _Quickstart.md_:

> ```
> ./sempre @mode=train \
>          @domain=webquestions \
>          @sparqlserver=localhost:3093 \
>          @cacheserver=local \
>          -Learner.maxTrainIters 0 \
>          -Dataset.inPaths test:testinput \
>          -Builder.inParamsPath lib/models/15.exec/params \
>          -Grammar.inPaths lib/models/15.exec/grammar \
>          -Dataset.readLispTreeFormat true
> ```

sometime _SparqlExecutor.execute_ output comes in my log and sometimes it dosent come!!!!!!!

> SparqlExecutor.execute: (fb:user.moritzstefaner.maceproject.lesson.interactivity_level fb:m.0sn7v5c) 
>               SparqlExecutor.execute: (fb:user.dsp13.default_domain.user_group.interesting_topics fb:en.three_plus) 
>               SparqlExecutor.execute: (fb:user.dsp13.default_domain.user_group.interesting_topics fb:m.0sn7v5c) 
>               SparqlExecutor.execute: (fb:user.avic.assertion_modeling_kit.freebase_proposition.input_2 fb:m.0smftlj) 
>               SparqlExecutor.execute: (fb:user.moritzstefaner.maceproject.lesson.interactivity_level fb:m.0smftlj) 
>               SparqlExecutor.execute: (fb:user.avic.assertion_modeling_kit.freebase_proposition.input_2 fb:en.three_plus) 
>               SparqlExecutor.execute:

I always use same command!
",neutral,negative
363,test pretrain model on celebA are strange,JiahuiYu/generative_inpainting,closed,,ljjcoder,ljjcoder,2018-07-12 13:38:36,2018-07-12 13:52:35,2.0,,82.0,340642053.0,"Hi, Jiahui! i want to test the pretrain model on celebA. I download the pretrain model of celebA and put it at ./model_logs/celebA_model. 
   when I run  python test.py --image examples/celeba/celebahr_patches_164787_input.png --mask examples/center_mask_256.png --output examples/output.png --checkpoint_dir model_logs/celebA_model
,I get the very bad result.
![celebahr_patches_164787_input](https://user-images.githubusercontent.com/40750189/42636183-ad4bf266-861a-11e8-838c-5f20d5c38377.png)

![output3_pretrain](https://user-images.githubusercontent.com/40750189/42635264-306d1b32-8618-11e8-9e08-383745fc9332.png)
I don't know why it happens. can you help me?

",neutral,negative
364,Bug during ScreamDSprites initialization,google-research/disentanglement_lib,open,,bonheml,,2021-02-23 10:18:49,,0.0,,34.0,814320484.0,"I have noticed a bug when trying to use ScreamDsprites:
```Python
from disentanglement_lib.data.ground_truth.dsprites import ScreamDSprites
ScreamDSprites([1,2,3,4,5])
```
Raises an error:
`Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""disentanglement_lib/disentanglement_lib/data/ground_truth/dsprites.py"", line 183, in __init__
    scream.thumbnail((350, 274, 3))
  File "".venv/disentanglement_lib/lib/python3.6/site-packages/PIL/Image.py"", line 2299, in thumbnail
    x, y = map(math.floor, size)
ValueError: too many values to unpack (expected 2)
`

The issue seems to be that thumbnail size is taking a tuple of 3 values instead of 2.
I have modified [l.182 of dsprites.py](https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/data/ground_truth/dsprites.py#L182) from `scream.thumbnail((350, 274, 3))` to `scream.thumbnail((350, 274))` and this is working properly.

I am using Pillow 8.0.1, but the thumbnail [size was already 2D in Pillow 5.0.0](https://github.com/python-pillow/Pillow/blob/5.0.x/src/PIL/Image.py#L2054) so this is not a compatibility issue.

I should add that [the bug is also present in cars3D](https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/data/ground_truth/cars3d.py#L110)
",neutral,negative
365,Ubuntu18.04下编译出现一连串的错误,open-mmlab/mmdetection,closed,,ghost,hellock,2019-08-24 03:50:54,2019-08-24 15:22:24,2.0,,1248.0,484774687.0,"环境是Ubuntu18.04，cuda10.0
最初的错误是
Cython.Compiler.Errors.CompilerCrash: 
Error compiling Cython file:
------------------------------------------------------------
...
    if PyDataType_HASSUBARRAY(d):
        return <tuple>d.subarray.shape
    else:
        return ()

cdef inline char* _util_dtypestring(dtype descr, char* f, char* end, int* offset) except NULL:
    ^
------------------------------------------------------------
接着是一堆的Exception，最后的提示是
  File ""Cython/Compiler/Visitor.py"", line 180, in Cython.Compiler.Visitor.TreeVisitor._visit
  File ""Cython/Compiler/Visitor.py"", line 312, in Cython.Compiler.Visitor.CythonTransform.visit_Node
  File ""Cython/Compiler/Visitor.py"", line 260, in Cython.Compiler.Visitor.VisitorTransform._process_children
  File ""Cython/Compiler/Visitor.py"", line 219, in Cython.Compiler.Visitor.TreeVisitor._visitchildren
  File ""Cython/Compiler/Visitor.py"", line 193, in Cython.Compiler.Visitor.TreeVisitor._visitchild
  File ""Cython/Compiler/Visitor.py"", line 188, in Cython.Compiler.Visitor.TreeVisitor._visit
  File ""Cython/Compiler/Visitor.py"", line 148, in Cython.Compiler.Visitor.TreeVisitor._raise_compiler_error
setuptools.sandbox.UnpickleableException: CompilerCrash((<FileSourceDescriptor:/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Includes/numpy/__init__.pxd>, 842, 5), 'AnalyseExpressionsTransform', 'Compiler crash in AnalyseExpressionsTransform\n\nModuleNode.body = StatListNode(__init__.pxd:17:0)\nStatListNode.stats[41] = StatListNode(__init__.pxd:842:5)\nStatListNode.stats[0] = CFuncDefNode(__init__.pxd:842:5,\n    args = [...]/4,\n    inline_in_pxd = True,\n    modifiers = [...]/1,\n    visibility = \'private\')\n\nCompiler crash traceback from this point on:\n  File ""Cython/Compiler/Visitor.py"", line 180, in Cython.Compiler.Visitor.TreeVisitor._visit\n  File ""/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Compiler/ParseTreeTransforms.py"", line 2215, in visit_FuncDefNode\n    node.body = node.body.analyse_expressions(node.local_scope)\n  File ""/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Compiler/Nodes.py"", line 436, in analyse_expressions\n    for stat in self.stats]\n  File ""/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Compiler/Nodes.py"", line 436, in <listcomp>\n    for stat in self.stats]\n  File ""/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Compiler/Nodes.py"", line 6675, in analyse_expressions\n    self.item = self.item.coerce_to(self.target.type, env)\n  File ""/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Compiler/ExprNodes.py"", line 952, in coerce_to\n    src = PyTypeTestNode(src, dst_type, env)\n  File ""/home/shuangt/ObjectDetectionCode/mmdetection/.eggs/Cython-0.29.13-py3.7-linux-x86_64.egg/Cython/Compiler/ExprNodes.py"", line 12999, in __init__\n    assert dst_type.is_extension_type or dst_type.is_builtin_type, ""PyTypeTest on non extension type""\nAssertionError: PyTypeTest on non extension type', AssertionError('PyTypeTest on non extension type'), <traceback object at 0x7f8163413eb0>)

",neutral,negative
366,Where to find the Detailed parameter name of ‘main_solve_options’？,vtjeng/MIPVerify.jl,open,,Herler66,,2022-09-23 02:08:17,,1.0,,127.0,1383202385.0,I want to set a timelimit ，but i cant  find the   related  parameter name of ‘main_solve_options’,neutral,neutral
367,在进行可视化的时候，num_iters_per_epoch经常出错，得到的结果图是错误的,open-mmlab/mmdetection,closed,"hhaAndroid
jshilong",liuyuan000,jshilong,2021-07-09 00:46:11,2021-08-23 03:18:26,5.0,,5571.0,940321102.0,"默认训练一次，测试一次，但是在可视化绘图的时候，默认第一次没有训练。
修改建议：
将anglyze_logs.py的第68行附近的num_iters_per_epoch的初始化赋值修改为：
```
                if log_dict[epochs[0]]['mode'][-1]=='val':
                    num_iters_per_epoch = log_dict[epochs[0]]['iter'][-2]
                else:
                    num_iters_per_epoch = log_dict[epochs[0]]['iter'][-1]
```
",neutral,neutral
368,torch 1.5 support,open-mmlab/mmdetection,closed,,jinfagang,hellock,2020-04-23 03:33:01,2020-04-24 15:06:36,3.0,,2513.0,605212826.0,"torch 1.5 was released!

c++ code inside ops will not built since AT_CHECK is deprecated and will thourgh not defined error.

Recommended replace AT_CHECK with TORCH_CHECK to support torch 1.5",neutral,negative
369,edit mmdet saved checkpoint pth file's meta data,open-mmlab/mmdetection,open,ZwwWayne,jinfagang,,2021-12-19 11:57:14,,1.0,enhancement#upstream,6829.0,1084080632.0,"Hi, I have trained a checkpoint file, I currently changed the custom_imports paths, but checkpoints saved one doesn't changed.

I want load the checkpoint back, and met the problem:

1). How to edit pth meta, and assign new custom_imports paths;
2). Why load checkpoint read meta, but override config paths. (in this case, I changed config file and old pth can not be loaded raise import error).. **this doesn make any sense, since pth should only concern about weights, not import paths. Even PyTorch vanilla pth, will not include import paths in their weights names.**",neutral,negative
370,"Does the ""validate"" procedure should be executed each epoch?",open-mmlab/mmdetection,closed,,ronghaoLee,ronghaoLee,2019-09-29 02:10:26,2019-11-11 07:13:57,3.0,,1464.0,499843904.0,"I have created a new dataset according to the data format of the COCO
But when I try to train the model with the ""validate"" setted by the 'parser', I found the validate procedure is too long because of the big validation dataset.

How can I validate the training model with a longer interval? (For example, how can I validate the model once per 5 training epochs)",neutral,neutral
371,Detail of using it to extract feature from video?,ZhaofanQiu/pseudo-3d-residual-networks,open,,leileestone,,2017-11-20 10:57:53,,2.0,,10.0,275314323.0,"I'm really sorry to bother you,I want use P3D to extract the feature of video like C3D,  Can you show me some detail of implement?
In the caffe version of P3D, I have compiled the original caffe， shoud I  complie the caffe with some file you have supported？",neutral,neutral
372,How to train fast_rcnn model?,open-mmlab/mmdetection,closed,zytx121,Soodep,zytx121,2022-02-04 19:07:23,2022-04-27 03:03:50,5.0,Doc#How-to,7120.0,1124525119.0,"For Faster RCNN I can easily  use the config/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py and train by making some changes to the voc0712.py file ( provide the path to ann_file) as I have the annotation and JPEG images but how do I train the Fast RCNN model? When I use the same method for Fast RCNN it gives me error saying:

Traceback (most recent call last):
  File ""tools/train.py"", line 196, in <module>
    main()
  File ""tools/train.py"", line 192, in main
    meta=meta)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmdet-2.20.0-py3.7.egg/mmdet/apis/train.py"", line 209, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 50, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 30, in run_iter
    **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py"", line 75, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmdet-2.20.0-py3.7.egg/mmdet/models/detectors/base.py"", line 248, in train_step
    losses = self(**data)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py"", line 109, in new_func
    return old_func(*args, **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmdet-2.20.0-py3.7.egg/mmdet/models/detectors/base.py"", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmdet-2.20.0-py3.7.egg/mmdet/models/detectors/two_stage.py"", line 150, in forward_train
    **kwargs)
  File ""/home/UNT/sd0570/anaconda3/envs/newmmdet/lib/python3.7/site-packages/mmdet-2.20.0-py3.7.egg/mmdet/models/roi_heads/standard_roi_head.py"", line 91, in forward_train
    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],
TypeError: 'NoneType' object is not subscriptable

Am I missing somethig? 
Your help is highly appreciated.",neutral,positive
373,How to split the Cityscapes for pretraining?,TRI-ML/packnet-sfm,closed,,UltronAI,VitorGuizilini-TRI,2020-05-21 16:17:52,2021-01-04 02:24:45,7.0,,17.0,622615921.0,"Is there a .txt file containing training files? just like eigen_zhou_files you provide in the readme. 

I'll appreciate it if you would like to share the dataset file for CS and its training files list. Thanks!",neutral,positive
374,How to extract feature with Faster RCNN?,open-mmlab/mmdetection,closed,hhaAndroid,dinhanhx,github-actions[bot],2022-08-09 07:17:44,2022-09-21 11:31:17,4.0,community help wanted#awaiting response#Stale,8525.0,1332820788.0,"the following code gives bounding boxes, classes, probabilities of the objects 
```
from mmdet.apis import init_detector, inference_detector

config_file = 'faster_rcnn_x101_64x4d_fpn_mstrain_3x_coco.py'
checkpoint_file = 'faster_rcnn_x101_64x4d_fpn_mstrain_3x_coco_20210524_124528-26c63de6.pth'
model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'
result = inference_detector(model, 'cat.jpg')
```

Now I want to get the feature vectors of each object. Or is there any way that I can partially execute the model? Like this one https://detectron2.readthedocs.io/en/latest/tutorials/models.html#partially-execute-a-model",neutral,neutral
375,Any faster feature extraction tools (better than the one in bottom-up-detection and lxmert)?,ChenRocks/UNITER,open,,yezhengli-Mr9,,2021-01-07 08:01:58,,2.0,,60.0,781115731.0,"Hi @ChenRocks @linjieli222,
It is still ok even if it is not used by  [VisualBERT](https://github.com/uclanlp/visualbert/), [LXMERT](https://github.com/airsplay/lxmert), and [UNITER](https://github.com/ChenRocks/UNITER).

- I have to handle 1,102,076+ personal images and have to find a fast one even the performance might be lower -- it is fine NOT to be any of [VisualBERT](https://github.com/uclanlp/visualbert/), [LXMERT](https://github.com/airsplay/lxmert), and [UNITER](https://github.com/ChenRocks/UNITER), but **better faster to implement**.

For example, by comparison with one GPU (cpu-only is presumably [not tolerable](https://github.com/airsplay/lxmert/issues/89)), for NLVR2 107,292 images,  [lxmert](https://github.com/airsplay/lxmert) takes 5-6 hours to extract faster-rcnn features by [this caffe](https://github.com/peteanderson80/bottom-up-attention/tree/master/caffe).
I also follow visualBERT's [issue#1](https://github.com/uclanlp/visualbert/issues/10) and [issue#10](https://github.com/uclanlp/visualbert/issues/10), [LXMERT](https://github.com/airsplay/lxmert), [transformers-VQA](https://github.com/YIKUAN8/Transformers-VQA/issues/5). 
",neutral,neutral
376,Why aren't annotations loaded in the test pipeline?,open-mmlab/mmdetection,closed,,freemagma,freemagma,2020-06-24 20:49:04,2020-07-01 18:16:38,2.0,,3121.0,644951907.0,"I'm trying to train a Faster-RCNN model on the COCO dataset with augmented class labels. I think I can easily change the labels when training by adding a custom data pipeline after LoadAnnotations, but I would also like to test and validate using the same relabeling. However, it doesn't appear that the annotations are ever even loaded into the test pipeline. What is the reason for this, and is there a workaround for my problem? If not, how can I easily modify the entire annotation file to reflect the label changes I want?",neutral,neutral
377,the training process may get into stuck,open-mmlab/mmdetection,closed,,miracle-fmh,miracle-fmh,2018-12-11 02:00:02,2018-12-13 13:34:49,17.0,,166.0,389564368.0,"After training some iterations, the GPU-Util may increased from about 50% to 100%, and then the training get into totally stuck and can not training any iterations, and the code can not throw any error.",neutral,neutral
378,Question about TCC,google-research/google-research,open,,daidedou,,2021-09-20 12:48:52,,1.0,,821.0,1000930481.0,"Hi, 
Thank you very much about all the sharings! I am currently trying to work on TCC - Temporal Cycle-Consistency Learning - but there is a technical question I can't figure out.
What exactly does the network sees? At first I thought it was pairs of videos of the same class, but the loss takes in account 1 batch of samples.

It seems that the loss is computed over all pairs of samples in the batch, but what is the condition on the samples of the batch? In particular, in the paper it is said that ""The core contribution of this work is a self-supervised
approach to learn an embedding space where two similar
video sequences can be aligned temporally."". So I would find it weird to align 2 videos that aren't similar (like pouring/tennis serve).

Do you know where in the code can I find information about such stuff? I am not really familiar with tensorflow :(",neutral,negative
379,simple-romp output .npz file,Arthur151/ROMP,closed,,marson666,marson666,2022-03-30 13:36:28,2022-03-31 03:28:01,1.0,,180.0,1186475678.0,"Hi, thanks for your great work!
I am trying to use the simple-romp output .npz file of 'joints' as 3D keypoints positions, but i find that it has no global root position movement, i find that in romp export folder, it uses 'cam_trans' [here](https://github.com/Arthur151/ROMP/blob/1b1c37322f7096da5f544b11a1e1c1c13bf6a259/romp/predict/base_predictor.py#L53) to trans global position. But the simple-romp output .npz file only has 'cam' param, does the cam has the same meaning with 'cam_trans' ?
Look forward to your reply!",neutral,positive
380,It takes too long to compile the train and eval process when reproducing muNet on 8 A100 GPUs,google-research/google-research,open,,gouchangjiang,,2022-06-30 07:39:36,,0.0,,1188.0,1289727703.0,"Hi there,

I am reproducing the muNet on 8 A100 GPUs. Compared to running it on Colab TPUv2 8 cores,  it takes too long to compile each child model. XLA also reminds me that it takes too long and suggests that there may be a bug. So, an [issue](https://github.com/google/jax/issues/11271) is also posted on the JAX repository, but so far no one respond. Maybe it's better to post it here. To be clear, I will add more main info here.

- The script I am using is [munet](https://colab.research.google.com/github/google-research/google-research/blob/master/muNet/muNet.ipynb), the experiment running is the smallest one, 'ViT tiny 3 layers / characters benchmark’. 
- Packages are installed in the way specified by munet script.
- The Cuda version is 11.4, then cudnn version is 8.2, python version is 3.9
- it only happens when running on 8 gpus, running it on one gpu is fine
- I am running it with Slurm, so maybe slurm does not give enough process to Python? The command is srun --partition=xxx --gres=gpu:8 -N1 -n1 --ntasks-per-node=1 --cpus-per-task=32 python munet_30June.py. I give 32 cores to this task. It should be fine.

Any hint is welcome. Thanks in advance.",neutral,positive
381,node regression,mdeff/cnn_graph,closed,,jdily,jdily,2016-10-21 23:18:05,2016-10-22 20:20:27,3.0,,4.0,184589894.0,"Hi, i am experimenting some of my own data, and I think it is relevant to your description about ""node regression"" setting, but I am not so sure.

So as I understand, (correct me if I am wrong.), take mnist experiment as example, 
Each sample feed into the network is a digit image, conveted to a graph with 28x28 nodes.
The goal is to classify this **entire graph** to a label, say, 8.

What I am looking for is that, I have my own data which is a bigger graph, and there is a value or a vector of values on each node.
And i am thinking if it's possible to:
1.  given a node, only feed in it's neighbors into network
2. the corresponding output of this node input is it's associate value.
   I think it is more like I want to use a subgraph as training....

I am not so sure if this is the same as you mentioned in the usage.ipynb node classification/regression..
Do you have any hint or suggestion of how to do that?

Thx in advance~!
",neutral,negative
382,keyerror(0),matterport/Mask_RCNN,open,,Seven-JQ,,2018-11-19 08:21:49,,0.0,,1130.0,382102529.0,"Train my own data and encountered the following problem. 

# Train the head branches
# Passing layers=""heads"" freezes all layers except the head
# layers. You can also pass a regular expression to select
# which layers to train by name pattern.
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE, 
            epochs=1, 
            layers='heads')




Starting at epoch 0. LR=0.001

Checkpoint Path: E:\Mask_RCNN-master-try\logs\shapes20181118T2016\mask_rcnn_shapes_{epoch:04d}.h5
Selecting layers to train
fpn_c5p5               (Conv2D)
fpn_c4p4               (Conv2D)
fpn_c3p3               (Conv2D)
fpn_c2p2               (Conv2D)
fpn_p5                 (Conv2D)
fpn_p2                 (Conv2D)
fpn_p3                 (Conv2D)
fpn_p4                 (Conv2D)
In model:  rpn_model
    rpn_conv_shared        (Conv2D)
    rpn_class_raw          (Conv2D)
    rpn_bbox_pred          (Conv2D)
mrcnn_mask_conv1       (TimeDistributed)
mrcnn_mask_bn1         (TimeDistributed)
mrcnn_mask_conv2       (TimeDistributed)
mrcnn_mask_bn2         (TimeDistributed)
mrcnn_class_conv1      (TimeDistributed)
mrcnn_class_bn1        (TimeDistributed)
mrcnn_mask_conv3       (TimeDistributed)
mrcnn_mask_bn3         (TimeDistributed)
mrcnn_class_conv2      (TimeDistributed)
mrcnn_class_bn2        (TimeDistributed)
mrcnn_mask_conv4       (TimeDistributed)
mrcnn_mask_bn4         (TimeDistributed)
mrcnn_bbox_fc          (TimeDistributed)
mrcnn_mask_deconv      (TimeDistributed)
mrcnn_class_logits     (TimeDistributed)
mrcnn_mask             (TimeDistributed)

C:\Users\Administrator\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\ops\gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""

Epoch 1/1
image_id 14
ERROR:root:Error processing image {'mask_path': 'train_data/cv2_mask/7.png', 'height': 1857, 'width': 1280, 'yaml_path': 'train_data/labelme_json/7_json/info.yaml', 'path': 'train_data/pic/7.jpg', 'source': 'shapes', 'id': 14}
Traceback (most recent call last):
  File ""E:\Mask_RCNN-master-try\mrcnn\model.py"", line 1710, in data_generator
    use_mini_mask=config.USE_MINI_MASK)
  File ""E:\Mask_RCNN-master-try\mrcnn\model.py"", line 1213, in load_image_gt
    mask, class_ids = dataset.load_mask(image_id)
  File ""<ipython-input-3-e3b8787a20d8>"", line 80, in load_mask
    labels = self.from_yaml_get_class(image_id)
  File ""<ipython-input-3-e3b8787a20d8>"", line 13, in from_yaml_get_class
    del labels[0]
KeyError: 0
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-8-b3af8777b8e4> in <module>()
      6             learning_rate=config.LEARNING_RATE,
      7             epochs=1,
----> 8             layers='heads')
      9 
     10 image=skimage.color.gray2rgb(image)

E:\Mask_RCNN-master-try\mrcnn\model.py in train(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)
   2373             max_queue_size=100,
   2374             workers=workers,
-> 2375             use_multiprocessing=True,
   2376         )
   2377         self.epoch = max(self.epoch, epochs)

C:\Users\Administrator\AppData\Roaming\Python\Python35\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---> 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

C:\Users\Administrator\AppData\Roaming\Python\Python35\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   2143                 batch_index = 0
   2144                 while steps_done < steps_per_epoch:
-> 2145                     generator_output = next(output_generator)
   2146 
   2147                     if not hasattr(generator_output, '__len__'):

E:\Mask_RCNN-master-try\mrcnn\model.py in data_generator(dataset, config, shuffle, augment, augmentation, random_rois, batch_size, detection_targets, no_augmentation_sources)
   1708                     load_image_gt(dataset, config, image_id, augment=augment,
   1709                                 augmentation=augmentation,
-> 1710                                 use_mini_mask=config.USE_MINI_MASK)
   1711 
   1712             # Skip images that have no instances. This can happen in cases

E:\Mask_RCNN-master-try\mrcnn\model.py in load_image_gt(dataset, config, image_id, augment, augmentation, use_mini_mask)
   1211     # Load image and mask
   1212     image = dataset.load_image(image_id)
-> 1213     mask, class_ids = dataset.load_mask(image_id)
   1214     original_shape = image.shape
   1215     image, window, scale, padding, crop = utils.resize_image(

<ipython-input-3-e3b8787a20d8> in load_mask(self, image_id)
     78             occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))
     79         labels = []
---> 80         labels = self.from_yaml_get_class(image_id)
     81         labels_form = []
     82         for i in range(len(labels)):

<ipython-input-3-e3b8787a20d8> in from_yaml_get_class(self, image_id)
     11             temp = yaml.load(f.read())
     12             labels = temp['label_names']
---> 13             del labels[0]
     14         return labels
     15 

KeyError: 0
",neutral,neutral
383,Resume training: should we update the learning rate?,matterport/Mask_RCNN,open,,netw0rkf10w,,2018-06-08 11:40:00,,0.0,,657.0,330624979.0,"Hello,

To resume a training session, in addition to reading the last saved weights, do we also have to manually set the learning rate (to a lower value) as well?

Thanks a lot!",neutral,positive
