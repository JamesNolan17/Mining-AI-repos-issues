Title,Name_Repo,Date_Created,Num_Comment,Label_Issue,Identity_Repo,Identity_Global,Body
Error Metric Clarification,ubc-vision/image-matching-benchmark,2022-08-23 15:44:59,0,,46,1348162765,"Hello, 

Thanks for you work!

I have a question about the error metrics:

```
# Compute err_q and err_t only when R, t are not None
    err_q, err_t = np.inf, np.inf
    if (R_1_actual is not None) and (R_2_actual is not None) and (
            t_1_actual is not None) and (t_2_actual is not None):
        # Compute dR, dt (actual)
        dR_act = np.dot(R_2_actual, R_1_actual.T)
        dt_act = t_2_actual - np.dot(dR_act, t_1_actual)

        # Get R, t from calibration information
        R_1, t_1 = calib1['R'], calib1['T'].reshape((3, 1))
        R_2, t_2 = calib2['R'], calib2['T'].reshape((3, 1))

        # Compute ground truth dR, dt
        dR = np.dot(R_2, R_1.T)
        dt = t_2 - np.dot(dR, t_1)

        # Save err_, err_t
        err_q, err_t = evaluate_R_t(dR, dt, dR_act, dt_act)
```

Taken from: https://github.com/ubc-vision/image-matching-benchmark/blob/master/utils/colmap_helper.py#L156

Which one is the ground truth here ? 
`dR` and `dt` seem to be but then you have `dR_act` and `dt_act` just above that are retrieved from COLMAP (which what you use for ground truth ?)

Also `dR_act = np.dot(R_2_actual, R_1_actual.T)` is the rotation between `R_2_actual` and `R_1_actual` ?

What is `dt_act = t_2_actual - np.dot(dR_act, t_1_actual)` ? If it is the difference why is `t_1_actual` multiplied by `dR_act`? 

Thanks!
"
Some simple corrections,ubc-vision/image-matching-benchmark,2022-04-28 13:35:54,0,,45,1218741660," depth_clean = f['depth'].value

gives errors and can be corrected like

 **depth_clean = f['depth'][:]**

______________________________________________________________

Then 

f2 = plt.imshow(data['depth'].astype(np.float))

can be corrected like

**# f2 = plt.imshow(data['depth'])**

______________________________________________________________

changing 

plt.imshow(rgb.transpose((1, 2, 0)).astype(np.float))

to 

plt.imshow(rgb.transpose((1, 2, 0)).astype(np.float64))

prevents some warnings


"
Eval metric produces wrong result if both t and t_gt are zeros,ubc-vision/image-matching-benchmark,2022-04-19 15:10:04,0,,44,1208503525,"I understand that it is an artificial thing, which likely would never happen in practice, but it is good to know anyway.
https://github.com/ubc-vision/image-matching-benchmark/blob/master/utils/eval_helper.py#L99

Code to reproduce:
```python3 
import numpy as np
t = np.zeros((3,1))
t_gt = np.zeros((3,1))
eps = 1e-15
loss_t = np.maximum(eps, (1.0 - np.sum(t * t_gt)**2))
err_t = np.arccos(np.sqrt(1 - loss_t))
print (err_t)

```

Out[11]: 1.5707963267948966

"
Accept subgroup-based matches.h5 files,ubc-vision/image-matching-benchmark,2020-10-05 14:20:13,2,,28,714885130,"To my best knowledge, you currently only accept `matches.h5` files with keys in form of `{filename1}-{filename2}`. I would request that support for `{filename1}/{filename2}` is added, which is interpreted by HDF as [subgroups](https://docs.h5py.org/en/stable/high/group.html#creating-groups) and has the advantage that hashing is performed in stages (first `filename1` then `filename2`). This has no impact on IMW submissions which only have thousands of file pairs, but single-stage hashing starts choking at > ~300k keys. It would be a quality of life improvement, because I could release my code with a simple script which is both compatible with IMW submissions and scales to large scenes (currently it's mutually exclusive)."
[suggestion] leaderboard with limited number of keypoints,ubc-vision/image-matching-benchmark,2020-05-21 01:53:26,2,enhancement,24,622190240,"i think there should be a leaderboard with limited keypoints, such as 500 and 1000, which will be useful for selecting keypoint for realtime SLAM."
amp lib,StevenHickson/4D_Segmentation,2017-03-28 06:47:30,1,,2,217463195,"Hi Steven,

I tried to compile your code under Ubuntu, it seems that the amp library is only available on windows. Could you please suggest how to make your code working under linux environment?

Thanks,
Trung"
Is the CUDA work now?,StevenHickson/4D_Segmentation,2017-03-14 01:42:52,0,,1,213945779,"Hi man, Just find your paper this morning,
and I am very interested in your research!
How is the gpu works going?
If your Supervoxel clustering and rgb-d segmentation can be GPGPU implemented, 
the great real time rgb-d segmentation will come true !  "
段错误 (核心已转储),tfwu/RGM-AOGTracker,2019-08-20 12:31:42,0,,3,482832095,"Hello, I meet a problem as follows.
Welcome to AOGTracker (v1.0)
=============== Run benchmark evaluation ===============
=============== Processing 1 out of 108 videos ==========
[ParameterLearner]: ptPool size 4000 w.r.t. (maxNumSamples=4000, maxMemoryMB=6000.000000)
[LBFGS Optimization] Function value changing by less than optTol
段错误 (核心已转储)
What should I do?
Thanks!"
Other OS support ?,tfwu/RGM-AOGTracker,2018-12-18 06:22:58,0,,2,392013070,When will you update the project for Windows and other OS ? Or will you ?
ParetoTI don't work since updating R,vitkl/ParetoTI,2022-08-18 18:17:36,0,,14,1343444711,"Hi, 

I've been working for a long time with ParetoTI (thank you for that), but after the latest update of R I can't upload the package anymore. I get this error:

library(ParetoTI)
Error: package or namespace load failed for ‘ParetoTI’:

Tried to reinstall all the other packages needed, but nothing worked.
This is especially important as because I can't work ParetoTI I also can load the environment with all the previous results I have from using ParetoTI.

Is there going to be an update for the new R version?

Thank you very much!"
installation of package has non-zero exit status,vitkl/ParetoTI,2022-02-06 20:45:55,0,,13,1125312713,"**I followed your instructions for downloading. But I get this error. Do you know how to resolve that?**

> BiocManager::install(""vitkl/ParetoTI"", dependencies = c(""Depends"", ""Imports"", ""LinkingTo""))
'getOption(""repos"")' replaces Bioconductor standard repositories, see '?repositories' for details

replacement repositories:
    CRAN: https://cran.rstudio.com/

Bioconductor version 3.14 (BiocManager 1.30.16), R 4.1.2 (2021-11-01)
Installing github package(s) 'vitkl/ParetoTI'
Downloading GitHub repo vitkl/ParetoTI@HEAD
Installing 2 packages: edgeR, BioQC
Warning: unable to access index for repository https://bioconductor.org/packages/3.14/bioc/bin/macosx/big-sur-arm64/contrib/4.1:
  cannot open URL 'https://bioconductor.org/packages/3.14/bioc/bin/macosx/big-sur-arm64/contrib/4.1/PACKAGES'
Warning: unable to access index for repository https://bioconductor.org/packages/3.14/data/annotation/bin/macosx/big-sur-arm64/contrib/4.1:
  cannot open URL 'https://bioconductor.org/packages/3.14/data/annotation/bin/macosx/big-sur-arm64/contrib/4.1/PACKAGES'
Warning: unable to access index for repository https://bioconductor.org/packages/3.14/data/experiment/bin/macosx/big-sur-arm64/contrib/4.1:
  cannot open URL 'https://bioconductor.org/packages/3.14/data/experiment/bin/macosx/big-sur-arm64/contrib/4.1/PACKAGES'
Warning: unable to access index for repository https://bioconductor.org/packages/3.14/workflows/bin/macosx/big-sur-arm64/contrib/4.1:
  cannot open URL 'https://bioconductor.org/packages/3.14/workflows/bin/macosx/big-sur-arm64/contrib/4.1/PACKAGES'
Warning: unable to access index for repository https://bioconductor.org/packages/3.14/books/bin/macosx/big-sur-arm64/contrib/4.1:
  cannot open URL 'https://bioconductor.org/packages/3.14/books/bin/macosx/big-sur-arm64/contrib/4.1/PACKAGES'
Packages which are only available in source form, and may need compilation of C/C++/Fortran: ‘edgeR’ ‘BioQC’
Do you want to attempt to install these from sources? (Yes/no/cancel) Yes
installing the source packages ‘edgeR’, ‘BioQC’

trying URL 'https://bioconductor.org/packages/3.14/bioc/src/contrib/edgeR_3.36.0.tar.gz'
Content type 'application/x-gzip' length 1738194 bytes (1.7 MB)
==================================================
downloaded 1.7 MB

trying URL 'https://bioconductor.org/packages/3.14/bioc/src/contrib/BioQC_1.22.0.tar.gz'
Content type 'application/x-gzip' length 4505195 bytes (4.3 MB)
==================================================
downloaded 4.3 MB

* installing *source* package ‘edgeR’ ...
** using staged installation
** libs
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_add_prior_count.cpp -o R_add_prior_count.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_ave_log_cpm.cpp -o R_ave_log_cpm.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_calculate_cpm.cpp -o R_calculate_cpm.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_check_poisson_bound.cpp -o R_check_poisson_bound.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_compute_apl.cpp -o R_compute_apl.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_compute_nbdev.cpp -o R_compute_nbdev.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_exact_test_by_deviance.cpp -o R_exact_test_by_deviance.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_fit_levenberg.cpp -o R_fit_levenberg.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_fit_one_group.cpp -o R_fit_one_group.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_get_one_way_fitted.cpp -o R_get_one_way_fitted.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_initialize_levenberg.cpp -o R_initialize_levenberg.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_loess_by_col.cpp -o R_loess_by_col.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_maximize_interpolant.cpp -o R_maximize_interpolant.o
clang -arch arm64 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_process_hairpin_reads.c -o R_process_hairpin_reads.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c R_simple_good_turing.cpp -o R_simple_good_turing.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c add_prior.cpp -o add_prior.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c adj_coxreid.cpp -o adj_coxreid.o
clang -arch arm64 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c fmm_spline.c -o fmm_spline.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c glm_levenberg.cpp -o glm_levenberg.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c glm_one_group.cpp -o glm_one_group.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c init.cpp -o init.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c interpolator.cpp -o interpolator.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c nbdev.cpp -o nbdev.o
clang++ -arch arm64 -std=gnu++11 -I""/Library/Frameworks/R.framework/Resources/include"" -DNDEBUG  -I'/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/Rcpp/include' -I/opt/R/arm64/include   -fPIC  -falign-functions=64 -Wall -g -O2  -c objects.cpp -o objects.o
clang++ -arch arm64 -std=gnu++11 -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/opt/R/arm64/lib -o edgeR.so R_add_prior_count.o R_ave_log_cpm.o R_calculate_cpm.o R_check_poisson_bound.o R_compute_apl.o R_compute_nbdev.o R_exact_test_by_deviance.o R_fit_levenberg.o R_fit_one_group.o R_get_one_way_fitted.o R_initialize_levenberg.o R_loess_by_col.o R_maximize_interpolant.o R_process_hairpin_reads.o R_simple_good_turing.o add_prior.o adj_coxreid.o fmm_spline.o glm_levenberg.o glm_one_group.o init.o interpolator.o nbdev.o objects.o -L/Library/Frameworks/R.framework/Resources/lib -lRlapack -L/Library/Frameworks/R.framework/Resources/lib -lRblas -L/opt/R/arm64/gfortran/lib/gcc/aarch64-apple-darwin20.2.0/11.0.0 -L/opt/R/arm64/gfortran/lib -lgfortran -lemutls_w -lm -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
ld: warning: directory not found for option '-L/opt/R/arm64/gfortran/lib/gcc/aarch64-apple-darwin20.2.0/11.0.0'
ld: warning: directory not found for option '-L/opt/R/arm64/gfortran/lib'
ld: library not found for -lgfortran
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [edgeR.so] Error 1
ERROR: compilation failed for package ‘edgeR’
* removing ‘/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/edgeR’
ERROR: dependency ‘edgeR’ is not available for package ‘BioQC’
* removing ‘/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/BioQC’

The downloaded source packages are in
	‘/private/var/folders/m3/_dck4dnd2rq0nggxjmg7glb80000gn/T/Rtmp6y3o3Y/downloaded_packages’
Running `R CMD build`...
* checking for file ‘/private/var/folders/m3/_dck4dnd2rq0nggxjmg7glb80000gn/T/Rtmp6y3o3Y/remotes10b529932339/vitkl-ParetoTI-5109906/DESCRIPTION’ ... OK
* preparing ‘ParetoTI’:
* checking DESCRIPTION meta-information ... OK
* checking for LF line-endings in source and make files and shell scripts
* checking for empty or unneeded directories
Omitted ‘LazyData’ from DESCRIPTION
* building ‘ParetoTI_0.1.13.tar.gz’
ERROR: dependency ‘BioQC’ is not available for package ‘ParetoTI’
* removing ‘/Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library/ParetoTI’
Warning messages:
1: In i.p(...) : installation of package ‘edgeR’ had non-zero exit status
2: In i.p(...) : installation of package ‘BioQC’ had non-zero exit status
3: In i.p(...) :
  installation of package ‘/var/folders/m3/_dck4dnd2rq0nggxjmg7glb80000gn/T//Rtmp6y3o3Y/file10b539c109e3/ParetoTI_0.1.13.tar.gz’ had non-zero exit status
> library(ParetoTI)
Error in library(ParetoTI) : there is no package called ‘ParetoTI’
"
R compatible version,vitkl/ParetoTI,2021-03-09 07:46:17,0,,12,825466801,"Hi

I'm trying to install ParetoTI within a condo environment with its R installation - I don't want to use the base path R.
Could I ask which R version is compatible with ParetoTI?
Thanks,

Francesco"
Not able to install paretoTI package,vitkl/ParetoTI,2021-02-02 17:51:24,1,,11,799521142,"Hi I am new to R. Not able to install pareto TI package. Below is the error message

Downloading GitHub repo vitkl/ParetoTI@HEAD
√  checking for file 'C:\Users\desaipa\AppData\Local\Temp\Rtmp42I98k\remotes70ac3fbf6754\vitkl-ParetoTI-5109906/DESCRIPTION' ...
-  preparing 'ParetoTI': (864ms)
√  checking DESCRIPTION meta-information ... 
-  checking for LF line-endings in source and make files and shell scripts
-  checking for empty or unneeded directories
-  building 'ParetoTI_0.1.13.tar.gz'
   
* installing *source* package 'ParetoTI' ...
** using staged installation
** R
** byte-compile and prepare package for lazy loading
Error: (converted from warning) package 'reticulate' was built under R version 4.0.3
Execution halted
ERROR: lazy loading failed for package 'ParetoTI'
* removing 'C:/Users/desaipa/Documents/R/win-library/4.0/ParetoTI'
Error: Failed to install 'ParetoTI' from GitHub:
  (converted from warning) installation of package ‘C:/Users/desaipa/AppData/Local/Temp/Rtmp42I98k/file70ac6bfc308/ParetoTI_0.1.13.tar.gz’ had non-zero exit status"
pch_fit_list[[1]] : subscript out of bounds,vitkl/ParetoTI,2020-07-06 08:50:51,1,,9,651363009,"I am trying out the package and running into some trouble. After setting up as described in readme(and making sure the right condaenv is loaded), I am trying to run the `fit_pch_bootstrap` function and getting the error in the title. 

I'm am running the code as follows:

    library(ParetoTI)
    reticulate::use_condaenv(""reticulate_PCHA"", conda = ""auto"", required = TRUE)
    arc = fit_pch_bootstrap(t(obj@reductions$pca@cell.embeddings), n = 200, sample_prop = 0.75, seed = 235,
                                             noc = 4, delta = 0, conv_crit = 1e-04, type = ""s"")

I saw this happened also in a previous issue (https://github.com/vitkl/ParetoTI/issues/4) but there isn't a solution for this problem there (I am already running with `type=""s""`)"
simplified tutorial notebook,vitkl/ParetoTI,2020-02-24 21:41:55,1,,6,570164761,"I tried the hepatocyte notebook and got an error in the first chunk related to singleCellExperiment
```
> hepatocytes = SingleCellExperiment(assays = list(counts = data),+ colData = design)
Error in `rownames<-`(`*tmp*`, value = .get_colnames_from_assays(assays)) :   invalid rownames length
```
Would it be possible for you to simplify this example and start with the pcs4arch data?"
请问关于论文中实验的相关部分,rookiepig/CrossScaleStereo,2022-05-06 14:40:05,0,,13,1227955054,"您好，请问您的论文里测试的M27的数据集使用的是原始分辨率的图像还是3倍下采样后的图像呢？看到您的论文里只有说新出的Middlebury 2014使用的是四分一分辨率的图像（这个可能是您的那篇TCSVT）。
还想请教您一个问题，就是我在利用左右视差真值计算被遮挡区域时，利用一致性计算（数据集给的SDK程序），但是通常得到的被遮挡图的左侧原本应该是被遮挡区域，而我计算得到的有的不是被遮挡，这个问题请问您当时是如何解决的？也就是左图的被遮挡区域应该包括左图像的最左侧几列的区域，而利用一致性计算时有时候那部分并不是遮挡区域。（我思考可能是因为那部分都是和相机平行的平面，视差与未被遮挡的部分相同，导致阈值选择为1时无法筛掉）主要针对Middlebury 2005及2006没有给出遮挡区域的图，故而想问问您！谢谢"
Does this algorithm works on unrectified images?,rookiepig/CrossScaleStereo,2021-06-08 15:27:04,2,,12,915175095,"Hi, 

I was wondering if it was possible to use this algorithm on unrectified images.

Thank you!"
the identifier “m_mst_value_sum_aggregated_from_child_to_parent” could not be found?,rookiepig/CrossScaleStereo,2020-05-20 06:59:02,0,,11,621510633,"When I tried to integrate the SSCA algorithm into my project, I encounterd this problem. The Chinese characters of the error information on this picture represents that the compiler could not find the identifier about ""m_mst_value_sum_aggregated_from_child_to_parent"", ""m_mst_value_sum_aggregated_from_parent_to_child"" and ""m_mst_value_to_be_filtered"".  My running environment is as follows:
windows10, 
Visual Studio 2019(community),
opencv 4.3.0.
Is there anyone who can help me with this problem? Thanks!

——
![error](https://user-images.githubusercontent.com/39852388/82414977-6e11ce80-9aaa-11ea-8cd6-1762950343a4.png)

"
about cost function:cost[ x ] = 2 - exp( - tmpCost / 35 ) - exp( - 255 * cost[ x ] / 15 );,rookiepig/CrossScaleStereo,2019-07-02 03:15:19,0,,10,462999131,"Hi,I want to know the meaning of the cost function.It is hard to understand for me.
Where is this come from?It looks not the same as :
cost=(1-lamda)*costA+lamda*costB
Thanks~"
"What if the x_min, y_min(or x_max, y_max) goes beyond (0, 1) when decoding gxgy to xy?",sgrvinod/a-pytorch-tutorial-to-object-detection,2022-10-16 13:57:30,0,,89,1410497209,Should we do  value clipping in the `gxgy_to_cxcy` function of the utils.py?
Pre-trained model is not available,sgrvinod/a-pytorch-tutorial-to-object-detection,2022-06-17 21:25:36,0,,88,1275497632,"Hi, I downloaded your pre-trained model from google drive. But it gives an error when loading the checkpoint. 

----> 1 ckp = torch.load('./checkpoint_ssd300.pth.tar')

~/anaconda3/envs/PyTorch/lib/python3.8/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    591                     return torch.jit.load(opened_file)
    592                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
--> 593         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
    594 
    595 

~/anaconda3/envs/PyTorch/lib/python3.8/site-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
    770     unpickler = pickle_module.Unpickler(f, **pickle_load_args)
    771     unpickler.persistent_load = persistent_load
--> 772     result = unpickler.load()
    773 
    774     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)

ModuleNotFoundError: No module named 'model'"
"PredictionConvolutions separates locs and classes, doesn't follow the paper?",sgrvinod/a-pytorch-tutorial-to-object-detection,2022-05-06 16:54:01,0,,87,1228088246,"I noticed in the paper each Prediction Convolution is formulated with output channels = n_boxes * (n_classes + 4), but in the [code](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/43fd8be9e82b351619a467373d211ee5bf73cef8/model.py#L218) you have separated each level into separate convolutions. 

```python
        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)
        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)
        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)
        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)
        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)
        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)

        # Class prediction convolutions (predict classes in localization boxes)
        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)
        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)
        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)
        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)
        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)
        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)...
```

But, I believe if it were implemented as in the paper, it should be:

```python
        self.conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * (4 + n_classes), kernel_size=3, padding=1)
        self.conv7 = nn.Conv2d(1024, n_boxes['conv7'] * (4 + n_classes), kernel_size=3, padding=1)
        self.conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * (4 + n_classes), kernel_size=3, padding=1)
        self.conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * (4 + n_classes), kernel_size=3, padding=1)
        self.conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * (4 + n_classes), kernel_size=3, padding=1)
        self.conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * (4 + n_classes), kernel_size=3, padding=1)
```

Did you try it the original way, or was this an intentional choice for some reason?

Thank you!
"
classification layers arent training,sgrvinod/a-pytorch-tutorial-to-object-detection,2022-03-29 06:05:57,0,,86,1184350845,Good morning! I am using SSD architecture to detect faces on images (2 classes - background and face). I have a labeled WIDERFACES dataset and Im trying to train a model on it. However on each epoch after using softmax each box has 50-50 confidence which means classification layer is not training .What should I do?
broken .tar file with pretrained model,sgrvinod/a-pytorch-tutorial-to-object-detection,2022-03-14 10:11:17,0,,85,1168154190,"Could you please update your pth.tar file with pretrained model - it is broken and i cannot unzip it.
Thank you!"
MultiBox Los Calculation,sgrvinod/a-pytorch-tutorial-to-object-detection,2022-03-10 03:18:18,0,,84,1164693621,"Hi,

 In the multi-box loss calculation, in the parameter **predicted_locs** what does N signify? Is it the batch size or the number of objects predicted by the network in the particular image? 

param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)

Also I could see batch size derived here : batch_size = predicted_locs.size(0)  . It would be helpful if any inputs on this. 

Thanks and Regards
Niranjan ravi.
"
Possible bug in the trained checkpoint?,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-12-27 00:07:17,0,,83,1088869304,"I downloaded the `checkpoint_ssd300.pth.tar` and ran the `detect.py` using  <img src=""https://github.com/ashBabu/Utilities/blob/master/test_img_/test.jpg"" alt=""image_test"" />
and I got the detection as <img src=""https://github.com/ashBabu/Utilities/blob/master/test_img_/Screenshot%20from%202021-12-26%2023-45-52.png"" alt=""image_result"" />

Any possible solutions? Thanks"
How to use on custom dataset,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-12-25 21:15:52,2,,82,1088656948,I am trying to use my own dataset to train SSD300 but I can't find the way to use it and the correct format. Could you please guide me on this?
gender classification,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-10-07 08:24:32,0,,81,1019759943,"wanted to know if we can detect or classify full person gender classification in the same model can you help with it 
or can you suggest better training model for detecting gender if it is male or female with training model."
"Accuracy, precision, recall ,TF,FP,FN,TN",sgrvinod/a-pytorch-tutorial-to-object-detection,2021-10-07 08:22:37,0,,80,1019758138,"need help for calculating these in eval.py 
can you guide how to calculate it for this model"
Predict box shape directly instead of offsets?,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-07-24 01:52:47,0,,79,951978019,"More of a question than an issue really. I was curious - if I'm understanding correctly the network will predict offsets for each anchor box, which in turn will describe a bounding box. This requires lots of conversions (cxcy to xy, encoding, decoding), so would it not be possible to simply train the network to output as [xmin, ymin, xmax, ymax] instead of [offset-x, offset-y, width, height]? If not, what are the issues with this?

In the same vein, is the encoding and decoding of the bounding box only necessary because we need to go from offsets -> bounding box described by offsets?"
"Error when all objects of an image are marked as ""difficult""",sgrvinod/a-pytorch-tutorial-to-object-detection,2021-06-15 14:14:19,2,,78,921446432,"Hi!
Im training this SSD with another dataset and i found that when all objects in an image are marked as difficult and the flag 
keep_difficult is set to False , it throws an error in Datasets.py after eliminating those objetcts from the tensors and try to apply transforms to empty tensors []. Lines 45 and following
"
how to extend this network to SSD 512,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-05-21 06:44:22,0,,77,897737330,
Operation does not have an identity,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-05-18 07:21:21,0,,76,894061414,"In utils.py, 
**overlap.max().item() < min_overlap**"
detection problem,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-04-18 11:30:44,1,,75,860640697,"hello,
while detection, i am getting a error 
NameError: name 'cxcy_to_xy' is not defined
how do i solve this?"
Question on checkpoint,sgrvinod/a-pytorch-tutorial-to-object-detection,2021-01-07 21:32:11,0,,74,781624573,"Hi,
      Thank you for a great repository. I have a couple of questions. I am trying to run the train.py from scratch. At the end of the training, I get a  checkpoint file. When I try to run the detect.py, I could see the checkpoint belongs to the last epoch? Is it a good approach to save the checkpoint file after each epoch? 
 
Also, while training I could see the loss for each epoch. But I would like to visualize the accuracy as well? An accuracy function is defined in utils.py! Can I use it?

I am trying to visualize the results which are happening to get a better understanding. 
Any guidance/inputs will be helpful. 

Thank you
"
Matching predictions to ground truths ,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-12-08 10:37:19,0,,73,759332601,I read the concept of **Matching predictions to ground truths**. You mentioned to check jaccard overlap for the matching. Did you follow any other matching criterion?
What could be causing the AP to come out negative? ,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-12-06 21:51:46,0,,72,758031936,"Hi there, I am training SSD for the first time using an XView dataset but during the evaluation step, the class average precision comes out as negative? Here's what it shows for a couple of the classes: 

AP for Fixed-wing Aircraft = -1.0000
AP for Small Aircraft = -1.0000
AP for Car = -1.0000

The mAP is 77.2. I am not sure why the AP is showing as a negative? I was expecting it to be in the range of [0,1] but maybe I am misunderstanding something. Any tips would be helpful, thank you!"
font error running detect.py,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-12-03 11:21:22,3,,71,756107064,"Traceback (most recent call last):
  File ""detect.py"", line 102, in <module>
    detect(original_image, min_score=0.2, max_overlap=0.5, top_k=200).show()
  File ""detect.py"", line 67, in detect
    **`_font = ImageFont.truetype(""./calibril.ttf"", 15)_`**
  File ""/home/rahim/anaconda3/lib/python3.7/site-packages/PIL/ImageFont.py"", line 642, in truetype
    return freetype(font)
  File ""/home/rahim/anaconda3/lib/python3.7/site-packages/PIL/ImageFont.py"", line 639, in freetype
    return FreeTypeFont(font, size, index, encoding, layout_engine)
  File ""/home/rahim/anaconda3/lib/python3.7/site-packages/PIL/ImageFont.py"", line 188, in __init__
    font, size, index, encoding, layout_engine=layout_engine
OSError: cannot open resource"
cannot open resource on detect.py,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-12-03 08:05:43,1,,70,755965941,"hi! Thank you for this SSD code, quite touching :1st_place_medal: 
I face with strange error running detect.py, can anyone help me to solve it !?
thank you.
 
Traceback (most recent call last):
  File ""detect.py"", line 102, in <module>
    detect(original_image, min_score=0.2, max_overlap=0.5, top_k=200).show()
  File ""detect.py"", line 67, in detect
    font = ImageFont.truetype(""./calibril.ttf"", 15)
  File ""/home/rahim/anaconda3/lib/python3.7/site-packages/PIL/ImageFont.py"", line 642, in truetype
    return freetype(font)
  File ""/home/rahim/anaconda3/lib/python3.7/site-packages/PIL/ImageFont.py"", line 639, in freetype
    return FreeTypeFont(font, size, index, encoding, layout_engine)
  File ""/home/rahim/anaconda3/lib/python3.7/site-packages/PIL/ImageFont.py"", line 188, in __init__
    font, size, index, encoding, layout_engine=layout_engine
OSError: cannot open resource"
eval.py error,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-09-23 12:53:29,0,,68,707340822,"Traceback (most recent call last):
  File ""eval.py"", line 93, in <module>
    evaluate(test_loader, model)
  File ""eval.py"", line 68, in evaluate
    top_k=200)
  File ""SSD-pytorch/model.py"", line 515, in detect_objects
    image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))
RuntimeError: CUDA error: device-side assert triggered"
Expected object of scalar type unsigned char but got scalar type bool for argument 'other'.,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-08-27 08:32:45,2,,66,687052720,"I ran the detect.py file and load the checkpoint but got the error: Expected object of scalar type unsigned char but got scalar type bool for argument 'other'.
`Traceback (most recent call last):

  File ""H:/SSD_object_detection/detect.py"", line 103, in <module>
    detect(original_image, min_score=0.1, max_overlap=0.4, top_k=150).show()

  File ""H:/SSD_object_detection/detect.py"", line 46, in detect
    max_overlap=max_overlap, top_k=top_k)

  File ""H:\SSD_object_detection\model.py"", line 494, in detect_objects
    suppress = torch.max(suppress, overlap[box] > max_overlap)

RuntimeError: Expected object of scalar type unsigned char but got scalar type bool for argument 'other'`
Could any one help to resolve this error?"
why the need for conf_loss_all.clone() ?,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-07-19 21:47:01,0,,63,661090371,"https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/783b39df5c69ce5b5c7fa1f4b70c68d368d90e68/model.py#L637

Why is there a need to do `.clone()` here? cant we directly use `conf_loss_all` as it is ?"
I made my own pytorch object detection with my own datasets but map is not increasing ,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-07-10 13:36:54,0,,62,654789051,"can you guys look through my pytorch code and figure out which part did I do wrong? 

https://github.com/SlowMonk/single_od_pytorch

simple one object detection with pytorch


[B]Epoch[0] loss[0.4150061990533556] MAP:[0.0%]
Train loss decreased (0.900000 --> 0.415006).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[1] loss[0.3658331222832203] MAP:[0.10714285714285714%]
Train loss decreased (0.415006 --> 0.365833).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[2] loss[0.3304239908854167] MAP:[0.11904761904761904%]
Train loss decreased (0.365833 --> 0.330424).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[3] loss[0.3068529662809202] MAP:[0.11111111111111109%]
Train loss decreased (0.330424 --> 0.306853).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[4] loss[0.2905081467969077] MAP:[0.11111111111111109%]
Train loss decreased (0.306853 --> 0.290508).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[5] loss[0.27660175262107733] MAP:[0.11111111111111109%]
Train loss decreased (0.290508 --> 0.276602).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[6] loss[0.26729247156454594] MAP:[0.11904761904761904%]
Train loss decreased (0.276602 --> 0.267292).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[7] loss[0.262052005755582] MAP:[0.11904761904761904%]
Train loss decreased (0.267292 --> 0.262052).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[8] loss[0.2574362024546616] MAP:[0.11904761904761904%]
Train loss decreased (0.262052 --> 0.257436).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[9] loss[0.25671960690191814] MAP:[0.11904761904761904%]
Train loss decreased (0.257436 --> 0.256720).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[10] loss[0.2525056573477658] MAP:[0.11904761904761904%]
Train loss decreased (0.256720 --> 0.252506).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[11] loss[0.2502458925092859] MAP:[0.11904761904761904%]
Train loss decreased (0.252506 --> 0.250246).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[12] loss[0.24753405503764914] MAP:[0.11904761904761904%]
Train loss decreased (0.250246 --> 0.247534).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[13] loss[0.2460326980229239] MAP:[0.11904761904761904%]
Train loss decreased (0.247534 --> 0.246033).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[14] loss[0.24430175177043392] MAP:[0.11904761904761904%]
Train loss decreased (0.246033 --> 0.244302).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[15] loss[0.24367255065590143] MAP:[0.11904761904761904%]
Train loss decreased (0.244302 --> 0.243673).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[16] loss[0.24275059456459613] MAP:[0.11904761904761904%]
Train loss decreased (0.243673 --> 0.242751).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
[A]Epoch[17] loss[0.2418948710851726] MAP:[0.11904761904761904%]
Train loss decreased (0.242751 --> 0.241895).  Saving model ...to /home/jake/PycharmProjects/balloon_detection/weights/best_fasterrcnn_resnet50_fpn.pth
"
eval bug,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-06-29 15:02:23,0,,61,647450507,"Evaluating: 100%|██████████| 78/78 [28:25<00:00, 21.87s/it]
Traceback (most recent call last):
  File ""F:/task/a-PyTorch-Tutorial-to-Object-Detection-master - a/eval.py"", line 88, in <module>
    evaluate(test_loader, model)
  File ""F:/task/a-PyTorch-Tutorial-to-Object-Detection-master - a/eval.py"", line 79, in evaluate
    APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)
  File ""F:\task\a-PyTorch-Tutorial-to-Object-Detection-master - a\utils.py"", line 202, in calculate_mAP
    det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)
IndexError: The shape of the mask [917585] at index 0 does not match the shape of the indexed tensor [954568, 4] at index 0"
Graphs,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-06-16 08:28:10,0,,59,639474722,"No graphics appear when I open the tensorboard. How can I see loss, IoU, precission-recall ect. graphs ? "
Negative data training,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-06-05 15:30:06,0,,58,631692031,"Hello, great explanation there. Can you please provide how to modify the training code ( data fetech and multiloss function ) to pass negative images without/with bounding boxes"
eval.py bugs,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-06-04 16:30:38,4,,57,630978444,"Hi, i am facing the problem with the evaluation step to get the mAP scores. The detail of error:
File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\utils\data\dataloader.py"", line 345, in __next__
    data = self._next_data()
  File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\utils\data\dataloader.py"", line 856, in _next_data
    return self._process_data(data)
  File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\utils\data\dataloader.py"", line 881, in _process_data
    data.reraise()
  File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\_utils.py"", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\utils\data\_utils\worker.py"", line 178, in _worker_loop
    data = fetcher.fetch(index)
  File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""C:\Users\ADMIN\anaconda3\envs\Pytorch\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "".\a-PyTorch-Tutorial-to-Object-Detection\datasets.py"", line 56, in __getitem__
    image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split)
  File "".\a-PyTorch-Tutorial-to-Object-Detection\utils.py"", line 705, in transform
    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))
  File "".\a-PyTorch-Tutorial-to-Object-Detection\utils.py"", line 616, in resize
    new_boxes = boxes / old_dims  # percent coordinates
RuntimeError: The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 1 

Are there any idea? Someone said that this is the bug of torch library, but i am not sure. The training and detect work well. "
How to run it on a video path?,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-06-04 06:14:15,1,,56,630540721,"Hi.. it would be helpful if you give me a script for running the program, given a video path as well as through webcam."
about calculate_mAP,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-06-02 02:56:37,0,,55,628861384,"`calculate_mAP` is wrong with some special cases.
So I would like to suggest to add code a bit to `calculate_mAP` class.


For example, when running this code
```python
det_boxes = [torch.tensor([[ 50.8456,  11.0575, 497.9483, 319.0857]])]
det_labels = [torch.tensor([1])]
det_scores = [torch.tensor([1])]
true_boxes = [torch.tensor([[ 67.9887, 155.5200, 276.2039, 240.4080],
                                      [ 11.3314,   7.7760, 498.5836, 322.7040]])]
true_labels = [torch.tensor([5, 1])]
true_difficulties = [torch.tensor([0,0])]

calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)
```
, we get output like below
```
({'person': 1.0,
  'bird': 0.0,
  'cat': 0.0,
  'cow': 0.0,
  'dog': 0.0,
  'horse': 0.0,
  'sheep': 0.0,
  'aeroplane': 0.0,
  'bicycle': 0.0,
  'boat': 0.0,
  'bus': 0.0,
  'car': 0.0,
  'motorbike': 0.0,
  'train': 0.0,
  'bottle': 0.0,
  'chair': 0.0,
  'diningtable': 0.0,
  'pottedplant': 0.0,
  'sofa': 0.0,
  'tvmonitor': 0.0},
 0.05000000074505806)
```
mAP is 0.05 here, but I think we should get mAP `0.5`.

I would like to suggest to add this code 
```python
true_labels = torch.tensor(true_labels, dtype=torch.long)
average_precisions = average_precisions[true_labels.unique()-1]
```
at 
https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py#L271
like
```python
# Calculate Mean Average Precision (mAP)
true_labels = torch.tensor(true_labels, dtype=torch.long)
average_precisions = average_precisions[true_labels.unique()-1]
mean_average_precision = average_precisions.mean().item()
```

Then the output is
`({'aeroplane': 1.0, 'bicycle': 0.0}, 0.5)`

This code works flexible like this when we want to try small dataset."
cxcy_to_gcxgcy function return nan values,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-05-23 02:32:26,6,,54,623562872,"I traced back where the nan is coming from the loss function. It leads to `cxcy_to_gcxgcy` function.
Division is causing nan value it seems.  please provide any solution for this. "
Batch size in train.py,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-05-18 19:47:14,1,,52,620462133,"Hello, first of all I want to say this is the most useful tutorial about object detection using SSD frameworks i've red so far. 
In line 20 of train.py a batch_size =8 is defined and then it is  passed to the DataLoader in line 71. But, later, in lines  78 and 79, there is a batch size of 32 hardcoded in the formula. Shouldn't be used here the variable batch_size or am I missing something? 
Thanks in advance!

"
What does decimate do in utils.py?,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-05-16 01:08:22,0,,51,619345675,Please explain the functinality of the **decimate** function in **utils.py**
detect.py can not display the picture,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-05-14 06:42:36,1,,50,617967433,"My local is window10, when I run the detect.py file on the server side, the picture cannot be displayed, I do n’t know where to modify, 
when i run [ sudo apt-get install imagemagick ] , the wrong is changed :
""display: unable to open X server `' @ error/display.c/DisplayImageCommand/426.""

if you know, please tell me, thanks"
Wrong scale values,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-04-17 09:55:24,5,,48,601849162,"These are the values used in 
```
obj_scales = {'conv4_3': 0.1,
                      'conv7': 0.2,
                      'conv8_2': 0.375,
                      'conv9_2': 0.55,
                      'conv10_2': 0.725,
                      'conv11_2': 0.9}
```
In paper this values are used:
![image](https://user-images.githubusercontent.com/47481781/79556866-5067d880-80aa-11ea-9a50-2418d93d1f86.png)
Proper values are:
0.2 , 0.34, 0.48, 0.62, 0.76, 0.9
Code to check:
```
m = 6
k = np.arange(1, m+1, 1)
scales = 0.2 + (0.9 - 0.2) / (m - 1) * (k - 1)
```"
VOC dataset link is invalid,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-04-12 14:14:31,1,,47,598494395,"![image](https://user-images.githubusercontent.com/32434681/79070851-dc74ab80-7d0a-11ea-8806-9fafc0786d7f.png)
I try to click the dataset superlink. However, these three links seem invalid for now. 
Could you please fix it up?"
MultiBox loss goes to infinity,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-03-17 09:48:43,5,,43,582886348,"Hi,
I hope you can help me to solve the following issue.
While training, in the first steps (first half of the batch) error was being computed well, but then suddenly the MultiBox error turned to infinity (I get inf)
Does anyone know what is the problem?
Thank you so much"
VGG backbone is not freezed for training,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-02-28 12:08:47,0,,41,572716758,"hi

Why did you not freeze the layers of VGG backbone before training or Did you leave them on purpose ?? "
"when traing, the loss becomes nan. why?",sgrvinod/a-pytorch-tutorial-to-object-detection,2020-01-26 08:14:10,2,,38,555194871,"![image](https://user-images.githubusercontent.com/43036573/73132481-def22e00-4056-11ea-8d21-a369b0e7b929.png)
"
backprop through loss,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-01-14 10:30:12,0,,37,549473417,"Hello. My apologies for such a (probably) stupid question, but I can not understand how does backpropagation works through the label matching operation via Jaccard index comparison. Can you, please, help me to find an explanation?
Thanks in advance."
AttributeError: 'DataParallel' object has no attribute 'detect_objects',sgrvinod/a-pytorch-tutorial-to-object-detection,2020-01-09 04:26:12,2,,36,547240384,"Thank you for your SSD , And when I try to run detect.py ,I got a error:
`AttributeError: 'DataParallel' object has no attribute 'detect_objects'`

Can you tell me how to solve it?"
Calculate jaccard overlap for two same sets in model.py?,sgrvinod/a-pytorch-tutorial-to-object-detection,2020-01-06 06:23:48,1,,35,545562521,"In model.py, Line 478

`overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score) `

Isn’t this two same sets? I think the purpose of the function is to calculate overlap for two different 
sets with shape (n_qualified, 4) and (n_min_score, 4)?
Am I missing something here?"
Why subtract 1 in flip?,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-12-04 08:40:29,1,,32,532530243,"Why 1 is subtract in adjusting the boxes coordinates after horizontal flipping?What is 1 mean? I thought this already satisfied without subtracting 1.Am i wrong?
The code is....
    new_boxes[:, 0] = image.width - boxes[:, 0] - 1
    new_boxes[:, 2] = image.width - boxes[:, 2] - 1"
how to change the backbone from VGG16 to Resnet?,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-11-23 02:53:15,1,,30,527499457,I do wish your reply
L1Loss or SmoothL1Loss?,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-11-18 08:52:22,0,,27,524221350,"Hi,
Thanks for your SSD tutorial, this is the most detailed SSD implementation as far as I know.
However, I just noticed you used L1Loss instead of SmoothL1Loss, is there any special reason?
Cheers,
P"
RuntimeError: expected device cpu but got device cuda:0 when I run train.py,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-11-04 16:23:51,2,,26,517260829,"Hi everyone !!!

I'm trying to run this code to train on my own dataset but I'm having this issue.

Here is my problem : 
Traceback (most recent call last):
  File ""train.py"", line 232, in <module>
    main()
  File ""train.py"", line 101, in main
    epoch=epoch)
  File ""train.py"", line 153, in train
    loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/content/drive/My Drive/Model/SSD/model.py"", line 593, in forward
    true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)
  File ""/content/drive/My Drive/Model/SSD/utils.py"", line 312, in cxcy_to_gcxgcy
    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y
RuntimeError: expected device cuda:0 but got device cpu



So I supposed one of the two elements (cxcy or priors_cxcy) is not passed in cuda. I looked in the code for those elements but it seems that every elements are using .cuda().

If someone already has had this problem please help me. Any thoughts are welcome !"
"Could u help me ?when I load the checkpoints, there is a problem... ",sgrvinod/a-pytorch-tutorial-to-object-detection,2019-10-24 08:56:28,1,,25,511804073,"When I use torch.load to load a model, there exist a problem as follows:
![image](https://user-images.githubusercontent.com/45613228/67469675-1b407780-f67f-11e9-85ab-97e6706c597b.png)
I don't know what happened , I just download the checkpoint that you uploaded.
Could u help me to solve this problem? 
Thank u so much .... "
 Expected object of scalar type Byte but got scalar type Bool for argument #2 'other',sgrvinod/a-pytorch-tutorial-to-object-detection,2019-10-01 13:07:55,4,,24,500883939,"@sgrvinod 
I tried to run the code as instructed in the discussion. The training part is completed but, during inference, it is raising below error. I didn't change anything in the code. Just executing as:

**python detect.py**

 Does anyone have any suggestion, I am using torchvision 0.4.0

This particular line (model.py file) is raising an error.

**suppress = torch.max(suppress, overlap[box] > max_overlap)**

![error](https://user-images.githubusercontent.com/35481426/65964241-22a6a380-e47a-11e9-8600-3e0d2e99a271.png)


 **Expected object of scalar type Byte but got scalar type Bool for argument #2 'other'**"
could you check this point?,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-09-03 16:29:06,0,,22,488705955,"![image](https://user-images.githubusercontent.com/24381042/64191383-51e5e700-ceb3-11e9-8dd5-c7fb430c9106.png)


i think second blue box is conv7.

is it right?

thank you!"
Not able to iterate voc07_path ids,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-07-16 18:14:38,0,,20,468794982,"In line 71, When I iterate I only see the ids of voc12_path. voc07_path ids are not in the list (ids)"
Loss calculations,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-07-11 15:16:39,0,,18,466955951,"Dear Sir, I am a bit confused about the loss calculations. These two snippets from the train log would help me better explain the confusion.

> Epoch: [7][0/250]	Batch Time 1.087 (1.087)	Data Time 0.641 (0.641)	Loss 4.4972 (4.4972)	
Epoch: [7][200/250]	Batch Time 0.347 (0.349)	Data Time 0.000 (0.003)	Loss 4.4606 (4.2053)	
[0/313]	Batch Time 0.727 (0.727)	Loss 4.6890 (4.6890)	
[200/313]	Batch Time 0.137 (0.140)	Loss 5.0137 (5.0349)	
 > * LOSS - 5.032

> Epoch: [138][0/250]	Batch Time 0.931 (0.931)	Data Time 0.560 (0.560)	Loss 0.1863 (0.1863)	
Epoch: [138][200/250]	Batch Time 0.344 (0.349)	Data Time 0.000 (0.003)	Loss 0.1613 (0.1790)	
[0/313]	Batch Time 0.697 (0.697)	Loss 10.3283 (10.3283)	
[200/313]	Batch Time 0.137 (0.141)	Loss 11.1063 (10.4203)	
>  * LOSS - 10.459

On epoch 7, the loss shown besides the 'Data Time' is about 4, while the average loss is about 5.
On epoch 138, the loss shown besides the 'Data Time' is about 0.2, while the average loss is about 10.
Looking at the loss shown besides 'Data Time', I see that my model is learning well, but looking at the average loss, it seems that the model is diverging.
Can you please guide?
Thanks"
IndexError: too many indices for tensor of dimension 1,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-06-28 06:18:01,4,,16,461875360,"  File ""/home/arsene/Project/mini-project2/Projekt/utils.py"", line 350, in find_intersection
    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)
IndexError: too many indices for tensor of dimension 1.

This happens when I try to load my own but with the same data sets format

"
RuntimeError: cuda runtime error (11) : invalid argument at /pytorch/aten/src/THC/THCGeneral.cpp:405,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-04-16 06:09:54,0,,13,433599356,"When I ran 
`python3 train`

I got the errors below:
`Loaded base model.

/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument
Traceback (most recent call last):
  File ""train.py"", line 234, in <module>
    main()
  File ""train.py"", line 101, in main
    epoch=epoch)
  File ""train.py"", line 151, in train
    predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/media/gaoya/disk/Downloads/a-PyTorch-Tutorial-to-Object-Detection-master/model.py"", line 353, in forward
    conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/media/gaoya/disk/Downloads/a-PyTorch-Tutorial-to-Object-Detection-master/model.py"", line 58, in forward
    out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py"", line 320, in forward
    self.padding, self.dilation, self.groups)
**RuntimeError: cuda runtime error (11) : invalid argument at /pytorch/aten/src/THC/THCGeneral.cpp:405**
`

CUDA:9.0
pytorch:1.0.1
OS: Ubuntu18.04

I can use pytorch with GPU in other simple pytorch examples.What's wrong with this?"
val and test data,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-04-11 21:34:03,1,,12,432262274,"Hi, 
Thanks for this code. I just found it will be overfitting if the val and test are the same data. I ran 200 epochs and the best epoch is num_146. Then I use the best epoch's ckpt to evaluate and I found the mAP is very close to yours in the instruction. The point is since I am using the same data for testing and validation, the mAP is not accurate since it might be overfitting.

Thanks!"
ModuleNotFoundError: No module named 'datasets',sgrvinod/a-pytorch-tutorial-to-object-detection,2019-03-25 05:30:46,1,,10,424729940,"OS:Win10 ,64bit
Python:3.6.6
Pytorch:1.0.1

I have created the PASCAL VOC 2007 and 2012 JSON file list. When I ran train.py, I got the errors below:
`
553433881it [19:06, 482647.72it/s]

Loaded base model.

C:\Applications\WPy-3661\python-3.6.6.amd64\lib\site-packages\torch\nn\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\spawn.py"", line 115, in _main
    self = reduction.pickle.load(from_parent)
ModuleNotFoundError: No module named 'datasets'
Traceback (most recent call last):
  File ""D:\Files\python\MachineLearning\pytorch\a-PyTorch-Tutorial-to-Object-Detection-master\train.py"", line 234, in <module>
    main()
  File ""D:\Files\python\MachineLearning\pytorch\a-PyTorch-Tutorial-to-Object-Detection-master\train.py"", line 101, in main
    epoch=epoch)
  File ""D:\Files\python\MachineLearning\pytorch\a-PyTorch-Tutorial-to-Object-Detection-master\train.py"", line 142, in train
    for i, (images, boxes, labels, _) in enumerate(train_loader):
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\site-packages\torch\utils\data\dataloader.py"", line 819, in __iter__
    return _DataLoaderIter(self)
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\site-packages\torch\utils\data\dataloader.py"", line 560, in __init__
    w.start()
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Applications\WPy-3661\python-3.6.6.amd64\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
`
Can anyone figure out what's wrong?"
Not able to display the detected image after executing detect.py ,sgrvinod/a-pytorch-tutorial-to-object-detection,2019-03-14 09:44:22,3,,9,420918232,
variable 'net' in function IRCNN_base replaced ,Insiyaa/IRCNN-keras,2019-08-02 16:57:11,0,,1,476272140,"In the function IRCNN_base(Input), the variable 'net' may not work as expected. 
When running the line 'net = IRCNN_block(input)', the program will clear the content of 'net' and give it a new value. As a result, the upper part of the function is useless and the function is equal to the following code:

def IRCNN_base(input):
  if K.image_data_format() == 'channels_first':
    channel_axis = 1
  else:
    channel_axis = -1
                 
  net = IRCNN_block(input)
                 
  net = conv2d_bn(net, 32, 3, 3, strides=(2,2), padding='valid')
  net = GlobalAveragePooling2D()(net)
  net = Dropout(0.5)(net)
  return net

Is that right or could you please make it clear?

Thanks."
Resize not working in mobile during inference,pytorch/vision,2022-11-08 11:13:32,4,,6928,1439985404,"Hi, I've trained a model that uses the Resize function, and it trains fine on my M1 laptop. However, when I modify the model from `pt` to `ptl` file and do inference on react native similar to the work here: https://github.com/pytorch/ios-demo-app/tree/master/StreamingASR 

I get the following error on the iOS side:
<img width=""941"" alt=""Screenshot 2022-11-08 at 6 59 43 PM"" src=""https://user-images.githubusercontent.com/9785146/200547580-f1796675-00e0-4d98-9d9e-4836e0455884.png"">


To give some context:
The input to the Resize function is of size `[128, 29479]` and the resize function is supposed to resize it to `[128, 150*60]` aka `[128, 15000]` but the `Resize` function is interpreting the input `x1` size to be `[29479]` instead of the supposed `[128, 29479]` that was logged. 


Minimal example of model:
```
import torchvision
import torchvision.models as models


resize_to = (128, 150*60)
num_classes = 5

class ResNet(nn.Module):
	def __init__(self, dataset, pretrained=True):
		super(ResNet, self).__init__()
                self.resize = torchvision.transforms.Resize(resize_to)
		self.model = models.resnet50(pretrained=pretrained)
		self.model.fc = nn.Linear(2048, num_classes)

    def forward(self, x1):
        print(""x1 log"", x1.size())
        x1 = self.resize(x1)
        output = self.model(x1)
        return output

```

Script used to convert from pt to ptl file:
```
import torch
import MyModelNet
from torch.utils.mobile_optimizer import optimize_for_mobile

model_filepath = ""model.pt""
mobile_model_filepath = ""mobile_model.ptl""

# https://stackoverflow.com/questions/53907073/problem-with-missing-and-unexpected-keys-while-loading-my-model-in-pytorch
# https://stackoverflow.com/questions/55047065/unexpected-keys-in-state-dict-model-opt
model = MyModelNet()
state_dict = torch.load(model_filepath)['model']
model.load_state_dict(state_dict)

model.eval()

scripted_module = torch.jit.script(model)

optimized_model = optimize_for_mobile(scripted_module)
optimized_model._save_for_lite_interpreter(mobile_model_filepath)
print(""Done _save_for_lite_interpreter"")

```

Happy to provide more details if the above is not enough! 🙏 


Version:
```
--2022-11-08 19:07:26--  https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 17278 (17K) [text/plain]
Saving to: ‘collect_env.py.1’

collect_env.   0%       0  --.-KB/s           collect_env. 100%  16.87K  --.-KB/s    in 0.008s  

2022-11-08 19:07:27 (1.97 MB/s) - ‘collect_env.py.1’ saved [17278/17278]

Collecting environment information...
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 13.0 (x86_64)
GCC version: Could not collect
Clang version: 14.0.0 (clang-1400.0.29.201)
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.13 (main, May 24 2022, 21:28:31)  [Clang 13.1.6 (clang-1316.0.21.2)] (64-bit runtime)
Python platform: macOS-13.0-x86_64-i386-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.6
[pip3] torch==1.12.1
[pip3] torch-stft==0.1.4
[pip3] torch-tb-profiler==0.4.0
[pip3] torchaudio==0.12.1
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.13.1
[conda] Could not collect
```
"
NEW Feature: DeTR Model to torchvision,pytorch/vision,2022-11-07 14:42:32,5,new feature,6922,1438479697,"### 🚀 The feature

Adding the first Transformer-based detection model to Torchvision. A draft PR was submitted regarding the model: #5922 
but a Github Issue regarding it did not exist. Therefore creating it now.

Paper: [here](https://arxiv.org/abs/2005.12872)
Official implementation: [here](https://github.com/facebookresearch/detr)

Pinging @xiaohu2015 and @deepwilson if you guys would like to contribute

### Motivation, pitch

First Transformer-based detection model to Torchvision, which has been missing until now in torchvision.

### Alternatives

_No response_

### Additional context

_No response_"
Scheduled workflow failed,pytorch/vision,2022-11-07 09:14:24,1,bug#module: datasets,6918,1438021756,"Oh no, something went wrong in the scheduled workflow tests/download. 
Please look into it:

https://github.com/pytorch/vision/actions/runs/3408923284

Feel free to close this if this was just a one-off error.


cc @pmeier"
[FR] multi-scale feature extraction of torchvision's models,pytorch/vision,2022-11-02 11:22:23,0,,6886,1432973641,"### 🚀 The feature

Using multi-scale features of backbone model is a commonly used method in object detection area. But torchvision's models are not supported with this function.

### Motivation, pitch

I am working on object detection and would like multi-scale feature extraction to be possible with torchvision's models, especially swin.


### Alternatives

_No response_

### Additional context

_No response_"
The backward calculation results of DCN in some cases are inconsistent with the DCN of mmcv.,pytorch/vision,2022-11-02 09:59:25,2,,6885,1432844241,"### 🐛 Describe the bug

When I used torchvision.ops.deform_conv2d to build DCNv2, I found that the calculation result of backward was inconsistent with mmcv's DCNv2.
As far as I know, DCNv2 can be constructed through torchvision's deform_conv2d interface as follows:
(refer to https://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/modulated_deform_conv.py)
```python
import torch
import torch.nn as nn
from torchvision.ops import deform_conv2d as tv_deform_conv2d

class ModulatedDeformConv2d_tv(nn.modules.Module):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_size: Union[int, Tuple[int]],
                 stride: int = 1,
                 padding: int = 0,
                 dilation: int = 1,
                 groups: int = 1,
                 deform_groups: int = 1,
                 bias: Union[bool, str] = True):
        super(ModulatedDeformConv2d_tv, self).__init__()
        self.in_channels  = in_channels
        self.out_channels = out_channels
        self.kernel_size  = _pair(kernel_size)
        self.stride       = _pair(stride)
        self.padding      = _pair(padding)
        self.dilation     = _pair(dilation)
        self.groups   = groups
        self.deform_groups = deform_groups
        self.weight = nn.Parameter(torch.Tensor(
            out_channels, in_channels, *self.kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)
        self.conv_offset = nn.Conv2d(
            self.in_channels,
            self.deform_groups * 3 * self.kernel_size[0] * self.kernel_size[1],
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            bias=True)
        self.init_weights()
    def init_weights(self):
        n = self.in_channels
        for k in self.kernel_size:
            n *= k
        stdv = 1. / math.sqrt(n)
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.zero_()
        self.conv_offset.weight.data.zero_()
        self.conv_offset.bias.data.zero_()
    def forward(self, x):
        out = self.conv_offset(x)
        o1, o2, mask = torch.chunk(out, 3, dim=1)
        offset = torch.cat((o1, o2), dim=1)
        mask = torch.sigmoid(mask)
        return tv_deform_conv2d(x, offset, self.weight,
                                bias=self.bias, stride=self.stride,
                                padding=self.padding, dilation=self.dilation,
                                mask=mask)
```
Next, I tested it in mmcv's test case. Test case location:
https://github.com/open-mmlab/mmcv/blob/master/tests/test_ops/test_modulated_deform_conv.py

I tested with **ModulatedDeformConv2d_tv** as described above in place of **ModulatedDeformConv2dPack** in _test_modulated_deform_conv.py_.
```python
dcn = ModulatedDeformConv2dPack_tv(1, 1, kernel_size=(2, 2), stride=1, padding=1, deform_groups=1, bias=False)
```
As a result, there is an error in the calculation of **dcn.conv_offset.weight.grad** and **dcn.conv_offset.bias.grad**.

I checked the cpu implementation of torchvision's DCN and found that it was the same paper as mmcv, and it seems that the implementation of tv refers to openmmlab. https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp#L67

But both are in https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp#L384 and https://github.com/pytorch/vision/blob/main/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp#L500 .The processing logic seems to be different from mmcv. 
The implementation version of mmcv can refer to : 
https://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/csrc/pytorch/cpu/modulated_deform_conv.cpp#L294

Finally, I modified the deform_conv2d_kernel.cpp of torchvision according to the version of mmcv, and the test case passed. It seems that in the case of index=-1, the logic of the two is different.

Therefore, I would like to ask the developers of torchvision, since the implementation of openmmlab is referenced and the reference papers are consistent, is it reasonable that the two frameworks are not aligned in this scenario.

Attached here are modifications to torchvision to align mmcv.
```patch
diff --git a/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp b/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp
index b1d15a1..7b3ca6b 100644
--- a/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp
+++ b/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp
@@ -388,6 +388,9 @@ scalar_t get_coordinate_weight(
     scalar_t y,
     scalar_t x,
     bool is_y_direction) {
+  if (y <= -1 || y >= height || x <= -1 || x >= width){
+    return 0;
+  }
   int y_l = floor(y);
   int x_l = floor(x);
   int y_h = y_l + 1;
@@ -499,6 +502,10 @@ void deformable_col2im_coord_kernel(
       scalar_t y = (out_y * stride_h - pad_h) + i * dilation_h + offset_h;
       scalar_t x = (out_x * stride_w - pad_w) + j * dilation_w + offset_w;
 
+      if (y <= -1 || x <= -1 || y >= height || x >= width)
+      {
+        x = y = -2;
+      }
       const scalar_t weight =
           get_coordinate_weight(im_ptr, height, width, y, x, is_y_direction);
       grad_offset_val += mask_value * weight * col_ptr[col_pos];
```

### Versions

The version I use is
```
python==3.6.9
torch==1.9.0
torchvision==0.10.0
mmcv-full==1.6.2
```"
JIT compatible way to convert base64 encoded image to a Tensor,pytorch/vision,2022-11-01 06:31:21,1,,6878,1430949083,"Hello -
I have an image which is encoded as a base64 string. I want to take this string as an input and convert it to a Tensor.

How can I achieve this in a way which is torchscriptable?

So far, I have tried the two step process: 1) Convert base64 encoded image to Pillow Image using pillow library, 2) convert PIL Image to Tensor using transformations provided in torchvision
However, this approach is not torchscript compatible.

Any recommendations on how to do this in torchscript-compatible manner?


Thank you!"
0.14.0 Regression in wheel metadata,pytorch/vision,2022-10-30 23:27:17,3,bug#topic: binaries,6869,1429055929,"### 🐛 Describe the bug

The wheel metadata for the 0.14.0 release seems corrupt. It is not installable through `installer`, which means that it probably isn't installable using Hatch or PDM either.

The error is:
```
AssertionError: In /Users/groodt/Downloads/torchvision-0.14.0-cp39-cp39-manylinux1_x86_64.whl, torchvision-0.14.0.dist-info/RECORD is not mentioned in RECORD
```

Full reproduction and traceback here:
```
python3 -m pip install installer
python3 -m installer --destdir . ~/Downloads/torchvision-0.14.0-cp39-cp39-manylinux1_x86_64.whl
Traceback (most recent call last):
  File ""/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/private/tmp/blah/.venv/lib/python3.10/site-packages/installer/__main__.py"", line 85, in <module>
    _main(sys.argv[1:], ""python -m installer"")
  File ""/private/tmp/blah/.venv/lib/python3.10/site-packages/installer/__main__.py"", line 81, in _main
    installer.install(source, destination, {})
  File ""/private/tmp/blah/.venv/lib/python3.10/site-packages/installer/_core.py"", line 96, in install
    for record_elements, stream, is_executable in source.get_contents():
  File ""/private/tmp/blah/.venv/lib/python3.10/site-packages/installer/sources.py"", line 158, in get_contents
    assert record is not None, ""In {}, {} is not mentioned in RECORD"".format(
AssertionError: In /Users/groodt/Downloads/torchvision-0.14.0-cp39-cp39-manylinux1_x86_64.whl, torchvision-0.14.0.dist-info/RECORD is not mentioned in RECORD
```

### Versions

0.14.0"
Implementation of LAMB optimizer,pytorch/vision,2022-10-28 23:59:12,0,,6868,1427967180,"### 🚀 The feature

#6323  Mentions that torchvision is looking to implement [LAMB optimizer](https://arxiv.org/abs/1904.00962). 
@datumbox  I would very much like to take this issue and create a PR. 



### Motivation, pitch

LAMB optimizer was created because LARS optimizer performed poorly on model with attention mechanism (mainly). LAMB has shown to achieve very good performance gains across various tasks and I believe that it should be implemented in torchvision.

### Alternatives

_No response_

### Additional context

_No response_"
PyTorch 1.13 release breaks torchvision 0.14.0 - bad python import from maxvit,pytorch/vision,2022-10-28 21:31:40,8,triaged,6925,1438922557,"### 🐛 Describe the bug

Bad import in TorchVision > Models > maxvit.py

```python
from torchvision import models
```

```
  File ""py3.7\lib\site-packages\torchvision\__init__.py"", line 5, in <module>
    from torchvision import datasets, io, models, ops, transforms, utils
  File ""py3.7\lib\site-packages\torchvision\models\__init__.py"", line 16, in <module>
    from .maxvit import *
  File ""py3.7\lib\site-packages\torchvision\models\maxvit.py"", line 3, in <module>

    from typing import Any, Callable, List, Optional, OrderedDict, Sequence, Tuple

ImportError: cannot import name 'OrderedDict' from 'typing'
```

### Versions

Collecting environment information...
PyTorch version: 1.13.0+cu116
Is debug build: False
CUDA used to build PyTorch: 11.6
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19041-SP0
Is CUDA available: True
CUDA runtime version: 11.1.74
CUDA_MODULE_LOADING set to:
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti
Nvidia driver version: 512.15
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] pytorch-fid==0.2.1
[pip3] pytorch-lightning==1.5.10
[pip3] pytorch-wavelets==1.3.0
[pip3] torch==1.13.0+cu116
[pip3] torchaudio==0.12.1
[pip3] torchfile==0.1.0
[pip3] torchmetrics==0.8.2
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.14.0+cu116
[conda] Could not collect

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm"
torchvision.datasets.Kinetics broken,pytorch/vision,2022-10-28 19:50:56,3,module: datasets,6862,1427752940,"### 🐛 Describe the bug

```
from torchvision.datasets import Kinetics
dataset = Kinetics(root='./data/', frames_per_clip = 40, num_classes = '700', download = True)
```

error:
```
FileNotFoundError: Found no valid file for the classes acting_in_play, adjusting_glasses, air_drumming, alligator_wrestling, answering_questions, applying_cream, archaeological_excavation, arm_wrestling, arranging_flowers, assembling_bicycle, assembling_computer, attending_conference, baby_waking_up, backflip_human, baking_cookies, base_jumping, bathing_dog, battle_rope_training, bee_keeping, being_excited, being_in_zero_gravity, belly_dancing, bench_pressing, bending_back, bending_metal, biking_through_snow, blasting_sand, blending_fruit, blowdrying_hair, blowing_bubble_gum, blowing_glass, blowing_leaves, blowing_nose, blowing_out_candles, bouncing_ball_not_juggling, bouncing_on_bouncy_castle, bouncing_on_trampoline, braiding_hair, breading_or_breadcrumbing, breaking_boards, breaking_glass, breathing_fire, brush_painting, brushing_floor, brushing_hair, brushing_teeth, building_cabinet, building_lego, building_sandcastle, building_shed, bungee_jumping, canoeing_or_kayaking, card_stacking, card_throwing, carrying_baby, carrying_weight, carving_ice, carving_marble, carving_pumpkin, carving_wood_with_a_knife, casting_fishing_line, catching_fish, catching_or_throwing_baseball, catching_or_throwing_frisbee, catching_or_throwing_softball, changing_gear_in_car, changing_oil, changing_wheel_not_on_bike, checking_tires, checking_watch, chewing_gum, chiseling_stone, chiseling_wood, chopping_meat, chopping_wood, clam_digging, clay_pottery_making, clean_and_jerk, cleaning_gutters, cleaning_pool, cleaning_shoes, cleaning_toilet, cleaning_windows, climbing_a_rope, climbing_ladder, climbing_tree, closing_door, coloring_in, combing_hair, contact_juggling, cooking_chicken, cooking_egg, cooking_on_campfire, cooking_sausages_not_on_barbeque, cooking_scallops, counting_money, country_line_dancing, cracking_back, cracking_knuckles, cracking_neck, crawling_baby, crossing_eyes, crossing_river, curling_eyelashes, curling_hair, curling_sport, cutting_apple, cutting_cake, cutting_nails, cutting_orange, cutting_pineapple, cutting_watermelon, dancing_ballet, dancing_charleston, dancing_gangnam_style, dancing_macarena, dealing_cards, decorating_the_christmas_tree, delivering_mail, directing_traffic, disc_golfing, diving_cliff, docking_boat, doing_aerobics, doing_jigsaw_puzzle, doing_laundry, doing_nails, doing_sudoku, dribbling_basketball, drinking_shots, driving_car, driving_tractor, drop_kicking, drumming_fingers, dumpster_diving, dunking_basketball, dyeing_eyebrows, dyeing_hair, eating_burger, eating_cake, eating_carrots, eating_chips, eating_doughnuts, eating_hotdog, eating_ice_cream, eating_nachos, eating_spaghetti, eating_watermelon, egg_hunting, entering_church, exercising_arm, exercising_with_an_exercise_ball, extinguishing_fire, falling_off_bike, falling_off_chair, feeding_birds, feeding_fish, feeding_goats, fencing_sport, filling_cake, filling_eyebrows, finger_snapping, fixing_bicycle, fixing_hair, flint_knapping, flipping_bottle, flipping_pancake, fly_tying, flying_kite, folding_clothes, folding_napkins, folding_paper, front_raises, frying_vegetables, getting_a_haircut, getting_a_piercing, getting_a_tattoo, giving_or_receiving_award, gold_panning, golf_chipping, golf_driving, golf_putting, gospel_singing_in_church, grinding_meat, grooming_cat, grooming_dog, grooming_horse, gymnastics_tumbling, hammer_throw, hand_washing_clothes, head_stand, helmet_diving, herding_cattle, high_fiving, high_jump, high_kick, historical_reenactment, hitting_baseball, hockey_stop, holding_snake, home_roasting_coffee, hugging_baby, hugging_not_baby, hula_hooping, hurling_sport, ice_climbing, ice_fishing, ice_skating, ice_swimming, inflating_balloons, installing_carpet, ironing_hair, javelin_throw, juggling_balls, juggling_fire, juggling_soccer_ball, jumping_bicycle, jumping_into_pool, jumping_jacks, jumping_sofa, jumpstyle_dancing, kicking_field_goal, kicking_soccer_ball, land_sailing, lawn_mower_racing, laying_bricks, laying_concrete, laying_decking, laying_stone, laying_tiles, letting_go_of_balloon, lifting_hat, lighting_candle, lighting_fire, listening_with_headphones, lock_picking, long_jump, looking_at_phone, looking_in_mirror, making_a_cake, making_a_sandwich, making_balloon_shapes, making_bubbles, making_cheese, making_horseshoes, making_jewelry, making_latte_art, making_paper_aeroplanes, making_pizza, making_slime, making_snowman, making_sushi, making_tea, making_the_bed, marriage_proposal, massaging_back, massaging_feet, massaging_legs, massaging_neck, massaging_persons_head, metal_detecting, milking_cow, milking_goat, mixing_colours, moon_walking, mopping_floor, mosh_pit_dancing, mountain_climber_exercise, moving_baby, moving_child, moving_furniture, mowing_lawn, mushroom_foraging, needle_felting, news_anchoring, opening_bottle_not_wine, opening_coconuts, opening_door, opening_present, opening_refrigerator, opening_wine_bottle, passing_American_football_in_game, passing_American_football_not_in_game, passing_soccer_ball, peeling_apples, peeling_banana, peeling_potatoes, person_collecting_garbage, petting_animal_not_cat, petting_cat, petting_horse, picking_apples, picking_blueberries, pillow_fight, planing_wood, planting_trees, playing_accordion, playing_american_football, playing_badminton, playing_bagpipes, playing_basketball, playing_bass_guitar, playing_beer_pong, playing_billiards, playing_blackjack, playing_cards, playing_cello, playing_checkers, playing_chess, playing_clarinet, playing_controller, playing_cricket, playing_cymbals, playing_darts, playing_didgeridoo, playing_dominoes, playing_drums, playing_field_hockey, playing_flute, playing_gong, playing_guitar, playing_hand_clapping_games, playing_harmonica, playing_harp, playing_ice_hockey, playing_keyboard, playing_kickball, playing_laser_tag, playing_lute, playing_mahjong, playing_maracas, playing_marbles, playing_monopoly, playing_netball, playing_nose_flute, playing_oboe, playing_ocarina, playing_organ, playing_paintball, playing_pan_pipes, playing_piano, playing_piccolo, playing_pinball, playing_ping_pong, playing_poker, playing_polo, playing_recorder, playing_road_hockey, playing_rounders, playing_rubiks_cube, playing_saxophone, playing_scrabble, playing_shuffleboard, playing_slot_machine, playing_squash_or_racquetball, playing_tennis, playing_trombone, playing_trumpet, playing_ukulele, playing_violin, playing_volleyball, playing_with_trains, playing_xylophone, poaching_eggs, poking_bellybutton, pole_vault, polishing_furniture, polishing_metal, popping_balloons, pouring_beer, pouring_milk, pouring_wine, preparing_salad, presenting_weather_forecast, pretending_to_be_a_statue, pull_ups, pulling_espresso_shot, pulling_rope_game, pumping_fist, pumping_gas, punching_bag, punching_person_boxing, push_up, pushing_car, pushing_cart, pushing_wheelbarrow, pushing_wheelchair, putting_in_contact_lenses, putting_on_eyeliner, putting_on_foundation, putting_on_lipstick, putting_on_mascara, putting_on_sari, putting_on_shoes, putting_wallpaper_on_wall, raising_eyebrows, reading_book, reading_newspaper, recording_music, repairing_puncture, riding_a_bike, riding_camel, riding_elephant, riding_mechanical_bull, riding_mule, riding_or_walking_with_horse, riding_scooter, riding_snow_blower, riding_unicycle, ripping_paper, roasting_marshmallows, roasting_pig, robot_dancing, rock_climbing, rock_scissors_paper, roller_skating, rolling_eyes, rolling_pastry, rope_pushdown, running_on_treadmill, salsa_dancing, sanding_floor, sanding_wood, sausage_making, sawing_wood, scrambling_eggs, scrubbing_face, scuba_diving, seasoning_food, separating_eggs, setting_table, shaking_hands, shaking_head, shaping_bread_dough, sharpening_knives, sharpening_pencil, shaving_head, shaving_legs, shearing_sheep, shining_flashlight, shining_shoes, shoot_dance, shooting_basketball, shooting_goal_soccer, shooting_off_fireworks, shot_put, shoveling_snow, shredding_paper, shucking_oysters, shuffling_cards, shuffling_feet, side_kick, sign_language_interpreting, silent_disco, sipping_cup, ski_ballet, ski_jumping, skiing_crosscountry, skiing_mono, skiing_slalom, skipping_rope, skipping_stone, sled_dog_racing, slicing_onion, smelling_feet, smoking_hookah, smoking_pipe, snatch_weight_lifting, spinning_plates, spinning_poi, splashing_water, spray_painting, springboard_diving, square_dancing, squeezing_orange, stacking_cups, stacking_dice, standing_on_hands, steer_roping, steering_car, sticking_tongue_out, stomping_grapes, stretching_arm, stretching_leg, sucking_lolly, surfing_crowd, surfing_water, sweeping_floor, swimming_backstroke, swimming_breast_stroke, swimming_butterfly_stroke, swimming_front_crawl, swimming_with_dolphins, swimming_with_sharks, swing_dancing, swinging_baseball_bat, swinging_on_something, sword_fighting, sword_swallowing, tagging_graffiti, tai_chi, taking_photo, talking_on_cell_phone, tango_dancing, tap_dancing, tapping_guitar, tapping_pen, tasting_beer, tasting_food, tasting_wine, threading_needle, throwing_axe, throwing_ball_not_baseball_or_American_football, throwing_discus, throwing_knife, throwing_snowballs, throwing_tantrum, throwing_water_balloon, tie_dying, tightrope_walking, tossing_coin, tossing_salad, training_dog, treating_wood, trimming_or_shaving_beard, trimming_shrubs, trimming_trees, triple_jump, twiddling_fingers, tying_bow_tie, tying_knot_not_on_a_tie, tying_necktie, tying_shoe_laces, uncorking_champagne, unloading_truck, using_a_microscope, using_a_paint_roller, using_a_power_drill, using_a_sledge_hammer, using_a_wrench, using_atm, using_bagging_machine, using_circular_saw, using_inhaler, using_megaphone, using_puppets, using_remote_controller_not_gaming, using_segway, vacuuming_car, vacuuming_floor, visiting_the_zoo, wading_through_mud, wading_through_water, waiting_in_line, waking_up, walking_on_stilts, walking_the_dog, walking_through_snow, walking_with_crutches, washing_dishes, washing_feet, washing_hair, washing_hands, watching_tv, water_skiing, water_sliding, watering_plants, waving_hand, waxing_armpits, waxing_back, waxing_chest, waxing_eyebrows, waxing_legs, weaving_basket, weaving_fabric, wood_burning_art, wrapping_present, yarn_spinning. Supported extensions are: avi, mp4
```


### Versions

PyTorch version: 1.12.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.2.5

Python version: 3.7.6 (default, Feb 18 2020, 21:28:31)  [GCC 9.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-70-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: 
GPU models and configuration: 
GPU 0: NVIDIA A100 80GB PCIe
GPU 1: NVIDIA A100 80GB PCIe
GPU 2: NVIDIA A100 80GB PCIe
GPU 3: NVIDIA A100 80GB PCIe

Nvidia driver version: 510.47.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.4
[pip3] torch==1.12.1
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.13.1
[conda] Could not collect

cc @pmeier"
Enforce contiguous outputs on the transforms v2 kernels?,pytorch/vision,2022-10-26 10:54:10,5,needs discussion#module: transforms#Perf#prototype,6839,1423826193,"All the performance benchmarks that did so far for transforms v1 vs. v2 were on contiguous inputs. However, we have a few kernels that leave the output in a noncontiguous state:

- `affine_image_tensor` in case `fill is not None and dtype.is_floating_point`
- `convert_color_space` in case we only strip the alpha channel, i.e. `RGB_ALPHA -> RGB` and `GRAY_ALPHA -> ALPHA`
- `rotate_image_tensor` in case `fill is not None and dtype.is_floating_point`
- `crop_image_tensor`
- `center_crop_image_tensor`
- `five_crop_image_tensor`
- `ten_crop_image_tensor`

If applicable, the same is also valid for the `*_mask` and `*_video` kernels since they are thin wrappers around the `*_image_tensor` ones.

We should benchmark at least for a few kernels whether noncontiguous inputs cause a performance degredation that is larger than enforcing contiguous outputs on the kernels above. If so we should probably enforce contiguous outputs of our kernels.


cc @vfdev-5 @datumbox @bjuncek"
Performance improvements for transforms v2 vs. v1,pytorch/vision,2022-10-24 09:47:51,5,module: transforms#Perf#prototype,6818,1420550387,"In addition to a lot of other goodies that transforms v2 will bring, we are also actively working on improving the performance. This is a tracker / overview issue of our progress.

Performance was measured with this [benchmark script](https://gist.github.com/pmeier/e0f1ea77c9cf75b682d7f30366a89bf8). Unless noted otherwise, the performance improvements reported above were computed on uint8, RGB images and videos while running single-threaded on CPU. You can find the full benchmark results alongside the benchmark script. The results will be constantly updated if new PRs are merged that have an effect on the kernels.

Kernels:

- color
  - [x] `adjust_brightness` #6784 
  - [x] `adjust_contrast` #6784
    - Performance increased for images by roughly 16%.
    - Performance increased for videos by roughly 38%.
  - [x] `adjust_gamma` #6820
  - [x] `adjust_hue` #6805
    - Performance increased for images by roughly 30%.
    - Performance increased for videos by roughly 47%.
  - [x] `adjust_saturation` #6784
    - Performance increased for images by roughly 16%.
    - Performance increased for videos by roughly 20%.
  - [x] `adjust_sharpness` #6784
  - [x] `autocontrast` #6811
    - Performance increased for images by roughly 2%.
    - Performance increased for videos by roughly 10%.
  - [x] `equalize` #6738, #6757, #6776
  - [x] `invert` #6819
  - [x] `posterize` #6823, #6847
    - No performance improvement possible
  - [x] `solarize` #6819
- geometry
  - [ ] `affine`
  - [x] `center_crop` #6880
  - [ ] `crop`
  - [ ] `elastic`
  - [ ] `erase`
  - [ ] `five_crop`: Composite kernel
  - [ ] `horizontal_flip`
  - [ ] `pad`
  - [ ] `perspective`
  - [ ] `resize`
  - [ ] `resized_crop`: Composite kernel
  - [ ] `rotate`
  - [ ] `ten_crop`: Composite kernel
  - [ ] `vertical_flip`
- meta
  - [x] `convert_color_space` #6784 #6832
  - [x] `convert_dtype` #6795
    - Performance increased for images by roughly 37%.
    - Performance increased for videos by roughly 33%.
    - There is still some performance gain left for `int` to `int` conversion. Currently, we are using a multiplication 
      but theoretically bit shifts are faster. However, on PyTorch core the CPU kernels for bit shifts are not 
      vectorized making them slower for regular sized images than a multiplication.
- misc
  - [x] `gaussian_blur` #6762 #6888
  - [x] `normalize` #6821


Transform Classes:

- [x] MixUp/CutMix #6835
- [x] ColorJitter, RandomPhotometricDistort #6837


cc @vfdev-5 @datumbox @bjuncek"
create_feature_extractor and get_graph_node_names should have a concrete_args argument,pytorch/vision,2022-10-24 03:46:38,0,topic: feature extraction,6817,1420164361,"### 🚀 The feature

Add a concrete_args argument to `tv.models.feature_extraction.create_feature_extractor` and `.get_graph_node_names`. The argument would be passed to torch.fx's trace().


### Motivation, pitch

Currently, one cannot use the torchvision feature extractor on modules whose forward method receives control-flow-affecting arguments. This limitation can be removed by passing the `concrete_args` keyword argument to all calls of .trace(). For example:
`
    train_tracer.trace(model.train(), concrete_args=concrete_args)
`

### Alternatives

Alternatively, one can edit the target module to eliminate the control-flow affecting arguments.

### Additional context

MRE:
```
import torch
from torchvision.models.feature_extraction import get_graph_node_names

class MyNet(torch.nn.Module):
    def forward(self, x, flag=True):
        return x if flag else x + 1

my_net = MyNet()
train_nodes, eval_nodes = get_graph_node_names(my_net)
```
This raises `TraceError: symbolically traced variables cannot be used as inputs to control flow`.

After the modification,  this following code will successfully run:
```
train_nodes, eval_nodes = get_graph_node_names(my_net, concrete_args={flag:True})
```"
Specify channel dim for transforms.Normalize,pytorch/vision,2022-10-23 07:27:01,2,needs discussion#module: transforms,6816,1419684773,"### 🚀 The feature

Specify channel dim for `transforms.Normalize`, `transforms.functional.normalize`, `transforms.functional_tensor.normalize`, To enable `transforms.Normalize` to normalize according mean and std by specified channel.

A solution is adding a new argument `dim_channel` to the classes and functions above and

```python
# in transforms.functional_tensor.normalize
broadcast_ch_shape = [1 for _ in range(tensor.ndim)]
broadcast_ch_shape[dim_channel] = -1
if mean.ndim == 1:
    mean = mean.view(*broadcast_ch_shape)
if std.ndim == 1:
    std = std.view(*broadcast_ch_shape)
return tensor.sub_(mean).div_(std)
```

### Motivation, pitch

Recent torchvision deprecated `transforms._transforms_video` and added features in many transforms to process [..., H, W] shaped tensors. For video transforming, it is a great improvement, meanwhile, `transforms.Normalize` is not lucky enough to be among these transforms. This means that the users either resort to other transforms such as `pytorchvideo.transforms.Normalize` or normalize each frame seperately. The requested feature will relieve this pain, and video transforms can be more nice and neat.

### Alternatives

_No response_

### Additional context

_No response_

cc @vfdev-5 @datumbox"
Latest PyAV version breaks torchvision.io.write_video when writing video with audio,pytorch/vision,2022-10-22 03:44:13,2,duplicate#module: video#dependency issue,6814,1419058132,"### 🐛 Describe the bug

#### Description

It seems the [latest PyPi release](https://pypi.org/project/av/#history) of PyAV (`av==10.0.0`) breaks `torchvision.io.write_video` when writing a video containing audio. If you call `write_video` without audio array, it still seems to work fine - it's just when an audio array is provided that it breaks.

#### Reproducible Example

Here is a colab notebook reproducing the issue: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/nateraw/1123039fc90cefd90d282cf6c297d1d2/torchvision-write-video-pyav-bug.ipynb)
Here is that same notebook as GitHub gist: https://gist.github.com/nateraw/1123039fc90cefd90d282cf6c297d1d2

#### Strack Trace

```
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
<ipython-input-3-28056a084160> in <module>
      6     audio_fps=44100,
      7     audio_codec='aac',
----> 8     audio_options=None
      9 )

/usr/local/lib/python3.7/dist-packages/torchvision/io/video.py in write_video(filename, video_array, fps, video_codec, options, audio_array, audio_fps, audio_codec, audio_options)
    114             num_channels = audio_array.shape[0]
    115             audio_layout = ""stereo"" if num_channels > 1 else ""mono""
--> 116             audio_sample_fmt = container.streams.audio[0].format.name
    117 
    118             format_dtype = np.dtype(audio_format_dtypes[audio_sample_fmt])

IndexError: tuple index out of range
```

#### Temporary Solution

Downgrading to `av==9.2.0` fixed the issue for me.

### Versions

```
PyTorch version: 1.12.1+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.22.6
Libc version: glibc-2.26

Python version: 3.7.15 (default, Oct 12 2022, 19:14:55)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
Is CUDA available: False
CUDA runtime version: 11.2.152
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.6
[pip3] torch==1.12.1+cu113
[pip3] torchaudio==0.12.1+cu113
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.13.1
[pip3] torchvision==0.13.1+cu113
[conda] Could not collect
```"
Name `remove_small_boxes` does not fit return value,pytorch/vision,2022-10-21 12:29:45,0,,6810,1418268264,"[`remove_small_boxes`](https://github.com/pytorch/vision/blob/5421f12a3d62286b7935c9b8d24a0841019e3449/torchvision/ops/boxes.py#L115) returns “indices of the boxes that have both sides larger than min_size”. In that case, I would expect the function to be called something like `find_big_boxes` instead."
videos in the assets might not be representable,pytorch/vision,2022-10-21 08:49:03,0,needs discussion#module: video,6808,1417989740,"### 🐛 Describe the bug

As noted in #6771 videos in the `test/assets/videos` might not be representative and/or have issues with decoding. 
To alleviate this, I propose going through some of the more used datasets and gathering a few videos from there.



### Versions

not relevant for this issue"
"torchvision Resize export doesn't support antialias=True,  ::_upsample_bicubic2d_aa to ONNX opset version 16 is not supported",pytorch/vision,2022-10-20 21:28:39,2,module: onnx,6806,1417319626,"### 🐛 Describe the bug

'''python

    resize = torchvision.transforms.Resize(size=(224, 224), interpolation=torchvision.transforms.InterpolationMode.BICUBIC, max_size=None, antialias=True)

    dummy_input = torch.randn(1,3,2000,2000)
    torch.onnx.export(resize, dummy_input, preprocess_model_path, input_names=['image'], output_names=['processed_image'], dynamic_axes={
                      # dict value: manually named axes
                      ""image"": {2:'h', 3:'w'},
                  })
'''

error ""torch.onnx.symbolic_registry.UnsupportedOperatorError: Exporting the operator ::_upsample_bicubic2d_aa to ONNX opset version 16 is not supported.""

### Versions

 ONNX opset version 16 is not supported

cc @neginraoof @BowenBao "
Video reader  segfaults on certain videos. Here's a partial-reproduction script,pytorch/vision,2022-10-20 15:38:57,1,,6802,1416852955,"Follow up on the following observation by @vedantroy :

This segfaults on certain videos. Here's a partial-reproduction script (the videos are stored in a pandas dataframe):

```
import pandas as pd

# df2 = pd.read_pickle(""df2.pkl"")
df = pd.read_pickle(""df.pkl"")
# print the # of rows in the df
print(len(df))
# print the keys in the df
print(df.keys())
# print the first row in the df
print(df.iloc[0])


# print the type of the 1st value in the 1st row
first_vid = df.iloc[0][0]
print(f""Length: {len(first_vid)}"")
print(f""Type: {type(first_vid)}"")

# write first_vid to a file
with open(""test.mp4"", ""wb"") as f:
    f.write(first_vid)

import itertools
import copy

import torch
from torchvision.io import VideoReader
import torchvision

def clip_from_start(buf: bytes, expected_frames: int):
    # import av
    # import io
    # buffer = io.BytesIO(buf)
    # container = av.open(buffer)
    # i = 0 
    # for frame in container.decode(video=0):
    #     print(type(frame))
    #     i += 1
    #     print(i)
    #     pass

    tensor = torch.frombuffer(buf, dtype=torch.uint8)
    tensor = copy.deepcopy(tensor)
    # torchvision.io.read_video()
    rdr = VideoReader(tensor)
    sampled_frames = list(itertools.islice(iter(rdr), expected_frames))
    if len(sampled_frames) != expected_frames:
        return None
    data = []
    for frame in sampled_frames:
        data.append(frame[""data""])
    return torch.stack(data, dim=0)

clip = clip_from_start(first_vid, 2)
print(clip.shape)
```

I'm working to get approval of the public copy of the data.

In the meantime, the error is:
```
test.py:41: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:1563.)
  tensor = torch.frombuffer(buf, dtype=torch.uint8)
malloc(): corrupted top size
Aborted (core dumped)
```

_Originally posted by @vedantroy in https://github.com/pytorch/vision/issues/6771#issuecomment-1283644752_
      "
`convert_image_dtype` overflows with low precision floating point dtypes,pytorch/vision,2022-10-20 08:39:20,1,bug#module: transforms,6799,1416206911,"While working on improving performance of `convert_image_dtype` in #6795, I found several cases where `convert_image_dtype` is silently failing for low precision floating point dtypes `torch.float16` and `torch.bfloat16`:

```py
import torch
from torchvision.transforms import functional as F

# torch.{float16, bfloat16} to any integer dtype
image = torch.tensor(1.0, dtype=torch.float16)
print(image, F.convert_image_dtype(image, torch.uint8), F.convert_image_dtype(image, torch.int8))

# torch.{int32, int64} to torch.float16
image = torch.tensor(2**31 - 1, dtype=torch.int32)
print(image, F.convert_image_dtype(image, torch.float16))
```

```
tensor(1., dtype=torch.float16) tensor(0, dtype=torch.uint8) tensor(-128, dtype=torch.int8)
tensor(2147483647, dtype=torch.int32) tensor(nan, dtype=torch.float16)
```

1. Converting an valid (b)float16 image in the value range `[0.0, 1.0]` to any integer dtype overflows the computation. This stems from the fact that `eps` is fixed:

   https://github.com/pytorch/vision/blob/7a62a545ce76f43ccc5cfe0009131f7db14ae7b5/torchvision/transforms/functional_tensor.py#L90-L93

   This value is simply to large for (b)float16:

   ```py
   >>> image = torch.tensor(1.0, dtype=torch.float16)
   >>> image.mul(255 + 1.0 - 1e-3)  # float16 -> uint8
   tensor(256., dtype=torch.float16)
   >>> image.to(torch.float32).mul(255 + 1.0 - 1e-3)  # float32 -> uint8
   tensor(255.9990)
   >>> image.mul(255 + 1.0 - 7e-2)  # float16 -> uint8 with adjusted eps
   tensor(255.8750, dtype=torch.float16)
   ```

   The whole point of `eps` is to be as small as possible to have an even value distribution. See https://github.com/pytorch/vision/pull/2078#issuecomment-613524965 for details.

   We could simply make `eps` dependent on the input dtype in a function similar to 

   https://github.com/pytorch/vision/blob/7a62a545ce76f43ccc5cfe0009131f7db14ae7b5/torchvision/transforms/functional_tensor.py#L47

2. Converting a int{32, 64} image to float16 should not be possible since it can't hold the maximum values:

   ```py
   >>> torch.finfo(torch.float16).max
   65504.0
   >>> torch.iinfo(torch.int16).max  # ok
   32767
   >>> torch.iinfo(torch.int32).max  # not ok
   2147483647
   >>> torch.iinfo(torch.int64).max  # not ok
   9223372036854775807
   >>> torch.finfo(torch.bfloat16).max  # bfloat does not have this issue
   3.3895313892515355e+38
   ```

   We are already raising an error for unsafe float to int conversions

   https://github.com/pytorch/vision/blob/7a62a545ce76f43ccc5cfe0009131f7db14ae7b5/torchvision/transforms/functional_tensor.py#L78-L83

   so we could simply do the same here.


cc @vfdev-5 @datumbox"
‘Hide-and-Seek’ Random Masking Transform,pytorch/vision,2022-10-19 21:00:07,2,needs discussion#module: transforms#new feature,6796,1415579080,"### 🚀 The feature

**Source** 
Hide-and-Seek: Forcing a Network to be Meticulous for Weakly-supervised Object and Action Localization ([Scholar](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Hide-and-Seek%3A+Forcing+a+Network+to+be+Meticulous+for+Weakly-supervised+Object+and+Action+Localization&btnG=), [Arxiv](https://arxiv.org/abs/1704.04232))
Number of citations: 542

**Method**
The image is divided into a grid and then every patch of this grid is masked with probability p. So the inputs are patch_size, p and fill_value.

### Motivation, pitch

Like described in the paper, this augmentation method can improve weakly-supervised object localization, as with it not only the most discriminative parts in the image are found, but all relevant ones.

I have already implemented this method, so I could open a PR, if you think this feature is a useful addition.

### Alternatives

_No response_

### Additional context

Here an example from the paper.
![Screenshot from 2022-10-19 22-56-54](https://user-images.githubusercontent.com/56137202/196802741-6a18c50e-ca83-4619-bdc8-f3f504cd0f46.png)


cc @vfdev-5 @datumbox"
`av==10.0.0` breaks CI,pytorch/vision,2022-10-18 15:30:52,1,module: video#module: ci,6790,1413434129,"The recent release of `av==10.0.0` breaks at least two tests: https://app.circleci.com/pipelines/github/pytorch/vision/21245/workflows/ecbc81ff-f864-4227-a35d-b4ed5e9f57ee/jobs/1724114?invite=true#step-108-3075

```
=================================== FAILURES ===================================
____________________ TestVideo.test_video_clips_custom_fps _____________________
Traceback (most recent call last):
  File ""/root/project/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1120, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File ""/root/project/env/lib/python3.7/multiprocessing/queues.py"", line 104, in get
    if not self._poll(timeout):
  File ""/root/project/env/lib/python3.7/multiprocessing/connection.py"", line 257, in poll
    return self._poll(timeout)
  File ""/root/project/env/lib/python3.7/multiprocessing/connection.py"", line 414, in _poll
    r = wait([self], timeout)
  File ""/root/project/env/lib/python3.7/multiprocessing/connection.py"", line 921, in wait
    ready = selector.select(timeout)
  File ""/root/project/env/lib/python3.7/selectors.py"", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File ""/root/project/env/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 5231) is killed by signal: Segmentation fault. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/root/project/test/test_datasets_video_utils.py"", line 63, in test_video_clips_custom_fps
    video_clips = VideoClips(video_list, num_frames, num_frames, fps, num_workers=2)
  File ""/root/project/torchvision/datasets/video_utils.py"", line 132, in __init__
    self._compute_frame_pts()
  File ""/root/project/torchvision/datasets/video_utils.py"", line 153, in _compute_frame_pts
    for batch in dl:
  File ""/root/project/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 628, in __next__
    data = self._next_data()
  File ""/root/project/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1316, in _next_data
    idx, data = self._get_data()
  File ""/root/project/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1282, in _get_data
    success, data = self._try_get_data()
  File ""/root/project/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1133, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 5231) exited unexpectedly
----------------------------- Captured stderr call -----------------------------
100.0%
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.

```

I'm going to pin to `< 10`. 

cc @seemethere"
`GeneralizedRCNN` raises an exception when encountering a bad bounding box during `forward`,pytorch/vision,2022-10-18 13:12:41,2,,6787,1413206654,"https://github.com/pytorch/vision/blob/0610b13ac4af3717f538454a9c6b1f441cb386f3/torchvision/models/detection/generalized_rcnn.py#L95

The current implementation of `GeneralizedRCNN` checks for degenerate bounding boxes during its forward pass. While -- in principle -- that is great, it can make training brittle, especially when randomized data augmentation is involved, because an exception is raised (via `assert` by `torch`). I see that the current implementation is experimental (it says `# TODO: Move this to a function` a couple of lines up) but I am wondering what could be more helpful ways of dealing with those bad bounding boxes:

1. Raise an explicit exception that can be handled during training to, for example, skip the current batch?
2. Filter out degenerate bounding boxes?
3. Keep the current implementation because torch's `_assert` is great and I just don't understand it properly? 🙄"
Performance difference between the conda and pip version in io.read_image,pytorch/vision,2022-10-17 22:28:35,9,needs reproduction#module: io#topic: binaries#Perf,6782,1412313209,"### 🐛 Describe the bug

There is a big performance difference in reading jpg images using the conda or pip version of torchvision using the function torchvision.io.read_image. 
When benchmarking reading 1000 images from a folder the pip version is more than 2x faster than the version installed from conda!
For the test I created 2 new conda environments using 
`conda create --name tvpip python=3.10`
In one environment I installed torchvision using conda:
`conda install pytorch torchvision cudatoolkit=11.3 -c pytorch`
and in the other using pip:
`pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113`

Then I used the following code to benchmark torchvision.io.read_image, Pillow and accimage:
```
import os, torchvision
from time import time as t

f = ""test""
files = [file for file in os.listdir(f)]
test_images = len(files)

def test(files, fct):
    s = t()
    for file in files:
        image = fct(os.path.join(f,file))
    return t()-s

torchvision.set_image_backend(""PIL"")
time_needed = test(files, torchvision.io.read_image)
print(f""Torchvision {torchvision.get_image_backend():13s} Loading {test_images} files took {time_needed:.1f}s"")

torchvision.set_image_backend(""accimage"")
time_needed = test(files, torchvision.io.read_image)
print(f""Torchvision {torchvision.get_image_backend():13s} Loading {test_images} files took {time_needed:.1f}s"")

from PIL import Image
s = t()
for file in files:
    image = Image.open(os.path.join(f,file)).convert(""RGB"")
time_needed = t() - s
print(f""{'Pillow':25s} Loading {test_images} files took {time_needed:.1f}s"")

import accimage
time_needed = test(files, accimage.Image)
print(f""{'AccImage':25s} Loading {test_images} files took {time_needed:.1f}s"")
```

Findings:
- In the conda environment the torchvision.io.read_image takes 4.6s, in the pip environment it takes 1.9s, Should be the same. I couln't figure out where the speed difference comes from, from the timings it looks like pip is using pillow-simd or libjpeg-turbo somehow.
- When using the accimage backend with torchvision (torchvision.set_image_backend) the time to load the images doesn't change at all. Which seems like the same bacend is used. That behavior is the same in the pip and conda environment. 
- Installing pillow-simd and accimage in the environment before installing torchvision doesn't change anything apart from the pillow time.
- When installing accimage in the conda environment, the time for torchvision.io.read_image with the accimage backend doesn't change, which in my understanding it should.

I hope you can reproduce the behavior or give some insights why this might be the case. Thanks already.

### Versions

## Environment pip
Collecting environment information...
PyTorch version: 1.12.1+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: 
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080 Ti
Nvidia driver version: 515.76
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.4
[pip3] torch==1.12.1+cu113
[pip3] torchvision==0.13.1+cu113
[conda] numpy                     1.23.4                   pypi_0    pypi
[conda] torch                     1.12.1+cu113             pypi_0    pypi
[conda] torchvision               0.13.1+cu113             pypi_0    pypi



## Environment conda
Collecting environment information...
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: 
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080 Ti
Nvidia driver version: 515.76
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.1
[pip3] torch==1.12.1
[pip3] torchvision==0.13.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0           py310h7f8727e_0  
[conda] mkl_fft                   1.3.1           py310hd6ae3a3_0  
[conda] mkl_random                1.2.2           py310h00e6091_0  
[conda] numpy                     1.23.1          py310h1794996_0  
[conda] numpy-base                1.23.1          py310hcba007f_0  
[conda] pytorch                   1.12.1          py3.10_cuda11.3_cudnn8.3.2_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.13.1              py310_cu113    pytorch
"
Implement tensor handling for ToTensor(),pytorch/vision,2022-10-17 10:34:38,5,needs discussion#module: transforms,6780,1411333510,"### 🚀 The feature

The ToTensor() function of torchvision.transforms must be able to handle torch Tensors. If it gets a tensor, it must return the same tensor without modification

### Motivation, pitch

The function ToTensor can take a NumPy array as input. But if a torch tensor is passed to it, the function raises an error. It will be better if the function can take PyTorch tensors too without raising error.

### Alternatives

_No response_

### Additional context

_No response_

cc @vfdev-5 @datumbox"
Only flatten a pytree once per container transform,pytorch/vision,2022-10-14 08:19:10,0,needs discussion#module: transforms#Perf#prototype,6769,1408962754,"_This issue is ablation from #6760. https://github.com/pytorch/vision/issues/6760#issuecomment-1277522535 was deemed good at first, but there were unexpected issues https://github.com/pytorch/vision/issues/6760#issuecomment-1277822471 that made this proposal more controversial._

---

When implementing augmentation pipelines, the individual transformations are usually wrapped in a container transform like `transforms.Compose`. Under the assumptions that the children transforms

1. support arbitrarily structured inputs, i.e. pytrees, and
2. keep the structure in tact

it would suffice to only flatten the input once in the container transform, let all children operate on the flattened inputs without trying to do so again, and only unflatten at the end of the container. This would reduce the number of `tree_{flatten, unflatten}` calls to a fixed and low single digit number per pipeline rather than being dependent on the number of transforms.

All builtin transforms fulfill these assumptions. However, our container transforms also support custom transforms and we have no way of knowing if they also fulfill them or not.

There are two ways we could communicate this information:

1. **Annotate the transforms**: Each transform could have a `supports_pytree: bool` attribute that the containers are looking for. For example, inside `transforms.Compose`

   https://github.com/pytorch/vision/blob/dc5fd831ed9f4a0c58a194853ffa9cce6c240026/torchvision/prototype/transforms/_container.py#L13

   we could do 

   ```py
   flatten_once = all(getattr(transform, ""supports_pytree"", False) for transform in self.transforms)
   ```

   Since a `transforms.Transform` supports pytree inputs by default, I think it is reasonable to add a `supports_pytree: bool = True` parameter to its constructor. That means all of our builtin transformations would be supported out of the box. If we do this, we need to clearly document that users that subclass from `transforms.Transform`, but opt out the `_check_inputs` / `_get_params` / `_transform` prototocol by overwriting `forward` need to set this flag to the appropriate value. Otherwise, wrapping their custom transformation into a `transforms.Compose` will likely fail.

2. **Annotate the container transform**: Instead of relying on automagic detection whether pytree objects are supported by the children, we could simply add a `flatten_once: bool = False` flag to container transforms. Note that this would need to be turned off by default to avoid failures if not all children support pytrees. Meaning, the user has to opt in into this feature.

Of the options above, I lean towards 1. Since 2. is opt-in, most users will probably never use the feature. I think that is worse than having users consciously set a flag if they subclass from our base class, but opt out of the features.

However, the benefits by adopting this proposal regardless of which option we choose are insignificant for a single call. They only manifest for [large scale trainings](https://github.com/pytorch/vision/issues/6760#issuecomment-1277299445). Even there we are looking at shaving double digit minutes on runs that take single digit days. Thus, we should also discuss if this change is worth it at all to introduce new API surface. If we want / need this performance gain, but don't want to touch the API, maybe there is a way to only implement this in our references. It will be probably be more complicated though, since some of the changes need to happen on `transforms.Transform` and no only on the container transforms.

cc @vfdev-5 @datumbox @bjuncek"
AttributeError: module 'torchvision.models' has no attribute 'get_model',pytorch/vision,2022-10-13 08:33:52,5,,6761,1407378032,"### 🐛 Describe the bug

[Reproduce with Colab](https://colab.research.google.com/drive/1pgLynchGmyuRkW1ru5cnixgWoU_gFVNE?usp=sharing)

I am trying to execute [RetinaNet training](https://github.com/pytorch/vision/tree/main/references/detection#retinanet) with
``` 
torchrun --nproc_per_node=1 /content/vision/references/detection/train.py\
    --dataset coco --data-path=/content/vision/dataset --model retinanet_resnet50_fpn --epochs 26\
    --lr-steps 16 22 --aspect-ratio-group-factor 3 --lr 0.01 --weights-backbone ResNet50_Weights.IMAGENET1K_V1
```
 but get's an `AttributeError: module 'torchvision.models' has no attribute 'get_model'`

Complete trace:
```
| distributed init (rank 0): env://
Namespace(amp=False, aspect_ratio_group_factor=3, batch_size=2, data_augmentation='hflip', data_path='/content/vision/dataset', dataset='coco', device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, epochs=26, gpu=0, lr=0.01, lr_gamma=0.1, lr_scheduler='multisteplr', lr_step_size=8, lr_steps=[16, 22], model='retinanet_resnet50_fpn', momentum=0.9, norm_weight_decay=None, opt='sgd', output_dir='.', print_freq=20, rank=0, resume='', rpn_score_thresh=None, start_epoch=0, sync_bn=False, test_only=False, trainable_backbone_layers=None, use_copypaste=False, use_deterministic_algorithms=False, weight_decay=0.0001, weights=None, weights_backbone='ResNet50_Weights.IMAGENET1K_V1', workers=4, world_size=1)
Loading data
loading annotations into memory...
Done (t=14.51s)
creating index...
index created!
loading annotations into memory...
Done (t=2.34s)
creating index...
index created!
Creating data loaders
Using [0, 0.5, 0.6299605249474366, 0.7937005259840997, 1.0, 1.2599210498948732, 1.5874010519681994, 2.0, inf] as bins for aspect ratio quantization
Count of instances per bin: [  104   982 24236  2332  8225 74466  5763  1158]
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Creating model
Traceback (most recent call last):
  File ""/content/vision/references/detection/train.py"", line 311, in <module>
    main(args)
  File ""/content/vision/references/detection/train.py"", line 222, in main
    model = torchvision.models.get_model(
AttributeError: module 'torchvision.models' has no attribute 'get_model'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1179) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py"", line 755, in run
    )(*cmd_args)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py"", line 247, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/content/vision/references/detection/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-10-13_08:26:16
  host      : 291a9c949d94
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1179)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

### Versions

```
PyTorch version: 1.12.1+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.22.6
Libc version: glibc-2.26

Python version: 3.7.14 (default, Sep  8 2022, 00:06:44)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic
Is CUDA available: True
CUDA runtime version: 11.2.152
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 460.32.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.6
[pip3] torch==1.12.1+cu113
[pip3] torchaudio==0.12.1+cu113
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.13.1
[pip3] torchvision==0.13.1+cu113
[conda] Could not collect
```"
[FEEDBACK] Transforms V2 API,pytorch/vision,2022-10-12 12:08:04,10,,6753,1406097874,"### 🚀 The feature

This issue is dedicated for collecting community feedback on the Transforms V2 API. Please review the dedicated [blogpost](https://pytorch.org/blog/extending-torchvisions-transforms-to-object-detection-segmentation-and-video-tasks/)  where we describe the API in detail and provide an overview of its features.

We would love to get your thoughts, comments and input in order to improve the API and graduate it from prototype on the near future.

-----------

Code example using [this](https://user-images.githubusercontent.com/5347466/195350223-8683ef25-1367-4292-9174-c15f85c7358e.jpg) image:
```python
import PIL
from torchvision import io, utils
from torchvision.prototype import features, transforms as T
from torchvision.prototype.transforms import functional as F


# Defining and wrapping input to appropriate Tensor Subclasses
path = ""COCO_val2014_000000418825.jpg""
img = features.Image(io.read_image(path), color_space=features.ColorSpace.RGB)
# img = PIL.Image.open(path)
bboxes = features.BoundingBox(
    [[2, 0, 206, 253], [396, 92, 479, 241], [328, 253, 417, 332],
     [148, 68, 256, 182], [93, 158, 170, 260], [432, 0, 438, 26],
     [422, 0, 480, 25], [419, 39, 424, 52], [448, 37, 456, 62],
     [435, 43, 437, 50], [461, 36, 469, 63], [461, 75, 469, 94],
     [469, 36, 480, 64], [440, 37, 446, 56], [398, 233, 480, 304],
     [452, 39, 463, 63], [424, 38, 429, 50]],
    format=features.BoundingBoxFormat.XYXY,
    spatial_size=F.get_spatial_size(img),
)
labels = features.Label([59, 58, 50, 64, 76, 74, 74, 74, 74, 74, 74, 74, 74, 74, 50, 74, 74])


# Defining and applying Transforms V2
trans = T.Compose(
    [
        T.ColorJitter(contrast=0.5),
        T.RandomRotation(30),
        T.CenterCrop(480),
    ]
)
img, bboxes, labels = trans(img, bboxes, labels)


# Visualizing results
viz = utils.draw_bounding_boxes(F.to_image_tensor(img), boxes=bboxes)
F.to_pil_image(viz).show()
```"
[prototype] Should we make `prototype.transforms._utilis.py` public?,pytorch/vision,2022-10-11 08:37:20,4,needs discussion#module: transforms#prototype,6737,1404242494,"### 🚀 The feature

A large number of utility methods such as `query_bounding_box`, `has_any`, `has_all` etc are being used regularly by our new Transform Classes but are not exposed publicly to the users. We should discuss which of them we want to expose publicly, to assist Transform authoring on the new API.

### Motivation, pitch

While users start developing transforms using the new API, they would be forced to write quite some extra code to achieve basic tasks like checking the provided input types, ensuring the spatial size is consistent across all items etc. Currently all these methods are private because they live in the `_utils.py` private module and they are not exposed via `__init__.py`. This is likely to push users of the new API to use private methods in their code or force them to write a lot of extra duplicate code to do basic operations.

### Alternatives

We can leave the methods private (or even prefix them with `_`, though this seems an overkill because they are not exposed via any public module) and let users write their own utilities.

### Additional context

_No response_

cc @vfdev-5 @bjuncek @pmeier"
not all prototype transforms are serializable,pytorch/vision,2022-10-10 08:08:16,0,bug#module: transforms#prototype,6728,1402763601,"This is important for two reasons:

1. If we stick to `torchdata` for datasets v2, every object on the datapipe graph has to be serializable for multiprocessing. IIRC, this is only a requirement if forking from the main process is not possible, but that is the case on Windows.
2. Our classification references have the ability to cache the dataset which includes the transformations:

https://github.com/pytorch/vision/blob/6e203b44098c3371689f56abc17b7c02bd51a261/references/video_classification/train.py#L189

So far we don't have a unified test framework for our prototype transforms and thus it is not easy to add tests for all transforms. Note that we don't need tests for the functional kernels and dispatchers, since they are already tested for JIT scriptability and that protocol is a lot stricter than pickling.

cc @vfdev-5 @datumbox @bjuncek"
Unify datasets cache path from references with regular PyTorch cache?,pytorch/vision,2022-10-10 07:58:05,4,enhancement#module: reference scripts#topic: classification,6727,1402751047,"In the `classification` and `video_classification` references, we cache here:

- https://github.com/pytorch/vision/blob/6e203b44098c3371689f56abc17b7c02bd51a261/references/classification/train.py#L108
- https://github.com/pytorch/vision/blob/6e203b44098c3371689f56abc17b7c02bd51a261/references/video_classification/train.py#L124

However, this directory is not used by PyTorch core. Instead, `~/.cache/torch` is used. For example, `torch.hub` caches in [`~/.cache/torch/hub`](https://github.com/pytorch/pytorch/blob/c6b7c33885eeff9dc125f87c7134772d59d0ba21/torch/hub.py#L326). The datasets v2 used the same root folder and will store the datasets by default in 

https://github.com/pytorch/vision/blob/6e203b44098c3371689f56abc17b7c02bd51a261/torchvision/_internally_replaced_utils.py#L7

which expands to `~/.cache/torch/datasets/vision`. 

Maybe we can use `~/.cache/torch/cached_datasets` or something similar as cache path in the references?

cc @datumbox @vfdev-5"
New Feature: Mixup Transform for Object Detection ,pytorch/vision,2022-10-07 12:18:22,0,module: transforms#topic: object detection,6720,1401086036,"### 🚀 The feature

Follow up to #6323 

Add a new transform: Mixup to the transforms in torchvision
For reference: [Mixup for Detection](https://arxiv.org/pdf/1902.04103v1.pdf) [[1](https://github.com/Megvii-BaseDetection/YOLOX/blob/a5f629a6d28fcc3742ce9483698b3376ce457533/yolox/data/datasets/mosaicdetection.py#L162-L234), [2](https://github.com/open-mmlab/mmdetection/blob/1376e77e6ecbaad609f6003725158de24ed42e84/mmdet/datasets/pipelines/transforms.py#L2347)]

### Motivation, pitch

Keeping torchvision updated with the latest SotA techniques

### Alternatives

_No response_

### Additional context

_No response_

cc @vfdev-5 @datumbox"
[POC] Potential ways for making Transforms V2 classes JIT-scriptable,pytorch/vision,2022-10-05 16:24:45,1,module: transforms#prototype,6711,1398060690,"### 🚀 The feature

**Note: This is an exploratory proof-of-concept to discuss potential workarounds for offering limited support of JIT in our Transforms V2 Classes. I am NOT advocating for following this approach. I'm hoping we can kick off the discussion for other alternative and simpler approaches.**

Currently the Transforms V2 classes are not JIT-scriptable. This breaks BC and will make the rollout of the new API harder. Here are some of the choices that are incompatible with JIT:

1. We wanted to support arbitrary number of inputs.
2. We rely on Tensor Subclassing to do the dispatch to the right kernel. 
3. We use real typing information on the inputs which often includes types that are not scriptable.
4. We opted for using more Pythonic idioms (such as `for ... else`)

Points 3 & 4 could be addressed by (painful) refactoring, nevertheless points 1 & 2 are our main blockers. 

To ensure our users can still do inference using JIT, we offer presets/transforms attached to each model weights. Those will remain JIT-scriptable. In addition, we applied a workaround (#6553) to maintain the `F` dispatcher JIT-scriptable for plain `Tensors`. Hopefully these mitigations will help most users migrate easier to the new API.

But what if they don't? Many downstream users might want to continue relying on transforms such as `Resize`, `CenterCrop`, `Pad` etc for inference. In that case, one option could be to offer JIT-scriptable alternatives that work only for pure tensors. Another alternative is to write a utility that can modify the existing implementations on-the-fly to update key functions and make them JIT-scriptable.

### Motivation, pitch

This is a proof-of-concept of how such a utility can work. It only supports a handful of transforms (due to points 3 & 4 from above) but it can be extended to support more.

There are 2 approaches show-cased below:
1. We use `ast` to replace on-the-fly problematic idioms from the Transform classes. Since JIT also uses `ast` internally, we need to make the updated code available to JIT during scripting.
2. We replace the `forward()` to remove the packing/unpacking of arbitrary number of inputs. We also hardcode plain tensors as the only accepted input type.

```python
import ast
import inspect
import tempfile
import torch
import types

from torchvision import transforms as V1
from torchvision.prototype import transforms as V2
from torchvision.prototype import features


class JITWrapper(torch.nn.Module):

    def __init__(self, cls, *args, **kwargs):
        super().__init__()
        # Patch _transform types, can be avoided by defining directly JIT-scriptable types
        code = inspect.getsource(cls)
        tree = ast.parse(code)
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                node.name = f""{cls.__name__}JIT""
            elif isinstance(node, ast.FunctionDef):
                if node.name == ""_transform"":
                    node.args.args[1].annotation.id = ""features.InputTypeJIT""
                    node.returns.id = ""features.InputTypeJIT""
        source = ast.unparse(tree)

        # Writes the source on a temp file. Needed for JIT's inspect calls to work properly.
        with tempfile.NamedTemporaryFile(mode=""w+"", delete=False) as temp:
            temp.write(source)
            filename = temp.name

        # Compiles the new modified Class from source
        code = compile(source, filename, ""exec"")
        mod = {}
        exec(code, vars(inspect.getmodule(cls)), mod)
        cls = next(iter(mod.values()))

        # initialize transform
        transform = cls(*args, **kwargs)

        # Patch forward
        if hasattr(transform, ""_jit_forward""):
            # Use the one defined in the class if available
            transform.forward = transform._jit_forward
        else:
            # Use the default implementation
            setattr(transform, ""forward"", types.MethodType(JITWrapper.__default_jit_forward, transform))

        self._wrapped = transform

    @staticmethod
    def __default_jit_forward(self, inputs: features.InputTypeJIT) -> features.InputTypeJIT:
        params = self._get_params(inputs)
        result = self._transform(inputs, params)
        return result

    def forward(self, inputs: features.InputTypeJIT) -> features.InputTypeJIT:
        return self._wrapped.forward(inputs)


def assert_jit_scriptable(t, inpt):
    torch.manual_seed(0)
    eager_out = t(inpt)

    t_scripted = torch.jit.script(t)
    with tempfile.NamedTemporaryFile(delete=False) as temp:
        t_scripted.save(temp.name)
        t_scripted = torch.jit.load(temp.name)

    torch.manual_seed(0)
    script_out = t_scripted(inpt)
    torch.testing.assert_close(eager_out, script_out)
    return script_out


img = torch.randn((1, 3, 224, 224))

t = V1.Resize((32, 32))
out1 = assert_jit_scriptable(t, img)
print(""T1: OK"")

t = JITWrapper(V2.Resize, (32, 32))
out2 = assert_jit_scriptable(t, img)
print(""T2: OK"")

torch.testing.assert_close(out1, out2)
print(""T1 == T2: OK"")
```

The above works on our latest main without modifications:
```
T1: OK
T2: OK
T1 == T2: OK
```

This approach can currently only support a handful of simple Transforms, that don't require overwriting the `forward()` and that contain most of their logic inside their `_get_params()` and `_transform()` methods. Many such simple transforms are still not supported because they inherit from `_RandomApplyTransform` which does the random call in its forward (this could be refactored to move to `_get_params()`). The rest of the existing inference transforms can be supported by addressing points 3  & 4 from above. 

The above approach is very over-engineered, brittle and opaque because it tries to fix the JIT-scriptability issues without any modifications on the code-base for the selected example. If we accept minor refactoring on the existing classes, we can remove the `ast` logic. We could also avoid defining a default JIT-compatible forward by explicitly defining such a method on the original class when available. Here is one potential simplified version that would require changes on our current API:

```python
class JITWrapper(torch.nn.Module):

    def __init__(self, transform: Transform):
        super().__init__()
        # Patch forward
        if hasattr(transform, ""_jit_forward""):
            # Use the one defined in the class if available, should reuse `_get_params` and `_transform`
            transform.forward = transform._jit_forward
        else:
            raise Exception(f""The {cls.__name__} transform doesn't support scripting"")

        self._wrapped = transform

    def forward(self, inputs: features.InputTypeJIT) -> features.InputTypeJIT:
        return self._wrapped.forward(inputs)


class Resize(Transform):
    # __init__ and _get_params() goes here

    def _transform(self, inpt: features.InputTypeJIT, params: Dict[str, Any]) -> features.InputTypeJIT:
        # we changed the types. Everything else in the method should be the same

    def _jit_forward(self, inputs: features.InputTypeJIT) -> features.InputTypeJIT:
        params = self._get_params(inputs)
        result = self._transform(inputs, params)
        return result

```



### Alternatives

There are several other alternatives we could follow. One of them could be to offer JIT-scriptable versions for a limited number of Transforms that are commonly used during inference. Another one could be to make some of our transforms FX-traceable instead of JIT-scriptable. Though not all classes can become traceable (because their behaviour branches based on the input), considering making them compatible will future proof us for PyTorch 2.

### Additional context

_No response_

cc @vfdev-5 @bjuncek @pmeier"
I cannot build vision from source because problem with ninja,pytorch/vision,2022-10-05 13:41:50,1,topic: build#module: rocm,6707,1397817279,"i cannot build aur python-torchvision-rocm in manjaro because

Emitting ninja build file /var/tmp/pamac-build-chameleon/python-torchvision-rocm/src/vision-0.13.1/build/temp.linux-x86_64-3.10/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
...
[2/32] **bin/hipcc**  -DWITH_HIP ...
/bin/sh: строка 1: bin/hipcc: Нет такого файла или каталога

in build.ninja

ninja_required_version = 1.3
cxx = g++
nvcc = bin/hipcc

but path to nvcc must be /opt/rocm/bin/hipcc !

how to change this? and where?

cc @jeffdaily @jithunnair-amd"
Implement ability to toggle inplace activation operations in the implemented architectures,pytorch/vision,2022-10-04 15:44:41,3,enhancement#module: models,6699,1396494532,"### 🚀 The feature

A lot of the available architectures like AlexNet, EfficientNet, ResNet, DenseNet, etc. are implemented with inplace activation operations, e.g.:
```python
nn.ReLU(inplace=True)
```
This is done in the name of saving memory (#807)

This could be toggled using a parameter for the model-building functions.

### Motivation, pitch

Often, this presents a problem when trying to register register full backward hooks, as you get errors like the following:

> UserWarning: Output 0 of BackwardHookFunctionBackward is a view and is being modified inplace. This view was created inside a custom Function (or because an input was returned as-is) and the autograd logic to handle view+inplace would override the custom backward associated with the custom Function, leading to incorrect gradients. This behavior is deprecated and will be forbidden starting version 1.6. You can remove this warning by cloning the output of the custom Function. (Triggered internally at /pytorch/torch/csrc/autograd/variable.cpp:547.)

This is annoying when trying to use libraries like [Captum](https://captum.ai/), which need to attach these hooks in layer attribution methods like [LRP](https://github.com/pytorch/captum/blob/50b2e981ab0dad77434c98496f23aebc4822eb98/captum/attr/_core/layer/layer_lrp.py#L49):
> Model cannot contain any in-place nonlinear submodules; these are not supported by the register_full_backward_hook PyTorch API starting from PyTorch v1.9.

Adding a parameter (something like `inplace_activations`, defaulted to `True`) would allow to maintain compatibility with old code while providing an option for anyone that needs it.

### Alternatives

Currently, my only option is to copy the model code in question (e.g. [ResNet](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)) and substituting every `inplace=True` with `inplace=False`. This approach works, but it is not really scalable when trying to benchmark as many architectures as possible.

There may be other implementation approaches that libraries like Captum could take in order to be able to implement these methods to work with inplace operations. Nonetheless, the proposed solution appears more simple and would allow a lot of already implemented methods to work out of the box with the standard implementations in torchvision.

### Additional context

I'm willing to implement this myself, but I'm having trouble setting up a torchvision development environment in Arch. I might need additional help for that.

cc @datumbox"
New datasets for human pose estimation,pytorch/vision,2022-10-04 14:34:27,6,module: datasets#new feature,6698,1396391082,"### 🚀 The feature

We propose to add the following datasets for human pose estimation to torchvision:

* https://www.robots.ox.ac.uk/~vgg/data/pose/
* https://www.robots.ox.ac.uk/~vgg/data/pose_evaluation/
* https://www.robots.ox.ac.uk/~vgg/data/tv_human_interactions/



### Motivation, pitch

These are some of our most downloaded datasets and we're trying to make them more useful by providing DataLoaders built into torchvision. Torchvision already includes some of our datasets (`OxfordIIITPet`, `VOCDetection`, `VOCSegmentation`, `Flowers102`, `DTD`, and `FGVCAircraft`) so we hope that the proposed new datastes would be accepted.

### Alternatives

_No response_

### Additional context

_No response_

cc @pmeier @NicolasHug "
Keeping track of possible issues and PR for release v0.14,pytorch/vision,2022-10-04 10:16:09,0,release-issue,6692,1396006215,"## Release Process

- [x] Create release branch `release/0.14`
- [x] Create release tracker issue: https://github.com/pytorch/vision/issues/6686
- [x] Remove prototypes from release branch: https://github.com/pytorch/vision/pull/6687
- [ ] Clean up live documentations
- [x] Bump the versions on the main branch: https://github.com/pytorch/vision/pull/6691
- [x] Provide release branch to the release engineer and make sure to change dependency from nightly to RC pytorch core
- [ ] After the release of iOS binaries, update the `ios/LibTorchvision.podspec` file on main.


## Known problem / bug

- [x] Fix video clips format bug: https://github.com/pytorch/vision/pull/6672
- [x] Gallery rendering issue: https://github.com/pytorch/vision/issues/6679"
[v0.14] Release Tracker,pytorch/vision,2022-10-04 01:26:56,6,,6686,1395542463,"We cut a [release branch](https://github.com/pytorch/vision/tree/release/0.14) for the 0.14 release. (common commit  with main is 07ae61bf9c21ddd1d5f65d326aa9636849b383ca)

This issue is for tracking cherry-picks to the release branch.
The POCs for the TorchVision team are @YosuaMichael and @datumbox 

## Cherry-Pick Criteria

**Phase 1 (until 10/10/22):**

Only low-risk changes may be cherry-picked from master:

1. Fixes to regressions against the most recent minor release (e.g. 0.13 for 0.14 release); 
2. Critical fixes for: [silent correctness](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+correctness+%28silent%29%22), [backwards compatibility](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+bc-breaking%22+), [crashes](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+crash%22+), [deadlocks](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+deadlock%22+), (large) [memory leaks](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+memory+usage%22+)
3. Fixes to new features introduced in the most recent minor release (e.g. 0.13 for 0.14 release)
4. Test/CI fixes
5. Documentation improvements
6. Compilation fixes or ifdefs required for different versions of the compilers or third-party libraries
7. Release branch specific changes (e.g. change version identifiers)

**Phase 2 (after 10/10/22):**

Note that changes here require us to rebuild a Release Candidate and restart extended testing (likely delaying the release). Therefore, the only accepted changes are **Release-blocking** critical fixes for: [silent correctness](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+correctness+%28silent%29%22), [backwards compatibility](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+bc-breaking%22+), [crashes](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+crash%22+), [deadlocks](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+deadlock%22+), (large) [memory leaks](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22topic%3A+memory+usage%22+)

Changes will likely require a discussion with the larger release team over VC or Slack.

## Cherry-Pick Process

1. Ensure your PR has landed in master. This does not apply for release-branch specific changes (see Phase 1 criteria).
2. Create a PR against the [release branch](https://github.com/pytorch/vision/tree/release/0.14).
   <details>

    ```bash
    # Find the hash of the commit you want to cherry pick
    # (for example, abcdef12345)
    git log

    git fetch origin release/0.14
    git checkout release/0.14
    git cherry-pick abcdef12345

    # Submit a PR based against 'release/0.14' either:
    # via the GitHub UI
    git push my-fork

    # via the GitHub CLI
    gh pr create --base release/0.14
    ```

    </details>
3. Make a request below with the following format:

```
Link to landed master PR (if applicable):
* 

Link to release branch PR:
* 

Criteria Category:
* 
```

1. the POC will reply with approved / denied or ask for more information.
2. If approved, the PR can be merged once the tests pass.

**NOTE: Our normal tools (ghstack / ghimport, etc.) do not work on the release branch.**

Please note HUD Link with branch CI status and link to the HUD to be provided here.
[HUD](https://hud2.pytorch.org/hud/pytorch/vision/release%2F0.14/0)

### Versions

0.14"
"""reflect"" padding used in Gaussian Blur Transform throws error on smaller images",pytorch/vision,2022-10-03 17:19:11,0,module: transforms,6683,1395067705,"### 🐛 Describe the bug

Because of the set `reflect` mode in `torch_pad`, the `GaussianBlur` throws exception for smaller images.

```python
from PIL import Image
import numpy as np
from torchvision import transforms

im  = Image.new(""L"", (1, 1), 255)
transforms.GaussianBlur(3, sigma=(0.1, 4.0))(im)
```

We should either have a parameter to set the padding mode or 'reflect' should not be used when image size is smaller than kernel size.

Thanks.

### Versions

PyTorch version: 1.12.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.15.0-1020-aws-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 515.65.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.3
[pip3] torch==1.12.1
[pip3] torch-summary==1.4.5
[pip3] torchvision==0.13.1
[conda] numpy                     1.23.3                   pypi_0    pypi
[conda] torch                     1.12.1                   pypi_0    pypi
[conda] torch-summary             1.4.5                    pypi_0    pypi
[conda] torchvision               0.13.1                   pypi_0    pypi

cc @vfdev-5 @datumbox"
The name folder in the compacted file '2021_train.tar.tgz' (inaturalist 2021 train) is 'train',pytorch/vision,2022-10-01 20:56:13,1,bug#help wanted#module: datasets,6674,1393548879,"### 🐛 Describe the bug

When trying to use the iNaturalist 2021_train version the program crashes (after downloading) with the message:

`Unable to find downloaded files at ...`

The md5sum from the downloaded file **2021_train.tar.tgz** is correct, but the name of the extracted folder is **train**.

Checking the code at [this](https://github.com/pytorch/vision/blob/d7d90f56117ce0955332846a5f90b8d1346c4c09/torchvision/datasets/inaturalist.py#L237) line it seems that the name of the extracted folder should be **2021_train**. So it seems to be an issue with the compacted file **2021_train.tar.tgz**.

### Versions

Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: 10.0.0-4ubuntu1
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:57:06)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.15.0-46-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.6.112
GPU models and configuration:
GPU 0: NVIDIA RTX A5000
GPU 1: NVIDIA RTX A5000

Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.2
[pip3] torch==1.11.0
[pip3] torch-tb-profiler==0.4.0
[pip3] torchaudio==0.11.0
[pip3] torchsample==0.1.3
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.3.1               h2bc3f7f_2
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640
[conda] mkl-service               2.4.0            py38h95df7f1_0    conda-forge
[conda] mkl_fft                   1.3.1            py38h8666266_1    conda-forge
[conda] mkl_random                1.2.2            py38h1abd341_0    conda-forge
[conda] numpy                     1.19.5                   pypi_0    pypi
[conda] numpy-base                1.21.2           py38h79a1101_0
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch                     1.6.0                    pypi_0    pypi
[conda] torch-tb-profiler         0.4.0                    pypi_0    pypi
[conda] torchaudio                0.11.0               py38_cu113    pytorch
[conda] torchsample               0.1.3                     dev_0    <develop>
[conda] torchvision               0.7.0                    pypi_0    pypi

cc @pmeier"
Request For New Gallery Examples,pytorch/vision,2022-10-01 17:49:33,2,module: documentation,6673,1393491203,"### 📚 The doc issue

This is an issue where end users can request for new gallery examples and tutorials.

New Examples needed.

- [ ] Add example for Video Action Recognition using 3d models such as mvit.



### To The Visitors

1. Note that this would involve community contributions and contributions from OSS community are welcome. The core maintainers might be busy and unable to add as per their workload. So, expect some delays.

2. Feel free to comment tutorials or examples you would like! Also add a clear and concise description of why it is needed. 

3. Note that each examples needs a demand and valid use case. As every tutorial will require maintenance!

"
Use correct annotations for prototype kernels?,pytorch/vision,2022-09-29 18:53:07,0,module: transforms#code quality#prototype,6668,1391343120,"Currently, we have a lot of functionals that annotate a parameter with `List[int]`

https://github.com/pytorch/vision/blob/ae83c9fdb6e8e0dc92facc0a763a3dd424df78de/torchvision/prototype/transforms/functional/_geometry.py#L106-L108

This annotation is wrong and the function actually handles `Union[int, Tuple[int], Tuple[int, int], List[int]]`, i.e. integers as well as tuples and lists of one or two integers. Our preferred annotation for this is `Union[int, Tuple[int, int]]`.

The wrong annotation is a remnant from a time, where JIT didn't support `Union`. The eager kernel supported integers all along. To emulate this behavior, we allowed sequences of one element to mean the same thing.

Our preferred annotation `Union[int, Tuple[int, int]]` communicates our intention a lot better: you can either pass a an integer or two of them. Since this is Python, one can of course use a list or any sequence of two elements as duck type for `Tuple[int, int]`. However, this won't work with JIT anymore. This discussion came up, because we trusted the wrong annotations while developing, which lead to bugs (#6636). Plus, in #6646 we added some metadata computation to the kernels and there was a discussion about if we should have different input `List[int]` vs. output types `Tuple[int, int]`. With our preferred annotation, the input type is still more diverse `Union[int, Tuple[int, int]]`, but that is a common idiom to allow the input to be less precise than the output.

Our prototype functional API is separated into two things: kernels and dispatchers. The actual computation is happening in the former, while the latter only dispatches the input to the right kernel based on its type. In the stable API has only the dispatchers, which gives us the freedom to choose what we want to do with the kernels without BC concerns. 

Thus, before we roll-out, we need to decide if we want to continue with the wrong annotations when they are no longer needed or break consistency between kernels and dispatchers.

## Status quo

Currently, we are using the same wrong annotation for kernel and dispatcher. That means the elaborate parsing of the parameter has to happen on the kernel:

```py
from typing import List, Tuple, Union
import torch.jit

InputSizeType = Union[int, Tuple[int, int]]
InputSizeTypeJIT = List[int]
OutputSizeType = Tuple[int, int]


def parse_input_size(size: InputSizeTypeJIT) -> OutputSizeType:
    if isinstance(size, int):
        height = width = size
    elif len(size) == 1:
        height = width = size[0]
    elif len(size) == 2:
        height, width = size
    else:
        raise ValueError
    return height, width


def eager_kernel1(size: InputSizeTypeJIT) -> OutputSizeType:
    height, width = parse_input_size(size)

    # some computation that possibly changes height and width

    return height, width


scripted_kernel1 = torch.jit.script(eager_kernel1)


def eager_dispatcher1(size: InputSizeTypeJIT) -> OutputSizeType:
    return eager_kernel1(size)


scripted_dispatcher1 = torch.jit.script(eager_dispatcher1)
```

## Proposal

We could also annotate the kernel correctly

```py
def eager_kernel2(size: InputSizeType) -> OutputSizeType:
    if isinstance(size, int):
        height = width = size
    else:
        height, width = size

    # some computation that possibly changes height and width

    return height, width
```

and perform the parsing only on the dispatchers:

```py
def eager_dispatcher2(size: InputSizeTypeJIT) -> OutputSizeType:
    return eager_kernel2(parse_input_size(size))


scripted_dispatcher2 = torch.jit.script(eager_dispatcher2)
```

The reason we keep the wrong annotations on the dispatcher, is that annotations play a role in BC in the context of JIT: imagine someone scripted our operators and used them from C++. They passed the equivalent of a list of two elements and the operator worked. If we now change the annotation, the operator can still be scripted, but the old inputs will no longer work.

## Implications

- By design, kernel2 is more restrictive than kernel1, but has correct annotations. In eager mode, duck typing still works.
- In contrast to the previous annotations, integers are supported by kernel2 while scripting

```py
import itertools

for input_size, kernel in itertools.product(
    [0, (0,), (0, 0), [0], [0, 0]],
    [eager_kernel1, scripted_kernel1, eager_kernel2, scripted_kernel2],
):
    # Integers are not supported when annotation `List[int]`. 
    # `Tuple[int]` and `Tuple[int, int]` seem to be handled automagically
    if kernel is scripted_kernel1 and isinstance(input_size, int):
        continue

    # Sequences of other than two elements are no longer supported. 
    # `[0, 0]` can still be used as duck type for `Tuple[int, int]`.
    if kernel is eager_kernel2 and isinstance(input_size, (list, tuple)) and len(input_size) != 2:
        continue

    # Only `Union[int, Tuple[int, int]]` is supported
    if kernel is scripted_kernel2 and not (
        isinstance(input_size, int) or isinstance(input_size, tuple) and len(input_size) == 2
    ):
        continue

    assert kernel(input_size) == (0, 0)
```

- Dispatchers are fully BC

```py
for input_size, dispatcher in itertools.product(
    [0, (0,), (0, 0), [0], [0, 0]],
    [
        eager_dispatcher1,
        scripted_dispatcher1,
        eager_dispatcher2,
        scripted_dispatcher2,
    ],
):
    if isinstance(input_size, int) and dispatcher in {scripted_dispatcher1, scripted_dispatcher2}:
        continue

    assert dispatcher(input_size) == (0, 0)
```

## Affected kernels and parameters

| kernel            | parameter            | old annotation          | new annotation                                                  |
|-------------------|----------------------|-------------------------|-----------------------------------------------------------------|
| `resize_*`        | `size`               | `List[int]`             | `Union[int, Tuple[int, int]]`                                   |
| `affine_*`        | `translate`          | `List[float]`           | `Union[float, Tuple[float, float]]`                             |
|                   | `shear`              | `List[float]`           | `Tuple[float, float]`                                           |
|                   | `center`             | `Optional[List[float]]` | `Optional[Tuple[float, float]]`                                 |
| `rotate_*`        | `center`             | `Optional[List[float]]` | `Optional[Tuple[float, float]]`                                 |
| `pad_*`           | `padding`            | `List[int]`             | `Union[int, Tuple[int, int], Tuple[int, int, int, int]]`        |
| `perspective_*`   | `perspective_coeffs` | `List[float]`           | `Tuple[float, float, float, float, float, float, float, float]` |
| `center_crop_*`   | `output_size`        | `List[int]`             | `Union[int, Tuple[int, int]]`                                   |
| `resized_crop_*`  | `size`               | `List[int]`             | `Union[int, Tuple[int, int]]`                                   |
| `five_crop_*`     | `size`               | `List[int]`             | `Union[int, Tuple[int, int]]`                                   |
| `ten_crop_*`      | `size`               | `List[int]`             | `Union[int, Tuple[int, int]]`                                   |
| `gaussian_blur_*` | `kernel_size`        | `List[int]`             | `Union[int, Tuple[int, int]]`                                   |
|                   | `sigma`              | `Optional[List[float]]` | `Optional[Union[float, Tuple[float, float]]]`                   |

## Unaffected parameters

Using `List[int]` or `List[float]` is not always the wrong choice. If we can't know the length statically, we should use it. Examples are the `fill` parameter for example on `resize_*` or `mean` and `std` on `normalize_*`:

https://github.com/pytorch/vision/blob/2907c4945abb1e0c3bc5e04b2460170338c9a5fd/torchvision/prototype/transforms/functional/_misc.py#L12-L14

## Conclusion

We can use correct annotations on the kernels. This makes the API inconsistent, but does not break BC since the kernels were not public before. IMO, wrong annotations are worse than an inconsistent API and thus I'm advocating for this change.

cc @vfdev-5 @datumbox @bjuncek"
compatibility layer between stable datasets and prototype transforms?,pytorch/vision,2022-09-28 15:22:12,7,module: datasets#module: transforms#new feature#prototype,6662,1389551136,"The original plan was to roll out the datasets and transforms revamp at the same time since they somewhat depend on each other. However, it is becoming more and more likely that the prototype transforms will be finished sooner. Thus, we need some compatibility layer in the meantime. This issue explains how transforms are currently used with the datasets, what will or will not work without a compatibility layer, and how such a compatibility layer might look like.

## Status quo

Most of our datasets support the `transform` and `target_transform` idiom. These transformations are applied separately to the first and second item of the raw sample returned by the dataset. For classification tasks this usually sufficient although I've never seen a practical use for `target_transform`:

https://github.com/pytorch/vision/blob/0fcfaa13d1a1402cc9788dbaeeadc47032cf214b/references/classification/train.py#L129-L137

However, the separation of the transforms breaks down in case image and label need to be transformed at the same time, e.g. `CutMix` or `MixUp`. They are currently applied through a custom collation function for the dataloader:

https://github.com/pytorch/vision/blob/3c9ae0ac5dda3d881a6ce5004ce5756ae7de7bc4/references/classification/train.py#L209-L210

Since these transforms do not work with the standard idioms, they never made it out of our references into the library.

The need to transform input and target at the same time is not a special case for other tasks such as segmentation or detection. Datasets for these tasks support the `transforms` parameter. It will be called with the complete sample and thus is able to support all use cases. 

Since even datasets for the same task have very diverse outputs, there were only two options without revamping the APIs completely:

1. Unify the datasets outputs on the dataset itself.
2. Unify the datasets outputs through a compatibility layer.

When this first came up in the past, we went with option 2. On our references we unified the output for a few select datasets for a specific task, so we can apply custom joint transformations to them. Since we didn't want to commit to the interface, neither the minimal compatibility layer nor the transformations made it into the library. Thus, although some of our datasets in theory support joint transformations, the users have to implement them themselves.   

## Do we need a compatibility layer?

The new transformations support the joint use case out of the box. Meaning, all the custom transformations from our references are now part of the library. Plus, all transformations that previously only supported images, e.g. resizing or padding, now also support bounding boxes, masks and so on.

The information which part of the sample is what kind of type is not communicated through the sample structure, i.e. first element is an image and second one is a mask, but rather through the actual type of the object. We introduced several tensor subclasses that will be rolled out together with the transforms.

By treating simple tensors, i.e. not the new subclasses, as images, the new transformations are full BC[^1]. Thus, if you previously only used the separated `transform` and `target_transform` idiom you can continue to do that and the new transforms will not get into your way:

```py
import torch
from torchvision import datasets
from torchvision.prototype import transforms

transform = transforms.Compose(
    [
        transforms.PILToTensor(),
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.ImageNet(..., transform=transform)

image, label = dataset[0]
assert isinstance(image, torch.Tensor)
assert image.shape[-2:] == (224, 224)
assert isinstance(label, int)
```

The transforms also work out of the box if you want to stick to PIL images:

```py
import PIL.Image
from torchvision import datasets
from torchvision.prototype import transforms

transform = transforms.Compose(
    [
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.ImageNet(..., transform=transform)

image, label = dataset[0]
assert isinstance(image, PIL.Image.Image)
assert image.size == (224, 224)
assert isinstance(label, int)
```

Although it seems the new transforms can also be used out of the box if the dataset supports the `transforms` parameter, this unfortunately not the case. While the new datasets will provide the sample parts wrapped into the new tensor subclasses, the old datasets, i.e. the only ones available during the roll-out of the new transforms, do not. 

Without the wrapping, the transform does not pick up on bounding boxes and subsequently does not transform them:

```py
import torch
import PIL.Image
from torchvision import datasets
from torchvision.prototype import transforms

transform = transforms.Compose(
    [
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.CocoDetection(..., transforms=transform)

image, target = dataset[0]
assert isinstance(image, PIL.Image.Image)
assert image.size == (224, 224)

assert len(target) == 8

bbox = target[2][""bbox""]
# bounding boxes were not downsized and thus are now out of sync with the image
torch.testing.assert_close([int(coord) for coord in target[2][""bbox""]], [249, 229, 316, 245])

segmentation = target[2][""segmentation""]
# masks were not downsized and thus are now out of sync with the image. Plus, they still encoded and the user has to
# decode them themselves
assert isinstance(segmentation, list) and all(isinstance(item, (int, float)) for item in segmentation)
```

Masks will be transformed, but without wrapping they will be treated as normal images. This means, by default `InterpolationMode.BILINEAR` is used for interpolation, which will corrupt the information:

```py
import torch
from torchvision import datasets
from torchvision.prototype import transforms

transform = transforms.Compose(
    [
        transforms.PILToTensor(),
        # we convert to float here to make the bilinear interpolation visible
        transforms.ConvertImageDtype(torch.float64),
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.VOCSegmentation(..., transforms=transform)

image, mask = dataset[0]
assert isinstance(image, torch.Tensor)
assert image.shape[-2:] == (224, 224)
assert isinstance(mask, torch.Tensor)
assert mask.shape[-2:] == (224, 224)
# If the interpolation worked correctly, we would only see integer values in the uint8 range of [0, 255]
assert torch.any(torch.fmod(mask * 255, 1) > 0)
```

Thus, if we don't provide a compatibility layer until our datasets wrap automatically, the prototype transforms don't bring any real benefit to the user of our datasets.

## Proposal

I propose to provide a thin wrapper for the datasets that does nothing else than wrapping the returned samples into the new tensor subclasses. This means, that the new object behaves exactly as the dataset as before, but upon accessing an element, i.e. calling `__getitem__`, we wrap the samples before passing them into the transforms.

```py
from torchvision import datasets
from torchvision.prototype import transforms, features

transform = transforms.Compose(
    [
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.ImageNet(..., transform=transform)
dataset = features.VisionDatasetFeatureWrapper.from_torchvision_dataset(dataset)

image, label = dataset[0]
assert isinstance(image, features.Image)
assert image.image_size == (224, 224)
assert isinstance(label, features.Label)
assert label.to_categories() == ""tench, Tinca tinca""
```

Going back to the segmentation example from above, with the wrapper in place the segmentation mask is now correctly
interpolated with `InterpolationMode.NEAREST`:

```py
import torch
from torchvision import datasets
from torchvision.prototype import transforms, features

transform = transforms.Compose(
    [
        # we convert to float here to make the bilinear interpolation visible
        transforms.ToDtype(torch.float64, features.Mask),
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.VOCSegmentation(..., transforms=transform)
dataset = features.VisionDatasetFeatureWrapper.from_torchvision_dataset(dataset)

image, mask = dataset[0]
assert isinstance(mask, torch.Tensor)
assert mask.shape[-2:] == (224, 224)
assert not torch.any(torch.fmod(mask * 255, 1) > 0)
```

In general, the wrapper should not change the structure of the sample unless it is necessary to be able to properly use 
the new transformations. For example, the `target` of `COCODetection` is a list of dictionaries, in which each 
dictionary holds the information for one object. Our models however require a dictionary where the value of the 
bounding box key is a `(N, 4)` tensor, where `N` is the number of objects. Furthermore, while our basic transform can 
work with individual bounding boxes, more elaborate ones that we ported from the reference scripts also require this 
format.

Thus, if needed, we also perform this collation inside the wrapper:

```py
import torch
from torchvision import datasets
from torchvision.prototype import transforms, features

transform = transforms.Compose(
    [
        transforms.Resize(256),
        transforms.CenterCrop(224),
    ]
)
dataset = datasets.CocoDetection(..., transforms=transform)
dataset = features.VisionDatasetFeatureWrapper.from_torchvision_dataset(dataset)

image, target = dataset[0]

assert isinstance(image, features.Image)
assert image.shape[-2:] == (224, 224)

bbox = target[""bbox""]
assert isinstance(bbox, features.BoundingBox)
assert bbox.shape == (8, 4)
torch.testing.assert_close(bbox[2].int().tolist(), [116, 106, 152, 114])
```

Furthermore, if the data is in an encoded state, like the masks the `COCODetection` provides, will be decoded so they can be used directly by the transforms and models:

```py
segmentation = target[""segmentation""]
assert isinstance(segmentation, features.Mask)
assert segmentation.shape == (8, 224, 224)
```

The `VisionDatasetFeatureWrapper` class in the examples above is implemented as a proof of concept in #6663.

## Conclusion

If we don't roll out the new datasets at the same time as the new transformations, the transformations on their own will bring little value to the user. Their whole power can only be unleashed if we add a thin compatibility layer between them and the ""old"" datasets. I've proposed an, IMO clean, implementation for such a compatibility layer.

[^1]: Fully BC for what is discussed here. The only thing that will be BC breaking is that the new transforms will no longer be `torch.jit.script`'able whereas they were before.

cc @vfdev-5 @datumbox @bjuncek"
link system ffmpeg to torchvision. Don't install ffmpeg that comes with torchvision,pytorch/vision,2022-09-27 17:56:36,2,module: video#topic: binaries,6656,1388152026,"### 🐛 Describe the bug

I want to install (via conda) torchvision without installing the ffmpeg that comes with it. I already have a hardware accelerated ffmpeg build on my computer and would like to keep using it. can you give me some idea on how I can do this with torchvision? 

What I tried:
I tried to build from source but that requires pytorch nightly which clashes with other libraries that depend on pytorch 0.12 so I can't use pytorch nightly. 

### Versions

yTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0
Libc version: glibc-2.27

Python version: 3.9.13 (main, Aug 25 2022, 23:26:10)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-48-generic-x86_64-with-glibc2.27
Is CUDA available: True
CUDA runtime version: 11.3.122
GPU models and configuration: 
GPU 0: NVIDIA RTX A5000
GPU 1: NVIDIA RTX A5000
GPU 2: NVIDIA RTX A5000
GPU 3: NVIDIA RTX A5000

Nvidia driver version: 470.141.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.3
[pip3] pytorch-lightning==1.6.0
[pip3] pytorchvideo==0.1.5
[pip3] torch==1.13.0.dev20220914+cu113
[pip3] torchaudio==0.12.1
[pip3] torchmetrics==0.7.3
[pip3] torchvision==0.13.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py39h7f8727e_0  
[conda] mkl_fft                   1.3.1            py39hd3c417c_0  
[conda] mkl_random                1.2.2            py39h51133e4_0  
[conda] numpy                     1.23.3                   pypi_0    pypi
[conda] numpy-base                1.23.1           py39ha15fc14_0  
[conda] pytorch                   1.12.1          py3.9_cuda11.3_cudnn8.3.2_0    pytorch
[conda] pytorch-lightning         1.6.0                    pypi_0    pypi
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] pytorchvideo              0.1.5                    pypi_0    pypi
[conda] torch                     1.13.0.dev20220914+cu113          pypi_0    pypi
[conda] torchaudio                0.12.1               py39_cu113    pytorch
[conda] torchmetrics              0.7.3                    pypi_0    pypi
[conda] torchvision               0.13.1                   pypi_0    pypi"
"Allow ""nearest-exact"" interpolation mode?",pytorch/vision,2022-09-26 07:56:19,0,module: transforms#new feature#deprecation,6645,1385607846,"Currently the following interpolation modes are allowed 

https://github.com/pytorch/vision/blob/7046e56fe4370e94339b3e8b6fd011e285294a3a/torchvision/transforms/functional.py#L21-L32

Since `torch==1.11.0` (pytorch/pytorch#64501 of @vfdev-5 to be exact), `torch.nn.functional` also supports `mode=""nearest-exact""`:

> Mode `mode='nearest-exact'` matches Scikit-Image and PIL nearest neighbours interpolation algorithms and fixes known issues with `mode='nearest'`. This mode is introduced to keep backward compatibility. Mode `mode='nearest'` matches buggy OpenCV's `INTER_NEAREST` interpolation algorithm.

Given that we are aligning more with PIL and `""nearest""` is described as ""buggy"", can we add support for `""nearest-exact""`? 

If yes, we should also think about changing all our default values to it. That might be a bit cumbersome for the users, but we could also remap its name. Meaning after the whole deprecation period is through, `""nearest""` just maps to `""nearest-exact""` of `interpolate` and we have a `""nearest-legacy""` or the like that maps to `""nearest""`. We already do a name mapping for other interpolation modes:

https://github.com/pytorch/vision/blob/7046e56fe4370e94339b3e8b6fd011e285294a3a/torchvision/transforms/functional_tensor.py#L412-L414

Deprecation process could look like this where `r` denotes the current release.

- `r+1`: add `""nearest-exact""` as valid interpolation mode and deprecate not passing a value explicitly. Plus, this should add a `""nearest-legacy""` that aliases the current `""nearest""`. We also need to warn if `""nearest""` is passed that the behavior will change in the future.

    ```py
    def foo(..., mode=None):
        if mode is None:
            warnings.warn(""Not passing a default value is deprecated."")
            mode = ""nearest-legacy""
        elif mode == ""nearest"":
            warnings.warn(
                ""Nearest is deprecated. Use nearest-legacy for the current behavior ""
                ""or nearest-exact for the future behavior""
            )
    ```
    
    
- `r+2`: fail if interpolation mode is not passed explicitly or `""nearest""` is passed.
- `r+3`: re-introduce `""nearest""` as default value for the interpolation mode, but map it internally to `""nearest-exact""`.

cc @vfdev-5 @datumbox"
[BC-breaking] Aligning `fill = None` with `fill = 0` on Transforms,pytorch/vision,2022-09-21 12:33:00,5,needs discussion#module: transforms#bc-breaking,6623,1380857554,"### 🚀 The feature

As discussed at https://github.com/pytorch/vision/issues/6517, passing `None` on `fill` for some transforms isn't the same as passing `0`. The difference applies on some cases but not others, it was potentially done to avoid JIT issues (or is it a bug?) and has unforeseen effects on the behaviour of the `Transforms`.

Currently on the prototype area:
- All functionals have `fill=None` as default value.
- All Transforms have `fill=0` as default.
- All AutoAugment Transforms have `fill=None`.

The above inconsistent behaviour maintains BC with stable but leads to a weird API.

### Motivation, pitch

Break BC on the `functional` level and make `fill=None` behave similar to `fill=0`.

This unfortunately will have accuracy effects on models trained with the Tensor API. Worth noting that such a change would align closer with PIL which already sets `fill=None` to 0:
https://github.com/pytorch/vision/blob/56e707bfccb62ada836d21e431d6db0d10dd73a1/torchvision/transforms/functional_pil.py#L264-L265

This proposal could be adopted if we decide to break BC and align the `Tensor` and `PIL` backends on other aspects (such as `antialias` etc).

### Alternatives

There are 2 other potential alternatives:
- Keep the Transforms and functionals as-is and make no breaking changes. 
- Make a breaking change on the Transforms level (AutoAugment) but keep the functionals as-is. This will minimize the effect of the change on model inference for pre-trained models but maintain the discrepancy between the 2 backends.

### Additional context

_No response_

cc @vfdev-5"
[RFC] torchvision performance optimization on CPU,pytorch/vision,2022-09-21 04:56:04,9,,6619,1380316978,"## 🚀 The feature

This RFC is targeting at improving performance of operators from torchvision on CPU.


## Motivation, pitch

Generally performance improvements can be made in 3 ways:
* **channels last memory format support**: in torch 1.12, majority of commonly used operators in CV are enabled with channels last support. Enabling channels last support for native kernels in torchvision such as `RoiAlign` pooling could be beneficial because: a) first of all `RoiAlign` can be vectorized on NHWC (on NCHW or channels first memory format, it can only use scalar logic); b) secondly, `Conv2d` can save memory format reorders between PyTorch's plain format and mkldnn's blocked formats.
* **parallelization on multicore CPUs**: current native kernels from torchvision are sequential, which could not utilize all the resources on multicore CPUs.
* **BFloat16 support**: `BFloat16` takes half of the memory footprint of `float32`.

The plan is to cover both inference and training optimizations at the same time.

## Affected Operators
The optimization scope will cover the native kernels from [csrc/ops/cpu](https://github.com/pytorch/vision/tree/main/torchvision/csrc/ops/cpu), including:
* roi_align_kernel
* roi_pool_kernel
* ps_roi_align_kernel
* ps_roi_pool_kernel
* nms_kernel
* deform_conv2d_kernel

These operators will affect models such as `FasterRCNN`, `MaskedRCNN`, etc.

**[Discussion Needed]**: need to sort out the priorities of these kernels.

## API and Behavior Change

Since all the optimizations will be done on the kernel level, no API change will be required.

Users will be able to run models in `channels last` as recommended from [memory_format_tutorial](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html):

```python
### convert input and model from NCHW to NHWC
input = input.to(memory_format=torch.channels_last)
model = model.to(memory_format=torch.channels_last)
```

To run model in bfloat16 with explicit data type conversion or AMP:
```python
### explicit data type conversion
input = input.to(dtype=torch.bfloat16)
model = model.to(dtype=torch.bfloat16)

### with AMP
with torch.autocast(device_type=""cpu"", dtype=torch.bfloat16):
    output = model(input)
```

### Non-Batch Mode Input

Some models will have the input in non-batch mode e.g. CHW (N = 1), this can not be converted to channels last in torch at the moment:
```python
### when input is 3-dimensional tensor, the following line will receive a runtime error:
input = input.to(memory_format=torch.channels_last)
```
`torch.nn.conv2d` will check the memory format of `input` and `weight`, if either one of them is channels last, the convolution wil use channels last path. Therefore, for non-batch mode input, we can only converting the `model` and still channels last will be used.

This part requires special attention and validation effort.

## Parallelization on Multi Core CPUs

We propose to follow the identical parallelization scheme with torch, e.g. using the wrapper `at::parallel_for`. It can be linked to **OpenMP** or **TBB** depending on the build option (by default OpenMP will be used).

This [commit](https://github.com/pytorch/vision/commit/1fa27d0f07d3451384ec698d4cd7ee5f4575982b) is an example of paralleling `roi_align` on the 1st dimension of the input tensor, e.g. `n_rois`, with help of `at::parallel_for`.

```C
 at::parallel_for(0, n_rois, 1, [&](int begin, int end) {
    for (int n = begin; n < end; n++) {
      int index_n = n * channels * pooled_width * pooled_height;

      const T* offset_rois = rois + n * 5;
      int roi_batch_ind = offset_rois[0];

      /* rest of the function is identical to original kernel*/
```

## Vectorization on x86 CPUs

Vectorization can be done multiple ways, namely:

### Auto Vectorization

Let compiler automatically vectorize with `#pragma omp simd`, this [commit](https://github.com/pytorch/vision/commit/e50cd530a79f32ade7243ff68ba8d3adbbd6274d) adds channels last support for `roi_align` and did vectorization on the last dimension, e.g. `channels`:

```C
  for (int iy = 0; iy < roi_bin_grid_h; iy++) {
    for (int ix = 0; ix < roi_bin_grid_w; ix++) {
      detail::PreCalc<T> pc = pre_calc[pre_calc_index];
      const T* in1 = input + pc.pos1 * channels;
      const T* in2 = input + pc.pos2 * channels;
      const T* in3 = input + pc.pos3 * channels;
      const T* in4 = input + pc.pos4 * channels;

      #pragma omp simd
      for (int c = 0; c < channels; c++) {
        out[c] += pc.w1 * in1[c] + pc.w2 * in2[c] + pc.w3 * in3[c] + pc.w4 * in4[c];
      }
      pre_calc_index += 1;
    }
  }
```
Note that on NCHW, this kernel can not be vectorized.

* **pros**: easy to implement;
* **cons**: `BFloat16` can not be vectorized by compiler properly, which means if we choose this approach, `RoiAlign` won't have BFloat16 support and will be put into fallback list of AMP;

### Manual Vectorization

Vectorize the code via `at::vec::Vectorized<>` struct, which will be compiled to different assembly depending on arch, **avx2/avx512** or **neon**.

```C
  using Vec = at::vec::Vectorized<T>;
  for (int iy = 0; iy < roi_bin_grid_h; iy++) {
    for (int ix = 0; ix < roi_bin_grid_w; ix++) {
      detail::PreCalc<T> pc = pre_calc[pre_calc_index];
      const T* in1 = input + pc.pos1 * channels;
      const T* in2 = input + pc.pos2 * channels;
      const T* in3 = input + pc.pos3 * channels;
      const T* in4 = input + pc.pos4 * channels;

      int64_t d = 0;
      for (; d < channels - (channels % Vec::size()); d += Vec::size()) {
        Vec out_vec =
            Vec(pc.w1) * Vec::loadu(in1 + d) +
            Vec(pc.w2) * Vec::loadu(in2 + d) +
            Vec(pc.w3) * Vec::loadu(in3 + d) +
            Vec(pc.w4) * Vec::loadu(in4 + d);
        out_vec.store(out + d);
      }
      /* handle the remainder here ... */
      pre_calc_index += 1;
    }
  }
```

* **pros**: support `BFloat16` vectorization; cross platform support.
* **cons**: more effort will be needed to map the build options from torch to torchvision.

From performance point of view, these two approaches would have similar results.

**[Discussion Needed]**: need to decide which way to go.

## Experiment Results

A demo shows performance improvement with `channels last` support on model `fast_rcnn_R_50_FPN_1x` from `detectron2`:

```bash
export DETECTRON2_DATASETS=../datasets
python benchmark.py --config-file ../configs/COCO-Detection/fast_rcnn_R_50_FPN_1x.yaml --task eval
```

**torch**: 1.13.0a0
**torchvision**: 0.14.0a0
**detectron2**: 0.6
**cpu**: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz

time of 300 iters   (unit: s) | NCHW (before) | NCHW (after) | NHWC (after) | SpeedUp
-- | -- | -- | -- | --
single core (C=1) | 638.21 | 639.01 | 503.04 | 126.87%
single socket (C=20) | 212.10 | 141.06 | 102.54 | 206.84%

### Breakdown

Here is performance breakdown of NCHW (before) v.s. NHWC (after):

* NCHW (before)
```
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     aten::conv2d         0.32%     676.582ms        41.71%       88.386s       4.830ms         18300
                aten::convolution         0.05%     109.323ms        41.67%       88.300s       4.825ms         18300
               aten::_convolution         0.09%     183.509ms        41.61%       88.168s       4.818ms         18300
         aten::mkldnn_convolution        41.48%       87.890s        41.54%       88.018s       4.810ms         18300
           torchvision::roi_align        38.33%       81.228s        38.99%       82.621s      68.850ms          1200
                     aten::linear         0.00%       7.534ms         5.33%       11.291s       9.410ms          1200
                      aten::addmm         5.11%       10.821s         5.32%       11.272s       9.393ms          1200
                 aten::batch_norm         0.03%      64.973ms         4.51%        9.552s     600.729us         15900
     aten::_batch_norm_impl_index         0.05%     110.204ms         4.48%        9.502s     597.630us         15900
          aten::native_batch_norm         4.40%        9.314s         4.43%        9.396s     590.974us         15900
                       aten::add_         2.06%        4.372s         2.06%        4.372s     910.892us          4800
                      aten::relu_         0.04%      74.794ms         1.76%        3.733s     253.958us         14700
                 aten::clamp_min_         1.73%        3.669s         1.73%        3.669s     249.608us         14700
```

* NHWC (after)
```
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                     aten::conv2d         0.95%     970.076ms        61.52%       63.082s       3.447ms         18300
                aten::convolution         0.12%     121.816ms        61.45%       63.008s       3.443ms         18300
               aten::_convolution         0.14%     140.402ms        61.33%       62.890s       3.437ms         18300
         aten::mkldnn_convolution        60.99%       62.543s        61.08%       62.634s       3.423ms         18300
                 aten::batch_norm         0.06%      57.762ms        10.56%       10.826s     680.901us         15900
     aten::_batch_norm_impl_index         0.12%     126.712ms        10.51%       10.775s     677.660us         15900
          aten::native_batch_norm        10.29%       10.547s        10.38%       10.648s     669.700us         15900
                     aten::linear         0.01%       6.772ms         8.98%        9.205s       7.671ms          1200
                      aten::addmm         8.77%        8.994s         8.96%        9.185s       7.654ms          1200
                       aten::add_         4.60%        4.718s         4.60%        4.718s     982.928us          4800
                      aten::relu_         0.07%      69.159ms         3.80%        3.900s     265.290us         14700
                 aten::clamp_min_         3.75%        3.841s         3.75%        3.841s     261.263us         14700
           torchvision::roi_align         1.61%        1.655s         2.25%        2.304s       1.920ms          1200
```

We can see that the performance improvement primarily comes from:
* `torchvision::roi_align` time reduced from 82.6s to 2.3s, due to parallelization and vectorization.
* `aten::conv2d` time reduced from 88.3s to 63.1s, on channels last, mkldnn reorders on activations will be saved.

## Additional

**[Discussion Needed]**: need to decide details of performance benchmarking, such as:

* models ? use `benchmark.py` from detectron2 or use torch-bench?
* configs ? single core and multi core ? CPU type ?

**[Discussion Needed]**: test cases: we will add new test cases in corresponding modules from [vision/test](https://github.com/pytorch/vision/tree/main/test) when making pull requests, what else is needed?
"
Ability to change the retinanet model to be modified after training,pytorch/vision,2022-09-20 13:38:29,2,enhancement#module: models#topic: object detection,6614,1379441494,"### 🚀 The feature

After training the retinanet model, we are not able to change the number of classes for the next training session. For example, if the retinanet model is trained on 56 classes, the classifier subnet of Retinanet outputs 56 hot encoded vector, how can we use the same model weights for the next dataset which only has 40 classes of the previous problem?
My issue is similar I trained my retinanet on 3 classes, and now I want to use the same model weights but for a new dataset that has only had the first 2 classes of the previous dataset, when I try to do torch.load(model.state_dict(), PATH) it throws me a classifier mismatch:
```

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)
   1496         if len(error_msgs) > 0:
   1497             raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
-> 1498                                self.__class__.__name__, ""\n\t"".join(error_msgs)))
   1499         return _IncompatibleKeys(missing_keys, unexpected_keys)
   1500 

RuntimeError: Error(s) in loading state_dict for RetinaNet:
	size mismatch for classifier.3.weight: copying a param with shape torch.Size([3, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 128, 3, 3]).
	size mismatch for classifier.3.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).
```

### Motivation, pitch

I'm working on object detection using retinanet and I face the above-mentioned issue.

### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox"
[RFC] U-Net framework,pytorch/vision,2022-09-19 23:36:24,2,enhancement#help wanted#needs discussion#module: models#new feature,6610,1378641486,"### 🚀 The feature

A module-based approach of building **U-Nets** inside torchvision, similar to torchmultimodals sub-network approach. Mostly a food for though experiment given the similar nature of other popular vision Architectural Frameworks (namely [DETR](https://arxiv.org/abs/2005.12872), [Deformable-DETR](https://arxiv.org/abs/2010.04159), [Mask2Former](https://arxiv.org/abs/2112.01527) or [Mask DINO](https://arxiv.org/abs/2206.02777) which are just a composition of sub-networks that can be adapted to most vision tasks)

### Motivation, pitch

U-Nets are a good example given the rising popularity of Diffusion Models in which the U-Net paradigm is used (layers or merge strategies being the main difference between most implementations). 

Unlike DETR or Mask2Former, which can be broken done quite simply into a module with 2-4 sub-modules followed by a task head, the U-Net framework presents some more intricate challenges at configuration specification level given that we have to sync cross-level encoder and decoder configurations.

Shifting towards a more Framework / Block based approach for larger architectures (think of `nn.Modules` like experience but for present-time vision architectures) would be beneficial for users when it comes to sharing code, improvements or simply swapping out backbones or different components. For instance, if someone would want to grab a Mask2Former they would have to go and integrate themselves into [Detectron2](https://github.com/facebookresearch/Mask2Former/blob/main/INSTALL.md).

Similarly, if someone would want to jump in into doing diffusion, they would first have to find or make their own U-Net implementation even though what they most-likely want to do is to add a bottleneck with attention or some residual connection somewhere throughout the network or simply a different normalization layer in comparison to the original paper.

These classic paradigms should be easy to configure or specify (the same way `torch.nn.Transformer`  handles the transformer), and if more severe changes are wanted a user can have access to a code-base which they can copy-paste and apply minimal modification to (similar to how DETR [handles positional embeddings](https://github.com/facebookresearch/detr/blob/8a144f83a287f4d3fece4acdf073f387c5af387d/models/transformer.py#L154) in the decoder).

Even if they opt for modifying to code-base and later on share their work, there is the bonus of familiarity when others might want to work on top of their code since it's not entirely different than the base version with which they are already familiar with.

Supporting some of these architectures or frame-works might attract users that are working on tasks that are currently not supported by torchvision (for instance [Monocular Depth Estimation SotA](https://arxiv.org/abs/2204.00987v1) makes use of Mask2Former) which might provide us with valuable insight about the needs of the larger vision community.

### Alternatives

Currently [Lucidrains](https://github.com/lucidrains) has been leading these kinds of efforts for Attention Operations and Transformers and more recently for Diffusion models.

### Additional context

_No response_

cc @datumbox"
Read videos from memory,pytorch/vision,2022-09-18 06:12:26,3,module: datasets#module: video,6603,1376957946,"### 🚀 The feature

I have a lot of small videos (3 million video files), in an attempt to reduce disk bottleneck, I store all the videos in LMDB using: https://lmdb.readthedocs.io/en/release/. I would love the ability to read videos from memory.

### Motivation, pitch

My training (8xA100 GPUs) process is data starved. I'm pretty sure the issue is disk throughput, hence my usage of LMDB. Allowing torchvision to read videos from memory would help me use LMDB for storing binary files.

### Alternatives

I am about to try manually doing this using pyav.

### Additional context

_No response_

cc @pmeier"
Add other swin architectures.,pytorch/vision,2022-09-17 18:17:35,5,enhancement#module: models,6602,1376840307,"### 🚀 The feature

The original paper describes a few more configurations based on swin Transformer.

1. Swin Large: Simply a large model of swin transformer, needs a few config tweaks and we can port weights probably? 
3. SwinMLP: MLP Mixer Based SwinTransformer. Described in the original paper.
4. SwinMoe: Mixture of experts for Swin  https://arxiv.org/pdf/2204.09636.pdf



### Motivation, pitch

I think that Swin Large and SwinMLP could be good candidates as they need few edits for implementation.

I'm not sure if we can port weights, or train from scratch. As adding weights and implementation would also add a CI job and maintaining it.

### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox"
VideoReader could not decode video frames correctly if the video width is not divisible by 8,pytorch/vision,2022-09-16 16:17:34,0,module: video,6600,1376157206,"### 🐛 Describe the bug

VideoReader could not decode video frames correctly if the video width is not divisible by 8. Specifically, the pixels in the last columns of each frame could not be decoded correctly. Please use the code below and the data available [here](https://drive.google.com/file/d/1WmRmFBo9-c8Y7Ssjo4InzwjsQs5RMxXW/view?usp=sharing) to reproduce the bug.

```python
import os
from PIL import Image
from numpy.typing import NDArray
import numpy as np
import torch
from torchvision.io import VideoReader as VR


input_file = 'short_snippet_828x480.mp4'

vr = VR(input_file, stream='video', num_threads=os.cpu_count())
for idx, data in enumerate(vr):
    print(idx)
    rgb_frame: torch.Tensor = data['data']
    rgb_arr: NDArray = rgb_frame.numpy()
    rgb_arr = np.transpose(rgb_arr, [1, 2, 0])

    im_vr = Image.fromarray(rgb_arr)
    im_vr.save('short_828x480_vr_frames/frame{:0>4d}.png'.format(idx))
```

Expected behavior:
      The python code above should decode video frames and save them in `short_828x480_vr_frames`. However, if we look at the saved images, we can see that pixels in the last columns of each frame are not decoded. To compare, we can also decode the video using ffmpeg cli directly, i.e. running 
`ffmpeg -i short_828x480.mp4 -pix_fmt rgb24 short_828x480_ffmpeg_frames/frame%04d.png`.
By inspecting the saved images in the folder `short_828x480_ffmpeg_frames`, we can see that the ffmpeg cli is able to decode the pixels in the last few columns correctly.

### Versions

Collecting environment information...
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.15.0-46-generic-x86_64-with-glibc2.17
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy==0.910
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.23.1
[pip3] pytorch-lightning==1.7.0
[pip3] torch==1.12.1
[pip3] torchaudio==0.12.1
[pip3] torchmetrics==0.9.3
[pip3] torchqa==0.2.0
[pip3] torchvision==0.13.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.23.1           py38h6c91a56_0  
[conda] numpy-base                1.23.1           py38ha15fc14_0  
[conda] pytorch                   1.12.1          py3.8_cuda11.3_cudnn8.3.2_0    pytorch
[conda] pytorch-lightning         1.7.0                    pypi_0    pypi
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                0.12.1               py38_cu113    pytorch
[conda] torchmetrics              0.9.3                    pypi_0    pypi
[conda] torchqa                   0.2.0                     dev_0    <develop>
[conda] torchvision               0.13.1               py38_cu113    pytorch"
`fasterrcnn_resnet50_fpn` Windows GPU tests failing on CUDA 11.6,pytorch/vision,2022-09-15 10:41:59,11,bug#dependency issue,6589,1374317000,"### 🐛 Describe the bug

After the removal of CUDA 11.3 and the setting of 11.6 as default the tests at `fasterrcnn_resnet50_fpn` started [failing](https://app.circleci.com/pipelines/github/pytorch/vision/20144/workflows/7dc88095-fe04-43a7-8c3f-64193a9f2661/jobs/1637810) across all python versions:
```
Traceback (most recent call last):
  File ""C:\Users\circleci\project\test\test_models.py"", line 775, in check_out
    _assert_expected(output, model_name, prec=prec)
  File ""C:\Users\circleci\project\test\test_models.py"", line 117, in _assert_expected
    torch.testing.assert_close(output, expected, rtol=rtol, atol=atol, check_dtype=False, check_device=False)
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\testing\_comparison.py"", line 1342, in assert_close
    assert_equal(
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\testing\_comparison.py"", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 61 / 80 (76.2%)
Greatest absolute difference: 182.7935028076172 at index (9, 1) (up to 0.01 allowed)
Greatest relative difference: inf at index (1, 0) (up to 0.01 allowed)

The failure occurred for item [0]['boxes']

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\circleci\project\test\test_models.py"", line 803, in test_detection_model
    full_validation &= check_out(out)
  File ""C:\Users\circleci\project\test\test_models.py"", line 783, in check_out
    torch.testing.assert_close(
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\testing\_comparison.py"", line 1342, in assert_close
    assert_equal(
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\testing\_comparison.py"", line 1093, in assert_equal
    raise error_metas[0].to_error(msg)
AssertionError: Tensor-likes are not close!

Mismatched elements: 1 / 20 (5.0%)
Greatest absolute difference: 0.019545435905456543 at index (19,) (up to 0.01 allowed)
Greatest relative difference: 0.026511636142229424 at index (19,) (up to 0.01 allowed)
```

The failure occurs only on Windows. The Linux tests pass fine.

### Versions

TorchVision latest main branch

cc @atalman @malfet @ptrblck "
Support RGBA for torchvision.io.encode_png(),pytorch/vision,2022-09-12 20:20:54,0,enhancement#module: io,6570,1370470139,"### 🚀 The feature

Right now torchvision.io.encode_png() can only encode RGB and GRAYSCALE images. It would be very useful to add alpha channel support for PNG images.

### Motivation, pitch

It is useful for segmentation masks to be displayed, for example, in a web browser.

### Alternatives

Opencv

### Additional context

_No response_"
Build against `libjpeg-turbo` instead of `libjpeg`,pytorch/vision,2022-09-12 11:57:17,1,topic: build#module: io#dependency issue#module: ci,6563,1369759254,"`torchvision` is currently building 

https://github.com/pytorch/vision/blob/cac4e228c9ca9e7564cb34406e7ebccfdd736976/setup.py#L321

https://github.com/pytorch/vision/blob/cac4e228c9ca9e7564cb34406e7ebccfdd736976/packaging/torchvision/meta.yaml#L13

and testing against `libjpeg`

https://github.com/pytorch/vision/blob/cac4e228c9ca9e7564cb34406e7ebccfdd736976/.circleci/unittest/linux/scripts/environment.yml#L10

https://github.com/pytorch/vision/blob/cac4e228c9ca9e7564cb34406e7ebccfdd736976/.circleci/unittest/windows/scripts/environment.yml#L10

`Pillow` is building against `libjpeg-turbo` on Windows for some time now and since [`Pillow=9` on all platforms](https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html#switched-to-libjpeg-turbo-in-macos-and-linux-wheels) (Jan 2022).

This has two downsides for us:

1. We can't use `Pillow` as reference for our own decoding and encoding ops. See 
    - #5184 
    - #5162 
    - #3913 
    - #5910 
2. As the name implies, `libjpeg-turbo` is a faster implementation of the JPEG standard. Thus, our I/O ops are simply slower than using `Pillow`, which hinders adoption.

Recently, @NicolasHug led a push to also use `libjpeg-turbo`, but hit a few blockers:

1. Our workflows use the `defaults` channel from `conda`. Unfortunately, on `defaults` `libjpeg-turbo` is [only available for Windows and macOS](https://github.com/pytorch/vision/pull/5951#discussion_r865913972).
2. Adding `conda-forge` to the channels for Linux, leads to crazy environment solve times (10+ minutes), which ultimately [time out the CI](https://app.circleci.com/pipelines/github/pytorch/vision/19979/workflows/9bad1bf8-09f2-4741-9b9b-aebe8b577332/jobs/1622373?invite=true#step-103-310). In general this change should be possible if `conda-forge` has a lower priority than `defaults`.
3. Depending on the [experimental `libmamba` solver](https://www.anaconda.com/blog/a-faster-conda-for-a-growing-community) indeed speeds ups the solve for the CI to not time out (it is still a little slower than before). Unfortunately, our CI setup does not properly work with it, since a CUDA 11.6 workflow is still pulling a [PyTorch version build against CUDA 11.3](https://app.circleci.com/pipelines/github/pytorch/vision/20016/workflows/d55de817-04b8-4c09-879e-0cbec9daaab6/jobs/1625961).

From here on I currently see four options:

1. Only build and test Windows and macOS binaries against `libjpeg-turbo`. This would mean that arguably most of our users won't see that speed-up.
2. Find a way to stop the CI from timing out when using `conda-forge` as extra channel. This can probably be done through the configuration or by emitting more output during the solve.
3. Fix our CI setup to work with the `libmamba` solver.
4. Package `libjpeg-turbo` for Linux ourselves. We already use the `pytorch` or `pytorch-nightly` channels. If it was available there, we wouldn't need to pull it from `conda-forge`. In https://github.com/pytorch/vision/pull/5941#discussion_r871546056 @malfet only talks about testing against it, but maybe we can also build against it.

cc @seemethere"
Allow Activation Layer of MLP In SwinTransformer to be customizable,pytorch/vision,2022-09-10 17:29:19,4,,6558,1368683952,"### 🚀 The feature

Currently we do 

```
self.mlp = MLP(dim, [int(dim * mlp_ratio), dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)
```

But according to original implementation it is 

https://github.com/microsoft/Swin-Transformer/blob/afeb877fba1139dfbc186276983af2abb02c2196/models/swin_transformer.py#L197

```

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 fused_window_process=False):
```









### Motivation, pitch

I think it might be better to align with the original implementation by creating extra init param activation_layer and allowing the flexibility.

### Alternatives

_No response_

### Additional context

_No response_"
add quantized vision transformer model,pytorch/vision,2022-09-08 09:34:33,5,question#module: models.quantization,6545,1365900211,"### 🚀 The feature

hi, thanks for your great work. I hope to be able to add quantized vit model (for ptq or qat).

### Motivation, pitch

In 'torchvision/models/quantization', there are several quantized model (Eager Mode Quantization) that is very useful for me to learn quantization. In recent years, Transformer model is very popular. I want to learn how to quantized Transformer model, e.g Vision Transformer, Swin Transformer etc, using pytorch official tools like Eager Mode Quantization. I also tried to modify it myself, but failed. I don't know how to quantify 'pos_embedding' (nn.Parameter) and nn.MultiheadAttention module. look forward to your reply.

### Alternatives

_No response_

### Additional context

_No response_"
Add support for CMYK -> RGB in `decode_jpeg()`,pytorch/vision,2022-09-05 13:35:44,0,,6538,1361986431,"Asking for a CYMK jpeg file to be converted to RGB by `decode_jpeg()` currently results in the following error:

```py
from torchvision.io import read_file, decode_jpeg, ImageReadMode

path = ""test/assets/fakedata/logos/cmyk_pytorch.jpg""
decode_jpeg(read_file(path), mode=ImageReadMode.RGB)  # ImageReadMode.UNCHANGED would work
```

```p
RuntimeError: Unsupported color conversion request
```

This is an important feature to support because the ImageNet dataset has ~20 CMYK images, which means we currently cannot decode ImageNet files with `decode_jpeg()`: we have to rely on PIL.


According to this comment in our code there should be a way to support this:
https://github.com/pytorch/vision/blob/9b432d074a1c078eaae2023d301093e49e9954c2/torchvision/csrc/io/image/cpu/decode_jpeg.cpp#L120-L124"
Classification references does not work without distributed setup,pytorch/vision,2022-09-01 08:26:46,2,bug#help wanted#module: reference scripts,6529,1358456298,"If you don't set the respective env vars 

https://github.com/pytorch/vision/blob/d5bd8b728f14c33b339fc45c90ca39be339bce3f/references/classification/utils.py#L255-L258

training will not be distributed and in turn the backend will not be initialized. However, during evaluation we check 

https://github.com/pytorch/vision/blob/d5bd8b728f14c33b339fc45c90ca39be339bce3f/references/classification/train.py#L88

unguarded, which then fails with

```
RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
```

cc @datumbox"
 Vmap causing TypeError when applied to Rotate ,pytorch/vision,2022-08-31 04:33:28,3,module: transforms,6524,1356756789,"I'm running into an issue trying to vmap over the torchvision `rotate` function. `rotate()` requires an int or float input and does not accept single valued tensors. However vmap requires all batched inputs come as tensors. Additionally, one can't create a helper function that calls `rotate(im, angle.item())`, as item calls are not allowed in `vmap`.  

There might be a simple solution to this I'm not seeing, but if not it would be nice for Rotate to accept single value tensors as inputs. 

```
from functorch import vmap
from torchvision.transforms.functional import rotate
vrot = vmap(rotate, in_dims=(0,0), out_dims=0)
b, dim = 10, 64
inp_ims = torch.rand((b, dim, dim))
angles = torch.rand((b))
vrot(inp_ims, angles)
```

This gives: `TypeError: Argument angle should be int or float`

cc @vfdev-5 @datumbox"
Fill arg and _apply_grid_transform improvements,pytorch/vision,2022-08-30 12:30:56,0,enhancement#module: transforms,6517,1355694342,"Few years ago we introduced non-const fill value handling in `_apply_grid_transform` using mask approach:

https://github.com/pytorch/vision/blob/0d69e35c4e951109dbaa8b42b0a8416d199aee0b/torchvision/transforms/functional_tensor.py#L550-L568

There are few minor problems with this approach:

1) if we pass `fill = [0.0, ]`, we would expect to have a similar result as `fill=None`. This is not exactly true for bilinear interpolation mode where we do linear interpolation: 
https://github.com/pytorch/vision/blob/0d69e35c4e951109dbaa8b42b0a8416d199aee0b/torchvision/transforms/functional_tensor.py#L567-L568

Most probably, we would like to skip `fill_img` creation for all fill values that has `sum(fill) == 0` as `grid_sample` pads with zeros.

```diff
- if fill is not None:
+ if fill is not None and sum(fill) > 0:
```

2) Linear `fill_img` and `img` interpolation may be replaced by directly applying a mask: 
```python
         mask = mask < 0.9999
         img[mask] = fill_img[mask] 
```
That would match better PIL Image behaviour.

https://github.com/pytorch/vision/blob/0d69e35c4e951109dbaa8b42b0a8416d199aee0b/torchvision/transforms/functional_tensor.py#L567-L568

![image](https://user-images.githubusercontent.com/2459423/187435735-8a13af09-8e80-4db1-82cb-6081b74d0c94.png)



cc @datumbox"
Evaluate models accuracy when using inference transforms on Tensors (instead of PIL images),pytorch/vision,2022-08-26 11:33:36,3,module: transforms#module: reference scripts,6506,1352145050,"The accuracy of our trained models that we currently report come from evaluations run on PIL images. However, our [inference-time transforms](https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py#L38) also support Tensors.

In the wild, our users might be passing Tensors to the pre-trained models (instead of PIL images), so it's worth figuring out whether the accuracy is consistent between Tensors and PIL.

Note: we do check that all the transforms are consistent between PIL and Tensors, so hopefully differences should be minimal. But models are known to learn interpolation tweaks and in particular the use of anti-aliasing. PIL uses anti-aliasing by default and this is what our models where trained on, but we don't pass `antialias=True` to the [Resize transform](https://github.com/pytorch/vision/blob/5737ed27c7937c6bbb6774be90d61bc6e68f6d06/torchvision/transforms/_presets.py#L56), so it might be a source of discrepancy.

As discussed internally with @datumbox, figuring that out is part of the transforms rework plan (although it's relevant outside of the rework as well).

cc @vfdev-5 @datumbox"
Port SwinTransformer3d from torchmultimodal,pytorch/vision,2022-08-25 19:22:00,1,,6499,1351352918,"### 🚀 The feature

The main Idea is to port the SwinTransformer3d model from torchmulitmodal to torchvision.

Need to keep in mind the nuances and code structure of torchvision

https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py

https://github.com/facebookresearch/multimodal/blob/main/examples/omnivore/LoadOriginalPretrainedWeightAndCompare.ipynb

We need to port the implementation as well as the weights.


### Motivation, pitch

The idea is to first port SwinTransformer3dV1 and port its weights successfully. Once done we can then think of having SwinTransformer3dV2 (there is no such paper or implementation but maybe it will benefit like the 2d case)



### Alternatives

_No response_

### Additional context

Additionally in discussion with @YosuaMichael the paper also mentioned that SwinTransformerV2 can be used for object detection tasks. If possible we should explore it (but only after we finish previous things)"
Stereo Depth Perception Transforms,pytorch/vision,2022-08-25 12:40:34,1,needs discussion#module: transforms,6495,1350835378,"Currently we are in progress of revamping transforms API for torchvision library. During this revamp, we are considering the components and behavior for new use cases such as detection, segmentation, and video classification. In this document, we just want to raise awareness on several transforms in depth perception quirkiness that might behave differently from the other use cases.


## Stereo Depth Perception Brief Introduction

In the Stereo Depth Perception problem, we are given **two images**, left and right, and for each pixel on the left image we want to guess the horizontal displacement of the pixel occurrence on the right image. The images are taken at the same time with two different cameras with the same upright orientation and their distance must be parallel with the horizontal axis / ground. The desired output is called **disparity map**, a 2d integer array with the same size as the image indicating how much displacement the pixel on that position is from left to right image (the unit of the displacement is in pixels). 

In a stereo depth perception dataset, they sometimes also give a **valid mask**. This is a 2d boolean array with the same size as the disparity map; the valid mask indicates whether the values in disparity maps are valid or not. Practically, we usually exclude the non-valid disparity maps from loss calculation during the training or validation.


## Components



1. Images: \
A pair of left and right image as input
2. Disparity map: \
2d integer array with same size as the image indicating the displacement of pixels on the image pair. Some dataset only give one disparity map which usually indicate the displacement from left to right image and some dataset provide a pair of left and right disparity maps.
3. Valid mask: \
2d boolean array with the same size as the disparity map indicating whether or not the disparity map value is valid and needs to be considered in loss calculation.

In general, a stereo depth perception dataset will have:
```
( 

(image_left, image_right), 

(disparity_map_left, [optional] disparity_map_right), 

([optional] valid_mask_left, [optional] valid_mask_right)

)
```

## Horizontal Flips

For horizontal flips, we do the horizontal flips on each component and then **we need to flip the order of left and right**. Supposing **HF** is an operation to do horizontal flip for a component, then:
```
horizontal_flips( (image_left, image_right), (disparity_map_left, disparity_map_right), (valid_mask_left, valid_mask_right) ) = 

(HF(image_right), HF(image_left)), (HF(disparity_map_right), HF(disparity_map_left)), (HF(valid_mask_right), HF(valid_mask_left))
```
The new behavior on this transform is that we need to flip the ordering of left and right which did not exist in previous use cases.

Here are a code reference: [https://github.com/pytorch/vision/blob/17ace9557f95be6dfa6683bd8b6c002bbcb668df/references/stereo_matching/transforms.py#L344](https://github.com/pytorch/vision/blob/17ace9557f95be6dfa6683bd8b6c002bbcb668df/references/stereo_matching/transforms.py#L344)


## Resize

For resizing, we resize the image normally, but for the disparity maps we need to adjust the value after the resize by multiplying the resizing scale factor and then convert the value into the nearest integer.

Remember that disparity map is the pixel displacement and the unit of the displacement is in how many pixels. If we resize the image, then the pixel displacement will also scaled with the same scaling factor of the resizing.

Take note that the flow from OpticalFlow behave similarly in resize as well.

Here is the code reference: [https://github.com/pytorch/vision/blob/17ace9557f95be6dfa6683bd8b6c002bbcb668df/references/stereo_matching/transforms.py#L370](https://github.com/pytorch/vision/blob/17ace9557f95be6dfa6683bd8b6c002bbcb668df/references/stereo_matching/transforms.py#L370)


## Erase / Occlusion

For erase or occlusion, commonly we only apply the operation on the right image without affecting other components (no transformation happens on left image, disparity maps, and valid masks).

Here are code references:

[https://github.com/pytorch/vision/blob/17ace9557f95be6dfa6683bd8b6c002bbcb668df/references/stereo_matching/transforms.py#L261](https://github.com/pytorch/vision/blob/17ace9557f95be6dfa6683bd8b6c002bbcb668df/references/stereo_matching/transforms.py#L261)

[https://github.com/princeton-vl/RAFT-Stereo/blob/main/core/utils/augmentor.py#L109](https://github.com/princeton-vl/RAFT-Stereo/blob/main/core/utils/augmentor.py#L109)

[https://github.com/megvii-research/CREStereo/blob/ad3a1613bdedd88b93247e5f002cb7c80799762d/dataset.py#L155](https://github.com/megvii-research/CREStereo/blob/ad3a1613bdedd88b93247e5f002cb7c80799762d/dataset.py#L155)


cc @vfdev-5 @datumbox @pmeier @TeodorPoncu "
query image size from bounding boxes or segmentation masks,pytorch/vision,2022-08-25 10:07:19,0,module: transforms#prototype,6491,1350656171,"Our current implementation of `query_chw` requires an image somewhere in the sample, since that is the only way to extract the number of channels. 

https://github.com/pytorch/vision/blob/f9966d228c5b809b0b68671ef168004f1e06d3fe/torchvision/prototype/transforms/_utils.py#L39

However, there are quite a few transformations that only require the image size, i.e. height and width:

https://github.com/pytorch/vision/blob/f9966d228c5b809b0b68671ef168004f1e06d3fe/torchvision/prototype/transforms/_geometry.py#L95

Although these transforms should technically work with `BoundingBox`'es and `SegmentationMask`'s as well, they will fail at the moment.

I see two possible solutions to this:

1. Split `query_chw` into `query_c` and `query_hw`. The former will only work with images, while the latter works with with images as well as `BoundingBox`'es and `SegmentationMask`'s. This was already implemented in a PR of mine, but I can't find it now. If someone does, feel free to link.
2. Option 1. requires us to go through the sample twice in case we need the number of channels and the image size. If we find that we need to reduce the number of times we do this, we could also allow `query_chw` to return `None` for the number of channels. In that case, I would introduce another flag `need_c: bool = False` that if set errors in case we don't find the number of channels. That would avoid much of duplicated error checking in the transformation like

    ```py
    c, h, w = query_chw(sample)
    if c is None:
        raise TypeError(""I need number of channels, but found no image!"")
    ```
    
    in favor of
    
    ```py
    c, h, w = query_chw(sample, needs_c=True)
    assert c is not None
    ```

cc @vfdev-5 @datumbox @bjuncek"
Issue to load vit_h_14 model with pretrained weights (DEFAULT or IMAGENET1K_SWAG_E2E_V1),pytorch/vision,2022-08-25 02:29:10,1,,6489,1350233297,"### 🐛 Describe the bug

I faced the below issue after training vit_h_14 model with pretrained weights. If I do not load pretrained weights, everything is fine.
# how to reproduce this bug
import torchvision
model = torchvision.models.get_model('vit_h_14', weights='DEFAULT')
#or 
#model = torchvision.models.get_model('vit_h_14', weights='IMAGENET1K_SWAG_E2E_V1')


```
Traceback (most recent call last):
  File ""train.py"", line 545, in <module>
    main(args)
  File ""train.py"", line 225, in main
    model = torchvision.models.get_model(args.model, weights=args.weights)
  File ""/usr/local/lib/python3.7/dist-packages/torchvision/models/_api.py"", line 225, in get_model
    return fn(**config)
  File ""/usr/local/lib/python3.7/dist-packages/torchvision/models/vision_transformer.py"", line 764, in vit_h_14
    **kwargs,
  File ""/usr/local/lib/python3.7/dist-packages/torchvision/models/vision_transformer.py"", line 335, in _vision_transformer
    model.load_state_dict(weights.get_state_dict(progress=progress))
  File ""/usr/local/lib/python3.7/dist-packages/torchvision/models/_api.py"", line 66, in get_state_dict
    return load_state_dict_from_url(self.url, progress=progress)
  File ""/usr/local/lib/python3.7/dist-packages/torch/hub.py"", line 731, in load_state_dict_from_url
    return torch.load(cached_file, map_location=map_location)
  File ""/usr/local/lib/python3.7/dist-packages/torch/serialization.py"", line 726, in load
    with _open_zipfile_reader(opened_file) as opened_zipfile:
  File ""/usr/local/lib/python3.7/dist-packages/torch/serialization.py"", line 262, in __init__
    super(_open_zipfile_reader, self).__init__(torch._C.PyTorchFileReader(name_or_buffer))
RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 33723) of binary: /usr/bin/python3
Traceback (most recent call last):
  File ""/usr/local/bin/torchrun"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py"", line 755, in run
    )(*cmd_args)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py"", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py"", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
```

### Versions

Collecting environment information...
PyTorch version: 1.13.0.dev20220810+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.26

Python version: 3.7.5 (default, Dec  9 2021, 17:04:37)  [GCC 8.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-122-generic-x86_64-with-Ubuntu-18.04-bionic
Is CUDA available: True
CUDA runtime version: 11.2.152
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.141.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.6
[pip3] pytorch-lightning==1.7.1
[pip3] pytorch-lightning-bolts==0.3.2.post1
[pip3] torch==1.13.0.dev20220810+cu113
[pip3] torchaudio==0.13.0.dev20220810+cu113
[pip3] torchmetrics==0.9.3
[pip3] torchvision==0.14.0.dev20220810+cu113
[conda] Could not collect"
prototype features cannot be deepcopied,pytorch/vision,2022-08-24 07:41:59,1,prototype#module: features,6478,1349008725,"```py
from torchvision.prototype import features
from copy import copy, deepcopy
import torch

image = features.Image(torch.rand(3, 16, 16))
image_copy = copy(image)  # works
image_deepcopy = deepcopy(image)  # fails
```

```
RuntimeError: The default implementation of __deepcopy__() for non-wrapper subclasses only works for subclass types 
that implement new_empty() and for which that function returns another instance of the same subclass. 
You should either properly implement new_empty() for your subclass or override __deepcopy__() if it is intended 
behavior for new_empty() to return an instance of a different type.
```

The error message is giving us two options:

1. Override [`new_empty`](https://pytorch.org/docs/stable/generated/torch.Tensor.new_empty.html): according to the error message, this is preferred since we want to return an instance of the same type. Some of our features have internal guards for the shape like 

    https://github.com/pytorch/vision/blob/b4b246a52ebbe29bff9099c6fbabff0b82d7edd3/torchvision/prototype/features/_image.py#L49-L50

    and the plan is to expand this to all features, since the transformations rely on a certain shape. However, you can call `new_empty(size)` with an arbitrary size and the default implementation of `__deepcopy__` does so with an [empty list `[]`](https://github.com/pytorch/pytorch/blob/432c508e71111f9d5382322e0e6b1bc1c66bf0ec/torch/_tensor.py#L181). Later on, [the correct shape of the copied tensor is set](https://github.com/pytorch/pytorch/blob/432c508e71111f9d5382322e0e6b1bc1c66bf0ec/torch/_tensor.py#L191-L193), but we still need to be able to create the tensor with an incorrect shape. We could left-pad the `size` with zeros to get to the correct number of dimensions in case it is needed.

2. Override `__deepcopy__`: Maybe I'm to naive here, but a deepcopy should just be a `.clone()` of the tensor as well as a deepcopy of attributes where it is needed, e.g. `Label.categories`. Not sure if there is a downside, but that seems easier than 1. while achieving the same goal.

cc @bjuncek"
Building torchvision for 3.0 cuda compute capability,pytorch/vision,2022-08-22 23:50:25,1,,6470,1347134632,"### 🐛 Describe the bug

I have successfully built PyTorch from source for my legacy hardware on Windows 10. I'm building torchvision for cuda 10.2, sm30 having the following env variables USE_CUDA=1 USE_CUDNN=1 USE_MKLDNN=1 TORCH_CUDA_ARCH_LIST=""3.0"" NVCC_FLAGS=""-D__CUDA_NO_HALF_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_30,code=sm_30"" TORCH_NVCC_FLAGS=""-Xfatbin -compress-all"".

When I trace detectron2 GeneralizedRCNN I get an error: RuntimeError: default_program(21): error: identifier ""__ldg"" is undefined.

Full stack trace:
```Traceback (most recent call last):
  File ""<repo>/detectron/export_model.py"", line 194, in <module>
    exported_model = export_tracing(torch_model, sample_inputs, ""model.ts"")
  File ""<repo>/detectron/export_model.py"", line 120, in export_tracing
    ts_model = torch.jit.trace(traceable_model, (image,))  # type: ignore
  File ""<py3.8>\lib\site-packages\torch\jit\_trace.py"", line 733, in trace
    return trace_module(
  File ""<py3.8>\lib\site-packages\torch\jit\_trace.py"", line 934, in trace_module
    module._c._create_method_from_trace(
  File ""<py3.8>\lib\site-packages\torch\nn\modules\module.py"", line 887, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File ""<py3.8>\lib\site-packages\torch\nn\modules\module.py"", line 860, in _slow_forward
    result = self.forward(*input, **kwargs)
  File ""<py3.8>\lib\site-packages\detectron2\export\flatten.py"", line 294, in forward
    outputs = self.inference_func(self.model, *inputs_orig_format)
  File ""<repo>/detectron/export_model.py"", line 111, in inference_func
    inst = model.inference(inputs, do_postprocess=False)[0]
  File ""<py3.8>\lib\site-packages\detectron2\modeling\meta_arch\rcnn.py"", line 213, in inference
    results, _ = self.roi_heads(images, features, proposals, None)
  File ""<py3.8>\lib\site-packages\torch\nn\modules\module.py"", line 887, in _call_impl
    result = self._slow_forward(*input, **kwargs)
  File ""<py3.8>\lib\site-packages\torch\nn\modules\module.py"", line 860, in _slow_forward
    result = self.forward(*input, **kwargs)
  File ""<py3.8>\lib\site-packages\detectron2\modeling\roi_heads\roi_heads.py"", line 747, in forward
    pred_instances = self._forward_box(features, proposals)
  File ""<py3.8>\lib\site-packages\detectron2\modeling\roi_heads\roi_heads.py"", line 815, in _forward_box
    pred_instances, _ = self.box_predictor.inference(predictions, proposals)
  File ""<py3.8>\lib\site-packages\detectron2\modeling\roi_heads\fast_rcnn.py"", line 479, in inference
    return fast_rcnn_inference(
  File ""<py3.8>\lib\site-packages\detectron2\modeling\roi_heads\fast_rcnn.py"", line 79, in fast_rcnn_inference
    result_per_image = [
  File ""<py3.8>\lib\site-packages\detectron2\modeling\roi_heads\fast_rcnn.py"", line 80, in <listcomp>
    fast_rcnn_inference_single_image(
  File ""<py3.8>\lib\site-packages\detectron2\modeling\roi_heads\fast_rcnn.py"", line 162, in fast_rcnn_inference_single_image     
    keep = batched_nms(boxes, scores, filter_inds[:, 1], nms_thresh)
  File ""<py3.8>\lib\site-packages\detectron2\layers\nms.py"", line 20, in batched_nms
    return box_ops.batched_nms(boxes.float(), scores, idxs, iou_threshold)
  File ""<py3.8>\lib\site-packages\torch\jit\_trace.py"", line 1094, in wrapper
    return compiled_fn(*args, **kwargs)
RuntimeError: default_program(21): error: identifier ""__ldg"" is undefined

1 error detected in the compilation of ""default_program"".

nvrtc compilation failed:

#define NAN __int_as_float(0x7fffffff)
#define POS_INFINITY __int_as_float(0x7f800000)
#define NEG_INFINITY __int_as_float(0xff800000)


template<typename T>
__device__ T maximum(T a, T b) {
  return isnan(a) ? a : (a > b ? a : b);
}

template<typename T>
__device__ T minimum(T a, T b) {
  return isnan(a) ? a : (a < b ? a : b);
}

extern ""C"" __global__
void fused_unsqueeze_add(float* t0, float* t1, float* aten_add) {
{
  if (512 * blockIdx.x + threadIdx.x<17808 ? 1 : 0) {
    float v = __ldg(t0 + 512 * blockIdx.x + threadIdx.x);
    float v_1 = __ldg(t1 + (512 * blockIdx.x + threadIdx.x) / 4);
    aten_add[512 * blockIdx.x + threadIdx.x] = v + v_1;
  }
}
}


### Versions

PyTorch version: 1.8.2a0+e0495a7
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.24.1
Libc version: N/A

Python version: 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19044-SP0
Is CUDA available: True
CUDA runtime version: 10.2.89
GPU models and configuration: GPU 0: GeForce GTX 680
Nvidia driver version: 441.22
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2\bin\cudnn_ops_train64_8.dll
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.23.2
[pip3] torch==1.8.2a0+e0495a7
[pip3] torchvision==0.9.0a0+761d09f
[conda] Could not collect"
Jupyter Kernel dies on Import,pytorch/vision,2022-08-22 15:21:48,4,,6467,1346589592,"### 🐛 Describe the bug

Importing torchvision in a Jupyter Notebook makes the kernel crash.
For example, a simple Jupyter notebook such as:
```
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
```
works until line 3, but when importing from `torchvision`, the kernel crashes. The jupyter logs in VSCode show:

```
error 16:11:13.96: Disposing session as kernel process died ExitCode: undefined, Reason: /Users/cornelius/anaconda3/envs/general/lib/python3.10/site-packages/traitlets/traitlets.py:2392: FutureWarning: Supporting extra quotes around strings is deprecated in traitlets 5.0. You can use 'hmac-sha256' instead of '""hmac-sha256""' if you require traitlets >=5.
  warn(
/Users/cornelius/anaconda3/envs/general/lib/python3.10/site-packages/traitlets/traitlets.py:2346: FutureWarning: Supporting extra quotes around Bytes is deprecated in traitlets 5.0. Use '9d320bd6-ee17-41f0-93dd-660ce9773e10' instead of 'b""9d320bd6-ee17-41f0-93dd-660ce9773e10""'.
  warn(

error 16:11:13.96: Raw kernel process exited code: undefined
```

I am aware of  #5953 but reinstalling the environment didnt help.

### Versions

MacOS: 12.5 (Intel)

Conda:
```
# Name                    Version                   Build  Channel
anyio                     3.6.1                    pypi_0    pypi
appnope                   0.1.3                    pypi_0    pypi
argon2-cffi               21.3.0                   pypi_0    pypi
argon2-cffi-bindings      21.2.0                   pypi_0    pypi
asttokens                 2.0.8                    pypi_0    pypi
attrs                     22.1.0                   pypi_0    pypi
babel                     2.10.3                   pypi_0    pypi
backcall                  0.2.0                    pypi_0    pypi
beautifulsoup4            4.11.1                   pypi_0    pypi
blas                      1.0                         mkl  
bleach                    5.0.1                    pypi_0    pypi
brotlipy                  0.7.0           py310hca72f7f_1002  
bzip2                     1.0.8                h1de35cc_0  
ca-certificates           2022.07.19           hecd8cb5_0  
certifi                   2022.6.15       py310hecd8cb5_0  
cffi                      1.15.1          py310hc55c11b_0  
charset-normalizer        2.0.4              pyhd3eb1b0_0  
cryptography              37.0.1          py310hf6deb26_0  
cycler                    0.11.0                   pypi_0    pypi
debugpy                   1.6.3                    pypi_0    pypi
decorator                 5.1.1                    pypi_0    pypi
defusedxml                0.7.1                    pypi_0    pypi
entrypoints               0.4                      pypi_0    pypi
executing                 0.10.0                   pypi_0    pypi
fastjsonschema            2.16.1                   pypi_0    pypi
ffmpeg                    4.3                  h0a44026_0    pytorch
fonttools                 4.36.0                   pypi_0    pypi
freetype                  2.11.0               hd8bbffd_0  
gettext                   0.21.0               h7535e17_0  
giflib                    5.2.1                haf1e3a3_0  
gmp                       6.2.1                he9d5cce_3  
gnutls                    3.6.15               hed9c0bf_0  
icu                       58.2                 h0a44026_3  
idna                      3.3                pyhd3eb1b0_0  
intel-openmp              2021.4.0          hecd8cb5_3538  
ipykernel                 6.15.1                   pypi_0    pypi
ipython                   8.4.0                    pypi_0    pypi
ipython-genutils          0.2.0                    pypi_0    pypi
jedi                      0.18.1                   pypi_0    pypi
jinja2                    3.1.2                    pypi_0    pypi
jpeg                      9e                   hca72f7f_0  
json5                     0.9.10                   pypi_0    pypi
jsonschema                4.14.0                   pypi_0    pypi
jupyter-client            7.3.4                    pypi_0    pypi
jupyter-core              4.11.1                   pypi_0    pypi
jupyter-server            1.18.1                   pypi_0    pypi
jupyterlab                3.4.5                    pypi_0    pypi
jupyterlab-pygments       0.2.2                    pypi_0    pypi
jupyterlab-server         2.15.0                   pypi_0    pypi
kiwisolver                1.4.4                    pypi_0    pypi
lame                      3.100                h1de35cc_0  
lcms2                     2.12                 hf1fd2bf_0  
lerc                      3.0                  he9d5cce_0  
libcxx                    12.0.0               h2f01273_0  
libdeflate                1.8                  h9ed2024_5  
libffi                    3.3                  hb1e8313_2  
libiconv                  1.16                 hca72f7f_2  
libidn2                   2.3.2                h9ed2024_0  
libpng                    1.6.37               ha441bb4_0  
libtasn1                  4.16.0               h9ed2024_0  
libtiff                   4.4.0                h2ef1027_0  
libunistring              0.9.10               h9ed2024_0  
libwebp                   1.2.2                h56c3ce4_0  
libwebp-base              1.2.2                hca72f7f_0  
libxml2                   2.9.14               hbf8cd5e_0  
llvm-openmp               12.0.0               h0dcd299_1  
lxml                      4.9.1                    pypi_0    pypi
lz4-c                     1.9.3                h23ab428_1  
markupsafe                2.1.1                    pypi_0    pypi
matplotlib                3.5.3                    pypi_0    pypi
matplotlib-inline         0.1.6                    pypi_0    pypi
mistune                   2.0.4                    pypi_0    pypi
mkl                       2021.4.0           hecd8cb5_637  
mkl-service               2.4.0           py310hca72f7f_0  
mkl_fft                   1.3.1           py310hf879493_0  
mkl_random                1.2.2           py310hc081a56_0  
nbclassic                 0.4.3                    pypi_0    pypi
nbclient                  0.6.6                    pypi_0    pypi
nbconvert                 7.0.0                    pypi_0    pypi
nbformat                  5.4.0                    pypi_0    pypi
ncurses                   6.3                  hca72f7f_3  
nest-asyncio              1.5.5                    pypi_0    pypi
nettle                    3.7.3                h230ac6f_1  
notebook                  6.4.12                   pypi_0    pypi
notebook-shim             0.1.0                    pypi_0    pypi
numpy                     1.23.1          py310hdcd3fac_0  
numpy-base                1.23.1          py310hfd2de13_0  
openh264                  2.1.1                h8346a28_0  
openssl                   1.1.1q               hca72f7f_0  
packaging                 21.3                     pypi_0    pypi
pandocfilters             1.5.0                    pypi_0    pypi
parso                     0.8.3                    pypi_0    pypi
pexpect                   4.8.0                    pypi_0    pypi
pickleshare               0.7.5                    pypi_0    pypi
pillow                    9.2.0           py310hde71d04_1  
pip                       22.1.2          py310hecd8cb5_0  
prometheus-client         0.14.1                   pypi_0    pypi
prompt-toolkit            3.0.30                   pypi_0    pypi
psutil                    5.9.1                    pypi_0    pypi
ptyprocess                0.7.0                    pypi_0    pypi
pure-eval                 0.2.2                    pypi_0    pypi
pycparser                 2.21               pyhd3eb1b0_0  
pygments                  2.13.0                   pypi_0    pypi
pyopenssl                 22.0.0             pyhd3eb1b0_0  
pyparsing                 3.0.9                    pypi_0    pypi
pyrsistent                0.18.1                   pypi_0    pypi
pysocks                   1.7.1           py310hecd8cb5_0  
python                    3.10.4               hdfd78df_0  
python-dateutil           2.8.2                    pypi_0    pypi
pytorch                   1.12.1                 py3.10_0    pytorch
pytz                      2022.2.1                 pypi_0    pypi
pyzmq                     23.2.1                   pypi_0    pypi
readline                  8.1.2                hca72f7f_1  
requests                  2.28.1          py310hecd8cb5_0  
send2trash                1.8.0                    pypi_0    pypi
setuptools                61.2.0          py310hecd8cb5_0  
six                       1.16.0             pyhd3eb1b0_1  
sniffio                   1.2.0                    pypi_0    pypi
soupsieve                 2.3.2.post1              pypi_0    pypi
sqlite                    3.39.2               h707629a_0  
stack-data                0.4.0                    pypi_0    pypi
terminado                 0.15.0                   pypi_0    pypi
tinycss2                  1.1.1                    pypi_0    pypi
tk                        8.6.12               h5d9f67b_0  
torchaudio                0.12.1                py310_cpu    pytorch
torchvision               0.13.1                py310_cpu    pytorch
tornado                   6.2                      pypi_0    pypi
traitlets                 5.3.0                    pypi_0    pypi
typing_extensions         4.3.0           py310hecd8cb5_0  
tzdata                    2022a                hda174b7_0  
urllib3                   1.26.11         py310hecd8cb5_0  
wcwidth                   0.2.5                    pypi_0    pypi
webencodings              0.5.1                    pypi_0    pypi
websocket-client          1.3.3                    pypi_0    pypi
wheel                     0.37.1             pyhd3eb1b0_0  
xz                        5.2.5                hca72f7f_1  
zlib                      1.2.12               h4dc903c_2  
zstd                      1.5.2                hcb37349_0 
```"
decode_jpeg() creates tensors with different stride from PIL.Image.open(),pytorch/vision,2022-08-22 15:06:03,4,,6465,1346567585,"Opening a file with `torchvision.io.decode_jpeg()` will create a tensor with different strides than opening with PIL and converting to a tensor (see snippet below).

This can be the cause of a very significant difference in training time because of https://github.com/pytorch/pytorch/issues/83840. Let's see if https://github.com/pytorch/pytorch/issues/83840 deserves a ""fix"" or not but regardless, this might not be the only transform that will be sensitive to strides, and it might be worth changing what we output in  `decode_jpeg()`.

@datumbox @vfdev-5 this is something to keep in mind when you'll benchmark the new transforms: the strides matter a ton.

```py
import torch
from torchvision.io import decode_jpeg, read_file
from torchvision.transforms import ToTensor
from PIL import Image

filepath = ""./test/assets/encode_jpeg/grace_hopper_517x606.jpg""  # 606 x 517

print(torch.randint(0, 256, (3, 606, 517), dtype=torch.uint8).stride())  # (313302, 517, 1)
print(ToTensor()(Image.open(filepath)).stride())  # (313302, 517, 1))
print(decode_jpeg(read_file(filepath)).stride())  # (1, 1551, 3)  -- this makes Resize() 8X slower because antialias is False by default.
```"
`image` extension can't be loaded in fresh conda env with torchvision 0.13.1,pytorch/vision,2022-08-19 09:07:29,2,,6453,1344165858,"### 🐛 Describe the bug

Hi,

Quick FYI that I'm getting this warning when installing torch and torchvision from a fresh conda env
```
/private/home/fmassa/.conda/envs/fairvit2/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot
open shared object file: No such file or directory
  warn(f""Failed to load image Python extension: {e}"")
```

This can be reproduced by doing
```
create -f conda.yaml
```
with the follwoing `conda.yaml`
```yaml
name: bug_repro
channels:
  - defaults
  - pytorch
  - conda-forge
dependencies:
  - python=3.9
  - pytorch=1.12.1
  - torchvision=0.13.1
  - cudatoolkit=11.3
  - nvidia-apex=0.1
  - timm
  - pip
```

### Versions

Python 3.9
PyTorch 1.12.1
TorchVision 0.13.1
CUDA 11.3"
Add support for PolyLoss in torchvision ,pytorch/vision,2022-08-17 17:41:43,0,,6439,1342084617,"### 🚀 The feature

The goal would be to add a `PolyLoss` PyTorch module behaving exactly the same way as any PyTorch core loss module. And the implementation would follow the following paper: https://arxiv.org/abs/2204.12511

One question still pending though: where should this be located in torchvision?

### Motivation, pitch

As discussed in #6323, some losses have popped up over the last year. Before they can make their way to core, we should integrate those in torchvision first.

### Alternatives

_No response_

### Additional context

_No response_"
Syntax error setting up torchvision,pytorch/vision,2022-08-17 14:37:32,4,,6438,1341868087,"Setting up torchvision for nvidia jetson.  Run into the following error when executing 
`$ git clone https://github.com/pytorch/vision<br/>
$ cd vision<br/>
$ sudo python setup.py install`

Error : 
`  File ""setup.py"", line 49<br/>
    f.write(f""__version__ = '{version}'\n"")<br/>
                                         ^
`"
Memory leak on GaussianBlur,pytorch/vision,2022-08-17 11:33:02,6,module: transforms,6437,1341624923,"### 🐛 Describe the bug

Hello. When using num_workers > 0 for dataloader and GaussianBlur BEFORE the resize function in transforms (images in dataset are of different size) a memory leak appears. The larger num_workers used, the more the leak is (I ran out of 128 GB RAM in 300 iterations with batch_size of 32 and num_workers of 16).
To reproduce (you should initialize images with array of filepaths to images):

```
import torch
import glob
from torchvision import transforms
from PIL import Image
class FramesDataset(torch.utils.data.Dataset):
    def __init__(self, images):
        self.images = images
        self.init_base_transform()

    def __len__(self):
        return len(self.images)

    def init_base_transform(self):
        self.tr_aug = transforms.Compose([transforms.GaussianBlur(7, (1, 5)),
                                          transforms.Resize((256, 256), antialias=True),
                                          transforms.ToTensor(),
                                          transforms.Normalize([0.5]*3, [0.5]*3) ])

    def __getitem__(self, idx):
        img = Image.open(self.images[idx]).convert('RGB')
        out = self.tr_aug(img)
        return out

dataset = TestDataset(images)
dl = torch.utils.data.DataLoader(dataset, batch_size = 16, num_workers = 8, pin_memory = False)
while True:
    for batch in dl:
        pass
```

### Versions

torch: 1.12.0+cu116
torchvision: 0.13.0+cu116
PIL: 9.0.0
Ubuntu: 20.04.4 LTS

cc @vfdev-5 @datumbox"
New Feature: Dice Loss,pytorch/vision,2022-08-17 10:47:34,3,module: ops#new feature,6435,1341570860,"### 🚀 The feature

Followup to #6323 

Addition of Dice Loss to torchvision.


### Motivation, pitch

Mainly Dice loss is used for semantic segmentation.

I want to understand the technical aspects of adding it to torchvision. Are we going to support boolean tensors or outputs from the semantic segmentation models?

Few references.

https://github.com/pytorch/pytorch/issues/1249#issuecomment-305088398

https://github.com/rogertrullo/pytorch/blob/rogertrullo-dice_loss/torch/nn/functional.py#L708

MONOAI

https://docs.monai.io/en/stable/_modules/monai/losses/dice.html#DiceLoss

https://kornia.readthedocs.io/en/latest/_modules/kornia/losses/dice.html#dice_loss



### Alternatives

_No response_

### Additional context

_No response_"
Inference pre-processing transforms when not using pre-trained weights?,pytorch/vision,2022-08-16 14:18:17,2,,6428,1340429021,"This is somewhat of a niche use-case. What would be a good way to run the pre-processing transforms when we don't need pre-trained weights? This happens in my case because I'm just benchmarking for speed, but it could also be the case of a user not having easy access to the internet.

```py
batch = torch.randint(0, 256, size=(4, 3, 224, 224))

model = models.vit_h_14()  # I don't need pre-trained weights
weights = models.ViT_H_14_Weights.DEFAULT  # I only need this to pre-process the input
inpt = weights.transforms()(batch)
model(inpt)
``` 

fails with

```
Wrong image height!   # BTW it'd be nice to indicate the expected and actual sizes, I had to dig to find that the issue was 518 != 224
```

because the `image_size` parameter wasn't overridden since `weights is None`:

https://github.com/pytorch/vision/blob/701d7731660ea54f1ab00c792e9e018569035e2d/torchvision/models/vision_transformer.py#L318-L322"
"Resume with torchrun: The CPU memory consumption keeps increasing when using the train code of image classification with ""--resume"" in references.",pytorch/vision,2022-08-15 03:13:15,0,,6416,1338481050,"### 🐛 Describe the bug

I am training the ViT-B16 using the reference code provided by PyTorch: 
[https://github.com/pytorch/vision/blob/a61e6ef6ff5af041661ecc70b1a7e3dacb2240b6/references/classification/train.py](url). 

However, when I resume the training using this code with the distributed mode (using torchrun), I observed increasing CPU memory consumption epoch by epoch, which leads to program termination after resuming and running several epochs. The train script is:
`torchrun --nproc_per_node=8 train.py\
    --model vit_b_16 --epochs 300 --batch-size 128--opt adamw --lr 0.003 --wd 0.3\
    --lr-scheduler cosineannealinglr --lr-warmup-method linear --lr-warmup-epochs 30\
    --lr-warmup-decay 0.033 --amp --label-smoothing 0.11 --mixup-alpha 0.2 --auto-augment ra\
    --clip-grad-norm 1 --ra-sampler --cutmix-alpha 1.0 --model-ema\
    --resume path_to_my_checkpoint`

Could you help to solve the bug?


### Versions

PyTorch version: 1.12.1+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.24.0-rc2
Libc version: glibc-2.31

Python version: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-121-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 515.48.07
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.19.5
[pip3] numpydoc==1.1.0
[pip3] pytorch3d==0.6.1
[pip3] torch==1.12.1+cu113
[pip3] torchaudio==0.12.1+cu113
[pip3] torchvision==0.13.1+cu113
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.3.1               h2bc3f7f_2
[conda] mkl                       2021.4.0           h06a4308_640
[conda] mkl-service               2.4.0            py39h7f8727e_0
[conda] mkl_fft                   1.3.1            py39hd3c417c_0
[conda] mkl_random                1.2.2            py39h51133e4_0
[conda] numpy                     1.19.5                   pypi_0    pypi
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] pytorch3d                 0.6.1                    pypi_0    pypi
[conda] torch                     1.12.1+cu113             pypi_0    pypi
[conda] torchaudio                0.12.1+cu113             pypi_0    pypi
[conda] torchvision               0.13.1+cu113             pypi_0    pypi

Not that for torchvision, I directly replaced with the version on github on August 10."
[FEAT] Add MobileViT v1 & v2 ,pytorch/vision,2022-08-12 05:35:33,12,module: models,6404,1336773099,"### 🚀 The feature

As described in the [RFC ](https://github.com/pytorch/vision/issues/6323)""Batteries includes, phase 3"", I am working on adding MobileViT v1 and v2 inspired by the following code repos/snippets:

- https://github.com/chinhsuanwu/mobilevit-pytorch
- https://github.com/apple/ml-cvnets/blob/main/cvnets/models/classification/mobilevit_v2.py 

The original paper can be found [here](https://arxiv.org/pdf/2110.02178.pdf).

### Motivation, pitch

This has been decided in the RFC.

### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox"
Performance Issue in nmk_kernel,pytorch/vision,2022-08-12 00:09:01,2,module: ops#Perf,6403,1336610709,"### 🚀 The feature



Hi, when analyzing the profiling results of [detectron2_fasterrcnn_r_101_fpn](https://github.com/pytorch/benchmark) in inference mode,  I found some inefficiencies. More details described in the following box.

### Motivation, pitch

The batch size is 4, so there will be 4 calls for `batched_nms`. Deep to the bottom of the calling context tree, there are some long calls for`cudaMemcpyAsync` and following empty.
<img width=""1152"" alt=""Screen Shot 2022-08-11 at 4 42 01 PM"" src=""https://user-images.githubusercontent.com/3823240/184260161-5767aa3b-cf62-40e6-a097-976078d5778a.png"">

The corresponding source code is the following part. Line 136 is the memory copy, and I believe the following empty are the for loop at Line 148-Line 159. 

https://github.com/pytorch/vision/blob/7fb8d068618f6b913f3d8489b74bf95911184e0f/torchvision/csrc/ops/cuda/nms_kernel.cu#L136-L164

From the profiling results I collected, if we can make it more efficient like half of the current execution time for those lines, the speedup for detectron2 series model would be 1.02X and for timm_efficientdet would be 1.15X. 

I did try to change it on GPU. please see [this minimal reproduce repo](https://github.com/FindHao/test_nms_kernel/blob/master/test.cu). But it looks simply moving the current logic to GPU will make it worse because of the data dependency. In my testing, the GPU version is 1.40X slower than CPU version. But I'll leave the repo there to inspire more people and make the testing simple.  

I'm not so familiar with the algorithm here, is there anyway to get rid of the data dependency here?

### Alternatives

_No response_

### Additional context

_No response_"
String parameter cannot be passed to `torchvision.ops.deform_conv2d`,pytorch/vision,2022-08-10 18:20:34,3,enhancement#module: ops,6394,1335006531,"### 🐛 Describe the bug

## Bug Explanation
The current implementation of `torchvision.ops.deform_conv2d` implicitly assumes that padding is passed as a tuple or an integer, this means that if the padding is passed as either ""same"" or ""valid"" then torchvision is going to parse it as `padding = (""same"")`.


## Sample Code
``` python
import torch

X = torch.randn((1,3,5,5))
torchvision.ops.deform_conv2d(
            X,
            torch.randn([1, 2 * 1 * 3 * 3, 3, 3]), #offset
            torch.randn([1, 3, 3, 3]),
            torch.randn([1]),
            stride=1,
            padding=""zero"",
            dilation=1,
            mask=None,
        )
```
## Stacktrace
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
d:\Documents\Python Scripts\test.ipynb Cell 13 in <cell line: 2>()
      [1](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=0) X = torch.randn((1,3,5,5))
----> [2](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=1) torchvision.ops.deform_conv2d(
      [3](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=2)             X,
      [4](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=3)             torch.randn([1, 2 * 1 * 3 * 3, 3, 3]), #offset
      [5](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=4)             torch.randn([1, 3, 3, 3]),
      [6](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=5)             torch.randn([1]),
      [7](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=6)             stride=1,
      [8](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=7)             padding=""zero"",
      [9](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=8)             dilation=1,
     [10](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=9)             mask=None,
     [11](vscode-notebook-cell:/d%3A/Documents/Python%20Scripts/test.ipynb#X14sZmlsZQ%3D%3D?line=10)         )

File d:\Installations\Anaconda\lib\site-packages\torchvision\ops\deform_conv.py:77, in deform_conv2d(input, offset, weight, bias, stride, padding, dilation, mask)
     74     bias = torch.zeros(out_channels, device=input.device, dtype=input.dtype)
     76 stride_h, stride_w = _pair(stride)
---> 77 pad_h, pad_w = _pair(padding)
     78 dil_h, dil_w = _pair(dilation)
     79 weights_h, weights_w = weight.shape[-2:]

ValueError: too many values to unpack (expected 2)
```








































### Versions

```
Collecting environment information...
PyTorch version: 1.12.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Home
GCC version: (x86_64-posix-seh, Built by strawberryperl.com project) 8.3.0
Clang version: Could not collect
CMake version: version 3.24.0-rc1
Libc version: N/A

Python version: 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19041-SP0
Is CUDA available: True
CUDA runtime version: 11.5.50
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1650
Nvidia driver version: 512.15
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.23.1
[pip3] numpydoc==1.4.0
[pip3] torch==1.12.1
[pip3] torch-tb-profiler==0.4.0
[pip3] torchaudio==0.12.1
[pip3] torchvision==0.13.1
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.3.1               h59b6b97_2
[conda] mkl                       2021.4.0           haa95532_640
[conda] mkl-service               2.4.0            py38h2bbff1b_0
[conda] mkl_fft                   1.3.1            py38h277e83a_0
[conda] mkl_random                1.2.2            py38hf11a4ad_0
[conda] numpy                     1.23.1           py38h7a0a035_0
[conda] numpy-base                1.23.1           py38hca35cd5_0
[conda] numpydoc                  1.4.0            py38haa95532_0
[conda] pytorch                   1.12.1          py3.8_cuda11.3_cudnn8_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch                     1.12.1                   pypi_0    pypi
[conda] torch-tb-profiler         0.4.0                    pypi_0    pypi
[conda] torchaudio                0.12.1               py38_cu113    pytorch
[conda] torchvision               0.13.1               py38_cu113    pytorch
```"
"`Label.from_category` is O(num_classes) complexity, for every single sample",pytorch/vision,2022-08-04 09:50:30,6,,6368,1328366758,"This function is called when loading every single sample, but its complexity is `O(num_classes)` because of the `.index()` call, which is probably overkill. A simple dict lookup should be enough and much faster here.

https://github.com/pytorch/vision/blob/96aa3d928c6faaf39defd846966421679244412d/torchvision/prototype/features/_label.py#L35-L43


CC @pmeier "
[FEEDBACK] Model Registration beta API,pytorch/vision,2022-08-03 14:42:44,2,,6365,1327330412,"### 🚀 Feedback Request

This issue is dedicated for collecting community feedback on the Model Registration API. Please review the dedicated [RFC](https://github.com/pytorch/vision/pull/6330) and [blogpost](https://pytorch.org/blog/easily-list-and-initialize-models-with-new-apis-in-torchvision/) where we describe the API in detail and provide an overview of its features.

We would love to get your thoughts, comments and input in order to finalize the API and include it on the new release of TorchVision."
[RFC] Support YOLOX detection model,pytorch/vision,2022-08-01 13:59:32,20,needs discussion#module: models#topic: object detection#new feature,6341,1324474963,"### 🚀 The feature

YOLO aka. You Only Look Once, which is a vibrant series of object detection models since the release of Joseph Redmon [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640).

So far a couple of more notable implementations are as follows (all PyTorch):

- [YOLOv3](https://arxiv.org/abs/1804.02767) 2018. Cited by 14369 [^yolov3-ultralytics][^yolov3-mmdet]
- [YOLOv4](https://arxiv.org/abs/2004.10934) 2020. Cited by 4905
- YOLOv5 2020. Starred at GitHub 29.4k [^yolov5-ultralytics]
- [ ] [YOLOX](https://arxiv.org/abs/2107.08430) 2021. Cited by 299 [^yolox]
- [YOLOv7](https://arxiv.org/abs/2207.02696) 2022. [^yolov7]

### Motivation, pitch

Until now, one of the most successful ones is probably YOLOv5. YOLOv5 is great, and they have also built up a very friendly community and ecosystem. We don't intend to copy YOLOv5 into TorchVision, our main goal here is to make training SoTA models easier and share reusable subcomponents to build the next SoTA models in the same/proxy family.[^yolo-keras]

YOLOX is a high-performance anchor-free YOLO, and it has a good balance in terms of copyright and code quality, it's enough to have a YOLOX implementation from the community's perspective.

### The License

YOLO{v5/v7} are built under the [GPL-3.0 license](https://github.com/ultralytics/yolov5/blob/master/LICENSE), and YOLOX is built under the [Apache-2.0 license](https://github.com/Megvii-BaseDetection/YOLOX/blob/main/LICENSE).

### More context

I have previously rewritten the code used in the inference part of YOLOv5 according to the style and specification of torchvision[^yolort], and I can relicense that part to [BSD-3-Clause license](https://github.com/pytorch/vision/blob/main/LICENSE). The amount of work involved in the model inference part is not much with the help of YOLOX base code.

Data augmentation and a new trainer engine will be the core of what we will do here.

The data augmentation section is in the planning list https://github.com/pytorch/vision/issues/6224 , and we have already merged some augmentation methods like https://github.com/pytorch/vision/pull/5825 , I think it would help us to build the next SoTA models with a new primitives like classification models.[^classification-primitives]

- [Mosaic](https://github.com/ultralytics/yolov5/blob/2e10909905b1e0e7eb7bac086600fe7ee2c0e6a5/utils/dataloaders.py#L691)
- [augment_hsv](https://github.com/ultralytics/yolov5/blob/0669f1b27bbdcbdbb0e2baf4e9f09c6fc8337ec7/utils/augmentations.py#L47)
- [random_perspective](https://github.com/ultralytics/yolov5/blob/0669f1b27bbdcbdbb0e2baf4e9f09c6fc8337ec7/utils/augmentations.py#L124)
- [mixup](https://github.com/ultralytics/yolov5/blob/0669f1b27bbdcbdbb0e2baf4e9f09c6fc8337ec7/utils/augmentations.py#L271)
- [random_flip, left-right](https://github.com/ultralytics/yolov5/blob/0669f1b27bbdcbdbb0e2baf4e9f09c6fc8337ec7/utils/dataloaders.py#L648)

As TorchVision adds more and more models, it may be time to abstract out a simple trainer engine for sharing reusable subcomponents. It might be more appropriate to open a new thread for necessity and specific steps about this part.

[^yolov3-ultralytics]: https://github.com/ultralytics/yolov3/tree/v9.1
[^yolov3-mmdet]: https://github.com/open-mmlab/mmdetection/tree/master/configs/yolo
[^yolov5-ultralytics]: https://github.com/ultralytics/yolov5
[^yolox]: https://github.com/Megvii-BaseDetection/YOLOX
[^yolov7]: https://github.com/WongKinYiu/yolov7
[^yolo-keras]: https://github.com/keras-team/keras-cv/issues/622#issuecomment-1198063712
[^yolort]: https://github.com/zhiqwang/yolov5-rt-stack/tree/main/yolort/models
[^classification-primitives]: https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/

cc @datumbox @YosuaMichael @oke-aditya "
download datasets using bittorrent,pytorch/vision,2022-08-01 08:29:04,4,module: datasets#new feature,6363,1327285033,"### 🚀 The feature, motivation and pitch

I'm the sysadmin of one of the servers that hosts datasets that appear on `torch.datasets` (`DTD`, `FGVCAircraft`, `Flowers102`, and `OxfordIIITPet`) . We're happy that our stuff is so widely used but that's also taking a hit on other stuff on the server and the downloads sometimes take a very long time.

We've prepared torrents for some of our datasets, uploaded them to [academic torrents](https://academictorrents.com/), and recommend to use it over http. I was wondering if it was possible to add the option to download datasets over bittorrent.

### Alternatives

_No response_

### Additional context

_No response_

cc @pmeier @YosuaMichael"
[RFC] Batteries Included - Phase 3,pytorch/vision,2022-07-27 14:43:03,48,help wanted#module: ops#module: models#module: transforms#module: reference scripts#new feature,6323,1319671706,"### 🚀 The feature

*Note: To track the progress of the project check out [this board](https://github.com/pytorch/vision/projects/6).*

This is the 3rd phase of TorchVision's modernization project (see phase [1](https://github.com/pytorch/vision/issues/3911) and [2](https://github.com/pytorch/vision/issues/5410)). We aim to keep TorchVision relevant by ensuring it provides off-the-shelf all the necessary primitives, model architectures and recipe utilities to produce SOTA results for the supported  Computer Vision tasks. 

<h2 id=""new-primitives"">1. New Primitives</h2>

To enable our users to reproduce the latest state-of-the-art research we will enhance TorchVision with the following data augmentations, layers, losses and other operators:

### Data Augmentations

- [ ] [AutoAugment for Detection](https://arxiv.org/abs/1906.11172) [[1](https://github.com/Jasonlee1995/AutoAugment_Detection), [2](https://github.com/tensorflow/tpu/blob/master/models/official/detection/utils/autoaugment_utils.py)] - #6224 #6609
- [ ] [Mosaic](https://arxiv.org/pdf/2004.10934.pdf) [[1](https://colab.research.google.com/drive/1YWb7a_3bHqG30SoIxU5S4lHKktkRseyY?usp=sharing), [2](https://github.com/ultralytics/yolov5/blob/2e10909905b1e0e7eb7bac086600fe7ee2c0e6a5/utils/dataloaders.py#L691)] - #6534
- [ ] [Mixup for Detection](https://arxiv.org/pdf/1902.04103v1.pdf) [[1](https://github.com/Megvii-BaseDetection/YOLOX/blob/a5f629a6d28fcc3742ce9483698b3376ce457533/yolox/data/datasets/mosaicdetection.py#L162-L234), [2](https://github.com/open-mmlab/mmdetection/blob/1376e77e6ecbaad609f6003725158de24ed42e84/mmdet/datasets/pipelines/transforms.py#L2347)] - #6720 #6721

### Losses
- [ ] [Dice Loss](https://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf) [[1](https://github.com/pytorch/pytorch/issues/1249#issuecomment-305088398), [2](https://github.com/rogertrullo/pytorch/blob/rogertrullo-dice_loss/torch/nn/functional.py#L708)] - #6435
- [ ] [Poly Loss](https://arxiv.org/abs/2204.12511) [[1](https://github.com/yiyixuxu/polyloss-pytorch/blob/master/PolyLoss.py), [2](https://github.com/abhuse/polyloss-pytorch/blob/main/polyloss.py)] - #6439 #6457

### Operators added in PyTorch Core

- [ ] [LARS Optimizer](https://arxiv.org/abs/1708.03888) [[1](https://github.com/4uiiurz1/pytorch-lars/blob/master/lars.py), [2](https://lightning-flash.readthedocs.io/en/0.5.0/_modules/flash/core/optimizers/lars.html#LARS)] - #6868
- [ ]  [LAMB Optimizer](https://arxiv.org/abs/1904.00962) [[1](https://pytorch-optimizer.readthedocs.io/en/latest/_modules/torch_optimizer/lamb.html), [2](https://lightning-flash.readthedocs.io/en/0.5.0/_modules/flash/core/optimizers/lamb.html)]
- [x] [Polynomial LR Scheduler](https://github.com/pytorch/pytorch/issues/79511) [[1](https://github.com/pytorch/vision/issues/4438), [2](https://github.com/cmpark0126/pytorch-polynomial-lr-decay/blob/master/torch_poly_lr_decay/torch_poly_lr_decay.py)] - [code](https://github.com/pytorch/vision/issues/4438#issuecomment-1202722898) - https://github.com/pytorch/pytorch/pull/82769

<h2 id=""new-models"">2. New Architectures & Model Iterations</h2>

To ensure that our users have access to the most popular SOTA models, we will add the following architectures along with pre-trained weights:

### Image Classification

- [x] [Swin Transformer V2](https://arxiv.org/abs/2111.09883) - #6242 #6246
- [ ] MobileViT [v1](https://arxiv.org/abs/2110.02178) & [v2](https://arxiv.org/abs/2206.02680) [[1](https://github.com/chinhsuanwu/mobilevit-pytorch), [2](https://github.com/apple/ml-cvnets/blob/main/cvnets/models/classification/mobilevit_v2.py)] - #6404
- [x] [MaxViT](https://arxiv.org/abs/2204.01697) - #6342

### Video Classification

- [x] [MViTv2](https://arxiv.org/abs/2112.01526) [[1](https://github.com/facebookresearch/SlowFast/commit/1aebd71a2efad823d52b827a3deaf15a56cf4932)] - #6373
- [ ] Swin3d [[1](https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py)] - #6499 #6521
- [x] [S3D](https://arxiv.org/abs/1712.04851) [[1](https://github.com/kylemin/S3D/blob/master/model.py)] - #6402 #6412 #6537

<h2 id=""new-weights"">3. Improved Training Recipes & Pre-trained models</h2>

To ensure that are users can have access to strong baselines and SOTA weights, we will improve our training recipes to incorporate the newly released primitives and offer improved pre-trained models:

### Reference Scripts
- [ ] Update the Reference Scripts to use the latest primitives - #6405 #6433

### Pre-trained weights
- [ ] Improve the accuracy of Video models

----

## Other Candidates

There are several other Operators (#5414), Losses (#2980), Augmentations (#3817) and Models (#2707) proposed by the community. Here are some potential candidates that we could implement depending on bandwidth. Contributions are welcome for any of the below:

- [YOLOX](https://arxiv.org/abs/2107.08430) [[1](https://github.com/Megvii-BaseDetection/YOLOX)] - #6341
- [DeTR](https://arxiv.org/abs/2005.12872) - #5922 #6922
- [U-Net](https://arxiv.org/abs/1505.04597) - #6610 #6611
- MViTv2 for Images [[1](https://github.com/facebookresearch/mvit)]
- [Video Transformer Network](https://arxiv.org/abs/2102.00719) [[1](https://github.com/bomri/SlowFast/blob/master/slowfast/models/video_model_builder.py#L765)]
- [MTV](https://arxiv.org/abs/2201.04288)
- [Deformable DeTR](https://arxiv.org/abs/2010.04159)
- [Shortcut Regularizer](https://github.com/pytorch/vision/pull/4549) (FX-based)
- [Hide-and-Seek](https://arxiv.org/abs/1704.04232) - #6796

cc @datumbox @vfdev-5"
[v0.13.1] Bugfix Release Tracker,pytorch/vision,2022-07-21 11:38:32,8,,6300,1313121547,Same as https://github.com/pytorch/vision/issues/6081 but for 0.13.1.
Binary MacOS wheels relocations are silently failing?,pytorch/vision,2022-07-21 11:31:47,0,,6299,1313113543,"I don't know if this is an actual issue or not, but it looks like the dylib relocation step of the macos wheel jobs are failing. For example  [this recent job](https://app.circleci.com/pipelines/github/pytorch/vision/18916/workflows/94506257-fb22-49a2-b6bc-d483aab97e13/jobs/1530556) below.  @atalman @malfet  would you know if this is a problem, or if this can be ignored?

```
+ delocate-wheel -v --ignore-missing-dependencies torchvision-0.14.0.dev20220721-cp310-cp310-macosx_10_9_x86_64.whl
Fixing: torchvision-0.14.0.dev20220721-cp310-cp310-macosx_10_9_x86_64.whl
ERROR:delocate.libsana:
@rpath/libc10.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libtorch.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libtorch_cpu.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libtorch_python.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libc10.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libc10.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
ERROR:delocate.libsana:
@rpath/libtorch.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libtorch.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
ERROR:delocate.libsana:
@rpath/libtorch_cpu.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libtorch_cpu.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
ERROR:delocate.libsana:
@rpath/libtorch_python.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libtorch_python.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/_C.so
ERROR:delocate.libsana:
@rpath/libc10.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libtorch.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libtorch_cpu.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libtorch_python.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:
@rpath/libc10.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libc10.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
ERROR:delocate.libsana:
@rpath/libtorch.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libtorch.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
ERROR:delocate.libsana:
@rpath/libtorch_cpu.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libtorch_cpu.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
ERROR:delocate.libsana:
@rpath/libtorch_python.dylib not found:
  Needed by: /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
  Search path:
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib
    /Users/distiller/miniconda3/envs/env3.10/lib/
    
ERROR:delocate.libsana:@rpath/libtorch_python.dylib not found, requested by /private/var/folders/bl/wbxjgtzx7j5_mjsmfr3ynlc00000gp/T/tmpndqnqrqe/wheel/torchvision/image.so
INFO:delocate.delocating:Copying library /Users/distiller/miniconda3/envs/env3.10/lib/libc++.1.0.dylib to torchvision/.dylibs/libc++.1.0.dylib
INFO:delocate.delocating:Copying library /Users/distiller/miniconda3/envs/env3.10/lib/libpng16.16.dylib to torchvision/.dylibs/libpng16.16.dylib
INFO:delocate.delocating:Copying library /Users/distiller/miniconda3/envs/env3.10/lib/libjpeg.9.dylib to torchvision/.dylibs/libjpeg.9.dylib
INFO:delocate.delocating:Copying library /Users/distiller/miniconda3/envs/env3.10/lib/libz.1.2.12.dylib to torchvision/.dylibs/libz.1.2.12.dylib
INFO:delocate.delocating:Modifying install name in torchvision/.dylibs/libpng16.16.dylib from @rpath/libz.1.dylib to @loader_path/libz.1.2.12.dylib
INFO:delocate.delocating:Modifying install name in torchvision/image.so from @rpath/libpng16.16.dylib to @loader_path/.dylibs/libpng16.16.dylib
INFO:delocate.delocating:Modifying install name in torchvision/_C.so from @rpath/libc++.1.dylib to @loader_path/.dylibs/libc++.1.0.dylib
INFO:delocate.delocating:Modifying install name in torchvision/image.so from @rpath/libc++.1.dylib to @loader_path/.dylibs/libc++.1.0.dylib
INFO:delocate.delocating:Modifying install name in torchvision/image.so from @rpath/libjpeg.9.dylib to @loader_path/.dylibs/libjpeg.9.dylib
Copied to package .dylibs directory:
  /Users/distiller/miniconda3/envs/env3.10/lib/libc++.1.0.dylib
  /Users/distiller/miniconda3/envs/env3.10/lib/libjpeg.9.dylib
  /Users/distiller/miniconda3/envs/env3.10/lib/libpng16.16.dylib
  /Users/distiller/miniconda3/envs/env3.10/lib/libz.1.2.12.dylib

CircleCI received exit code 0
```"
Position aware circular convolution,pytorch/vision,2022-07-20 09:04:33,4,needs discussion#module: ops#feature,6288,1310706263,"### 🚀 The feature

Recently, we propose a new basic operation position aware global circular convolution (ParC). Differing from previous convolution operations, the proposed ParC has global receptive field. Experimental results show that ParC uniformly
improves performance of various typical models.

This work has been accpted by ECCV 2022, we hope the proposed ParC can be used by other researchers. Please refer to refer to https://arxiv.org/abs/2203.03952 to find more details.

### Motivation, pitch

We are suggested to reach out to [torchvision](https://github.com/pytorch/vision) when we post this issue in pytorch. 

https://github.com/pytorch/pytorch/issues/80932#issuecomment-1178268557

### Alternatives

We will prepare a CUDA optimized version ParC following tutorial in https://pytorch.org/tutorials/advanced/cpp_extension.html.

We also want to propose a pull request to merge our code into torchvision. Will such a pull request be accpted ? If it is acceptable, is there anyone can provide us a demo of proposing this kind of pull request?

### Additional context

_No response_"
error: ‘AutoDispatchBelowADInplaceOrView’ is not a member of ‘at’,pytorch/vision,2022-07-19 13:43:47,0,needs reproduction#module: c++ frontend,6287,1309546960,"### 🐛 Describe the bug

When I tried to compile torchvision on the server, after I run ""**make""**, it reported ""error: ‘AutoDispatchBelowADInplaceOrView’ is not a member of ‘at'

I use libtorch 1.8.0 with cuda 10.2.

### Versions

[root@kml-dtmachine-7222-prod vision]# python3 collect_env.py 
Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: CentOS Linux release 7.8.2003 (Core) (x86_64)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
Clang version: Could not collect
CMake version: version 3.12.3
Libc version: glibc-2.3.4

Python version: 3.6.8 (default, Apr  2 2020, 13:34:55)  [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] (64-bit runtime)
Python platform: Linux-4.18.0-2.4.3.kwai.x86_64-x86_64-with-centos-7.8.2003-Core
Is CUDA available: N/A
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce GTX 1080 Ti
GPU 1: NVIDIA GeForce GTX 1080 Ti

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/lib64/libcudnn.so.8.0.5
/usr/lib64/libcudnn_adv_infer.so.8.0.5
/usr/lib64/libcudnn_adv_train.so.8.0.5
/usr/lib64/libcudnn_cnn_infer.so.8.0.5
/usr/lib64/libcudnn_cnn_train.so.8.0.5
/usr/lib64/libcudnn_ops_infer.so.8.0.5
/usr/lib64/libcudnn_ops_train.so.8.0.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[conda] Could not collect"
Places365 download failing,pytorch/vision,2022-07-14 09:11:36,4,bug#module: datasets,6268,1304494287,"Oh no, something went wrong in the scheduled workflow tests/download. 
Please look into it:

https://github.com/pytorch/vision/actions/runs/2669191752

Feel free to close this if this was just a one-off error.


cc @pmeier @YosuaMichael"
[RFC] Stereo Matching Datasets API,pytorch/vision,2022-07-11 13:51:08,3,needs discussion#module: datasets#new feature,6259,1300743326,"### 🚀 The feature

The proposed feature aims to **extend the current datasets API** with datasets that are geared towards `the task of Stereo Matching`. It's main use case is that of providing  **a unified way for consuming classic Stereo Matching datasets** such as: 

- [Middlebury2014](https://vision.middlebury.edu/stereo/data/scenes2014/)
- [Kitti](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo)
- [SceneFlow](https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html)

Other considered dataset additions are: Sintel, FallingThings, InStereo2K, ETH3D, Holopix50k. A high level preview of the dataset interface would be:

```python3
class StereoMatchingDataset(Dataset):
    def __init__(self, ...):
        # constructor code / dataset specific code

    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        # processing code
        # ...

        # imgs: Tuple[Tensor, Tensor] 
        # dispartieis: Tuple[Tensor, Tensor]
        # occlusion_masks: Tuple[Tensor, Tensor]

        if self.transforms is not None:
            imgs, disparities, occlusion_masks = self.transforms(imgs, disparities, occlusion_masks)

        img_left = imgs[0]
        img_right = imgs[1]
        disparity = disparities[0]
        occlusion_mask = occlusion_masks[0]

        return img_left, img_right, disparity, occlusion_mask
```

### Motivation, pitch

This API addition would cut down engineering time required for people that are looking into working on, experimenting, or evaluating Stereo Matching models or that want easy access to stereo image data.

Throughout the literature, recent methods ([1](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf), [2](https://arxiv.org/pdf/2109.07547.pdf)) make use of multiple datasets that all have different formatting or specifications. A unified dataset API would streamline interacting with different datasources at the same time.

### Alternatives

The official repo for [RAFT-Stereo](https://github.com/princeton-vl/RAFT-Stereo/blob/main/core/stereo_datasets.py) provides a similar functionality for the datasets on which the network proposed in the paper was trained / evaluated. The proposed `StereoMatchingDataset` API would be largely similar to it, whilst following idiomatic `torchvision`.

### Additional context

### Stereo Matching task formulation.
**Commonly** throughout the literature the task of stereo matching requires `a reference image` (traditionally left image), `its stereo pair` (traditionally the right image), `the disparity map` (traditionally the left->right disparity) between the two images and `an occlusion / validity mask`  for pixels from the reference image that do not have a correspondent in the stereo pair (traditionally left->right). The proposed API would server data towards the user in the following manner:

### Proposal 1.
```python3
class StereoMatchingDataset(Dataset):
    def __init__(self, ...):
        # constructor code / dataset specific code

    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        # processing code
        # ...

        # imgs: Tuple[Tensor, Tensor] 
        # dispartieis: Tuple[Tensor, Tensor]
        # occlusion_masks: Tuple[Tensor, Tensor]

        if self.transforms is not None:
            imgs, disparities, occlusion_masks = self.transforms(imgs, disparities, occlusion_masks)

        img_left = imgs[0]
        img_right = imgs[1]
        disparity = disparities[0]
        occlusion_mask = occlusion_masks[0]

        return img_left, img_right, disparity, occlusion_mask
```

The above interface for data consumption is more aligned with the larger dataset ecosystem in `torchvision` where a dataset provides all the required tensors to perform training. **However**, this approach makes the **assumption** the user / algorithm does not require the right disparity map or the right occlusion mask. An alternative to this assumption would be a modification of the interface such that the user may be able to access the right-channel annotations:

### Proposal 2.
```python3
def __getitem__(self, index: int) -> Tuple[Tuple, Tuple, Tuple]:
        # processing code
        # ...

        # imgs: Tuple[Tensor, Tensor] 
        # dispartieis: Tuple[Tensor, Tensor]
        # occlusion_masks: Tuple[Tensor, Tensor]

        if self.transforms is not None:
            imgs, disparities, occlusion_masks = self.transforms(imgs, disparities, occlusion_masks)

        return imgs, disparities, occlusion_masks
         
# in user land, the API feeds all the available data to the user 
for (imgs, disparities, occlusion_masks) in stereo_dataloader:
        # however, the user becomes responsible to deconstruct the batch in order to get
        # the classic task definition data
        img_l, img_r, disparity, occlusion_mask = imgs[0], imgs[1], disparities[0], occlusion_masks[0]
        # ...
```

User feedback would be highly appreciated as it is highly unlikely one can be aware of all the use-cases / methods in Stereo Matching. Some preliminary pros and cons for each proposal:

### Proposal 1
**_Pros_**:
- Provides **strong guarantees** about the data (not all datasets provide disparities for both views, i.e ETH3D)
- Follows **the common specification** of the Stereo Matching task.
- The user is provided with a **familiar / idiomatic experience** and receives all the necessary data for training with **no need for additional data handling / manipulation**

**_Cons_**:
- It **can restrict data access** to the user with no out of the box way of recovering it (i.e. right disparity maps / occlusion masks). This would render the API unusable for some use-cases (if there are any).

### Proposal 2
**_Pros_**:
- The user **gets all the data** (left / right annotations instead of just left)

**_Cons_**:
- **Breaks away from the standard** of other dataset APIs in `torchvision`.
- **Forces the user to check** his data / data merging strategies (i.e. using ETH3D would yield `None` for the right channel annotations)
- Users need to **manually unpack** the data into `tensors` that are provided to `models / losses`.




cc @pmeier @YosuaMichael"
Issue in save_image utility in torch vision.utils,pytorch/vision,2022-07-11 01:31:38,4,,6255,1300087156,"### 🐛 Describe the bug

The `make_grid` utility here - https://github.com/pytorch/vision/blob/main/torchvision/utils.py#L24 - 
1. The normalize option here - https://github.com/pytorch/vision/blob/main/torchvision/utils.py#L43 - shifts the image to 0 and 1 only if `normalize = True`, which by default is `False`. 
2. Then in the `save_image` function here - https://github.com/pytorch/vision/blob/main/torchvision/utils.py#L138 - the images are being multiplied by 255 irrespective of weather the `normalize` argument is passed or not. 

The issue - 
1. Only when `normalize = True` should we multiply by 255 since if the values aren't between 0 and 1, then the multiplication by 255 will not make sense since the values can overflow above 255 which can cause problems depending on which file format is being used to save the `PIL.Image`. Also, 255 is very specific to the `PNG` format. If one wants to save the image as a TIFF file say, then the TIFF format takes care of floating point values for pixels as well (this could be made a feature). 

The solution according to me - 
1. `if` condition in `save_image` to multiply by 255 only when `normalize = True`.
2. 255 should only be multiplied if the `Image` is being saved as a .png image

### Versions

Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: CentOS Linux release 7.9.2009 (Core) (x86_64)
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
Clang version: Could not collect
CMake version: version 2.8.12.2
Libc version: glibc-2.17

Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-3.10.0-514.26.2.el7.x86_64-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 2080 Ti
GPU 1: NVIDIA GeForce RTX 2080 Ti

Nvidia driver version: 470.74
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.21.5
[pip3] numpydoc==1.2
[pip3] torch==1.11.0
[pip3] torchaudio==0.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.21.5           py38he7a7128_1  
[conda] numpy-base                1.21.5           py38hf524024_1  
[conda] numpydoc                  1.2                pyhd3eb1b0_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                0.11.0               py38_cu113    pytorch
[conda] torchvision               0.12.0               py38_cu113    pytorch"
Make `padding_mode` as Enum and strings,pytorch/vision,2022-07-08 11:25:25,4,enhancement#module: transforms#prototype,6250,1298888656,"Currently `padding_mode` is a string: ""constant"", ""edge"", ""reflect"", ""symmetric"" and in the codebase we already have enums like InterpolationMode and others in prototype space.

Let's create an enum for padding mode and convert the type into it while keeping BC by supporting string values.

cc @vfdev-5 @datumbox @bjuncek @pmeier"
GroupedBatchSampler in detection reference,pytorch/vision,2022-07-07 09:59:30,2,,6244,1297130979,"### 🐛 Describe the bug

Since we have `multiscale` and `lsj` augmentations which use fix sizes, we no longer resize the images in the transforms of `GeneralizedRCNNTransform`. However, we still use the `GroupedBatchSampler` but for me the grouping by aspect ratio is no longer mandatory. Am I right ?

Even though I don't think it is very useful (if the statement above is true), we can easily patch it.

We can change in the training reference script the following lines:
https://github.com/pytorch/vision/blob/bd19fb8ea9b1f67df2a2a1ee116874609ad3ee8c/references/detection/train.py#L191-L195
as
```
    if args.aspect_ratio_group_factor >= 0 and args.data_augmentation not in [""multiscale"", ""lsj""]:
        group_ids = create_aspect_ratio_groups(dataset, k=args.aspect_ratio_group_factor)
        train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, args.batch_size)
    else:
        train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, args.batch_size, drop_last=True)
```

### Versions

[pip3] numpy==1.21.5
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37he7a7128_1  
[conda] numpy-base                1.21.5           py37hf524024_1  
[conda] pytorch                   1.11.0          py3.7_cuda10.2_cudnn7.6.5_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.12.0               py37_cu102    pytorch"
Compilation error with cuda,pytorch/vision,2022-07-06 09:29:50,6,,6240,1295548747,"### 🐛 Describe the bug

When compiling vision with cuda enabled I am getting following error
CMake Error at /usr/local/share/cmake-3.22/Modules/CMakeTestCUDACompiler.cmake:56 (message):
   The CUDA compiler
     ""/usr/bin/nvcc""
is not able to compile a simple test program.

I was able to compile the library after adding following in CMakeLists.txt after if(WITH_CUDA) 
set(CMAKE_CUDA_COMPILER ""/usr/local/cuda-11.1/bin/nvcc"").


### Versions

PyTorch version: 1.8.2+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.22.5
Libc version: glibc-2.17

Python version: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.14.0-1044-oem-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: GPU 0: NVIDIA T600 Laptop GPU
Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.3.2
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.1
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.19.5
[pip3] numpy-quaternion==2021.11.4.15.26.3
[pip3] numpy-ros==0.1.3
[pip3] torch==1.8.2+cu111
[pip3] torchaudio==0.8.2
[pip3] torchvision==0.9.2+cu111
[conda] blas                      1.0                         mkl    conda-forge
[conda] cudatoolkit               10.1.243             h6bb024c_0    anaconda
[conda] libblas                   3.8.0                    14_mkl    conda-forge
[conda] libcblas                  3.8.0                    14_mkl    conda-forge
[conda] liblapack                 3.8.0                    14_mkl    conda-forge
[conda] mkl                       2019.4                      243    anaconda
[conda] mkl-service               2.3.0            py36he904b0f_0    anaconda
[conda] mkl_fft                   1.2.0            py36h23d657b_0    anaconda
[conda] mkl_random                1.1.0            py36hd6b4f25_0    anaconda
[conda] numpy                     1.19.5           py36hfc0c790_2    conda-forge
[conda] numpy-quaternion          2021.11.4.15.26.3          pypi_0    pypi
[conda] numpy-ros                 0.1.3                    pypi_0    pypi
[conda] torch                     1.8.2+cu111              pypi_0    pypi
[conda] torchaudio                0.8.2                    pypi_0    pypi
[conda] torchvision               0.9.2+cu111              pypi_0    pypi"
resize with pad transformation,pytorch/vision,2022-07-05 06:58:13,11,enhancement#module: transforms,6236,1293873060,"### 🚀 The feature

In tensorflow tf.image has a method, tf.image.resize_with_pad, that pads and resizes if the aspect ratio of input and output images are different to avoid distortion. I couldn't find an equivalent in torch transformations and had to write it myself. I think it would be a useful feature to have.  


### Motivation, pitch

When moving to pytoch from Tensorflow, one does not want to lose handy features! 

### Alternatives

_No response_

### Additional context

_No response_

cc @vfdev-5 @datumbox"
Nvidia Jetson Xavier - fails to load image Python extension and Couldn't load custom C++ ops when drawing bounding boxes,pytorch/vision,2022-07-02 17:11:25,1,,6231,1292098687,"### 🐛 Describe the bug

Note: This was posted to the PyTorch repo as issue # 80576

Errors when running approved combinations of: 
A) Pytorch 11.0 and Torchvision 0.12.0
and 
B) Pytorch 12.0 and Torchvision 0.13.0

Case A environment and run results:
________________________________________
davo@ubuntu:~$ python3 collect_env.py
Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (aarch64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.65-tegra-aarch64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.4.239
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] Could not collect

davo@ubuntu:~/yolov5$ python3 detect.py --source 0
'''/home/davo/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:
warn(f""Failed to load image Python extension: {e}"")'''
detect: weights=yolov5s.pt, source=0, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False
YOLOv5 rocket v6.1-258-g1156a32 Python-3.8.10 torch-1.11.0 CUDA:0 (Xavier, 31011MiB)

Fusing layers...
YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients
1/1: 0... Success (inf frames 640x480 at 30.00 FPS)

0: 480x640 Done. (4.190s)
0: 480x640 Done. (0.041s)
0: 480x640 Done. (0.041s)
0: 480x640 Done. (0.045s)

'''Traceback (most recent call last):
File ""detect.py"", line 252, in
main(opt)
File ""detect.py"", line 247, in main
run(**vars(opt))
File ""/home/davo/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
return func(*args, **kwargs)
File ""detect.py"", line 127, in run
pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
File ""/home/davo/yolov5/utils/general.py"", line 859, in non_max_suppression
i = torchvision.ops.nms(boxes, scores, iou_thres) # NMS
File ""/home/davo/.local/lib/python3.8/site-packages/torchvision/ops/boxes.py"", line 39, in nms
_assert_has_ops()
File ""/home/davo/.local/lib/python3.8/site-packages/torchvision/extension.py"", line 33, in _assert_has_ops
raise RuntimeError(
RuntimeError: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.version and your torchvision version with torchvision.version and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.
terminate called without an active exception
Aborted (core dumped)'''
______________________________________
______________________________________
Case B environment and run results:
_________________________________________
davo@ubuntu:~/yolov5$ python3 ../collect_env.py
Collecting environment information...
PyTorch version: 1.12.0a0+2c916ef.nv22.3
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (aarch64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.65-tegra-aarch64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.4.239
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: False

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==1.12.0a0+2c916ef.nv22.3
[pip3] torchvision==0.13.0
[conda] Could not collect

davo@ubuntu:~/yolov5$ python3 detect.py --source 0
'''/home/davo/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension:
warn(f""Failed to load image Python extension: {e}"")'''
detect: weights=yolov5s.pt, source=0, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False
YOLOv5 rocket v6.1-258-g1156a32 Python-3.8.10 torch-1.12.0a0+2c916ef.nv22.3 CUDA:0 (Xavier, 31011MiB)

Fusing layers...
YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients
1/1: 0... Success (inf frames 640x480 at 30.00 FPS)

0: 480x640 Done. (4.156s)
0: 480x640 Done. (0.043s)
0: 480x640 Done. (0.050s)
0: 480x640 Done. (0.045s)

'''Traceback (most recent call last):
File ""detect.py"", line 252, in
main(opt)
File ""detect.py"", linIncluded above - added here as well...
Case A environment

davo@ubuntu:~$ python3 collect_env.py
Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (aarch64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.65-tegra-aarch64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.4.239
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] Could not collect

Case B environment

davo@ubuntu:~/yolov5$ python3 ../collect_env.py
Collecting environment information...
PyTorch version: 1.12.0a0+2c916ef.nv22.3
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (aarch64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.65-tegra-aarch64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.4.239
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: False

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==1.12.0a0+2c916ef.nv22.3
[pip3] torchvision==0.13.0
[conda] Could not collect
cc @fmassa @vfdev-5 @pmeier
e 247, in main
run(**vars(opt))
File ""/home/davo/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
return func(*args, **kwargs)
File ""detect.py"", line 127, in run
pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
File ""/home/davo/yolov5/utils/general.py"", line 859, in non_max_suppression
i = torchvision.ops.nms(boxes, scores, iou_thres) # NMS
File ""/home/davo/.local/lib/python3.8/site-packages/torchvision/ops/boxes.py"", line 40, in nms
_assert_has_ops()
File ""/home/davo/.local/lib/python3.8/site-packages/torchvision/extension.py"", line 33, in _assert_has_ops
raise RuntimeError(
RuntimeError: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.version and your torchvision version with torchvision.version and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.
terminate called without an active exception
Aborted (core dumped)'''

### Versions

Included above - added here as well...
**_Case A environment_**

davo@ubuntu:~$ python3 collect_env.py
Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (aarch64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.65-tegra-aarch64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.4.239
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] Could not collect

**_Case B environment_**

davo@ubuntu:~/yolov5$ python3 ../collect_env.py
Collecting environment information...
PyTorch version: 1.12.0a0+2c916ef.nv22.3
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (aarch64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.65-tegra-aarch64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.4.239
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: False

Versions of relevant libraries:
[pip3] numpy==1.22.4
[pip3] torch==1.12.0a0+2c916ef.nv22.3
[pip3] torchvision==0.13.0
[conda] Could not collect

cc @fmassa @vfdev-5 @pmeier"
Add pos_weight in Focal Loss to trade off recall and precision,pytorch/vision,2022-07-01 14:24:16,1,,6229,1291479569,"### 🚀 The feature

I suggest adding the `pos_weight` (extra weights to positive examples) in Focal Loss implementation [here](https://github.com/pytorch/vision/blob/main/torchvision/ops/focal_loss.py#L38). 
[F.binary_cross_entropy_with_logits](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits) already has a `pos_weight` argument, so to add this feature, it's enough to create a new parameter and pass it to 
```
ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=""none"", pos_weight=pos_weight)
```


With the addition of this feature it will be possible to trade off recall and precision by adding weights to positive examples.

### Motivation, pitch

It’s possible to trade off recall and precision by adding weights to positive examples. For example, if a dataset contains $100$ positive and $300$ negative examples of a single class, then `pos_weight` for the class should be equal to 
$\frac{300}{100}=3$. The loss would act as if the dataset contains $3*100 = 300$ positive examples.

This idea already used in BCEWithLogitsLoss ([description](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)) PyTorch implementation.

For Binary Focal Loss this improvement will mean the following: $$FL_c = ( l_{1,c}, ... , l_{N,c} )^T$$

$$
l_{n, c} = -[p_c\ y_{n, c}\ \alpha\ (1-\sigma(x_{n,c}))^{\gamma}\ log(\sigma(x_{n,c})) + (1 - y)\ (1-\alpha)\ \sigma(x_{n,c})^{\gamma}\ log(1-\sigma(x_{n,c})]
$$

where $c$ is the class number ($c > 1$ for multi-label binary classification, $c=1$ for single-label binary classification), 
$n$ is the number of the sample in the batch and $p_c$ is the weight of the positive answer (`pos_weight`) for the class $c$.

$p_c > 1$ increases the recall, $p_c < 1$ increases the precision.








### Alternatives

_No response_

### Additional context

Ready to code it :)"
Swin in this repo + dynamic resolution,pytorch/vision,2022-06-30 21:28:26,7,,6227,1290658083,"### 📚 The doc issue

Does Swin impl in this repo support arbitrary dynamic execution-time-defined input resolution (same as other backbones)?


Initially Swin was trained to support only one resolution, but then hacks can be done to support arbitrary resolution. Two repos with such hacks:
* https://github.com/SwinTransformer/Swin-Transformer-Object-Detection/blob/master/mmdet/models/backbones/swin_transformer.py
* https://github.com/megvii-research/SOLQ/blob/main/models/swin_transformer.py

Related issues: https://github.com/microsoft/SimMIM/issues/13 https://github.com/microsoft/esvit/issues/17

cc @YosuaMichael "
Implement AutoAugment for Detection,pytorch/vision,2022-06-30 13:33:07,19,help wanted#module: transforms,6224,1290146251,"### 🚀 The feature

Implement [Learning Data Augmentation Strategies for Object Detection](https://arxiv.org/abs/1906.11172)
Refers to: #3817 

### Motivation, pitch

Good to have augmentation in Torchvision

### Alternatives

_No response_

### Additional context

_No response_

cc @vfdev-5 @datumbox"
Post Paper Architectural Optimization ReadMe update,pytorch/vision,2022-06-30 12:39:56,5,module: documentation,6223,1290078817,"### 📚 The doc issue

The announcement about the V2 improvements of RetinaNet, FasterRCNN and MaskRCNN has been made (Post Paper Architectural Optimization). Thanks for the amazing work.

However, if the references/detection/Readme is updated conveniently with the new configuration, it would be amazing to reproduce the results and utilize the new training scheme in other datasets. 

Thanks in advance 

### Suggest a potential alternative/fix

_No response_"
Swin_V2 addition with ImageNet22K pre-training,pytorch/vision,2022-06-29 08:51:56,2,module: models#new feature,6214,1288383437,"### 🚀 The feature

Thanks for the addition of Swin backbone to the repository. I think the added version is the V1 version of swin transformer. Is it possible to add also Swin-V2 with ImageNet22k pretraining? 

### Motivation, pitch

The motivation behind this request is that Swin-V2 with imagenet22k pre-trainings are utilized for recent SOTA performances. If this addition is completed, torchvision society will have an ability to replicate the SOTA results. 

### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox"
GeneralizedRCNNTransform output different shapes of images and targets,pytorch/vision,2022-06-29 07:54:33,5,enhancement#module: models#module: transforms#topic: object detection,6213,1288317281,"### 🐛 Describe the bug

I would expect the shape of images and targets consistent with each other (that is, have the same width and height). 

However, since torchvision.models.detection.transform.GeneralizedRCNNTransform call self.batch_images at the end of its forward function only on images, the output images and targets usually don't have the same shapes.

Example:
```
from torchvision.models.detection.transform import GeneralizedRCNNTransform
import torch

#initialize transformation
image_mean = [0.485, 0.456, 0.406]
image_std = [0.229, 0.224, 0.225]
T=GeneralizedRCNNTransform(800,1333,image_mean,image_std)

#initialize image & target
images=[torch.randn([3,1000,900])]
targets=[{'boxes':torch.tensor([[0,0,1000,900]]),'masks':torch.randn([1,1000,900]).byte()}]

#do experiment
images,targets=T(images,targets)

print(images.tensors.shape) 
print(images.image_sizes)
print(targets[0]['boxes'])
print(targets[0]['masks'].shape)

```

output:

torch.Size([1, 3, 896, 800])
[(888, 800)]
tensor([[  0.0000,   0.0000, 888.8889, 799.2000]])
torch.Size([1, 888, 800])

The default size_divisible parameter in self.batch_images is 32. Since 888 is not divisible by 32, it pads the image (888,800) to 
(896,800). But there is no change on 'boxes' and 'masks' and even images.image_sizes.

### Versions

Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-5.11.0-37-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.120
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070
Nvidia driver version: 470.57.02
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.20.1
[pip3] numpydoc==1.1.0
[pip3] pytorch-lightning==1.3.3
[pip3] torch==1.11.0
[pip3] torch-summary==1.4.5
[pip3] torchaudio==0.11.0
[pip3] torchmetrics==0.5.1
[pip3] torchvision==0.12.0
[pip3] vit-pytorch==0.22.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.2.0           h06a4308_296  
[conda] mkl-service               2.3.0            py38h27cfd23_1  
[conda] mkl_fft                   1.3.0            py38h42c9631_2  
[conda] mkl_random                1.2.1            py38ha9443f7_2  
[conda] numpy                     1.20.1           py38h93e21f0_0  
[conda] numpy-base                1.20.1           py38h7d8b39e_0  
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-lightning         1.3.3                    pypi_0    pypi
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torch                     1.11.0                   pypi_0    pypi
[conda] torch-summary             1.4.5                    pypi_0    pypi
[conda] torchaudio                0.11.0               py38_cu113    pytorch
[conda] torchmetrics              0.5.1                    pypi_0    pypi
[conda] torchvision               0.12.0                   pypi_0    pypi
[conda] vit-pytorch               0.22.0                   pypi_0    pypi


cc @datumbox @vfdev-5 @YosuaMichael"
Wrong for pytorch-nightly version,pytorch/vision,2022-06-27 08:44:00,1,question#module: models,6206,1285456890,"### 🐛 Describe the bug

The wrong is below:
Traceback (most recent call last):
  File ""/home/hxj/PycharmProjects/ImageNetTrain/main.py"", line 9, in <module>
    weights = P.models.ResNet50_Weights.IMAGENET1K_V1
AttributeError: module 'torchvision.prototype.models' has no attribute 'ResNet50_Weights'

### Versions

pytorch-nightly 1.13

cc @datumbox"
`video_reader` core dumps on specific video,pytorch/vision,2022-06-26 12:59:55,4,bug#module: video#triage review#high priority,6204,1284906972,"### 🐛 Describe the bug

```python
from torchvision.io import read_video
from torchvision import set_video_backend

path = ""./3caPS4FHFF8_000036_000046.mp4""

set_video_backend(""pyav"")
video, audio, info = read_video(path)
print(""pyav OK"")

set_video_backend(""video_reader"")
video, audio, info = read_video(path)
print(""video_reader OK"")
```
Output:
```
PyAV OK
malloc(): memory corruption
Aborted (core dumped)
```

The video file is part of Kinetics400. You can download it from [here](https://user-images.githubusercontent.com/5347466/175815132-86c9cb95-4d0c-4c10-ad3c-aa32e31026c1.mp4). Other videos pass. 


Dump from gdb:
```
malloc(): memory corruption

Thread 1 ""python"" received signal SIGABRT, Aborted.
__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
51	../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51
#1  0x00007ffff705e7f1 in __GI_abort () at abort.c:79
#2  0x00007ffff70a7837 in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7ffff71d4a7b ""%s\n"") at ../sysdeps/posix/libc_fatal.c:181
#3  0x00007ffff70ae8ba in malloc_printerr (str=str@entry=0x7ffff71d2cfc ""malloc(): memory corruption"") at malloc.c:5342
#4  0x00007ffff70b2a04 in _int_malloc (av=av@entry=0x7ffff7409c40 <main_arena>, bytes=bytes@entry=8304) at malloc.c:3748
#5  0x00007ffff70b355b in _int_memalign (av=0x7ffff7409c40 <main_arena>, alignment=64, bytes=<optimized out>) at malloc.c:4683
#6  0x00007ffff70b8fda in _mid_memalign (address=<optimized out>, bytes=8192, alignment=<optimized out>) at malloc.c:3324
#7  __posix_memalign (memptr=0x7fffffff9ca0, alignment=<optimized out>, size=8192) at malloc.c:5361
#8  0x00007ffec8079d76 in av_malloc () from ~/conda/envs/slowfast/lib/libavutil.so.56
#9  0x00007ffec806664a in av_buffer_alloc () from ~/conda/envs/slowfast/lib/libavutil.so.56
#10 0x00007ffec8066d34 in av_buffer_pool_get () from ~/conda/envs/slowfast/lib/libavutil.so.56
#11 0x00007ffec8786ac6 in avcodec_default_get_buffer2 () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#12 0x00007ffec878762b in ?? () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#13 0x00007ffec863323d in ?? () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#14 0x00007ffec8639ff5 in ?? () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#15 0x00007ffec863c0fe in ?? () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#16 0x00007ffec8784590 in ?? () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#17 0x00007ffec8785278 in avcodec_send_packet () from ~/conda/envs/slowfast/lib/libavcodec.so.58
#18 0x00007ffec9a58c02 in ffmpeg::Stream::analyzePacket(AVPacket const*, bool*) () from ~/repos/vision/torchvision/video_reader.so
#19 0x00007ffec9a59081 in ffmpeg::Stream::decodePacket(AVPacket const*, ffmpeg::DecoderOutputMessage*, bool, bool*) () from ~/repos/vision/torchvision/video_reader.so
#20 0x00007ffec9a543ee in ffmpeg::Decoder::processPacket(ffmpeg::Stream*, AVPacket*, bool*, bool*, bool) () from ~/repos/vision/torchvision/video_reader.so
#21 0x00007ffec9a5499a in ffmpeg::Decoder::getFrame(unsigned long) () from ~/repos/vision/torchvision/video_reader.so
#22 0x00007ffec9a5a6e1 in ffmpeg::SyncDecoder::decode(ffmpeg::DecoderOutputMessage*, unsigned long) () from ~/repos/vision/torchvision/video_reader.so
#23 0x00007ffec9a865bd in vision::video_reader::(anonymous namespace)::readVideo(bool, at::Tensor const&, std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long) ()
   from ~/repos/vision/torchvision/video_reader.so
#24 0x00007ffec9a88549 in vision::video_reader::read_video_from_file(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long) ()
   from ~/repos/vision/torchvision/video_reader.so
#25 0x00007ffec9a8b3be in std::decay<c10::guts::infer_function_traits<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::List<at::Tensor> (*)(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long), c10::List<at::Tensor>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long> > >::type::return_type>::type c10::impl::call_functor_with_args_from_stack_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::List<at::Tensor> (*)(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long), c10::List<at::Tensor>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long> >, false, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, 9ul, 10ul, 11ul, 12ul, 13ul, 14ul, 15ul, 16ul, 17ul, 18ul, std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long>(c10::OperatorKernel*, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*, std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, 9ul, 10ul, 11ul, 12ul, 13ul, 14ul, 15ul, 16ul, 17ul, 18ul>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long>*) () from ~/repos/vision/torchvision/video_reader.so
#26 0x00007ffec9a8b4f2 in c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::List<at::Tensor> (*)(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long), c10::List<at::Tensor>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long> >, false>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) () from ~/repos/vision/torchvision/video_reader.so
#27 0x00007fff4eeb95d0 in c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const () from ~/conda/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so
#28 0x00007fffc62202b4 in torch::jit::invokeOperatorFromPython(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, pybind11::args, pybind11::kwargs const&) ()
   from ~/conda/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#29 0x00007fffc61f872c in torch::jit::initJITBindings(_object*)::{lambda(std::string const&)#136}::operator()(std::string const&) const::{lambda(pybind11::args, {lambda(std::string const&)#136}::kwargs)#1}::operator()(pybind11, pybind11::args) const ()
   from ~/conda/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#30 0x00007fffc61f8f03 in void pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::{lambda(std::string const&)#136}::operator()(std::string const&) const::{lambda(pybind11::args, pybind11::kwargs)#1}, pybind11::object, {lambda(std::string const&)#136}, pybind11::args, pybind11::name, pybind11::doc>(torch::jit::initJITBindings(_object*)::{lambda(std::string const&)#136}::operator()(std::string const&) const::{lambda(pybind11::args, pybind11::kwargs)#1}&&, pybind11::object (*)({lambda(std::string const&)#136}, pybind11::args), pybind11::name const&, pybind11::doc const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail) () from ~/conda/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#31 0x00007fffc5ed98a4 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from ~/conda/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#32 0x000055555569ae64 in cfunction_call () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/methodobject.c:539
#33 0x00005555556947e4 in _PyObject_Call () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:281
#34 0x0000555555725bc3 in PyObject_Call (kwargs=0x7ffff6dbce40, args=0x7ffeb8a66b80, callable=0x7ffeb89ff9a0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:293
#35 do_call_core (kwdict=0x7ffff6dbce40, callargs=0x7ffeb8a66b80, func=0x7ffeb89ff9a0, tstate=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:5092
#36 _PyEval_EvalFrameDefault () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:3580
#37 0x00005555556dab32 in _PyEval_EvalFrame () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/internal/pycore_ceval.h:40
#38 _PyEval_EvalCode () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4327
#39 0x00005555556db697 in _PyFunction_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:396
#40 0x00005555556cb60a in _PyObject_FastCallDictTstate () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:118
#41 0x00005555556cba63 in _PyObject_Call_Prepend (kwargs=0x0, args=0x7ffeb8a66ac0, obj=0x7ffeb8a3c850, callable=0x7ffecb061670, tstate=0x5555559108c0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:489
#42 slot_tp_call () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/typeobject.c:6703
#43 0x0000555555694e4f in _PyObject_MakeTpCall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:191
#44 0x0000555555724e29 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=<optimized out>, args=0x555566d12818, callable=<optimized out>, tstate=<optimized out>)
    at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:116
#45 PyObject_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:127
#46 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x5555559108c0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:5072
#47 _PyEval_EvalFrameDefault () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:3487
---Type <return> to continue, or q <return> to quit---
#48 0x00005555556dab32 in _PyEval_EvalFrame () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/internal/pycore_ceval.h:40
#49 _PyEval_EvalCode () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4327
#50 0x00005555556db697 in _PyFunction_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:396
#51 0x0000555555652f00 in _PyObject_VectorcallTstate (kwnames=0x7ffeca7c7280, nargsf=<optimized out>, args=<optimized out>, callable=0x7ffeca253700, tstate=<optimized out>)
    at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:118
#52 PyObject_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:127
#53 call_function (kwnames=0x7ffeca7c7280, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:5072
#54 _PyEval_EvalFrameDefault (tstate=<optimized out>, f=0x55555f7aed50, throwflag=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:3535
#55 0x00005555556dad2b in _PyEval_EvalFrame () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/internal/pycore_ceval.h:40
#56 _PyEval_EvalCode () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4327
#57 0x00005555556db697 in _PyFunction_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:396
#58 0x00005555556531ca in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=<optimized out>, args=0x555558c72b48, callable=0x7ffeca253a60, tstate=<optimized out>)
    at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:118
#59 PyObject_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:127
#60 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x5555559108c0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:5072
#61 _PyEval_EvalFrameDefault (tstate=<optimized out>, f=0x555558c72950, throwflag=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:3487
#62 0x00005555556dab32 in _PyEval_EvalFrame () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/internal/pycore_ceval.h:40
#63 _PyEval_EvalCode () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4327
#64 0x00005555556db697 in _PyFunction_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Objects/call.c:396
#65 0x00005555556533be in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=<optimized out>, args=0x55555596dd20, callable=0x7ffeca259820, tstate=<optimized out>)
    at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:118
#66 PyObject_Vectorcall () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/cpython/abstract.h:127
#67 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x5555559108c0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:5072
#68 _PyEval_EvalFrameDefault (tstate=<optimized out>, f=0x55555596dbb0, throwflag=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:3518
#69 0x00005555556dab32 in _PyEval_EvalFrame () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Include/internal/pycore_ceval.h:40
#70 _PyEval_EvalCode () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4327
#71 0x000055555578addc in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=0, kwargs=<optimized out>, kwnames=<optimized out>, argcount=<optimized out>, args=<optimized out>, 
    locals=<optimized out>, globals=<optimized out>, _co=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4359
#72 PyEval_EvalCodeEx () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:4375
#73 0x00005555556dbb5b in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/ceval.c:826
#74 0x000055555578ae8b in run_eval_code_obj () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/pythonrun.c:1219
#75 0x00005555557bb215 in run_mod () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/pythonrun.c:1240
#76 0x0000555555666677 in pyrun_file (fp=0x5555559766c0, filename=0x7ffff6ec3ab0, start=<optimized out>, globals=0x7ffff6f53480, locals=0x7ffff6f53480, closeit=1, flags=0x7fffffffce28)
    at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/pythonrun.c:1138
#77 0x00005555557bfcef in pyrun_simple_file (flags=0x7fffffffce28, closeit=1, filename=0x7ffff6ec3ab0, fp=0x5555559766c0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/pythonrun.c:449
#78 PyRun_SimpleFileExFlags () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Python/pythonrun.c:482
#79 0x00005555557c03b8 in pymain_run_file (cf=0x7fffffffce28, config=0x5555559116d0) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Modules/main.c:379
#80 pymain_run_python (exitcode=0x7fffffffce20) at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Modules/main.c:604
#81 Py_RunMain () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Modules/main.c:683
#82 0x00005555557c05b9 in Py_BytesMain () at /home/builder/ktietz/cos6/ci_cos6/python-split_1622827272909/work/Modules/main.c:1129
#83 0x00007ffff703fc87 in __libc_start_main (main=0x55555566e430 <main>, argc=2, argv=0x7fffffffd018, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffd008) at ../csu/libc-start.c:310
#84 0x0000555555748a64 in _start ()
```


### Versions

Latest main branch.

```
Collecting environment information...
PyTorch version: 1.12.0.dev20220223
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.19.6
Libc version: glibc-2.27

Python version: 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-1069-aws-x86_64-with-glibc2.27
Is CUDA available: False
CUDA runtime version: 11.1.105
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy==0.910
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.20.3
[pip3] pytorchvideo==0.1.5
[pip3] torch==1.12.0.dev20220223
[pip3] torchdata==0.4.0a0+e8b010b
[pip3] torchvision==0.14.0a0+8cd742a
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.1.74              h6bb024c_0    nvidia
[conda] magma-cuda110             2.5.2                         1    pytorch
[conda] mkl                       2021.3.0           h06a4308_520  
[conda] mkl-include               2021.3.0           h06a4308_520  
[conda] mkl-service               2.4.0            py39h7f8727e_0  
[conda] mkl_fft                   1.3.0            py39h42c9631_2  
[conda] mkl_random                1.2.2            py39h51133e4_0  
[conda] numpy                     1.20.3           py39hf144106_0  
[conda] numpy-base                1.20.3           py39h74d4b33_0  
[conda] pytorch                   1.12.0.dev20220223 py3.9_cuda11.1_cudnn8.0.5_0    pytorch-nightly
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly
[conda] pytorchvideo              0.1.5                    pypi_0    pypi
[conda] torch                     1.11.0.dev20211020+cu111          pypi_0    pypi
[conda] torchdata                 0.4.0a0+e8b010b          pypi_0    pypi
[conda] torchvision               0.14.0a0+8cd742a           dev_0    <develop>
```"
Torch-TensorRT Integration,pytorch/vision,2022-06-23 16:24:29,5,,6200,1282659604,"### 🚀 The feature

[pytorch-tensorrt](https://github.com/pytorch/TensorRT) hit release 1.0 (is actually 1.1 right now), but most of the models available are not out of the box convertable to it.

### Motivation, pitch

Read feature description

### Alternatives

Alternatives would be to convert to onnx, and then convert to tensorrt, which is exactly what torch-trt tries to avoid; This would also require more work, because somebody would have to make sure that models are onnx compatible *and* tensort-trt compatible, with the latter being a torchscript-supporting runtime.

### Additional context

AFAIK, from some quick tests like:
```py
    module = fcos_resnet50_fpn()
    generalized_rcnn_transform_max_size = module.transform.max_size
    inputs = torch_trt.Input(
        min_shape=[1, 3, 224, 224],
        opt_shape=[1, 3, 1080, 1920],
        max_shape=[
            1,
            3,
            generalized_rcnn_transform_max_size,
            generalized_rcnn_transform_max_size,
        ],
    )

    precisions = {torch.half} # doesn't really matter

    trt_module = torch_trt.compile(
        module=module, inputs=[inputs], enabled_precisions=precisions
    )
```

And looking into some blocking issue from tensorrt:

- [[Feature] Adding support for collections as part of many inputs/outputs in TRTorch programs](https://github.com/pytorch/TensorRT/issues/428)

... the first thing to do would be to remove self/state (`self.a = b`) mutations, on the `forward` method, and... almost everything will work out of the box?

For reference, from a quick attempt to port some fcos model, I found 2 places where this happens. I am not sure if this is required.
- [While printing the warning statement](https://github.com/pytorch/vision/blob/0e688ce085392e748467d0858dd5d7af0f21ee9c/torchvision/models/detection/fcos.py#L645)
- And on [anchor utils here ](https://github.com/pytorch/vision/blob/61f82669acb71b45c55b83b025f54062f24445f6/torchvision/models/detection/anchor_utils.py#L126)
"
Scheduled workflow failed,pytorch/vision,2022-06-23 09:29:14,0,bug#module: datasets,6194,1282099226,"Oh no, something went wrong in the scheduled workflow tests/download. 
Please look into it:

https://github.com/pytorch/vision/actions/runs/2547990057

Feel free to close this if this was just a one-off error.


cc @pmeier @YosuaMichael"
Add Gaussian noise transformation,pytorch/vision,2022-06-22 21:27:14,8,module: transforms#new feature,6192,1280945658,"### 🚀 The feature

Add gaussian noise transformation in the functionalities of torchvision.transforms.

### Motivation, pitch

Using Normalizing Flows, is good to add some light noise in the inputs. Right now I am using albumentation for this but, would be great to use it in the torchvision library

### Alternatives

[Albumentation has a gaussian noise implementation](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.GaussianBlur)

### Additional context

_No response_

cc @vfdev-5 @datumbox"
Scheduled workflow failed,pytorch/vision,2022-06-22 09:28:51,1,bug#module: datasets,6186,1279848254,"Oh no, something went wrong in the scheduled workflow tests/download. 
Please look into it:

https://github.com/pytorch/vision/actions/runs/2541119862

Feel free to close this if this was just a one-off error.


cc @pmeier @YosuaMichael"
Replicate PyTorch transforms like transforms.ToTensor() and transforms.Normalize() in opencv c++,pytorch/vision,2022-06-20 13:16:11,1,,6183,1278467290,"## Issue description

In my development code, I have pytorch transforms on my test dataset (torchvision.transforms), I have exported the model into ONNX and I want to replicate these transforms with OpenCV c++. but, when i make predictions using pytorch (with the exported ONNX model), I get different result than when making predictions with OpenCV c++ 

## PyTorch transforms

``` 

test_transforms = transforms.Compose([
                           transforms.Resize(pretrained_size),
                           transforms.CenterCrop(pretrained_size),
                           transforms.ToTensor(),
                           transforms.Normalize(mean = pretrained_means, 
                                                std = pretrained_stds)
                       ])

```


- Output of Inference with PyTorch:
```
pytorch image type: torch.float32
detected class: cross-over
confidence: 0.8553193807601929
output probabilities of softmax layer: [[2.9558592e-05 8.5531938e-01 6.2924426e-04 1.1440608e-02 5.4786936e-04
  4.9833752e-02 2.7969838e-04 8.1919849e-02]]

```

## My Inference with OpenCV c++

```
    py::dict VClassdict;

    // Convert input Image from Python Code to Numpy Array
    Mat frame = nparray_to_mat(img);

    // Color Conversion to RGB
    Mat Img;
    //cvtColor(frame, Img, COLOR_BGR2RGB);

    // Resize Image to ResNet Input size
    cv::resize(frame, Img, Size(ClassinpWidth, ClassinpWidth));

    Mat normImg;
    Img.convertTo(normImg, CV_32FC3, 1.f / 255);
    Scalar mean;
    Scalar std;
    cv::meanStdDev(normImg, mean, std);

    mean[0] *= 255.0;
    mean[1] *= 255.0;
    mean[2] *= 255.0;

    double scale_factor = 0.003921569;  // equivalent to 1/255
   // Scalar mean = Scalar(117.8865, 128.52, 137.0115); // each channel mean is multiplied by 255.0
    //Scalar std = Scalar(0.2275, 0.2110, 0.2140);
    bool swapRB = true;
    bool crop = false;

    Mat blob;
    cv::dnn::blobFromImage(Img, blob, scale_factor, Size(ClassinpWidth, ClassinpWidth), mean, swapRB, crop);

    if (std.val[0] != 0.0 && std.val[1] != 0.0 && std.val[2] != 0.0)
    {
        // Divide blob by std.
        divide(blob, std, blob);
    }

    VClassResNet_Net.setInput(blob);

    // predict
    Mat prob = VClassResNet_Net.forward();

    cout << ""output probabilities of softmax layer: "" << prob;
    cout << endl;

    // extract prediction with highest confidence
    Point classIdPoint;
    double confidence;
    minMaxLoc(prob.reshape(1, 1), 0, &confidence, 0, &classIdPoint);
    int classId = classIdPoint.x;

    // Setup Dict returned to Python code for detections
    VClassdict[""ObjectClass""] = VClassResNet_classes[classId].c_str(); // Detected Class
    VClassdict[""Confidence""] = confidence; // Confedence Level
```
- Output of OpenCV c++ Inference:
```
detected class: cross-over
confidence: 0.9028045535087585
output probabilities of softmax layer: [2.5416075e-05, 0.90280455, 0.0031091773, 0.0042484328, 0.00012638989, 0.05069441, 9.7391217e-05, 0.038894258]

```

I've followed the OpenCV documentation : https://docs.opencv.org/4.x/dd/d55/pytorch_cls_c_tutorial_dnn_conversion.html

## System Info
- PyTorch version: 1.10.2+cu113
- OpenCV version: 4.5.1
- Python version: 3.9.7
- OS: Windows
"
Using any torchvision pretrained model as backbone for FasterRcnn,pytorch/vision,2022-06-15 14:53:13,5,module: models#topic: object detection,6172,1272358119,"### 🚀 The feature

Adding only the name of the backbone network, loads the pretrained model in the FasterRcnn model as backbone.

### Motivation, pitch

Its possible to use any backbone, but we have to specify many things like feature size, feature_name etc., wont be it easy for the user to just give the name of the backbone and we internally do everything.


### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox @YosuaMichael"
RuntimeError: Not compiled with GPU decoder support.,pytorch/vision,2022-06-14 03:47:53,2,module: video,6161,1270210065,"### 🐛 Describe the bug

When I use the following command, I get an error:
“”“
import torchvision
reader = torchvision.io.VideoReader('/home/ljf/vision/test4.mp4', device='cuda:0')
”“”
“”“
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ljf/vision/torchvision/io/video_reader.py"", line 97, in __init__
    raise RuntimeError(""Not compiled with GPU decoder support."")
RuntimeError: Not compiled with GPU decoder support.
”“”

I compile and install torchvision based on the following [steps:](https://github.com/pytorch/vision/tree/main/torchvision/csrc/io/decoder/gpu)
""""""
conda uninstall ffmpeg
pip uninstall torchvision

export TORCHVISION_INCLUDE=/home/ljf/Video_Codec_SDK_11.1.5/Interface/
export TORCHVISION_LIBRARY=/home/ljf/Video_Codec_SDK_11.1.5/Lib/linux/stubs/x86_64/

conda install -c conda-forge ffmpeg
export CUDA_HOME=/usr/local/cuda-11.3

python setup.py install
""""""
The entire process of compiling and installing did not report any errors.
I don't know why I can't read the video stream using cuda.

### Versions

root@b0b26070e5b8:/home/ljf/pytorch/torch/utils# python collect_env.py
Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.27

Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.11.0-49-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060
Nvidia driver version: 470.86
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.2
[pip3] torch==1.11.0
[pip3] torchelastic==0.2.2
[pip3] torchtext==0.12.0
[pip3] torchvision==0.14.0a0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               ha36c431_9    nvidia
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.21.2           py38h20f2e39_0  
[conda] numpy-base                1.21.2           py38h79a1101_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchelastic              0.2.2                    pypi_0    pypi
[conda] torchtext                 0.12.0                     py38    pytorch
[conda] torchvision               0.14.0a0                 pypi_0    pypi
"
build from source error,pytorch/vision,2022-06-13 04:05:13,5,,6157,1268869145,"### 🐛 Describe the bug

background:
I want to use the new function, which need to build torchvison from source.
""""""
import torchvision
reader = torchvision.io.VideoReader(file_name, device='cuda:0')
for frame in reader:
    print(frame)
""""""

information about my computer:
“”“
System: Ubuntu 18.04
Cuda: 10.2 (I also try 11.1)
GPU: RTX3090
”“”

when I run the operation：
python setup.py install

I meet a build problem:
""""""
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/cpp_extension.py"", line 1865, in _run_ninja_build
    subprocess.run(
  File ""/civi/anaconda3/envs/torch/lib/python3.9/subprocess.py"", line 528, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/civi/Torchvision_Decoding_Note/vision/setup.py"", line 514, in <module>
    setup(
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/__init__.py"", line 87, in setup
    return distutils.core.setup(**attrs)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/core.py"", line 148, in setup
    return run_commands(dist)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/core.py"", line 163, in run_commands
    dist.run_commands()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/dist.py"", line 967, in run_commands
    self.run_command(cmd)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/dist.py"", line 1214, in run_command
    super().run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/dist.py"", line 986, in run_command
    cmd_obj.run()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/install.py"", line 74, in run
    self.do_egg_install()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/install.py"", line 123, in do_egg_install
    self.run_command('bdist_egg')
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/dist.py"", line 1214, in run_command
    super().run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/dist.py"", line 986, in run_command
    cmd_obj.run()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/bdist_egg.py"", line 165, in run
    cmd = self.call_command('install_lib', warn_dir=0)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/bdist_egg.py"", line 151, in call_command
    self.run_command(cmdname)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/dist.py"", line 1214, in run_command
    super().run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/dist.py"", line 986, in run_command
    cmd_obj.run()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/install_lib.py"", line 11, in run
    self.build()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/command/install_lib.py"", line 107, in build
    self.run_command('build_ext')
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/dist.py"", line 1214, in run_command
    super().run_command(command)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/dist.py"", line 986, in run_command
    cmd_obj.run()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/build_ext.py"", line 79, in run
    _build_ext.run(self)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py"", line 339, in run
    self.build_extensions()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/cpp_extension.py"", line 778, in build_extensions
    build_ext.build_extensions(self)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py"", line 448, in build_extensions
    self._build_extensions_serial()
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py"", line 473, in _build_extensions_serial
    self.build_extension(ext)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/command/build_ext.py"", line 202, in build_extension
    _build_ext.build_extension(self, ext)
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/setuptools/_distutils/command/build_ext.py"", line 528, in build_extension
    objects = self.compiler.compile(sources,
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/cpp_extension.py"", line 593, in unix_wrap_ninja_compile
    _write_ninja_file_and_compile_objects(
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/cpp_extension.py"", line 1544, in _write_ninja_file_and_compile_objects
    _run_ninja_build(
  File ""/civi/anaconda3/envs/torch/lib/python3.9/site-packages/torch/utils/cpp_extension.py"", line 1881, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error compiling objects for extension
“”“”

My ninja version：1.11.0
My cuda version：10.2 （I also try 11.1, but failed too）

My installation process：
①conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-nightly
②conda install -c conda-forge ffmpeg
③git clone https://github.com/pytorch/vision.git
④cd vision
⑤python setup.py install
Then I met the above build problem
How can  I solve this problem?




### Versions

The latest ( until 2022 06 13)"
Adding dataset Tiny-Imagenet,pytorch/vision,2022-06-02 16:50:09,3,module: datasets,6127,1258446966,"### 🚀 The feature

Hello, 

I would like to contribute to torchvision by providing a implementation of Tiny-Imagenet dataset.

home : https://www.kaggle.com/c/tiny-imagenet
paper : http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf
zip : http://cs231n.stanford.edu/tiny-imagenet-200.zip

This challenge is part of Stanford Class CS 231N.
Label Classes and Bounding Boxes are provided

**details:**
classes : 200
image_size : 64x64x3
bbox : x0, y0, x1, y1 for each image
train split : 100 000  (500 per class) 
val split : 10 000 (50 per class)
test split : 10 000 (50 per class)



### Motivation, pitch


Note: the original test split doesn't have targets and bboxes.
Thus, in this implementation, I used the val split when passing `train=True`.

Features:
- fast loading by creating numpy files (npy/*.npy) from the raw folder/image datasets
- can leverage bbox

Structure:
```
root
├───tiny-imagenet-200.zip
├───tiny-imagenet-200
│   ├───npy <-- generated
│   │       ├───test_bboxes.npy
│   │       ├───test_data.npy
│   │       ├───test_targets.npy
│   │       ├───train_bboxes.npy
│   │       ├───train_data.npy
│   │       ├───train_targets.npy
│   ├───test
│   ├───train
│   ├───val
│   ├───words.txt
│   └───wnids.txt
```

Here the implementation: 
https://github.com/towzeur/vision/commit/a67feb569361f440fd48ed492183de8bd8f6b585


### Alternatives

_No response_

### Additional context

_No response_

cc @pmeier @YosuaMichael"
Add title to the tables in the `Models and Pretrained Weights` section of torchvision documentation.,pytorch/vision,2022-06-02 13:28:00,1,,6126,1258188134,"### 🚀 The feature

Currently, there is no title to the tables in the `Models and Pretrained Weights` section.  

[click here](https://pytorch.org/vision/0.13/generated/detection_table.html?) to open docs .

![image](https://user-images.githubusercontent.com/72816663/171637097-380e04fc-0845-498d-b039-31957b086b7c.png)


As you can see `Docs <no title>`, doesn't look nice. Also, adding the titles will also help users in searching the tables and other details effectively.

same goes for [classification models table](https://pytorch.org/vision/0.13/generated/classification_table.html), [segmentation models table](https://pytorch.org/vision/0.13/generated/segmentation_table.html), [quantized model table](https://pytorch.org/vision/0.13/generated/classification_quant_table.html?) 

### Motivation, pitch

Enhance the searching experience for the users? 

### Alternatives

_No response_

### Additional context

_No response_"
draw_segmentation_instances,pytorch/vision,2022-06-02 12:36:23,7,,6125,1258124439,"### 🚀 The feature

Development a function torchvision.utils.draw_segmentation_instances to draw just the external contour of a mask.

### Motivation, pitch

For torchvision.utils we have draw_bounding_boxes, draw, draw_segmentation_masks, draw_keypoints but still need to do some extra preprocess for getting the instance segmentation. Would be great to have a function draw_segmentation_instances to get just the external area of the ROI.

### Alternatives

Similar to draw_bounding boxes, a optional argument fill could be added into draw_segmentation_instances to achieve this behavior.

### Additional context

As far as I know. We need to use other libraries to achieve this."
Scripted inception_v3 has wrong output format,pytorch/vision,2022-06-02 01:40:22,1,,6121,1257478787,"### 🐛 Describe the bug

Scripted `inception_v3` model has wrong output format - Instead of `Tensor` it returns `InceptionOutputs`.
I checked other models - resnet50, mobilenet_v2 - After scripting they return `tensor`.
Inference tools such as torch_tensorrt expects models to return `Tensor` or `[Tensor]`.

To reproduce the issue with inception_v3:
```
import torch
import torchvision.models as models

model = models.inception_v3(pretrained=True).eval()
x = torch.rand(1, 3, 299, 299)
y=model(x)
smodel = torch.jit.script(model, x).eval()
y=smodel(x)

print(type(y))

```
It prints `<class 'torch._jit_internal.InceptionOutputs'>`.

Expected output: `<class 'torch.Tensor'>` 

What should be fixed in [inception.py](https://github.com/pytorch/vision/blob/main/torchvision/models/inception.py) code to make it output Tensor after applying `torch.jit.script`?

### Versions

```
PyTorch version: 1.12.0a0+bd13bc6
Is debug build: False
CUDA used to build PyTorch: 11.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.3
Libc version: glibc-2.31

Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-1075-aws-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.6.124
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 515.43.04
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.4.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] pytorch-quantization==2.1.2
[pip3] torch==1.12.0a0+bd13bc6
[pip3] torch-tensorrt==1.1.0a0
[pip3] torchtext==0.13.0a0
[pip3] torchvision==0.13.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.22.3           py38h1d589f8_2    conda-forge
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] torch                     1.12.0a0+bd13bc6          pypi_0    pypi
[conda] torch-tensorrt            1.1.0a0                  pypi_0    pypi
[conda] torchtext                 0.13.0a0                 pypi_0    pypi
[conda] torchvision               0.13.0a0                 pypi_0    pypi
```"
Scheduled workflow failed,pytorch/vision,2022-05-27 09:10:55,0,bug#module: datasets,6102,1250530467,"Oh no, something went wrong in the scheduled workflow tests/download. 
Please look into it:

https://github.com/pytorch/vision/actions/runs/2395583002

Feel free to close this if this was just a one-off error.


cc @pmeier @YosuaMichael"
The Release 0.13 is not using the correct torch core version,pytorch/vision,2022-05-26 12:33:09,9,bug#module: ci,6099,1249513332,"### 🐛 Describe the bug

Currently the CI on branch `release/0.13` is failing (I create dummy PR https://github.com/pytorch/vision/pull/6092 to track this). 
We fail on type_check  and this does not happen on main branch.
After looking at the error: https://app.circleci.com/pipelines/github/pytorch/vision/17906/workflows/8f008f5a-13b2-4207-a378-51ae705b99b7/jobs/1450718
especially on the installation of pytorch, we get:
```
#!/bin/bash -eo pipefail
pip install  --user  --progress-bar=off --pre torch --extra-index-url https://download.pytorch.org/whl/test/cpu/torch_test.html

Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/test/cpu/torch_test.html
Collecting torch
  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)
```

We should use the RC branch 1.12 from pytorch core but instead we get version 1.11.

~~After digging a bit deeper, if we search ""torch-"" on https://download.pytorch.org/whl/test/cpu/torch_test.html currently I can only see 4 versions and the latest one is indeed 1.11. I think there is a problem here.~~

cc @malfet @seemethere @datumbox @atalman 
"
Scheduled workflow failed,pytorch/vision,2022-05-26 09:11:12,0,bug#module: datasets,6093,1249307833,"Oh no, something went wrong in the scheduled workflow tests/download. 
Please look into it:

https://github.com/pytorch/vision/actions/runs/2389724333

Feel free to close this if this was just a one-off error.


cc @pmeier @YosuaMichael"
Keeping track of PRs and issues for upcoming 0.13 release,pytorch/vision,2022-05-23 10:07:08,0,,6071,1244917222,"Just opening this to keep track of issues and PRs that need to be in / fixed before the release (and hopefully before our cut)


Current CI issues

- [x] https://github.com/pytorch/vision/issues/6070
- [x] https://github.com/pytorch/vision/issues/6069


From us

- [ ] https://github.com/pytorch/vision/pull/5993 - nice to have, not mandatory
- [x] https://github.com/pytorch/vision/pull/5857 - no brainer
- [x] https://github.com/pytorch/vision/pull/6061
- [x] https://github.com/pytorch/vision/pull/6059

From RelEng

- [ ] https://github.com/pytorch/vision/pull/5996 - Apparently need to wait for domains cut https://github.com/pytorch/vision/pull/5996#issuecomment-1133228149
- [x] https://github.com/pytorch/vision/pull/5948 and https://github.com/pytorch/vision/pull/6051 - waiting for clarifications  https://github.com/pytorch/vision/pull/5948#issuecomment-1134468617
- [ ] https://github.com/pytorch/vision/pull/5698 - waiting for clarifications https://github.com/pytorch/vision/pull/5698#issuecomment-1132694446"
`read_image` does not work on Apple M1 (from torchvision.io),pytorch/vision,2022-05-23 09:27:28,0,,6067,1244864459,"### 🐛 Describe the bug

It fails to load the image.
<img width=""1088"" alt=""image"" src=""https://user-images.githubusercontent.com/18441985/169788643-8a8c4fb3-e4d9-447d-ba33-b798863a6d5f.png"">

The input is a string.

### Versions

Collecting environment information...
PyTorch version: 1.13.0.dev20220522
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 12.4 (arm64)
GCC version: Could not collect
Clang version: 13.1.6 (clang-1316.0.21.2.5)
CMake version: version 3.23.1
Libc version: N/A

Python version: 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:42:03) [Clang 12.0.1 ] (64-bit runtime)
Python platform: macOS-12.4-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] torch==1.13.0.dev20220522
[pip3] torchaudio==0.11.0
[pip3] torchvision==0.12.0
[conda] numpy                     1.22.3                   pypi_0    pypi
[conda] torch                     1.13.0.dev20220522          pypi_0    pypi
[conda] torchaudio                0.11.0                   pypi_0    pypi
[conda] torchvision               0.12.0                   pypi_0    pypi"
Inversion of prototype transforms,pytorch/vision,2022-05-22 12:32:05,3,module: transforms#new feature#prototype,6062,1244240880,"When debugging vision models, it is often useful to be able to map predicted bounding boxes, segmentation masks, or keypoints back onto the original image. To do this conveniently, each transformation should know how to invert itself. A discussion about this can be found in [this thread](https://github.com/pmeier/torchvision-datasets-rework/pull/1#issuecomment-911210196). While useful, it was deemed a lower priority than adding general support for non-image input types in the prototype transforms. However, from the preliminary discussions, inverting transformations seems not to conflict with the proposal and thus can be added later.

Apart from the thread linked above there were some discussions without written notes. They are listed here so they don’t get lost:
- While some transformations can be statically inverted, transformations with random elements can only be inverted for a specific sampled parameter set. In the thread linked above, this parameter set would need to be returned by the forward transformation and used for the inverse. Instead of passing the parameter set around, the transformation could also save it from the last call and use that for inversion.
- Some transformations are only pseudo-invertible. For example, while cropping is the true inverse of padding, the same is not true the other way around. By cropping first, information is eliminated that cannot be revived by padding. Thus, padding is just the pseudo-inverse of cropping. The inversion functionality should have a strict flag that, if set, disallows pseudo-inverses.

cc @vfdev-5 @datumbox @bjuncek @pmeier"
Cannot obtain the accuracy stated in the doc for inception_v3 pretrained on Imagenet,pytorch/vision,2022-05-22 11:52:58,6,,6066,1244818952,"Hi.
I'm trying to evaluate `inception_v3` pretrained model from the hub on Imagenet (ILSVRC 2012) test set. I use the following evaluation code:
```Python
def compute_accuracy(self):
        num_correct = 0
        num_images = 0

        _IMAGE_MEAN_VALUE = [0.485, 0.456, 0.406]
        _IMAGE_STD_VALUE = [0.229, 0.224, 0.225]
        imgnet_loader = torch.utils.data.DataLoader(
            ImageFolder('/home/amin/dataset/ILSVRC/val', 
            transforms.Compose([
                transforms.Resize(299),
                transforms.CenterCrop(299),
                transforms.ToTensor(),
                transforms.Normalize(mean=_IMAGE_MEAN_VALUE, std=_IMAGE_STD_VALUE),
            ])),
            batch_size=16, shuffle=True,
            num_workers=8, pin_memory=True)
        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True).cuda()
        self.model.eval()

        for i, (images, targets) in \
            enumerate(tqdm(imgnet_loader,  desc=""Compute Accuracy"", total=len(imgnet_loader))):
            images = images.cuda() if torch.cuda.is_available() else images.cpu()
            targets = targets.cuda() if torch.cuda.is_available() else targets.cpu()
            output_dict = self.model(images)
            pred=output_dict.argmax(dim=1)
            
            num_correct += (pred == targets).sum().item()
            num_images += images.size(0)

        classification_acc = num_correct / float(num_images) * 100
        return classification_acc
```

However, the accuracy I get is `77.216` while it should be `77.45` according to [this page](https://pytorch.org/hub/pytorch_vision_inception_v3/). I figured that the model has a transform_input as preprocess in itself. So if we are doing normalization beforehand (as suggested in the [example code](https://pytorch.org/hub/pytorch_vision_inception_v3/)), we should set `transform_input` to false. So if I add `self.model.transform_input = False`, I get `77.472`, Which is closer to the expected value but not exactly the same.


Assuming that the issue isn't from my code, I also found the thread on `std`, `mean` values (#1439). So I tested some of the values suggested there as well, and got these results:
| mean | std | model's transform_input | Accuracy |
| ----- | ----- | ----- |  ----- |   
|[0.485, 0.456, 0.406]|[0.229, 0.224, 0.225]|disabled|77.472|
|[0.485, 0.456, 0.406]|[0.229, 0.224, 0.225]|enabled|77.216|
| [0.4803, 0.4569, 0.4083] | [0.2806, 0.2736, 0.2877] | disabled | 77.448|
| [0.4803, 0.4569, 0.4083] | [0.2806, 0.2736, 0.2877] | enabled | 76.986|
|[0.4845, 0.4541, 0.4025]| [0.2724, 0.2637, 0.2761]| disabled | 77.456|
|[0.4845, 0.4541, 0.4025]| [0.2724, 0.2637, 0.2761]| enabled | 77.03|
[0.4701, 0.4340, 0.3832]|[0.2845, 0.2733, 0.2805]|disabled|77.44|
[0.4701, 0.4340, 0.3832]|[0.2845, 0.2733, 0.2805]|enabled|77.01|


I appreciate any input on this.
Thanks."
Test failures on A100 / tf32,pytorch/vision,2022-05-18 00:45:37,0,dependency issue#core issue,6040,1239298750,"### 🐛 Describe the bug

The following tests are failing on A100 with numerical differences, full logs at https://gist.githubusercontent.com/davidberard98/74009082166d2ac37d65c481b300376f/raw/b5160f8abfc14490f985d19cfb95696a10251e99/torchvision_0513.txt.
```
FAILED test/test_models.py::test_classification_model[cuda-resnet101] - Asser...
FAILED test/test_models.py::test_segmentation_model[cuda-fcn_resnet101] - Ass...
FAILED test/test_models.py::test_detection_model[cuda-fasterrcnn_resnet50_fpn]
FAILED test/test_models.py::test_detection_model[cuda-maskrcnn_resnet50_fpn]
FAILED test/test_models.py::test_detection_model[cuda-maskrcnn_resnet50_fpn_v2]
```

By disabling tf32 the failures go away for all except `cuda-fasterrcnn_resnet50_fpn`
```patch
diff --git a/test/test_models.py b/test/test_models.py
index c0afe9f10..8f6dda357 100644
--- a/test/test_models.py
+++ b/test/test_models.py
@@ -20,6 +20,12 @@ from torchvision import models
 ACCEPT = os.getenv(""EXPECTTEST_ACCEPT"", ""0"") == ""1""
 SKIP_BIG_MODEL = os.getenv(""SKIP_BIG_MODEL"", ""1"") == ""1""

+# The flag below controls whether to allow TF32 on matmul. This flag defaults to True.
+torch.backends.cuda.matmul.allow_tf32 = False
+
+# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
+torch.backends.cudnn.allow_tf32 = False
+

 def get_models_from_module(module):
     # TODO add a registration mechanism to torchvision.models
```

`cuda-fasterrcnn_resnet50_fpn` failures remain even after disabling tf32.

### Versions

```
Collecting environment information...
PyTorch version: 1.12.0a0+git98a20eb
Is debug build: True
CUDA used to build PyTorch: 11.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.22.3
Libc version: glibc-2.27

Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-1069-aws-x86_64-with-glibc2.27
Is CUDA available: True
CUDA runtime version: 11.6.112
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA A100-SXM4-40GB

Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.1
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] torch==1.12.0a0+git98a20eb
[pip3] torchvision==0.13.0a0+edb7bbb
[conda] numpy                     1.22.3                   pypi_0    pypi
[conda] torch                     1.12.0a0+git98a20eb           dev_0    <develop>
[conda] torchvision               0.13.0a0+edb7bbb           dev_0    <develop>
```
cc @jjsjann123"
Add support for VOC on detection references,pytorch/vision,2022-05-17 11:32:49,3,enhancement#module: reference scripts#topic: object detection,6037,1238502031,"The segmentation references already support VOC and COCO

https://github.com/pytorch/vision/blob/08c8f0e0b68195a4a5a21cdd3f86106f59c2e854/references/segmentation/train.py#L20-L24

Unfortunately, the detection references only support COCO

https://github.com/pytorch/vision/blob/a1232c212d7cf84806189910ba83bc36bcea916c/references/detection/train.py#L37

There was a first push for this in #1216 by @lpuglia. We can probably salvage some parts from the PR.

We are currently reworking the datasets and transforms, and in turn the reference scripts will change quite a bit. Thus, we should only go for this after they are stable-ish again.

cc @datumbox @YosuaMichael"
Restructuring torchvision tests,pytorch/vision,2022-05-16 10:16:48,7,,6020,1236931909,"## Motivation

Our tests are getting a bit lengthy and considering we are adding new features, we are actually placing multiple tests in single file.
This leads to problems as the tests become  harder to refactor and harder to read, manage. Standalone files for tests or grouped according to purpose are easier to anaylse and debug as well.

Our longest test files


Copy paste-able command
```
find test -name ""*.py"" | xargs -I {} wc -l {} | sort -nr | head -n 10
```

```
oke@psycon:~/Aditya/PyTorch/vision$ find test -name ""*.py"" | xargs -I {} wc -l {} | sort -nr | head -n 10
2675 test/test_datasets.py
2242 test/test_transforms.py
1535 test/builtin_dataset_mocks.py
1528 test/test_ops.py
1343 test/test_functional_tensor.py
1339 test/test_prototype_transforms_functional.py
1254 test/test_video_reader.py
982 test/test_transforms_tensor.py
954 test/datasets_utils.py
942 test/test_models.py
```

Many splits are possible that can make the tests smaller and easier to manage

```
E.g. 

test_models.py
-----  test_classification_models.py
----- test_detection_models.py
----- test_video_models.py

test_ops.py
------ test_post_processors.py
------ test_losses.py
------ test_layers.py

```

Possibilities are many, we can have multiple files with better grouping.

## Pitch

We should think of grouping tests in logically similar way together. or consider creating test folder a package. See 
https://github.com/pytorch/vision/pull/4436 . Earlier we have faced problems doing the same.

From discussion with Phillip offline
Considering we don't Pin pytest. We can make use of  https://github.com/pytest-dev/pytest/pull/9134

With the python path param, we can probably make use of `common_utils.py` file as well. So that we can test files without hurting other scenarios which were mentioned in the comments. 

## Alternatives

This is quite tricky but very helpful as we don't want tests to hurt us later. 

## Reference
https://github.com/pytorch/vision/pull/4436#issuecomment-923131237

https://github.com/pytorch/vision/pull/4436#issuecomment-924005657

https://github.com/pytorch/vision/pull/4436#issuecomment-924250093


cc @NicolasHug @datumbox @pmeier "
Port `RASampler` functionality to iterable datasets,pytorch/vision,2022-05-16 08:11:19,7,module: datasets#module: reference scripts#topic: classification#prototype,6018,1236785891,"Our current classification references use the ""Repeated Augment Sampler"" (`RASampler`) from #5051:

https://github.com/pytorch/vision/blob/d585f86d94f07a3bc083e48c6534d93a409cbcb2/references/classification/sampler.py#L7

Since after the revamp we will have iterable- rather than map-style datasets, samplers are no longer supported. 

Given that the `RASampler` increases accuracy, we need to support the same functionality going forward. It can probably be achieved by appending a custom `RepeatedAugmentIterDataPipe` to the dataset graph, but we need to make sure it works correctly with the shuffling and sharding.

cc @pmeier @YosuaMichael @datumbox @vfdev-5 @bjuncek"
Imagenet Version not documented?,pytorch/vision,2022-05-13 11:24:32,3,question,6011,1235115857,"### 📚 The doc issue

Hello torchvision team,

First, thanks for the epic work you are all putting into this tool! I would like to know the exact version of imagenet used at pertaining different models in torchvision, for research purposes regarding model inversion. All of them use the 2012 Imagenet Dataset version or maybe some newer version?

Thank you,
Tudor

### Suggest a potential alternative/fix

_No response_"
"""arbitrary number of leading dimensions"" only supports up to 4 dimensions",pytorch/vision,2022-05-12 19:47:01,9,module: transforms,6008,1234413114,"### 📚 The doc issue

I was attempting do an affine transform of medical imaging data organized using two channel dimensions (SLICE and SEQUENCE) for a total of five: BATCH x SLICE x SEQUENCE x HEIGHT x WIDTH, but `torchvision.transforms.functional.affine` only supports up to four dimensions.

```
import torchvision
x = torch.empty((1, 320, 320))
x_t = torchvision.transforms.functional.affine(x, angle=0, translate=(0,0), scale=1, shear=0)
print(x.shape, x_t.shape)

x = torch.empty((1, 1, 320, 320))
x_t = torchvision.transforms.functional.affine(x, angle=0, translate=(0,0), scale=1, shear=0)
print(x.shape, x_t.shape)

x = torch.empty((1, 1, 1, 320, 320))
x_t = torchvision.transforms.functional.affine(x, angle=0, translate=(0,0), scale=1, shear=0)
print(x.shape, x_t.shape)

x = torch.empty((1, 1, 1, 1, 320, 320))
x_t = torchvision.transforms.functional.affine(x, angle=0, translate=(0,0), scale=1, shear=0)
print(x.shape, x_t.shape)
```

Resulting output and error message:

```
torch.Size([1, 320, 320]) torch.Size([1, 320, 320])
torch.Size([1, 1, 320, 320]) torch.Size([1, 1, 320, 320])
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-211-2ee8b4ff09ce> in <module>
      9 
     10 x = torch.empty((1, 1, 1, 320, 320))
---> 11 x_t = torchvision.transforms.functional.affine(x, angle=0, translate=(0,0), scale=1, shear=0)
     12 print(x.shape, x_t.shape)
     13 

~/bmelseg/venv/lib/python3.6/site-packages/torchvision/transforms/functional.py in affine(img, angle, translate, scale, shear, interpolation, fill, resample, fillcolor)
   1123     translate_f = [1.0 * t for t in translate]
   1124     matrix = _get_inverse_affine_matrix([0.0, 0.0], angle, translate_f, scale, shear)
-> 1125     return F_t.affine(img, matrix=matrix, interpolation=interpolation.value, fill=fill)
   1126 
   1127 

~/bmelseg/venv/lib/python3.6/site-packages/torchvision/transforms/functional_tensor.py in affine(img, matrix, interpolation, fill)
    696     # grid will be generated on the same device as theta and img
    697     grid = _gen_affine_grid(theta, w=shape[-1], h=shape[-2], ow=shape[-1], oh=shape[-2])
--> 698     return _apply_grid_transform(img, grid, interpolation, fill=fill)
    699 
    700 

~/bmelseg/venv/lib/python3.6/site-packages/torchvision/transforms/functional_tensor.py in _apply_grid_transform(img, grid, mode, fill)
    645         img = torch.cat((img, dummy), dim=1)
    646 
--> 647     img = grid_sample(img, grid, mode=mode, padding_mode=""zeros"", align_corners=False)
    648 
    649     # Fill with required color

~/bmelseg/venv/lib/python3.6/site-packages/torch/nn/functional.py in grid_sample(input, grid, mode, padding_mode, align_corners)
   4009         align_corners = False
   4010 
-> 4011     return torch.grid_sampler(input, grid, mode_enum, padding_mode_enum, align_corners)
   4012 
   4013 

RuntimeError: grid_sampler(): expected 4D or 5D input and grid with same number of dimensions, but got input with sizes [1, 1, 1, 320, 320] and grid with sizes [1, 320, 320, 2]
```



cc @vfdev-5 @datumbox"
Change number of coco classes in detection recipe,pytorch/vision,2022-05-12 09:09:01,3,,5999,1233690055,"### 🚀 The feature

Infer the number of classes from the dataset without hard-coding it, for example with
```
def get_dataset(name, image_set, transform, data_path):
    paths = {""coco"": (data_path, get_coco), ""coco_kp"": (data_path, get_coco_kp)}
    p, ds_fn = paths[name]

    ds = ds_fn(p, image_set=image_set, transforms=transform)
    num_classes = len(ds.coco.cats)
    return ds, num_classes
```
This means that we need to remap the classes with continuous indexes, for example with
```
class CocoDetection(torchvision.datasets.CocoDetection):
    def __init__(self, img_folder, ann_file, transforms):
        super().__init__(img_folder, ann_file)
        self._transforms = transforms
        self.mapping = self.map_coco()

    def map_coco(self,):
        mapping_coco = {}
        for idx, c in enumerate(self.coco.cats):
            mapping_coco[c] = idx+1
        return mapping_coco


    def __getitem__(self, idx):
        img, target = super().__getitem__(idx)
        target = copy.deepcopy(target)
        for t in target:
            t['category_id'] = self.mapping[t['category_id']]
        image_id = self.ids[idx]
        target = dict(image_id=image_id, annotations=target)
        if self._transforms is not None:
            img, target = self._transforms(img, target)
        return img, target
```

Lastly, the number of classes for the model could be `num_classes+1`
```
model = torchvision.models.detection.__dict__[args.model](
     weights=args.weights, weights_backbone=args.weights_backbone, num_classes=num_classes+1, **kwargs
)
```


### Motivation, pitch

The number of classes to train `coco` is hard coded and is 90+1. 
https://github.com/pytorch/vision/blob/18b39e362e7ae4b58f9638f90246f7651fe3de98/references/detection/train.py#L37
This value does not reflect the number of classes that `coco` has (80) but the highest index since some indexes are not used.

Since the number of classes defines the head of detection architectures
https://github.com/pytorch/vision/blob/18b39e362e7ae4b58f9638f90246f7651fe3de98/torchvision/models/detection/faster_rcnn.py#L354-L357
it results in a small increase of memory usage (and may result in strange behaviors).

### Alternatives

_No response_

### Additional context

_No response_"
DeformConv2d offset and mask backprop,pytorch/vision,2022-05-11 12:20:39,4,enhancement#module: ops,5989,1232548852,"### 🚀 The feature

I notice that the current vision of DeformConv2d module doesn’t treat offset and mask as parameters, so it seems that the standard backward routine cannot update the offset.  Is there any possibility that you could add this feature in the future?

### Motivation, pitch

I’m trying to build DNNs with above module, so it is not that convenient if I need to rewrite the backward process by myself

### Alternatives

_No response_

### Additional context

_No response_"
Add option to choose the loss function in `compute_loss` function. ,pytorch/vision,2022-05-10 14:12:43,1,module: models#topic: object detection,5983,1231232827,"### 🚀 The feature

In the [compute loss](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/fcos.py#L54) function,  `giou_loss`  is used to calculate the loss by default [see here](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/fcos.py#L96), but now as `CIOU` and `DIOU` loss functions are added to torchvision, maybe we could let the user choose in between the loss functions

cc @datumbox @YosuaMichael, @NicolasHug,

### Motivation, pitch

This gives little more freedom to the user to choose between the loss functions.

### Alternatives

_No response_

### Additional context

_No response_"
Document  other weight meta-data fields,pytorch/vision,2022-05-10 10:37:37,0,module: documentation,5982,1230956008,"We currently document the number of parameters in the `Weight`s meta-data and we display these on our [summary tables](https://pytorch.org/vision/main/models_new.html#table-of-all-available-classification-weights). Other values might be of interest for users:

- [ ] FLOPs. @Chillee 's [FLOP counter](https://dev-discuss.pytorch.org/t/the-ideal-pytorch-flop-counter-with-torch-dispatch/505) might be useful for this
- [ ] Model size in MB. This will usually be `~= 4 * num_params`, except for quantized models. Either-way, it's worth computing and documenting directly
-  [ ] Memory requirements for 1 image (forward and backward pass). This can be [estimated manually](https://youtu.be/DAOcjicFr1Y?t=1202) but can quickly become untractable, so it's best to do that automatically with a script. We should be able to use  [`memory_stats`](https://pytorch.org/docs/stable/generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats) module for this (accounting for the caching allocator, etc). I'll need to think about this more to make sure what we report is meaningful.


CC @datumbox "
install torchvision on jetson nano,pytorch/vision,2022-05-10 02:38:02,1,,5980,1230514895,"When I want to install torchvision on jetson nano, it keeps stuck at writing top-level names to torchvision.egg-info/top_level.txt, nothing else changes, what can I do to make it install faster?
![image](https://user-images.githubusercontent.com/44689801/167531629-bbcd3053-1603-4d35-8276-f288de309688.png)
"
Use OpInfos for operator tests,pytorch/vision,2022-05-08 18:54:02,1,module: tests#triaged#feature,5969,1228960912,"### 🐛 Describe the bug

Defining OpInfos for torchvision's operators would make it possible to run stock PyTorch correctness tests (that are generic over operators) in torchvision. For example, I want to add meta function support to torchvision operators but my preference is to reuse the testing infrastructure in PyTorch.

cc @pmeier @mruberry I don't know if it is actually supported to run tests in PyTorch with extra opinfos, doesn't feel like it.

### Versions

master"
Allow weight tables to be sorted,pytorch/vision,2022-05-08 14:20:46,1,,5965,1228900633,"It'd be nice if [weight tables](https://pytorch.org/vision/main/models_new.html#table-of-all-available-classification-weights) could be sorted by number of params or accuracies.

This post might be a good start: https://stackoverflow.com/questions/64443832/sorting-table-with-rst-and-python-sphinx-in-html-output"
Single channel support for optical flow,pytorch/vision,2022-05-07 02:54:20,4,,5960,1228506101,"### 🚀 The feature

Would love to be able to run black and white images through the new RAFT optical flow model! (Same goes for ResNet)

### Motivation, pitch

I have a B+W dataset. Duplicating channels feels wasteful. 

### Alternatives

_No response_

### Additional context

_No response_"
ffcv integration,pytorch/vision,2022-05-05 16:43:24,3,feature,5954,1226933248,"### 🚀 The feature

Integrate https://github.com/libffcv/ffcv for accelerated image decoding, preprocessing and loading

### Motivation, pitch

I maintain [torchserve](https://github.com/pytorch/serve), we've recently had customers complain about slow image preprocessing decoding https://github.com/pytorch/serve/issues/1546 - the performance implications are large. It's possible for me to solve them locally in torchserve but solving them a level higher in torchvision means anyone can benefit from the improvements

Summarizing discussion with @NicolasHug  

### Alternatives

Some alternatives exist like DALI
There's also the do nothing alternative where we just provide a tutorial in ffcv instead of having a tight integration

### Additional context

If this is a reasonable first issue to torch/vision I can pick this up"
typing_extensions is only needed for python < 3.8,pytorch/vision,2022-05-04 13:56:23,9,help wanted#dependency issue#good first issue,5942,1225409877,"### 🚀 The feature

specify the dep typing_extensions is only needed for python < 3.8

### Motivation, pitch

specify the dep typing_extensions is only needed for python < 3.8

### Alternatives

_No response_

### Additional context

According to https://github.com/pytorch/vision/search?q=typing_extensions, torchvision only uses `typing_extensions.Literal`, which is available since python 3.8, see also [this](https://docs.python.org/3/library/typing.html).
we only need to change [setup.py](https://github.com/pytorch/vision/blob/22400011d6a498ecf77797a56dfe13bc94c426ca/setup.py) and replace `typing_extensions` with something like `typing_extensions; python_version<""3.8""`.

ref issue https://github.com/archlinuxcn/repo/issues/2762"
Failed to load image python extension/Segmentation fault in Raspberry pi 4B with python 3.9/10,pytorch/vision,2022-04-28 19:25:31,4,topic: binaries,5919,1219174912,"On installing the raspberry pi (aarch64) hosted wheels on both the raspbian os(bullseye) with python 3.9, and ubuntu 22.04 with python 3.10. on loading torch vision I get a user warning :

python3.10/site-packages/torchvision/io/image.py:11 UserWarning: Failed to load Image Python extension:
 warn(""Failed to load image python extension: {e}"")

On running the yolov5 detection, after loading the model, the program exits with segmentation fault in ubuntu.
On the raspbian os, it runs for sometime but on pressing anything on the keyboard the program ends with segmentation fault (core dumped) 

This is weird because my yolov5 detection applications were running  on raspbian os older version(buster os) which by default has python 3.7 with even the pytorch 1.11.0 and vision 0.12.0 wheels installed directly from hosted using `pip install torch torchvision`.
Since both ubuntu and new raspbian os have 3.10 and 3.9 by default, i installed all dependencies. Since the only difference in both y test scenarios are the python versions I believe this could be a hosted wheel issue or a deeper python version issue I am unaware of.

Thanks for any help in advance!"
Mac M1 pro segmentation fault on import,pytorch/vision,2022-04-27 09:50:11,5,,5893,1217102096,"### 🐛 Describe the bug

Hello! Can not import torchvision on my mac. Get an error:
`libc++abi: terminating with uncaught exception of type std::length_error: vector`
as well as segmentation fault.

code:
```
import torch
from torch import nn
from torch.nn import functional as F
from torch.distributions.normal import Normal
from torch.distributions.kl import kl_divergence
from torchvision.utils import save_image
```

it also fails just with `from torchvision.utils import save_image`

Similar issue: https://github.com/pyg-team/pytorch_geometric/issues/4419
But not a single proposed solution works...


### Versions

PyTorch version: 1.10.2
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 12.1 (arm64)
GCC version: Could not collect
Clang version: 13.1.6 (clang-1316.0.21.2.3)
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:25:34)  [Clang 11.1.0 ] (64-bit runtime)
Python platform: macOS-12.1-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.10.2
[pip3] torchaudio==0.11.0
[pip3] torchvision==0.12.0
[conda] numpy                     1.21.5           py39h47fb9ce_0    conda-forge
[conda] torch                     1.10.2                   pypi_0    pypi
[conda] torchaudio                0.11.0                   pypi_0    pypi
[conda] torchvision               0.12.0                   pypi_0    pypi"
Imagenet 10 or Imagenette,pytorch/vision,2022-04-27 09:42:45,8,module: datasets#new feature,5892,1217093360,"### 🚀 The feature

I think it would be cool to have a readily-downloading dataset of 10 categories and imagenet shape (i.e. to be fed into 224x224). Currently the closes is likely STL10, but I think one of Imagenet-10 ( https://paperswithcode.com/sota/image-clustering-on-imagenet-10 ) or Imagenette ( https://github.com/fastai/imagenette ) might be worthwhile to add.

### Motivation, pitch

With vision transformers, it seems even more interesting to get a reasonably-sized dataset with images of similar type (size) of ImageNet.

### Alternatives

Manually downloading (torch.hub.download?), extracting and using ImageFolder.

### Additional context

_No response_

cc @pmeier @YosuaMichael"
MobileNetV3 quantization bug,pytorch/vision,2022-04-27 08:23:25,0,bug#module: models.quantization,5890,1216986427,"### 🐛 Describe the bug

This issue documents an bug discovered on February 2021 while quantizing the MobileNetV3. We worked around the issue but it probably remains unresolved on the eager quantization API. Here I'm gathering all the information exchanged with the quantization team so that we can investigate on the future.

-----------

TLDR: During the QAT training of MobileNetV3 the accuracies are at the expected levels. After serializing and loading the weights of the quantized weights, the accuracies appear similar to a randomly initialized model. The workaround is to instead load the QAT fine-tuned weights and convert the model.

Originally to quantize the MobileNetV3 Large we took the standard approach (see https://github.com/pytorch/vision/pull/3323). Our initial setup was extremely similar to the one of MobileNetV2. We trained a model with the reference script and we got good accuracies. Unfortunately when we tried to deploy the weights of the quantized model we found out that loading them and doing predictions leads to extremely low accuracy (less than 1%).

After trial and error, we found implemented the following workaround that allows to load the weights, quantize the model and get good accuracies: https://github.com/pytorch/vision/pull/3323/commits/274c6a1393384054876d701ffa1b54eb6750f1d8. Effectively we create a QAT model, load the fine-tuned weights and then quantize it.

The above workaround should be unnecessary. Instead MobileNetV3 should follow the same approach as with every other QAT model (such as in the case of MobileNetV2)."
Deeplabv3 model builders accept unused **kwargs parameter.,pytorch/vision,2022-04-26 13:51:27,4,,5883,1215994174,"Not sure if this is by design or a minor bug, but the deeplabV3 model builders accept a `**kwargs`  parameter and do not use it.

https://github.com/pytorch/vision/blob/d425f007782051ab08e9c0fbfe06b072313f4649/torchvision/models/segmentation/deeplabv3.py#L222:L222

@datumbox should we remove these?"
Keypoint RCNN visibility flag for keypoints,pytorch/vision,2022-04-24 21:44:35,2,question#topic: object detection,5872,1213777506,"### 🚀 The feature

Hello All,

This is only my first day posting a request here so I apologize for any errors on my part. Also, sorry for the long post below.

The purpose of this post is to request an improvement/correction for the visibility flag behavior of Keypoint RCNN. Based on my results and those of other users I have encountered on different forums and sites, Keypoint RCNN always predicts a flag value of v=1 for all keypoints, no matter the training flag value for v>0 (even v=0), and predicts coordinates for them as well. In other words, the model does not appear to actually learn the flag value. My understanding is that the flag should be learned and is supposed to follow the COCO convention (v=0 ‘not in image’; v=1 ‘occluded’; v=2 ‘visible’) but does not do so.






### Motivation, pitch

Given the usefulness of the visibility flags, being able to accurately predict them and use the information during inference to mark occluded vs. visible keypoints would be an important addition to the model capability. My understanding is that this is already supposed to be the case, but for some reason the documentation as well as the model behavior on this are lacking. I have found the performance of Keypoint RCNN overall to be very good and I have successfully fine-tuned it on my custom (multiclass) dataset with very good success in predicting the class, bbox, and keypoints. It would be very helpful to be able to distinguish between keypoints using visibility flag.  

### Alternatives

_No response_

### Additional context

My hope in writing here is to request and encourage updating of the model to address the issue/addition suggested. If not, then if I could please get some help in tracking down the source code where Keypoint RCNN is converting all flags to v=1 and handling/training flags so that I might be able to modify this behavior, as the model does not seem to learn the flag values presently. In my use case, what I want is for Keypoint RCNN to successfully predict the right flag (e.g. v=0) so that I can use it later on, or at least predict a coordinate of (0.0,0.0) (or some other fixed value) for keypoints with v=0. The need is to be able to distinguish between visible and occluded keypoints. Even just two learned flags that work as expected (v=0 and v=1) would be very useful to have. Any suggestions or guidance would be great. Thanks for taking the time to reply.

cc @datumbox @YosuaMichael"
Error Downloading Kinetics 400 dataset,pytorch/vision,2022-04-22 18:24:13,7,module: datasets,5865,1212655903,"### 🐛 Describe the bug

Error in downloading Kinetics 400 dataset. Download stops at training set tar file 121 (full k400 dataset is 200+ tar files).

I know this isn't a problem with the disk being out of write space. This has happened twice with different users and environments.

Not seeing this exact error raised as an issue anywhere else. Seems something is wrong with one of the downloaded files? Any ideas or workaround would be greatly appreciated! 

Thank you!!!!!

Code to produce Error:
```
import torchvision
kinetics_path = '~/kinetics400'
kinetics400_dataset = torchvision.datasets.Kinetics(root=kinetics_path, 
                                                    frames_per_clip=5,
                                                    num_classes='400',
                                                    download=True)
```

Last part of error message (top of message is cut off, sorry.)
```
Traceback (most recent call last):
  File ""/bsdir/donwload_kinetics.py"", line 4, in <module>
    kinetics400_dataset = torchvision.datasets.Kinetics(root=kinetics_path,
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/site-packages/torchvision/datasets/kinetics.py"", line 127, in __init__
    self.download_and_process_videos()
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/site-packages/torchvision/datasets/kinetics.py"", line 152, in download_and_process_videos
    self._download_videos()
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/site-packages/torchvision/datasets/kinetics.py"", line 184, in _download_videos
    download_and_extract_archive(line, tar_path, self.split_folder)
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/site-packages/torchvision/datasets/utils.py"", line 431, in download_and_extract_archive
    extract_archive(archive, extract_root, remove_finished)
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/site-packages/torchvision/datasets/utils.py"", line 408, in extract_archive
    extractor(from_path, to_path, compression)
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/site-packages/torchvision/datasets/utils.py"", line 271, in _extract_tar
    with tarfile.open(from_path, f""r:{compression[1:]}"" if compression else ""r"") as tar:
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/tarfile.py"", line 1629, in open
    return func(name, filemode, fileobj, **kwargs)
  File ""/bsdir/miniconda3/envs/torch_cuda_11_3/lib/python3.9/tarfile.py"", line 1686, in gzopen
    raise ReadError(""not a gzip file"")
tarfile.ReadError: not a gzip file
````

### Versions

PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: version 3.5.1
Libc version: glibc-2.23

Python version: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.4.0-210-generic-x86_64-with-glibc2.23
Is CUDA available: True
CUDA runtime version: 9.0.176
GPU models and configuration: 
GPU 0: TITAN Xp
GPU 1: TITAN Xp
GPU 2: TITAN Xp
GPU 3: TITAN Xp
GPU 4: TITAN Xp
GPU 5: TITAN Xp

Nvidia driver version: 455.45.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10
/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.21.2
[pip3] numpydoc==1.2
[pip3] steerable-pytorch==0.1
[pip3] torch==1.11.0
[pip3] torchaudio==0.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] libblas                   3.9.0            12_linux64_mkl    conda-forge
[conda] libcblas                  3.9.0            12_linux64_mkl    conda-forge
[conda] liblapack                 3.9.0            12_linux64_mkl    conda-forge
[conda] liblapacke                3.9.0            12_linux64_mkl    conda-forge
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py39h7f8727e_0  
[conda] mkl_fft                   1.3.1            py39hd3c417c_0  
[conda] mkl_random                1.2.2            py39h51133e4_0  
[conda] mypy_extensions           0.4.3            py39h06a4308_1  
[conda] numpy                     1.21.2           py39h20f2e39_0  
[conda] numpy-base                1.21.2           py39h79a1101_0  
[conda] numpydoc                  1.2                pyhd3eb1b0_0  
[conda] pytorch                   1.11.0          py3.9_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch

cc @pmeier @YosuaMichael"
Unable to generate a TensorRT GeneralizedRCNNTransform model with a dynamic batch,pytorch/vision,2022-04-21 17:12:51,1,needs discussion#topic: object detection,5858,1211325602,"### 🐛 Describe the bug

I'm trying to generate a TensorRT engine of `RetinaNet` that uses `GeneralizedRCNNTransform`. By bypassing a couple of layers, it works fine for a static batch size. However, when using a dynamic batch size, I get an issue that I don't know how to fix.

In `class GeneralizedRCNNTransform(nn.Module)`, there is this part in the `forward` method:

```python
for i in range(len(images)):
    image = images[i]
    target_index = targets[i] if targets is not None else None

    if image.dim() != 3:
        raise ValueError(""images is expected to be a list of 3d tensors ""
                            ""of shape [C, H, W], got {}"".format(image.shape))
    image = self.normalize(image) # here are where the sub and div nodes comes from
    image, target_index = self.resize(image, target_index) # I'm bypassing this
    images[i] = image
    if targets is not None and target_index is not None:
        targets[i] = target_index
```

When converting my model with:
```python
torch.onnx.export(
    model,
    example,
    onnx_model_path.split('.')[0] + '_dynamic.onnx',
    verbose=False,
    opset_version=11,
    input_names=['input'],
    dynamic_axes={
        'input': {0: 'batch_size'}
    }
)
```

I get the following tree:

![image](https://user-images.githubusercontent.com/29719028/164513180-71806ed3-d55c-43a1-b467-74cd74e1bedb.png)

Which is prefectly fine for the given `example` batch size (3 in this case), but this tree will not work for any other batch size once it is converted to a TensorRT engine. I get this kind of error:

```bash
[12/10/2021-13:29:40] [TRT] [E] 7: [shapeMachine.cpp::execute::565] Error Code 7: Internal Error (Split_0_0: ISliceLayer has out of bounds access on axis 0
condition '<' violated
Instruction: CHECK_LESS 1 1
)
[12/10/2021-13:29:40] [TRT] [E] 2: [executionContext.cpp::enqueueInternal::366] Error Code 2: Internal Error (Could not resolve slots: )
```

Is there any way I can bypass this `for` loop which is creating the `split` and `concat` nodes? Do I need to redefine the forward method? I can't really bypass the normalisation, or else I will loose in accuracy. Can I maybe normalize it all at once without that `for` loop?

Thanks 😄 

### Versions

Collecting environment information...
PyTorch version: 1.10.0
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (aarch64)
GCC version: (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.10.2
Libc version: glibc-2.25

Python version: 3.6.9 (default, Mar 15 2022, 13:55:28)  [GCC 8.4.0] (64-bit runtime)
Python platform: Linux-4.9.253-tegra-aarch64-with-Ubuntu-18.04-bionic
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.2.1
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.2.1
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.2.1
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.2.1
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.2.1
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.2.1
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.2.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.10.0
[pip3] torch2trt==0.3.0
[pip3] torchvision==0.11.1
[conda] Could not collect

cc @datumbox @YosuaMichael"
num_classes in segmentation examle,pytorch/vision,2022-04-15 02:22:56,1,,5822,1205184930,"Hi, I have some questions about `num_classes` in the [segmentation example](https://github.com/pytorch/vision/tree/main/references/segmentation). 

Since the pretraining classes is 21, should the target mask be integers from 0 to 21, where 0 is the background?

If so, my next question is do we need to compute loss over 0 (background)? I think generally background pixels is the majority, will it hurt the model so that the model would always predict 0 (background) as the majority? Or should we ignore background label by passing ignore_index=0 in cross entropy loss? I see that [here](https://github.com/pytorch/vision/blob/main/references/segmentation/train.py#L51) we are ignoring index 255, but I am not sure where 255 comes from.

Thanks! Any input is highly appreciated!"
Torch Hub download fails due to Torchvision dependency?,pytorch/vision,2022-04-13 04:58:00,7,needs reproduction#module: hub,5813,1202758328,"### 🐛 Describe the bug

Running the following code returns the error seen below. It seems like there is some type of dependency or call to torchvision. Torchvision, however, doesn't list the new efficientnet models. 

![Screenshot 2022-04-13 004546](https://user-images.githubusercontent.com/72167800/163102125-ebedc6ec-18bf-4f4d-9d9f-a09f99ac6a16.png)

```py
import torch
from torch import hub
#import torchvision.models as models

resnet18_model = torch.hub.load('pytorch/vision:main',
                         'resnet18',
                         pretrained = True)
```

![Screenshot 2022-04-13 004413](https://user-images.githubusercontent.com/72167800/163101924-3195a906-2653-4efe-b875-6533089c6d3f.png)

It seems like there is something missing. I have cleared my .cache folder, and even updated the efficientnet.py code on my local machine to match the latest. Still receiving this Import Error. I am running the following versions:

![Screenshot 2022-04-13 004800](https://user-images.githubusercontent.com/72167800/163102337-7fc56af6-415c-4607-9381-bdd42b2440b5.png)

Even if I do a simple:

```py
torch.hub.list('pytorch/vision')
```

I receive the following error:

![Screenshot 2022-04-13 004900](https://user-images.githubusercontent.com/72167800/163102604-6a4885c1-2034-43b9-95a1-8c0664922aac.png)


### Versions

![Screenshot 2022-04-13 005622](https://user-images.githubusercontent.com/72167800/163103278-84e96b39-4c0a-4bb3-8b67-fc61c29a4212.png)
"
Provide full stack of video pipeline by FFMPEG-GPU or pyav to accelerate.,pytorch/vision,2022-04-12 16:54:19,2,module: io#module: video#new feature,5810,1202117375,"### 🚀 The feature

 1. Provide the API for reading video by ffmpeg-gpu (or pyav) from not only the file but the video stream.
 2. Provide the API for encoding frames by ffmpeg-gpu (or pyav) from not only the file but the video stream.
 3. Provide the API for writing video by ffmpeg-gpu (or pyav) from not only the file but the video stream.

### Motivation, pitch

As a video worker, output and encoding is the same important thing as input and decoding. 
I am very happy to see you to add the video functions.
In the future , I hope i can use vision to finish the pipeline: input -> decode->inference->encode>output.

### Alternatives

_No response_

### Additional context

_No response_"
Write text on images as an augmentation,pytorch/vision,2022-04-08 14:50:47,5,needs discussion#module: transforms,5791,1197491634,"### 🚀 Torchvision GPU compatible text writing on images
Hi. 

Right now, I believe that if you wanna write text on a GPU Tensor, you're gonna have to do it in CPU memory.

This is unfortunate since text writing is a very good augmentation is some cases, where input data might have timestamps for example. Also, most efficient loading libraries make use of the GPU (for example Nvidia Dali, decord) for efficient loading, meaning that converting a tensor back to numpy array sacrifices this advantage they have for ""large"" training (when you don't have the whole dataset in RAM).

I think it'd be great if writing random text on an image was a torchvision feature :D

### Alternatives

Convert the tensor it back to numpy array and use OpenCV.


cc @vfdev-5 @datumbox"
MaskRCNN from ScriptModule to ONNX - Unknown Type BoxCoder,pytorch/vision,2022-04-08 11:20:45,4,module: onnx,5790,1197194325,"### 🐛 Describe the bug

While attempting to create an ONNX version of Maskrcnn, starting from a ``ScriptModule``, an error occurs, indicating that ``__torch__.torchvision.models.detection._utils.BoxCoder`` is an unknown type.

MWE:
```python
import torch
from torchvision.models.detection.mask_rcnn import maskrcnn_resnet50_fpn

model = maskrcnn_resnet50_fpn()
model.eval()
script_model = torch.jit.script(model)
example_image = torch.rand((3, 800, 1000))
torch.onnx.export(
	script_model,
	[example_image],
	""test.onnx"",
	example_outputs=script_model([example_image])[1], # index 0 is losses
	opset_version = 11
)
```

Error traceback:
```
Traceback (most recent call last):
  File ""/home/nmota/test_onnx.py"", line 8, in <module>
    torch.onnx.export(
  File ""/usr/lib/python3.9/site-packages/torch/onnx/__init__.py"", line 275, in export
    return utils.export(model, args, f, export_params, verbose, training,
  File ""/usr/lib/python3.9/site-packages/torch/onnx/utils.py"", line 88, in export
    _export(model, args, f, export_params, verbose, training, input_names, output_names,
  File ""/usr/lib/python3.9/site-packages/torch/onnx/utils.py"", line 689, in _export
    _model_to_graph(model, args, verbose, input_names,
  File ""/usr/lib/python3.9/site-packages/torch/onnx/utils.py"", line 458, in _model_to_graph
    graph, params, torch_out, module = _create_jit_graph(model, args,
  File ""/usr/lib/python3.9/site-packages/torch/onnx/utils.py"", line 402, in _create_jit_graph
    module, params = torch._C._jit_onnx_list_model_parameters(freezed_m)
RuntimeError: 
Unknown type __torch__.torchvision.models.detection._utils.BoxCoder (of Python compilation unit at: 0x55bbdb787f00) encountered in handling model params. This class type does not extend __getstate__ method.:

✗ - status code 1
```

Unfortunately, I cannot test with a more recent version. Is this something that has been fixed recently?

### Versions

```
PyTorch version: 1.9.0
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Arch Linux (x86_64)
GCC version: (GCC) 11.1.0
Clang version: 12.0.1
CMake version: version 3.21.2
Libc version: glibc-2.33

Python version: 3.9.6 (default, Jun 30 2021, 10:22:16)  [GCC 11.1.0] (64-bit runtime)
Python platform: Linux-5.13.13-arch1-1-x86_64-with-glibc2.33
Is CUDA available: True
CUDA runtime version: 11.4.100
GPU models and configuration: 
GPU 0: NVIDIA TITAN X (Pascal)

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/libcudnn.so.8.2.2
/usr/lib/libcudnn_adv_infer.so.8.2.2
/usr/lib/libcudnn_adv_train.so.8.2.2
/usr/lib/libcudnn_cnn_infer.so.8.2.2
/usr/lib/libcudnn_cnn_train.so.8.2.2
/usr/lib/libcudnn_ops_infer.so.8.2.2
/usr/lib/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.20.3
[pip3] torch==1.9.0
[pip3] torchvision==0.10.0a0
[conda] Could not collect
```

cc @neginraoof"
Using Kinetics400 from references raises a deprecation warning,pytorch/vision,2022-04-07 17:37:42,3,bug#module: datasets#module: video,5787,1196364014,"### 🐛 Describe the bug

Using Kinetics400 from references leads to a deprecation warning. The fastest way to reproduce:
```
torchrun --nproc_per_node=8 train.py --data-path /datasets01/kinetics/070618/ --train-dir=val_avi-480p --val-dir=val_avi-480p --batch-size=64 --sync-bn --test-only --weights R2Plus1D_18_Weights.DEFAULT --cache-dataset

./vision/torchvision/io/video.py:160: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  warnings.warn(
```

It seems this is due to the following line:
https://github.com/pytorch/vision/blob/b7c59a086f4e45b8e5e799084e97cca775977a3b/torchvision/datasets/video_utils.py#L327

Which calls the method with the default parameter for `pts_unit=""pts""` which is deprecated:
https://github.com/pytorch/vision/blob/b7c59a086f4e45b8e5e799084e97cca775977a3b/torchvision/io/video.py#L155-L163

If the specific option is deprecated, TorchVision shouldn't be using the feature.

### Versions

Latest main branch

cc @pmeier @YosuaMichael @bjuncek "
FocalLoss without logits and as class,pytorch/vision,2022-04-06 11:54:45,1,enhancement#needs discussion#module: ops,5762,1194505858,"### 🚀 The feature

Making the focal loss api similar to BCE api. In which the difference between BCELoss and BCELossWithLogits is clear and is made as a class as well as a function.

### Motivation, pitch

When using a model that explicitly uses the sigmoid activation at the end it is useful to be able to simply use focal loss without injecting the loss before this activation. The use of focal loss in the similar way to BCELoss as a class would also be more elegant and concise.

### Alternatives

_No response_

### Additional context

_No response_"
[RFC] `torchvision.transforms` revamp,pytorch/vision,2022-04-06 08:56:35,0,module: transforms,5754,1194293184,"### 🚀 The feature

*Note: To track the progress of the project check out [this board](https://github.com/pytorch/vision/projects/5).*

The current transforms in the `torchvision.transforms` name space are limited to images. This makes it hard to use them for tasks that require the transform to be applied not only to the input image, but also to the target. For example, in object detection, resizing or cropping the input image also affects the bounding boxes. 

This projects aims to resolve this by providing transforms that can handle the full sample possibly including images, bounding boxes, segmentation masks, and so on without the need for user interference. The implementation of this project happens in the `torchvision.prototype.transforms` namespace. For example:


```py
from torchvision.prototype import features, transforms

# this will be supplied by a dataset from torchvision.prototype.datasets
image = features.EncodedImage.from_path(""test/assets/fakedata/logos/rgb_pytorch.png"")
label = features.Label(0)

transform = transforms.Compose(
    transforms.DecodeImage(),
    transforms.RandomHorizontalFlip(p=1.0),
    transforms.Resize((100, 300)),
)

transformed_image1 = transform(image)
transformed_image2, transformed_label = transform(image, label)

# whether we call the transform with label or not has no effect on the image transform
assert transformed_image1.eq(transformed_image2).all()
# the transform is a no-op for labels
assert transformed_label.eq(label).all()
```

![before_image](https://user-images.githubusercontent.com/6849766/161979289-b580ef3c-b947-4f9d-be3e-6b05659736aa.png) ![after_image](https://user-images.githubusercontent.com/6849766/161979304-2f3a30d0-991f-4ad1-b091-61600d49fdb3.png)


```py
# this will be supplied by a dataset from torchvision.prototype.datasets
bounding_box = features.BoundingBox(
    [60, 30, 15, 15], format=features.BoundingBoxFormat.CXCYWH, image_size=(100, 100)
)

transformed_image, transformed_bounding_box = transform(image, bounding_box)
```

![before_image_and_bounding_box](https://user-images.githubusercontent.com/6849766/161932345-652f3012-7427-44b4-aed8-952249db3345.png) ![after_image_and_bounding_box](https://user-images.githubusercontent.com/6849766/161932375-47ca2a19-ff36-40c4-af78-579351466184.png)

```py
# this will be supplied by a dataset from torchvision.prototype.datasets
segmentation_mask = torch.zeros((100, 100), dtype=torch.bool)
segmentation_mask[24:36, 55:66] = True
segmentation_mask = features.SegmentationMask(segmentation_mask)

transformed_image, transformed_segmentation_mask = transform(image, segmentation_mask)
```

![before_image_and_segmentation_mask](https://user-images.githubusercontent.com/6849766/161980409-58afc8ce-9b4c-4c47-937d-f3ed551218e5.png) ![after_image_and_segmentation_mask](https://user-images.githubusercontent.com/6849766/161980425-e6e8f4d5-18f9-4f83-a3e4-8a6a19de6805.png)

### Classification

- [x] Port transforms from `torchvision.transforms` #5520
- [x] Port transforms from the classification references #5780

### Detection

- [x] New kernels for bounding boxes #5514
- [x] Port transforms from the detection references #5542

### Segmentation

- [x] New kernels for segmentation masks #5782
- [ ] Port transforms from segmentation references. This will automatically be achieved with the completion above, since there are no special [segmentation transforms](https://github.com/pytorch/vision/blob/main/references/segmentation/transforms.py) in our references. If the respective kernels are implemented, the transforms will handle the segmentation case out of the box.

### Other

- [ ] #5626

cc @vfdev-5 @datumbox"
MNIST import fails: cannot import name 'OpOverloadPacket' from 'torch._ops',pytorch/vision,2022-04-06 02:33:10,7,needs reproduction#topic: binaries,5748,1193931047,"### 🐛 Describe the bug

Trying to import the MNIST dataset on Linux as follows:

```python
import torchvision.datasets as datasets
mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)
```

fails with an ImportError:

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
/tmp/ipykernel_13067/1857463301.py in <module>
----> 1 import torchvision.datasets as datasets
      2 
      3 mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torchvision/__init__.py in <module>
      5 from torchvision import datasets
      6 from torchvision import io
----> 7 from torchvision import models
      8 from torchvision import ops
      9 from torchvision import transforms

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torchvision/models/__init__.py in <module>
      1 from .alexnet import *
----> 2 from .convnext import *
      3 from .resnet import *
      4 from .vgg import *
      5 from .squeezenet import *

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torchvision/models/convnext.py in <module>
      8 from .._internally_replaced_utils import load_state_dict_from_url
      9 from ..ops.misc import ConvNormActivation
---> 10 from ..ops.stochastic_depth import StochasticDepth
     11 from ..utils import _log_api_usage_once
     12 

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torchvision/ops/__init__.py in <module>
     16 from .giou_loss import generalized_box_iou_loss
     17 from .misc import FrozenBatchNorm2d, SqueezeExcitation
---> 18 from .poolers import MultiScaleRoIAlign
     19 from .ps_roi_align import ps_roi_align, PSRoIAlign
     20 from .ps_roi_pool import ps_roi_pool, PSRoIPool

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torchvision/ops/poolers.py in <module>
      3 
      4 import torch
----> 5 import torch.fx
      6 import torchvision
      7 from torch import nn, Tensor

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torch/fx/__init__.py in <module>
     81 '''
     82 
---> 83 from .graph_module import GraphModule
     84 from ._symbolic_trace import symbolic_trace, Tracer, wrap, PH, ProxyableClassMeta
     85 from .graph import Graph

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torch/fx/graph_module.py in <module>
      6 import linecache
      7 from typing import Type, Dict, List, Any, Union, Optional, Set
----> 8 from .graph import Graph, _is_from_torch, _custom_builtins, PythonCode
      9 from ._compatibility import compatibility
     10 from torch.package import Importer, sys_importer

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torch/fx/graph.py in <module>
----> 1 from .node import Node, Argument, Target, map_arg, _type_repr, _get_qualified_name
      2 import torch.utils._pytree as pytree
      3 from . import _pytree as fx_pytree
      4 from ._compatibility import compatibility
      5 import contextlib

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torch/fx/node.py in <module>
      7 import types
      8 import warnings
----> 9 from torch.fx.operator_schemas import normalize_function, normalize_module, ArgsKwargsPair
     10 
     11 if TYPE_CHECKING:

~/miniforge3/envs/bios8366/lib/python3.9/site-packages/torch/fx/operator_schemas.py in <module>
      8 from torch._jit_internal import boolean_dispatched
      9 from ._compatibility import compatibility
---> 10 from torch._ops import OpOverloadPacket
     11 
     12 if TYPE_CHECKING:

ImportError: cannot import name 'OpOverloadPacket' from 'torch._ops' (/home/fonnesbeck/miniforge3/envs/bios8366/lib/python3.9/site-packages/torch/_ops.py)
```


### Versions

PyTorch version: 1.11.0+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Debian GNU/Linux 11 (bullseye) (x86_64)
GCC version: (Debian 10.2.1-6) 10.2.1 20210110
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:41:03)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] numpyro==0.9.1
[pip3] torch==1.11.0
[pip3] torchvision==0.12.0
[conda] Could not collect"
Ground-truth bounding boxes included in RPN proposals?,pytorch/vision,2022-04-04 18:14:16,2,,5735,1192142005,"Hi,

I am training a Faster-RCNN-like architecture and wanted to log the detection losses on the validation dataset. To obtain these losses, I passed in the `targets` of the validation data to the `forward()` method of `GeneralizedRCNN`. When investigating the results, however, I noticed that each ground-truth bounding box had a near-exact match with a predicted bounding box. When passing `targets=None`, these bounding boxes were gone. Is this the intended behavior? If so, what is the rationale behind it?

I figured that this line is where the predicted and ground-truth boxes get mixed:

https://github.com/pytorch/vision/blob/095cabb76cdcd4763bad629481d189b91e3df42c/torchvision/models/detection/roi_heads.py#L646

It seems to have something to do with balancing positive/negative sampling. Yet, I don't see how a model would improve from boxes that it did not generate itself."
Segmentation masks IoU support,pytorch/vision,2022-04-02 23:28:27,4,module: ops#topic: object detection#new feature,5726,1190763414,"### 🚀 The feature

pairwise distance on segmentation masks like (torchvision.ops.box_iou)

### Motivation, pitch

motivation: cost matrix construction in object tracking

### Alternatives

pycocotools.maskUtils.iou

### Additional context

_No response_

cc @datumbox @YosuaMichael"
2022: state of video IO in torchvision,pytorch/vision,2022-04-01 10:59:15,3,module: video,5720,1189623994,"There have been many developments over the last couple of months with a big push in 2022H1 to get things closed up (mainly by @prabhat00155 and @datumbox). Here I'll try to summarize what is the current state of things.

## Features (current, in-dev)

At the moment, `torchvision` has **two API's** one can use for video-reading.
1. `read_video` video API (stable) -- this is a legacy video-reading solution that we're looking to move away from. However, due to external use, we continue to support and patch it. It supports `pyav` and `video_reader` backends.
2. `VideoReader` fine-grained API (prototypem #2660) -- we're moving towards this as a goal for 2022. The API itself is finished, however, due to issues with various backends it still remains unused (see the installation issue below). Supports `video_reader` and `GPU` backends.

Furthermore, we also have three **backends** for video reading.
1. `pyav` -- naive extension of pyAV capabilities
2. `video_reader` -- our own C++ implementation that allows video IO to be torchscriptable. If JIT requirement is dropped, might be deprecated despite minor speed improvements over `pyav`.
3. `GPU` -- highly experimental and not-yet properly tested. Maintenance and further development will depend on the demand from customers and community. 

Overall goal in 2022 is to migrate all APIs (and prototype datasets) to the `VideoReader` API, and hopefully depricate `read_video` as much as possible.

 Related tasks include (will be updated):

- [ ] Datasets to use new API #5250 
- [ ] Reference scripts to use new API


## Currently known issues and enhancements needed

Probably the biggest issue plaguing video is **installation** (see #4260 for some reference). If user wants to install ffmpeg or GPU backends and support for `VideoReader` API, they need to install torchvision from source, and in the case of GPU also download proprietary drivers from NVIDIA. This process should be properly documented until a better/alternative solution is found. 

- [ ] Add proper build documentation #3460

Due to the lack of users, the real-world bug reports have been scarce. Here is the (non-exhaustive) list of known issues, and their progress, sorted by topic, with additional comments in italics if applicable.

#### General

- [ ] Change CPU decoder output frames to use ITU709 colour space #5245 -- _done, but not merged_
- [x] Assertion error during dataset creation #4839 #4112 #4357 #2184 #1884
- [ ] Mismatch in audio frames returned by pyav and video reader #3986 -- _needs revisiting based on latest improvements and bugfixes_


#### `video_reader` backend and `VideoReader` API

- [ ] new video reading API crash #5419 (can't reproduce -- help welcome)
- [ ] read_video_from_file() causes seg fault with Python 3.9 #4430 -- _flakey, can't reproduce on all machines_
- [ ] video_reader test crashes on Windows #4429
- [ ] Black band at certain videos #3534 -- _suspected issue in FFMPEG, needs revisiting_


#### GPU decoding issues and enhancements (note, these are low-pri due to lack of developers and road-map changes so we'll be relatively slow in fixing these):

- [ ] GPU VideoReader not working #5702
- [ ] video classification experiments using GPU decoder #5252
- [ ] video classification reference script with GPU decoder support #5251
- [ ] GPU decoder refactoring #5148
- [ ] Run GPU decoding tests in CI #5147
- [ ] Support reading video from memory #5142
- [ ] Return pts per frame after video decoding on GPU #5140


## Archived feature requests

- [ ] FFmpeg-based rescaling and frame rate #3016 -- _enhancement we've put on pause due to low adoption_
- [Feat] Camera Stream API proposal #2920
- Contribution: select classes in UCF101 dataset #1791


cc @datumbox for visibility
"
Feature extraction in torchvision.models.vit_b_16,pytorch/vision,2022-04-01 08:09:20,7,module: models#module: features,5718,1189425197,"### 🐛 Describe the bug

Hi

It’s easy enough to obtain output features from the CNNs in torchvision.models by doing this:

```python
import torch
import torch.nn as nn
import torchvision.models as models

model = models.resnet18()
feature_extractor = nn.Sequential(*list(model.children())[:-1])
output_features = feature_extractor(torch.randn(1, 3, 224, 224))
```

However, when I attempt to do this with torchvision.models.vit_b_16:
```python
import torch
import torch.nn as nn
import torchvision.models as models

model = models.vit_b_16()
feature_extractor = nn.Sequential(*list(model.children())[:-1])
output_features = feature_extractor(torch.randn(1, 3, 224, 224))
```
I get the following error:
```
AssertionError: Expected (batch_size, seq_length, hidden_dim) got torch.Size([1, 768, 14, 14])
```

Any help would be greatly appreciated.

### Versions

Torch version: 1.11.0+cu102
Torchvision version: 0.12.0+cu102

cc @datumbox"
[Feedback] Warn Developers Regarding Major Code Changes,pytorch/vision,2022-03-30 21:13:20,6,,5710,1186998103,"### 🚀 The feature

Flag parts of the repo that are undergoing major changes (breaking changes between two releases) + have a bot post on PRs that touch these high traffic files.

### Motivation, pitch

**TL;DR:** torchvision moves pretty fast and external developers want to know if they are developing on something that may change very soon. 

Imagine you are an interested developer and you would love to contribute to torchvision. You scan through the issues and you see an interesting one you can help tackle! You follow some directions, get right into the task, and a few days later, you have a PR. You iterate based on CI results and you're pretty proud of your feature. 

You get a reviewer to look through and hopefully approve your awesome change. BUT that's not what happens. Instead, the reviewer lets you know that the code you're touching is actually migrating to some other structure and the work you did on your PR has become irrelevant. What a downer. **You wish you knew this earlier, before you spent so much time on the PR.**

The current system can be quite discouraging for an external developer. To promote contributions, torchvision should notify developers of major code changes in some way. One way is to flag those paths that are likely changing soon and having a bot post on PRs touching those paths with a warning message. This way, developers have more clarity on what they should work on.

### Alternatives

Other alternatives are for torchvision to update existing issues/feature requests with up-to-date content so that developers can fix an issue knowing that their work would not be in vain. 

### Additional context

This feedback was gathered from talking to current external contributors."
GPU VideoReader not working,pytorch/vision,2022-03-29 06:08:55,9,bug#module: video,5702,1184871411,"### 🐛 Describe the bug

```
from torchvision.io import VideoReader
import urllib.request
import matplotlib.pyplot as plt
%matplotlib inline

urllib.request.urlretrieve(""https://downloads.videezy.com/system/protected/files/000/004/210/4.mp4"", ""/tmp/cat.mp4"")

used_timestamps = sorted(np.random.choice(np.arange(0, 8, 0.1), 10, replace=False).tolist())

video_reader_cpu = VideoReader(""/tmp/cat.mp4"", device=""cpu"", num_threads=4)


images_cpu = []
for seek in used_timestamps:
    video_reader_cpu.seek(seek)
    frame = next(video_reader_cpu)
    images_cpu.append(frame[""data""].permute(1,2,0))

    
video_reader_gpu = VideoReader(""/tmp/cat.mp4"", device=""cuda"")

images_gpu = []
for seek in used_timestamps:
    video_reader_gpu.seek(seek)
    frame = next(video_reader_gpu)
    images_gpu.append(frame[""data""].cpu())

for i1, i2 in zip(images_cpu, images_gpu):
    plt.figure()
    plt.subplot(121).imshow(i1)
    plt.subplot(122).imshow(i2)
```
<img width=""423"" alt=""Screen Shot 2022-03-28 at 10 59 44 PM"" src=""https://user-images.githubusercontent.com/8623092/160543628-000de7d3-a23c-4a3d-8757-229c457f0392.png"">
<img width=""466"" alt=""Screen Shot 2022-03-28 at 10 59 56 PM"" src=""https://user-images.githubusercontent.com/8623092/160543634-07632a8d-0b56-462f-9bc1-fe2e2fc93bf3.png"">


When seeking to specific timestamps in the video and trying to extract the closest image frames, the cpu implementation of VideoReader works exactly as expected. However, the gpu implementation outputs progressively more corrupted versions of a single frame, with halo effects of other frames getting more prevalent in the latter frames.

### Versions

```
Collecting environment information...
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.27

Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.15.0-163-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration:
GPU 0: NVIDIA RTX A5000
GPU 1: NVIDIA RTX A5000
GPU 2: NVIDIA RTX A5000
GPU 3: NVIDIA RTX A5000
GPU 4: NVIDIA RTX A5000
GPU 5: NVIDIA RTX A5000
GPU 6: NVIDIA RTX A5000
GPU 7: NVIDIA RTX A5000

Nvidia driver version: 495.29.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy==0.931
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.21.2
[pip3] pytorch-lightning==1.6.0rc1
[pip3] torch==1.11.0
[pip3] torchelastic==0.2.2
[pip3] torchmetrics==0.7.3
[pip3] torchtext==0.12.0
[pip3] torchvision==0.13.0a0+1db8795
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.3.1               ha36c431_9    nvidia
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640
[conda] mkl-service               2.4.0            py38h7f8727e_0
[conda] mkl_fft                   1.3.1            py38hd3c417c_0
[conda] mkl_random                1.2.2            py38h51133e4_0
[conda] mypy                      0.931                    pypi_0    pypi
[conda] mypy-extensions           0.4.3                    pypi_0    pypi
[conda] numpy                     1.21.2           py38h20f2e39_0
[conda] numpy-base                1.21.2           py38h79a1101_0
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-lightning         1.6.0rc1                 pypi_0    pypi
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchelastic              0.2.2                    pypi_0    pypi
[conda] torchmetrics              0.7.3                    pypi_0    pypi
[conda] torchtext                 0.12.0                     py38    pytorch
[conda] torchvision               0.13.0a0+1db8795          pypi_0    pypi
```"
MIssing torchvision::nms error in the C++ CUDA TorchVision API,pytorch/vision,2022-03-28 16:15:27,1,,5697,1183668760,"### 🐛 Describe the bug

I'm unable to load a my trained MaskRCNN model (using the one from the `torchvision` Python module). I'm converting it to TorchScript using `torch.jit.script`, saving it as a `.pt` file and finally using the `torch::jit::load` from LibTorch:

```c++
torch::NoGradGuard no_grad;
model = torch::jit::load(model_path);
```

Nothing fancy, but I'm getting the following error:

```
terminate called after throwing an instance of 'torch::jit::ErrorReport'
  what():  
Unknown builtin op: torchvision::nms.
Could not find any similar ops to torchvision::nms. This op may not exist or may not be currently supported in TorchScript.
:
  File ""/home/aurelien/Documents/Projects/autotrain-env/lib/python3.8/site-packages/torchvision/ops/boxes.py"", line 40
        _log_api_usage_once(nms)
    _assert_has_ops()
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
           ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
Serialized   File ""code/__torch__/torchvision/ops/boxes.py"", line 154
  _64 = __torch__.torchvision.extension._assert_has_ops
  _65 = _64()
  _66 = ops.torchvision.nms(boxes, scores, iou_threshold)
        ~~~~~~~~~~~~~~~~~~~ <--- HERE
  return _66
'nms' is being compiled since it was called from '_batched_nms_vanilla'
  File ""/home/aurelien/Documents/Projects/autotrain-env/lib/python3.8/site-packages/torchvision/ops/boxes.py"", line 108
    for class_id in torch.unique(idxs):
        curr_indices = torch.where(idxs == class_id)[0]
        curr_keep_indices = nms(boxes[curr_indices], scores[curr_indices], iou_threshold)
                            ~~~ <--- HERE
        keep_mask[curr_indices[curr_keep_indices]] = True
    keep_indices = torch.where(keep_mask)[0]
Serialized   File ""code/__torch__/torchvision/ops/boxes.py"", line 83
    _31 = torch.index(boxes, _30)
    _32 = annotate(List[Optional[Tensor]], [curr_indices])
    curr_keep_indices = __torch__.torchvision.ops.boxes.nms(_31, torch.index(scores, _32), iou_threshold, )
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    _33 = annotate(List[Optional[Tensor]], [curr_keep_indices])
    _34 = torch.index(curr_indices, _33)
'_batched_nms_vanilla' is being compiled since it was called from 'batched_nms'
Serialized   File ""code/__torch__/torchvision/ops/boxes.py"", line 35
    idxs: Tensor,
    iou_threshold: float) -> Tensor:
  _9 = __torch__.torchvision.ops.boxes._batched_nms_vanilla
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
  _10 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
  _11 = torch.numel(boxes)
'batched_nms' is being compiled since it was called from 'RegionProposalNetwork.filter_proposals'
Serialized   File ""code/__torch__/torchvision/models/detection/rpn.py"", line 72
    _11 = __torch__.torchvision.ops.boxes.clip_boxes_to_image
    _12 = __torch__.torchvision.ops.boxes.remove_small_boxes
    _13 = __torch__.torchvision.ops.boxes.batched_nms
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    num_images = (torch.size(proposals))[0]
    device = ops.prim.device(proposals)
'RegionProposalNetwork.filter_proposals' is being compiled since it was called from 'RegionProposalNetwork.forward'
  File ""/home/aurelien/Documents/Projects/autotrain-env/lib/python3.8/site-packages/torchvision/models/detection/rpn.py"", line 353
        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)
        proposals = proposals.view(num_images, -1, 4)
        boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)
                        ~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    
        losses = {}
Serialized   File ""code/__torch__/torchvision/models/detection/rpn.py"", line 43
    proposals0 = torch.view(proposals, [num_images, -1, 4])
    image_sizes = images.image_sizes
    _8 = (self).filter_proposals(proposals0, objectness0, image_sizes, num_anchors_per_level, )
                                                                       ~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    boxes, scores, = _8
    losses = annotate(Dict[str, Tensor], {})

Aborted (core dumped)
```

I tried running [this project](https://github.com/pytorch/vision/tree/main/test/tracing/frcnn) and it's working on CPU but not on GPU, I get this output:

```
Loading model
Model loaded
[W faster_rcnn.py:107] Warning: RCNN always returns a (Losses, Detections) tuple in scripting (function )
ok
output({}, [{boxes: [ CPUFloatType{0,4} ], labels: [ CPULongType{0} ], scores: [ CPUFloatType{0} ]}, {boxes: [ CPUFloatType{0,4} ], labels: [ CPULongType{0} ], scores: [ CPUFloatType{0} ]}])
terminate called after throwing an instance of 'c10::NotImplementedError'
  what():  The following operation failed in the TorchScript interpreter.
Traceback of TorchScript, serialized code (most recent call last):
  File ""code/__torch__/torchvision/models/detection/rpn.py"", line 122, in forward
      lvl1 = torch.index(lvl0, _28)
      nms_thresh = self.nms_thresh
      keep1 = _13(boxes2, scores1, lvl1, nms_thresh, )
              ~~~ <--- HERE
      keep2 = torch.slice(keep1, 0, None, (self).post_nms_top_n())
      _29 = annotate(List[Optional[Tensor]], [keep2])
  File ""code/__torch__/torchvision/ops/boxes.py"", line 52, in batched_nms
    _16 = _17
  else:
    _18 = _10(boxes, scores, idxs, iou_threshold, )
          ~~~ <--- HERE
    _16 = _18
  return _16
  File ""code/__torch__/torchvision/ops/boxes.py"", line 109, in _batched_nms_coordinate_trick
    _47 = torch.unsqueeze(torch.slice(offsets), 1)
    boxes_for_nms = torch.add(boxes, _47)
    keep = __torch__.torchvision.ops.boxes.nms(boxes_for_nms, scores, iou_threshold, )
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    _42 = keep
  return _42
  File ""code/__torch__/torchvision/ops/boxes.py"", line 154, in nms
  _64 = __torch__.torchvision.extension._assert_has_ops
  _65 = _64()
  _66 = ops.torchvision.nms(boxes, scores, iou_threshold)
        ~~~~~~~~~~~~~~~~~~~ <--- HERE
  return _66

Traceback of TorchScript, original code (most recent call last):
  File ""/home/aurelien/.local/lib/python3.8/site-packages/torchvision/models/detection/rpn.py"", line 266, in forward
    
            # non-maximum suppression, independently done per level
            keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)
                   ~~~~~~~~~~~~~~~~~~~ <--- HERE
    
            # keep only topk scoring predictions
  File ""/home/aurelien/.local/lib/python3.8/site-packages/torchvision/ops/boxes.py"", line 74, in batched_nms
        return _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)
    else:
        return _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
  File ""/home/aurelien/.local/lib/python3.8/site-packages/torchvision/ops/boxes.py"", line 40, in nms
        _log_api_usage_once(nms)
    _assert_has_ops()
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
           ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
RuntimeError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].

CPU: registered at /home/aurelien/Downloads/vision/torchvision/csrc/ops/cpu/nms_kernel.cpp:112 [kernel]
BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]
Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]
AutogradOther: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:35 [backend fallback]
AutogradCPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:39 [backend fallback]
AutogradCUDA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:47 [backend fallback]
AutogradXLA: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:51 [backend fallback]
AutogradLazy: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:55 [backend fallback]
AutogradXPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:43 [backend fallback]
AutogradMLC: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:59 [backend fallback]
AutogradHPU: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:68 [backend fallback]
Tracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:293 [backend fallback]
AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]
Autocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]
Batched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]


Exception raised from reportError at ../aten/src/ATen/core/dispatch/OperatorEntry.cpp:434 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6b (0x7f21dcc040eb in /opt/libtorch/lib/libc10.so)
frame #1: c10::impl::OperatorEntry::reportError(c10::DispatchKey) const + 0xa48 (0x7f215c2e2d98 in /opt/libtorch/lib/libtorch_cpu.so)
frame #2: c10::Dispatcher::callBoxed(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*) const + 0x44e (0x7f215e9dc1ae in /opt/libtorch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x3527b72 (0x7f215e641b72 in /opt/libtorch/lib/libtorch_cpu.so)
frame #4: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x52 (0x7f215e62fd02 in /opt/libtorch/lib/libtorch_cpu.so)
frame #5: <unknown function> + 0x3508f4d (0x7f215e622f4d in /opt/libtorch/lib/libtorch_cpu.so)
frame #6: torch::jit::Method::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) const + 0x194 (0x7f215e2d2da4 in /opt/libtorch/lib/libtorch_cpu.so)
frame #7: torch::jit::Module::forward(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) + 0xc3 (0x555faadc7cb5 in ./test_frcnn_tracing)
frame #8: main + 0x592 (0x555faadc31b0 in ./test_frcnn_tracing)
frame #9: __libc_start_main + 0xf3 (0x7f21222460b3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #10: _start + 0x2e (0x555faadc28ee in ./test_frcnn_tracing)

Aborted (core dumped)
```

I'm not sure what I'm missing! I've seen other people having this issue, but I have not been able to resolve it.

### Versions

LibTorch version 1.11.0 downloaded from the site.
Tested with TorchVision 0.11 and 0.12 (built with `cmake -DWITH_CUDA=on -DCMAKE_PREFIX_PATH=/opt/libtorch/share/cmake/Torch ..` from their branch)
Pop-OS 20.04
CUDA 11.3"
[RFC] Add `expand` option to `affine` transformation,pytorch/vision,2022-03-23 11:07:22,0,needs discussion#module: transforms#prototype,5665,1177964445,"
We would like to ask for some feedback about adding `expand` argument to affine image/mask transformations.

Currently, [rotate op](https://pytorch.org/vision/stable/generated/torchvision.transforms.functional.rotate.html#torchvision.transforms.functional.rotate) has `extend` argument which does the following:

<img width=512 src=""https://user-images.githubusercontent.com/2459423/158867251-666728e1-64dc-4a77-9656-8a52a53f920f.png""/>

Do you think that it would be helpful to have `expand` option with affine image/mask transformations ?


_Originally posted by @vfdev-5 in https://github.com/pytorch/vision/pull/5638#discussion_r833095644_

cc @vfdev-5 @datumbox @bjuncek"
Let's enable ND support for images/masks,pytorch/vision,2022-03-23 10:37:03,0,module: transforms#prototype,5664,1177930205,"_Originally posted by @vfdev-5 in https://github.com/pytorch/vision/pull/5613#discussion_r828077791_

cc @vfdev-5 @datumbox @bjuncek"
Clean up prototype transforms before migration,pytorch/vision,2022-03-16 09:22:00,4,module: transforms#prototype,5626,1170748450,"The transforms in `torchvision.transforms` are fully JIT scriptable. To achieve this we needed a lot of helper methods and unnecessary strict or plain wrong annotations.

The revamped transforms in `torchvision.prototype.transforms` will no longer be JIT scriptable due to their ability to handle more complex inputs. Thus, we should clean-up our code removing everything that was just added to appease `torch.jit.script`. This should only happen as one of the last steps before migration to the main area.

cc @vfdev-5 @datumbox @bjuncek"
[RFC] Different menaning of angle parameter in affine and rotate ops,pytorch/vision,2022-03-14 14:05:18,2,needs discussion#module: transforms,5608,1168427796,"In Torchvision stable API, there are 2 ops: affine and rotate that has `angle` as parameter. 

However, positive `angle` parameter in `F.affine` op performs clockwise rotation. 

https://github.com/pytorch/vision/blob/8aadef5f55a8a0098de943dbf0480a796022e825/torchvision/transforms/functional.py#L1092

while in `F.rotate` it is an opposite meaning, it does counter clockwise rotation:

https://github.com/pytorch/vision/blob/8aadef5f55a8a0098de943dbf0480a796022e825/torchvision/transforms/functional.py#L1004

For Detectron2, Pillow or albumentations, they do counter clockwise rotation for positive angle value. 

Question: Should we align `F.affine` op to perform counter clockwise rotation for positive angle value ? 




cc @vfdev-5 @datumbox"
Return only images from `torchvision.dataset` datasets,pytorch/vision,2022-03-12 21:15:34,2,enhancement#module: datasets,5601,1167402280,"### 🚀 The feature

`torchvision.dataset` datasets should provide an option (e.g., a constructor argument) to return only the image (i.e., rather than an image and target pair).


### Motivation, pitch

I work on compression research and would like to use `torchvision.dataset` datasets but, as of now, they are too cumbersome to use because they provide more input than what is needed by my models (i.e., images and targets rather than just images).

### Alternatives

It is entirely possible a straightforward alternative exists! If so, I’d love to read about it!

### Additional context

I suspect this would also be welcome by any other community working on image to image methods.

cc @pmeier"
Strange RoIPool docs,pytorch/vision,2022-03-12 16:56:10,1,module: ops#module: documentation,5600,1167339795,"### 🐛 Describe the bug

https://pytorch.org/vision/stable/generated/torchvision.ops.RoIPool.html#torchvision.ops.RoIPool  (navigated to this page from https://pytorch.org/vision/stable/ops.html)

Seems that parent class information got copied:
![image](https://user-images.githubusercontent.com/1041752/158027163-66a87863-8744-4cec-acc2-fb652dd1e4ba.png)

And also the docs miss any RoIPool-related info or links

### Versions

N/A"
DeformConv2d with zero offsets deviates from Conv2d,pytorch/vision,2022-03-11 10:04:20,0,,5594,1166227275,"### 🐛 Describe the bug

Hi, 

when using torchvision.ops.DeformConv2d with an offset of all zeros, the result derivates from the result that I get from torch.nn.Conv2d. Since the offsets are all zeros, I would expect the outputs to be equal. When having only a few input and output channels the difference is quite small (but still unexpected) but for a larger network the difference can get quite significant.

Here is a minimal example to reproduce the issue:

```python
import torch
import torchvision

torch.backends.cudnn.deterministic = True

C_in, H, W = 1, 3, 3 # size of input
C_out = 1 # number of out channels
device = 'cpu'
torch.manual_seed(1)

num_tries = 100
num_correct = 0

for i in range(num_tries):
    img = torch.rand((1,C_in,H, W)).to(device)

    weight = torch.rand((C_out,C_in,3,3)).to(device) # create weight and bias
    bias = torch.rand((C_out)).to(device)

    conv1 = torch.nn.Conv2d(C_in,C_out,kernel_size = 3, padding=0).to(device)
    conv2 = torchvision.ops.DeformConv2d(C_in, C_out, kernel_size = 3, padding = 0).to(device)

    conv1.weight = torch.nn.parameter.Parameter(weight)
    conv1.bias = torch.nn.parameter.Parameter(bias)

    conv2.weight = torch.nn.parameter.Parameter(weight)
    conv2.bias = torch.nn.parameter.Parameter(bias)
    offsets = torch.zeros((1,18,1,1)).to(device) # H_out, W_out = 1 --> shape is 1,18,1,1

    out1 = conv1(img)
    out2 = conv2(img,offsets)
    
    num_correct += (out1 == out2).all().float()
    
print('Out of %d runs, only %d produced identical results.' % (num_tries, num_correct))
```

Output:
```python
Out of 100 runs, only 56 produced identical results.
```

### Versions

Collecting environment information...
PyTorch version: 1.10.1
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.10

Python version: 3.7.9 (default, Aug 31 2020, 12:42:55)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-4.15.0-135-generic-x86_64-with-debian-buster-sid
Is CUDA available: True
CUDA runtime version: 10.2.89
GPU models and configuration: GPU 0: GeForce RTX 3090
Nvidia driver version: 460.39
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] torch==1.10.1
[pip3] torchfile==0.1.0
[pip3] torchvision==0.11.2
[conda] blas                      1.0                         mkl    anaconda
[conda] captum                    0.3.1                         0    pytorch
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2019.4                      243    anaconda
[conda] mkl-service               2.3.0            py37he904b0f_0    anaconda
[conda] mkl_fft                   1.2.0            py37h23d657b_0    anaconda
[conda] mkl_random                1.0.4            py37hd81dba3_0  
[conda] numpy                     1.19.1           py37hbc911f0_0    anaconda
[conda] numpy-base                1.19.1           py37hfa32c7d_0    anaconda
[conda] pytorch                   1.10.1          py3.7_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchfile                 0.1.0                      py_0    conda-forge
[conda] torchvision               0.11.2               py37_cu113    pytorch"
Extra dot in 0.12 docs,pytorch/vision,2022-03-10 09:06:58,7,,5582,1164947448,"Looks like something similar to the previous release happened https://github.com/pytorch/vision/issues/4754

The newly pushed docs are https://pytorch.org/vision/0.12./ instead of https://pytorch.org/vision/0.12 (extra dot at the end), and the version tag shows a lot of extra info:

![image](https://user-images.githubusercontent.com/1190450/157628238-5e10b9ce-3e7f-4364-8822-ac838023bbab.png)


@mattip  is this something you could help us with, like you did in https://github.com/pytorch/vision/pull/4755 ?"
Seeing maskrcnn_resnet50_fpn + FP32 performance drop by 15% after #5443,pytorch/vision,2022-03-10 02:00:03,1,,5580,1164648381,"### 🐛 Describe the bug

Seeing maskrcnn_resnet50_fpn + FP32 performance drop by 15% after #5443

We're running benchmarks for torchvision maskrcnn_resnet50_fpn + FP32 model with `pretrained = False, pretrained_backbone = False`. After the PR, we saw training throughput drops by ~15% on various GPU models (A100, V100, 3090). 

### Versions

commit: after #5443

GPU: Nvidia A100, V100, 3090
Library: cuda 11.6 + cudnn 8.3.2

cc @ptrblck @ngimel
"
Add quantization support for BackboneWithFPN ,pytorch/vision,2022-03-09 10:18:34,2,module: models.quantization,5571,1163746372,"### 🚀 The feature

Add quantization support for `BackboneWithFPN`.

### Motivation, pitch

Currently, it is possible to use `from torchvision.models.detection.backbone_utils.resnet_fpn_backbone/BackboneWithFPN` in order to produce a backbone fpn network, given some sort of network that can produce features.

While the original resnet/other base networks you want to use (ex. efficientnet), have quantized variants supported (with pretrained weights too), it would be nice to support QAT in the context of FPN networks, to speed up the detectors even more.

### Alternatives

_No response_

### Additional context

_No response_"
repr of `_Feature` attribute will print the entire underlying tensor,pytorch/vision,2022-03-07 09:48:30,6,prototype#module: features,5558,1161151653,"The repr of `_Feature` attributes also prints the underlying tensor data, which causes very long outputs:


```py
[ins] In [21]: Image(torch.rand(3, 10, 10)).draw_bounding_box
Out[21]: 
<bound method Image.draw_bounding_box of Image([[[4.0873e-01, 7.5065e-01, 3.9609e-01, 8.0974e-01, 5.4882e-01,
          2.1480e-01, 1.9220e-01, 8.9547e-02, 3.0651e-01, 4.5459e-01],
         [9.9827e-01, 4.7832e-01, 7.1335e-01, 8.0415e-02, 7.2666e-02,
          9.9300e-01, 8.6541e-01, 4.3845e-01, 8.4263e-01, 4.7697e-01],
         [3.2785e-01, 7.8933e-01, 1.5514e-01, 3.8665e-01, 8.1245e-01,
          4.2014e-01, 2.2072e-01, 7.7170e-02, 5.5109e-01, 5.3057e-02],
         [5.4669e-01, 9.1501e-02, 9.2565e-01, 2.2452e-01, 5.6280e-02,
          7.8277e-01, 4.4243e-01, 7.7856e-01, 1.0936e-01, 8.9094e-01],
         [4.8039e-01, 3.1899e-01, 2.5593e-02, 4.7847e-01, 2.2275e-01,
          1.0460e-01, 1.2189e-01, 7.3094e-01, 4.5118e-02, 3.4907e-01],
         [7.4443e-01, 9.9318e-01, 2.2914e-01, 8.3202e-02, 5.8815e-01,
          8.0911e-01, 8.1423e-01, 8.2576e-01, 3.3891e-01, 9.2449e-01],
         [7.5176e-01, 5.0756e-01, 4.1137e-01, 6.0892e-01, 5.3783e-01,
          9.1445e-01, 6.3350e-01, 2.4363e-01, 7.2003e-01, 5.5256e-01],
         [7.5895e-01, 1.7527e-01, 9.1331e-01, 7.5056e-01, 1.8107e-01,
          5.7988e-01, 4.5102e-01, 8.0804e-01, 8.2611e-01, 8.0811e-01],
         [6.6590e-01, 1.6192e-01, 5.8999e-01, 2.6620e-01, 1.6387e-01,
          9.2958e-01, 3.3420e-01, 6.7019e-01, 2.5101e-01, 8.1988e-01],
         [7.0850e-01, 5.8186e-01, 5.9809e-01, 8.1654e-01, 4.5879e-01,
          6.3905e-01, 1.6292e-02, 5.9739e-01, 8.6138e-01, 3.5606e-01]],

        [[9.5507e-01, 3.3366e-01, 5.8719e-01, 2.6078e-02, 1.6709e-01,
          7.5720e-01, 6.1315e-01, 4.0394e-02, 7.9815e-01, 3.3518e-02],
         [3.3897e-01, 7.2987e-01, 8.9848e-01, 3.2715e-01, 9.6447e-01,
          8.5132e-02, 5.0986e-01, 1.5826e-01, 3.1247e-02, 9.6661e-01],
         [8.2750e-01, 4.6704e-01, 8.5818e-01, 1.9662e-01, 8.8024e-01,
          1.6543e-01, 4.4301e-01, 4.4911e-01, 4.0136e-02, 7.1862e-01],
         [5.7486e-01, 5.3307e-01, 7.1824e-01, 4.4500e-01, 6.7797e-01,
          5.9381e-01, 8.7389e-01, 7.6719e-01, 8.4404e-01, 2.1255e-01],
         [8.2372e-01, 9.3936e-01, 4.9353e-01, 1.9709e-01, 3.6740e-01,
          7.0572e-02, 4.2863e-01, 4.8570e-01, 6.3078e-01, 8.8221e-01],
         [1.6377e-02, 7.9993e-01, 4.4288e-01, 1.8638e-01, 8.1610e-01,
          3.6124e-01, 3.5702e-01, 3.8548e-01, 1.7436e-01, 4.7504e-01],
         [5.0559e-02, 2.7376e-01, 7.4328e-01, 9.4717e-01, 5.5722e-01,
          6.2125e-01, 8.0292e-01, 4.3011e-01, 2.8253e-01, 9.3614e-02],
         [7.0182e-02, 6.0573e-02, 2.9685e-03, 4.1341e-01, 5.7604e-01,
          1.0426e-01, 8.2337e-01, 8.5063e-01, 7.6418e-01, 1.1340e-01],
         [6.9553e-01, 3.5248e-01, 1.2961e-01, 7.4044e-01, 4.8059e-01,
          4.4696e-01, 5.5423e-01, 9.4397e-01, 8.9288e-01, 9.9230e-01],
         [1.3812e-01, 5.1467e-01, 1.8532e-01, 4.9763e-01, 9.6478e-02,
          6.1383e-01, 6.7228e-01, 2.6693e-01, 4.4474e-01, 9.5765e-01]],

        [[2.3120e-03, 3.9812e-01, 9.2486e-01, 1.2589e-01, 9.9600e-01,
          8.0888e-01, 3.1951e-01, 8.4245e-04, 1.4322e-01, 1.5887e-01],
         [3.5884e-01, 2.4698e-01, 9.6083e-01, 4.9495e-02, 2.2114e-02,
          7.6026e-01, 3.3269e-01, 1.1430e-01, 3.3527e-01, 7.9470e-01],
         [3.5903e-01, 2.9208e-02, 9.7464e-02, 3.5521e-01, 8.8108e-01,
          2.2401e-01, 2.8647e-01, 2.5528e-01, 8.0556e-01, 7.5128e-01],
         [4.7297e-02, 4.2609e-01, 6.7254e-01, 7.3022e-02, 3.0666e-01,
          1.1532e-01, 5.0260e-01, 5.4788e-01, 6.9028e-01, 7.3613e-01],
         [2.5933e-01, 2.9482e-02, 2.8598e-03, 1.8094e-01, 9.7714e-02,
          1.1055e-01, 8.1342e-01, 5.4440e-01, 9.5468e-01, 5.5490e-01],
         [7.7533e-01, 8.5751e-01, 7.7613e-01, 2.7636e-01, 9.4333e-01,
          7.6025e-01, 1.2512e-01, 5.6924e-01, 2.4738e-01, 7.5842e-01],
         [6.6902e-01, 6.8566e-01, 7.4345e-01, 8.7196e-01, 6.3307e-01,
          7.3642e-01, 7.8220e-01, 9.6173e-01, 2.0066e-01, 9.6700e-01],
         [6.5669e-01, 1.5788e-01, 8.6183e-02, 3.8050e-01, 8.7424e-01,
          1.7555e-01, 4.4631e-01, 9.7290e-01, 5.0672e-02, 7.0405e-01],
         [6.2806e-01, 4.9109e-01, 2.9952e-01, 8.1287e-01, 1.6529e-01,
          9.3985e-01, 1.4229e-01, 2.2581e-01, 3.0107e-01, 9.4073e-01],
         [7.7054e-01, 5.4258e-01, 5.2548e-02, 8.9711e-01, 5.8684e-01,
          5.9772e-01, 4.9756e-01, 2.2608e-01, 5.9295e-01, 6.6310e-03]]])>

```

In contrast

```py
[ins] In [22]: torch.rand(3, 10, 10).sum
Out[22]: <function Tensor.sum>
```

cc @bjuncek"
xavier（arm64）   ,pytorch/vision,2022-03-03 07:55:02,1,topic: binaries,5519,1158084627,"### 🐛 Describe the bug

My  platform is xavier(arm64).
I want to  using sourse code to build torchvision.
I find that the current compilation is x86.
Can torchvision be compiled  on arm64?  


### Versions

My  platform is xavier(arm64).
I want to  using sourse code to build torchvision.
I find that the current compilation is x86.
Can torchvision be compiled  on arm64?  "
"[FEEDBACK] Documentation revamp: New layout, New models docs",pytorch/vision,2022-03-02 10:14:37,13,,5511,1156978640,"### 📚 Feedback Request

This issue is dedicated for collecting community feedback on the Documentation revamping efforts. Here are some highlighted changes:
- **New documentation layout:** Each function / class is now documented in a separate page, clearing up some space in the per-module pages, and easing the discovery of the proposed APIs. Compare e.g. our [previous docs](https://pytorch.org/vision/0.11/transforms.html) vs the [new ones](https://pytorch.org/vision/main/transforms.html).
- **New models docs:** With the upcoming multi-weight API, we took this opportunity to completely re-write our models documentation. We now have a [main model page](https://pytorch.org/vision/main/models.html) with various [summary tables](https://pytorch.org/vision/main/models.html#table-of-all-available-classification-weights) of available weights, and each model has a [dedicated page](https://pytorch.org/vision/main/models/resnet.html). Each model builder is also documented in their [own page](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50), with more details about the available weights. For comparison, our previous models docs are [here](https://pytorch.org/vision/0.12/models.html).
We would love to get your thoughts, comments and input on how to improve our documentation.
"
`create_feature_extractor`  V.S. `_utils.IntermediateLayerGetter`,pytorch/vision,2022-03-02 01:38:58,10,,5508,1156148489,"### 🐛 Describe the bug

When I used `resnet50` in `torchvision.models`, I replaced `BatchNorm2d` with `FrozenBatchNorm2d`. Then I needed to extract the output of `layer4`, but I found that `create_feature_extractor` does not display `FrozenBatchNorm2d`. Then I used `IntermediateLayerGetter` and it displayed it properly.
As shown below:

```python
import torch
from torchvision import models
from torchvision.models.feature_extraction import create_feature_extractor
from torchvision.models._utils import IntermediateLayerGetter

print(torch.__version__)  # 1.10.2+cu113

class FrozenBatchNorm2d(torch.nn.Module):
    """"""
    BatchNorm2d where the batch statistics and the affine parameters are fixed.

    Copy-paste from torchvision.misc.ops with added eps before rqsrt,
    without which any other models than torchvision.models.resnet[18,34,50,101]
    produce nans.
    """"""

    def __init__(self, n):
        super(FrozenBatchNorm2d, self).__init__()
        self.register_buffer(""weight"", torch.ones(n))
        self.register_buffer(""bias"", torch.zeros(n))
        self.register_buffer(""running_mean"", torch.zeros(n))
        self.register_buffer(""running_var"", torch.ones(n))

    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
                              missing_keys, unexpected_keys, error_msgs):
        num_batches_tracked_key = prefix + 'num_batches_tracked'
        if num_batches_tracked_key in state_dict:
            del state_dict[num_batches_tracked_key]

        super(FrozenBatchNorm2d, self)._load_from_state_dict(
            state_dict, prefix, local_metadata, strict,
            missing_keys, unexpected_keys, error_msgs)

    def forward(self, x):
        # move reshapes to the beginning
        # to make it fuser-friendly
        w = self.weight.reshape(1, -1, 1, 1)
        b = self.bias.reshape(1, -1, 1, 1)
        rv = self.running_var.reshape(1, -1, 1, 1)
        rm = self.running_mean.reshape(1, -1, 1, 1)
        eps = 1e-5
        scale = w * (rv + eps).rsqrt()
        bias = b - rm * scale
        return x * scale + bias

model = models.resnet50(pretrained=True, norm_layer=FrozenBatchNorm2d)
model1 = create_feature_extractor(model, return_nodes={'layer4': '0'})
model2 = IntermediateLayerGetter(model, return_layers={'layer4': '0'})

# model1.load_state_dict(model2.state_dict())  # error !

print(model1)
print(model2)
```

The partial output of `create_feature_extractor` is as follows:

```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Module(
    (0): Module(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (downsample): Module(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
```

The partial output of `IntermediateLayerGetter` is as follows

```
(layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): FrozenBatchNorm2d()
      )
    )
```

So, why does `create_feature_extractor` not work as expected. And, I wonder if there is a built-in function with the same functionality as `FrozenBatchNorm2d` above.

### Versions

PyTorch version: 1.10.2+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 专业版
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19041-SP0
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.22.1
[pip3] numpydoc==1.1.0
[pip3] torch==1.10.2+cu113
[pip3] torch-tb-profiler==0.2.0
[pip3] torchaudio==0.10.2+cu113
[pip3] torchfile==0.1.0
[pip3] torchsummary==1.5.1
[pip3] torchvision==0.11.3+cu113
[pip3] torchviz==0.0.2
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               11.1.1               heb2d755_9    conda-forge
[conda] mkl                       2020.2                      256
[conda] mkl-service               2.3.0            py38hb782905_0
[conda] mkl_fft                   1.2.0            py38h45dec08_0
[conda] mkl_random                1.1.1            py38h47e9c7a_0
[conda] mypy-extensions           0.4.3                    pypi_0    pypi
[conda] numpy                     1.22.1                   pypi_0    pypi
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1
[conda] torch                     1.10.2+cu113             pypi_0    pypi
[conda] torch-tb-profiler         0.2.0                    pypi_0    pypi
[conda] torchaudio                0.10.2+cu113             pypi_0    pypi
[conda] torchfile                 0.1.0                    pypi_0    pypi
[conda] torchsummary              1.5.1                    pypi_0    pypi
[conda] torchvision               0.11.3+cu113             pypi_0    pypi
[conda] torchviz                  0.0.2                    pypi_0    pypi"
Cannot find libQt5Core.so.5 during build,pytorch/vision,2022-02-27 09:15:21,0,,5490,1153093293,"### 🐛 Describe the bug

Error log:

```
Processing /tmp/scratch/vision
  Running command python setup.py egg_info
  Traceback (most recent call last):
    File ""<string>"", line 2, in <module>
    File ""<pip-setuptools-caller>"", line 34, in <module>
    File ""/tmp/scratch/vision/setup.py"", line 9, in <module>
      import torch
    File ""/usr/local/lib/python3.9/dist-packages/torch/__init__.py"", line 199, in <module>
      from torch._C import *  # noqa: F403
  ImportError: libQt5Core.so.5: cannot open shared object file: No such file or directory
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /usr/bin/python3 -c '
  exec(compile('""'""''""'""''""'""'
  # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py
  #
  # - It imports setuptools before invoking setup.py, to enable projects that directly
  #   import from `distutils.core` to work with newer packaging standards.
  # - It provides a clear error message when setuptools is not installed.
  # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so
  #   setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:
  #     manifest_maker: standard file '""'""'-c'""'""' not found"".
  # - It generates a shim setup.py, for handling setup.cfg-only projects.
  import os, sys, tokenize
  
  try:
      import setuptools
  except ImportError as error:
      print(
          ""ERROR: Can not execute `setup.py` since setuptools is not available in ""
          ""the build environment."",
          file=sys.stderr,
      )
      sys.exit(1)
  
  __file__ = %r
  sys.argv[0] = __file__
  
  if os.path.exists(__file__):
      filename = __file__
      with tokenize.open(__file__) as f:
          setup_py_code = f.read()
  else:
      filename = ""<auto-generated setuptools caller>""
      setup_py_code = ""from setuptools import setup; setup()""
  
  exec(compile(setup_py_code, filename, ""exec""))
  '""'""''""'""''""'""' % ('""'""'/tmp/scratch/vision/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /tmp/pip-pip-egg-info-snwhrsxy
  cwd: /tmp/scratch/vision/
  Preparing metadata (setup.py) ... error
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
```

Based on my understanding, this is due to a special combination of environment.
We build on CentOS 7 host but inside a Debian 11 docker.
Debian11 ships Qt 5.15.2, while CentOS 7 uses 3.10 kernel.
From a certain version of Qt5, it starts to use a new kernel feature (renameat2) only available a few versions later.
So even the .so is there, it'll be ignored on 3.10 kernel.

Ubuntu 18.04 has Qt 5.9 and builds just fine, same docker can build on Debian 11 host as well.

Because docker can be built by one host and run elsewhere, it does not have to fail, that's why I'm filing this as bug.

Running `uic` (a simple app from Qt5) and `ldconfig` can demonstrate this lib-not-found behavior.
```
root@30f6f97c32b0:/etc/roaster/scripts# uic
/usr/lib/qt5/bin/uic: error while loading shared libraries: libQt5Core.so.5: cannot open shared object file: No such file or directory
root@30f6f97c32b0:/etc/roaster/scripts# ldconfig -p | grep -i libQt5Core
        libQt5Core.so.5 (libc6,x86-64, OS ABI: Linux 3.17.0) => /usr/lib/x86_64-linux-gnu/libQt5Core.so.5
        libQt5Core.so (libc6,x86-64, OS ABI: Linux 3.17.0) => /usr/lib/x86_64-linux-gnu/libQt5Core.so
```

Here's some related discussion:
- https://bbs.archlinux.org/viewtopic.php?id=232682

### Versions

Current master"
Single Channel GaussianBlur over 23x23 kernels fails on Windows,pytorch/vision,2022-02-24 07:10:30,3,bug#windows#module: transforms#cuda,5464,1148939021,"### 🐛 Describe the bug

See https://app.circleci.com/pipelines/github/pytorch/vision/14951/workflows/dd6ed737-f31d-4baa-8f42-9e69d55650aa/jobs/1205316

For some reason the same code works with CUDA-11.1, but fails with 11.3, which makes me highly suspicious of bug in cuDNN side

<details>
<summary>cuda-memcheck reports invalid memory access in `cudnnConvolutionForward ` call</summary>

```
(C:\Users\circleci\project\env) C:\Users\circleci\project\test>""c:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin\cuda-memcheck.exe"" pytest test_transforms_tensor.py -k test_gaussian_blur[1-meth_kwargs1
========= CUDA-MEMCHECK
================================================================================== test session starts ===================================================================================
platform win32 -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0
rootdir: C:\Users\circleci\project, configfile: pytest.ini
plugins: cov-3.0.0, mock-3.6.1
collected 3066 items / 3064 deselected / 2 selected

test_transforms_tensor.py .FE                                                                                                                                                       [100%]

========================================================================================= ERRORS =========================================================================================
______________________________________________________________ ERROR at teardown of test_gaussian_blur[1-meth_kwargs1-cuda] ______________________________________________________________
Traceback (most recent call last):
  File ""C:\Users\circleci\project\test\conftest.py"", line 104, in prevent_leaking_rng
    torch.cuda.set_rng_state(cuda_rng_state)
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\cuda\random.py"", line 64, in set_rng_state
    _lazy_call(cb)
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\cuda\__init__.py"", line 155, in _lazy_call
    callable()
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\cuda\random.py"", line 62, in cb
    default_generator.set_state(new_state_copy)
RuntimeError: CUDA error: unspecified launch failure
======================================================================================== FAILURES ========================================================================================
________________________________________________________________________ test_gaussian_blur[1-meth_kwargs1-cuda] _________________________________________________________________________
Traceback (most recent call last):
  File ""C:\Users\circleci\project\test\test_transforms_tensor.py"", line 963, in test_gaussian_blur
    _test_class_op(
  File ""C:\Users\circleci\project\test\test_transforms_tensor.py"", line 85, in _test_class_op
    _test_transform_vs_scripted_on_batch(f, scripted_fn, batch_tensors)
  File ""C:\Users\circleci\project\test\test_transforms_tensor.py"", line 36, in _test_transform_vs_scripted_on_batch
    transformed_batch = transform(batch_tensors)
  File ""C:\Users\circleci\project\env\lib\site-packages\torch\nn\modules\module.py"", line 1111, in _call_impl
    return forward_call(*input, **kwargs)
  File ""c:\users\circleci\project\torchvision\transforms\transforms.py"", line 1817, in forward
    return F.gaussian_blur(img, self.kernel_size, [sigma, sigma])
  File ""c:\users\circleci\project\torchvision\transforms\functional.py"", line 1326, in gaussian_blur
    output = F_t.gaussian_blur(t_img, kernel_size, sigma)
  File ""c:\users\circleci\project\torchvision\transforms\functional_tensor.py"", line 774, in gaussian_blur
    img = conv2d(img, kernel, groups=img.shape[-3])
RuntimeError: CUDA error: unspecified launch failure
================================================================================ short test summary info =================================================================================
ERROR test_transforms_tensor.py::test_gaussian_blur[1-meth_kwargs1-cuda] - RuntimeError: CUDA error: unspecified launch failure
FAILED test_transforms_tensor.py::test_gaussian_blur[1-meth_kwargs1-cuda] - RuntimeError: CUDA error: unspecified launch failure
================================================================= 1 failed, 1 passed, 3064 deselected, 1 error in 35.57s =================================================================
========= Invalid __shared__ read of size 4
=========     at 0x00001d10 in volta_scudnn_128x32_3dconv_fprop_xregs_large_nn_v1
=========     by thread (95,0,0) in block (24,0,0)
=========     Address 0x0000250c is out of bounds
=========     Device Frame:volta_scudnn_128x32_3dconv_fprop_xregs_large_nn_v1 (volta_scudnn_128x32_3dconv_fprop_xregs_large_nn_v1 : 0x1d10)
=========     Saved host backtrace up to driver entry point at kernel launch time
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x76888]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x76bb1]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x7b0da]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll (cuProfilerStop + 0x11cc6a) [0x33d9ea]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x17069d]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll (cuProfilerStop + 0xf0c72) [0x3119f2]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x38bdb]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x390af]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll [0x39394]
=========     Host Frame:C:\Windows\system32\DriverStore\FileRepository\nv_dispswi.inf_amd64_8fb2f986cb3224d8\nvcuda64.dll (cuLaunchKernel + 0x234) [0x20fc44]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll [0x3896]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll [0x26fd]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cask_cudnn::getPlatform + 0xe9) [0x1d54529]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cask_cudnn::transformTensor + 0x1bc1) [0x1dc0651]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cask_cudnn::transformTensor + 0xbe6a) [0x1dca8fa]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cask_cudnn::ConvDgradShader::isSplitK + 0x49b) [0x1ddcd9b]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::backend::Descriptor::initialize_internal + 0x618e) [0x5c67ce]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::backend::Descriptor::initialize_internal + 0x6eb1) [0x5c74f1]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::cnn::EngineInterface::execute + 0x7e) [0x4e163e]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::cnn::EngineContainer<1012,113664>::execute_internal_impl + 0x2a) [0x54f27a]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::cnn::EngineInterface::execute + 0x7e) [0x4e163e]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cask_cudnn::TensorDesc::operator== + 0x2d2) [0x544612]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::cnn::EngineContainer<1,4096>::execute_internal_impl + 0xd241) [0x55c4d1]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::cnn::EngineInterface::execute + 0x7e) [0x4e163e]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::backend::execute + 0x103f) [0x54eebf]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::backend::Tensor::Tensor + 0x18b6) [0x5ab246]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::backend::Tensor::Tensor + 0xbe1) [0x5aa571]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnn::cnn::convolutionForward + 0x10b) [0x65609b]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll (cudnnConvolutionForward + 0x331) [0x657081]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution_transpose + 0x4263) [0x48863]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution_transpose + 0x83a7) [0x4c9a7]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution_transpose + 0x7736) [0x4bd36]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution_transpose + 0x1ae5) [0x460e5]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution_transpose + 0x752f) [0x4bb2f]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution_add_relu + 0x16ec) [0x43fec]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cpp.dll (at::native::cudnn_convolution + 0xc5) [0x428e5]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cu.dll (at::cuda::view_as_real + 0x14adc) [0x456680c]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cuda_cu.dll (at::cuda::bucketize_outf + 0x3df7a) [0x450361a]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::_ops::cudnn_convolution::call + 0x242) [0x70175b2]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::native::_convolution + 0xf5e) [0x692064e]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::compositeexplicitautograd::xlogy_ + 0x40e) [0x72d1bee]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::compositeexplicitautograd::bmm + 0x1a1ed) [0x72975bd]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::_ops::_convolution::call + 0x2d6) [0x6d5a226]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::native::convolution + 0x164) [0x6928914]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::compositeexplicitautograd::xlogy_ + 0xc6b) [0x72d244b]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::compositeexplicitautograd::bmm + 0x1a2ca) [0x729769a]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::TensorMaker::make_tensor + 0x88e49) [0x6d40779]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::_ops::convolution::redispatch + 0x123) [0x6dc39a3]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (torch::autograd::GraphRoot::apply + 0x157b1) [0x7bfd851]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (torch::autograd::GraphRoot::apply + 0xc6c8) [0x7bf4768]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::_ops::convolution::call + 0x26f) [0x6d71b6f]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::native::conv2d + 0x1be) [0x69277be]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::compositeimplicitautograd::where + 0x1db4) [0x73ac8e4]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::compositeimplicitautograd::broadcast_to + 0x2a7a3) [0x738d953]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::_ops::conv2d::call + 0x219) [0x70ba239]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_cpu.dll (at::conv2d + 0x64) [0x67106d4]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_python.dll (torch::FunctionSignature::operator= + 0x1096fc) [0x14d77c]
=========     Host Frame:C:\Users\circleci\project\env\lib\site-packages\torch\lib\torch_python.dll (torch::FunctionSignature::operator= + 0x12f7ab) [0x17382b]
=========     Host Frame:C:\Users\circleci\project\env\python38.dll (PyMethodDef_RawFastCallKeywords + 0x410) [0x126fe0]
=========     Host Frame:C:\Users\circleci\project\env\python38.dll (PyObject_MakeTpCall + 0x106) [0x125fa6]
=========     Host Frame:C:\Users\circleci\project\env\python38.dll (PyEval_GetFuncDesc + 0x408) [0x2036b8]
=========
...
```
</details>


cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @mszhanyi @vfdev-5 @datumbox"
Move torchvision.utils.save_image -> torchvision.io.save_image (or introduce this function from scratch),pytorch/vision,2022-02-23 09:25:14,8,enhancement#module: io#module: utils,5461,1147847875,"### 🚀 The feature

I think the `io` namespace makes more sense, and it's where `torchvision.io.read_image` is located...

### Motivation, pitch

N/A

### Alternatives

_No response_

### Additional context

_No response_"
fatal error: ATen/autocast_mode.h: No such file or directory compilation terminated.,pytorch/vision,2022-02-21 09:34:10,2,awaiting response,5449,1145544106,"### 🐛 Describe the bug

error information:

Building wheel torchvision-0.13.0a0+7bb5e41
PNG found: True
libpng version: 1.6.37
Building torchvision with PNG image support
libpng include path: /home/chy/anaconda3/envs/maskrcnn_benchmark/include/libpng16
Running build on conda-build: False
Running build on conda: True
JPEG found: True
Building torchvision with JPEG image support
NVJPEG found: False
FFmpeg found: True
ffmpeg include path: ['/home/chy/anaconda3/envs/maskrcnn_benchmark/include', '/home/chy/anaconda3/envs/maskrcnn_benchmark/include/x86_               64-linux-gnu']
ffmpeg library_dir: ['/home/chy/anaconda3/envs/maskrcnn_benchmark/lib', '/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/x86_64-linux-               gnu']
video codec found: False
The installed version of ffmpeg is missing the header file 'bsf.h' which is required for GPU video decoding. Please install the latest                ffmpeg from conda-forge channel: `conda install -c conda-forge ffmpeg`.
running install
running bdist_egg
running egg_info
writing torchvision.egg-info/PKG-INFO
writing dependency_links to torchvision.egg-info/dependency_links.txt
writing requirements to torchvision.egg-info/requires.txt
writing top-level names to torchvision.egg-info/top_level.txt
reading manifest file 'torchvision.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no previously-included files matching '__pycache__' found under directory '*'
warning: no previously-included files matching '*.py[co]' found under directory '*'
writing manifest file 'torchvision.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
copying torchvision/version.py -> build/lib.linux-x86_64-3.6/torchvision
running build_ext
building 'torchvision._C' extension
gcc -pthread -B /home/chy/anaconda3/envs/maskrcnn_benchmark/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wa               ll -Wstrict-prototypes -fPIC -DWITH_CUDA -I/home/chy/github/vision/torchvision/csrc -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/               python3.6/site-packages/torch/include -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/python3.6/site-packages/torch/include/torch/cs               rc/api/include -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/python3.6/site-packages/torch/include/TH -I/home/chy/anaconda3/envs/m               askrcnn_benchmark/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda-9.0.176/include -I/home/chy/anaconda3/envs/maskrcnn_               benchmark/include/python3.6m -c /home/chy/github/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp -o build/temp.linux-x86               _64-3.6/home/chy/github/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_               NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=image -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
<command-line>:0:0: warning: ""TORCH_EXTENSION_NAME"" redefined
<command-line>:0:0: note: this is the location of the previous definition
/home/chy/github/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp:3:32: fatal error: ATen/autocast_mode.h: No such file o               r directory
compilation terminated.
error: command 'gcc' failed with exit status 1
(maskrcnn_benchmark) chy@guest-PR4908P:~/github/vision$ python setup.py build
Building wheel torchvision-0.13.0a0+7bb5e41
PNG found: True
libpng version: 1.6.37
Building torchvision with PNG image support
libpng include path: /home/chy/anaconda3/envs/maskrcnn_benchmark/include/libpng16
Running build on conda-build: False
Running build on conda: True
JPEG found: True
Building torchvision with JPEG image support
NVJPEG found: False
FFmpeg found: True
ffmpeg include path: ['/home/chy/anaconda3/envs/maskrcnn_benchmark/include', '/home/chy/anaconda3/envs/maskrcnn_benchmark/include/x86_64-linux-gnu']
ffmpeg library_dir: ['/home/chy/anaconda3/envs/maskrcnn_benchmark/lib', '/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/x86_64-linux-gnu']
video codec found: False
The installed version of ffmpeg is missing the header file 'bsf.h' which is required for GPU video decoding. Please install the latest ffmpeg from conda-forge channel: `conda install -c conda-forge ffmpeg`.
running build
running build_py
copying torchvision/version.py -> build/lib.linux-x86_64-3.6/torchvision
running build_ext
building 'torchvision._C' extension
gcc -pthread -B /home/chy/anaconda3/envs/maskrcnn_benchmark/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA -I/home/chy/github/vision/torchvision/csrc -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/python3.6/site-packages/torch/include -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/python3.6/site-packages/torch/include/TH -I/home/chy/anaconda3/envs/maskrcnn_benchmark/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda-9.0.176/include -I/home/chy/anaconda3/envs/maskrcnn_benchmark/include/python3.6m -c /home/chy/github/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp -o build/temp.linux-x86_64-3.6/home/chy/github/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=image -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
<command-line>:0:0: warning: ""TORCH_EXTENSION_NAME"" redefined
<command-line>:0:0: note: this is the location of the previous definition
/home/chy/github/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp:3:32: fatal error: ATen/autocast_mode.h: No such file or directory
compilation terminated.


### Versions

cuda 9.0
pytorch 1.1.0
Linux Ubuntu 16.04
install torchvision 0.3.0"
Torch Script C++ : Mask_RCNN module.forward() operation works with CPU but Fails on Cuda in Windows,pytorch/vision,2022-02-18 10:43:18,25,,5439,1142817509,"### 🐛 Describe the bug

Hi,

I am trying to do inference on Mask_RCNN model in Torch Script C++. model.forward() works correctly for CPU but fails after moving both model and input to gpu. CAn you please advise what can be the issue?

Below is the code I used for CPU

Python code to create model script file 'maskrcnn_ts.pt'
```
import torch
import torchvision
import torchvision.models.detection
import torchvision.models.detection.mask_rcnn
from PIL import Image, ImageDraw
from torchvision import transforms

kwargs = {""trainable_backbone_layers"": None}
checkpoint_name = ""checkpoint.pth""

model = torchvision.models.detection.__dict__[""maskrcnn_resnet50_fpn""](pretrained=False, num_classes=5, **kwargs)
model.load_state_dict(torch.load(checkpoint_name)['model'])
model.eval()
convert_tensor = transforms.ToTensor()
org_image = Image.open(""test.jpg"")
image = convert_tensor(org_image) 
print(""Image size : "", image.size())
model.to(""cpu"")
image= image.to(""cpu"")
images = [image]

script_model = torch.jit.script(model)
torch.jit.save(script_model, 'maskrcnn_ts.pt')

```
C++ Code
```
#include <torch/torch.h>
#include <torch/script.h>
#include <iostream>
#include <memory>
#include <torchvision/vision.h>

int main(int argc, const char* argv[]) {

std::string model_pt_name = ""maskrcnn_ts.pt"";     
torch::jit::script::Module module = torch::jit::load(model_pt_name);

torch::Tensor sample1 = torch::rand({ 3, 875, 606 });
c10::List<torch::Tensor> sampleList = c10::List<torch::Tensor>({ sample1 });
std::vector<torch::jit::IValue> inputs;
inputs.push_back(sampleList);

torch::jit::IValue output = module.forward(inputs);

std::cout << ""output :: "" << output << ""\n"";
}

```
output::
```
[W ..\..\aten\src\ATen\native\TensorShape.cpp:2157] Warning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (function operator ())
[W mask_rcnn.py:107] Warning: RCNN always returns a (Losses, Detections) tuple in scripting (function )
output :: ({}, [{boxes: [ CPUFloatType{0,4} ], labels: [ CPULongType{0} ], scores: [ CPUFloatType{0} ], masks: [  CPUFloatType{0,1,875,606} ]}])
```
The above code for CPU works fine. 

But when I do the same steps for CUDA, it fails in  ""torch::jit::IValue output = module.forward(inputs); "" step without any error message.
Code for GPU is given below : 

```
import torch
import torchvision
import torchvision.models.detection
import torchvision.models.detection.mask_rcnn
from PIL import Image, ImageDraw
from torchvision import transforms

kwargs = {""trainable_backbone_layers"": None}
checkpoint_name = ""checkpoint.pth""

model = torchvision.models.detection.__dict__[""maskrcnn_resnet50_fpn""](pretrained=False, num_classes=5, **kwargs)
model.load_state_dict(torch.load(checkpoint_name)['model'])
model.eval()
convert_tensor = transforms.ToTensor()
org_image = Image.open(""test.jpg"")
image = convert_tensor(org_image) 
print(""Image size : "", image.size())
model.to(""cuda"")
image= image.to(""cuda"")
images = [image]

script_model = torch.jit.script(model)
torch.jit.save(script_model, 'maskrcnn_ts.pt')

```
C++ Code
```
#include <torch/torch.h>
#include <torch/script.h>
#include <iostream>
#include <memory>
#include <torchvision/vision.h>

int main(int argc, const char* argv[]) {

std::string model_pt_name = ""maskrcnn_ts.pt"";     

//##Moving both Model and Input Tensor to GPU
torch::jit::script::Module module = torch::jit::load(model_pt_name, torch::kCUDA);
torch::Tensor sample1 = torch::rand({ 3, 875, 606 }, torch::kCUDA);

c10::List<torch::Tensor> sampleList = c10::List<torch::Tensor>({ sample1 });
std::vector<torch::jit::IValue> inputs;
inputs.push_back(sampleList);

torch::jit::IValue output = module.forward(inputs);

std::cout << ""output :: "" << output << ""\n"";
}

```

I created the tourchvision c++ as per steps in https://discuss.pytorch.org/t/building-torchvision-c-api-from-source/68281/2 with visual studio 2019. enabling CUDA by changing ""option(WITH_CUDA ""Enable CUDA support"" ON)"" in CMakeLists.txt.

Can you please advise what can be the issue here. Thanks.

### Versions

Collecting environment information...
PyTorch version: 1.10.2+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Pro
GCC version: (MinGW.org GCC-6.3.0-1) 6.3.0
Clang version: Could not collect
CMake version: version 3.19.6
Libc version: N/A

Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19044-SP0
Is CUDA available: True
CUDA runtime version: 11.3.58
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti
Nvidia driver version: 511.79
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.3\bin\cudnn_ops_train64_8.dll
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.22.2
[pip3] pytorch-lightning==1.5.0
[pip3] torch==1.10.2+cu113
[pip3] torch-tensorrt==0.0.0
[pip3] torchaudio==0.10.2+cu113
[pip3] torchmetrics==0.6.0
[pip3] torchvision==0.11.3+cu113
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.2.89              h74a9793_1
[conda] mkl                       2021.4.0           haa95532_640
[conda] mkl-include               2021.4.0           haa95532_640
[conda] mkl-service               2.4.0            py39h2bbff1b_0
[conda] mkl_fft                   1.3.1            py39h277e83a_0
[conda] mkl_random                1.2.2            py39hf11a4ad_0
[conda] mypy-extensions           0.4.3                    pypi_0    pypi
[conda] numpy                     1.22.2                   pypi_0    pypi
[conda] pytorch-lightning         1.5.0                    pypi_0    pypi
[conda] torch                     1.10.2+cu113             pypi_0    pypi
[conda] torch-tensorrt            0.0.0                    pypi_0    pypi
[conda] torchaudio                0.10.2+cu113             pypi_0    pypi
[conda] torchmetrics              0.6.0                    pypi_0    pypi
[conda] torchvision               0.11.3+cu113             pypi_0    pypi"
Error building torchvision from source,pytorch/vision,2022-02-16 13:30:31,14,,5431,1140025780,"### 🐛 Describe the bug

Hello, I am trying to build custom docker image with torch & torchvision built from sources and getting this error:

```
 RUN git clone --depth 1 -b main https://github.com/pytorch/vision.git &&    cd vision &&     FORCE_CUDA=1 DEBUG=1 pip install -v . &&    cd .. && rm -rf vision
 ---> Running in a3547b30a89f
Cloning into 'vision'...
Using pip 22.0.3 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)
Processing /workdir/vision
  Installing build dependencies: started
  Running command pip subprocess to install build dependencies
  Collecting setuptools>=40.8.0
    Downloading setuptools-60.9.1-py3-none-any.whl (1.1 MB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 2.5 MB/s eta 0:00:00
  Collecting wheel
    Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)
  Installing collected packages: wheel, setuptools
  Successfully installed setuptools-60.9.1 wheel-0.37.1
  WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Running command Getting requirements to build wheel
  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
  Building wheel torchvision-0.13.0a0+8bf46d4
  Compile in debug mode
  PNG found: False
  Running build on conda-build: False
  Running build on conda: False
  JPEG found: False
  NVJPEG found: True
  Building torchvision with NVJPEG image support
  FFmpeg found: True
  ffmpeg include path: ['/usr/local/include', '/usr/local/include/x86_64-linux-gnu']
  ffmpeg library_dir: ['/usr/local/lib', '/usr/local/lib/x86_64-linux-gnu']
  video codec found: False
  The installed version of ffmpeg is missing the header file 'bsf.h' which is required for GPU video decoding. Please install the latest ffmpeg from conda-forge channel: `conda install -c conda-forge ffmpeg`.
  running egg_info
  creating torchvision.egg-info
  writing manifest file 'torchvision.egg-info/SOURCES.txt'
  warning: no previously-included files matching '__pycache__' found under directory '*'
  warning: no previously-included files matching '*.py[co]' found under directory '*'
  writing manifest file 'torchvision.egg-info/SOURCES.txt'
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Running command Preparing metadata (pyproject.toml)
  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
  Building wheel torchvision-0.13.0a0+8bf46d4
  Compile in debug mode
  PNG found: False
  Running build on conda-build: False
  Running build on conda: False
  JPEG found: False
  NVJPEG found: True
  Building torchvision with NVJPEG image support
  FFmpeg found: True
  ffmpeg include path: ['/usr/local/include', '/usr/local/include/x86_64-linux-gnu']
  ffmpeg library_dir: ['/usr/local/lib', '/usr/local/lib/x86_64-linux-gnu']
  video codec found: False
  The installed version of ffmpeg is missing the header file 'bsf.h' which is required for GPU video decoding. Please install the latest ffmpeg from conda-forge channel: `conda install -c conda-forge ffmpeg`.
  running dist_info
  creating /tmp/pip-modern-metadata-q7d2np88/torchvision.egg-info
  writing manifest file '/tmp/pip-modern-metadata-q7d2np88/torchvision.egg-info/SOURCES.txt'
  warning: no previously-included files matching '__pycache__' found under directory '*'
  warning: no previously-included files matching '*.py[co]' found under directory '*'
  writing manifest file '/tmp/pip-modern-metadata-q7d2np88/torchvision.egg-info/SOURCES.txt'
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.0a0+8bf46d4) (4.0.1)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.0a0+8bf46d4) (9.0.1)
Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.0a0+8bf46d4) (2.27.1)
Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.0a0+8bf46d4) (1.21.5)
Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.0a0+8bf46d4) (1.12.0a0+gite421492)
Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.0a0+8bf46d4) (2019.11.28)
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.0a0+8bf46d4) (2.8)
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.0a0+8bf46d4) (2.0.11)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.0a0+8bf46d4) (1.25.8)
Building wheels for collected packages: torchvision
  Building wheel for torchvision (pyproject.toml): started
  Running command Building wheel for torchvision (pyproject.toml)
  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'
  Building wheel torchvision-0.13.0a0+8bf46d4
  Compile in debug mode
  PNG found: False
  Running build on conda-build: False
  Running build on conda: False
  JPEG found: False
  NVJPEG found: True
  Building torchvision with NVJPEG image support
  FFmpeg found: True
  ffmpeg include path: ['/usr/local/include', '/usr/local/include/x86_64-linux-gnu']
  ffmpeg library_dir: ['/usr/local/lib', '/usr/local/lib/x86_64-linux-gnu']
  video codec found: False
  The installed version of ffmpeg is missing the header file 'bsf.h' which is required for GPU video decoding. Please install the latest ffmpeg from conda-forge channel: `conda install -c conda-forge ffmpeg`.
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.8
  creating build/lib.linux-x86_64-3.8/torchvision
  copying torchvision/_internally_replaced_utils.py -> build/lib.linux-x86_64-3.8/torchvision
  copying torchvision/version.py -> build/lib.linux-x86_64-3.8/torchvision
  copying torchvision/extension.py -> build/lib.linux-x86_64-3.8/torchvision
  copying torchvision/__init__.py -> build/lib.linux-x86_64-3.8/torchvision
  copying torchvision/utils.py -> build/lib.linux-x86_64-3.8/torchvision
  creating build/lib.linux-x86_64-3.8/torchvision/prototype
  copying torchvision/prototype/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype
  creating build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/autoaugment.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/_functional_video.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/functional_pil.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/transforms.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/functional.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/_transforms_video.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  copying torchvision/transforms/functional_tensor.py -> build/lib.linux-x86_64-3.8/torchvision/transforms
  creating build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/dtd.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/country211.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/svhn.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/vision.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/coco.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/food101.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/cityscapes.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/lsun.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/flickr.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/fgvc_aircraft.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/fer2013.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/stanford_cars.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/kinetics.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/hmdb51.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/phototour.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/gtsrb.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/video_utils.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/imagenet.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/sun397.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/ucf101.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/sbu.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/places365.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/flowers102.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/sbd.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/stl10.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/oxford_iiit_pet.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/cifar.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/_optical_flow.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/lfw.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/eurosat.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/omniglot.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/pcam.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/kitti.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/folder.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/inaturalist.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/mnist.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/semeion.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/rendered_sst2.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/voc.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/clevr.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/utils.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/widerface.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/caltech.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/fakedata.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/celeba.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  copying torchvision/datasets/usps.py -> build/lib.linux-x86_64-3.8/torchvision/datasets
  creating build/lib.linux-x86_64-3.8/torchvision/io
  copying torchvision/io/image.py -> build/lib.linux-x86_64-3.8/torchvision/io
  copying torchvision/io/_video_opt.py -> build/lib.linux-x86_64-3.8/torchvision/io
  copying torchvision/io/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/io
  copying torchvision/io/video.py -> build/lib.linux-x86_64-3.8/torchvision/io
  copying torchvision/io/_load_gpu_decoder.py -> build/lib.linux-x86_64-3.8/torchvision/io
  creating build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/boxes.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/_register_onnx_ops.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/ps_roi_pool.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/_box_convert.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/stochastic_depth.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/deform_conv.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/focal_loss.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/misc.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/giou_loss.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/roi_pool.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/ps_roi_align.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/feature_pyramid_network.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/roi_align.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/poolers.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  copying torchvision/ops/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/ops
  creating build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/feature_extraction.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/squeezenet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/googlenet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/mobilenet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/densenet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/mnasnet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/mobilenetv3.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/alexnet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/vgg.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/mobilenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/shufflenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/resnet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/vision_transformer.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/convnext.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/inception.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/regnet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/efficientnet.py -> build/lib.linux-x86_64-3.8/torchvision/models
  copying torchvision/models/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/models
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/transforms
  copying torchvision/prototype/transforms/_presets.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms
  copying torchvision/prototype/transforms/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  copying torchvision/prototype/datasets/generate_category_files.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  copying torchvision/prototype/datasets/benchmark.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  copying torchvision/prototype/datasets/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  copying torchvision/prototype/datasets/_folder.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  copying torchvision/prototype/datasets/_home.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  copying torchvision/prototype/datasets/_api.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/_feature.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/_encoded.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/_bounding_box.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/_image.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/_segmentation_mask.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  copying torchvision/prototype/features/_label.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/features
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/utils
  copying torchvision/prototype/utils/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/utils
  copying torchvision/prototype/utils/_internal.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/utils
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/squeezenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/googlenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/mobilenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/densenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/mnasnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/mobilenetv3.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/_meta.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/alexnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/vgg.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/mobilenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/shufflenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/resnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/vision_transformer.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/convnext.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/_api.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/inception.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/regnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/efficientnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  copying torchvision/prototype/models/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  copying torchvision/prototype/transforms/functional/_augment.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  copying torchvision/prototype/transforms/functional/_geometry.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  copying torchvision/prototype/transforms/functional/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  copying torchvision/prototype/transforms/functional/_color.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  copying torchvision/prototype/transforms/functional/_misc.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  copying torchvision/prototype/transforms/functional/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/functional
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/_augment.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/_meta_conversion.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/_geometry.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/_type_conversion.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/_color.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  copying torchvision/prototype/transforms/kernels/_misc.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/transforms/kernels
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/dtd.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/svhn.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/coco.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/fer2013.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/gtsrb.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/imagenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/sbd.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/oxford_iiit_pet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/cifar.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/pcam.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/mnist.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/semeion.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/voc.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/clevr.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/caltech.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/cub200.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/celeba.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/utils
  copying torchvision/prototype/datasets/utils/_resource.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/utils
  copying torchvision/prototype/datasets/utils/_dataset.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/utils
  copying torchvision/prototype/datasets/utils/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/utils
  copying torchvision/prototype/datasets/utils/_internal.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/utils
  copying torchvision/prototype/datasets/utils/_query.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/utils
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/googlenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/mobilenet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/mobilenetv3.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/mobilenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/shufflenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/resnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  copying torchvision/prototype/models/quantization/inception.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/quantization
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/models/optical_flow
  copying torchvision/prototype/models/optical_flow/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/optical_flow
  copying torchvision/prototype/models/optical_flow/raft.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/optical_flow
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/models/video
  copying torchvision/prototype/models/video/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/video
  copying torchvision/prototype/models/video/resnet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/video
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/models/segmentation
  copying torchvision/prototype/models/segmentation/deeplabv3.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/segmentation
  copying torchvision/prototype/models/segmentation/fcn.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/segmentation
  copying torchvision/prototype/models/segmentation/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/segmentation
  copying torchvision/prototype/models/segmentation/lraspp.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/segmentation
  creating build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/retinanet.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/mask_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/ssdlite.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/fcos.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/keypoint_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/faster_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  copying torchvision/prototype/models/detection/ssd.py -> build/lib.linux-x86_64-3.8/torchvision/prototype/models/detection
  creating build/lib.linux-x86_64-3.8/torchvision/datasets/samplers
  copying torchvision/datasets/samplers/clip_sampler.py -> build/lib.linux-x86_64-3.8/torchvision/datasets/samplers
  copying torchvision/datasets/samplers/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/datasets/samplers
  creating build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/googlenet.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/mobilenet.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/mobilenetv3.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/mobilenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/shufflenetv2.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/resnet.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/utils.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  copying torchvision/models/quantization/inception.py -> build/lib.linux-x86_64-3.8/torchvision/models/quantization
  creating build/lib.linux-x86_64-3.8/torchvision/models/optical_flow
  copying torchvision/models/optical_flow/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/models/optical_flow
  copying torchvision/models/optical_flow/raft.py -> build/lib.linux-x86_64-3.8/torchvision/models/optical_flow
  copying torchvision/models/optical_flow/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/models/optical_flow
  creating build/lib.linux-x86_64-3.8/torchvision/models/video
  copying torchvision/models/video/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/models/video
  copying torchvision/models/video/resnet.py -> build/lib.linux-x86_64-3.8/torchvision/models/video
  creating build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  copying torchvision/models/segmentation/deeplabv3.py -> build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  copying torchvision/models/segmentation/fcn.py -> build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  copying torchvision/models/segmentation/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  copying torchvision/models/segmentation/segmentation.py -> build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  copying torchvision/models/segmentation/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  copying torchvision/models/segmentation/lraspp.py -> build/lib.linux-x86_64-3.8/torchvision/models/segmentation
  creating build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/retinanet.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/mask_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/anchor_utils.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/rpn.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/backbone_utils.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/image_list.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/__init__.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/ssdlite.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/fcos.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/transform.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/generalized_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/keypoint_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/roi_heads.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/_utils.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/faster_rcnn.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/models/detection/ssd.py -> build/lib.linux-x86_64-3.8/torchvision/models/detection
  copying torchvision/prototype/datasets/_builtin/cifar100.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/sbd.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/caltech101.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/caltech256.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/dtd.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/cub200.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/oxford-iiit-pet.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/imagenet.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/coco.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/voc.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  copying torchvision/prototype/datasets/_builtin/cifar10.categories -> build/lib.linux-x86_64-3.8/torchvision/prototype/datasets/_builtin
  running build_ext
  creating /workdir/vision/build/temp.linux-x86_64-3.8
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/quantized
  creating /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/quantized/cpu
  Emitting ninja build file /workdir/vision/build/temp.linux-x86_64-3.8/build.ninja...
  Compiling objects...
  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
  1.10.0
  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/nms_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/ps_roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/ps_roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/deform_conv2d_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/ps_roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/ps_roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/deform_conv2d_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/nms_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/ps_roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/ps_roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/nms_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/ps_roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/ps_roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/roi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/roi_pool_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/deform_conv2d.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/nms.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/ps_roi_align.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/ps_roi_pool.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/quantized/cpu/qnms_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/quantized/cpu/qroi_align_kernel.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/roi_align.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/roi_pool.o /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/vision.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.8/torchvision/_C.so
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/nms_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/ps_roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/ps_roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autocast/roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/deform_conv2d_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/ps_roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/ps_roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/autograd/roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/deform_conv2d_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/nms_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/ps_roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/ps_roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cpu/roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/nms_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/ps_roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/ps_roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/roi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/cuda/roi_pool_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/deform_conv2d.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/nms.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/ps_roi_align.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/ps_roi_pool.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/quantized/cpu/qnms_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/quantized/cpu/qroi_align_kernel.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/roi_align.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/ops/roi_pool.o: No such file or directory
  x86_64-linux-gnu-g++: error: /workdir/vision/build/temp.linux-x86_64-3.8/workdir/vision/torchvision/csrc/vision.o: No such file or directory
  error: command '/usr/bin/x86_64-linux-gnu-g++' failed with exit code 1
  error: subprocess-exited-with-error
```

### Versions

Here is my Dockerfile for custom image:

```
FROM nvidia/cuda:11.5.1-cudnn8-devel-ubuntu20.04

ENV LANG C.UTF-8
ENV DEBIAN_FRONTEND noninteractive
ENV NVIDIA_DRIVER_CAPABILITIES video,compute,utility
ENV PYTHONPATH $PYTHONPATH:/workdir
WORKDIR /workdir

# Install python and apt-get packages
RUN apt-get update && apt -y upgrade &&\
    apt-get -y install \
    software-properties-common \
    build-essential yasm nasm ninja-build \
    unzip git wget curl nano vim tmux \
    sysstat libtcmalloc-minimal4 pkgconf \
    autoconf libtool flex bison \
    libsm6 libxext6 libxrender1 libgl1-mesa-glx \
    libx264-dev libsndfile1 libmp3lame-dev libssl-dev \
    python3 python3-dev python3-pip \
    liblapack-dev libopenblas-dev gfortran &&\
    ln -s /usr/bin/python3 /usr/bin/python &&\
    apt-get clean &&\
    apt-get -y autoremove &&\
    rm -rf /var/lib/apt/lists/* &&\
    rm -rf /var/cache/apt/archives/*

# Install CMake
RUN CMAKE_VERSION=3.22.2 &&\
    wget https://github.com/Kitware/CMake/releases/download/v${CMAKE_VERSION}/cmake-${CMAKE_VERSION}.tar.gz &&\
    tar -zxvf cmake-${CMAKE_VERSION}.tar.gz &&\
    cd cmake-${CMAKE_VERSION} &&\
    ./bootstrap &&\
    make && make install &&\
    cd .. && rm -rf cmake-${CMAKE_VERSION} cmake-${CMAKE_VERSION}.tar.gz

# Install pip and setuptools
RUN pip3 install --upgrade --no-cache-dir \
    pip==22.0.3 \
    setuptools==59.5.0 \
    packaging==21.3

# Build nvidia codec headers
RUN git clone --depth 1 -b sdk/11.0 --single-branch https://git.videolan.org/git/ffmpeg/nv-codec-headers.git &&\
    cd nv-codec-headers && make install &&\
    cd .. && rm -rf nv-codec-headers

# Build FFmpeg with NVENC support
RUN git clone --depth 1 -b release/5.0 --single-branch https://github.com/FFmpeg/FFmpeg.git &&\
    cd FFmpeg &&\
    mkdir ffmpeg_build && cd ffmpeg_build &&\
    ../configure \
    --enable-cuda \
    --enable-cuvid \
    --enable-shared \
    --disable-static \
    --disable-doc \
    --extra-cflags=-I/usr/local/cuda/include \
    --extra-ldflags=-L/usr/local/cuda/lib64 \
    --enable-gpl \
    --enable-libx264 \
    --enable-libmp3lame \
    --extra-libs=-lpthread \
    --enable-openssl \
    --enable-nonfree \
    --nvccflags=""-arch=sm_52 \
-gencode=arch=compute_52,code=sm_52 \
-gencode=arch=compute_60,code=sm_60 \
-gencode=arch=compute_61,code=sm_61 \
-gencode=arch=compute_70,code=sm_70 \
-gencode=arch=compute_75,code=sm_75 \
-gencode=arch=compute_80,code=sm_80 \
-gencode=arch=compute_86,code=sm_86 \
-gencode=arch=compute_86,code=compute_86"" &&\
    make -j$(nproc) && make install && ldconfig &&\
    cd ../.. && rm -rf FFmpeg

# Install python packages
RUN pip3 install --no-cache-dir \
    numpy==1.21.5 \
    opencv-python==4.5.5.62 \
    scipy==1.7.3 \
    matplotlib==3.5.1 \
    pandas==1.4.0 \
    scikit-learn==1.0.2 \
    scikit-image==0.19.1 \
    Pillow==9.0.1 \
    librosa==0.8.1 \
    albumentations==1.1.0 \
    pyzmq==22.3.0 \
    Cython==0.29.27 \
    numba==0.55.1 \
    requests==2.27.1 \
    psutil==5.9.0 \
    pydantic==1.9.0 \
    PyYAML==6.0 \
    notebook==6.4.8 \
    ipywidgets==7.6.5 \
    tqdm==4.62.3 \
    pytest==6.2.5 \
    pytest-cov==3.0.0 \
    mypy==0.931 \
    pre-commit==2.17.0 \
    flake8==4.0.1

ENV TORCH_CUDA_ARCH_LIST 5.2;6.0;6.1;7.0;7.5;8.0;8.6

# Build MAGMA
COPY docker/magma/make.inc make.inc
RUN MAGMA_VERSION=2.6.1 &&\
    ln -s /usr/local/cuda/lib64/libcudart.so /usr/lib/libcudart.so &&\
    wget http://icl.utk.edu/projectsfiles/magma/downloads/magma-${MAGMA_VERSION}.tar.gz &&\
    tar -xzf magma-${MAGMA_VERSION}.tar.gz &&\
    cp make.inc magma-${MAGMA_VERSION} &&\
    cd magma-${MAGMA_VERSION} &&\
    make -j$(nproc) && make install &&\
    cd .. && rm -rf magma-${MAGMA_VERSION} magma-${MAGMA_VERSION}.tar.gz make.inc

# Install PyTorch
RUN git clone --depth 1 -b master --single-branch https://github.com/pytorch/pytorch.git &&\
    cd pytorch &&\
    git submodule sync && git submodule update --init --recursive && \
    TORCH_NVCC_FLAGS=""-Xfatbin -compress-all"" && \
    TORCH_CUDA_ARCH_LIST=""${TORCH_CUDA_ARCH_LIST}"" && \
    USE_CUDA=ON && \
    pip install -v . && \
    cd .. && rm -rf pytorch

# Hack to fix small bug
RUN sed -i ""s/, '-v'/, '--version'/"" \
    ""/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py""

# Install torchvision
RUN git clone --depth 1 -b main https://github.com/pytorch/vision.git &&\
    cd vision && \
    FORCE_CUDA=1 DEBUG=1 pip install -v . &&\
    cd .. && rm -rf vision

```"
new video reading API crash,pytorch/vision,2022-02-14 13:19:15,11,bug#module: io#module: video,5419,1137293950,"### 🐛 Describe the bug

I get `malloc(): memory corruption` when running the following code with a video file.
```
reader = torchvision.io.VideoReader(path, num_threads=1)
data = next(reader)
print(data)
data = next(reader)
print(data)
data = next(reader)
print(data)
data = next(reader)
print(data)
```

Video metadata:
```
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/data/home/prabhatroy/data/output.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf58.45.100
  Duration: 00:01:02.00, start: 0.000000, bitrate: 838 kb/s
    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 360x360 [SAR 1:1 DAR 1:1], 694 kb/s, 60 fps, 60 tbr, 16k tbn, 2k tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 127 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
```

On debugging, it points at this line as the culprit: https://github.com/pytorch/vision/blob/0db67d857d612b8b5f196d1b9e1314d07b8a7a29/torchvision/csrc/io/video/video.cpp#L314

### Versions

Collecting environment information...
PyTorch version: 1.11.0.dev20220203+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.20.4
Libc version: glibc-2.27

Python version: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-1051-aws-x86_64-with-glibc2.17
Is CUDA available: False
CUDA runtime version: 11.1.105
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.5
/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7.6.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.22.2
[pip3] torch==1.11.0.dev20220203+cu111
[pip3] torchvision==0.12.0a0+22f8dc4
[conda] numpy                     1.22.2                   pypi_0    pypi
[conda] torch                     1.11.0.dev20220203+cu111          pypi_0    pypi
[conda] torchvision               0.12.0a0+22f8dc4           dev_0    <develop>
"
[RFC] New Ops in TorchVision,pytorch/vision,2022-02-13 12:07:19,7,needs discussion#module: ops,5414,1135745429,"### 🚀 The feature

Consider adding the following operators in TorchVision:

## Layers

There is a separate ticket for tracking common layers: #4333

- [x] [DropBlock Layer](https://arxiv.org/abs/1810.12890) #5416
- [ ] [AutoDropout Layer](https://arxiv.org/abs/2101.01761)
- [ ] [DropConnect Layer](http://yann.lecun.com/exdb/publis/pdf/wan-icml-13.pdf)
- [ ] [ShakeDrop Layer](https://arxiv.org/abs/1802.02375)

## Operators
- [ ] [SoftNMS](https://arxiv.org/pdf/1704.04503.pdf)
- [ ] [masks_to_boxes for N-dimensional masks](https://github.com/pytorch/vision/issues/4339)

## Losses

There is a separate ticket tracking Losses proposals: #2980

## Schedulers & Optimizers (Core upstreaming)
- [ ] [Polynomial LR scheduler](https://github.com/pytorch/vision/issues/4438) 
- [ ] [Random Noise LR Scheduler](https://arxiv.org/abs/1810.01322)
- [ ] [LARS](https://arxiv.org/abs/1708.03888)
- [ ] [LAMB](https://arxiv.org/abs/1904.00962)"
image.so on Apple Silicon can't load libpng16.16.dylib and libjpeg.9.dylib,pytorch/vision,2022-02-12 17:19:04,15,topic: binaries,5413,1134320503,"### 🐛 Describe the bug

code:
```python
import torchvision
```
UserWarning:
```
/Users/alanyoung/Documents/Codes/Kaggle/ClassifyLeaves/venv/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: dlopen(/Users/alanyoung/Documents/Codes/Kaggle/ClassifyLeaves/venv/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libpng16.16.dylib
  Referenced from: /Users/alanyoung/Documents/Codes/Kaggle/ClassifyLeaves/venv/lib/python3.9/site-packages/torchvision/image.so
  Reason: tried: '/Users/malfet/miniforge3/envs/py_39_torch-1.10.2/lib/libpng16.16.dylib' (no such file), '/Users/malfet/miniforge3/envs/py_39_torch-1.10.2/lib/libpng16.16.dylib' (no such file), '/Users/malfet/miniforge3/envs/py_39_torch-1.10.2/lib/libpng16.16.dylib' (no such file), '/Users/malfet/miniforge3/envs/py_39_torch-1.10.2/lib/libpng16.16.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/lib/libpng16.16.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/lib/libpng16.16.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/lib-dynload/../../libpng16.16.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/lib/libpng16.16.dylib' (no such file), '/opt/homebrew/Caskroom/miniforge/base/bin/../lib/libpng16.16.dylib' (no such file), '/usr/local/lib/libpng16.16.dylib' (no such file), '/usr/lib/libpng16.16.dylib' (no such file)
  warn(f""Failed to load image Python extension: {e}"")
```
on M1 Max.
In https://github.com/pytorch/vision/issues/5137#issuecomment-1023708994, it is said to be fixed but I don't think so.

Therefore you can't use torchvision.io.read_img().
```python
import torchvision
img = torchvision.io.read_img('./1.jpg')
```

```
RuntimeError: No such operator image::read_file
```
In https://discuss.pytorch.org/t/failed-to-load-image-python-extension-could-not-find-module/140278/20, Andrade has the same problem.

### Versions

Collecting environment information...
PyTorch version: 1.10.2
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 12.1 (arm64)
GCC version: Could not collect
Clang version: 13.0.0 (clang-1300.0.29.30)
CMake version: version 3.22.0
Libc version: N/A

Python version: 3.9.7 (default, Sep 16 2021, 23:53:23)  [Clang 12.0.0 ] (64-bit runtime)
Python platform: macOS-12.1-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.22.2
[pip3] torch==1.10.2
[pip3] torchaudio==0.10.2
[pip3] torchvision==0.11.3
[conda] Could not collect

cc @seemethere @malfet "
Allow for arbitrary sequences in torchvision/transforms/functional,pytorch/vision,2022-02-09 15:21:11,2,module: transforms,5398,1128678824,"### 🚀 The feature

Replace all checks for (list,tuple) in functional with Iterable. 

### Motivation, pitch

There are about 8 checks in functional right now which requires the argument to be either list or tuple, e.g.
https://github.com/pytorch/vision/blob/main/torchvision/transforms/functional.py#L1059
Instead of performing these checks, simply checking for Iterable allows to use e.g. numpy arrays.

In my usecase i generate parameters using numpy, and I don't want to convert them.

The documentation indicates that general sequences should work, so the current checks are counter-intuitive. 

### Alternatives

_No response_

### Additional context

_No response_

cc @vfdev-5 @datumbox"
old version is installed for arm64 based conda,pytorch/vision,2022-02-04 09:58:50,2,topic: binaries,5374,1124014330,"### 🐛 Describe the bug

I installed `miniforge` for *apple silicon* with the following specs:

```
          conda version : 4.11.0
    conda-build version : not installed
         python version : 3.9.7.final.0
               platform : osx-arm64
```

I then created a `conda` environment and then install `pytorch` and `torchvision`,

```
conda create --name pytorch_env python=3.9
conda activate pytorch_env
conda install -c pytorch pytorch torchvision
```

When trying to run [this](https://github.com/pytorch/examples/blob/master/mnist/main.py) `main.py` script I get the following **error**:

```
from PIL import Image, ImageOps, ImageEnhance, PILLOW_VERSION 
ImportError: cannot import name 'PILLOW_VERSION' from 'PIL
```

Looking at the `torchvision` package version it installed a very old version in-spite of the new `pillow` and `pytorch` versions installed

```
pillow                    9.0.0            py39hcb29f89_0    conda-forge
python                    3.9.10          h38ef502_2_cpython    conda-forge
pytorch                   1.10.1          cpu_py39hbfdb42d_0    conda-forge
torchvision               0.2.2                      py_3    pytorch
```

After installing the right version as advised on the `readme` it all worked fine!

```
conda install torchvision==0.11.2 -c pytorch
```

But seems the requirements / package dependencies are not aligned as an old version `0.2.2` was installed with new `pytorch` and `pillow` versions. 

> I also did not have this problem when I installed the **Intel** based `miniforge`

(more details [here](https://gist.github.com/kadereub/53923bd99c81a1a019450386126593e6))

### Versions

conda setup:

```
          conda version : 4.11.0
    conda-build version : not installed
         python version : 3.9.7.final.0
               platform : osx-arm64
```

pytorch and python:

```
pillow                    9.0.0            py39hcb29f89_0    conda-forge
python                    3.9.10          h38ef502_2_cpython    conda-forge
pytorch                   1.10.1          cpu_py39hbfdb42d_0    conda-forge
torchvision               0.2.2                      py_3    pytorch
```
"
Installation fails using PDM,pytorch/vision,2022-02-03 21:57:16,2,topic: binaries,5371,1123595426,"### 🐛 Describe the bug

Within a pdm project, running `pdm add torchvision` results in:

```
ERRORS:
add torchvision failed:
Traceback (most recent call last):
  File ""c:\users\classner\.pyenv\pyenv-win\versions\3.9.0\lib\concurrent\futures\thread.py"", line 52, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""C:\Users\classner\AppData\Roaming\Python\Python39\site-packages\pdm\installers\synchronizers.py"", line 190, in install_candidate 
    self.manager.install(can)
  File ""C:\Users\classner\AppData\Roaming\Python\Python39\site-packages\pdm\installers\manager.py"", line 38, in install
    installer(candidate.build(), self.environment, candidate.direct_url())
  File ""C:\Users\classner\AppData\Roaming\Python\Python39\site-packages\pdm\installers\installers.py"", line 176, in install_wheel        
    _install_wheel(
  File ""C:\Users\classner\AppData\Roaming\Python\Python39\site-packages\pdm\installers\installers.py"", line 280, in _install_wheel       
    for record_elements, stream in source.get_contents():
  File ""C:\Users\classner\AppData\Roaming\Python\Python39\site-packages\installer\sources.py"", line 170, in get_contents
    assert record is not None, ""In {}, {} is not mentioned in RECORD"".format(
AssertionError: In C:\Users\classner\AppData\Local\Temp\pip-unpack-49uvth_n\torchvision-0.11.2+cu113-cp39-cp39-win_amd64.whl, torchvision/extension.py is not mentioned in RECORD
```

It seems the pip RECORD file is not correctly built for torchvision. Similar issues are being detected for other packages, too, for example: https://github.com/microsoft/playwright-python/issues/518 .

### Versions

Currently tested with 0.11.2+cu113-cp39-cp39 ."
WIDERFace,pytorch/vision,2022-02-03 09:26:22,0,module: datasets#prototype,5367,1122843372,cc @pmeier @bjuncek
HD1k,pytorch/vision,2022-02-03 09:14:56,0,module: datasets#prototype,5366,1122831803,cc @pmeier @bjuncek
FlyingThings3D,pytorch/vision,2022-02-03 09:14:52,0,module: datasets#prototype,5365,1122831733,cc @pmeier @bjuncek
FlyingChairs,pytorch/vision,2022-02-03 09:14:48,0,module: datasets#prototype,5364,1122831679,cc @pmeier @bjuncek
Sintel,pytorch/vision,2022-02-03 09:14:45,0,module: datasets#prototype,5363,1122831638,cc @pmeier @bjuncek
KittiFlow,pytorch/vision,2022-02-03 09:14:41,0,module: datasets#prototype,5362,1122831570,cc @pmeier @bjuncek
UCF101,pytorch/vision,2022-02-03 09:14:36,0,module: datasets#prototype,5361,1122831490,cc @pmeier @bjuncek
Kinetics,pytorch/vision,2022-02-03 09:14:33,0,module: datasets#prototype,5360,1122831419,cc @pmeier @bjuncek
HMDB51,pytorch/vision,2022-02-03 09:14:29,0,module: datasets#prototype,5359,1122831363,cc @pmeier @bjuncek
PhotoTour,pytorch/vision,2022-02-03 09:14:25,0,module: datasets#prototype,5358,1122831304,cc @pmeier @bjuncek
LFWPairs,pytorch/vision,2022-02-03 09:14:21,0,module: datasets#prototype,5357,1122831239,cc @pmeier @bjuncek
Kitti,pytorch/vision,2022-02-03 09:13:56,1,module: datasets#prototype,5355,1122830837,cc @pmeier @bjuncek
FGVCAircraft,pytorch/vision,2022-02-03 09:13:54,3,module: datasets#prototype,5354,1122830784,cc @pmeier @bjuncek
Cityscapes,pytorch/vision,2022-02-03 09:13:51,3,module: datasets#prototype,5353,1122830739,cc @pmeier @bjuncek
SUN397,pytorch/vision,2022-02-03 09:13:41,0,module: datasets#prototype,5351,1122830576,cc @pmeier @bjuncek
STL10,pytorch/vision,2022-02-03 09:13:38,2,module: datasets#prototype,5350,1122830531,cc @pmeier @bjuncek
SBU,pytorch/vision,2022-02-03 09:13:34,2,module: datasets#prototype,5349,1122830466,cc @pmeier @bjuncek
RenderedSST2,pytorch/vision,2022-02-03 09:13:30,2,module: datasets#prototype,5348,1122830398,cc @pmeier @bjuncek
Places365,pytorch/vision,2022-02-03 09:13:27,9,module: datasets#prototype,5347,1122830355,cc @pmeier @bjuncek
Omniglot,pytorch/vision,2022-02-03 09:13:22,2,module: datasets#prototype,5346,1122830272,cc @pmeier @bjuncek
LSUN,pytorch/vision,2022-02-03 09:13:19,0,module: datasets#prototype,5345,1122830218,cc @pmeier @bjuncek
LFWPeople,pytorch/vision,2022-02-03 09:13:15,0,module: datasets#prototype,5344,1122830159,cc @pmeier @bjuncek
INaturalist,pytorch/vision,2022-02-03 09:13:12,2,module: datasets#prototype,5343,1122830102,cc @pmeier @bjuncek
Flowers102,pytorch/vision,2022-02-03 09:13:03,2,module: datasets#prototype,5341,1122829952,cc @pmeier @bjuncek
Flickr30k,pytorch/vision,2022-02-03 09:13:00,0,module: datasets#prototype,5340,1122829894,cc @pmeier @bjuncek
Flickr8k,pytorch/vision,2022-02-03 09:12:54,1,module: datasets#prototype,5339,1122829809,cc @pmeier @bjuncek
port datasets from the old to the new API,pytorch/vision,2022-02-03 09:12:30,12,module: datasets#prototype,5336,1122829412,"The new dataset API is now stable enough to start porting more datasets from the old API. For the `0.13.0` release planned for 2022H2 we want to achieve at least feature parity for the new API. If you want to help out, please comment on the respective issue so we can assign it to you.

The process of adding a dataset to the new API is described [here](https://github.com/pytorch/vision/blob/main/torchvision/prototype/datasets/_builtin/README.md). In addition, we already ported some datasets that you could use as reference. In any case, if you are blocked by something feel free to send a partial PR and ping me there so I can help.

The following datasets need to be ported:

## Image classification

Image classification datasets are good starting point if you are not familiar with the dataset or the new API since they these datsets tend to be the easiest.

- [x] #5337
- [x] #5338
- [ ] #5339 [^1]
- [ ] #5340 [^1]
- [ ] #5341
- [x] #5342
- [ ] #5343
- [ ] #5344 [^3]
- [ ] #5345
- [ ] #5346
- [ ] #5347
- [ ] #5348
- [ ] #5349 [^1]
- [ ] #5350
- [ ] #5351
- [x] #5352

## Image detection or segmentation

Image detection or segmentation datasets tend to be a little harder since one needs to merge more infomation into one sample compared to classification. My suggestion is to only pick one of these if you are either familiar with the dataset or the new API so you don't have two manage two things at once.

- [ ] #5353 (@vfdev-5 )
- [ ] #5354 [^2]
- [ ] #5355 [^2] [^3]
- [x] #5356 [^2]
- [ ] #5367

## Image pairs

We are still designing how exactly image pair datasets should be implemented. I list them here for completeness, but I suggest not picking up any of them until the design is finished.

- [ ] #5357
- [ ] #5358

## Video classification

We are still designing how exactly video datasets should be implemented. I list them here for completeness, but I suggest not picking up any of them until the design is finished.

- [ ] #5359
- [ ] #5360
- [ ] #5361

## Optical flow

We are still designing how exactly optical flow datasets should be implemented. I list them here for completeness, but I suggest not picking up any of them until the design is finished.

- [ ] #5362 [^3]
- [ ] #5363
- [ ] #5364
- [ ] #5365
- [ ] #5366

[^1]: These datasets do not provide public download links for the data so they might be harder to work on.
[^2]: These datasets are implemented as classification datasets in the old API, but provide extra annotations for detection or segmentation.
[^3]: Maybe we should have `lfw/people`, `kitti/object`, and `kitti/flow` datasets to cleanly separate the different variants. This also applies to `coco` as discussed in https://github.com/pytorch/vision/pull/5326#discussion_r796813705


cc @pmeier @bjuncek"
Pass losses as callables when building detection models,pytorch/vision,2022-02-01 09:22:26,2,needs discussion#topic: object detection,5325,1120417663,"### 🚀 The feature

Some models currently accept normalization strategies as callables (```mobilenet_backbone``` accepts a ```norm_layer``` argument for example) but loss functions are currently hardcoded (```F.cross_entropy``` for ```fastercnn.roi_heads``` for example).

Following what has been done on normalization strategies loss function could be passed as callables in the modules constructor. This shouldn't break any backward compatibility. Reduction strategies still need to be properly handled.

### Motivation, pitch

Currently, trying different loss functions requires to use some dirty model patches. Accepting the losses in the model constructors would provide a much cleaner way to hack around the models.

If any interest I can propose a first PR modifying the Faster-RNN models.

### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox"
New Model Architectures - Implementation and Documentation Details,pytorch/vision,2022-01-31 10:42:19,5,,5319,1119259291,"### 🚀 The feature

When adding a new model architecture there are some design/implementation details and documentation requirements that need to be taken into account. This issue intents to track such details in a dynamic manner, as it is possible to change over time.

### Motivation, pitch

## New Model Architectures - Implementation Details

### Model development and training steps

When developing a new model there are some details not to be missed:

- Implement a model factory function for each of the model variants

- in the module constructor, [pass layer constructor instead of instance](https://github.com/pytorch/vision/blob/47bd962069ba03f753e7ba711cb825317be0b00a/torchvision/models/efficientnet.py#L88) for configurable layers like norm, activation, and log the api usage with `_log_api_usage_once(self)`

- fuse layers together with existing common blocks if possible; For example consecutive conv, bn, activation layers could be replaced by [ConvNormActivation](https://github.com/pytorch/vision/blob/47bd962069ba03f753e7ba711cb825317be0b00a/torchvision/ops/misc.py#L104)

- define `__all__` in the beginning of the model file to expose model factory functions; import model public APIs (e.g. factory methods) in `torchvision/models/__init__.py`

- create the model builder using the new API and add it to the prototype area. Here is an [example](https://github.com/pytorch/vision/pull/4784/files) on how to do this. The new API requires adding more information about the weights such as the preprocessing transforms necessary for using the model, meta-data about the model, etc

- Make sure you write tests for the model itself (see `_check_input_backprop`, `_model_params` and `_model_params` in `test/test_models.py`) and for any new operators/transforms or important functions that you introduce

- the new model should be torch scriptable (using `torch.jit.script`)

- the new model should be fx compatible (using `torch.fx.symbolic_trace`)

Note that this list is not exhaustive and there are details here related to the code quality etc, but these are rules that apply in all PRs (see [Contributing to TorchVision](https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md)).

Once the model is implemented, you need to train the model using the reference scripts. For example, in order to train a classification resnet18 model you would:

1. go to `references/classification`

2. run the train command (for example `torchrun --nproc_per_node=8 train.py --model resnet18`)

After training the model, select the best checkpoint and estimate its accuracy with a batch size of 1 on a single GPU. This helps us get better measurements about the accuracy of the models and avoid variants introduced due to batch padding (read [here](https://github.com/pytorch/vision/pull/4609/commits/5264b1a670107bcb4dc89e83a369f6fd97466ef8) for more details).

Finally, run the model test to generate expected model files for testing. Please include those generated files in the PR as well.:

`EXPECTTEST_ACCEPT=1 pytest test/test_models.py -k {model_name}`


### Documentation and Pytorch Hub

- `docs/source/models.rst`:

    - add the model to the corresponding section (classification/detection/video etc.)

    - describe how to construct the model variants (with and without pre-trained weights)

    - add model metrics and reference to the original paper

- `hubconf.py`:

    - import the model factory functions

    - submit a PR to [https://github.com/pytorch/hub](https://github.com/pytorch/hub) with a model page (or update an existing one)

- `README.md` under the reference script folder:

    - command(s) to train the model

### Alternatives

_No response_

### Additional context

_No response_"
[feature request] ImageNet dataset to (optionally) return detections,pytorch/vision,2022-01-31 10:18:33,1,module: datasets,5318,1119233455,"### 🚀 The feature

ImageNet dataset features extra annotated bounding box detections for some classes: https://image-net.org/download-bboxes.php. It would be nice if torchvision supported parsing this `bboxes_annotations.tar.gz` tarball and returning these detections (e.g. as a transform).

Currently it's hard to do because ImageNet / ImageFolder / DatasetFolder do not return original file name as part of targets, so enriching the dataset is impossible to do just in a form of transform (and subclassing / patching is needed).

### Motivation, pitch

N/A

### Alternatives

_No response_

### Additional context

_No response_

cc @pmeier"
[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1122),pytorch/vision,2022-01-28 20:33:04,2,windows,5311,1117824789,"### 🐛 Describe the bug

When downloading weights data from the PyTorch hub we get the following:
```
[stdlog] 2022-01-27 14:41:18,434 client           L0864 INFO | Traceback (most recent call last):
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\site-packages\torchvision\models\detection\faster_rcnn.py"", line 369, in fasterrcnn_resnet50_fpn
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |     state_dict = load_state_dict_from_url(model_urls['fasterrcnn_resnet50_fpn_coco'],
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\site-packages\torch\hub.py"", line 524, in load_state_dict_from_url
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |     download_url_to_file(url, cached_file, hash_prefix, progress=progress)
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\site-packages\torch\hub.py"", line 394, in download_url_to_file
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |     u = urlopen(req)
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\urllib\request.py"", line 214, in urlopen
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |     return opener.open(url, data, timeout)
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\urllib\request.py"", line 517, in open
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |     response = self._open(req, data)
[stdlog] 2022-01-27 14:41:18,435 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\urllib\request.py"", line 534, in _open
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |     result = self._call_chain(self.handle_open, protocol, protocol +
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\urllib\request.py"", line 494, in _call_chain
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |     result = func(*args)
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\urllib\request.py"", line 1385, in https_open
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |     return self.do_open(http.client.HTTPSConnection, req,
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |   File ""C:\Program Files\Python39\lib\urllib\request.py"", line 1345, in do_open
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO |     raise URLError(err)
[stdlog] 2022-01-27 14:41:18,436 client           L0864 INFO | urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1122)>
```
whereby checks of the certificate validity passed and this issue only happens on Windows (tested the same code on Linux without any problem). Downloading the same data file from a browser works fine (don't mind the loaded page, I loaded `download.pytorch.org` at first):

![image](https://user-images.githubusercontent.com/1969979/151399895-9007f506-f593-483b-b5ab-ff1202b271e9.png)



### Versions

C:\>python.exe collect_env.py
Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.
                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe
Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Enterprise 2016 LTSB
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.14393-SP0
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] Could not collect
[conda] Could not collect

cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @mszhanyi"
Segfault after tracing quantized mobilenet v3 using nightly version,pytorch/vision,2022-01-27 19:58:07,6,,5303,1116681925,"### 🐛 Describe the bug

Sample code to reproduce:

```python
import torch
import torchvision
from torchvision import transforms
from PIL import Image

def download_file(url, filename):
    import urllib
    try:
        urllib.URLopener().retrieve(url, filename)
    except:
        urllib.request.urlretrieve(url, filename)


def download_data():
    download_file(""https://github.com/pytorch/hub/raw/master/images/dog.jpg"", ""dog.jpg"")

def trace_mobilenet():
    model = torchvision.models.quantization.mobilenet_v3_large(pretrained=True, progress=True, quantize=True)
    model.eval()

    # validate that model runs
    input_image = Image.open(""dog.jpg"")
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model

    print('start tracing...')
    export_model = torch.jit.trace(model, input_batch)
    print('end tracing...')
    print(model)
    print(export_model)

download_data()
trace_mobilenet()
```

Output:
```
/anaconda3/envs/torch37/lib/python3.7/site-packages/torch/ao/quantization/utils.py:175: UserWarning: must run observer before calling calculate_qparams. Returning default values.
  ""must run observer before calling calculate_qparams. "" +
start tracing...
end tracing...
Segmentation fault (core dumped)
```

The segfault happens randomly. When I tried to debug using pdb, the segfault happens at different lines each time (original code tried to do other stuff after tracing, the above `print` code seems to be the simplest repro I could create), and they all happen after `torch.jit.trace` is called.

### Versions

Collecting environment information...
PyTorch version: 1.11.0.dev20220127+cpu
Is debug build: False
CUDA used to build PyTorch: Could not collect
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.22.2
Libc version: glibc-2.9

Python version: 3.7.0 (default, Oct  9 2018, 10:31:47)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-5.13.0-27-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: False
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1080
Nvidia driver version: 510.39.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.3.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.3.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.2
[pip3] torch==1.11.0.dev20220127+cpu
[pip3] torchvision==0.12.0.dev20220127+cpu
[conda] mkl                       2022.0.1           h06a4308_117
[conda] mkl-include               2022.0.1           h06a4308_117
[conda] numpy                     1.21.2           py37hd8d4704_0
[conda] numpy-base                1.21.2           py37h2b8c604_0
[conda] torch                     1.11.0.dev20220127+cpu          pypi_0    pypi
[conda] torchvision               0.12.0.dev20220127+cpu          pypi_0    pypi"
pil_to_tensor / to_tensor fails when converting zero-sized PIL images,pytorch/vision,2022-01-27 14:01:33,1,,5297,1116251186,"### 🐛 Describe the bug

```python
import torchvision
import numpy as np
from PIL import Image

img = Image.fromarray(np.empty((496, 0), dtype=np.uint8))

torchvision.transforms.functional.pil_to_tensor(img) # *** SystemError: tile cannot extend outside image
torchvision.transforms.functional.to_tensor(img) # *** SystemError: tile cannot extend outside image

np.asarray(img) # same error, because this upstream bug https://github.com/python-pillow/Pillow/issues/5992
```

torchvision could special-case conversion of zero-sized PIL images while the upstream bug is getting fixed. zero-sized images can sometimes occur as result of not super-well guarded cropping (that's how I hit this bug, the above is just a minimally reproducing example). IMO it's much better to propagate zero-sized images at least through these basic conversion functions. At the very least it should provide a good error message and links to the github issues about the bug

### Versions

(Pdb) torchvision.__version__
'0.10.1+cu111'
(Pdb) PIL.__version__
'8.4.0'
(Pdb) np.__version__
'1.21.2'"
[docs] Table of accepted image tensor dtypes for all transforms,pytorch/vision,2022-01-27 00:23:27,1,enhancement#module: documentation,5294,1115632973,"### 📚 The doc issue

Many transforms support equally float32 and uint8, but it seems not documented. Or sometimes some image formats aren't supported but it's not clear why so: e.g. torchvision.transforms.functional.normalize throws when given a uint8 image, but there seems no big reason why it can't autocast uint8 inputs to float32 (just as most pytorch core ops do now).

Sometimes uint8 can be more convenient since it saves memory or sometimes even int16 is necessary: https://discuss.pytorch.org/t/colorjitter-transformation-for-16-bit-images/108897. Uint8 images are also more convenient for no-copy interop with OpenCV or PIL.

### Suggest a potential alternative/fix

_No response_"
Automate closing of scheduled workflows,pytorch/vision,2022-01-26 18:24:31,5,module: datasets#module: ci,5290,1115347726,"### 🚀 The feature

We need to currently close the Issues raised by our automated tests for datasets manually.

https://github.com/pytorch/vision/issues?q=is%3Aissue+is%3Aopen+Scheduled+workflow+failed ? 

Sometimes they get bunched up or we forget to close them.

### Motivation

The cronjob runs *daily*. So ideally we should be closing the issue the next day if it doesn't fail. (If it does fail anyway another issue is created).

I propose a simple GitHub action that closes *only this issue* once the issue gets stale by 3 days. The logic is that if the issue is open for 3 days.
1. Everything is working fine and hence we don't have a problem to close it. (It was one off error)
2. There is another issue which is newer that supersedes this old issue.

So we should be fine closing it.

### Pitch

I think a simple GitHub action can do this job. Would need to investigate but think that technically it's very much possible.

### Additional context

I don't propose to extend this functionality to close all stale issues. An issue is well documented user feedback which is very valuable indeed. An old issue can be gem of resource for someone and keeping it open doesn't harm. 

But for bot issues :( It isn't nice to be flooded with them being open :( 

Some references

https://github.com/marketplace/actions/close-stale-issues

cc @pmeier @seemethere "
Potential improvements to the prototype dataset implementation #5269 ,pytorch/vision,2022-01-26 11:38:14,0,module: datasets#prototype,5281,1114922868,"Keeping track of a meeting we just had with @pmeier , here are a few things we can try:

- Instead of having a separate `DatasetInfo` class, we could try having all these fields be part of the `Dataset` class itself. Then `_make_info()` would become `Dataset.__init__()`, and we could expose an `info()` method / property which would expose the relevant fields to the user (currently, this is the `info` attribute).
- The current logic for defining valid parameters  and their defaults is a bit unconventional (though the `DatasetInfo` class), and potentially limiting. This issue might resolve itself by tackling the one above
- The content of the `extra` field could maybe be exposed directly at the `DatasetInfo` level (or if we pursue with the first item above, exposed at the `Dataset` level).
- We might want to allow a `root=None` parameter to the `datasets.load()` so users can set a custom `home`, without having to go through the `home()` global function
- Dataset registration logic: currently this is done somewhat magically by adding the dataset class to `__init__.py` file. Perhaps we could do that with a more explicit registration mechanism (e.g. a decorator on the class or something)
- The decoder parameter will eventually be removed from `load()`, and from the `_make_datapipe()` method
- The current default of `skip_integrity_check` is False, but it doesn't exactly reflect the actual behaviour: we still skip the integrity checks if the data is already downloaded (because the check can take a while on big archives). Perhaps we can try to find an API that accurately describes this behaviour.
- When we're ready to ""graduate"" the prototypes, we should review all of the private / public decisions that were made so far
- Similarly we should review, document and test the utilities in `folder.py`

cc @pmeier @bjuncek"
Can't build windows torchVsion C++ API library,pytorch/vision,2022-01-25 10:48:26,8,windows,5277,1113726894,"# 🐛 Describe the bug
* I try to use TorchVision c++ API in windows10. I follow https://github.com/pytorch/vision Using the models on C++. I download libtorch1.10.1-cu102 and unzip it at D:\thirdpartyforwindows_for_ai\libtorch\libtorch1.10.1-cu102.
```
git clone https://github.com/pytorch/vision.git
cd vision
git checkout v0.11.2
mkdir build
cd build 
cmake -DCMAKE_GENERATOR_PLATFORM=x64 -DCMAKE_PREFIX_PATH=""D:\thirdpartyforwindows_for_ai\libtorch\libtorch1.10.1-cu102;C:\Users\user\Desktop\pybind11\install;C:\Users\user\anaconda3\envs\webv2\libs"" -DCMAKE_INSTALL_PREFIX=""C:\Users\user\Desktop\torchVision\install"" DCMAKE_BUILD_TYPE=Release ..
cmake --build .
```
* After Build, I get some error message
```
D:\thirdpartyforwindows_for_ai\libtorch\libtorch1.10.1-cu102\include\ATen/core/boxing/KernelFunction_impl.h(75): error
C2672: 'c10::KernelFunction::call': 找不到相符的多載函式 [C:\Users\user\Desktop\torchVision\build\torchvision.vcxproj]
D:\thirdpartyforwindows_for_ai\libtorch\libtorch1.10.1-cu102\include\ATen/core/boxing/KernelFunction_impl.h(81): error
C2783: 'Return c10::KernelFunction::call(const c10::OperatorHandle &,c10::DispatchKeySet,Args...) const': 無法針對 'Return'
 推算 樣板 引數 [C:\Users\user\Desktop\torchVision\build\torchvision.vcxproj]
```
* Is my Libtorch version wrong? Is there anything wrong with my build steps?
# Versions
* Torch 1.10.1+cu102
* Torchvision checkout v0.11.2
* windows10


cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @mszhanyi"
Potential simplifications to the prototype dataset testing,pytorch/vision,2022-01-24 15:57:24,1,enhancement#module: datasets#prototype,5269,1112829664,"As discussed offline with @pmeier, there are a few potential simplification / improvements we could make to the prototype datasets tests. 

- The tests currently create a tmp dir, but this tmp dir isn't cleaned up after the tests are run.
- The tests are currently designed such that the mocked dataset root is the same across all tests. Is this worth enforcing? We might be able to simplify the code if we don't.
- We currently rely on a cache, which isn't really needed in terms of speed (tests are fast enough for now). The cache leads to slightly more complicated code, in particular regarding what each dataset mock can return (a dict with datasetConfig as keys). Removing the cache might simplify the code without sacrificing too much in testing time. If the test time becomes an issue this is something we can revisit.

- qmnist test 50k is quite slow as it needs to generate a lot of images. Is it worth keeping the test? Is it worth even keeping the dataset at all? Should we think of a faster way to test it?

cc @pmeier @bjuncek"
FCOS empty box images,pytorch/vision,2022-01-24 11:58:48,17,,5266,1112551460,"I am playing around with new FCOS models (thanks for that) and am encountering issues when providing images without box annotations. This is a common use case in object detection, and also works for other detector models in torchvision.

A simple example to replicate:

```
model = fcos_resnet50_fpn(pretrained=True)
model(torch.zeros((1,3,512,512)), targets=[{""boxes"": torch.empty(0,4), ""labels"": torch.empty(0,1).to(torch.int64)}])
```

An indexing error happens in `FCOSHead` when running `compute_loss` in this part:

```
all_gt_classes_targets = []
all_gt_boxes_targets = []
for targets_per_image, matched_idxs_per_image in zip(targets, matched_idxs):
    gt_classes_targets = targets_per_image[""labels""][matched_idxs_per_image.clip(min=0)]
    gt_classes_targets[matched_idxs_per_image < 0] = -1  # backgroud
    gt_boxes_targets = targets_per_image[""boxes""][matched_idxs_per_image.clip(min=0)]
    all_gt_classes_targets.append(gt_classes_targets)
    all_gt_boxes_targets.append(gt_boxes_targets)
```

A workaround seems to be necessary, when having empty targets. Happy for any guidance, maybe there is also a different way necessary for me to train on empty images.

@jdsgomes @xiaohu2015 @zhiqwang 


### Versions

Torchvision @ master"
video classification experiments using GPU decoder,pytorch/vision,2022-01-21 17:51:25,0,module: video,5252,1110738730,Run video classification experiments using GPU decoder and compare the results with CPU decoder.
video classification reference script with GPU decoder support,pytorch/vision,2022-01-21 17:48:41,0,enhancement#module: reference scripts#module: video,5251,1110736611,
Add kinetics dataset to use new video reading API,pytorch/vision,2022-01-21 17:47:50,0,enhancement#module: video,5250,1110735957,"Currently, kinetics video dataset uses the old video reading API. Adding support for new video reading API would enable users to choose between CPU and GPU decoding functionality."
Change CPU decoder output frames to use ITU709 colour space,pytorch/vision,2022-01-21 14:45:21,0,module: video,5245,1110549525,"GPU decoder outputs frame with ITU709 colour space, whereas CPU decoder uses the default colour space. We need to change the CPU decoder frames to use ITU709 colour space to match the GPU decoder output. 
For verification, we could use pyav like this:
```
av_frames = torch.tensor(av_frame.to_rgb(src_colorspace=""ITU709"").to_ndarray())
```
and compare the CPU decoded frames against `av_frames`."
Allow more consistent degrees=None in RandomAffine constructor,pytorch/vision,2022-01-21 11:38:13,7,help wanted#module: transforms,5241,1110372401,"### 🐛 Describe the bug

All other arguments allow for setting None to desactivate, except degrees. For the cases of when only some of these arguments are set as kwargs, allowing same values for desactivation is a bit better.

### Versions

N/A"
Find a better way to compare GPU decoder results with pyav results,pytorch/vision,2022-01-19 12:18:16,0,module: tests#module: video,5216,1108032994,"https://github.com/pytorch/vision/pull/5191#discussion_r787632380
"
save_image docs do not specify the format of the images to be saved,pytorch/vision,2022-01-19 07:41:10,2,,5211,1107761231,"### 📚 The doc issue

By looking at the code, it seems like the `save_image` function expects floats tensors with values between 0 and 1, but in the docs that is nowhere to be found. A very reasonable alternative for the user (which is actually what I was initially providing, and what is expected by `sklearn.io.imsave` for instance) is a tensor of uint8 with values between 0 and 255 (which is actually what internally `save_image` casts to).

Moreover, from the docs it's not clear the expected order of channels. By looking at the code again it seems channels, height and width is the expected order, because the code does a permute.

Reference line from `save_image`:
`7933    ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()`

### Suggest a potential alternative/fix

Please add this expected tensor format (datadype, value ranges and channel order) to the docs.

Ideally also adding support for passing directly the uint8 would be great as it would make porting from other libraries much simpler than taking the uint8 tensor, transforming it to float and passing it to torchvision just to be caster again to uint8 internally."
Add the ADE20K dataset in TorchVision,pytorch/vision,2022-01-16 12:17:18,5,module: datasets#prototype,5200,1105036497,"### 🚀 The feature

[ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/) is a dataset introduced in 2017 for Object Detection and Segmentation (code [here](https://github.com/CSAILVision/ADE20K)). It has [371 citations](https://paperswithcode.com/dataset/ade20k) and it's become more and more popular as a benchmark.

We should consider adding it to TorchVision using the new Dataset API.

cc @pmeier @bjuncek"
[docs] PIL image/enhance ; OpenCV; scikit-image ops <> torchvision transforms migration advice / summary table / test+diff images / comments in individual functions ,pytorch/vision,2022-01-13 14:54:58,9,needs discussion#module: transforms#module: documentation,5194,1101898047,"Original name: `[docs] F.gaussian_blur should specify relation of kernel_size / sigma to PIL's sigma`

### 📚 The doc issue

A lot of legacy GaussianBlur augmentations like in https://github.com/facebookresearch/dino/blob/d2f3156/utils.py#L36 and in https://github.com/facebookresearch/unbiased-teacher/blob/main/ubteacher/data/transforms/augmentation_impl.py#L20 have used PIL's GaussianBlur which has only a single `radius` parameter. New torchvision's `GaussianBlur` has two parameters: hard `kernel_size` and soft `sigma`. It would be very useful if their semantics is explained in relation to existing/popular/legacy PIL's arguments. This will help in porting the legacy code to new torchvision's native GaussianBlur. 

For reference, native pillow's implementation: https://github.com/python-pillow/Pillow/blob/95cff6e959bb3c37848158ed2145d49d49806a31/src/libImaging/BoxBlur.c#L286. It also seems that Pillow's implementation is actually a uniform weighted blur, not a true gaussian one

Related question on SO: https://stackoverflow.com/questions/62968174/for-pil-imagefilter-gaussianblur-how-what-kernel-is-used-and-does-the-radius-par which suggests that radius ~= sigma

Related issues:
* https://github.com/pytorch/vision/issues/5204
* https://github.com/pytorch/vision/issues/2950
* https://github.com/mindee/doctr/issues/402

cc @vfdev-5 @datumbox"
Forward hook is not called when using FX-based feature extractor,pytorch/vision,2022-01-12 22:18:57,9,,5193,1100843289,"### 🐛 Describe the bug

This is an issue I talked to @datumbox 

According to #4540, it looks like IntermediateLayerGetter will be replaced with FX-based feature extractor.
My ML OSS, [torchdistill](https://github.com/yoshitomo-matsubara/torchdistill), is built on PyTorch / torchvision and heavily dependent on forward hook in PyTorch for knowledge distillation without modifying a model implementation to extract its intermediate feature.

However, the FX-based feature extractor disables some nn.Module features of its covered modules such as forward hook.
Here is a minimal example to demonstrate the issue. 


```python
import torch
from torch import nn
from torchvision import models
from torchvision.models.feature_extraction import create_feature_extractor
from torchvision.models._utils import IntermediateLayerGetter


def printnorm(self, input, output):
    # input is a tuple of packed inputs
    # output is a Tensor. output.data is the Tensor we are interested
    print('Inside ' + self.__class__.__name__ + ' forward')
    print('')
    print('input: ', type(input))
    print('input[0]: ', type(input[0]))
    print('output: ', type(output))
    print('')
    print('input size:', input[0].size())
    print('output size:', output.data.size())
    print('output norm:', output.data.norm())


resnet50 = models.resnet50()
return_layers = {'layer4': 'out'}
backbone_fx = create_feature_extractor(resnet50, return_layers)
backbone_org = IntermediateLayerGetter(resnet50, return_layers)


backbone_fx.layer1.register_forward_hook(printnorm)
backbone_org.layer1.register_forward_hook(printnorm)

x = torch.rand(2, 3, 224, 224)
# FX-based feature extractor doesn't call forward hook
z_fx = backbone_fx(x)

# IntermediateLayerGetter calls forward hook and print messages
z_org = backbone_org(x)
'''
Inside Sequential forward

input:  <class 'tuple'>
input[0]:  <class 'torch.Tensor'>
output:  <class 'torch.Tensor'>

input size: torch.Size([2, 64, 56, 56])
output size: torch.Size([2, 256, 56, 56])
output norm: tensor(1973.4232)
'''
```

Could you please enable the features for those wrapped by FX-based feature extractor?
Thank you!

### Versions

Collecting environment information...
PyTorch version: 1.10.0+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.8.10 (default, Nov 26 2021, 20:14:08)  [GCC 9.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-94-generic-x86_64-with-glibc2.29
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] pytorch-msssim==0.2.1
[pip3] torch==1.10.0
[pip3] torchdistill==0.1.4
[pip3] torchvision==0.11.1
[conda] Could not collect
"
Streamline contributing experience,pytorch/vision,2022-01-06 09:36:37,6,needs discussion,5168,1095134847,"Related to #5167.

---

The largest part of our [contribution guidelines](https://github.com/pytorch/vision/blob/main/CONTRIBUTING.md) explain contributors how to work on `torchvision`. Due to sheer amount of techniques and tools that can be quite overwhelming for a new contributor. 

I'm going to use `$TOOL` as a placeholder for what I imagine:

- `$TOOL install`: Installs `torch` from nightlies and builds `torchvision` in the current environment
- `$TOOL format`: Runs autoformatters 
- `$TOOL lint`: Runs linters
- `$TOOL test`: Runs tests
- `$TOOL docs`: Builds documentation

There are a couple of tools out there that do this. Of the top of my head we could use [`pydoit`](https://pydoit.org/), [`tox`](https://tox.wiki/en/latest/), or simply `make`. AFAIK, all of them allow parametrization of the top-level commands so we can provide sensible default while not losing flexibility. For example, `$TOOL test` will run all tests by default, but dispatches additional arguments directly to `pytest`.

I can do more in depth research about the tools and which would be the best fit for us if we see this proposal as net-benefit in general."
Split autoformatters and linters into different workflows and CI jobs,pytorch/vision,2022-01-06 09:22:18,5,needs discussion#module: ci#code quality,5167,1095123256,"## Status quo

Currently we mix formatters and linters with `pre-commit`:

### formatters

https://github.com/pytorch/vision/blob/578c1546c328c68e601bfd2e2309ed175ee343d9/.pre-commit-config.yaml#L9-L11

https://github.com/pytorch/vision/blob/578c1546c328c68e601bfd2e2309ed175ee343d9/.pre-commit-config.yaml#L23-L26

### linters

https://github.com/pytorch/vision/blob/578c1546c328c68e601bfd2e2309ed175ee343d9/.pre-commit-config.yaml#L5-L8

https://github.com/pytorch/vision/blob/578c1546c328c68e601bfd2e2309ed175ee343d9/.pre-commit-config.yaml#L31-L32

https://github.com/pytorch/vision/blob/578c1546c328c68e601bfd2e2309ed175ee343d9/.pre-commit-config.yaml#L37

In addition we have a separate CI job that lints with `mypy`

https://github.com/pytorch/vision/blob/578c1546c328c68e601bfd2e2309ed175ee343d9/.circleci/config.yml#L298

## Proposal

I propose we change the rationale from the two CI jobs from `pre-commit` and `mypy` to code format and lint. That means, we would only keep autoformatters in our `pre-commit` configuration and move all linters to what is currently the `mypy` job. 

### Pros

- We could use `pre-commit` as ""single source of truth"" for formatting code. Currently they mentioned as ""purely optional"" in our contribution guide. Since `pre-commit` supports running the hooks manually, we can simply treat it as our way to bundle all autoformatters while the user does not need to know or care what exactly is run.
- We could simply add new autoformatters if they are available through `pre-commit`, e.g. #5158. If anyone applies code formatting through `pre-commit` anyway, that won't break any workflow.

### Cons

- We would loose the ability to run linters in a bundled manner. I don't think this is a strong con, since linters such as `flake8` or `pydocstyle` trigger very seldom anyway due to the auto-formatting. Plus, we already need to run `mypy` separately.

cc @seemethere"
fix JPEG reference tests,pytorch/vision,2022-01-04 15:38:54,3,needs discussion#module: tests#dependency issue,5162,1093475792,"We are currently skipping JPEG reference tests on Windows, since the Windows wheels of `Pillow` are built against `libjpeg-turbo` while we build `torchvision` against `libjpeg`

https://github.com/pytorch/vision/blob/bbeb32035c0df752a0fe0cd5921de537e7f68d72/test/test_image.py#L481

As of [`Pillow==9.0.0`](https://pillow.readthedocs.io/en/stable/releasenotes/9.0.0.html#switched-to-libjpeg-turbo-in-macos-and-linux-wheels), all wheels are now built against `libjpeg-turbo`, which leads to the [CI failures](https://app.circleci.com/pipelines/github/pytorch/vision/13688/workflows/8d5ddae1-d149-4114-888d-8713aa668626/jobs/1101779) that are currently happening. Thus, these tests will now fail for every OS or at least only succeed by chance. 

There are multiple ways out in descending order of preference by me:

1. Install `Pillow` from conda-forge rather from PyPI to enforce the same JPEG library for `torchvision` and `Pillow`.
2. Pin `Pillow<9` for unittest CI. This basically preserves the behavior we had before.
3. Remove these tests completely.

The only downside of 1. that I see is that `Pillow` on conda-forge is released later than on PyPI. IIUC, the conda-forge package is community managed and they only start building when it is released on PyPI. If there are build failures it might take some time until the versions converge again. In the meantime, our tests would not pick up on any incompatibilities with `Pillow` in general not just for JPEG I/O. Thus, we would need to rely on user feedback that something is not working.

Given that this is an dependency issue, I would be fine with waiting for user reports in the time of `Pillow` version divergence between conda-forge and PyPI. If we don't want to accept that, we could also hide all JPEG compatibility tests behind a runtime flag and add another CI job that only runs these tests while enforcing 1. from above.


cc @pmeier @NicolasHug @datumbox "
[RFC] Make `transforms.functional` methods differential w.r.t. their parameters,pytorch/vision,2022-01-04 10:47:31,1,needs discussion#module: transforms#new feature,5157,1093229269,"This is an RFC that continues the discussion #5000 by @ain-soph and PRs: #4995 and #5110 on updating functional tensor methods from `F.*` to accept learnable parameters (tensors with `requires_grad=True`) and propagating the gradient.

For the motivation and the context, please see https://github.com/pytorch/vision/issues/5000

### Proposal

Torchvision transformations can work on PIL images and torch Tensors and accept scalars, list of scalars as parameters. For example,
```python
x = torch.rand(1, 3, 32, 32)
alpha = 45
center = [1, 2]
out = F.rotate(x, alpha, interpolation=InterpolationMode.BILINEAR, center=center)
# out is tensor
```

The proposal is to be able to learn parameters like `alpha` and `center` using gradients descent:
```diff
x = torch.rand(1, 3, 32, 32)
- alpha = 45
+ alpha = torch.tensor(45.0, requires_grad=True)
- center = [1, 2]
+ center = torch.tensor([1.0, 2.0], requires_grad=True)]
out = F.rotate(x, alpha, interpolation=InterpolationMode.BILINEAR, center=center)
# out is tensor that requires grad
assert out.requires_grad

# parameters can have grads:
out.mean().backward()  # some dummy criterion
assert alpha.grad is not None
assert center.grad is not None
```
and also keep previous API (no BC breaking changes).

### Implementation

In terms of API, it would require updates like:
```diff
def rotate(
    img: Tensor,
-   angle: float,
+   angle: Union[float, int, Tensor],
    interpolation: InterpolationMode = InterpolationMode.NEAREST,
    expand: bool = False,
-   center: Optional[List[int]] = None,
+   center: Optional[Union[List[int], Tuple[int, int], Tensor]] = None,
    fill: Optional[List[float]] = None,
    resample: Optional[int] = None,
) -> Tensor:
```

Note: we need to keep transforms torch jit scriptable and thus we can also be limited by what is supported by torch jit script (simply adding `Union[float, Tensor]` does not always work and can break compatibility with the stable version).

In terms of implementation, we have to ensure that: 
- methods with updated parameters still support all previous data types
- methods are torch jit scriptable
- methods verify that input image is float tensor (no grad propagation otherwise)
- methods propagate grads for tensor inputs <=> all internal ops for tensor branch are propagating grads
- only floating parameters can accept values as Tensors
  - for example, rotate with learnable floating angle
  - IMO, we can't make output (integer) size learnable in resize op (please fix me if there is a way)
  - certain integer parameters can be promoted to float, e.g. translate in affine

Example with affine and rotate ops : #5110

### Transforms to update

- [ ] [normalize](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L320), params: mean and std
- [ ] [adjust_brightness](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L812), params: brightness_factor
- [ ] [adjust_contrast](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L834), params: contrast_factor
- [ ] [adjust_saturation](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L856), params: saturation_factor
- [ ] [adjust_hue](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L878), params: hue_factor
- [ ] [adjust_gamma](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L914), params: gamma, gain
- [ ] [rotate](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L998), params: angle, center, #5110 
- [ ] [affine](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L1078), params: angle, translate, scale, shear, #5110 
- [ ] [gaussian_blur](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L1268), params: kernel_size, sigma ?
- [ ] [posterize](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L1355), params: bits ?
- [ ] [solarize](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L1379), params: threshold
- [ ] [adjust_sharpness](https://github.com/pytorch/vision/blob/e65a857b5487a8493bc8a80a95d64d9f049de347/torchvision/transforms/functional.py#L1399), params: sharpness_factor

Please comment here if I'm missing any op that we could add into the list.








cc @vfdev-5 @datumbox"
GPU decoder refactoring,pytorch/vision,2021-12-30 19:53:42,0,module: video#code quality,5148,1091279743,"https://github.com/pytorch/vision/pull/5019#discussion_r776814346
https://github.com/pytorch/vision/pull/5019#discussion_r776807823
https://github.com/pytorch/vision/pull/5019#discussion_r776807411
https://github.com/pytorch/vision/pull/5019#discussion_r776804991
https://github.com/pytorch/vision/pull/5019#discussion_r776802688
https://github.com/pytorch/vision/pull/5019#discussion_r776800733
https://github.com/pytorch/vision/pull/5019#discussion_r776800512
https://github.com/pytorch/vision/pull/5191/files#r786020984"
Run GPU decoding tests in CI,pytorch/vision,2021-12-30 19:50:23,0,enhancement#module: video,5147,1091278614,https://github.com/pytorch/vision/pull/5019#issuecomment-1003133991
guard cuCtxPushCurrent / cuCtxPopCurrent with a RAII-style guard,pytorch/vision,2021-12-30 19:44:04,0,enhancement#module: video,5144,1091276380,https://github.com/pytorch/vision/pull/5019#discussion_r776799312
Add Windows support,pytorch/vision,2021-12-30 19:38:45,0,enhancement#module: video,5143,1091274557,Support GPU decoding on Windows.
Support reading video from memory,pytorch/vision,2021-12-30 19:37:43,0,enhancement#module: video,5142,1091274181,Add support for reading video from memory in GPU decoding.
Return pts per frame after video decoding on GPU,pytorch/vision,2021-12-30 19:34:25,1,enhancement#module: video,5140,1091273026,
torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow for some published model using BatchNorm2d,pytorch/vision,2021-12-16 04:15:02,5,,5104,1081752140,"### 🐛 Describe the bug

Using the new torch fx feature extractor has trouble with the standard module BatchNorm2d (and presumably others?). Edited as original report was not quite correct.



```
import torch
from structs.torch import shape
from torchvision.models import feature_extraction, resnet18
import timm

model = timm.create_model('regnetx_004')
input = torch.randn((1, 3, 224, 224))

extractor = feature_extraction.create_feature_extractor(model, [""s1"", ""s2"", ""s3"", ""s4""])
x = extractor(input)

```


```
  File ""/home/oliver/miniconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py"", line 135, in forward
    self._check_input_dim(input)
  File ""/home/oliver/miniconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py"", line 406, in _check_input_dim
    if input.dim() != 4:
  File ""/home/oliver/miniconda3/lib/python3.9/site-packages/torch/fx/proxy.py"", line 251, in __bool__
    return self.tracer.to_bool(self)
  File ""/home/oliver/miniconda3/lib/python3.9/site-packages/torch/fx/proxy.py"", line 152, in to_bool
    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')
torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow
```
Code in question:

```
    def _check_input_dim(self, input):
       if input.dim() != 4:
            raise ValueError(""expected 4D input (got {}D input)"".format(input.dim()))
```

### Versions

Collecting environment information...
PyTorch version: 1.10.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-91-generic-x86_64-with-glibc2.31
Is CUDA available: True
CUDA runtime version: 11.4.120
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070
Nvidia driver version: 470.86
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.1
"
Faster R-CNN training with negative images(non-object image),pytorch/vision,2021-12-14 10:33:21,0,,5098,1079586981,"### 📚 The doc issue

I'm using torchvision.models.detection.fasterrcnn to train my custom dataset.
For annotations that do not have any bounding box, I use target [0, 0, 0, 0], label 0.
But it return ValueError(All bounding boxes should have positive height and width).

And when I use target [0, 0, 0.5, 0.5], label 0.
But it returns nan Loss..

Is there any solution for this problem?

### Suggest a potential alternative/fix

_No response_"
[FEEDBACK] Multi-weight support prototype API,pytorch/vision,2021-12-11 09:53:51,42,,5088,1077488522,"### 🚀 Feedback Request

This issue is dedicated for collecting community feedback on the Multi-weight support API. Please review the [dedicated article](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/) where we describe the API in detail and provide an overview of its features.

We would love to get your thoughts, comments and input in order to finalize the API and include it on the new release of TorchVision.

cc @oke-aditya @frgfm @zhiqwang @xiaohu2015"
[RFC] How should datasets handle decoding of files?,pytorch/vision,2021-12-09 09:20:02,7,enhancement#needs discussion#module: datasets#prototype,5075,1075340159,"It is a common feature request (for example #4991) to be able to disable the decoding when loading a dataset. To solve this we added a `decoder` keyword argument to the load mechanism (`torchvision.prototype.datasets.load(..., decoder=...)`). It takes an `Optional[Callable]` with the following signature:

```python
def my_decoder(buffer: BinaryIO) -> torch.Tensor: ...
```

If it is a callable, it will be passed a buffer from the dataset and the result will be integrated into the sample dictionary. If the decoder is `None` instead, the buffer will be integrated in the sample dictionary instead, leaving it to the user to decode.

https://github.com/pytorch/vision/blob/4cacf5a19f68f6b6483c758e3ac95d1dd9b6194c/torchvision/prototype/datasets/_builtin/imagenet.py#L132-L134

This works well for images, but already breaks down for videos as discovered in #4838. The issue is that decoding a video results in more information than a single tensor. The tentative plan in #4838 was to change the signature to

```python
def my_decoder(buffer: BinaryIO) -> Dict[str, Any]: ...
```

With this, a decoder can now return arbitrary information, which can be integrated in the top level of the sample dictionary.

Unfortunately, looking ahead, I don't think even this architecture will be sufficient. Two issues came to mind:

1. The current signature assumes that there is only one type of payload to decode in a dataset, i.e. images or videos. Other types, for example annotation files stored as `.mat`, `.xml`, or `.flo`, will always be decoded. Thus, the user can't completely deactivate the decoding after all. Furthermore, they can also not use any custom decoding for these types if need be.
2. The current signature assumes that all payloads of a single type can be decoded by the same decoder. Counter examples to this are the HD1K optical flow datasets that uses 16bit `.png` images as annotations which have sub-par support by `Pillow`.

To overcome this, I propose a new architecture that is similar to the [`RoutedDecoder` datapipe](https://github.com/pytorch/pytorch/blob/d44d59aa70c71091d878e4b0ced963e3074727cd/torch/utils/data/datapipes/iter/routeddecoder.py#L13-L14). We should have a `Decoder` class that has a sequence of `Handler`'s (name up for discussion):

```python
class Decoder:
    def __init__(
        self,
        *handlers: Callable[[str, BinaryIO], Optional[str, Any]],
        must_decode: bool = True,
    ):
        self.handlers = handlers
        self.must_decode = must_decode

    def __call__(
        self,
        path: str,
        buffer: BinaryIO,
        *,
        prefix: str = """",
        include_path: bool = True,
    ) -> Dict[str, Any]:
        for handler in self.handlers:
            output = handler(path, buffer)
            if output is not None:
                break
        else:
            if self.must_decode:
                raise RuntimeError(
                    f""No handler was responsible for decoding the file {path}.""
                )
            output = {(f""{prefix}_"" if prefix else """") + ""buffer"": buffer}

        if include_path:
            output[(f""{prefix}_"" if prefix else """") + ""path""] = path

        return output
```

If called with a path-buffer-pair the decoder iterates through the registered handlers and returns the first valid output. Thus, each handler can determine based on the path if it is responsible for decoding the current buffer. By default, the decoder will bail if no handler decoded the input. This can be relaxed by the `must_decode=False` flag (name up for discussion), which is a convenient way to have a non-decoder.

We would need to change `datasets.load` function to

```python
def load(
    ...,
    decoder: Optional[
        Union[
            Decoder,
            Callable[[str, BinaryIO], Optional[str, Any]],
            Sequence[Callable[[str, BinaryIO], Optional[str, Any]]],
        ]
    ] = ()
):
    ...
    if decoder is None:
        decoder = Decoder(must_decode=False)
    elif not isinstance(decoder, Decoder):
        decoder = Decoder(
            *decoder if isinstance(decoder, collections.abc.Sequence) else decoder,
            *dataset.info.handlers,
            *default_handlers,
        )
    ...
```

By default the user would get the dataset specific handlers as well as the default ones. By supplying custom ones, they would be processed with a higher priority and thus overwriting the default behavior if needs be. If `None` is passed we get a true non-encoder. Finally, by passing a `Decoder` instance the user has full control over the behavior.

Within the dataset definition, the call to the `decoder` would simply look like

```python
path, buffer = data

sample = dict(...)
sample.update(decoder(path, buffer))
```

or, if multiple buffers need to be decoded,

```python
image_data, ann_data = data
image_path, image_buffer = image_data
ann_path, ann_buffer = ann_data

sample = dict()
sample.update(decoder(image_path, image_buffer, prefix=""image""))
sample.update(decoder(ann_path, ann_buffer, prefix=""ann""))
```


cc @pmeier @bjuncek"
[feature request] Connected components labeling and some morphological operations in general,pytorch/vision,2021-12-08 15:30:46,2,,5064,1074535729,"### 🚀 The feature

Some useful morphological operations would be nice to have in the core (and on CUDA):
- Connected components labeling (hard to do with existing primitives): skimage.measure.label, pytorch cuda impl: https://github.com/zsef123/Connected_components_PyTorch
- Labeled component areas (related to batched bincount/histc)
- Batched retrieval of unique labels count
- Dilation / erosion (can be modeled with max/min pooling)

### Motivation, pitch

N/A

### Alternatives

_No response_

### Additional context

_No response_"
Type annotations of normalize() are incorrect,pytorch/vision,2021-12-07 13:29:00,1,,5046,1073354383,"I'm seeing the following typing error:

```
torchvision/prototype/transforms/_presets.py:114: error: Argument ""mean"" to
""normalize"" has incompatible type ""float""; expected ""List[float]""  [arg-type]
            img1 = F.normalize(img1, mean=0.5, std=0.5)
```

This is incorrect, passing single numbers is explicitly supported by `noramlize()`

https://github.com/pytorch/vision/blob/47bd962069ba03f753e7ba711cb825317be0b00a/torchvision/transforms/functional.py#L340-L345"
[Discussion] How do we want to handle `torchvision.prototype.features.Feature`'s?,pytorch/vision,2021-12-07 13:17:58,19,needs discussion#prototype,5045,1073343075,"This issue should spark a discussion about how we want to handle `Feature`'s in the future. There are a lot of open questions I'm trying to summarize. I'll give my opinion to each of them. You can find the current implementation under `torchvision.prototype.features`.

## What are `Feature`'s?

`Feature`'s are subclasses of `torch.Tensor` and their purpose is threefold:

1. With their type, e.g. `Image`, they information about the data they carry. The prototype transformations (`torchvision.prototype.transforms`) use this information to automatically dispatch an input to the correct kernel.
2. They can optionally carry additional meta data that might be needed for transforming the feature. For example, most geometric transformations can only be performed on bounding boxes if the size of the corresponding image is known.
3. They provide a convenient interface for feature specific functionality, for example transforming the format of a bounding box.

There are currently three `Feature`'s implemented

- `Image`,
- `BoundingBox`, and
- `Label`,

but in the future we should add at least three more:

- `SemanticSegmentationMask`,
- `InstanceSegementationMask`, and
- `Video`.

## What is the policy of adding new `Feature`'s?

We could allow subclassing of `Feature`'s. On the one hand, this would make it easier for datasets to conveniently bundle meta data. For example, the COCO dataset could return a `CocoLabel`, which in addition to the default `Label.category` could also have the `super_category` field. On the other hand, this would also mean that the transforms need to handle subclasses of features well, for example a `CocoLabel` could be treated the same as a `Label`.

I see two downsides with that:

1. What if a transform needs the additional meta data carried by a feature subclass? Imagine I've added a special transformation that needs `CocoLabel.super_category`. Although from the surface this now supports plain `Label`'s this will fail at runtime.
2. Documentation custom features is more complicated than documenting a separate field in the sample dictionary of a dataset.

Thus, I'm leaning towards only having a few base classes.

## From what data should a `Feature` be instantiable?

Some of the features like `Image` or `Video` have non-tensor objects that carry the data. Should these features know how to handle them? For example should something like `Image(PIL.Image.open(...))` work?

My vote is out for yes. IMO this is very convenient and also not an unexpected semantic compared to passing the data directly, e.g. `Image(torch.rand(3, 256, 256))`

## Should `Feature`'s have a fixed shape?

Consider the following table:

| `Feature`                   | `.shape`                      |
|-----------------------------|-------------------------------|
| `Image`                     | `(*, C, H, W)`                |
| `Label`                     | `(*)`                         |
| `BoundingBox`               | `(*, 4)`                      |
| `SemanticSegmentationMask`  | `(*, H, W)` or `(*, C, H, W)` |
| `InstanceSegementationMask` | `(*, N, H, W)`                |
| `Video`                     | `(*, T, C, H, W)`             |

(For `SemanticSegmentationMask` I'm not sure about the shape yet. Having an extra channel dimension makes the tensor unnecessarily large, but it aligns well with segmentation image files, which are usually stored as RGB)

Should we fix the shape to a single feature, i.e. remove the `*` from the table above, or should we only care about the shape in the last dimensions to be correct?

My vote is out for having a flexible shape, since otherwise batching is not possible. For example, if we fix bounding boxes to shape `(4,)` a transformation would need to transform `N` bounding boxes individually, while for shape `(N, 4)` it could make use of parallelism.

On the same note, if we go for the flexible shape, do we keep the singular name of the feature? For example, do we regard a batch of images with shape `(B, C, H, W)` still as `Image` or should we go for the plural `Images` in general? My vote is out for always keeping the singular, since I've often seen something like:

```python
for image, target in data_loader(dataset, batch_size=4):
    ...
```

## Should `Feature`'s have a fixed dtype?

This makes sense for `InstanceSegementationMask` which should always be `torch.bool`. For all the other features I'm unsure. My gut says to use a default dtype, but also allow other dtypes.

## What meta data should `Feature`'s carry?

IMO, this really depends on the decision above about the fixed / flexible shapes. If we go for fixed shapes, it can basically carry any information. If we go for flexible shapes instead, we should only have meta data, which is the same for batched features. For example, `BoundingBox.image_size` is fine, but `Label.category` is not.

## What methods should `Feature`'s provide?

For now I've only included typical conversion methods, but of course this is not exhaustive.

| `Feature`                   | method(s)          |
|-----------------------------|--------------------|
| `Image`                     | `.to_dtype()`      |
|                             | `.to_colorspace()` |
| `Label`                     | `.to_str()`        |
| `BoundingBox`               | `.to_format()`     |
| `InstanceSegementationMask` | `.to_semantic()`   |


cc @bjuncek"
Legacy interface for prototype datasets?,pytorch/vision,2021-12-07 08:41:35,7,enhancement#needs discussion#module: datasets#prototype,5040,1073073889,"The prototype datasets change the interface in two ways:

1. The input parameters are a little different in some cases. For example in the current API the MNIST dataset would be instantiated with `datasets.MNIST(..., train=True)` whereas now it looks like `datasets.load(""mnist"", split=""train"")`. 
2. The output is completely different. Before we returned a tuple (sometimes of varying length) whereas now we always return a dictionary. Furthermore, before we used `PIL` images and `numpy` arrays as return types, whereas now we always use tensor subclasses. 

To lower the burden to move to the new style datasets a little, we could have a `legacy: bool = False` keyword argument on `datasets.load()`. For all datasets that have a legacy variant, we could simply implement two functions that map the input and output. If a dataset doesn't support a legacy variant, we could simply error out.

cc @pmeier @bjuncek"
Trying to CIFAR-10 from torchvision.datasets and receiving URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>,pytorch/vision,2021-12-06 18:19:10,5,needs reproduction#module: datasets,5039,1072857915,"## 🐛 Bug

Trying to download CIFAR-10 from torchvision.datasets and receiving `URLError:<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)> when downloading` Download fails

## To Reproduce

Steps to reproduce the behavior:

`dataset = torchvision.datasets.CIFAR10(root='dataset/',train=True,transform=transform,download=True)`

## Expected behavior
Pre-trained models should be downloaded without certificate failures

## Environment

PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Pro
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19044-SP0
Is CUDA available: True
CUDA runtime version: 11.2.67

GPU models and configuration: GPU 0: NVIDIA GeForce GT 1030
Nvidia driver version: 465.89
cuDNN version: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2\bin\cudnn_ops_train64_8.dll
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5

[pip3] pytorch-lightning==1.5.1

[pip3] torch==1.9.1+cu111

[pip3] torch-tb-profiler==0.2.1

[pip3] torchaudio==0.9.1

[pip3] torchmetrics==0.6.0

[pip3] torchvision==0.10.1+cu111



cc @pmeier @YosuaMichael @fmassa @vfdev-5"
To do GeneralizedRCNNTrasnform.py similar to Detectron2 ,pytorch/vision,2021-12-06 16:01:57,6,needs discussion#module: models#module: transforms#topic: object detection,5034,1072319076,"### 🚀 The feature

Now with GeneralizedRCNN.py in faster_rcnn.py and retinanet.py you can to specify

MIN_SIZE_TRAIN
MIN_SIZE_TRAIN_SAMPLING = [choice, range]
MAX_SIZE_TRAIN
MIN_SIZE_TEST
MAX_SIZE_TEST

where

- MIN_SIZE_TRAIN: Size of the smallest side of the image during training
- MIN_SIZE_TRAIN_SAMPLING: Sample size of smallest side by choice or random selection from range give by INPUT.MIN_SIZE_TRAIN
- MIN_SIZE_TEST: Size of the smallest side of the image during testing
- MAX_SIZE_TEST: Maximum size of the side of the image during testing

This is a similar as detectron2

### Motivation, pitch

I have been working in a project where needed to train in a complete range of resolutions and not testing in the biggest one of min attribute which is the default behaviur of the **GeneralizedRCNNTrasnform.py**. So, inplementing this new features allows you:

1. To use different inputs to train and testing with {MIN,MAX}_SIZE_TRAIN and {MIN,MAX}_SIZE_TEST attributes as Detectron2 does
2. Not just specify several input resolutions to train but ranges of resolution. In order to do this, the user can use MIN_SIZE_TRAIN and MIN_SIZE_TRAIN_SAMPLING = 'range' as Detectron2 does.

So i would like to contribute in this implementation. For this i would use just the Torch library



### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox @vfdev-5"
r2plus1d - strange relationship with inference batch size and inference speed,pytorch/vision,2021-11-30 21:36:16,0,,5016,1067698382,"I'm seeing a weird relationship between inference batch size and inference speed that I can't explain.  I have an r2plus1d resnet18 model that I trained.  Input size is 8x32x32x3. I've been running some tests with fake data (random arrays) to measure inference speed as a function of batch size. For some reason, when the batch size increases from 6 to 7, the throughput suddenly drops significantly. It then slowly increases with the batch size until going from batch size of 24 to 25, at which point the throughput nearly doubles.  I'm running this on a machine with one RTX 2080 with no other major processes running at the same time. I can run the experiment multiple times and I see the same phenomenon. I'm using Torch 1.9.1+cu102 and Torchvision 0.10.1+cu102. If anyone has any insight that might explain what I'm seeing, I'd appreciate it. 

[![r2plus1d-timing-8frames.png](https://i.postimg.cc/T1bmwRdR/r2plus1d-timing-8frames.png)](https://postimg.cc/LgHhQFg7)"
[feature request] Make `transforms.functional_tensor` functions differential w.r.t. their parameters,pytorch/vision,2021-11-28 22:24:15,1,enhancement#module: transforms,5000,1065453263,"### 🚀 The feature

Make operations in `torchvision.transforms.functional_tensor` differential w.r.t. hyper-parameters, which is helpful for Faster AutoAugment search (hyper-parameters are learnable parameters via backward). **(while keeping the backward compatibility to previous codes)**

Some operations are not differential (e.g., Posterize), which might require users to write their own implementations.


### Motivation, pitch

The main motivation is for research purpose. [Faster Autoaugment](https://arxiv.org/abs/1911.06987) proposes to search for augment architectures using a DARTS-like framework, and all magnitudes and weights are trainable parameters. This requires all operations to have gradients w.r.t. magnitudes. This idea provides a faster search strategy as state-of-the-art AutoAugment policy search algorithms.  
This work has been maintained by [autoalbument](https://github.com/albumentations-team/autoalbument) and applied on some industrial scenarios from their document claims.

I think adding the backward feature wrt magnitudes would be more convenient and support future research as well.


### Alternatives

_No response_

### Additional context

Linked PR: #4995

cc @vfdev-5 @datumbox"
"Adding a list of model's param size( MB ) and layernum of models in torchvision with a standard input shape like 3,224,224",pytorch/vision,2021-11-26 10:29:17,1,enhancement#module: documentation,4996,1064317636,"### 📚 The doc issue

Torchvision is lacking a table of models' size and layernum

### Suggest a potential alternative/fix

Recently, I'm working on choosing some models from torchvision.
And as we all know, a model's size and layers are critical to many aspects.
Especially for beginners who don't familiar with these models, this table can help a lot.
If you think this is necessary, I'm glad to help collect these models' size and layernum and pull a request"
Documentation and examples for prototype torchvision,pytorch/vision,2021-11-25 17:59:45,5,module: documentation#prototype,4992,1063836766,"### 📚 The doc issue

First of all, hats off to all the maintainers @fmassa @datumbox @NicolasHug @pmeier  It's been a great revamp, and I think all the new features being added are great!
If software engineering is an art then torchvision is Picaso!

Any thoughts or ideas on how to effectively use the new prototype torchvision? The new models, datasets and transforms.
As an end user perspective 

It would great to read documentation on why these were created over legacy ones and what are the benefits.

Also examples pertaining to same would be nice, maybe before next release ?

Also porting the current examples to the new APIs?


### Suggest a potential alternative/fix

P.S.
I would be glad to contribute :smile: It will take some time for me to dive into the codebase and understand though. But I'm up for it.




cc @bjuncek"
[feature request] Batched encode_jpeg,pytorch/vision,2021-11-24 20:32:31,0,enhancement#module: io,4988,1062870688,"### 🚀 The feature

If it can leverage parallelism, will be useful for speeding-up preparing HTML-encoded visualizations with many embedded jpegs.

### Motivation, pitch

N/A

### Alternatives

_No response_

### Additional context

_No response_"
Move image loading logic to _load_image in VOC datasets (similar to COCO),pytorch/vision,2021-11-22 19:20:26,3,,4975,1060510691,"### 🚀 The feature

This would allow simple stubbing out image loading if needed for fast dataset transformed metadata access (when image reading is not needed) by `dataset._load_image = lambda _: None` and also make API more consistent across datasets.

It's also impossible to monkey-patch `__getitem__` at instance level: https://stackoverflow.com/questions/57413453/is-it-possible-to-override-getitem-at-instance-level-in-python :(

### Motivation, pitch

N/A

### Alternatives

_No response_

### Additional context

_No response_"
Move TorchVision App Demo to `pytorch_android_lite`,pytorch/vision,2021-11-17 20:26:56,0,enhancement#module: models#topic: mobile,4953,1056601739,"### 🚀 The feature

Currently we depend our Android Demo app depends on `pytorch_android`. It's recommended to switch to `pytorch_android_lite`.

Transitioning to the lite version requires us to update the following things:
- Change the exported file from `frcnn_mnetv3.pt` to `frcnn_mnetv3.ptl`
- Export the `fasterrcnn_mobilenet_v3_large_320_fpn` model using `_save_for_lite_interpreter()` 
- Update the dependencies to `org.pytorch:pytorch_android_lite` and `org.pytorch:pytorch_android_torchvision_lite`
- Replace `Module.load()` with `LiteModuleLoader.load()` on the code-base

### Motivation, pitch

The recommended dependency for mobile is `pytorch_android_lite`.

### Alternatives

_No response_

### Additional context

There are a few bugs that block this at the moment:
- The release pipeline for `pytorch_android` is buggy. The `-SNAPSHOT` version contains only the `_lite` binaries and there is no snapshots for `pytorch_android_lite`. 
- There seems to be a bug on the serialization mechanism that exports the model. See https://github.com/pytorch/pytorch/issues/68536

cc @IvanKobzarev"
conversion to coco format changes bboxes in dataset,pytorch/vision,2021-11-16 16:03:09,0,,4943,1055052506,"### 🐛 Describe the bug

I'm following the object detection tutorial [here](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) which uses the coco evaluation metrics.

It uses `convert_to_coco_api` in  https://github.com/pytorch/vision/blob/main/references/detection/coco_utils.py to convert a dataset to coco format.

But when the bounding boxes are preloaded into memory in a dataset (i.e. not generated on the fly like in the tutorial) this conversion changes them in place. So the dataset gets modified unexpectedly.
 I think it happens here :
[`bboxes[:, 2:] -= bboxes[:, :2]`](https://github.com/pytorch/vision/blob/1deb2ec22796350801aea44a000e4669cbf2fe1c/references/detection/coco_utils.py#L160)
and could be fixed by doing a copy first.

### Versions

not applicable"
torchvision.roi_align performance optimization with openMP ,pytorch/vision,2021-11-15 09:31:14,6,module: ops,4935,1053411906,"### 🚀 The feature

Looking at the implementation of roi_align_kernel, it seems as if this can be further optimized using openmp parallelization

https://github.com/pytorch/vision/blob/840ad8abd60b76d340ae0bde33e2230fad38e95a/torchvision/csrc/ops/cpu/roi_align_kernel.cpp#L27

Here's what can be done to get performance boost:
1. Added `#pragma omp parallel for` to the kernel (line 27)
2. Added -fopenmp as CFLAG to the compilation
3. Set torch.set_num_threads() to desired num of OMP threads (on test/WL side).



### Motivation, pitch

I did some experimentation locally in which:
- I've added this optimization 
- Built a small test case that calls roi_align 
- Profiled `torchvision.ops.roi_align()` and measured time using current implementation vs. 18 threads on simple CLX machine. 

On my humble experiments it shows 10X performance boost! 


### Alternatives

There can be other libraries/tooling that can do optimization to this CPU kernel. One can think of oneTBB or something alike. 
Nevertheless, the current implementation is a really naive and can easily be much performant.

### Additional context

_No response_"
torchvision conda package missing requests dependency,pytorch/vision,2021-11-11 16:45:01,10,,4917,1051168780,"### 🐛 Describe the bug

torchvision depends on requests, but it is not specified as conda package dependency, which results in the following failures:
```
% python
Python 3.8.11 (default, Aug  6 2021, 08:56:27) 
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from torchvision import datasets
>>> calldetch_ds = datasets.Caltech101(""datasets"", download=True)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/nshulga/miniconda3/envs/py_38/lib/python3.8/site-packages/torchvision/datasets/caltech.py"", line 52, in __init__
    self.download()
  File ""/Users/nshulga/miniconda3/envs/py_38/lib/python3.8/site-packages/torchvision/datasets/caltech.py"", line 124, in download
    download_and_extract_archive(
  File ""/Users/nshulga/miniconda3/envs/py_38/lib/python3.8/site-packages/torchvision/datasets/utils.py"", line 427, in download_and_extract_archive
    download_url(url, download_root, filename, md5)
  File ""/Users/nshulga/miniconda3/envs/py_38/lib/python3.8/site-packages/torchvision/datasets/utils.py"", line 135, in download_url
    return download_file_from_google_drive(file_id, root, filename, md5)
  File ""/Users/nshulga/miniconda3/envs/py_38/lib/python3.8/site-packages/torchvision/datasets/utils.py"", line 204, in download_file_from_google_drive
    import requests
ModuleNotFoundError: No module named 'requests'
```

### Versions

0.11.1 on MacOS"
Assumption about bounds in floating point images,pytorch/vision,2021-11-11 15:16:39,3,module: documentation,4915,1051085472,"Took me a while to figure out while the [ColorJitter](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ColorJitter) transform produced strange results (mostly fully saturated images) until I realized that there is a hard coded assumption that images of type float are bounded to max value 1.0.

Might either need a warning in the doc, or even better, the bound could be provided as an argument:
https://github.com/pytorch/vision/blob/a2f65656b2b9dd7cc4363c8ad4afcbf89a25952b/torchvision/transforms/functional_tensor.py#L314"
Unable to install 0.11.1 on macOS on arm64,pytorch/vision,2021-11-11 10:33:48,0,release-issue,4909,1050831630,"### 🐛 Describe the bug

There are no arm64 builds of 0.11.1 for macOS available on pip.

### Versions

```
PyTorch version: 1.10.0
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 12.0.1 (arm64)
GCC version: Could not collect
Clang version: 13.0.0 (clang-1300.0.29.3)
CMake version: version 3.21.4
Libc version: N/A

Python version: 3.8.12 (default, Oct 22 2021, 17:47:41)  [Clang 13.0.0 (clang-1300.0.29.3)] (64-bit runtime)
Python platform: macOS-12.0.1-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/AMIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.3
[pip3] torch==1.10.0
[conda] Could not collect
```"
Enhance torchvision's draw_keypoint method ,pytorch/vision,2021-11-10 15:29:17,6,enhancement#needs discussion#module: utils,4903,1049971679,"### 🚀 The feature

Follow up of #4216 . The current utility works fine, but we can make it better :)

### Motivation, pitch

I think the following can be added.

- [ ] Support multiple colors

There were 2 proposals one by @fmassa and one by me. The problem was handling excess colors.
I think that in fmassa's version we can broadcast to bigger dimensions and make it work?

- [ ] Support specifying line color.

In my opinion, we shouldn't complicate this and just allow a single string.
Happy to hear thoughts though.

- [ ] Plot the circle above the line or vice versa?

We currently plot the line above the circle. So at places I see that the line comes above the circle, somehow it looks like the line cuts / bisects the circle. I feel we should do the opposite.

- [ ] Don't use default white for all colors. Use either a random palette / better default one.

I guess we should extend this functionality to all plotting utils, by default both `draw_bounding_boxes` and `draw_keypoints` plot with white color. While segmentation masks uses a fixed color palette. 
Either we could use random colors / just have a nice fixed default palette.
(Also see #4658 )

cc @fmassa @datumbox @NicolasHug "
"RuntimeError: offset output dims: (160, 160) - computed output dims: (80, 80)",pytorch/vision,2021-11-10 11:25:30,7,module: ops,4899,1049706004,"### 🐛 Describe the bug

I run source build pytorch(torch==1.10.0) and torchvision(torchvision==0.12.0). 
train code snippet:
```
                from torchvision.ops import DeformConv2d as ModulatedDeformConv
                conv_op = ModulatedDeformConv
                offset_channels = 27
                self.conv2_offset = nn.Conv2d(
                    planes, deformable_groups * offset_channels,
                    kernel_size=3,
                    padding=1)
                self.conv2 = conv_op(
                    planes, planes, kernel_size=3, padding=1, stride=stride,
                    groups=deformable_groups, bias=False)

                offset_mask = self.conv2_offset(out)
                offset = offset_mask[:, :18, :, :]
                mask = offset_mask[:, -9:, :, :].sigmoid()
                out = self.conv2(out, offset, mask)
```
tensor shape below:
```
(Pdb) offset_mask.shape                                                                                                                                                                  
    torch.Size([16, 27, 160, 160])                                                                                                                                                           
(Pdb) offset.shape                                                                                                                                                                       
    torch.Size([16, 18, 160, 160])                                                                                                                                                           
(Pdb) mask.shape                                                                                                                                                                         
    torch.Size([16, 9, 160, 160])                                                                                                                                                            
(Pdb) self.conv2.__doc__                                                                                                                                                                 
    '\n    See :func:`deform_conv2d`.\n    '                                                                                                                                                 
(Pdb) out.shape                                                                                                                                                                          
    torch.Size([16, 128, 160, 160])                                                                                                                                                          
```

When training(run `out = self.conv2(out, offset, mask)` this line), issue the error below, pls help me, tks. 
```
Traceback (most recent call last):                                                                                                                                                       
  File ""/DB/train.py"", line 89, in <module>                                                                                                                                              
    main()                                                                                                                                                                               
  File ""/DB/train.py"", line 86, in main                                                                                                                                                  
    trainer.train()                                                                                                                                                                      
  File ""/DB/trainer.py"", line 115, in train                                                                                                                                              
    epoch=epoch, step=self.steps)                                                                                                                                                        
  File ""/DB/trainer.py"", line 138, in train_step                                                                                                                                         
    results = model.forward(batch, training=True)                                                                                                                                        
  File ""/DB/structure/model.py"", line 56, in forward                                                                                                                                     
    pred = self.model(data, training=self.training)                                                                                                                                      
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl                                                                                     
    return forward_call(*input, **kwargs)                                                                                                                                                
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 150, in forward                                                                                 
    return self.module(*inputs, **kwargs)                                                                                                                                                
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl                                                                                     
    return forward_call(*input, **kwargs)                                                                                                                                                
  File ""/DB/structure/model.py"", line 19, in forward                                                                                                                                     
    return self.decoder(self.backbone(data), *args, **kwargs)                                                                                                                            
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl                                                                                     
    return forward_call(*input, **kwargs)                                                                                                                                                
  File ""/DB/backbones/resnet.py"", line 242, in forward                                                                                                                                   
    x3 = self.layer2(x2)                                                                                                                                                                 
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl                                                                                     
    return forward_call(*input, **kwargs)                                                                                                                                                
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 141, in forward                                                                                      
    input = module(input)                                                                                                                                                                
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl                                                                                     
    return forward_call(*input, **kwargs)                                                                                                                                                
  File ""/DB/backbones/resnet.py"", line 161, in forward                                                                                                                                   
    out = self.conv2(out, offset, mask)                                                                                                                                                  
  File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl                                                                                     
    return forward_call(*input, **kwargs)                                                                                                                                                
  File ""/opt/conda/lib/python3.7/site-packages/torchvision-0.12.0a0+031e129-py3.7-linux-x86_64.egg/torchvision/ops/deform_conv.py"", line 177, in forward                                 
    mask=mask,                                                                                                                                                                           
  File ""/opt/conda/lib/python3.7/site-packages/torchvision-0.12.0a0+031e129-py3.7-linux-x86_64.egg/torchvision/ops/deform_conv.py"", line 106, in deform_conv2d                           
    use_mask,                                                                                                                                                                            
RuntimeError: offset output dims: (160, 160) - computed output dims: (80, 80)
```
tks~

### Versions

Collecting environment information...
PyTorch version: 1.10.0a0+git36449ea
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.21.4
Libc version: glibc-2.10

Python version: 3.7.6 (default, Jan  8 2020, 19:59:22)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-86-generic-x86_64-with-debian-buster-sid
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] intel-extension-for-pytorch==1.10.0+cpu
[pip3] numpy==1.21.2
[pip3] torch==1.10.0a0+git36449ea
[pip3] torchvision==0.12.0a0+031e129
[conda] blas                      1.0                         mkl  
[conda] intel-extension-for-pytorch 1.10.0+cpu               pypi_0    pypi
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-include               2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.2           py37h20f2e39_0  
[conda] numpy-base                1.21.2           py37h79a1101_0  
[conda] torch                     1.10.0a0+git36449ea          pypi_0    pypi
[conda] torchvision               0.12.0a0+031e129          pypi_0    pypi"
LabelMap not tranformed in tio.Subject when label shape is 1xHxWx1,pytorch/vision,2021-11-10 10:13:36,1,,4897,1049636053,"### 🐛 Describe the bug

Situation: In a dataloader a tio composition of intensity and spatial augmentation is performed.
When using 3D samples the tio.ScalarImage and tio.LabelMap are transformed fine, when using 2D samples the tio.LabelMap is not spatially transformed. This only happens when within the tio.Compose a intensity transform is applied along with spatial transform (either prior or after spatial transformation).

```
import torchio as tio

if self.yield_2d:            
    self.spatial_transform = tio.OneOf({
        tio.transforms.RandomAffine(.05): 0.8,
        tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=(7.5,7.5,0)): 0.2,
        },
        p=0.75,
    )
else:
    self.spatial_transform = tio.OneOf({
        tio.transforms.RandomAffine(.05): 0.8,
        tio.transforms.RandomElasticDeformation(num_control_points=7, max_displacement=7.5): 0.2,
        },
        p=0.75,
    )

self.transform = tio.Compose([
    self.intensity_transform,
    self.spatial_transform
])

# 2D image are dim=1xHxWx1 after unsqueezing
# 3D image are dim=1xDxHxW after unsqueezing

if yield_2d:
    image = image.unsqueeze(0).unsqueeze(-1)
    label = label.unsqueeze(0).unsqueeze(-1)
else:
    image = image.unsqueeze(0)
    label = label.unsqueeze(0)

subject = tio.Subject(
    image=tio.ScalarImage(tensor=image),  
    label=tio.LabelMap(tensor=label)
)
subject = self.spatial_transform(subject)
```

3D images are transformed fine:
![image](https://user-images.githubusercontent.com/13519345/141091929-f3bbc486-9d2f-486f-a0cc-3af31bf35d53.png)

2D images are not:
![image](https://user-images.githubusercontent.com/13519345/141092848-d9c4eb2d-7424-4238-a126-a98fafc1d9ef.png)



### Versions

Collecting environment information...
PyTorch version: 1.9.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: 3.8.0-2ubuntu4 (tags/RELEASE_380/final)
CMake version: version 3.5.1
Libc version: glibc-2.23

Python version: 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.4.0-210-generic-x86_64-with-glibc2.23
Is CUDA available: True
CUDA runtime version: 7.5.17
GPU models and configuration: 
GPU 0: Tesla T4
GPU 1: Tesla T4
GPU 2: GeForce RTX 2080 Ti
GPU 3: Quadro RTX 8000
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti

Nvidia driver version: 455.45.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8.1.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.1.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.1.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.1.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.1.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.1.0
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.1.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] torch==1.9.1
[pip3] torchio==0.18.63
[pip3] torchvision==0.10.1
[conda] Could not collect"
Windows torchvision-0.11.1 CUDA-10.2 wheel packages are not bundled with nvjpeg dll,pytorch/vision,2021-11-10 06:52:03,6,windows#topic: binaries#release-issue,4894,1049461971,"### 🐛 Describe the bug

Steps to reproduce:
1) Download TorchVision 0.11.1 windows wheel from https://download.pytorch.org/whl/cu102/torchvision/
2) Unpack the binary
3) Run `dumpbin /dependents image.pyd` to make sure it depends on `nvjpeg64_10.dll`:
```
C:\Users\circleci\Downloads\torchvision>dumpbin /dependents image.pyd
Microsoft (R) COFF/PE Dumper Version 14.00.24245.0
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file image.pyd

File Type: DLL

  Image has the following dependencies:

    api-ms-win-crt-heap-l1-1-0.dll
    api-ms-win-crt-runtime-l1-1-0.dll
    api-ms-win-crt-filesystem-l1-1-0.dll
    api-ms-win-crt-stdio-l1-1-0.dll
    libpng16.dll
    nvjpeg64_10.dll
    c10.dll
    torch_cpu.dll
    cudart64_102.dll
    c10_cuda.dll
    MSVCP140.dll
    KERNEL32.dll
    VCRUNTIME140_1.dll
    VCRUNTIME140.dll
    api-ms-win-crt-string-l1-1-0.dll
    api-ms-win-crt-environment-l1-1-0.dll

  Summary

        2000 .data
        3000 .pdata
       2E000 .rdata
        1000 .reloc
        1000 .rsrc
       3C000 .text

```
4) Noticesthat `nvjpeg64_10.dll` is missing from the archive

This causes `torchvision.io.read_image`  fail on Windows

### Versions

0.11.1 win+cu102

cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @mszhanyi"
Torchscript C++ Inference Error,pytorch/vision,2021-11-09 01:59:33,1,,4887,1048103438,"### 🐛 Describe the bug

I saved a Detectron2 Torchscript model using the following code:
```
fields = {
        ""proposal_boxes"": Boxes,
        ""objectness_logits"": Tensor,
        ""pred_boxes"": Boxes,
        ""scores"": Tensor,
        ""pred_classes"": Tensor,
        ""pred_masks"": Tensor,
        ""pred_keypoints"": torch.Tensor,
        ""pred_keypoint_heatmaps"": torch.Tensor,
    }
    torch_script_module = scripting_with_instances(torch_model, fields)
    extra_files = {}
    extra_files[""module_info.json""] = json.dumps({""input_names"": [""data""], ""output_names"": [""predicted""]})
    torch.jit.save(torch_script_module, os.path.join(outdir, ""model.pt""), _extra_files=extra_files)
```
However, when I tried to load it in C++, I'm getting the following error:
```
terminate called after throwing an instance of 'torch::jit::ErrorReport'
  what():
Unknown type name 'NoneType':
Serialized   File ""code/__torch__/detectron2/modeling/backbone/fpn.py"", line 4
  __parameters__ = []
  __buffers__ = []
  _is_full_backward_hook : NoneType
                           ~~~~~~~~ <--- HERE
  in_features : Tuple[str, str, str, str]
  _out_feature_strides : Dict[str, int]
```

I've tried to remove the `_is_full_backward_hook` field in `torch_script_module`, but seems like it will still be initialized as ""None"" when I load the model (in both Python and C++). What's the best way to resolve this issue?

### Versions

Collecting environment information...
PyTorch version: 1.9.0a0+gitdfbd030
Is debug build: False
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: version 3.5.1
Libc version: glibc-2.2.5

Python version: 3.7.5 (default, Aug 26 2021, 16:53:13)  [GCC 5.4.0 20160609] (64-bit runtime)
Python platform: Linux-4.15.0-1065-aws-x86_64-with-debian-stretch-sid
Is CUDA available: True
CUDA runtime version: 11.0.221
GPU models and configuration:
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB
GPU 2: Tesla V100-SXM2-32GB
GPU 3: Tesla V100-SXM2-32GB
GPU 4: Tesla V100-SXM2-32GB
GPU 5: Tesla V100-SXM2-32GB
GPU 6: Tesla V100-SXM2-32GB
GPU 7: Tesla V100-SXM2-32GB

Nvidia driver version: 450.142.00
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.5
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] botorch==0.4.0
[pip3] gpytorch==1.5.1
[pip3] mypy-extensions==0.4.3
[pip3] mypy-protobuf==2.4
[pip3] numpy==1.19.4
[pip3] pytorch-lamb==1.0.0
[pip3] torch==1.9.0a0+gitdfbd030
[pip3] torch-tb-profiler==0.3.1
[pip3] torchfile==0.1.0
[pip3] torchmetrics==0.3.1
[pip3] torchscript==0.2.10
[pip3] torchvision==0.8.0a0+2f40a48
[conda] Could not collect"
Bug with transforms.Resize when used with transforms.ConvertImageDtype,pytorch/vision,2021-11-06 07:01:14,11,module: transforms,4880,1046448246,"### 🐛 Describe the bug

Recent releases of Torchvision and the documentations that support it seem to  suggest that we can use `io.read_image + transforms.ConvertImageDtype` instead of the traditional `PIL.Image.read_fn + transforms.ToTensor`. However, I have found that there are two issues:

1. `io.read_image + transforms.ConvertImageDtype` do not actually return the same tensor values as `PIL + transforms.ToTensor`, even though they are supposed to provide the same functionality.
2. While `io.read_image + transforms.ConvertImageDtype` itself is significantly faster than using PIL, combining it with the `transforms.Resize` operation - specifically when upsampling - makes the operation much slower than the PIL alternative.

To add onto point 2, the two sets of functions I mention return the same type of tensor: `torch.float`. However, applying `transforms.Resize` on the tensor generated by `io.read_image + transforms.ConvertImageDtype` is much slower than applying the same resize operation on the output of `PIL read + transforms.ToTensor`. I can't really understand why this happens, since both calls to `Resize` are on tensors of type `torch.FloatTensor`. Also, this only occurs when upsampling.

Please refer to my post on the Pytorch Forum [here](https://discuss.pytorch.org/t/no-speedup-with-torchvision-io-and-weird-behavior-with-convertimagedtype/135943) for the full analysis.

### Versions

Collecting environment information...
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.23

Python version: 3.9.4 (default, Apr  9 2021, 01:15:05)  [GCC 5.4.0 20160609] (64-bit runtime)
Python platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.23
Is CUDA available: True
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: NVIDIA GeForce RTX 2080 Ti
GPU 1: NVIDIA GeForce RTX 2080 Ti
GPU 2: NVIDIA GeForce RTX 2080 Ti
GPU 3: NVIDIA GeForce RTX 2080 Ti

Nvidia driver version: 465.19.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.3
[pip3] torch==1.10.0+cu113
[pip3] torchaudio==0.10.0+cu113
[pip3] torchvision==0.11.1+cu113
[conda] Could not collect

cc @vfdev-5 @datumbox"
Padding transform to a multiple of 2,pytorch/vision,2021-11-05 02:19:02,2,,4865,1045391446,"### 🚀 The feature

Introduce a new API to pad a tensor, such as `transforms.PadSizeDivisor` (I don't come up with a proper name at the moment so that it could be changed)

Expectation:
```
inputs = torch.rand(1,3,32,254)
tf = transforms.PadSizeDivisor(size_divisor=32, dim=3, pad_method='constant', pad_value=0)  # maybe we can support dim=[2,3]?
outputs = tf(inputs)
assert outputs.shape == torch.Size([1,3,32,256])
```

### Motivation, pitch

I think padding with the `size_divisor` argument is widely used in segmentation or U-like network where we apply downsampling followed by upsampling the input tensor. In most cases, the up/down ratio is multiple 2, e.g. 2, 4, 8, 16, 32. Therefore, the input is usually padded to some sort of these multiples such as 352, 768 in spatial dimensions (H, W).

The current implementation of `transforms.Pad` only supports fixed padding size. It works great during training where we usually apply the tensor crop or resize to a fixed size to the input tensor. However, during inference, we usually maintain the ratio when resizing. This leads to the remaining edge is not guaranteed to be a multiple of 2, and thus, the following up/down operators would be failed due to the size mismatch. At the moment, the solution is usually to compute the padding size on each edge before or after resizing the input image.

Therefore, I think it would be great to have a library-included transformation so that we can reuse it in our application.

### Alternatives

_No response_

### Additional context

_No response_"
ssdlite320_mobilenet_v3_large only have ~ 50% CUDA usage even with large batch size,pytorch/vision,2021-11-04 09:47:04,2,needs reproduction#module: models#Perf,4853,1044531632,"### 🐛 Describe the bug

```
import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large

with torch.inference_mode():
    model = ssdlite320_mobilenet_v3_large(True)
    model = model.eval()
    model = model.to('cuda')
    inputs = (torch.randint(0, 255, (256, 3, 320, 320)) / 255).to('cuda')
    for _ in range(64):
        outputs = model(inputs)
```

```
nvidia-smi
```

The CUDA usage is around 36% during inferencing, one of the CPU usage is 100%

### Versions
```
Collecting environment information...
PyTorch version: 1.10.0+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Sep 28 2021, 16:10:42)  [GCC 9.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-89-generic-x86_64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 10.1.243
GPU models and configuration: 
GPU 0: NVIDIA GeForce GTX 1080 Ti
GPU 1: NVIDIA GeForce GTX 1080 Ti

Nvidia driver version: 495.29.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.3.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.3.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.3
[pip3] torch==1.10.0+cu113
[pip3] torchvision==0.11.1+cu113
[conda] Could not collect
```

cc @datumbox"
Check if fused inplace Relu|LeakyRelu + Dropout works in core and provide recipes / tutorial,pytorch/vision,2021-11-03 17:03:59,2,needs discussion,4851,1043853066,"### 🚀 The feature

If it works, check it speed and memory usage compared to just inplace relu + outplace dropout

https://github.com/pytorch/pytorch/issues/30801

Manual version is at https://gist.github.com/vadimkantorov/360ece06de4fd2641fa9ed1085f76d48, but it allocates 1 float copy of input + 2 boolean masks. It would be nice to eliminate these allocations by fusion

### Motivation, pitch

Given that torchvision started to add things like ConvNormAct, things like fused inplace ReluDropout start to make more sense

### Alternatives

_No response_

### Additional context

_No response_"
RuntimeError: INTERNAL ASSERT FAILED in F.rotate,pytorch/vision,2021-11-03 03:40:34,2,needs reproduction#module: transforms,4857,1044775442,"## 🐛 Bug

I get an assertion error from native code when trying to run `torchvision.RandomRotate` on a fairly large image (2000x2000)

## To Reproduce

The snippet below *does not* reproduce the bug, but it is the best I could do without delivering the entire code base to you.

```python
import time

import torch
import torchvision
from torch.utils.data import Dataset


class MyDataset(Dataset):
    def __init__(self, transforms):
        self._transforms = transforms

    def __getitem__(self, item):
        loaded_image = torch.load('c0d28963-01c4-4b91-add6-3a8edb782610.pt')
        return self._transforms(loaded_image)

    def __len__(self):
        return 1000


transforms = torch.nn.Sequential(
    torchvision.transforms.RandomRotation(degrees=180, expand=True, fill=0.),
    torchvision.transforms.Resize(size=(224, 224)),
)
start_time = time.monotonic()

dataset = MyDataset(transforms)
batch_size = 32
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=6, prefetch_factor=2)

for i, image in enumerate(dataloader):
    estimated_total_time = (time.monotonic() - start_time) * len(dataset) / batch_size / (i + 1)
    print(f""processing batch {i + 1}/{len(dataset) / batch_size} in ""
          f""{time.monotonic() - start_time:.0f}/{estimated_total_time:.0f}s"")
```
[saved image tensor](https://github.com/pytorch/pytorch/files/7465134/c0d28963-01c4-4b91-add6-3a8edb782610.pt.zip)

traceback (not from the snippet):
```
Traceback (most recent call last):
  File ""/home/ian.pegg/projects/shining_software/src/shining_research/map_divergence_detection/dataset.py"", line 241, in <module>
    print(dataset.get_dataset_stats())
  File ""/home/ian.pegg/projects/shining_software/src/shining_research/map_divergence_detection/dataset.py"", line 212, in get_dataset_stats
    for i, entry in enumerate(self):
  File ""/home/ian.pegg/projects/shining_software/src/shining_research/map_divergence_detection/dataset.py"", line 150, in __getitem__
    transformed_image = self._transforms(image)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torch/nn/modules/container.py"", line 141, in forward
    input = module(input)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torchvision/transforms/transforms.py"", line 1351, in forward
    return F.rotate(img, angle, self.resample, self.expand, self.center, fill)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torchvision/transforms/functional.py"", line 1028, in rotate
    return F_t.rotate(img, matrix=matrix, interpolation=interpolation.value, expand=expand, fill=fill)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py"", line 750, in rotate
    return _apply_grid_transform(img, grid, interpolation, fill=fill)
  File ""/home/ian.pegg/miniconda3/envs/torch-nightly/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py"", line 661, in _apply_grid_transform
    img[mask] = fill_img[mask]
RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1635405091770/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp"":1725, please report a bug to PyTorch. 
munmap_chunk(): invalid pointer
```

## Expected behavior

Rotation happens without any errors.


## Environment

```
Collecting environment information...
PyTorch version: 1.11.0.dev20211102
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2
Libc version: glibc-2.27

Python version: 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-89-generic-x86_64-with-glibc2.27
Is CUDA available: True
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: NVIDIA GeForce RTX 2070 with Max-Q Design
Nvidia driver version: 470.74
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.21.3
[pip3] torch==1.11.0.dev20211102
[pip3] torchvision==0.12.0.dev20211102
[conda] blas                      2.112                       mkl    conda-forge
[conda] blas-devel                3.9.0            12_linux64_mkl    conda-forge
[conda] cudatoolkit               11.3.1               ha36c431_9    conda-forge
[conda] libblas                   3.9.0            12_linux64_mkl    conda-forge
[conda] libcblas                  3.9.0            12_linux64_mkl    conda-forge
[conda] liblapack                 3.9.0            12_linux64_mkl    conda-forge
[conda] liblapacke                3.9.0            12_linux64_mkl    conda-forge
[conda] mkl                       2021.4.0           h8d4b97c_729    conda-forge
[conda] mkl-devel                 2021.4.0           ha770c72_730    conda-forge
[conda] mkl-include               2021.4.0           h8d4b97c_729    conda-forge
[conda] mypy_extensions           0.4.3            py39hf3d152e_3    conda-forge
[conda] numpy                     1.21.3           py39hdbf815f_0    conda-forge
[conda] pytorch                   1.11.0.dev20211102 py3.9_cuda11.3_cudnn8.2.0_0    pytorch-nightly
[conda] pytorch-mutex             1.0                        cuda    pytorch-nightly
[conda] torchvision               0.12.0.dev20211102      py39_cu113    pytorch-nightly
```
This also happened with the pytorch installed with `conda install pytorch -c pytorch`; I just switched to the nightly for debugging.

## Additional context

This does not happen on my other machine with a duplicated environment. It also *seemed* to happen less frequently when I switched from `torch.nn.Sequential` to `torchvision.transforms.Compose`.


cc @vfdev-5 @datumbox @fmassa @pmeier"
"RoIAlign segfaults when ROIS is a ""big enough"" tensor of shape (K, 5)",pytorch/vision,2021-11-02 08:14:02,0,bug#needs reproduction#module: ops,4828,1042235010,"## 🐛 Bug
Applying `RoIAlign` (torchvision.ops.RoIAlign) on `rois` that is a tensor of shape (K, 5) results in a segfault : `Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)`. 

The faulty behavior is **not** observed when :

* Applying `RoIAlign` on `rois` that is a list of tensor of shape (4,)
* `RoIAlign` input size is small enough (typically <= 32,10,10)

## To Reproduce

Here is a code snippet that should allow you to reproduce my observations.

```python
import argparse

import torch
from torchvision.ops import RoIAlign

NB_ROI_PER_DOC = 5

if __name__ == ""__main__"":

    torch.manual_seed(0)

    parser = argparse.ArgumentParser(description='minimum reproducible example')
    parser.add_argument('--bug', action='store_true', default=False, help='toggle on the issue')
    parser.add_argument('--batch-size', type=int, default=1)
    parser.add_argument('--input-size', type=str, default=""64,10,10"")
    args = parser.parse_args()

    align = RoIAlign((3, 3), spatial_scale=14 / 224, sampling_ratio=2)
    batch_size = args.batch_size
    input_ = torch.randn((batch_size, *(int(dim) for dim in args.input_size.split("",""))))
    rois = [torch.abs(torch.randn(NB_ROI_PER_DOC, 4)) for _ in input_]

    if args.bug:
        nb_rois = NB_ROI_PER_DOC * batch_size
        boxes = torch.abs(torch.randn(nb_rois, 4))
        roi_ids = torch.arange(nb_rois).view(-1, 1)
        rois = torch.cat((roi_ids, boxes), dim=1)

    output = align(input=input_, rois=rois)
    print(rois)
    print(output.shape)
```

Running the code in docker to be freeze packages versions and the like.
```dockerfile
FROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime
RUN mkdir -p /opt/debug
WORKDIR /opt/debug
ADD mre.py .
ENTRYPOINT [""python"", ""/opt/debug/mre.py""]
```

To run `build -f Dockerfile.minimal -t debug-torch:v0 . ; docker run  docker run debug-torch:v0 --bug; docker ps -a |grep debug-torch:v0` ends up with a code 139 (SIGSEGV)
```                                                                                                                                                                                                                                                                                                                      
46b40fe6efee   debug-torch:v0 ""python /opt/debug/m…""     16 seconds ago   Exited (139) 
```


## Expected behavior

To run : `docker run  docker run debug-torch:v0` results with:
```
[tensor([[1.4164, 0.2379, 0.9334, 1.1331],
        [0.3530, 2.0928, 0.6356, 1.5069],
        [0.9527, 1.0599, 0.9549, 1.3355],
        [0.5251, 0.7416, 0.4269, 0.4008],
        [0.7872, 0.0834, 1.1256, 1.5490]])]
torch.Size([5, 64, 3, 3])
```
To reduce the input size also solves the problem, to run `docker run  docker run debug-torch:v0 --bug --input-size 5,5,5` results with:
```
tensor([[0.0000, 0.3584, 1.5616, 0.3546, 1.0811],
        [1.0000, 0.8760, 0.2871, 1.0216, 0.5111],
        [2.0000, 1.7137, 0.5101, 0.4749, 0.6334],
        [3.0000, 1.2063, 0.6074, 0.5472, 1.1005],
        [4.0000, 0.7201, 0.0119, 0.3398, 0.2635]])
torch.Size([5, 64, 3, 3])
```

## Environment
The problem is observed using the `pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime` docker image  
```bash
python collect_env.py
PyTorch version: 1.10.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.6 LTS (x86_64)
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7.11 (default, Jul 27 2021, 14:32:16)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.11.0-38-generic-x86_64-with-debian-buster-sid
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.2
[pip3] torch==1.10.0
[pip3] torchelastic==0.2.0
[pip3] torchtext==0.11.0
[pip3] torchvision==0.11.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               ha36c431_9    nvidia
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.3.0           h06a4308_520  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.2           py37h20f2e39_0  
[conda] numpy-base                1.21.2           py37h79a1101_0  
[conda] pytorch                   1.10.0          py3.7_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchelastic              0.2.0                    pypi_0    pypi
[conda] torchtext                 0.11.0                     py37    pytorch
[conda] torchvision               0.11.0               py37_cu113    pytorch
```

cc @fmassa @vfdev-5 @pmeier"
FashionMNIST Download issue,pytorch/vision,2021-11-02 06:23:59,4,needs reproduction#module: datasets,4829,1042309666,"## 🐛 Bug

Unable to download MNIST Dataset as per pytorch Documentation

https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html


## To Reproduce

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor, Lambda, Compose
import matplotlib.pyplot as plt

training_data = datasets.FashionMNIST(
    root=""data"",
    train=True,
    download=True,
    transform=ToTensor(),
)
```

**Error message is below**

```
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Failed to download (trying next):
<urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>
```
```
RuntimeError                              Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_8408/1648373759.py in <module>
      6 import matplotlib.pyplot as plt
      7 
----> 8 training_data = datasets.FashionMNIST(
      9     root=""data"",
     10     train=True,

G:\ineuronai\o\envs\odtf\lib\site-packages\torchvision\datasets\mnist.py in __init__(self, root, train, transform, target_transform, download)
     83 
     84         if download:
---> 85             self.download()
     86 
     87         if not self._check_exists():

G:\ineuronai\o\envs\odtf\lib\site-packages\torchvision\datasets\mnist.py in download(self)
    167                 break
    168             else:
--> 169                 raise RuntimeError(""Error downloading {}"".format(filename))
    170 
    171         # process and save as torch files

RuntimeError: Error downloading train-images-idx3-ubyte.gz
```


## Expected behavior

- Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to `data/FashionMNIST/raw/train-images-idx3-ubyte.gz`
- Extracting `data/FashionMNIST/raw/train-images-idx3-ubyte.gz` to `data/FashionMNIST/raw`


cc @fmassa @vfdev-5 @pmeier @brianjo @mruberry"
installation dependencies,pytorch/vision,2021-11-01 17:05:01,0,,4819,1041392707,"### 🚀 The feature

pip install  torchvision==0.8.2+cu101  requires torch==1.7.1 to be installed instead of torch==1.7.1+cu101

this causes poetry package manager crushing during the installation.


Updating dependencies
Resolving dependencies... (1720.5s)

  SolverProblemError

  Because torchvision (0.8.2+cu101) depends on torch (1.7.1)
   and <myenv> depends on torch (1.7.1+cu101), torchvision is forbidden.
  So, because <myenv> depends on torchvision (0.8.2+cu101), version solving failed.



It would be great if torchvision requested the appropriate pytorch version.


### Motivation, pitch

pip install  torchvision==0.8.2+cu101  -f https://download.pytorch.org/whl/torch_stable.html
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Looking in links: https://download.pytorch.org/whl/torch_stable.html
Collecting torchvision==0.8.2+cu101
  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8 MB)
     |████████████████████████████████| 12.8 MB 1.1 MB/s 
Collecting pillow>=4.1.1
  Downloading Pillow-8.4.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
     |████████████████████████████████| 3.1 MB 1.1 MB/s 
Collecting torch==1.7.1
  Downloading https://download.pytorch.org/whl/rocm3.8/torch-1.7.1%2Brocm3.8-cp36-cp36m-linux_x86_64.whl (588

### Alternatives

_No response_

### Additional context

_No response_"
[docs] torchvision.ops.roi_pool doesn't mention dtype of boxes tensor,pytorch/vision,2021-11-01 15:32:28,0,,4816,1041304762,"### 🐛 Describe the bug

Is it integer, some specific integer type or float32? Again, this is related to bbox formats / coordinate frames

Same problem for roi_align

### Versions

N/A"
"[docs] box_convert seems to support batch-less mode as well, but this is not documented",pytorch/vision,2021-11-01 15:18:49,2,module: documentation,4815,1041291468,"### 🐛 Describe the bug

It seems that box_convert does support Tensor[4] arguments as well, but this is not documented. This is a useful mode to support, so would be nice if this was documented

### Versions

N/A"
Not possible to torch.jit.trace a faster r cnn caused by : double device found,pytorch/vision,2021-10-30 18:28:41,2,,4803,1040267617,"### 🐛 Describe the bug

Hi I tried to run this code on google collab and got an error. I don’t understand why this error occurs while I just downloaded a faster r cnn model and called the torh.jit.trace function.

```
import torch
import torchvision

device = ""cuda"" if torch.cuda.is_available() else ""cpu""
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.to(device)
model.eval()

input_t = torch.rand(1,3,224,224).to(device)
module = torch.jit.trace(model,input_t)

module.save(""module.pth"")
```

I got this error : 

```
  for i in range(dim)
/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]

---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

<ipython-input-3-cfa0d532d4aa> in <module>()
      8 
      9 input_t = torch.rand(1,3,224,224).to(device)
---> 10 module = torch.jit.trace(model,input_t)
     11 
     12 module.save(""module.pth"")

9 frames

/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py in _get_top_n_idx(self, objectness, num_anchors_per_level)
    223                 pre_nms_top_n = min(self.pre_nms_top_n(), num_anchors)
    224             _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)
--> 225             r.append(top_n_idx + offset)
    226             offset += num_anchors
    227         return torch.cat(r, dim=1)

RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

### Versions

--2021-10-30 18:27:45--  https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 16723 (16K) [text/plain]
Saving to: ‘collect_env.py’

collect_env.py      100%[===================>]  16.33K  --.-KB/s    in 0s      

2021-10-30 18:27:46 (86.5 MB/s) - ‘collect_env.py’ saved [16723/16723]"
"Can ResNet18 / BasicBlock support replace_stride_with_dilation = [False, False, True]?",pytorch/vision,2021-10-30 12:18:23,0,,4801,1040131916,"### 🐛 Describe the bug

Currently, it doesn't work because of https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L77

But I guess, one could just dilate conv1? (just like conv2 is dilated in Bottleneck)

e.g. this is how it's done in [ResNet-WS](https://proceedings.neurips.cc/paper/2020/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html): https://github.com/shenyunhang/UWSOD/blob/UWSOD/projects/WSL/wsl/modeling/backbone/resnet_ws.py#L32

### Versions

N/A"
[docs] Pretrained model docs should explain how to specify cache dir and norm_layer,pytorch/vision,2021-10-29 11:50:17,2,,4795,1039483139,"### 🐛 Describe the bug

https://pytorch.org/vision/stable/models.html?highlight=resnet18#torchvision.models.resnet18

should document:
- how to set cache dir for downloaded models. many university systems have tight quota for home dir that prohibits clogging it with weights. it is explained at the very top of very long document (`TORCH_MODEL_ZOO`) but it would be nice to duplicate it / link to this from every pretrained method

- how to set `norm_layer = torchvision.ops.misc.FrozenBatchNorm2d` since this is a very frequent need for fine-tuning

- how to replace stride with dilation for ResNet and to what layers it applies and what it can help achieving

Currently docs just specify `**kwargs: Any` which isn't very helpful

### Versions

N/A"
Let's avoid *.pth extension at least for newer models,pytorch/vision,2021-10-29 11:42:30,5,needs discussion,4794,1039477439,"### 🐛 Describe the bug

pth is a special extension for Python related to module search paths

https://github.com/pytorch/pytorch/issues/14864

### Versions

N/A"
dataset format missing in readme.md in torchvision detection,pytorch/vision,2021-10-28 19:50:43,0,,4785,1038896372,"### 📚 The doc issue

Hi Team,

Thanks for the detection example. Would you be able to provide a sample input dataset format as an example for the following repo https://github.com/pytorch/vision/tree/main/references/detection.

Thanks in advance :)



### Suggest a potential alternative/fix

example input sample to the detection python script is required to understand."
"[docs] CocoDetection, VOCDetection should specify format of targets",pytorch/vision,2021-10-28 10:14:21,1,module: documentation,4774,1038337819,"### 🐛 Describe the bug

And be very clear on the format of bounding boxes: xywh, are x/y zero-based or not and if width do/not include the final pixel

Complications about this are described in:
- https://ppwwyyxx.com/blog/2021/Where-are-Pixels/#Choices-of-Sampling-Grid
- https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/


COCO seems to have floating-point annotations, VOC seems to have integer-based annotations (0/1-based? inclusive/exclusive for final point?)"
voc.py to report the directory it tried in the error message,pytorch/vision,2021-10-28 09:47:31,1,,4773,1038311628,"### 🐛 Describe the bug

https://github.com/pytorch/vision/blob/main/torchvision/datasets/voc.py#L113:
```python
        if not os.path.isdir(voc_root):
            raise RuntimeError(""Dataset not found or corrupted. You can use download=True to download it"")
```

I propose the error message should report `voc_root` as well, it will help debugging wrongly passed dataaset root dirs.

### Versions

N/A"
Makefile not found while installing torchvision c++.,pytorch/vision,2021-10-28 03:42:14,2,,4771,1038049774,"Not able to run `make` command after `cmake ..`. It gives **make: *** No targets specified and no makefile found.  Stop.**. 

The output of `cmake .. 
` (2nd time run) was
```
-- Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.18363.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/Lenovo/Desktop/vision/build

```
I tried it on Windows 10 through anaconda prompt."
Replace `pathlib.Path` with raw strings in `prototype.datasets`,pytorch/vision,2021-10-27 12:28:57,2,module: datasets#prototype,4760,1037361544,"### 🐛 Describe the bug

We currently use `pathlib.Path` everywhere in the new datasets.
`pathlib.Path` is nice, but doesn't work well at all with URL-like paths. Indeed, it replaces `//` with `/`, so that `http://www` becomes an invalid url `http:/www`.

We should either remove `pathlib.Path` altogether and depend on raw strings, using `os.path.join` (which works without issues), or figure out a way of `pathlib` to work with urls, 

### Versions

Current nightly

cc @pmeier @bjuncek"
"Somehow, tagging a release is creating ""main"" docs",pytorch/vision,2021-10-26 15:39:13,3,,4754,1036461833,"### 📚 The doc issue

In #4687 I needed to change all the page's title and version. It seems tagging the version builds and uploads the docs to the `gh-pages` branch properly, but somehow the tag is not propagated into the doc build. 

Also the last tag pushed generated `docs/0.11.` (with a '.' at the end) due to #4693. So that ""proves"" that the `CIRCLECI_TAG` is properly read in the CI, but something else is a bit off. So right now there are `0.11` docs and `0.11.` docs, where the correct ones are `0.11.`

I will try to figure out what is going on, and in any case immediately submit a PR to 
- move the `0.11.` docs on top of the `0.11` ones
- replace all the version/title info with `0.11.0`

@brianjo

### Suggest a potential alternative/fix

_No response_"
create_feature_extractor does not track 'if' in forward,pytorch/vision,2021-10-26 15:05:12,8,,4753,1036418050,"### 🐛 Describe the bug

It looks like `create_feature_extractor` does not track `if` statement in `forward`, for example with the `efficientnet` architecture:

```
import torchvision

efficientnetb0 = torchvision.models.efficientnet_b0()
return_nodes = {f'features.{k}': str(v) for v, k in enumerate([3, 4, 6, 7])}

backbone = torchvision.models.feature_extraction.create_feature_extractor(efficientnetb0, return_nodes=return_nodes)

# print first Sequential module for efficientnetb0
print(efficientnetb0.features[1])
>>> Sequential(
  (0): MBConv(
    (block): Sequential(
      (0): ConvNormActivation(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU(inplace=True)
      )
      (1): SqueezeExcitation(
        (avgpool): AdaptiveAvgPool2d(output_size=1)
        (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
        (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
        (activation): SiLU(inplace=True)
        (scale_activation): Sigmoid()
      )
      (2): ConvNormActivation(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (stochastic_depth): StochasticDepth(p=0.0, mode=row)
  )
)

# print same module for backbone
>>> (1): Module(
    (0): Module(
      (block): Module(
        (0): Module(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): SiLU(inplace=True)
        )
        (1): Module(
          (avgpool): AdaptiveAvgPool2d(output_size=1)
          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))
          (activation): SiLU(inplace=True)
          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))
          (scale_activation): Sigmoid()
        )
        (2): Module(
          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
```

Is the `StochasticDepth` layer implemented but hidden in the `backbone` or is it missing (due to the `if`) ?

### Versions

Collecting environment information...
PyTorch version: 1.10.0
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: CentOS Linux release 8.2.2004 (Core)  (x86_64)
GCC version: (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5)
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.9

Python version: 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-4.18.0-193.6.3.el8_2.x86_64-x86_64-with-centos-8.2.2004-Core
Is CUDA available: False
CUDA runtime version: Could not collect
GPU models and configuration: No devices found.
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] torch==1.10.0
[pip3] torchvision==0.11.1
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py36he8ac12f_0  
[conda] mkl_fft                   1.3.0            py36h54f3939_0  
[conda] mkl_random                1.1.1            py36h0573a6f_0  
[conda] numpy                     1.19.1           py36hbc911f0_0  
[conda] numpy-base                1.19.1           py36hfa32c7d_0  
[conda] pytorch                   1.10.0          py3.6_cuda10.2_cudnn7.6.5_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.11.1               py36_cu102    pytorch"
Inconsistent representation of box in torchvision.ops,pytorch/vision,2021-10-26 05:02:46,23,,4745,1035838855,"### Feature suggestion

When checking the how the new operations in the torchvision 0.11 work, I found an inconsistency between different function.

```
import torchvision.ops as tops
import torch
import torch.nn as nn

a = torch.zeros(1, 1, 8, 8)
a[..., :4, 4:] = 1
bbox = tops.masks_to_boxes((a==1)[0].float()) 
area = tops.box_area(bbox) # the returned area is 9 which should be 16

new_box = tops.box_convert(bbox, 'xyxy', 'xywh') # which returns [4, 4, 3, 3] => expected output [4, 4, 4, 4]
```


### Suggest a potential alternative/fix

This is not actually an error but could cause confusion.

I suggest to make the box representations between different function more consistent."
16 bits png support and compatibility with current transforms,pytorch/vision,2021-10-25 10:21:11,7,,4731,1034950804,"We added support for decoding 16 bit pngs in https://github.com/pytorch/vision/pull/4657.

Pytorch doesn't support `uint16` dtype, so we have to return a `int32` tenor instead. However as @nairbv pointed out, this can lead to unexpected behaviour for the [`convert_image_dtype()`](https://github.com/pytorch/vision/blob/main/torchvision/transforms/functional_tensor.py#L58:L58), because this function relies on the max value of a given dtype to carry the conversions.

While the actual max value of a 16 bits image is `2**16 - 1`,  `convert_image_dtype()` will assume that it's `(2**31 - 1)` because the image is in a `int32` tensor. This leads to e.g. a tensor full of zeros when converting to uint8.

Potential solutions are:

- native support for int16 in pytorch
- add a new `input_range=None` parameter to `convert_image_dtype()` (not a fan of that)

For now, I'll just make this a private API (e.g. `_decode_16bits_png()` or something). It will still allow to continue the work on RAFT, which was the main reason I needed 16bits decoding in the first place."
Add some network structure diagrams,pytorch/vision,2021-10-22 07:10:14,6,enhancement#module: documentation,4719,1033253516,"### 🚀 The feature

 Add some network structure diagrams in torchvision repository. I want to contribute some drawn diagrams.

### Motivation, pitch

In order to facilitate everyone to learn the model in the torchvison.

### Alternatives

_No response_

### Additional context

I draw the structure diagram of FCN network, reference [torchvision official FCN model](https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/fcn.py). I wish it can be added to the repository to facilitate everyone's study.

![torch_fcn](https://user-images.githubusercontent.com/31005897/138584085-2ada3295-2976-4230-9eba-ba48b4922be4.png)

"
Fashion MNIST not creating processed directory when downloading,pytorch/vision,2021-10-21 10:40:24,2,,4685,1032334717,"### 🐛 Describe the bug

Normally, when downloading the fashion-mnist dataset, two folders are created, namely `raw` and `processed`. 
In the latest torchvision version (0.10.1), only `raw` is being created and no error is being raised. 

Steps to reproduce:

```
import torchvision
torchvision.datasets.FashionMNIST(root=""."", download=True, train=True)
```

### Versions

I verified this issue on two  different machines.
Version 0.9.1 provides the two folders, whereas 0.10.1 does not.  "
Drop torchvision 0.2.2 from conda-forge as it's often a silent indicator of major package conflicts,pytorch/vision,2021-10-20 09:44:18,14,,4665,1031200341,"### 🐛 Describe the bug

Running the following triggers install of torchvision 0.2.2.

We need a stable conda install recipes and suggestions that deal with common bugged dependencies such as OpenCV and ffmpeg given all the mess with glibc / python versions etc. At least some known working commands (even if not the latest python).

I understand that it can't be the job of pytorch maintainers to test all conda packages, but the combination of torch / opencv / ffmpeg (also maybe some packages for 3d vision) is super-common one. If someone succeeds in installing them together, it is super-valuable to collect and publish these experiences. Or maybe some mamba recipes?

cc @malfet 

```shell
PREFIX=/path/lab/vadim/prefix
mkdir -p $PREFIX

curl -L -so $PREFIX/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh && chmod +x $PREFIX/miniconda.sh && $PREFIX/miniconda.sh -b -p $PREFIX/miniconda && rm $PREFIX/miniconda.sh

export PATH=$PREFIX/miniconda/bin:$PATH

conda install -y opencv pytorch torchvision cudatoolkit=11.1 -c pytorch -c conda-forge
```

### Versions

N/A"
Missing model training details in documentation,pytorch/vision,2021-10-19 13:33:05,0,module: reference scripts#module: documentation,4654,1030316404,"### 📚 The doc issue

Under [references](https://github.com/pytorch/vision/tree/main/references/classification) we are missing details of the model training setup for many of our models. The idea here is to fix that by

- [ ] Recovering the commands used to train all missing models (have a look to list them if possible)
- [ ] Add them on the references
- [ ] Update the meta-data on prototype to point to them.


### Suggest a potential alternative/fix

_No response_

cc @datumbox"
torchvision conda packaging fails to resolve when opencv constraint is present,pytorch/vision,2021-10-19 05:56:52,24,topic: binaries#release-issue,4648,1029890190,"### 🐛 Describe the bug

 This seems to be somewhat serious to figure out and resolve by the next release or asap.

Here's how the bug manifests.
There's a very common scenario when someone installs `torchvision` and `opencv` within the same `conda` environment.

However, running the following command:

```
conda create -n tvtest python=3.7 ""torchvision>=0.10"" ""cudatoolkit>=11"" opencv -c pytorch -c conda-forge
```

This results in the `torchvision` resolving as the `cpu` torchvision packge from `conda-forge`, the `pytorch` being resolved from `pytorch` channel, and the worst part, old versions of `pytorch` and `pytorch-cpu` being installed.

Tested with latest anaconda: `conda --version` resolves to `conda 4.10.3`.

Removing `opencv` in the above command results in `torchvision` and `pytorch` correctly resolving to their CUDA binaries, and `pytorch-cpu` not being installed.

cc: @malfet 




### Versions

```
Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.31

Python version: 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31
Is CUDA available: N/A
CUDA runtime version: 11.1.105
GPU models and configuration:
GPU 0: GeForce GTX TITAN X
GPU 1: GeForce GTX TITAN X

Nvidia driver version: 460.73.01
cuDNN version: Probably one of the following:
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.5
/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] Could not collect
[conda] # packages in environment at /home/soumith/miniconda3:
```"
Feature Request for torchvision ImageFolder using/inheriting DatasetFolder,pytorch/vision,2021-10-16 13:53:53,5,module: datasets,4633,1028054335,"I came across a feature other users also demanded as may be seen in pytorch forums.
For a detailed problem Description and how to solve it see here in a discussion from users, @ptrblck and me:
https://discuss.pytorch.org/t/how-to-sample-images-belonging-to-particular-classes/43776/9

In short: Using ImageFolder, which inherits from DatasetFolder, is limiting the user to retrieve a whole dataset from a folder, instead of just using some classes/dirs of the folder structure. Even though one can implement a custom find_classes() method or rather call it a function if one passes it an overwritten DatasetFolder custom implementation, this is often hidden to the user, since one only uses ImageFolder which uses DatasetFolder under the hood. 

For users getting this wrong also see the pytorch discussion from the [link ](https://discuss.pytorch.org/t/how-to-sample-images-belonging-to-particular-classes/43776/9) above in the forum, where @ptrblck and I figured out that it would be nice to be able to just pass such a function that only selects a subset of a folder structure directly by passing an optional function to the ImageFolder.

The line I am talking about in current torchvision DatasetFolder implementation, where subsets from a folder may be retrieved, by overwriting this function:
https://github.com/pytorch/vision/blob/fba4f42e3bc24b7b2c6cad09b6db653ac73dc6b7/torchvision/datasets/folder.py#L144

My Suggestion for this improvement that users can use only a subset of a folder structure in ImageFolder looks as follows as also stated in the pytorch forum:


```
def find_classes(directory: str, desired_class_names: List) -> Tuple[List[str], Dict[str, int]]:
    """"""Finds the class folders in a dataset.""""""
    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())
    classes = classes [desired_class_names] # TODO: do something like this line! Not tested it yet!
    if not classes:
        raise FileNotFoundError(f""Couldn't find any class folder in {directory}."")

    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
    return classes, class_to_idx
```

Current implementation suggest overwriting the function as follows within DatasetFolder, but most Users tend to be using ImageFolder as I inferred from posts.

https://github.com/pytorch/vision/blob/fba4f42e3bc24b7b2c6cad09b6db653ac73dc6b7/torchvision/datasets/folder.py#L191-L218


Also as stated @ptrblck suggested to make it possible to pass a function to ImageFolder directly instead of overwriting DatasetFolder. Regarding this i have no code to suggest but it might be trivial by just passing parameters.

cc @pmeier"
Split _validate_trainable_layers into validator and num_of_layers selector,pytorch/vision,2021-10-15 12:38:24,0,module: models#code quality,4625,1027401140,"We use  [`_validate_trainable_layers`](https://github.com/pytorch/vision/blob/3d7244b5280301792a6959c59154d4809ad1209a/torchvision/models/detection/backbone_utils.py#L119)  as a validator but also as a way to determine the actual number of trainable layers based on default values, pertained flag etc.

- [ ] It would be clearer to have separate checks for validation and another method to set the actual value instead of having a checker with side effects. 
- [ ] Check if it makes sense to add the default value as a parameter default value instead of None?
- [ ] Should we use ValueError exception instead of assert?


cc @jdsgomes




cc @datumbox"
ONNX export of Faster R-CNN MobileNet V3 fails in ONNX Runtime,pytorch/vision,2021-10-15 09:37:10,0,module: onnx,4623,1027265092,"### 🐛 Describe the bug

Hi, I'm trying to export Faster R-CNN with MobileNet V3 backbone to ONNX, but have encountered several problems.

1. When I feed an image containing only a single object such as [this](https://farm8.staticflickr.com/7178/6859626097_3b561d69bf_z.jpg), ONNX Runtime raises the following error:
```
Traceback (most recent call last):
  File ""test_onnx.py"", line 51, in <module>
    ort_output = ort_session.run(None, ort_input)
  File ""/usr/local/anaconda/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py"", line 188, in run
    return self._sess.run(output_names, input_feed, run_options)
onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_1537' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:36 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&, bool) size != 0 && (input_shape.Size() % size) == 0 was false. The input tensor cannot be reshaped to the requested shape. Input shape:{71,363}, requested shape:{-1,4}
```

2. Feeding images with multiple objects such as [this](https://farm4.staticflickr.com/3334/3253284080_3268267cc5_z.jpg) causes no errors. However, it seems the results between PyTorch and ONNX Runtime are not consistent.
```
PyTorch scores:  tensor([0.9987, 0.9986, 0.8645, 0.2566, 0.1377, 0.0731, 0.0546])

ORT scores:  [0.9980155  0.99794    0.9972451  0.9964805  0.99622935 0.995884
 0.99581414 0.99375397 0.98647714 0.9856754  0.62823844 0.42497706
 0.20502456 0.16712733 0.07833275 0.07327942 0.06243803 0.05234617]
```

Here is a script to reproduce the issues.
```python
import io
import requests

import numpy as np
import onnx
import onnxruntime
from PIL import Image
import torch
import torchvision
import torchvision.transforms.functional as TF


if __name__ == '__main__':
    # Load pretrained model
    torch_model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)
    torch_model.eval()

    # Export to ONNX
    dummy_input = torch.randn(1, 3, 300, 300)
    onnx_model_path = './fasterrcnn_mobilenet_v3.onnx'
    
    torch.onnx.export(torch_model, dummy_input, onnx_model_path,
                      input_names=['images'],
                      output_names=['boxes', 'labels', 'scores'],
                      do_constant_folding=True,
                      export_params=True, opset_version=11, verbose=True)
    
    # Test the ONNX model with ONNX Runtime
    ort_session = onnxruntime.InferenceSession(onnx_model_path)

    def to_numpy(tensor):
        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()
    
    with torch.no_grad():
        # Load an image with only single object
        image = Image.open(io.BytesIO(requests.get('https://farm8.staticflickr.com/7178/6859626097_3b561d69bf_z.jpg').content))
        
        # Image with multiple objects: no error, but produces inconsistent results
        # image = Image.open(io.BytesIO(requests.get('https://farm4.staticflickr.com/3334/3253284080_3268267cc5_z.jpg').content))
        
        image = image.resize((300, 300))
        torch_input = torch.unsqueeze(TF.to_tensor(image), 0)

        # PyTorch works fine
        torch_output = torch_model(torch_input)

        print('PyTorch scores: ', torch_output[0]['scores'])

        # ONNX Runtime raises an error or produces inconsistent results
        ort_input = {ort_session.get_inputs()[0].name: to_numpy(torch_input)}
        ort_output = ort_session.run(None, ort_input)

        print('ORT scores: ', ort_output[2])
```

### Versions

PyTorch version: 1.9.1
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.27

Python version: 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-80-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 10.2.89
GPU models and configuration:
GPU 0: Tesla V100-DGXS-32GB
GPU 1: Tesla V100-DGXS-32GB
GPU 2: Tesla V100-DGXS-32GB
GPU 3: Tesla V100-DGXS-32GB

Nvidia driver version: 450.142.00
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.20.1
[pip3] numpydoc==1.1.0
[pip3] torch==1.9.1
[pip3] torchvision==0.10.1
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.2.89              hfd86e86_1
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.2.0           h06a4308_296
[conda] mkl-service               2.3.0            py38h27cfd23_1
[conda] mkl_fft                   1.3.0            py38h42c9631_2
[conda] mkl_random                1.2.1            py38ha9443f7_2
[conda] mypy_extensions           0.4.3                    py38_0
[conda] numpy                     1.20.1           py38h93e21f0_0
[conda] numpy-base                1.20.1           py38h7d8b39e_0
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1
[conda] pytorch                   1.9.1           py3.8_cuda10.2_cudnn7.6.5_0    pytorch
[conda] torchvision               0.10.1               py38_cu102    pytorch

cc @neginraoof"
torch.onnx.export failes due to missing __round__ function,pytorch/vision,2021-10-13 08:13:31,2,module: onnx,4607,1024933355,"### 🐛 Describe the bug

When exporting a trained CNN to onnx file format using torch.onnx.export(), an exception is thrown from torchvision/transforms/functional.py and the model is not saved.

## To Reproduce

Steps to reproduce the behavior:

1. Define a CNN-torch.Model instance similar to a VGG-Network, train it
2. Try to export the trained model to an *.onnx - file using the save_model() function below

``` python
from os import makedirs, join
import torch
import torch.onnx as onnx

def save_model(model, batch_size, image_size, save_dir, input_channels, tensor_type):
    makedirs(save_dir, exist_ok=True)

    torch.save(model.state_dict(), join(save_dir, f""model.pt""))

    x = torch.randn(batch_size, len(input_channels), *image_size, requires_grad=True)  # a bogus input tensor; needed for tracing onnx export
    x = x.type(tensor_type) # move to device

    onnx.export(model,                                          # model being run
                x,                                              # model input (or a tuple for multiple inputs)
                join(save_dir, f""model.onnx""),                  # where to save the model (can be a file or file-like object)
                export_params=True,                             # store the trained parameter weights inside the model file
                opset_version=10,                               # the ONNX version to export the model to
                do_constant_folding=True,                       # whether to execute constant folding for optimization
                input_names = ['input'],                        # the model's input names
                output_names = ['output'],                      # the model's output names
                dynamic_axes={'input' : {0 : 'batch_size'},     # variable length axes
                            'output' : {0 : 'batch_size'}})
    print(""onnx model exported"")  
```


``` bash
~/miniconda3/envs/playground/lib/python3.8/site-packages/torchvision/transforms/functional.py in center_crop(img, output_size)
    514             return img
    515 
--> 516     crop_top = int(round((image_height - crop_height) / 2.))
    517     crop_left = int(round((image_width - crop_width) / 2.))
    518     return crop(img, crop_top, crop_left, crop_height, crop_width)

TypeError: type Tensor doesn't define __round__ method

```

## Expected behavior

Model is saved to file and can be used with onnxruntime library later on.

## Workaround (or possible solution)

in `torchvision/transforms/functional.py` function `center_crop` change lines 516-517 to use the numpy.round() instead of built-in round():

``` python
# old code for reference:    
# crop_top = int(round((image_height - crop_height) / 2.))
# crop_left = int(round((image_width - crop_width) / 2.))
# working by using numpy.round():
crop_top = int(np.round((image_height - crop_height) / 2.))
crop_left = int(np.round((image_width - crop_width) / 2.))
```

### Versions

Collecting environment information...
PyTorch version: 1.9.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Jun  4 2021, 15:09:15)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 460.91.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.4
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.2
[pip3] torch==1.9.1
[pip3] torchvision==0.10.1
[conda] numpy                     1.21.2                   pypi_0    pypi
[conda] torch                     1.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1                   pypi_0    pypi

cc @neginraoof"
Adding CenterNet to Torchvision/detection,pytorch/vision,2021-10-12 03:49:25,5,needs discussion#topic: object detection#new feature,4595,1023327995,"### 🚀 The feature

Add [CenterNet](https://arxiv.org/abs/1904.08189) to torchvision


### Motivation, pitch

centernet is a better one-stage object detector than SSD and it is also very practical.This anchor-free detector is currently relatively popular.

### Alternatives

_No response_

### Additional context

_No response_

cc @datumbox"
Add typing annotations to models/detection,pytorch/vision,2021-10-10 12:25:29,3,module: models#topic: object detection#code quality,4582,1021970135,"### 🚀 The feature

This is an issue to track down all the files in detection module and work up the chain.

- [x] _utils.py #4583
- [ ] anchor_utils.py #4599
- [x] backbone_utils.py #4603
- [x] image_list.py #4602
- [ ] roi_heads.py #4612
- [x] rpn.py #4619
- [x] generalized_rcnn.py #4631
- [ ] transform.py #4630

To type the models, all the above files would need to have annotations.

- [ ] faster_rcnn.py #4636
- [ ] mask_rcnn.py
- [ ] keypoint_rcnn.py  
- [ ] retinanet.py
- [ ] ssd.py
- [ ] ssdlite.py



### Motivation, pitch

@khushi-411 and I will sync and work on these files.

Solving This issue also helps #2025 

As first step I'm closing #4220 and splitting it up into multiple PRs preferably one file each.

Hopefully we should complete this soon.

### Alternatives

Note that there are cases where we have make JIT happy over mypy.

Most of these cases occur as JIT does not Support Tuple fully. In most cases we should prefer List.


### Additional context

cc @datumbox @pmeier @NicolasHug "
Put back build_docs job caching,pytorch/vision,2021-10-07 17:40:32,0,module: documentation,4568,1020298665,"The caching of the build_docs job was removed in https://github.com/pytorch/vision/pull/4565 in order to fix https://github.com/pytorch/vision/issues/4245

We should put the caching back, while making sure that https://github.com/pytorch/vision/issues/4245 is still fixed, i.e. that the sphinx-gallery backreferences are present in the docs."
No longer able to build from source with `pip install -v .`,pytorch/vision,2021-10-05 20:07:41,3,,4542,1016759979,"### 🐛 Describe the bug

After PR https://github.com/pytorch/vision/pull/4384, I'm no longer able to build torchvision with 

```
pip install -v .
```

, though 

```
python setup.py develop
```
still works.

**Is this the intended behavior now?** 

The `pip install -v .` now reports error message

```
xxx@yyy:/opt/pytorch/vision# pip install -v .
Using pip 21.2.2 from /opt/conda/lib/python3.8/site-packages/pip (python 3.8)
Processing /opt/pytorch/vision
  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.
   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.
  Running command /opt/conda/bin/python /tmp/pip-standalone-pip-qzzditq9/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-klifg95l/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools>=40.8.0' wheel
  Collecting setuptools>=40.8.0
    Using cached setuptools-58.2.0-py3-none-any.whl (946 kB)
  Collecting wheel
    Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)
  Installing collected packages: wheel, setuptools
  Successfully installed setuptools-58.2.0 wheel-0.37.0
  WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
  Installing build dependencies ... done
  Running command /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmptw3tr8z6
  Traceback (most recent call last):
    File ""/opt/conda/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 349, in <module>
      main()
    File ""/opt/conda/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 331, in main
      json_out['return_val'] = hook(**hook_input['kwargs'])
    File ""/opt/conda/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py"", line 117, in get_requires_for_build_wheel
      return hook(config_settings)
    File ""/tmp/pip-build-env-klifg95l/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 154, in get_requires_for_build_wheel
      return self._get_build_requires(
    File ""/tmp/pip-build-env-klifg95l/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 135, in _get_build_requires
      self.run_setup()
    File ""/tmp/pip-build-env-klifg95l/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 258, in run_setup
      super(_BuildMetaLegacyBackend,
    File ""/tmp/pip-build-env-klifg95l/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 150, in run_setup
      exec(compile(code, __file__, 'exec'), locals())
    File ""setup.py"", line 12, in <module>
      import torch
  ModuleNotFoundError: No module named 'torch'
  Getting requirements to build wheel ... error
WARNING: Discarding file:///opt/pytorch/vision. Command errored out with exit status 1: /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmptw3tr8z6 Check the logs for full command output.
ERROR: Command errored out with exit status 1: /opt/conda/bin/python /opt/conda/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmptw3tr8z6 Check the logs for full command output.
```



### Versions

I'm using the latest pytorch version 1.11.0a0+git458a00b https://github.com/pytorch/pytorch/commit/458a00bacb68998a75a87af153e6ae3690698bc2, 

```
OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6
Libc version: glibc-2.31

Python version: 3.8.11 (default, Aug  3 2021, 15:09:35)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.15.0-123-generic-x86_64-with-glibc2.17
Is CUDA available: True
```

cc @ptrblck @pmeier @NicolasHug "
[BLOCKED] Replace IntermediateLayerGetter with feature_extraction.create_feature_extractor(),pytorch/vision,2021-10-05 12:11:17,10,enhancement#needs discussion#module: models,4540,1016249262,"### 🚀 The feature

A new feature extraction utility based on FX was introduced by #4302. It allows us to extract internal outputs of a module in a simpler way and it should be a drop-in replacement of the old `IntermediateLayerGetter` class.

Unfortunately the `create_feature_extractor()` util is not currently used anywhere on vision. We should investigate using it to replace some of the old approaches. Here is a list of candidates:
- [ ] Replace `IntermediateLayerGetter` calls in Segmentation models [[1](https://github.com/pytorch/vision/blob/5f0edb97b46e5bff71dc19dedef05c5396eeaea2/torchvision/models/segmentation/segmentation.py#L63), [2](https://github.com/pytorch/vision/blob/5f0edb97b46e5bff71dc19dedef05c5396eeaea2/torchvision/models/segmentation/segmentation.py#L119)]
- [ ] Replace `IntermediateLayerGetter` calls in Detection models [[3](https://github.com/pytorch/vision/blob/5f0edb97b46e5bff71dc19dedef05c5396eeaea2/torchvision/models/detection/backbone_utils.py#L37)]
- [ ] Rewrite model surgery from the `SSDFeatureExtractorVGG` [[4](https://github.com/pytorch/vision/blob/5f0edb97b46e5bff71dc19dedef05c5396eeaea2/torchvision/models/detection/ssd.py#L441)] and `SSDLiteFeatureExtractorMobileNet` modules [[5](https://github.com/pytorch/vision/blob/5f0edb97b46e5bff71dc19dedef05c5396eeaea2/torchvision/models/detection/ssdlite.py#L125-L129)].

Extra care needs to be given to ensure that the old weights of pre-trained models can still load and the validation statistics remain the same.

### Motivation, pitch

Using the latest FX-based utility will help us future proof our code-base.


cc @datumbox"
Resolving merge conflicts for PRs opened before Oct' 4,pytorch/vision,2021-10-05 09:42:49,0,,4539,1016108669,"After #4384 was merged on Oct' 4, we now use `ufmt` to format our code base and enforce this code style on every PR. In case you opened a PR before that, you will likely see merge conflicts popping up. Here is what you do:

```bash
$ git checkout main
$ git pull
$ git checkout $MY_FEATURE_BRANCH
$ git merge -X ours main
$ pre-commit run --all-files
$ git commit -am ""fix code format""
$ git push
```

This should work for most cases. If you encounter any issues with that, feel free to ping `@pmeier` on your PR to help you fix it."
Torchvision transform hangs when using python multiprocessing and model inference,pytorch/vision,2021-10-03 16:07:05,0,module: transforms,4529,1014444078,"### 🐛 Describe the bug

Torchvision transforms cause code to hang when using python multiprocessing and a model on inference. 

In particular, I'm seeing hangs when using a CLIP model and entirely unrelated torch code running in a multiprocess. An issue was filed against the CLIP repository here (https://github.com/openai/CLIP/issues/130) but I figured this should also be flagged on torchvision because I don't think this issue has to do with their model in particular. The model hangs on `img.permute((2, 0, 1)).contiguous()` [on this line](https://github.com/pytorch/vision/blob/eb2fb25304dd3180c092231bbe83cd88d25f7cb3/torchvision/transforms/functional.py#L148) in transforms/functional.py. This is in turn called by the ToTensor transform at `F.to_tensor(pic)` [on this line](https://github.com/pytorch/vision/blob/eb2fb25304dd3180c092231bbe83cd88d25f7cb3/torchvision/transforms/transforms.py#L98). 

Minimal code sample in which I split the to_tensor transform and remove any unneeded parts:
```
import torch
import clip
from PIL import Image
import multiprocessing as mp

model = clip.model.CLIP(512, 224, 12, 768, 32, 77, 49408, 512, 8, 12)


def test():
  print(""GETTING IMAGE"")
  im = Image.open(""CLIP.png"")
  print(""CONVERTING"")
  im = im.convert('RGB')
  print(""MADE TENSOR"")
  img = torch.ByteTensor(torch.ByteStorage.from_buffer(im.tobytes()))
  print(""VIEW"")
  img = img.view(im.size[1], im.size[0], len(im.getbands()))
  print(""PERMUTING"")
  img = img.permute((2, 0, 1))
  print(""CONTINGUOUS"")
  img = img.contiguous()
  print(""DIV"")
  img = img.float().div(255)
  print(""UNSQUEEZE"")
  img = img.unsqueeze(0)
  return img


p = mp.Process(target=test, daemon=True)
p.start()
p.join()
```
This code will hang on the `img.contiguous()` call, but _only_ if the model is initialized at the top. If the model at the top is commented out, this works as expected. Further, note that the multiprocess function does not even use the model. 

### Versions

```
Collecting environment information...
/home/amol/code/soot/debugging/clip_tests/env/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
PyTorch version: 1.7.1
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.3 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.10 (default, Jun  2 2021, 10:49:15)  [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.29
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.0
[pip3] torch==1.7.1
[pip3] torchvision==0.8.2
[conda] Could not collect
```

cc @vfdev-5 @datumbox"
torchvision.utils.make_grid pad_value attribute to accept RGB value,pytorch/vision,2021-09-29 13:39:38,5,,4526,1013621915,"## 🚀 Feature
change torchvision.utils.make_grid pad_value attribute to accept an RGB value (in whatever format, either hex string, or 3-tuple)

## Motivation

This would allow to pad image grids with colors other than just grayscale

## Pitch

I want to be able to pass an RGB value to pad_value of torchvision.utils.make_grid

## Alternatives

changing the padded value manually, that's quite tedious. Or just not using this function.
"
can  torchvision build from source  on windows?,pytorch/vision,2021-09-28 02:55:35,6,,4492,1009070954,"I have got the pt model of mask rcnn, and now I want to use it on C++, does libtoch seem to not support it? This seems to require torchvison build from source, but I failed. Does torchvison build support window? Or do I need to pay attention to something?



`git clone https://github.com/pytorch/vision.git

 cd vision
 
mkdir build
 
cd build
 
cmake -DCMAKE_PREFIX_PATH=""D:\libtorch;D:\pybind11"" ..`"
Use stable sort in nms,pytorch/vision,2021-09-27 12:41:03,1,enhancement#module: ops,4488,1008100843,"### 🚀 The feature

Please use stable sort in nms since torch1.9 has supported stable sort.

### Motivation, pitch

Why we need stable sort:
In quantization model, many score are equal due to quantization, so when using unstable sort, the result of nms is non-reproducible.

### Alternatives

_No response_

### Additional context

_No response_"
"""libtorch.so: error adding symbols: File in wrong format"" when trying to compile torchvision on Linux on IBM z machine",pytorch/vision,2021-09-24 07:13:37,2,topic: build,4476,1006156777,"Hello Guys,

I'm getting below error while trying to compile torchvision on linux on IBM z14 machine.
Could anyone help have a look on this?

My env:
-------------------------------------------------------------------------------------------
redhat 8.4 on IBM LinuxONE
gcc (GCC) 8.4.1 20200928 (Red Hat 8.4.1-1)

command sequence to compile:
-------------------------------------------------------------------------------------------
wget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip
unzip libtorch-shared-with-deps-latest.zip 

git clone git://github.com/pytorch/vision.git
cd vision
mkdir build
cd build
# Add -DWITH_CUDA=on support for the CUDA if needed
cmake -DCMAKE_PREFIX_PATH=$PWD/../../libtorch ..
make
make install

cmake and make logs:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

(base) [root@qiskit build]# cmake -DCMAKE_PREFIX_PATH=$PWD/../../libtorch .. 
-- Found PNG: /usr/lib64/libpng.so (found version ""1.6.34"") 
-- Found JPEG: /usr/lib64/libjpeg.so (found version ""62"") 
-- Configuring done
CMake Warning at CMakeLists.txt:73 (add_library):
  Cannot generate a safe runtime search path for target torchvision because
  files in some directories may conflict with libraries in implicit
  directories:

    runtime library [libpng16.so.16] in /usr/lib64 may be hidden by files in:
      /opt/anaconda/lib

  Some of these libraries may not be found correctly.


-- Generating done
-- Build files have been written to: /root/vision/build
(base) [root@qiskit build]# 
(base) [root@qiskit build]# 
(base) [root@qiskit build]# 
(base) [root@qiskit build]# 
(base) [root@qiskit build]# 
(base) [root@qiskit build]# 
(base) [root@qiskit build]# make 
Scanning dependencies of target torchvision
[  2%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/common_jpeg.cpp.o
[  5%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o
[  7%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_jpeg.cpp.o
[ 10%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_png.cpp.o
[ 12%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_jpeg.cpp.o
[ 15%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_png.cpp.o
[ 17%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/read_write_file.cpp.o
[ 20%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cuda/decode_jpeg_cuda.cpp.o
[ 22%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/image.cpp.o
[ 25%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/alexnet.cpp.o
[ 27%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/densenet.cpp.o
[ 30%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/googlenet.cpp.o
[ 32%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/inception.cpp.o
[ 35%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/mnasnet.cpp.o
[ 37%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/mobilenet.cpp.o
[ 40%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/resnet.cpp.o
[ 42%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/shufflenetv2.cpp.o
[ 45%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/squeezenet.cpp.o
[ 47%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/vgg.cpp.o
[ 50%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/deform_conv2d_kernel.cpp.o
[ 52%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/ps_roi_align_kernel.cpp.o
[ 55%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/ps_roi_pool_kernel.cpp.o
[ 57%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/roi_align_kernel.cpp.o
[ 60%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/roi_pool_kernel.cpp.o
[ 62%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp.o
[ 65%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/interpolate_aa_kernels.cpp.o
[ 67%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/nms_kernel.cpp.o
[ 70%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/ps_roi_align_kernel.cpp.o
[ 72%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/ps_roi_pool_kernel.cpp.o
[ 75%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_align_kernel.cpp.o
[ 77%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp.o
[ 80%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/deform_conv2d.cpp.o
[ 82%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/interpolate_aa.cpp.o
[ 85%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/nms.cpp.o
[ 87%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/ps_roi_align.cpp.o
[ 90%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/ps_roi_pool.cpp.o
[ 92%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/roi_align.cpp.o
[ 95%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/roi_pool.cpp.o
[ 97%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/vision.cpp.o
In file included from /root/vision/torchvision/csrc/vision.cpp:1:
/root/vision/torchvision/csrc/vision.h:10:40: warning: ‘_register_ops’ initialized and declared ‘extern’
 extern ""C"" VISION_INLINE_VARIABLE auto _register_ops = &cuda_version;
                                        ^~~~~~~~~~~~~
[100%] Linking CXX shared library libtorchvision.so
/root/libtorch/lib/libtorch.so: error adding symbols: File in wrong format
collect2: error: ld returned 1 exit status
make[2]: *** [CMakeFiles/torchvision.dir/build.make:680: libtorchvision.so] Error 1
make[1]: *** [CMakeFiles/Makefile2:95: CMakeFiles/torchvision.dir/all] Error 2
make: *** [Makefile:149: all] Error 2
(base) [root@qiskit build]# 

Thanks
Regards"
C++ compiling torchvision is too troublesome！！！,pytorch/vision,2021-09-19 13:42:59,3,awaiting response#topic: build,4447,1000317252,"### 🚀 The feature

C++ compiling torchvision is too troublesome. After debugging for 3 days, I haven't found the problem. Since libtorch can solve the C++ installation problem of torch, why does torchvison not provide an interface such as libtorchvison and package compiled code?

### Motivation, pitch

C++ compiling torchvision is too troublesome. After debugging for 3 days, I haven't found the problem. Since libtorch can solve the C++ installation problem of torch, why does torchvison not provide an interface such as libtorchvison and package compiled code?

### Alternatives

_No response_

### Additional context

_No response_"
How to use TenCrop and FiveCrop on video,pytorch/vision,2021-09-18 16:08:03,0,question#module: video,4445,1000071662,"Hi everyone,
I am wanting to use TenCrop and FiveCrop on video but I have no idea how to do this.
Can you tell me how to do it?
Sorry, I am new to this field.
Thank you very much!"
CU_VERSION and CUDA_VERSION is confusing on Windows CI.,pytorch/vision,2021-09-18 03:58:32,0,,4443,999899047,"### 🐛 Describe the bug

It's just a memo for developer to understand some code to get the cuda version.

In unittest_windows_cpu and cmake_windows_gpu
`CU_VERSION is cpu, CUDA_VERION is empty.`
In unitest_windows_gpu,
`CU_VERSION is emtpy, CUDA_VERSION is MAJOR.MINOR (e.g. 10.2)`
In cmake_windows_gpu
`CU_VERSION is something like cu102, CUDA_VERSION is empty`

### Versions

It has existed for a long time."
Investigate decrease in mAP in retinanet resnet50 model,pytorch/vision,2021-09-17 10:26:14,5,module: models#topic: object detection,4437,999197302,"### 🐛 Describe the bug

Reference: https://github.com/pytorch/vision/pull/4409#issuecomment-920723588

cc @datumbox"
Log polar transformation in torchvision.transforms,pytorch/vision,2021-09-16 22:18:50,5,needs discussion#module: transforms,4434,998706703,"### 🚀 The feature

Log polar transformation for images, which maps an image from the 2D Cartesian coordinate system to the log-polar system.
This transformation each pixel [x, y] to the (ρ, θ) coordinate system, where ρ is the log of the distance from the center and θ is the angle.

### Motivation, pitch

I've been researching ways of making one of my CNNs rotation invariant and happened to find [this paper](https://arxiv.org/abs/1911.01141). I looked all over the docs for a similar transformation but couldn't find any, so I was wondering if this transformation could be included in the `torchvision.transforms` module in order to make it easier to do this type of preprocessing on images.

### Alternatives

_No response_

### Additional context

I already have a minimal implementation for my personal use that is based on existing transformations in the package. If you think this feature will be useful, I could polish it up and open a pull request to merge it.

cc @vfdev-5 @datumbox"
read_video_from_file() causes seg fault with Python 3.9,pytorch/vision,2021-09-16 17:19:51,0,module: video,4430,998448520,"### 🐛 Describe the bug

https://app.circleci.com/pipelines/github/pytorch/vision/10411/workflows/5783f083-26e2-4209-b9c6-976de289d637/jobs/791355

```
Program received signal SIGSEGV, Segmentation fault.
0x00007f87b3031d22 in ?? () from /root/project/env/lib/libswscale.so.5
Missing separate debuginfos, use: debuginfo-install glibc-2.17-324.el7_9.x86_64
(gdb) bt
#0  0x00007f87b3031d22 in ?? () from /root/project/env/lib/libswscale.so.5
#1  0x00007f87b3030be4 in ?? () from /root/project/env/lib/libswscale.so.5
#2  0x00007f87b3006af4 in sws_scale () from /root/project/env/lib/libswscale.so.5
#3  0x00007f87b4ab1932 in ffmpeg::(anonymous namespace)::transformImage(SwsContext*, unsigned char const* const*, int*, ffmpeg::VideoFormat, ffmpeg::VideoFormat, unsigned char*, unsigned char**, int*) [clone .isra.9] ()
   from /root/project/torchvision/video_reader.so
#4  0x00007f87b4ab1bd9 in ffmpeg::VideoSampler::sample(unsigned char const* const*, int*, ffmpeg::ByteStorage*) () from /root/project/torchvision/video_reader.so
#5  0x00007f87b4ab2ac7 in ffmpeg::VideoStream::copyFrameBytes(ffmpeg::ByteStorage*, bool) () from /root/project/torchvision/video_reader.so
#6  0x00007f87b4aaea9a in ffmpeg::Stream::decodePacket(AVPacket const*, ffmpeg::DecoderOutputMessage*, bool, bool*) () from /root/project/torchvision/video_reader.so
#7  0x00007f87b4aaa183 in ffmpeg::Decoder::processPacket(ffmpeg::Stream*, AVPacket*, bool*, bool*) () from /root/project/torchvision/video_reader.so
#8  0x00007f87b4aaa80a in ffmpeg::Decoder::getFrame(unsigned long) () from /root/project/torchvision/video_reader.so
#9  0x00007f87b4aafe81 in ffmpeg::SyncDecoder::decode(ffmpeg::DecoderOutputMessage*, unsigned long) () from /root/project/torchvision/video_reader.so
#10 0x00007f87b4adbb55 in vision::video_reader::(anonymous namespace)::readVideo(bool, at::Tensor const&, std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long) ()
   from /root/project/torchvision/video_reader.so
#11 0x00007f87b4add99a in vision::video_reader::read_video_from_file(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long) () from /root/project/torchvision/video_reader.so
#12 0x00007f87b4ae0d8f in std::decay<c10::guts::infer_function_traits<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::List<at::Tensor> (*)(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long), c10::List<at::Tensor>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long> > >::type::return_type>::type c10::impl::call_functor_with_args_from_stack_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::List<at::Tensor> (*)(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long), c10::List<at::Tensor>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long> >, false, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, 9ul, 10ul, 11ul, 12ul, 13ul, 14ul, 15ul, 16ul, 17ul, 18ul, std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long>(c10::OperatorKernel*, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*, std::integer_sequence<unsigned long, 0ul, 1ul, 2ul, 3ul, 4ul, 5ul, 6ul, 7ul, 8ul, 9ul, 10ul, 11ul, 12ul, 13ul, 14ul, 15ul, 16ul, 17ul, 18ul>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long>*) () from /root/project/torchvision/video_reader.so
#13 0x00007f87b4ae0e92 in c10::impl::make_boxed_from_unboxed_functor<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::List<at::Tensor> (*)(std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long), c10::List<at::Tensor>, c10::guts::typelist::typelist<std::string, double, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long, long> >, false>::call(c10::OperatorKernel*, c10::OperatorHandle const&, c10::DispatchKeySet, std::vector<c10::IValue, std::allocator<c10::IValue> >*) () from /root/project/torchvision/video_reader.so
#14 0x00007f881b9ef98b in std::_Function_handler<void (std::vector<c10::IValue, std::allocator<c10::IValue> >&), torch::jit::(anonymous namespace)::createOperatorFromC10(c10::OperatorHandle const&)::{lambda(std::vector<c10::IValue, std::allocator<c10::IValue> >&)#1}>::_M_invoke(std::_Any_data const&, std::vector<c10::IValue, std::allocator<c10::IValue> >&) () from /root/project/env/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so
#15 0x00007f88239dbb53 in torch::jit::invokeOperatorFromPython(std::vector<std::shared_ptr<torch::jit::Operator>, std::allocator<std::shared_ptr<torch::jit::Operator> > > const&, pybind11::args, pybind11::kwargs const&) ()
   from /root/project/env/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#16 0x00007f88239af864 in torch::jit::initJITBindings(_object*)::{lambda(std::string const&)#132}::operator()(std::string const&) const::{lambda(pybind11::args, {lambda(std::string const&)#132}::kwargs)#1}::operator()(pybind11, pybind11::args) const ()
   from /root/project/env/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#17 0x00007f88239b002f in void pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::{lambda(std::string const&)#132}::operator()(std::string const&) const::{lambda(pybind11::args, pybind11::kwargs)#1}, pybind11::object, {lambda(std::string const&)#132}, pybind11::args, pybind11::name, pybind11::doc>(torch::jit::initJITBindings(_object*)::{lambda(std::string const&)#132}::operator()(std::string const&) const::{lambda(pybind11::args, pybind11::kwargs)#1}&&, pybind11::object (*)({lambda(std::string const&)#132}, pybind11::args), pybind11::name const&, pybind11::doc const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail) () from /root/project/env/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#18 0x00007f882312d63b in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /root/project/env/lib/python3.9/site-packages/torch/lib/libtorch_python.so
#19 0x0000555bedf0d964 in cfunction_call () at /tmp/build/80754af9/python-split_1629314979626/work/Objects/methodobject.c:539
#20 0x0000555bedf07c0f in _PyObject_MakeTpCall () at /tmp/build/80754af9/python-split_1629314979626/work/Objects/call.c:191
#21 0x0000555bedf97789 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=<optimized out>, args=0x555bf2271418, callable=<optimized out>, tstate=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:116
#22 PyObject_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:127
#23 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x555bef577810) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:5072
#24 _PyEval_EvalFrameDefault () at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:3487
#25 0x0000555bedf4d552 in _PyEval_EvalFrame () at /tmp/build/80754af9/python-split_1629314979626/work/Include/internal/pycore_ceval.h:40
#26 _PyEval_EvalCode () at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:4327
#27 0x0000555bedf4e0b7 in _PyFunction_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Objects/call.c:396
#28 0x0000555bedec6336 in _PyObject_VectorcallTstate (kwnames=0x7f87b4b1dfa0, nargsf=<optimized out>, args=<optimized out>, callable=0x7f87b4b2d550, tstate=<optimized out>)
    at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:118
#29 PyObject_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:127
#30 call_function (kwnames=0x7f87b4b1dfa0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:5072
#31 _PyEval_EvalFrameDefault (tstate=<optimized out>, f=0x555bf226e6f0, throwflag=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:3535
#32 0x0000555bedf4d74b in _PyEval_EvalFrame () at /tmp/build/80754af9/python-split_1629314979626/work/Include/internal/pycore_ceval.h:40
#33 _PyEval_EvalCode () at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:4327
#34 0x0000555bedf4e0b7 in _PyFunction_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Objects/call.c:396
#35 0x0000555bedec6ff4 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=<optimized out>, args=0x555bf226e698, callable=0x7f87b4b2d940, tstate=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:118
#36 PyObject_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:127
#37 call_function (kwnames=0x0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=0x555bef577810) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:5072
#38 _PyEval_EvalFrameDefault (tstate=<optimized out>, f=0x555bf226e4a0, throwflag=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:3487
#39 0x0000555bedf4d552 in _PyEval_EvalFrame () at /tmp/build/80754af9/python-split_1629314979626/work/Include/internal/pycore_ceval.h:40
#40 _PyEval_EvalCode () at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:4327
#41 0x0000555bedf4e0b7 in _PyFunction_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Objects/call.c:396
#42 0x0000555bedec6336 in _PyObject_VectorcallTstate (kwnames=0x7f883a6eb4f0, nargsf=<optimized out>, args=<optimized out>, callable=0x7f87b4b2de50, tstate=<optimized out>)
    at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:118
#43 PyObject_Vectorcall () at /tmp/build/80754af9/python-split_1629314979626/work/Include/cpython/abstract.h:127
#44 call_function (kwnames=0x7f883a6eb4f0, oparg=<optimized out>, pp_stack=<synthetic pointer>, tstate=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:5072
#45 _PyEval_EvalFrameDefault (tstate=<optimized out>, f=0x555bef5cfd80, throwflag=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:3535
#46 0x0000555bedf4d552 in _PyEval_EvalFrame () at /tmp/build/80754af9/python-split_1629314979626/work/Include/internal/pycore_ceval.h:40
#47 _PyEval_EvalCode () at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:4327
#48 0x0000555bedffcfcc in _PyEval_EvalCodeWithName (qualname=0x0, name=0x0, closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwstep=2, kwcount=0, kwargs=<optimized out>, kwnames=<optimized out>, argcount=<optimized out>, args=<optimized out>, 
    locals=<optimized out>, globals=<optimized out>, _co=<optimized out>) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:4359
#49 PyEval_EvalCodeEx () at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:4375
#50 0x0000555bedf4e57b in PyEval_EvalCode (co=co@entry=0x7f883a5b8190, globals=globals@entry=0x7f883a684b00, locals=locals@entry=0x7f883a684b00) at /tmp/build/80754af9/python-split_1629314979626/work/Python/ceval.c:826
#51 0x0000555bedffd07b in run_eval_code_obj () at /tmp/build/80754af9/python-split_1629314979626/work/Python/pythonrun.c:1219
#52 0x0000555bee0334d5 in run_mod () at /tmp/build/80754af9/python-split_1629314979626/work/Python/pythonrun.c:1240
#53 0x0000555beded9f34 in pyrun_file (fp=0x555bef5ada10, filename=0x7f883a64d6b0, start=257, globals=0x7f883a684b00, locals=0x7f883a684b00, closeit=1, flags=0x7ffd6d8540f8) at /tmp/build/80754af9/python-split_1629314979626/work/Python/pythonrun.c:1138
#54 0x0000555bee0381cf in pyrun_simple_file (flags=0x7ffd6d8540f8, closeit=1, filename=0x7f883a64d6b0, fp=0x555bef5ada10) at /tmp/build/80754af9/python-split_1629314979626/work/Python/pythonrun.c:449
#55 PyRun_SimpleFileExFlags () at /tmp/build/80754af9/python-split_1629314979626/work/Python/pythonrun.c:482
#56 0x0000555bee0382eb in PyRun_AnyFileExFlags () at /tmp/build/80754af9/python-split_1629314979626/work/Python/pythonrun.c:91
#57 0x0000555bee03891c in pymain_run_file (cf=0x7ffd6d8540f8, config=0x555bef576300) at /tmp/build/80754af9/python-split_1629314979626/work/Modules/main.c:379
```"
video_reader test crashes on Windows,pytorch/vision,2021-09-16 17:11:32,0,module: video,4429,998441679,"### 🐛 Describe the bug

User@WinDev2108Eval MINGW64 ~/Documents/repos/vision (main)
$ pytest test/test_video_reader.py

============================= test session starts =============================
platform win32 -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0
rootdir: C:\Users\User\Documents\repos\vision, configfile: pytest.ini
collected 21 items

test\test_video_reader.py sFWindows fatal exception: access violation

Current thread 0x00000320 (most recent call first):
  File ""C:\Users\User\Documents\repos\vision\test\test_video_reader.py"", line 491 in test_read_video_from_file_read_single_stream_only
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\python.py"", line 183 in pytest_pyfunc_call
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\python.py"", line 1641 in runtest
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 162 in pytest_runtest_call
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 255 in <lambda>
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 311 in from_call
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 254 in call_runtest_hook
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 215 in call_and_report
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 126 in runtestprotocol
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\runner.py"", line 109 in pytest_runtest_protocol
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\main.py"", line 348 in pytest_runtestloop
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\main.py"", line 323 in _main
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\main.py"", line 269 in wrap_session
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\main.py"", line 316 in pytest_cmdline_main
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_callers.py"", line 39 in _multicall
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_manager.py"", line 80 in _hookexec
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\pluggy\_hooks.py"", line 265 in __call__
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\config\__init__.py"", line 162 in main
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\site-packages\_pytest\config\__init__.py"", line 185 in console_main
  File ""C:\Users\User\anaconda3\envs\py38_env1\Scripts\pytest.exe\__main__.py"", line 7 in <module>
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\runpy.py"", line 87 in _run_code
  File ""C:\Users\User\anaconda3\envs\py38_env1\lib\runpy.py"", line 194 in _run_module_as_main
Segmentation fault


### Versions

$ python collect_env.py
Collecting environment information...
PyTorch version: 1.10.0.dev20210910
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 Enterprise Evaluation
GCC version: Could not collect
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.8.11 (default, Aug  6 2021, 09:57:55) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19043-SP0
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.2
[pip3] torch==1.10.0.dev20210910
[pip3] torchvision==0.11.0a0+719e120
[conda] Could not collect
"
[feature request] [discussion] mask utils in core,pytorch/vision,2021-09-15 13:54:44,11,,4415,997115751,"### 🚀 The feature

1. Extracting bounding boxes from label map: https://github.com/pytorch/pytorch/issues/22378#issuecomment-881954924, https://github.com/pytorch/vision/issues/3960
2. Extracting label maps from RGB label maps (https://github.com/pytorch/pytorch/issues/5436)
3. Conversion of RGB uint8 tensors to RGBA (or ARGB) uint32 tensor (https://github.com/pytorch/pytorch/issues/5436#issuecomment-920034956) for extracting ""unique"" labels faster
4. Compression of masks (relevant for images with many objects / segments), e.g. RLE encoding / decoding as found in pycocotools.mask

### Motivation, pitch

In detection/segmentation these utils are very frequent

### Alternatives

_No response_

### Additional context

_No response_"
Torchvision decode_jpeg memory leak,pytorch/vision,2021-09-07 15:15:23,23,bug#wontfix#module: io,4378,990093576,"### 🐛 Describe the bug

nvJPEG leaks memory and fails with OOM after ~1-2k images.

```
import torch
from torchvision.io import read_file, decode_jpeg

for i in range(1000): # increase to your liking till gpu OOMs (:
    img_u8 = read_file('lena.jpg')
    img_nv = decode_jpeg(img_u8, device='cuda')
```

Probably related to first response to https://github.com/pytorch/vision/issues/3848

```
RuntimeError: nvjpegDecode failed: 5
```

is exactly the message you get after OOM.

### Versions

PyTorch version: 1.9.0+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Arch Linux (x86_64)
GCC version: (GCC) 11.1.0
Clang version: 12.0.1
CMake version: version 3.21.1
Libc version: glibc-2.33

Python version: 3.8.7 (default, Jan 19 2021, 18:48:37)  [GCC 10.2.0] (64-bit runtime)
Python platform: Linux-5.13.8-arch1-1-x86_64-with-glibc2.2.5
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 2080 Ti
GPU 1: NVIDIA GeForce RTX 2080 Ti
GPU 2: NVIDIA GeForce GTX 1080

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/lib/libcudnn.so.8.2.2
/usr/lib/libcudnn_adv_infer.so.8.2.2
/usr/lib/libcudnn_adv_train.so.8.2.2
/usr/lib/libcudnn_cnn_infer.so.8.2.2
/usr/lib/libcudnn_cnn_train.so.8.2.2
/usr/lib/libcudnn_ops_infer.so.8.2.2
/usr/lib/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] adabelief-pytorch==0.2.0
[pip3] mypy-extensions==0.4.3
[pip3] numpy==1.19.5
[pip3] pytorch-lightning==1.4.5
[pip3] torch==1.9.0+cu111
[pip3] torchaudio==0.9.0
[pip3] torchfile==0.1.0
[pip3] torchmetrics==0.4.1
[pip3] torchvision==0.10.0+cu111
[conda] Could not collect
"
maskrcnn_resnet50_fpn use _save_for_lite_interpreter error,pytorch/vision,2021-09-06 02:10:14,9,module: models#torchscript,4386,992897467,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.
model = torchvision.models.detection.maskrcnn_resnet50_fpn(num_classes=24,pretrained=False).to(device)
model.load_state_dict(checkpoint['model'])
model.eval()
2.
script_model = torch.jit.script(model)
opt_model = optimize_for_mobile(script_model)
opt_model._save_for_lite_interpreter(""mask_rcnn_1.pt"")
3.
RuntimeError: __torch__ types other than torchbind (__torch__.torch.classes)are not supported in lite interpreter. Workaround: instead of using arbitrary class type (class Foo()), define a pytorch class (class Foo(torch.nn.Module)).
![SharedScreenshot](https://user-images.githubusercontent.com/45025677/132150714-0b7dcb03-30bb-4024-a37e-63dda279f93d.jpg)


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

PyTorch version: 1.9.0+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 10 专业版
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.18.1
Libc version: N/A

Python version: 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.19041-SP0
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] torch==1.9.0
[pip3] torchaudio==0.9.0
[pip3] torchvision==0.10.0
[conda] Could not collect

## Additional context

<!-- Add any other context about the problem here. -->


cc @datumbox @fmassa @vfdev-5 @pmeier"
Add AVA dataset,pytorch/vision,2021-09-04 11:49:40,2,module: datasets,4364,988276848,"### 🚀 The feature

Add the AVA dataset ([AVA: A Large-Scale Database for Aesthetic Visual Analysis](http://refbase.cvc.uab.es/files/MMP2012a.pdf)) in TorchVision.

### Motivation, pitch

It currently has [+600 citations](https://scholar.google.com/scholar?cites=4890286335335387755&as_sdt=2005&sciodt=0,5&hl=en). It's a commonly used dataset for image Aesthetics score prediction which is used for building image ranking models. It might be worth including it so that we can provide some Learn-to-rank examples using TorchVision.


cc @pmeier"
Use Union in Torchscript,pytorch/vision,2021-09-03 16:19:17,4,enhancement#torchscript,4360,987928045,"### 🚀 The feature

Use Union in Torchscript.

https://github.com/pytorch/pytorch/pull/64234

### Motivation, pitch

Now that Union typing support is merged. We can rewrite the workarounds which don't support Union.
E.g we can now proceed with #4323 

There maybe more places so we can track them in this Issue too.

### Alternatives

_No response_

### Additional context

_No response_"
Generalize masks_to_boxes op to support N-dimensional masks to bounding boxes conversion,pytorch/vision,2021-08-31 17:10:34,0,enhancement#module: ops,4339,984093531,"### 🚀 The feature

Ops to convert N-dimensional segmentation masks to N-dimensional bounding boxes.



### Motivation, pitch

Build upon #3960 to support the conversion of N-dimensional asks to bounding boxes.

Use cases:

1) I have 3D images (from medical imaging) and would like to convert segmentation masks (of tumors, aneurysms, etc.) to bounding boxes.

2) Sometimes different sources of data are stacked together, giving me 4D images (3D images, stacked along a new dimension to make the image source clear). I would also like to convert these segmentation masks to bounding boxes, although I will admit, I would compute bounding boxes in 3D and then stack those as well.


"
[RFC] API For Common Layers In Torchvision,pytorch/vision,2021-08-29 19:52:43,3,needs discussion#module: models#new feature,4333,982188040,"Picks up from discussion in https://github.com/pytorch/vision/pull/4293#discussion_r696471536

## 🚀 Feature

API for Commonly used Layers in building models.

## Motivation

A huge code duplication is involved in building very basic blocks for large neural networks. Some of these blocks can be standardized and re-used. Also these could be offered to end user as an API so that downstream libraries can build models easily.

E.g. for duplication are SqueezeExcitation, ConvBNRelu, etc.

## Pitch

Create an API called `torchvision.nn` or `torchvision.layers`
Our implementations need to be generic but not locked to certain activation functions or channels, etc.
These can be simply classes based on `nn.Module`.

An example

```
class ConvNormAct(nn.Module):
    def __init__(
        self,
        in_planes: int,
        out_planes: int,
        kernel_size: int = 3,
        stride: int = 1,
        groups: int = 1,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
        activation_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None: 

        super().__init__()
        #  Thsi could be even a nn.Sequential Block instead.
        padding = (kernel_size - 1) // 2 * dilation
        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation, groups, bias=False)
        self.norm_layer = norm_layer
        self.activation = activation_layer

    def forward(x: Tensor):
         x = self.conv1(x)
         if self.norm_layer is not None:
              x = self.norm_layer(x)
         x = self.activation(x)
        return x

```

User can use this as

```
from torchvision import layers
l1 = layers.ConvNormAct(3, 10, norm_layer=nn.BatchNorm2d(), activation_layer=nn.Relu(inplace=True))
dummy_input = torch.randn(3, 224, 224)
out = l1(dummy_input)
```

Also layers can be mixed into new custom models.

E.g.

```
class SimpleModel(nn.Module):
    def __init__():
        self.l1 = layers.ConvNormAct(3, 10, activation_layer=nn.Relu(inplace=True))
        self.l2  = layers.ConvNormAct(10, 3, activation_layer=nn.Relu(inplace=True))

    def forward(x: Tensor):
        x = self.l1(x)
        x = self.l2(x)
        return x
```

## Points to Consider

### We have torchvision.ops then why layers? 

Ops are transforms that do manipulations with pre-processing and post processing of structures such as Boxes, Masks, Anchors, etc. These are not used in ""model building"" but are optional steps for specific models.
Also these are 

E.g. NMS, IoU, RoI, etc.

One doesn't need ops for every model.
E.g. You don't need to do RoI align, for DeTR. Or you don't computer IoU for segmentation masks.

With separate API can be clear distinction in what are `layer` for models and operators for `tasks` such as detection, segmentation.

## Should `torchvision.nn` contain losses?

This is tricky, and for now I see no clear winner.
PyTorch does not differentiate the API for losses or layers.
E.g. we do `nn.Conv2d` which builds a convolutional layer. Also we do `nn.CrossEntropy` or `nn.MSE` which builds a loss function.

I'm not sure whether layers should be `torchvision.layers` or `torchvision.nn` (if implemented of course)

Users don't need to worry about colliding namespaces. They can do.

```
from torch import nn
from torchvision import nn as tnn
```

Note that `nn` seems to be the convention adopted by torchtext.

### Other points to consider.

1. Portability: - 
Currently most of the torchvision models are easily copy pastable. E.g. We can easily copy paste mobilenetv2.py file and edit it on the go to customize models.
By Adding such API we can reduce the internal code duplication but these files would no longer be single standalone files for models.

2. Layer Customization : -
Layer Customization has far too many options to consider. 
E.g. there are several implementations possible for BasicBlock of ResNet or some slight modifications of inverted residual layer.
One can't create an implementation that will suit all the needs for everyone. If one tries to, then the API would be significantly complicated.

4. TorchScript: -
We shouldn't be hampering torchscript compatibility of any model while implementing above API. 

## Additional context

Some candidates for layers
1. [ConvNormAct](https://github.com/pytorch/vision/blob/5ef75d7bc666c9af3dd05b62b075df4c23ca467c/torchvision/models/mobilenetv2.py#L32)  
2. LinearDropoutAct
3. [SqueezeExcitation](https://github.com/pytorch/vision/blob/5ef75d7bc666c9af3dd05b62b075df4c23ca467c/torchvision/models/efficientnet.py#L34)
4. [StochasticDepth](https://github.com/pytorch/vision/blob/main/torchvision/ops/stochastic_depth.py)
5. [BasicBlock](https://github.com/pytorch/vision/blob/5ef75d7bc666c9af3dd05b62b075df4c23ca467c/torchvision/models/resnet.py#L37)
6. [MLP](https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/detr.py#L289) Simple Multi Layer Perceptron, often duplicated in downstream library codebases E.g. Detr
7. [FrozenBatchNorm2d](https://github.com/pytorch/vision/blob/5ef75d7bc666c9af3dd05b62b075df4c23ca467c/torchvision/ops/misc.py#L45)  Also used in [DETR](https://github.com/facebookresearch/detr/blob/eb9f7e03ed8e2ed2cd55528989fe7df890bc3fc0/models/backbone.py#L19).
Not sure why it is under ops, it doesn't it there

Also Quantizable versions of these !
1. QuantizableConvNormAct
2. QuantizableLinearDropoutAct

Quantizable versions will allow users to directly fuse up the models.

Additionally I will recommend not hurrying this feature, we could create `torchvision.experimental.nn` and start working out things.(or probably I can try in a fork)
Linking plans #3911 #4187 

P.S. I'm a junior developer and all my thoughts are probably step too far. So please forgive me if I'm wrong.

cc @datumbox @pmeier @NicolasHug  @fmassa "
Adding imshow function to directly display image/video without moving tensor off GPU.,pytorch/vision,2021-08-29 06:32:35,2,,4331,982009038,"## 🚀 Feature
Looking for a `imshow` function similar to that of `cv2` to directly display tensor without moving it off GPU.

## Motivation

The main use case is for displaying video frames for real-time applications (such as webcam demo etc). 

If the image is already on the graphics card, it should be a way to directly display it. I currently cannot find other tools available to achieve this and I don't know how to implement it. I think it would be a useful function to include in torchvision.


## Alternatives

Moving tensor to CPU every frame, but this is very slow for real-time.

"
Question about torchvision.io.decode_image,pytorch/vision,2021-08-27 09:14:43,3,awaiting response#needs reproduction#module: io,4325,981044178,"when we use torchvision.io.decode_image(img,device = local_rank) to train with ddp，we find num_workers>0 can't work.

RuntimeError: DataLoader worker (pid 58353) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace."
[feature request] detection/roi_heads.py:postprocess_detections to also return keep indicator,pytorch/vision,2021-08-24 17:11:18,2,enhancement#needs discussion#module: models#topic: object detection,4309,978313935,"With this it's easier to hack and retrieve the necesssary box_features for kept boxes without having to reimplement the whole pipeline

cc @datumbox"
First layer with single channel,pytorch/vision,2021-08-20 07:55:10,2,,4297,975362529,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

When calling the model, it would be nice to be able to set the number of channels in the first layer.

## Motivation
<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

When using a grayscale image, I want the input to be a single channel.

## Pitch
<!-- A clear and concise description of what you want to happen. -->

ex)
```
self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
-> self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)

layers.append(ConvBNActivation(3, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer, activation_layer=nn.Hardswish))
-> layers.append(ConvBNActivation(1, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer,  activation_layer=nn.Hardswish))
```

## Alternatives
<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

My idea is to use parameters when calling the model.
ex)
`self.conv1 = nn.Conv2d(input, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)`

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
"
Document  the subtleties of to/from tensor conversions in a gallery example,pytorch/vision,2021-08-19 15:53:30,1,help wanted#module: documentation,4295,974812203,"Our utils for converting to and from tensors (to_tensor, pil_to_tensor, to_pil_image, etc.) are fairly subtle and can sometimes create confusion (https://github.com/pytorch/vision/pull/2483#issuecomment-902018493, https://github.com/pytorch/vision/issues/3782). 

It'd be nice to have a gallery example that exhaustively illustrates the stuff one should be aware of when using them, for example in terms of contiguity, range conversion, dtype, etc. and also explain when to use one over the other; In particular we could try to illustrate the differences between `to_tensor` and `pil_to_tensor` on the same set of inputs"
Models not pushed to torch hub,pytorch/vision,2021-08-18 12:54:49,4,module: documentation,4289,973635672,"## 📚 Documentation

Hi!

The following models are listed in our [hubconf.py](https://github.com/pytorch/vision/blob/master/hubconf.py)
but these models are not listed over [pytorch hub](https://pytorch.org/hub/research-models)

- [ ] MobileNetv3: - Mobilenet v3 small, Mobilenet v3 large
- [ ] Mobilenet Segmentation: - lraspp_mobilenet_v3_large
- [x] DeepLab Models: - deeplabv3_mobilenet_v3_large deeplabv3_resnet50 
- [x] FCN: - fcn_resnet50,
- [ ] MNasNet: - mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3
- [ ] EfficientNet: - efficientnet_b0, efficientnet_b1, efficientnet_b2, efficientnet_b3, efficientnet_b4, efficientnet_b5, efficientnet_b6, efficientnet_b7

Additional Context: -

I think there is ongoing issue with torch hub. #4156
Also we do not publish detection models to torch hub see  #1945 

P.S.
Please verify the list and feel to free to edit if I have missed something. 
Also probably this issue can be transferred to torch hub repository.

cc @datumbox @NicolasHug 
"
Addition of OHEM sampler,pytorch/vision,2021-08-18 09:12:35,10,needs discussion#new feature,4286,973449735,"## 🚀 Feature
Addition of OHEM (online hard example mining) sampler

## Motivation

Currently, a random sampler is being used in RoIHeads. However, other samplers have been shown to perform better than random sampling and heuristics. Thus, this motivates the need to introduce other samplers for example OHEM.

## Pitch

There can be flag/option introduced to use a particular sampler. Based on the flag, the corresponding sampler would be used."
Check Type Annotations for transforms/functional_pil and transforms/functional_tensor,pytorch/vision,2021-08-17 14:33:06,4,module: transforms,4282,972757302,"## 🚀 Feature

As spotted in https://github.com/pytorch/vision/pull/4234#pullrequestreview-731356320
There are some minor cases where type annotations do not match.

We should probably rectify them and ensure that the types annotations match.


P.S.
But I'm willing to leave for any new contributor! Otherwise I will complete it in the next week

cc @vfdev-5 @datumbox @pmeier I think this can be a good first issue for people awaiting to contribute :smile: 

"
"when install torchvision, conda can not install latest version, only version 0.2.2 can be installed",pytorch/vision,2021-08-13 11:29:35,3,topic: binaries#triage review#high priority,4273,970315323,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
I used conda to install pytorch in a new environment ,  it  can not install the latest torchvision version 0.10.0, only version 0.2.2 can be installed.
I find some similar issues already closed,  but I still don't know how to install latest version
![0 2 2](https://user-images.githubusercontent.com/35035210/129350174-cb287722-07a9-421e-bb75-2ccbde4fc0a5.png)

## To Reproduce
Steps to reproduce the behavior:

command from official website,
conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
install the latest version
<!-- A clear and concise description of what you expected to happen. -->

## Environment

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0):1.9.0
 - OS (e.g., Linux):windows10
 - How you installed PyTorch / torchvision (`conda`, `pip`, source):conda
 - Build command you used (if compiling from source):
 - Python version:3.9
 - CUDA/cuDNN version:11.1


## Additional context

<!-- Add any other context about the problem here. -->
"
C++ Inferencing using Torchscript Exported Torchvision model Erorr,pytorch/vision,2021-08-12 17:41:40,21,needs discussion#needs reproduction#module: c++ frontend#torchscript,4271,969261398,"# 🐛** C++ Inferencing using Torchscript Exported Torchvision model Erorr

I'm trying to use this approach to make my model (Mobilenetv3 small) using Torchvison models, In train and validation phase (python) worked Whiteout any problem but after saving Torchscript to use in c++ inference, got this error:


```
terminate` called after throwing an instance of 'torch::jit::ErrorReport'
  what():  
Unknown type name 'NoneType':
Serialized   File ""code/__torch__/torch/nn/modules/linear.py"", line 6
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.Identity) -> NoneType:
                                                                   ~~~~~~~~ <--- HERE
    return None
class Linear(Module):

Aborted (core dumped)
```

My simplified Torchscript exporting code:
```
import sys
import time
from pathlib import Path

import torch
import torch.nn as nn
from model import initialize_model,BSConv_init
num_classes=14
device = torch.device('cpu')
model = models.mobilenet_v3_small(pretrained=use_pretrained)
num_ftrs = model.classifier[3].in_features
model.classifier[3] = nn.Linear(num_ftrs, num_classes)
model = model.to(device)
checkpoint = torch.load('checkpoint/best_model_MobBsconv_ckpt.t7', map_location=device)  
model.load_state_dict(checkpoint['model'])

# Input
img = torch.rand(1, 3, 224, 224).to(device)
model.eval()
ts = torch.jit.trace(model, img, strict=False)
ts.save(""traced_mob_bsconv_model.pt"")
```

this exporting script run successfully, but using c++ produce error.
this is my simpilified C++ code that works for other models:

``   
    try{
        this->module = torch::jit::load(ModelAddress);
    }catch (const c10::Error& e) {
          std::cerr << ""error loading the model: "" << e.what() << std::endl;
          std::exit(EXIT_FAILURE);
        }
half_ = (device_ != torch::kCPU);
    this->module.to(device_);
    if (half_) {
        module.to(torch::kHalf);
        }
    torch::NoGradGuard no_grad;
    module.eval();
``
Even got error until this initializing, but my other exported models work fine at forward and ... . 
I'm confused and need help.

## Environment

env 1: System which trained and export torchscript (by above code):
OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.10.2
Libc version: glibc-2.25

Python version: 3.6.9 (default, Jul 17 2020, 12:50:27)  [GCC 8.4.0] (64-bit runtime)
Python platform: Linux-5.4.0-48-generic-x86_64-with-Ubuntu-18.04-bionic
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce GTX 1060 6GB
Nvidia driver version: 450.66
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] torch==1.9.0
[pip3] torchvision==0.10.0


env 2: system which run c++ code and got error:

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2
Libc version: glibc-2.15

Python version: 2.7.17 (default, Jul 20 2020, 15:37:01)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.18.1


## Additional context

"
annotation_path parameter in torchvision.datasets.UCF101 is not clear.,pytorch/vision,2021-08-12 03:40:59,6,question#module: datasets#module: documentation,4270,967958511,"## 📚 Documentation

Please describe what kind of files should be in annotation_path, and what the files should contain. It is not obvious.

cc @pmeier"
Sparse masks for torchvision maskrcnn - useful for training on big images with small objects,pytorch/vision,2021-08-10 09:44:39,1,enhancement#topic: object detection,4266,964792286,"## 🚀 Feature
Hello everyone,

To have more efficient GPU memory management, I propose it's a nice idea to allow for sparse masks in the torchvision mask rcnn implementation.

## Motivation

For tasks involving the prediction of many small objects in large images it becomes increasingly painful to a large, extremely sparse mask for each object. For example, I may have images of size 8k x 8k pixels with between 1 and up to 100 objects of interest of size around 40 x 40 pixels. In this case, the [current dataloader tutorial ](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) creates by default a dense mask of size 8k x 8k x N_objects, which is extremely sparse <<1% but takes a lot of memory.

Having this feature would facilitate training mask rcnns on much larger images in this scenario.

## Pitch

To allow for masks to be defined as [torch sparse tensors](https://pytorch.org/docs/stable/sparse.html), in addition to the usual dense tensors. 

I think the only thing to be done is to adjust the way the maskrcnn loss is defined and allow it to take both dense or sparse masks, potentially with a if/else depending on the mask instance.
https://github.com/pytorch/vision/blob/7d52be76c8eaf02b12338afe0822396ab3547fe2/torchvision/models/detection/roi_heads.py#L101

## Alternatives

The current alternative is to cut a big image into many smaller ones to be inside the GPU memory, but this is suboptimal when the objects of interest are rare, and we want to include as many as possible hard negatives in addition to positives.

## Additional context

N/A

cc @datumbox"
"Recommend a working conda installation sequence of torchvision, ffmpeg, opencv (both python and shared libraries + includes) together",pytorch/vision,2021-08-09 05:35:05,10,bug#topic: build#module: documentation#module: video#high priority#triaged#feature,4260,963661749,"Original context: https://github.com/pytorch/vision/issues/4240

Dfeault installation of torchvision and ffmpeg without version installs a non-workable ffmpeg 4.3 missing some libraries and **gets installed for some reason from PyTorch channel**.

I also read in other threads https://github.com/pytorch/vision/issues?q=is%3Aissue+is%3Aopen+ffmpeg that torchvision currently needs ffmpeg 4.2, but this is not reflected in dependency versions.

Also, building torchvision with ffmpeg from source is source of a lot of problems (in the issue search above)

Could README / torchvision:
1) recommend a known workable installation command of ffmpeg (maybe version-pinned? or with fixed dependency versions?)
2) recommend a known workable installation command of ffmpeg that lets you build torchvision from sources

At best it would be great to have a simple CI test installing from conda pytorch / torchvision / ffmpeg / opencv / maybe pillow-simd / open3d and a simple CI test for building from sources (as a regular user would).

@soumith "
Cannot use VideoReader due to ffmpeg issue,pytorch/vision,2021-07-29 06:46:13,6,,4218,955517328,"Hey experts, 

Recently I've been trying to use the `torchvision.io.VideoReader` API on my MacOS machine.


## Description
At first, I was trying to import the API directly, but hitting this issue:
```
  File ""/Users/sallysyw/workspace/vision/gallery/plot_video_api.py"", line 36, in <module>
    video = torchvision.io.VideoReader(video_path, stream)
  File ""/Users/sallysyw/workspace/vision/torchvision/io/__init__.py"", line 104, in __init__
    raise RuntimeError(
RuntimeError: Not compiled with video_reader support, to enable video_reader support, please install ffmpeg (version 4.2 is currently supported) andbuild torchvision from source.
```

Then I realized that I need to manually install ffmpeg package to get this work.
Firstly I tried conda install:
```
conda install ffmpeg=4.2=h677a3f5_0
```
After this is done, I re-built the torchvision package with
```
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py develop
```

Then I tried 
```
import torchvision
``` 
this time I'm hitting a new error:
```
libc++abi: terminating with uncaught exception of type c10::Error: Type c10::tagged_capsule<vision::video::Video> could not be converted to any of the known types.
Exception raised from operator() at /Users/sallysyw/opt/anaconda3/lib/python3.8/site-packages/torch/include/ATen/core/jit_type.h:1606 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) + 98 (0x102646e12 in libc10.dylib)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 106 (0x10264552a in libc10.dylib)
frame #2: c10::detail::getTypePtr_<c10::tagged_capsule<vision::video::Video> >::call()::'lambda'()::operator()() const + 263 (0x129f49497 in video_reader.so)
frame #3: c10::detail::getTypePtr_<c10::tagged_capsule<vision::video::Video> >::call() + 27 (0x129f492eb in video_reader.so)
frame #4: c10::detail::infer_schema::(anonymous namespace)::createArgumentVector(c10::ArrayRef<c10::detail::infer_schema::ArgumentDef>) + 216 (0x10e27c508 in libtorch_cpu.dylib)
frame #5: c10::detail::infer_schema::make_function_schema(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&, c10::ArrayRef<c10::detail::infer_schema::ArgumentDef>, c10::ArrayRef<c10::detail::infer_schema::ArgumentDef>) + 123 (0x10e27c2ab in libtorch_cpu.dylib)
frame #6: torch::jit::Function* torch::class_<vision::video::Video>::defineMethod<torch::class_<vision::video::Video>& torch::class_<vision::video::Video>::def<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(torch::detail::types<void, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::initializer_list<torch::arg>)::'lambda'(c10::tagged_capsule<vision::video::Video>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::initializer_list<torch::arg>) + 477 (0x129f4872d in video_reader.so)
frame #7: torch::class_<vision::video::Video>& torch::class_<vision::video::Video>::def<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(torch::detail::types<void, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::initializer_list<torch::arg>) + 114 (0x129f35242 in video_reader.so)
frame #8: _GLOBAL__sub_I_video.cpp + 226 (0x129f5b202 in video_reader.so)
frame #9: 0x0 + 4338874055 (0x1029df6c7 in ???)
frame #10: 0x0 + 4338875090 (0x1029dfad2 in ???)
frame #11: 0x0 + 4338853046 (0x1029da4b6 in ???)
frame #12: 0x0 + 4338844271 (0x1029d826f in ???)
frame #13: 0x0 + 4338844432 (0x1029d8310 in ???)
frame #14: 0x0 + 4338780690 (0x1029c8a12 in ???)
frame #15: 0x0 + 4338825966 (0x1029d3aee in ???)
frame #16: dlopen_internal(char const*, int, void*) + 185 (0x7fff203edcb4 in libdyld.dylib)
frame #17: dlopen + 28 (0x7fff203dc09e in libdyld.dylib)
frame #18: py_dl_open + 135 (0x102303ca7 in _ctypes.cpython-38-darwin.so)
<omitting python frames>

[1]    216 abort      python
```

I also tried the following options:
1. Download ffmpeg-4.2.4.tar.xz on their website and build it manually
2. Uninstall the conda package and use brew to install ffmpeg again

However, neither of them solves the issue, I'm still getting the python frames error.
Can anyone help? Thanks a lot!

## Environment

```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```
Results:
```
Collecting environment information...
PyTorch version: 1.10.0.dev20210728
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 11.4 (x86_64)
GCC version: Could not collect
Clang version: 11.0.0 (clang-1100.0.33.17)
CMake version: Could not collect
Libc version: N/A

Python version: 3.8.8 (default, Apr 13 2021, 12:59:45)  [Clang 10.0.0 ] (64-bit runtime)
Python platform: macOS-10.16-x86_64-i386-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.1
[pip3] numpydoc==1.1.0
[pip3] pytorch-sphinx-theme==0.0.24
[pip3] torch==1.10.0.dev20210728
[pip3] torchvision==0.11.0a0+b29ed34
[conda] blas                      1.0                         mkl
[conda] mkl                       2021.2.0           hecd8cb5_269
[conda] mkl-service               2.3.0            py38h9ed2024_1
[conda] mkl_fft                   1.3.0            py38h4a7008c_2
[conda] mkl_random                1.2.1            py38hb2f4e1b_2
[conda] numpy                     1.20.1           py38hd6e1bb9_0
[conda] numpy-base                1.20.1           py38h585ceec_0
[conda] numpydoc                  1.1.0              pyhd3eb1b0_1
[conda] pytorch                   1.10.0.dev20210728         py3.8_0    pytorch-nightly
[conda] pytorch-sphinx-theme      0.0.24                    dev_0    <develop>
[conda] torch                     1.9.0                    pypi_0    pypi
[conda] torchvision               0.11.0a0+b29ed34           dev_0    <develop>
```



"
Lone test should not call torch.set_deterministic,pytorch/vision,2021-07-27 18:08:00,1,,4213,954135668,"in test/test_functional_tensor.py we have

```
@pytest.mark.parametrize('device', cpu_and_gpu())
def test_equalize(device):
    torch.set_deterministic(False)
    check_functional_vs_PIL_vs_scripted(
        F.equalize,
        F_pil.equalize,
        F_t.equalize,
        {},
        device,
        dtype=None,
        tol=1.0,
        agg_method=""max"",
    )
```

This usage of set_deterministic is an antipattern, as this setting will leak out to all subsequent tests. Can this be deleted? Or barring that, the setting of the value made temporary (e.g., via a context manager?)"
Parameter for transforms.ToTensor,pytorch/vision,2021-07-26 21:15:20,2,module: transforms,4210,953294858,"## 🚀 Feature
The method converts a PIL Image or numpy.ndarray to a tensor with scaling pixel values to the range [0,1]. The new parameter will provide an option to the user whether the pixel values will be scaled or not.

## Motivation

I am working on Variational Encoders on images which requires processing images. I use a categorical distribution for the images to predict every pixel value which requires each pixel value to stay in the range [0, 255]. I use datasets from torchvision.datasets. However, when I load the data, the type is as PIL Image, and the only transform that converts the image to a tensor is ToTensor() and it scales the pixel values. I think this is really impractical because I think there should be an opiton for the user so it can be a generalizable method for every use. Thus, I ended up taking the method definitions from library and change it instead of using the package for transforms itself.


## Pitch

I would suggest adding a parameter ""scale"" to the transform ToTensor(scale=None) or ToTensor(scale=[0, 1]) which scales the input based on the appropriate input or othwersie does what it already does as default. I saw few methods before with this scaling option so I found it appropriate and the scaling in the original method only happens in line of code so it should be easy to make it optional. Or otherwise I do not understand the motivation to not put an option even though I understand a lot of time the input is normalized by the user in application.

## Alternatives

Alternatively, it could be a boolean variable which gives the option to whether scale or not if the math could cause any unwanted results.

## Additional context

NA

cc @vfdev-5"
Wrong prediction when position of object is changed,pytorch/vision,2021-07-20 17:59:16,4,,4198,948902481,"I am doing an object detection on set of images. I am finetuning using the pytorch Object detection tutorial with my own set of images. I have 100-200 images. And 22 classes. 

What I notice is that the model predicts well as long as the specific object is in a set position. For instance if object 1 is in position1, it predicts it as object 1. But when object 1 is moved to position2, it predicts it as object2. Object 1 and Object 2 look very similar except for few minute changes.

Training data: It is difficult for me to create a training data for 22 classes, with each class in 22 positions (Plus consider combinations of negative data, etc.). Is there a way to solve this?"
TorchVision Roadmap - 2021 H2,pytorch/vision,2021-07-16 14:10:39,3,,4187,946330692,"This issue is be used to keep track of the roadmap of TorchVision for the second half of 2021 and get feedback from our users.

For the 2021 H1 roadmap, please see  #3221

- Batteries included:
  - Scope is at https://github.com/pytorch/vision/issues/3911
- Datasets:
  - [x] New datasets design supporting `DataPipes`
- Video
  - [x] Video GPU Decoding (https://github.com/pytorch/vision/issues/2439)
  - [x] Optical Flow support: [RAFT](https://github.com/princeton-vl/RAFT) implementation with datasets and training /evaluation reference and pre-trained weights https://github.com/pytorch/vision/issues/4644
  - [ ] New Video datasets e.g. Charades / SSV2 / EpicKitchen (pending discussion)
- IO:
  - [ ] Faster image decoding. Possible tracks: A100 support (https://github.com/pytorch/vision/issues/3848), Batch-decoding support, better libjpeg-turbo integration
- Perf:
  - [ ] Support for [SIMD instructions for ops](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/README.md)
- Docs:
  - [x] More [gallery examples](https://pytorch.org/vision/stable/auto_examples/index.html) (finish porting the [examples folder](https://github.com/pytorch/vision/tree/master/examples/python))
  - [ ] Possibly move the vision [tutorials](https://pytorch.org/tutorials/) in the torchvision gallery
- Items moved from 2021 H1:
  - [ ] Rotated Boxes (#2761)

"
"Failing to compile torchvision C++ API from source (>C++14 compiler required, which is installed)",pytorch/vision,2021-07-13 20:28:14,5,topic: build,4175,943801759,"## 🐛 Bug

I have installed PyTorch version 1.8.0 from source and am now attempting to install the C++ API to torchvision 0.9.0 from source.

When making torchvision's C++ API I am getting an error that I need to use a C++14 or later compatible compiler (I am using gcc 9.3.0 which supports C++14):
```
root@pc:/resources/vision/build# make VERBOSE=1 -j4
...
...
...
[ 71%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp
[ 73%] Building CUDA object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu.o
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem=/usr/local/lib/python3.8/dist-packages/torch/include -isystem=/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem=/usr/include/python3.8  --expt-relaxed-constexpr -Xcompiler=-fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11 -x cu -c /resources/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu.o
In file included from /resources/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu:69:
/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:4:2: error: #error C++14 or later compatible compiler is required to use ATen.
    4 | #error C++14 or later compatible compiler is required to use ATen.
      |  ^~~~~
In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/ArrayRef.h:19,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DispatchKey.h:7,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Backend.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Layout.h:3,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9,
                 from /resources/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu:69:
/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/C++17.h:24:2: error: #error You need C++14 to compile PyTorch
   24 | #error You need C++14 to compile PyTorch
      |  ^~~~~
[ 75%] Building CUDA object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/nms_kernel.cu.o
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem=/usr/local/lib/python3.8/dist-packages/torch/include -isystem=/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem=/usr/include/python3.8  --expt-relaxed-constexpr -Xcompiler=-fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11 -x cu -c /resources/vision/torchvision/csrc/ops/cuda/nms_kernel.cu -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/nms_kernel.cu.o
In file included from /resources/vision/torchvision/csrc/ops/cuda/nms_kernel.cu:1:
/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:4:2: error: #error C++14 or later compatible compiler is required to use ATen.
    4 | #error C++14 or later compatible compiler is required to use ATen.
      |  ^~~~~
```
I think the places of interest are the following:
- [ 71%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp.o
  - Uses the flag `-std=gnu++14`
- [ 75%] Building CUDA object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/nms_kernel.cu.o
  - Uses the flag `-std=c++11`

It seems like all of the CPU builds use the C++14 flag and the CUDA builds (incorrectly) use the C++11 flag. 
Can I point this to `CMAKE_CUDA_FLAGS` being set in `vision/CMakeLists.txt`? It looks like `CUDA_FLAGS` contains the flag `-std=c++11` as shown in `vision/build/CMakeFiles/torchvision.dir/flags.make`:
```
# CMAKE generated file: DO NOT EDIT!
# Generated by ""Unix Makefiles"" Generator, CMake Version 3.16

# compile CUDA with /usr/local/cuda/bin/nvcc
# compile CXX with /usr/bin/c++
CUDA_FLAGS =  --expt-relaxed-constexpr -Xcompiler=-fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11
```

Any idea where this is coming from? How it's getting set? And how to change it so that it is set to `-std=gnu++14` or similar? Is it coming from my PyTorch install?

## To Reproduce

Steps to reproduce the behavior:

`git clone https://github.com/pytorch/vision.git`
`cd vision`
`git checkout v0.9.0`
`mkdir build && cd build`

<details><summary>cmake -DTorch_DIR=/usr/local/lib/python3.8/dist-packages/torch/share/cmake/Torch/ -DWITH_CUDA=on ..</summary>
<p>

```
root@pc:/resources/vision/build# cmake -DTorch_DIR=/usr/local/lib/python3.8/dist-packages/torch/share/cmake/Torch/ -DWITH_CUDA=on ..
-- The C compiler identification is GNU 9.3.0
-- The CXX compiler identification is GNU 9.3.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- The CUDA compiler identification is NVIDIA 11.2.67
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc
-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- works
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Found Python3: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8"") found components: Development 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Caffe2: Found protobuf with new-style protobuf targets.
-- Caffe2: Protobuf version 3.6.1
-- Found CUDA: /usr/local/cuda (found version ""11.2"") 
-- Caffe2: CUDA detected: 11.2
-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc
-- Caffe2: CUDA toolkit directory: /usr/local/cuda
-- Caffe2: Header version is: 11.2
-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so  
-- Found cuDNN: v8.1.0  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)
CMake Warning at /usr/local/lib/python3.8/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:198 (message):
  Failed to compute shorthash for libnvrtc.so
Call Stack (most recent call first):
  /usr/local/lib/python3.8/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:88 (include)
  /usr/local/lib/python3.8/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)
  CMakeLists.txt:17 (find_package)


-- Autodetected CUDA architecture(s):  6.1 6.1 6.1
-- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61
-- Found Torch: /usr/local/lib/python3.8/dist-packages/torch/lib/libtorch.so  
-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version ""1.2.11"") 
-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version ""1.6.37"") 
-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version ""80"") 
-- Configuring done
-- Generating done
-- Build files have been written to: /resources/vision/build
```
</p>
</details>


<details><summary>make -j4 </summary>
<p>

```
root@pc:/resources/vision/build# make VERBOSE=1 -j4 
/usr/bin/cmake -S/resources/vision -B/resources/vision/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /resources/vision/build/CMakeFiles /resources/vision/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory '/resources/vision/build'
make -f CMakeFiles/torchvision.dir/build.make CMakeFiles/torchvision.dir/depend
make[2]: Entering directory '/resources/vision/build'
cd /resources/vision/build && /usr/bin/cmake -E cmake_depends ""Unix Makefiles"" /resources/vision /resources/vision /resources/vision/build /resources/vision/build /resources/vision/build/CMakeFiles/torchvision.dir/DependInfo.cmake --color=
make[2]: Leaving directory '/resources/vision/build'
make -f CMakeFiles/torchvision.dir/build.make CMakeFiles/torchvision.dir/build
make[2]: Entering directory '/resources/vision/build'
[  2%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/common_jpeg.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/common_jpeg.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/common_jpeg.cpp
[  6%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_png.cpp.o
[  6%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o
[  8%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_jpeg.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_png.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/decode_png.cpp
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/decode_image.cpp
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_jpeg.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/decode_jpeg.cpp
[ 10%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_jpeg.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_jpeg.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/encode_jpeg.cpp
[ 12%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_png.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_png.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/encode_png.cpp
[ 14%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/read_write_file.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/read_write_file.cpp.o -c /resources/vision/torchvision/csrc/io/image/cpu/read_write_file.cpp
[ 16%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/image.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/io/image/image.cpp.o -c /resources/vision/torchvision/csrc/io/image/image.cpp
[ 18%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/alexnet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/alexnet.cpp.o -c /resources/vision/torchvision/csrc/models/alexnet.cpp
[ 20%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/densenet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/densenet.cpp.o -c /resources/vision/torchvision/csrc/models/densenet.cpp
[ 22%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/googlenet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/googlenet.cpp.o -c /resources/vision/torchvision/csrc/models/googlenet.cpp
[ 24%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/inception.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/inception.cpp.o -c /resources/vision/torchvision/csrc/models/inception.cpp
[ 26%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/mnasnet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/mnasnet.cpp.o -c /resources/vision/torchvision/csrc/models/mnasnet.cpp
[ 28%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/mobilenet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/mobilenet.cpp.o -c /resources/vision/torchvision/csrc/models/mobilenet.cpp
[ 30%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/resnet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/resnet.cpp.o -c /resources/vision/torchvision/csrc/models/resnet.cpp
[ 32%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/shufflenetv2.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/shufflenetv2.cpp.o -c /resources/vision/torchvision/csrc/models/shufflenetv2.cpp
[ 34%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/squeezenet.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/squeezenet.cpp.o -c /resources/vision/torchvision/csrc/models/squeezenet.cpp
[ 36%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/models/vgg.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/models/vgg.cpp.o -c /resources/vision/torchvision/csrc/models/vgg.cpp
[ 38%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autocast/deform_conv2d_kernel.cpp
[ 40%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/nms_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/nms_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autocast/nms_kernel.cpp
[ 42%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/ps_roi_align_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/ps_roi_align_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autocast/ps_roi_align_kernel.cpp
[ 44%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/ps_roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/ps_roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autocast/ps_roi_pool_kernel.cpp
[ 46%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/roi_align_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/roi_align_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autocast/roi_align_kernel.cpp
[ 48%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autocast/roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autocast/roi_pool_kernel.cpp
[ 51%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/deform_conv2d_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/deform_conv2d_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autograd/deform_conv2d_kernel.cpp
[ 53%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/ps_roi_align_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/ps_roi_align_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autograd/ps_roi_align_kernel.cpp
[ 55%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/ps_roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/ps_roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autograd/ps_roi_pool_kernel.cpp
[ 57%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/roi_align_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/roi_align_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autograd/roi_align_kernel.cpp
[ 59%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/autograd/roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/autograd/roi_pool_kernel.cpp
[ 61%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/deform_conv2d_kernel.cpp
[ 63%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/nms_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/nms_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/nms_kernel.cpp
[ 65%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/ps_roi_align_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/ps_roi_align_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/ps_roi_align_kernel.cpp
[ 67%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/ps_roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/ps_roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/ps_roi_pool_kernel.cpp
[ 69%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_align_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_align_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/roi_align_kernel.cpp
[ 71%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp.o
/usr/bin/c++  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.8  -fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=gnu++14 -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp.o -c /resources/vision/torchvision/csrc/ops/cpu/roi_pool_kernel.cpp
[ 73%] Building CUDA object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu.o
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem=/usr/local/lib/python3.8/dist-packages/torch/include -isystem=/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem=/usr/include/python3.8  --expt-relaxed-constexpr -Xcompiler=-fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11 -x cu -c /resources/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu.o
In file included from /resources/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu:69:
/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:4:2: error: #error C++14 or later compatible compiler is required to use ATen.
    4 | #error C++14 or later compatible compiler is required to use ATen.
      |  ^~~~~
In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/ArrayRef.h:19,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DispatchKey.h:7,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Backend.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Layout.h:3,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9,
                 from /resources/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu:69:
/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/C++17.h:24:2: error: #error You need C++14 to compile PyTorch
   24 | #error You need C++14 to compile PyTorch
      |  ^~~~~
[ 75%] Building CUDA object CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/nms_kernel.cu.o
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -D__CUDA_NO_HALF_OPERATORS__ -Dtorchvision_EXPORTS -I/resources/vision/torchvision/csrc -isystem=/usr/local/lib/python3.8/dist-packages/torch/include -isystem=/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem=/usr/include/python3.8  --expt-relaxed-constexpr -Xcompiler=-fPIC   -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11 -x cu -c /resources/vision/torchvision/csrc/ops/cuda/nms_kernel.cu -o CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/nms_kernel.cu.o
In file included from /resources/vision/torchvision/csrc/ops/cuda/nms_kernel.cu:1:
/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:4:2: error: #error C++14 or later compatible compiler is required to use ATen.
    4 | #error C++14 or later compatible compiler is required to use ATen.
      |  ^~~~~
make[2]: *** [CMakeFiles/torchvision.dir/build.make:518: CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/ArrayRef.h:19,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DispatchKey.h:7,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Backend.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Layout.h:3,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4,
                 from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9,
                 from /resources/vision/torchvision/csrc/ops/cuda/nms_kernel.cu:1:
/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/C++17.h:24:2: error: #error You need C++14 to compile PyTorch
   24 | #error You need C++14 to compile PyTorch
      |  ^~~~~
make[2]: *** [CMakeFiles/torchvision.dir/build.make:531: CMakeFiles/torchvision.dir/torchvision/csrc/ops/cuda/nms_kernel.cu.o] Error 1
make[2]: Leaving directory '/resources/vision/build'
make[1]: *** [CMakeFiles/Makefile2:76: CMakeFiles/torchvision.dir/all] Error 2
make[1]: Leaving directory '/resources/vision/build'
make: *** [Makefile:130: all] Error 2
```
</p>
</details>


<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

The make (build) completes without error. 



## Environment

<details><summary>Click to display environment</summary>
<p>

```
root@pc:~# python3 -m torch.utils.collect_env
Collecting environment information...
PyTorch version: 1.8.0a0+56b43f4
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce GTX 1080
GPU 1: GeForce GTX 1080
GPU 2: GeForce GTX 1080

Nvidia driver version: 460.84
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.0
[pip3] pytorch-lightning==1.3.8
[pip3] torch==1.8.0a0+56b43f4
[pip3] torchmetrics==0.4.1
[pip3] torchvision==0.9.0a0+01dfa8e
[conda] Could not collect
```
</p>
</details>

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): 1.8.0 / 0.9.0
 - OS (e.g., Linux): Ubuntu 20.04
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): source / source
 - Build command you used (if compiling from source): Listed above
 - Python version: 3.8
 - CUDA/cuDNN version: 11.2
 - GPU models and configuration: 3x GeForce GTX 1080

## Additional context

The commands I used to build PyTorch: 
`git clone https://github.com/pytorch/pytorch.git`
`cd pytorch`
`git checkout v1.8.0`
`git submodule sync`
`git submodule update --init --recursive`
`python3 setup.py build -j4 --cmake-only`
`cd build`
`cmake -DBUILD_CUSTOM_PROTOBUF=0 .. `
`cmake --build . -j4 --target install`
`cd ..`
`python3 setup.py install `

Additionally,
`python` maps to Python 2.7.18
`python3` maps to Python 3.8.10

Lastly, I do have a Dockerfile that I am using... but it builds cuDNN, PyTorch, openCV, and torchvision from source... it's about 20GB and takes a few hours to build. 

"
HTTP Error 403: rate limit exceeded when loading model,pytorch/vision,2021-07-06 15:41:36,31,bug#triage review#module: hub#high priority,4156,938029217,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1. import torch
2. model = torch.hub.load('pytorch/vision:v1.9.0','resnet50', pretrained=True)

## Expected behavior

Load the model 

## Environment

Google Colab and also local with the latest pytorch and torchvision installed

## Error Output

HTTPError                                 Traceback (most recent call last)

<ipython-input-38-3e6f2f256b2a> in <module>()
      1 import torchvision
      2 
----> 3 model = torch.hub.load('pytorch/vision:v1.9.0','resnet50', pretrained=True)

8 frames

/usr/lib/python3.7/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs)
    647 class HTTPDefaultErrorHandler(BaseHandler):
    648     def http_error_default(self, req, fp, code, msg, hdrs):
--> 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    650 
    651 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 403: rate limit exceeded

"
RuntimeError: CUDA out of memory. (But I have 24G memory available),pytorch/vision,2021-07-02 02:38:04,1,,4149,935358524,"Hi,
  I run the train.py script of [the reference code](https://github.com/pytorch/vision/tree/master/references/detection). 

  But got cuda out of memory error shown below:

Start training
Epoch: [0]  [   0/2507]  eta: 0:15:11  lr: 0.000040  loss: 3.9314 (3.9314)  loss_classifier: 3.1079 (3.1079)  loss_box_reg: 0.0669 (0.0669)  loss_objectness: 0.6984 (0.6984)  loss_rpn_box_reg: 0.0583 (0.0583)  time: 0.3635  data: 0.0372  max mem: 2224
Epoch: [0]  [  20/2507]  eta: 0:13:41  lr: 0.000440  loss: 3.4063 (3.2239)  loss_classifier: 2.6814 (2.4672)  loss_box_reg: 0.0116 (0.0211)  loss_objectness: 0.6958 (0.6942)  loss_rpn_box_reg: 0.0332 (0.0413)  time: 0.3285  data: 0.0173  max mem: 2657
Epoch: [0]  [  40/2507]  eta: 0:13:35  lr: 0.000839  loss: 1.2476 (2.1992)  loss_classifier: 0.5336 (1.5398)  loss_box_reg: 0.1127 (0.0682)  loss_objectness: 0.3905 (0.5478)  loss_rpn_box_reg: 0.0425 (0.0434)  time: 0.3309  data: 0.0168  max mem: 2657
Epoch: [0]  [  60/2507]  eta: 0:13:24  lr: 0.001239  loss: 0.5988 (1.7346)  loss_classifier: 0.3086 (1.1685)  loss_box_reg: 0.1608 (0.1021)  loss_objectness: 0.1539 (0.4219)  loss_rpn_box_reg: 0.0258 (0.0421)  time: 0.3249  data: 0.0168  max mem: 2821
Epoch: [0]  [  80/2507]  eta: 0:13:17  lr: 0.001638  loss: 0.5835 (1.4710)  loss_classifier: 0.3268 (0.9616)  loss_box_reg: 0.1620 (0.1204)  loss_objectness: 0.0840 (0.3487)  loss_rpn_box_reg: 0.0230 (0.0402)  time: 0.3278  data: 0.0178  max mem: 2821
Epoch: [0]  [ 100/2507]  eta: 0:13:09  lr: 0.002038  loss: 0.4755 (1.2898)  loss_classifier: 0.2139 (0.8241)  loss_box_reg: 0.1554 (0.1294)  loss_objectness: 0.0611 (0.2994)  loss_rpn_box_reg: 0.0190 (0.0369)  time: 0.3258  data: 0.0158  max mem: 2821
Traceback (most recent call last):
  File ""train.py"", line 235, in <module>
    main(args)
  File ""train.py"", line 208, in main
    train_one_epoch(model, optimizer, data_loader, device, epoch, args.print_freq)
  File ""/home/songhongguang/lwh/vision-master/references/detection/engine.py"", line 30, in train_one_epoch
    loss_dict = model(images, targets)
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py"", line 78, in forward
    images, targets = self.transform(images, targets)
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torchvision/models/detection/transform.py"", line 104, in forward
    image, target_index = self.resize(image, target_index)
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torchvision/models/detection/transform.py"", line 151, in resize
    image, target = _resize_image_and_masks(image, size, float(self.max_size), target)
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torchvision/models/detection/transform.py"", line 52, in _resize_image_and_masks
    mask = F.interpolate(mask[:, None].float(), scale_factor=scale_factor, recompute_scale_factor=True)[:, 0].byte()
  File ""/home/songhongguang/anaconda3/envs/torchdistill/lib/python3.7/site-packages/torch/nn/functional.py"", line 3532, in interpolate
    return torch._C._nn.upsample_nearest2d(input, output_size, scale_factors)
RuntimeError: CUDA out of memory. Tried to allocate 20.51 GiB (GPU 0; 23.87 GiB total capacity; 5.62 GiB already allocated; 17.22 GiB free; 5.87 GiB reserved in total by PyTorch)


I'm confused because I have 24G memory available:

![figure1_nvidia_before](https://user-images.githubusercontent.com/26431187/124212251-3c7a8c00-db21-11eb-9348-564a572fe733.png)

I have tried to set batch_size to 2, num_workers=0, also pin_memory=False.

Could you please tell me the reason?  Thank you!"
compile c++ failed of the version release/0.9,pytorch/vision,2021-06-30 11:25:25,2,awaiting response,4140,933594033,"Hello,
I met an error when compile the torchvision of c++ in linux. 

**The enviroment as belows:**
cmake version 3.14.5
torch version 1.8.1+cpu

**the command i excute:**
- git clone https://github.com/pytorch/vision.git
- cd vision
- git checkout release/0.9
- mkdir build && cd build
- make -j4
- sudo make install


**the error log as belows:**


> -- The C compiler identification is GNU 7.5.0
> -- The CXX compiler identification is GNU 7.5.0
> -- Check for working C compiler: /usr/bin/cc
> -- Check for working C compiler: /usr/bin/cc -- works
> -- Detecting C compiler ABI info
> -- Detecting C compiler ABI info - done
> -- Detecting C compile features
> -- Detecting C compile features - done
> -- Check for working CXX compiler: /usr/bin/c++
> -- Check for working CXX compiler: /usr/bin/c++ -- works
> -- Detecting CXX compiler ABI info
> -- Detecting CXX compiler ABI info - done
> -- Detecting CXX compile features
> -- Detecting CXX compile features - done
> -- Found Python3: /environment/python/versions/miniconda3-4.7.12/lib/libpython3.7m.so (found version ""3.7.4"") found components:  Development 
> -- Looking for pthread.h
> -- Looking for pthread.h - found
> -- Looking for pthread_create
> -- Looking for pthread_create - not found
> -- Looking for pthread_create in pthreads
> -- Looking for pthread_create in pthreads - not found
> -- Looking for pthread_create in pthread
> -- Looking for pthread_create in pthread - found
> -- Found Threads: TRUE  
> -- Found Torch: /environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/torch/lib/libtorch.so  
> -- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version ""1.2.11"") 
> -- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version ""1.6.34"") 
> -- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version ""80"") 
> -- Configuring done
> CMake Warning at CMakeLists.txt:73 (add_library):
>   Cannot generate a safe runtime search path for target torchvision because
>   files in some directories may conflict with libraries in implicit
>   directories:
> 
>     runtime library [libpng16.so.16] in /usr/lib/x86_64-linux-gnu may be hidden by files in:
>       /environment/python/versions/miniconda3-4.7.12/lib
> 
>   Some of these libraries may not be found correctly.


> -- Generating done
> -- Build files have been written to: /home/featurize/work/yolov5-rt-stack/deployment/vision/build
> make: Warning: File 'Makefile' has modification time 79 s in the future
> make[1]: Warning: File 'CMakeFiles/Makefile2' has modification time 79 s in the future
> make[2]: Warning: File 'CMakeFiles/torchvision.dir/flags.make' has modification time 79 s in the future
> Scanning dependencies of target torchvision
> make[2]: warning:  Clock skew detected.  Your build may be incomplete.
> make[2]: Warning: File 'CMakeFiles/torchvision.dir/flags.make' has modification time 78 s in the future
> [  2%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/common_jpeg.cpp.o
> [ 10%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_image.cpp.o
> [ 10%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_jpeg.cpp.o
> [ 10%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_png.cpp.o
> [ 13%] Building CXX object CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_jpeg.cpp.o
> c++: internal compiler error: Killed (program cc1plus)
> 

> Please submit a full bug report,
> with preprocessed source if appropriate.
> See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
> CMakeFiles/torchvision.dir/build.make:101: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_png.cpp.o' failed
> make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_png.cpp.o] Error 4
> make[2]: *** Waiting for unfinished jobs....
> c++: internal compiler error: Killed (program cc1plus)
> Please submit a full bug report,
> with preprocessed source if appropriate.
> See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
> CMakeFiles/torchvision.dir/build.make:88: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_jpeg.cpp.o' failed
> make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/decode_jpeg.cpp.o] Error 4
> c++: internal compiler error: Killed (program cc1plus)
> Please submit a full bug report,
> with preprocessed source if appropriate.
> See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
> CMakeFiles/torchvision.dir/build.make:114: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_jpeg.cpp.o' failed
> make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/io/image/cpu/encode_jpeg.cpp.o] Error 4
> CMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/torchvision.dir/all' failed
> make[1]: *** [CMakeFiles/torchvision.dir/all] Error 2
> Makefile:129: recipe for target 'all' failed
> make: *** [all] Error 2


"
GoogleNet is secretly transforming input,pytorch/vision,2021-06-30 08:05:21,5,,4136,933420708,"Hello!

I recently noticed that I might be doing image normalization twice in my experiments.
The [documentation](https://pytorch.org/vision/0.8/models.html#torchvision.models.googlenet) says that the default value of the `transform_input` parameter is False.

So when calling 
```python3
model = torchvision.models.googlenet(pretrained=True)
```

I would probably expect the model not to do any input transformations, but accidentally it does ([permalink](https://github.com/pytorch/vision/blob/a83b9a17e441e6d77e9d59ca14d75fb5ba8c31f6/torchvision/models/googlenet.py#L41)) until you directly specify `transform_input=False`. So in case of `pretrained=True` and not-specified `transform_input` model suddenly sets its value to True:

```python3
    if pretrained:
        if 'transform_input' not in kwargs:
            kwargs['transform_input'] = True
```

It is confusing for me. This thing is only happens in GoogleNet.
"
Problems building Torchvision from source,pytorch/vision,2021-06-27 15:27:07,6,,4125,930956946,"Hello, I have compiled ffmpeg version 4.2.4 from source and am trying to do `python setup.py install` after git-cloning to build Torchvision from source (Python 3.8.8, GNU/Linux 5.4.0-58-generic x86_64). However, I'm getting the following error:
```
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
g++ -pthread -shared -B /home/user/anaconda3/compiler_compat -L/home/user/anaconda3/lib -Wl,-rpath=/home/user/anaconda3/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/video_reader/video_reader.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/video_stream.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/sync_decoder.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/subtitle_sampler.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/audio_sampler.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/cc_stream.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/audio_stream.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/seekable_buffer.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/memory_buffer.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/subtitle_stream.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/stream.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/decoder.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/time_keeper.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/util.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/decoder/video_sampler.o build/temp.linux-x86_64-3.8/home/user/trial/data/vision/torchvision/csrc/io/video/video.o -L/usr/local/lib -L/usr/local/lib/x86_64-linux-gnu -L/home/user/anaconda3/lib/python3.8/site-packages/torch/lib -lavcodec -lavformat -lavutil -lswresample -lswscale -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/torchvision/video_reader.so -std=c++14
/home/user/anaconda3/compiler_compat/ld: /usr/local/lib/libavcodec.a(vc1dsp_mmx.o): relocation R_X86_64_PC32 against symbol `ff_pw_9' can not be used when making a shared object; recompile with -fPIC
/home/user/anaconda3/compiler_compat/ld: final link failed: bad value
collect2: error: ld returned 1 exit status
error: command 'g++' failed with exit status 1
```
How do I resolve this issue? Thanks!"
Again details about how pretrained models are trained?,pytorch/vision,2021-06-25 05:51:59,2,question,4115,929852868,"1. I use [v0.5.0/references](https://github.com/pytorch/vision/blob/build/v0.5.0/references/classification/train.py) to train a resnet50 wirth defalut config. But, I got Best_val Top1=75.806%, which has a gap of 0.3% about the pretrained model. How can I to repreduce your accuracy ?
2. I notice you said you use you recompute the batch norm statistics after training, can you show more details?"
torchvision.ImageFolder type hint need update to include Path type,pytorch/vision,2021-06-24 05:38:13,3,module: datasets,4111,929315378,"The type hint of `root` in ImageFolder in torchvion is `str`, but it also support `pathlib.Path` type, this will result in [pyright warning,](https://github.com/microsoft/pyright/issues/2028#issuecomment-867341879) so seems we should update the type hint to `root: str | Path`?
```
 def __init__(
            self,
            root: str,
            transform: Optional[Callable] = None,
            target_transform: Optional[Callable] = None,
            loader: Callable[[str], Any] = default_loader,
            is_valid_file: Optional[Callable[[str], bool]] = None,
    ):
```


cc @pmeier"
Conda package should support pillow-simd,pytorch/vision,2021-06-23 16:19:48,1,dependency issue,4103,928433385,"## 🐛 Bug

`torchvision` Conda package, at least with v0.10.0, depends on `pillow`, which complicates using pillow-SIMD.

## To Reproduce

Steps to reproduce the behavior:

1. Install torchvision with Conda.
2. Try to uninstall pillow, but would try to uninstall torchvision. Alternatively, you can do `conda remove --force pillow` to only remove it, but the environment will be in an inconsistent state. It's the best workaround AFAIK though.
3. Install pillow-SIMD.

## Expected behavior

The expected behavior would be to be able to choose between pillow and pillow-SIMD, or be able to uninstall pillow without breaking things, and then install pillow-SIMD. After this, every time you try installing something with conda, it's gonna try to install `pillow` again.

Maybe pillow shouldn't be a dependency then? Maybe the ideal solution is to pillow-SIMD to provide a conda package called `pillow`, or something like that (or like a virtual package to support both?)."
Release binaries of torchvision for ppc64le,pytorch/vision,2021-06-20 20:07:39,0,topic: binaries,4087,925672659,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
Make binaries of torchvision for ppc64le available to download e.g. via anaconda.
## Motivation
I'm trying to use pytorch and torchvision on ppc64le. Installing from source is really non trivial and I rely on https://github.com/open-ce/open-ce for downloading some releases of torchvision. Since this is an open-source software, the latest versions of torchvision (and pytorch) are always lacking. I need the latest versions available in order to keep up with the latest research.

After a few google searches, it seems that I'm not the only one faced with this problem. Allowing for an easy install of torchvision on ppc64le would allow the community to stop wasting time on installs and focus on research.

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
"
Conda installation: incompatibility with openvc,pytorch/vision,2021-06-20 07:25:39,1,release-issue,4085,925538687,"## 🐛 Bug

Trying to install torchvision together with opencv (from conda-forge channel) fails due to compatibility issues.

Here's the `env.yaml` file to reproduce:

``` yaml
channels:
  - nvidia
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.8
  - pytorch=1.9.0
  - torchvision=0.10.0
  - cudatoolkit=11.1
  - opencv=4.5.2
```

Try and install this with
```
conda env create -f env.yaml -n test-torchvision
```

Remove either torchvision or opencv from the list, and the installation succeeds. This is on Ubuntu 20.04.

The compatibility errors I get seem a bit puzzling, I would guess that the real reason might be `jpeg` pinning. Here's the output:

<details>
  <summary>Output</summary>

```
torchvision=0.10.0 -> pillow[version='>=5.3.0'] -> python_abi[version='3.6|3.6.*|3.7|3.7.*|3.8.*',build='*_cp38|*_pypy36_pp73|*_cp36m|*_pypy37_pp73|*_cp37m']
torchvision=0.10.0 -> python_abi=3.9[build=*_cp39]

Package pypy3.6 conflicts for:
opencv=4.5.2 -> pypy3.6[version='>=7.3.3']
pytorch=1.9.0 -> ninja -> pypy3.6[version='7.3.0.*|7.3.1.*|7.3.2.*|7.3.3.*|>=7.3.1']
opencv=4.5.2 -> python_abi==3.6[build=*_pypy36_pp73] -> pypy3.6=7.3
torchvision=0.10.0 -> pillow[version='>=5.3.0'] -> pypy3.6[version='7.3.0.*|7.3.1.*|7.3.2.*|7.3.3.*|>=7.3.1|>=7.3.2|>=7.3.3']

Package libgcc-ng conflicts for:
pytorch=1.9.0 -> blas=[build=mkl] -> libgcc-ng[version='>=4.9|>=7.2.0|>=7.3.0|>=7.5.0|>=9.3.0']
python=3.8 -> zlib[version='>=1.2.11,<1.3.0a0'] -> libgcc-ng[version='>=4.9|>=7.2.0']
torchvision=0.10.0 -> cudatoolkit[version='>=11.1,<11.2'] -> libgcc-ng[version='>=4.9|>=7.2.0|>=7.3.0|>=9.3.0|>=7.5.0']
python=3.8 -> libgcc-ng[version='>=7.3.0|>=7.5.0|>=9.3.0']
opencv=4.5.2 -> libopencv==4.5.2=py36hf40240d_0 -> libgcc-ng[version='>=9.3.0']
cudatoolkit=11.1 -> libgcc-ng[version='>=7.3.0|>=9.3.0']

Package cudatoolkit conflicts for:
torchvision=0.10.0 -> cudatoolkit[version='>=10.2,<10.3|>=11.1,<11.2']
cudatoolkit=11.1

Package libtiff conflicts for:
opencv=4.5.2 -> libopencv==4.5.2=py36hf40240d_0 -> libtiff[version='>=4.2.0,<5.0a0']
torchvision=0.10.0 -> pillow[version='>=5.3.0'] -> libtiff[version='>=4.0.10,<5.0a0|>=4.1.0,<5.0a0|>=4.2.0,<5.0a0|>=4.0.9,<5.0a0']

Package pytorch conflicts for:
torchvision=0.10.0 -> pytorch==1.9.0
pytorch=1.9.0

Package readline conflicts for:
python=3.8 -> readline[version='>=7.0,<8.0a0|>=8.0,<9.0a0|>=8.1,<9.0a0']
pytorch=1.9.0 -> python[version='>=3.7,<3.8.0a0'] -> readline[version='6.2.*|7.0|>=7.0,<8.0a0|>=8.0,<9.0a0|7.*|>=8.1,<9.0a0']
torchvision=0.10.0 -> python[version='>=3.9,<3.10.0a0'] -> readline[version='6.2.*|7.0|>=7.0,<8.0a0|>=8.0,<9.0a0|>=8.1,<9.0a0|7.*']

Package jpeg conflicts for:
torchvision=0.10.0 -> pillow[version='>=5.3.0'] -> jpeg[version='>=9b,<10a|>=9c,<10a|>=9d,<10a']
torchvision=0.10.0 -> jpeg[version='<=9b']

Package _openmp_mutex conflicts for:
opencv=4.5.2 -> libopencv==4.5.2=py36hf40240d_0 -> _openmp_mutex[version='>=4.5']
python=3.8 -> libgcc-ng[version='>=9.3.0'] -> _openmp_mutex[version='>=4.5']
cudatoolkit=11.1 -> libgcc-ng[version='>=7.3.0'] -> _openmp_mutex[version='>=4.5']
pytorch=1.9.0 -> blas=[build=mkl] -> _openmp_mutex[version='*|>=4.5',build=*_llvm]

Package libpng conflicts for:
torchvision=0.10.0 -> ffmpeg[version='>=4.2'] -> libpng[version='>=1.6.37,<1.7.0a0']
torchvision=0.10.0 -> libpng

Package python conflicts for:
torchvision=0.10.0 -> python[version='>=3.6,<3.7.0a0|>=3.8,<3.9.0a0|>=3.9,<3.10.0a0|>=3.7,<3.8.0a0']
torchvision=0.10.0 -> pillow[version='>=5.3.0'] -> python[version='3.9.*|>=2.7,<2.8.0a0|>=3.5,<3.6.0a0']
python=3.8
opencv=4.5.2 -> py-opencv==4.5.2=py36he2ddec3_0 -> python[version='3.6.*|3.9.*|>=3.6,<3.7.0a0|>=3.9,<3.10.0a0|>=3.7,<3.8.0a0|3.7.*|>=3.8,<3.9.0a0|3.8.*']

Package liblapacke conflicts for:
opencv=4.5.2 -> libopencv==4.5.2=py36hf40240d_0 -> liblapacke[version='>=3.8.0,<4.0a0']
pytorch=1.9.0 -> blas=[build=mkl] -> liblapacke[version='3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.8.0|3.9.0',build='4_mkl|6_mkl|7_mkl|9_mkl|10_mkl|12_mkl|13_mkl|15_mkl|18_mkl|19_mkl|7_mkl|8_mkl|9_mkl|6_mkl|5_mkl|21_mkl|20_mkl|16_mkl|14_mkl|11_mkl|8_mkl|5_mkl']

Package _libgcc_mutex conflicts for:
python=3.8 -> libgcc-ng[version='>=9.3.0'] -> _libgcc_mutex[version='*|0.1',build='main|main|conda_forge']
cudatoolkit=11.1 -> libgcc-ng[version='>=7.3.0'] -> _libgcc_mutex[version='*|0.1|0.1',build='main|main|conda_forge']The following specifications were found to be incompatible with your system:

  - feature:/linux-64::__glibc==2.31=0
  - feature:|@/linux-64::__glibc==2.31=0
  - cudatoolkit=11.1 -> __glibc[version='>=2.17,<3.0.a0']
  - torchvision=0.10.0 -> cudatoolkit[version='>=11.1,<11.2'] -> __glibc[version='>=2.17,<3.0.a0']

Your installed version is: 2.31
```

</details>

"
Cannot install any version of torchvision newer than 0.2.2 with opencv for python 3.9 and pytorch 1.9.0,pytorch/vision,2021-06-17 01:55:17,24,bug#triage review#high priority#release-issue,4076,923312767,"## 🐛 Bug
Issue #3207 has cropped up again for pytorch 1.9.0
Cannot install any version of torchvision newer than 0.2.2 with opencv for python 3.9

## To Reproduce
Contents of `~/.condarc`:
```
channels:
  - defaults
  - anaconda
  - pytorch
  - conda-forge
channel_priority: disabled
```

On the command line:
```
conda create -n temp python=3.9
conda activate temp
conda install torchvision opencv pytorch=1.9.0
```
This installs torchvision version 0.2.2.

Replacing the last line with
```
conda install torchvision=0.10.0 opencv pytorch=1.9.0
```
produces the error
```
UnsatisfiableError: The following specifications were found to be incompatible with each other:

Output in format: Requested package -> Available versions

Package bzip2 conflicts for:
opencv -> pypy3.6[version='>=7.3.3'] -> bzip2[version='1.0.*|>=1.0.6,<2.0a0|>=1.0.8,<2.0a0|>=1.0.6,<1.1.0a0']
torchvision==0.10.0=py39_cu111 -> ffmpeg[version='>=4.2'] -> bzip2[version='>=1.0.8,<2.0a0']

Package jpeg conflicts for:
torchvision==0.10.0=py39_cu111 -> jpeg[version='<=9b']
torchvision==0.10.0=py39_cu111 -> pillow[version='>=5.3.0'] -> jpeg[version='>=9b,<10a|>=9d,<10a|>=9c,<10a']

Package libpng conflicts for:
torchvision==0.10.0=py39_cu111 -> libpng
torchvision==0.10.0=py39_cu111 -> ffmpeg[version='>=4.2'] -> libpng[version='>=1.6.37,<1.7.0a0']

Package python_abi conflicts for:
torchvision==0.10.0=py39_cu111 -> python_abi=3.9[build=*_cp39]
torchvision==0.10.0=py39_cu111 -> pillow[version='>=5.3.0'] -> python_abi[version='3.6|3.6.*|3.7|3.7.*|3.8.*',build='*_cp38|*_pypy36_pp73|*_pypy37_pp73|*_cp36m|*_cp37m']

Package python conflicts for:
torchvision==0.10.0=py39_cu111 -> python[version='>=3.9,<3.10.0a0']
opencv -> py-opencv==4.5.2=py39hef51801_0 -> python[version='3.7.*|3.9.*|>=3.9,<3.10.0a0|>=3.8,<3.9.0a0|3.8.*']
opencv -> python[version='2.7.*|3.5.*|3.6.*|>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.5,<3.6.0a0|3.4.*']
torchvision==0.10.0=py39_cu111 -> pillow[version='>=5.3.0'] -> python[version='3.9.*|>=2.7,<2.8.0a0|>=3.6,<3.7.0a0|>=3.7,<3.8.0a0|>=3.8,<3.9.0a0|>=3.5,<3.6.0a0']
python=3.9

Package _libgcc_mutex conflicts for:
opencv -> libgcc-ng[version='>=7.3.0'] -> _libgcc_mutex[version='*|0.1|0.1',build='main|conda_forge']
python=3.9 -> libgcc-ng[version='>=7.5.0'] -> _libgcc_mutex[version='*|0.1|0.1',build='main|conda_forge']

Package tzdata conflicts for:
torchvision==0.10.0=py39_cu111 -> python[version='>=3.9,<3.10.0a0'] -> tzdata
python=3.9 -> tzdataThe following specifications were found to be incompatible with your system:

  - feature:/linux-64::__glibc==2.33=0
  - torchvision==0.10.0=py39_cu111 -> cudatoolkit[version='>=11.1,<11.2'] -> __glibc[version='>=2.17,<3.0.a0']

Your installed version is: 2.33
```

## Expected behavior
Torchvision version 0.10.0 should be selected for installation.
If version is explicitly specified, packages should get installed without error.

## Environment
OS: Ubuntu 20.10 (x86_64)
GCC version: (Ubuntu 10.2.0-13ubuntu1) 10.2.0
Clang version: 11.0.0-2
CMake version: version 3.16.3
Python version: 3.9 (64-bit runtime)
GPU models and configuration: GPU 0: GeForce 930MX
Nvidia driver version: 460.80"
Mask R-CNN with MobileNet v3 backbone,pytorch/vision,2021-06-11 08:56:40,3,module: models#topic: object detection#new feature#needs training,4048,918461289,"## 🚀 Feature
Add Mask R-CNN with MobileNet v3 backbone

## Motivation

In the recent release torchvision added support for mobilenet v3 and extended it to work with instance segmentation models. 
`torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn`
However, support for Mask R-CNN with MobileNet v3 backbone is still missing and would be a worthy addition.

## Pitch

- Similar module like  `torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn` but to work with Mask R-CNN like resnet has` torchvision.models.detection.maskrcnn_resnet50_fpn`
- MobileNet V3 would make inference much faster
- Mask R-CNN is widely used owing to its dual use of getting mask+bounding box

"
Using torchvision on AMD GPU,pytorch/vision,2021-06-10 19:55:31,7,awaiting response,4044,917826635,"Hi,

I have installed and used [the AMD version of PyTorch](https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/) without any problem. I have also installed the AMD version of torchvision. However, I can not use torchvision with AMD GPU. Is there a solution to use torchvision on AMD GPU?

Thanks"
[feature request] rgb2lab / rgb2hsv / rgb2gray and other color space conversions (maybe upstream from kornia? or colorsys python core module?),pytorch/vision,2021-06-09 18:36:44,24,enhancement#needs discussion#module: transforms,4029,916536934,"It would be nice to have them available in the standard library (both on CPU/GPU) rather than rolling your own. Kornia seems to have implemented it: https://kornia.readthedocs.io/en/latest/color.html, skimage as well: https://scikit-image.org/docs/dev/api/skimage.color.html, python core as well: https://docs.python.org/3/library/colorsys.html, but I think it's basic enough to merit inclusion of similar functions in core.

(my usecase: SLIC superpixel extraction which requires rgb2lab and selective search algorithm which requires rgb2hsv and rgb2lab)

cc @vfdev-5"
Re-enable or remove test_cpp_models.py?,pytorch/vision,2021-06-07 13:25:12,2,module: tests,3992,913548005,"Currently, all the tests in `test_cpp_models.py` are unconditionally skipped:

```py
@unittest.skipIf(
    sys.platform == ""darwin"" or True,
    ""C++ models are broken on OS X at the moment, and there's a BC breakage on master; ""
    ""see https://github.com/pytorch/vision/issues/1191"")
```

(note the `or True`).

I tried running them on my laptop and they all passed except for a few of the `mnasnet` tests that failed with `RuntimeError: Given groups=32, expected weight to be divisible by 32 at dimension 0, but got weight of size [[40, 1, 3, 3]] instead` , which is probably fairly easy to fix. BTW, I'm on OS X so the ` sys.platform == ""darwin""` can probably be removed too.


Should we fix and re-activate these tests? Or perhaps they are dedundant with the ones in `test_models.py` now? The tests ran in 2+ minutes for me, so reactivating them will have a significant impact on CI usage, which is not negligible.

CC @fmassa 

cc @pmeier"
Mismatch in audio frames returned by pyav and video reader,pytorch/vision,2021-06-07 10:03:51,3,module: io#module: video,3986,913370515,"[audio-video sync test](https://github.com/pytorch/vision/blob/964ce1e95cba640768c5a39dbc018531c49d476c/test/test_video_reader.py#L1238) was failing in https://github.com/pytorch/vision/pull/3934. However, it turns out that the audio frames returned by our video reader doesn't match with pyav results(both output shapes and values are different).
Here is the [notebook](https://colab.research.google.com/drive/1G1_ejPJgEB-ljaAanBYnvlE02u-8ZWlb?usp=sharing) with details.

Tasks:

- [ ] Investigate the mismatch
- [ ] Fix the issue
- [ ] Add audio-video sync test(disabled in https://github.com/pytorch/vision/pull/4050).

cc @bjuncek"
Remove need of include `Python.h` in `vision.cpp`,pytorch/vision,2021-06-04 13:27:13,1,enhancement#module: ops,3965,911500276,"## 🚀 Feature

Related to https://github.com/pytorch/vision/issues/2349

We currently need to `#include <Python.h>` in https://github.com/pytorch/vision/blob/96ad7f0c80538e06323ab2a4d2d4fb513022f59a/torchvision/csrc/vision.cpp#L4
as well as we need to define `PyInit__C` for Windows https://github.com/pytorch/vision/blob/96ad7f0c80538e06323ab2a4d2d4fb513022f59a/torchvision/csrc/vision.cpp#L15-L22

This is ideally not needed. Windows builds currently require it due to an artifact on how we build our C++ extensions I believe (with setuptools).

We should try to remove this. This might involve using CMake to compiling the extensions (instead of setuptools), so might involve significant rework.

Further discussion can be found in https://github.com/pytorch/vision/pull/3087#discussion_r539154713

For reference, this [is how TF extensions does it](https://github.com/tensorflow/addons/blob/3bae3813f86a748d8d3833377368c98e5289da5d/tensorflow_addons/utils/resource_loader.py#L52) for loading extensions, vs our custom code in https://github.com/pytorch/vision/blob/96ad7f0c80538e06323ab2a4d2d4fb513022f59a/torchvision/extension.py#L8-L47 (and for image and video as well).

Given that our extensions are now completely independent from Python, we just need to know the location where they live and [directly call into `torch.ops.load_library`](https://github.com/pytorch/vision/blob/96ad7f0c80538e06323ab2a4d2d4fb513022f59a/torchvision/extension.py#L47).

Note that we will also need to make the solution compatible with fbcode"
Rename some transforms using appropriate wide adopted naming conventions,pytorch/vision,2021-05-31 18:46:54,2,needs discussion#module: transforms,3941,907660323,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
Rename confusing transformation names such as `transforms.Normalize()` to `transforms.Standardize()` and decouple combined operations `transforms.ToTensor()` into `transforms.Normalize()/transforms.Rescale()` and `transforms.ToTensor()` for clarity.

## Motivation

I'm always frustrated when trying to communicate the above operations (i.e. `transforms.Normalize()`, `transforms.ToTensor()`) to statisticians, the reason for that is that statisticians use the following conventional naming `standardize = rescale data to zero mean and unit variance` i.e., what `transforms.Normalize()` does, and, `normalize = rescale data value into the range[0, 1]`, i.e., what `tranforms.ToTensor()` does, which is basically 2 operations, transforming the PIL image into a tensor and rescaling it.

## Pitch
<!-- A clear and concise description of what you want to happen. -->
Is it possible to rename according to the world wide convention that statisticians use in order to avoid confusion in the future and have better clarity of what each operation is meant to be doing?

1. Rename `transforms.Normalize()` into `transforms.Standardize()`
2. Break `transforms.ToTensor` into two separate operations `transforms.Normalize()` which basically rescales the values into the range[0, 1], and `transforms.ToTensor()` which basically transforms a PIL image into a torch tensor.



cc @vfdev-5"
Documentation enhancement: Specifying detailed shape requirements for pretrained models,pytorch/vision,2021-05-26 02:18:45,7,module: models#module: documentation,3921,901641416,"## 📚 Documentation

In the documentation of [PyTorch Model Zoo](https://pytorch.org/vision/stable/models.html), it is suggested that:
> H and W are expected to be at least 224.

Technically, for H/W < 224 it is also workable, but there might be some lower bound.

For example, the AlexNet model is able to consume a tensor of [3, 200, 200] but not applicable for that of [3, 62, 62].

Similar cases also apply to vgg* and densenet*. Therefore, I am wondering if it is necessary to specify the undefined behaviours of models in the documentation to help users better leverage the models. Thanks.

"
Fix jpeg encoding tests on windows,pytorch/vision,2021-05-25 12:26:38,4,dependency issue,3913,900739563,"In https://github.com/pytorch/vision/pull/3908 we fixed the test logic of the tests for `encode_jpeg`. Unfortunately, the new ""correct"" tests don't work on windows.

This issue is about a) making those new correct test work on windows and b) remove the old incorrect tests

For b), having a roundtrip test as proposed in #3912 might be enough to remove the old tests.

CC @fmassa @datumbox "
Roundtrip test for encoding and decoding images,pytorch/vision,2021-05-25 12:26:26,7,,3912,900739350,"This goal of this issue is to add tests for both jpeg and png implementation on all platforms, to test that:

`decode(encode(image)) ~= image`.

This test will require an image-comparison util that is robust to minor changes. One possible approach is to compare histograms as done in PIL: https://github.com/python-pillow/Pillow/blob/affa059e959280bf7826ec1a023a64cb8f111b6d/Tests/helper.py#L110-L134


This test is not as robust as the ones we currently have where we individually test `encode` and `decode` w.r.t. to a reference implementation (PIL), but it's still good to have as a functional / integration test and it will hopefully help https://github.com/pytorch/vision/issues/3913 move forward.

CC @fmassa @datumbox @pmeier"
Potential improvements to jpeg decoding on GPU,pytorch/vision,2021-05-17 11:39:10,5,help wanted#module: io,3848,893242475,"A minimal version of jpeg decoding on GPUs was implemented in https://github.com/pytorch/vision/pull/3792. Here's a list of potential future improvements:

- Support for A100 devices
- Support for batch decoding (I didn't see any speed improvement in my experiments in https://github.com/pytorch/vision/pull/2786#issuecomment-832148710, but perhaps I missed something)
- Use a finer-grained API for the decoding phases, and potentially change the decoding backend depending on the image size, taking inspiration from https://github.com/NVIDIA/CUDALibrarySamples/tree/master/nvJPEG/nvJPEG-Decoder-MultipleInstances
- As per https://github.com/pytorch/vision/pull/3792#discussion_r629290933, we could:
  - Avoid creating tensor views and use some pointer arithmetic
  - investigate whether the layout (CHW vs HWC) has an impact on performance"
Bug in mAP/mAR calculation for reference object detection tutorial,pytorch/vision,2021-05-14 21:56:59,2,,3841,892272017,"## 🐛 Bug

I believe I've found a bug in the mAP/mAR evaluation code for the [TorchVision Object Detection Finetuning Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).

## To Reproduce

If you run the Jupyter Notebook version of the tutorial on Colab and make any of the following modifications, you can reproduce this bug.

1. In the training loop, add `evaluate(model, data_loader, device=device)` to evaluate performance on the training set, or
2. When defining `dataset_test`, use `get_transform(train=True)` instead of `train=False`, or
3. Always add `RandomHorizontalFlip` in `get_transform`

## Expected behavior

Whether or not `RandomHorizontalFlip` is added shouldn't have a substantial impact on mAP/mAR performance. However, I've noticed that adding `RandomHorizontalFlip` results in significantly lower mAP/mAR scores (~0.2 instead of ~0.7).

## Environment

This is the default environment on Google Colab:

PyTorch version: 1.8.1+cu101
Is debug build: False
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.12.0

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.0.221
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 460.32.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.8.1+cu101
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.9.1
[pip3] torchvision==0.9.1+cu101
[conda] Could not collect"
Error converting to onnx: forward function contains for loop,pytorch/vision,2021-05-14 03:52:58,2,question#awaiting response#module: onnx,3832,891574115,"Hello, there is a for loop in my forward function. When I turned to onnx, the following error occurred:

`[ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running Split node. Name:'Split_ 1277' Status Message: Cannot split using values in 'split' attribute. Axis=0 Input shape={59} NumOutputs=17 Num entries in 'split' (must equal number of outputs) was 17 Sum of sizes in 'split' (must equal size of selected axis) was 17`

Part of my forward code：
```
    y, ey, x, ex = pad(boxes, w, h)
    if len(boxes) > 0:
        im_data = []
        indx_y = torch.where(ey > y-1)[0]
        for ind in indx_y:
             img_k =  imgs[image_inds[ind],:, (y[ind] - 1).type(torch.int64):ey[ind].type(torch.int64), (x[ind]-1).type(torch.int64):ex[ind].type(torch.int64)].unsqueeze(0)
             im_data.append(imresample(img_k, (24, 24)))
        im_data = torch.cat(im_data, dim=0)
        return im_data
```
I found that during the first onnx conversion, the for loop was executed 17 times, but when I tested it, the for loop required 59 times, so there was an error. In the forward function, indx_y is dynamic, so the number of for loops is also dynamic. Is there any way to solve this problem？


cc @neginraoof"
[RFC] New Augmentation techniques in Torchvison,pytorch/vision,2021-05-12 11:55:34,13,module: transforms,3817,890007453,"## 🚀 Feature

Inclusion of new Augmentation techniques in `torchvision.transforms`. 

## Motivation

Transforms are important for data augmentation :sweat_smile: 


## Proposals

- [x] [RandAugment](https://arxiv.org/abs/1909.13719v2) Citations 306 #4348
- [x] ~~[Cutout](https://arxiv.org/abs/1708.04552) Citations 964~~ Superseded by [RandomErasing](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomErasing)
- [x] [MixUp](https://arxiv.org/abs/1710.09412) Citations 1624 [Code](https://github.com/facebookresearch/mixup-cifar10) (Note it is CC-BY-NC-license) #4379
- [x] [CutMix](https://arxiv.org/abs/1905.04899) Citations 437 [Code](https://github.com/clovaai/CutMix-PyTorch)  ICCV 2019 #4379 
- [x] [TrivialAugment](https://arxiv.org/abs/2103.10158) ICCV 2021 #4221 
- [x] [AugMix](https://github.com/google-research/augmix) Citations 157
- [x] [Scale-Jitter](https://arxiv.org/pdf/2012.07177.pdf) Citations 11 - [code](https://github.com/facebookresearch/detectron2/blob/main/configs/new_baselines/mask_rcnn_R_50_FPN_100ep_LSJ.py#L44) - [benchmarks](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md#new-baselines-using-large-scale-jitter-and-longer-training-schedule)
- [x] [Simple CopyPaste](https://arxiv.org/pdf/2012.07177v2.pdf) #5825
- [ ] Port [SSD & SSDlite Augmentations](https://github.com/pytorch/vision/blob/730c5e1eab130e2900c8e839ea08fa11f024516f/references/detection/transforms.py#L30-L239) and [Mixup/Cutmix](https://github.com/pytorch/vision/blob/c8e3b2a5925e7b7ed21662e86a7e9553170a5633/references/classification/transforms.py#L9-L175) from references to vision
- [ ] Detection Transforms #1406 #2213 #3980 
- [ ] [AutoAugment Detection](https://arxiv.org/pdf/1906.11172.pdf) 231 Citations [code](https://github.com/tensorflow/tpu/blob/b24729de804fdb751b06467d3dce0637fa652060/models/official/detection/utils/autoaugment_utils.py#L38)
- [ ] [Greedy Search Policy](https://arxiv.org/pdf/2002.09103v2.pdf)
- [ ] [FastAutoAugment](https://arxiv.org/abs/1905.00397) Citations 135 NeurIPS 2019 [Code](https://github.com/kakaobrain/fast-autoaugment) (1k+ GitHub stars)
- [ ] [ReMixMatch](https://arxiv.org/abs/1911.09785) Citations 165
- [ ] [FixMatch](https://arxiv.org/abs/2001.07685) Citations 258
- [ ] Transforms for color spaces augmentations E.g. rgb2hsv, rgb2bgr, rgb2lab, etc #4029
- [ ] Gaussian Noise See #6192

## Additional context

To visitors
Kindly give a  :+1:  if you think any of these would help in your work. 

Also if you have any transform in mind please provide few details here!

Linked to #3221 

cc @vfdev-5 @fmassa "
Mask-rcnn training - all AP and Recall scores in “IoU Metric: segm” remain 0,pytorch/vision,2021-05-11 12:09:41,1,question#topic: semantic segmentation,3811,887072635,"With torchvision’s pre-trained mask-rcnn model, trying to train on a custom dataset prepared in COCO format.

Using torch/vision/detection/engine’s `train_one_epoch` and `evaluate` methods for training and evaluation, respectively.

The loss_mask metric is reducing as can be seen here:
```
Epoch: [5]  [ 0/20]  eta: 0:00:54  lr: 0.005000  loss: 0.5001 (0.5001)  loss_classifier: 0.2200 (0.2200)  loss_box_reg: 0.2616 (0.2616)  loss_mask: 0.0014 (0.0014)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0120 (0.0120)  time: 2.7308  data: 1.2866  max mem: 9887
Epoch: [5]  [10/20]  eta: 0:00:26  lr: 0.005000  loss: 0.4734 (0.4982)  loss_classifier: 0.2055 (0.2208)  loss_box_reg: 0.2515 (0.2595)  loss_mask: 0.0012 (0.0013)  loss_objectness: 0.0038 (0.0054)  loss_rpn_box_reg: 0.0094 (0.0113)  time: 2.6218  data: 1.1780  max mem: 9887
Epoch: [5]  [19/20]  eta: 0:00:02  lr: 0.005000  loss: 0.5162 (0.5406)  loss_classifier: 0.2200 (0.2384)  loss_box_reg: 0.2616 (0.2820)  loss_mask: 0.0014 (0.0013)  loss_objectness: 0.0051 (0.0062)  loss_rpn_box_reg: 0.0120 (0.0127)  time: 2.6099  data: 1.1755  max mem: 9887
```
But the `evaluate` output shows absolutely no improvement from zero for IoU segm metric:

IoU metric: bbox
```
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.653
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.843
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.723
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.788
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.701
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.738
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.739
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.832
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.456
IoU metric: segm
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
````
The segm metrics don’t improve even after training 500 epochs.

And, the masks that I get as output after training for 100 or 500 epochs, if I visualize, they are showing a couple of dots here and there.

With the same dataset and annotations json, I was able to train instance seg model on detectron2. the the segmentation IoU metrics have clearly improved by each epoch.

Please suggest, what needs to be done. Posting here as there was no response on discuss.pytorch forum for 5 days

cc @vfdev-5"
ToTensor should support conversion to tensors of any dtype and device,pytorch/vision,2021-05-08 07:57:32,1,enhancement#module: transforms,3798,880467776,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
[`torchvision.transforms.ToTensor`][totensor] should support parameters to specify the dtype and target device of the tensors produced.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
Currently, `ToTensor` always converts images to FP32 tensors in the range [0, 1] if they have 8-bit values. This behavior is inefficient for use cases where the images need to be converted to another data type (such as FP16 or bfloat16) and/or transferred to an accelerator such as a GPU. Two cases come to mind:
- Case 1: We want to load images with 8-bit values into PyTorch and convert them to `HalfTensor`s.
  - First, `ToTensor` converts the images into `FloatTensor`s, quadrupling the memory they take up. Then, the calling code converts the images from `FloatTensor`s to `HalfTensor`s, which are half as large as the `FloatTensor`s.
  - It would be better not to have to allocate the memory used to store the `FloatTensor`s, and go straight from the original image format to `HalfTensor`s.
- Case 2: We want to load images with 8-bit values onto a GPU as `FloatTensor`s.
  - First, `ToTensor` converts the images into `FloatTensor`s in the range [0, 1], so they take up 4 times as much memory as they did before. Then, this data must be transferred to the GPU.
  - It would be better to convert the images into `ByteTensor`s, then move them to the GPU and convert them to `FloatTensor`s in the range [0, 1] on the GPU. This would use 1/4 of the data transfer bandwidth as the approach outlined above, and because the division by 255 is done in parallel on the GPU, this step is much faster. I've written [code that implements this][tobytetensor] and have noticed slight improvements in performance as a result.

## Pitch

<!-- A clear and concise description of what you want to happen. -->
At the least, we should add `dtype` and `device` parameters to the `ToTensor` constructor, like those for [`Tensor.to()`][tensor-to], which would determine the dtype and device of the tensors produced by this transform. If the `dtype` is an integer or boolean data type, we don't divide by 255. We can copy other parameters from `Tensor.to()` if they would be helpful.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->
I've written custom code using `transforms.Lambda` that converts images to FP32 tensors _after_ moving them to the GPU, but I think this functionality would be more widely available if it were built into `ToTensor`.

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->

[totensor]: https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor
[tobytetensor]: https://github.com/aidan-fitz/SolarTracer/blob/draft-multi-branch-seg/utils.py#L37
[tensor-to]: https://pytorch.org/docs/master/generated/torch.Tensor.to.html

cc @vfdev-5"
Could T.Lambda be nn.Module?,pytorch/vision,2021-05-06 12:22:19,6,question#module: transforms,3784,877439048,"It would allow it to be placed in nn.ModuleList for passing to RandomApply (for scriptability)

https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomApply

cc @vfdev-5"
[docs] Unclear if to_pil_image / to_tensor copy or zero-copy for CPU<->CPU,pytorch/vision,2021-05-06 11:52:59,5,question#module: transforms,3783,877415769,"It currently uses a vague language ""convert"". It's not sure if ""conversion"" incurs a copy or not

cc @vfdev-5"
[RFC] A common dataset root,pytorch/vision,2021-05-05 12:10:49,12,needs discussion#module: datasets#new feature,3776,876383352,"Currently, all datasets have a mandatory `root` parameter, indicating where the dataset will be or has been downloaded.

It would be more convenient if users didn't need to pass the root, and just rely on some predefined default behaviour. Also, having a default for all datasets will allow places with no internet access (looking at you fbcode 👀) to dump all datasets once and for all at the root, and have a seamless access to it afterwards.

Note that for downloading model weights e.g. using `fasterrcnn_resnet50_fpn(pretrained=True)`, we internally rely on  `load_state_dict_from_url()` which will download the weights in what [torch.hub.getdir()](https://pytorch.org/docs/stable/hub.html#torch.hub.get_dir) returns (by default, this is ``$TORCH_HOME/hub/``).

(BTW, *where* the models are downloaded doesn't seem to be configurable on the torchvision side, but that's another story)

In scikit-learn, a similar logic is used: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.get_data_home.html


### Solution

As previously discussed a bit with @pmeier and @datumbox we could do something similar for torchvision datasets:

- Introduce `torchvision.datasets.getdir()` and `torchvision.datasets.setdir()`. By default `getdir()` would return `$TORCH_HOME/torchvision_datasets/`, which is consistent with ``$TORCH_HOME/hub/``. 
- Introduce a default for all `root` parameters, where the default is what `torchvision.datasets.getdir()` returns

### Problem

(Yes, here the problem comes *after* the solution :))

For most datasets this should work OK. For a few of them (namely phototour, UCF101, Kinetics-400, HMDB51, Flickr, EMNIST, COCO), the `root` parameter is followed by other parameters without a default, so we can't introduce a default for `root` without changing its place, which would break backward compatibility.

The easiest workaround here would be to introduce defaults for these other parameters. Otherwise, [things get tricky](https://twitter.com/hug_nicolas/status/1389843180124786688).

### Other considerations

Currently, datasets are inconsistent with respect to how they treat the `root`: some will dump their data in `root/TheDatasetName` like MNIST, but some will dump their data directly in `root` like `Places365`.

While unlikely, this can create conflicts between datasets if they use the same file names. Perhaps it would be safe here to ""fix"" the datasets like `Places365` so that they all use a `root/TheDatasetName` directory. This will create a minor inconvenience of re-downloading the dataset for some users, but it's probably for the best?

CC @fmassa @datumbox @pmeier @prabhat00155 @parmeet 

cc @pmeier"
[docs] RandomResizeCrop signature quirks,pytorch/vision,2021-05-03 14:04:27,11,module: documentation,3763,874604534,"1. Why do we need to have so much precision: `1.333333333333`? `1.3` would suffice and be more readable
2. `<InterpolationMode.BILINEAR: 'bilinear'>`. It's not clear from this if the enum should be preferred or the constant (and it's also not copy-pastable in the code). It's also not clear which type is used for InterpolationMode, especially given that for a long time only PIL.Image.InterpolationMode was supported, hence even new codebases are using the legacy PIL types: https://github.com/facebookresearch/dino/blob/58aabc0/main_dino.py#L420
3. Why does `InterpolationMode` live in torchvision.transforms.functional whereas the type using it lives in torchvision.transforms? This enum is basic enough to deserve placement in torchvision.transforms directly IMHO.
4. The doc text mentions `torchvision.transforms.InterpolationMode`, but it's incorrect since the type currently lives in transforms.functional: https://pytorch.org/vision/master/transforms.html?highlight=randomresizedcrop#torchvision.transforms.functional.InterpolationMode

![image](https://user-images.githubusercontent.com/1041752/116885862-aa412f00-ac28-11eb-92ad-dffc292c8c51.png)
"
Ecoset - a large-scale vision dataset,pytorch/vision,2021-05-03 13:14:13,1,needs discussion#module: datasets#new feature,3762,874564610,"## 🚀 Feature
We (original authors) would like to suggest the inclusion of ecoset, a new large-scale image dataset, to be included in torchvision.datasets.

## Motivation
Ecoset consists of 1.5m images originating from 565 basic-level categories that are of importance to humans. Categories were chosen to be (a) basic level (think ""dog"" instead of ""cocker spaniel""), (b) most common in English language usage, and (c) rated as concrete by human observers. Compared to ImageNet, networks trained on ecoset are more similar to representations found in the brain (see https://www.pnas.org/content/118/8/e2011417118). Ten AlexNet and vNet instances trained on ecoset are available with the dataset (https://codeocean.com/capsule/9570390/tree/v1). We have been approached by multiple laboratories who would like to easily access ecoset from within the pytorch infrastructure, hence the request.

## Pitch
Please consider including ecoset as a dataset option in torchvision.

## Alternatives

## Additional context



cc @pmeier"
Refactor the `forward` method of the RoiHeads into separate methods for easier customization,pytorch/vision,2021-04-29 15:13:58,0,enhancement#module: ops#triage review,3753,871127848,"## 🚀 Feature
Break up the code in the `forward` method into several sub-methods and allow custom loss and inference functions.

## Motivation
I have a use case that requires implementing a different loss function for masks during training a Mask-RCNN, but designing this is somewhat messy right now. Ideally, I'd like to be able to inherit RoiHeads and then make a small change to a method like `process_masks` rather than overriding the whole forward method. (Or even simpler, just pass in custom loss functions during initialization).

## Pitch
The structure of `forward` lends itself to re-factoring really simply. 
`forward` could call, in sequence:
`process_boxes(...)`
`process_masks(...)`
`process_keypoints(...)`

This way, changing the behaviour requires only re-implementing one of these methods in the child class rather than all of `forward`. Optionally, these functions could use customized loss and inference functions, maybe passed into RoiHeads during initialization.

## Alternatives
For my use case, it would also be sufficient to be able to optionally pass RoiHeads custom loss functions during initialization, but I think breaking it up is still a good idea because it makes other changes that do require changing how some of these processes are handled a little simpler and cleaner.

Another option would be to add the functions related to loss and inference to the class itself, so they could be directly overriden by a child class, but I think this may muddy the responsibilities of RoiHeads a little.

## Additional context
Direct link to module: https://github.com/pytorch/vision/blob/master/torchvision/models/detection/roi_heads.py
I'm happy to take this on if approved.
"
MultiScaleRoIAlign.infer_scale fails on very oblong images,pytorch/vision,2021-04-28 11:47:38,2,bug#module: ops#topic: object detection,3747,869856677,"## 🐛 Bug

If the first given image is very tall or very wide, `MultiScaleRoIAlign.infer_scale` throws an AssertionError.

## To Reproduce

in `test.py`:

```
import torch
from torchvision.models.detection.transform import GeneralizedRCNNTransform
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.ops import MultiScaleRoIAlign

transform = GeneralizedRCNNTransform(
    min_size=100,
    max_size=100,
    image_mean=(0.5, 0.5, 0.5),
    image_std=(0.1, 0.1, 0.1)
)
backbone = resnet_fpn_backbone(backbone_name='resnet50', pretrained=True)

for width in range(100, 1000, 100):
    print(f'image size: H 100 * W {width}')
    images = [torch.rand((3, 100, width))]
    images, _ = transform(images)  # ImageList
    features = backbone(images.tensors)

    # instantiate MultiScaleRoIAlign here because the code of interest lies
    #   inside MultiScaleRoIAlign.infer_scale, which only runs once per instance
    pool = MultiScaleRoIAlign(
        featmap_names=['0', '1', '2', '3'],
        output_size=(7, 7),
        sampling_ratio=2
    )
    features = pool(features, [torch.tensor([[0., 0., 0., 0.]])], images.image_sizes)
```

execute:

```
$ python test.py
image size: H 100 * W 100
image size: H 100 * W 200
image size: H 100 * W 300
image size: H 100 * W 400
image size: H 100 * W 500
image size: H 100 * W 600
Traceback (most recent call last):
  File ""test.py"", line 27, in <module>
    features = pool(features, [torch.tensor([[0., 0., 0., 0.]])], images.image_sizes)
  File ""/[...]/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/[...]/torchvision/ops/poolers.py"", line 231, in forward
    self.setup_scales(x_filtered, image_shapes)
  File ""/[...]/torchvision/ops/poolers.py"", line 192, in setup_scales
    scales = [self.infer_scale(feat, original_input_shape) for feat in features]
  File ""/[...]/torchvision/ops/poolers.py"", line 192, in <listcomp>
    scales = [self.infer_scale(feat, original_input_shape) for feat in features]
  File ""/[...]/torchvision/ops/poolers.py"", line 176, in infer_scale
    assert possible_scales[0] == possible_scales[1]
AssertionError
```

## Expected behavior

Very oblong images should not cause this failure. At the very least, there should be a way to manually set up the pooling scales without relying on heuristics from the initial image.

## Environment

```
$ python collect_env.py 
Collecting environment information...
PyTorch version: 1.8.1+cu102
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.152
GPU models and configuration: GPU 0: GeForce GTX 1650
Nvidia driver version: 460.73.01
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] efficientnet-pytorch==0.7.0
[pip3] numpy==1.19.2
[pip3] torch==1.8.1
[pip3] torchvision==0.9.1
[conda] Could not collect
```

## Additional context

* I'm not familiar with the process of inferring pooling scale, nor have I read the paper specified in the docstring (""eq. 1 of the Feature Pyramid Network paper at https://arxiv.org/abs/1612.03144"").
* I think the code is slightly different in python 3.9, but I don't think it affects this specific issue.
* The replication code demonstrates failure on very wide images, but it similarly fails on very tall images.
"
Ability to import CocoEvaluator,pytorch/vision,2021-04-26 08:45:28,2,topic: object detection#new feature,3728,867439081,"## 🚀 Feature
It would be great if we could do the following:

```
from torchvision import CocoEvaluator

base_ds = dataset.coco
iou_types = ['bbox']
coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths

model = ... # fancy deep learning model
model.eval()

print(""Running evaluation..."")
for batch in data_loader:
    outputs = model(**batch)
    result = ... # convert outputs of model to COCO api
    coco_evaluator.update(result)

coco_evaluator.synchronize_between_processes()
coco_evaluator.accumulate()
coco_evaluator.summarize()
```

Right now, `CocoEvaluator` cannot be imported as it's currently under the [references](https://github.com/pytorch/vision/tree/master/references) directory in this repository, it's not part of the `torchvision` package.

## Motivation

I'm currently implementing [DETR](https://github.com/facebookresearch/detr) (end-to-end object detection with Transformers), and right now I have to copy all of this code of COCO evaluation in order to evaluate the model. The authors of DETR also copied a lot of the code for evaluation into their own repository. It would be great if we can simply import it, and run evaluation of a deep learning model. 

Even in the [official torchvision tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#putting-everything-together), they state that:

""In references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py. Just copy everything under references/detection to your folder and use them here."" 
=> life would be easier if users don't need to look into Github repos and copy files into their own folder. Also, there would be a central place (namely this repository) where the official COCO evaluation is defined, and can be updated in the future. Right now evaluation is cluttered across hundreds of Github repos.

This would also foster reproducability of experiments with object detection models, as right now it's a lot of work to just evaluate a model with metrics like mAP."
Move GroupedBatchSampler into torchvision,pytorch/vision,2021-04-23 11:57:58,4,enhancement#needs discussion#topic: object detection,3714,866042845,"## 🚀 Feature
Move the [`GroupedBatchSampler`](https://github.com/pytorch/vision/blob/d7ad62ff83bb329b7e5f2725bfa84e1266febc55/references/detection/group_by_aspect_ratio.py#L23) into the core library

## Motivation

Grouping minibatch elements is often a useful feature for vision tasks, especially in object detection and segmentation problems where it is commonplace to use images with different shapes and aspect ratios. Torchvision already has an implementation of a sampler that can do this, however it isn't part of the library itself, only in the references. As such users either have to copy-paste it into their own code or import `detectron2` to utilise it.

## Pitch

Probably just copy the entire [`group_by_aspect_ratio.py`](https://github.com/pytorch/vision/blob/d7ad62ff83bb329b7e5f2725bfa84e1266febc55/references/detection/group_by_aspect_ratio.py) into `torchvision/datasets/samplers`, perhaps split it out if necessary. I think this would all be useful for people working with detection or segmentation tasks that don't want to bring in all of d2."
Add new illustrations of transforms,pytorch/vision,2021-04-19 14:51:24,10,enhancement#module: transforms#module: documentation,3688,861395482,"With https://github.com/pytorch/vision/pull/3652 merged, we can now illustrate how each transforms affects an image via a `sphinx-gallery` example:

https://519065-73328905-gh.circle-artifacts.com/0/docs/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py

The goal of this issue is to add more transforms to this file: https://github.com/pytorch/vision/blob/master/gallery/plot_transforms.py

Instructions:

- pick you favourite transform that isn't already present in the example (after checking below that no one is already working on it)
- comment below ""I'm working on `<the transform>`"" so that others don't pick the same one
- edit the example file with something that properly illustrate the transform, taking inspiration from the already present transforms
- Build the docs following the instructions [here](https://github.com/pytorch/vision/blob/master/CONTRIBUTING.md#documentation), check that everything is fine, then submit a PR.

You can pick more than one transform in the same PR if you feel like it.

cc @vfdev-5"
Deformable convolution best practice? ,pytorch/vision,2021-04-16 07:20:24,1,question#module: ops,3678,859539778,"## ❓ Questions and Help
Would appreciate it if anyone has some insight on how to use deformable convolution correctly. 

Deformable convolution is tricky as even the official implementation is different from what's described in the paper. The paper claims to use 2N offset size instead of 2 x ks x ks. 

Anyway, we're using the 2 x ks x ks offset here, but I always got poor performance. Accuracy drops in CIFAR10 and YOLACT. Anything wrong with my usage? 
```
from torchvision.ops import DeformConv2d

class DConv(nn.Module):
    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False):
        super(DConv, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, 2 * kernel_size * kernel_size, kernel_size=kernel_size,
                               stride=stride, padding=padding, bias=bias)
        self.conv2 = DeformConv2d(inplanes, planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(x, out)
        return out
```
"
The document of torchvision.ops.deform_conv2d is not clear,pytorch/vision,2021-04-15 06:43:49,12,question,3673,858548194,"## 📚 Documentation
From the documentation, I cannot get the exact meaning of 18(ie, 2*3*3) channels of the offset in a deformable convolution? 

I want to visualize the offset of the deformable convolution with kernel size 3*3.
So It’s essential for me to know what’s the exact meaning of these channels.

I write down something possible here:
```python
upper-left: ul
upper-right: ur
bottom-left: bl
bottom-right: br
up: u
bottom: b
right: r
left: l
center: c

possible offset layout (maybe not correct):
delta_ul_x, delta_ul_y,   delta_u_x, delta_u_y,     delta_ur_x, delta_ur_y;
delta_l_x, delta_l_y,       delta_c_x, delta_c_y,      delta_r_x, delta_r_y;
delta_bl_x, delta_bl_y,   delta_b_x, delta_b_y,     delta_br_x, delta_br_y;
```

"
[Feature proposal] Apply `adjust_contrast` transformation for grayscale tensors,pytorch/vision,2021-04-14 14:37:20,4,enhancement#module: transforms,3670,857964384,"## Motivation

Currently, `adjust_contrast` function in `torchvision.transformations.functional_tensor` only works for 3-channel (=RGB) tensors. However, I would like to change the contrast of a 1-channel (= grayscale) tensor.

## Pitch

Just like the 3-channel case, the desired function changes the image contrast. I.e., `contrast_factor = 0` gives a uniform gray image, 1 gives the original image while 2 increases the contrast by a factor of 2.

## Additional context

This feature can be implemented by slightly modifying the `adjust_contrast` function in `torchvision.transformations.functional_tensor` like this;

```
def adjust_contrast(img: Tensor, contrast_factor: float) -> Tensor:
    if contrast_factor < 0:
        raise ValueError('contrast_factor ({}) is not non-negative.'.format(contrast_factor))

    _assert_image_tensor(img)

    _assert_channels(img, [1, 3])

    dtype = img.dtype if torch.is_floating_point(img) else torch.float32

    num_channels = _get_image_num_channels(img)
    if num_channels == 1:
        gray_img = img.to(dtype)
    elif num_channels == 3:
        gray_img = rgb_to_grayscale(img).to(dtype)

    mean = torch.mean(gray_img, dim=(-3, -2, -1), keepdim=True)

    return _blend(img, mean, contrast_factor)
```

cc @vfdev-5"
Failed to compile torchvision for ROCm as documented in pytorch.org,pytorch/vision,2021-04-11 12:32:10,2,question#topic: build#topic: binaries,3658,855295461,"## 🐛 Bug

Failed to compile torchvision for ROCm as documented in pytorch.org/get-started

## To Reproduce

Steps to reproduce the behavior:

as in: https://pytorch.org/get-started/locally/
1. python -m venv ptamd; source ptamd/bin/activate
1. pip install torch -f https://download.pytorch.org/whl/rocm4.0.1/torch_stable.html
1. pip install ninja && pip install 'git+https://github.com/pytorch/vision.git@v0.9.1'

same with v0.9.0

## Error

ptamd/lib/python3.8/site-packages/torch/include/c10/util/complex.h:9:10: fatal error: 'thrust/complex.h' file not found
#include <thrust/complex.h>
       ^~~~~~~~~~~~~~~~~~
1 error generated when compiling for gfx803.

## Environment

```
PyTorch version: 1.8.1+rocm4.0.1
Is debug build: False
ROCM used to build PyTorch: 4.0.20496-4f163c68

OS: CentOS Linux 8 (x86_64)
GCC version: (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5)   # same on GCC 10

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
GPU models and configuration: Vega 20
HIP runtime version: 3.21.2
MIOpen runtime version: 2.9.0

Versions of relevant libraries:
[pip3] numpy==1.20.2
[pip3] torch==1.8.1+rocm4.0.1
```


"
Allow batch_size > 1 in quantized RoIAlign,pytorch/vision,2021-04-09 12:36:26,0,module: ops,3655,854484973,"https://github.com/pytorch/vision/pull/3624 was recently merged and we identified a potential issue: https://github.com/pytorch/vision/pull/3624#discussion_r608533402

In short, the rois tensor contains indices in the first column, but depending on the quantization, some indices cannot be properly represented. For example uneven numbers can't be represented if the tensor was quantized with `qscale = 2`.

To prevent any potential bug, we currently force the batch size to be 1 and hard-code the index to 0:

https://github.com/pytorch/vision/blob/07fb8ba7fad7b5b458ff862919825df4e6f60b52/torchvision/csrc/ops/quantized/cpu/qroi_align_kernel.cpp#L156-L158

https://github.com/pytorch/vision/blob/07fb8ba7fad7b5b458ff862919825df4e6f60b52/torchvision/csrc/ops/quantized/cpu/qroi_align_kernel.cpp#L39-L40

We should try to allow more than one element per batch. A potential solution would involve using per-channel quantized tensors for the roi tensor, where the first column containing the indices would be quantized in a different way from the rest of the columns.

In roi_align python op:
  - if a tensor with 5 columns is passed, raise an error if it's not per-channel: there's a high changes the indices are wrong and it's too risky. If the tensor is per-chanel, pass it through:  we can assume that the user knows what they're doing and that the indices are properly represented. As a good sanity check, we can still check that the batch size is within the range of the quantized type of the first column.
  - if a list of tensors is passed, convert that list of tensors into a per-channel quantized tensor with 5 columns.

The `convert_boxes_to_roi_format` utils should be modified. To ensure consistency throughout the library, it should also be used in `MultiScaleRoIAlign`."
Pretrained inception_v3 without aux_logits,pytorch/vision,2021-04-08 00:14:46,0,,3648,852938606,"## 🐛 Bug
> RuntimeError: Error(s) in loading state_dict for Inception3:
        Unexpected key(s) in state_dict: ""AuxLogits.conv0.conv.weight"", ""AuxLogits.conv0.bn.weight"", ""AuxLogits.conv0.bn.bias"", ""AuxLogits.conv0.bn.running_mean"", ""AuxLogits.conv0.bn.running_var"", ""AuxLogits.conv1.conv.weight"", ""AuxLogits.conv1.bn.weight"", ""AuxLogits.conv1.bn.bias"", ""AuxLogits.conv1.bn.running_mean"", ""AuxLogits.conv1.bn.running_var"", ""AuxLogits.fc.weight"", ""AuxLogits.fc.bias"". 


## To Reproduce
```python
import torchvision.models
my_model = torchvision.models.inception_v3(pretrained=True, aux_logits=False)
```

## Expected behavior
From the common sense perspective, I'm not sure if it makes any sense to load a pretrained `inception_v3` without `aux_logits`? If it doesn't make any sense to use it like this, it would be useful to print a warning instead of (or in addition to) the current error for unexpected key(s).

But if it actually makes sense to load the model like this, this could be a fix for it:
([`torchvision/models/inception.py`](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py#L54))
```python
model.load_state_dict(state_dict, strict=model.aux_logits)
```
 
"
Question about kernel size of class LastLevelMaxPool(ExtraFPNBlock) in feature_pyramid_network.py,pytorch/vision,2021-04-05 12:00:54,1,,3632,850319737,"## ❓ Questions and Help
Why the pool kernel = 1? I think it should be 2 (my understanding is that kernel = 1 won't do max pooling correctly.
line 175:  x.append(F.max_pool2d(x[-1], **1**, 2, 0))

"
COCO dataset to also return image_id,pytorch/vision,2021-03-25 16:30:11,0,module: datasets#new feature,3608,841103161,"So that copies like https://github.com/facebookresearch/detr/blob/master/datasets/coco.py are not needed

In general, I found it useful to return original datum ids, in case the user wants to load some extra information (regardless of COCO). Part of the problem is the Tuple return type which means any changes are not BC.

Maybe `return_dict` constructor arg could be introduced which would be more extensible, just like DETR did

cc @pmeier"
Centralize nightly version,pytorch/vision,2021-03-24 09:50:20,0,enhancement#topic: build,3600,839541838,"## 🚀 Feature

The current nightly version of torchvision is spread over many different locations, see https://github.com/pytorch/vision/pull/3599

It would be great if we could centralize it instead in a single place (like `version.txt`), so that we only need to change this file whenever we want to bump the nightly version"
Torchvision ops registration still not fixed in C++,pytorch/vision,2021-03-20 20:56:36,3,module: c++ frontend,3593,836909107,"It turns out the solution I implemented for automatic ops registration in C++ is not robust enough.
If falls apart when the actual torchvision code is enclosed in a static library, which is then meant to be consumed by an executable. This indirect hop through a library exposes the pruning problem all over again. Apologies for not realizing earlier and implementing it this way :/.

In the end, I think the only two viable solutions are the ones we discussed a while ago:
1) Force include all symbols through linker commands (eg. `--whole-archive`)
2) Add, only for the C++ API, a `register_operators` function that must be called in order to explicitly load the operators, and stop relying on the implicit behavior

As before my preference goes to 2 because it's simpler to implement and maintain, and it doesn't risk bloating the binary with unneeded symbols. However, option 1 is  what torch already does (sadly).

Thoughts? @fmassa @datumbox "
Exported Mask-RCNN to ONNX produces wrong results,pytorch/vision,2021-03-18 21:17:35,0,,3588,835270587,"## 🐛 Bug

I exported the pretrained rcnn model to onnx via
```
model_tv = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model_tv.eval()
torch.onnx.export(model_tv, torch.rand(1,3,800,800), ""mask_rcnn_r50_fpn.onnx"",
                  do_constant_folding=True,
                  opset_version=12  # opset_version 11 required for Mask R-CNN
                  )
```

However, when I perform inference on that model:
```
import onnxruntime as rt
import numpy
import argparse
import numpy as np
import cv2

parser = argparse.ArgumentParser(description='ImageNet native ORT')
parser.add_argument('--model', type=str, required=True)
parser.add_argument('--image', type=str, required=True)
args, args_other = parser.parse_known_args()

sess = rt.InferenceSession(args.model)
input_name = sess.get_inputs()[0].name

def load_image(img_path):
    # CV loads in BGR, and rcnn expects rgb
    loaded = cv2.imread(img_path)
    loaded = cv2.cvtColor(loaded, cv2.COLOR_BGR2RGB)
    img_data = loaded.transpose(2, 0, 1)

    # The mean values provided are in RGB format
    mean_vec = np.array([0.485, 0.456, 0.406])
    stddev_vec = np.array([0.229, 0.224, 0.225])

    norm_img_data = np.zeros(img_data.shape).astype('float32')
    for i in range(img_data.shape[0]):  
        norm_img_data[i,:,:] = (img_data[i,:,:]/255 - mean_vec[i]) / stddev_vec[i]
    norm_img_data = np.expand_dims(norm_img_data, axis=0)
    return norm_img_data

def perform_inference(file):
    preprocessed = load_image(file)
    print(file)
    result = sess.run(None, {input_name: preprocessed})
    import pdb; pdb.set_trace()


perform_inference(args.image)
```

with the attached image
![preprocessed](https://user-images.githubusercontent.com/10335022/111702714-ee53ae00-87f9-11eb-8ac2-a42cf0bc2067.jpg)


The results are incorrect:

![image](https://user-images.githubusercontent.com/10335022/111698667-556e6400-87f4-11eb-9091-9248c37d03d1.png)

I assumed the labels are the standard Coco labels as provided e.g. [here](https://learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/)

I'm fairly certain this is either a preprocessing issue or an issue of using the wrong label set, but as far as I could find both are correct.

In addition to the labels being mismatched, the probabilities are a lot lower than I got using the mask-rcnn model from the [onnx model zoo](https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/mask-rcnn)
"
[RFC] New datasets to torchvision,pytorch/vision,2021-03-12 17:41:24,12,module: datasets#new feature,3562,830325454,"## 🚀 Feature

This is a proposal to add more highly cited datasets. Thanks to [papers with code datasets](https://paperswithcode.com/datasets) which made this search easy. 

## Motivation

These datasets are used quite frequently and would provide benefits to both researchers as well as people who work in computer vision. I'm not sure of the citation metric, but we can verify the count of papers once.

## Pitch

The following datasets can be considered. Papers are reported as per the last 5 years count on papers with code. They can be inaccurate, feel free to edit. I'm also adding previously approved or proposed ones

- [x] [KITTI](https://paperswithcode.com/dataset/kitti) 1709 papers #3640 
- [x] [iNaturalist](https://www.inaturalist.org/) #3292 #4123 
- [x] [LFW Labeled Faces in Wild](https://paperswithcode.com/dataset/lfw) 640 papers #4255 
- [x] [Caltech-UCSD Birds-200-2011](https://www.paperswithcode.com/dataset/cub-200-2011)  839 papers #4128 #4126 #5154
- [ ] [ADE20K](https://github.com/pytorch/vision/issues/5200)
- [ ] [Tiny-Imagenet](https://github.com/pytorch/vision/issues/6127)
- [ ] [Ego4d](https://ai.facebook.com/research/publications/ego4d-unscripted-first-person-video-from-around-the-world-and-a-benchmark-suite-for-egocentric-perception/)
- [ ] [Market-1501](https://paperswithcode.com/dataset/market-1501) 492 papers
- [ ] [MPII Human Pose](https://paperswithcode.com/dataset/mpii)
- [ ] [VGGFace2](https://paperswithcode.com/dataset/vggface2-1) Earlier requested in #1193 #2910 [Here](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz) is tar.gz file. Hopefully we can add it
- [ ] [MovingMNIST](http://www.cs.toronto.edu/~nitish/unsupervised_video/)  Perviously approved in #2676  #2690.
- [ ] [LVIS](https://www.lvisdataset.org) 
- [ ] [CamVid](http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/)
- [ ] [Div2k](https://paperswithcode.com/dataset/div2k)
- [ ] [Berkley Segmentation Dataset BSD](https://paperswithcode.com/dataset/bsd)
- [ ] [ffhq-dataset](https://github.com/NVlabs/ffhq-dataset)
- [ ] [SmallNORB](https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/.)

See #5108

Probably, we should think and add these, one by one. Also support downloading, not just loading of the dataset.

## Additional context
Please feel free to discuss about datasets before opening PRs!


cc @pmeier"
Black band at certain videos,pytorch/vision,2021-03-09 11:08:17,0,module: video,3534,825743888,"## 🐛 Bug

There is an issue in memory alignment before SWS scale which causes an error in about 4% of the frames on my sample of random videos from various datasets and about 1% of them on the test videos in TV. 

The issue most likely lies in memory alignment in our usage of `av_image_fill_arrays` as this works when hardcoded av_frame buffer is allocated. I'm investigating the fix in the meantime

## To Reproduce

Open and try to visualize the first frame of `vision/test/assets/videos/WUzgd7C1pWA.mp4` using any method that relies on video_reader backend



## Expected behavior

A full frame with no artifacts is decoded

## Environment

```
PyTorch version: 1.8.0a0+d555768
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.19.4

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: Quadro RTX 8000
GPU 1: Quadro RTX 8000

Nvidia driver version: 460.27.04
cuDNN version: Probably one of the following:
/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7
/usr/local/cuda-10.2.89/targets/x86_64-linux/lib/libcudnn.so.7
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.0
[pip3] torch==1.8.0a0+unknown
[pip3] torchvision==0.9.0a0+8ee9092
[conda] blas                      1.0                         mkl
[conda] magma-cuda112             2.5.2                         1    pytorch
[conda] mkl                       2020.2                      256
[conda] mkl-include               2020.2                      256
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.2.0            py38h23d657b_0
[conda] mkl_random                1.1.1            py38h0573a6f_0
[conda] numpy                     1.20.0                   pypi_0    pypi
[conda] torch                     1.8.0a0+unknown          pypi_0    pypi
[conda] torchvision               0.9.0a0+8ee9092           dev_0    <develop>
```



## Additional context

This issue arose in https://github.com/pytorch/vision/pull/2916 and I'm opening a new one to have the change of the code needed (using `torch.max` instead of `torch.mean` for video stream tests in `tests/test_video.py`. I'm hoping to merge that PR and continue to track the issue here. 


cc @bjuncek"
Replace uses of `_is_tracing` with `torch.onnx.is_in_onnx_export()`,pytorch/vision,2021-03-02 10:05:01,0,enhancement#module: models#module: onnx,3487,819857717,"Through the codebase we have special implementations for ONNX guarded by `torchvision._is_tracing()`. We should see if it would be possible to replace it with `torch.onnx.is_in_onnx_export()` so that the meaning of those code-blocks is simpler, and so that we don't depend on internal PyTorch APIs.

I would start by checking if replacing https://github.com/pytorch/vision/blob/5b663c8eb691d540d064197a681a3a0fa924599c/torchvision/__init__.py#L92 with the aforementioned function passes ONNX tests, and then replace all remaining uses of it in the codebase.

cc @neginraoof"
[feature request] Support batches (arbitrary number of batch dims) for box_iou / generalized_box_iou / box_area / box_convert / clip_boxes_to_image,pytorch/vision,2021-03-01 14:05:47,2,,3478,818899544,"Currently it supports only `(N4, M4) -> NM`. I propose to also support `(BN4, BM4) -> BNM`. It would also be nice if they supported arbitrary number of batch dimensions.

This is useful for computing a cost matrix between predicted boxes and ground truth boxes for a batch of frames. Probably it can be done by adjusting tensor indexing. Something like that:
```python
def _box_inter_union(boxes1: Tensor, boxes2: Tensor) -> Tuple[Tensor, Tensor]:
    area1 = box_area(boxes1)
    area2 = box_area(boxes2)

    lt = torch.max(boxes1[..., None, :2], boxes2[..., :2, None])
    rb = torch.min(boxes1[..., None, 2:], boxes2[..., 2:, None])

    wh = _upcast(rb - lt).clamp(min=0) 
    inter = wh[..., :, 0] * wh[..., :, 1]

    union = area1[..., None] + area2 - inter

    return inter, union
```"
Can`t export mobilenetv3 model to onnx,pytorch/vision,2021-02-26 02:26:45,8,module: onnx,3463,816969119,"` model = models.mobilenet_v3_small(pretrained=True)
   input_np = np.random.uniform(0, 1, (1, 3, 224, 224))
   input_var = torch.FloatTensor(input_np)
   torch.onnx.export(model, args=(input_var), f=""cnn.onnx"", verbose=False, input_names=[""input""], output_names=[""output""])`


Error: RuntimeError: Exporting the operator hardsigmoid to ONNX opset version 9 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub.

cc @neginraoof"
README section for video backends,pytorch/vision,2021-02-25 10:13:42,1,module: documentation#module: video,3460,816291216,"## 📚 Documentation

Describing the video backends, APIs, installation and requirements in the main README.md.
Specifically targeted to address issues such as #3367 #3440 and have a clearer reference point for the future users. 


cc @bjuncek"
Add download functionality if available to all datasets,pytorch/vision,2021-02-25 06:38:40,1,enhancement#module: datasets,3452,816136186,"## 🚀 Feature

Add download functionality if available to all datasets.

## Motivation

Currently, the following datasets have no option to download them directly:

- `Cityscapes`
- `Coco(Detection|Captions)`
- `Flickr(8|30)k`
- `HMDB51`
- `ImageNet`
- `Kinetics400`
- `LSUN`
- `UCF101`

## Pitch

Of those listed above only

- `Cityscapes`
- `Flickr(8|30)k`
- `ImageNet`

require some manual action that cannot be provided by us. For the others we can provide a `download` flag.


cc @pmeier"
[docs] torchvision.io.read_video docs do not (but should) specify image scale format,pytorch/vision,2021-02-23 14:35:42,1,module: documentation#module: io#module: video,3440,814523662,"Is it int8, uint8, float32 from 0 to 1, -1/1? Is the format always same or input-file dependent? What happens for grayscale single-channel input videos? Will it still produce 3 channels?

cc @bjuncek"
Create reset_parameters method for ResNet blocks and models + move existing parameters init code there,pytorch/vision,2021-02-17 17:26:29,1,module: models#new feature,3410,810386134,"This would ease re-initialization some blocks that are not meant to be pretrained/frozen:
https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L188-L203"
"TorchVision support for pre-trained, quantised, ResNet50 using QNNpack backend",pytorch/vision,2021-02-08 21:38:34,5,enhancement#module: models.quantization,3362,803991019,"## 🚀 Feature
Addition of pre-trained, quantised ResNet50 model to `torchvision.models` with support for QNNpack backend.

## Motivation

Quantised models have been shown to deliver greater performance than their fp32 equivalents without sacrificing accuracy.
The quantised ResNet50 model currently available in TorchVision supports the FBGEMM backend, but does not provide support for QNNpack. On AArch64, only the QNNpack backend is currently supported, so expanding the functionality of the ResNet50 model would allow its use one AArch64 platforms.
Expanding the capabilities of the quantised models available to TorchVision users would provide functionality equivalent to what is currently available in other frameworks.

## Pitch

Quantised, pre-trained, ResNet50 models are available for use on with TensorFlow (https://github.com/IntelAI/models/blob/master/benchmarks/image_recognition/tensorflow/resnet50v1_5/README.md#int8-inference-instructions) and typically deliver improved performance over their fp32 equivalents.

Are there any plans to expand the supported backends for the quantised ResNet50 model to TorchVision to cover QNNpack?

## Alternatives

Where the workload or benchmark requires ResNet50, the alternative is to make your own quantised model using the qnnpack backend.
"
[RFC] Simplify release note creation,pytorch/vision,2021-02-04 16:08:17,12,needs discussion,3351,801424476,"Few ideas on how to simplify the release note creation by structuring PRs.

For example, those projects are using tools to automate some part of the process:
- spyder ide, https://github.com/spyder-ide/loghub/
- sympy, https://github.com/sympy/sympy/pull/19640#issuecomment-649379944
   - https://github.com/sympy/sympy-bot
   - https://github.com/sympy/sympy/wiki/Writing-Release-Notes
- https://pypi.org/project/towncrier/

There are pros/cons for automation approaches:
+ automation
- should explain how to use the bots for the PR note writing

cc @datumbox "
"libtorch C++, fasterrcnn_resnet50_fpn module.forward() Assert",pytorch/vision,2021-02-04 11:49:10,16,topic: classification#module: c++ frontend,3349,801210558,"## 🐛 Bug

module.forward() launches Debug assert 

File: minkernel\crts\ucrt\src\appcrt\heap\debug_heap.cpp
Line: 966

Expression: __acrt_first_block == header

## To Reproduce

Loaded scripted model with 
```
torch::jit::script::Module module;
try {
	module = torch::jit::load(model_path);
}
catch (const c10::Error& e) {
	std::cerr << e.what();
	return -1;
}
module.eval();
```

Loaded image into tensor with
```
cv::Mat image; 
cv::Mat3f image_32fc3;

image = cv::imread(image_path, cv::IMREAD_COLOR);
auto h = image.rows;
auto w = image.cols;
auto c = image.channels();

image.convertTo(image_32fc3, CV_32FC3, 1.0f / 255.0f);
at::Tensor inputTensor = torch::from_blob(image_32fc3.data, { 1, h, w, c });
inputTensor = inputTensor.permute({ 0, 3, 1, 2 });
torch::DeviceType device_type = torch::kCPU;
inputTensor = inputTensor.to(device_type);

```

Both model and tensor seem to be loaded correctly anyway 

```
std::vector<torch::jit::IValue>  input_to_net;
input_to_net.push_back(inputTensor);
at::Tensor output = module.forward(input_to_net).toTensor();
```

does not work.

call stack is:
![image](https://user-images.githubusercontent.com/78026635/106888160-77010980-66e6-11eb-97e0-1d848a417014.png)

## Environment

OS: Microsoft Windows 7 Professional
Language: C++
CMake version: version 3.17.1
Python version: 3.7 (64-bit runtime)
Is CUDA available: N/A
numpy==1.18.5
torch==1.7.1+cpu
torchaudio==0.7.2
torchvision==nightly
Python version:


## Additional context

<!-- Add any other context about the problem here. -->


cc @vfdev-5"
VideoClips.get_clip to provide to users some clipId and a way to get meta information about clip pts/time in video,pytorch/vision,2021-02-02 17:10:31,3,enhancement#module: datasets#module: video,3340,799482139,"It would be nice for debugging to have VideoClips provide not only `video_idx` but also `clip_idx`. With that index one should have a way to get the temporal location corresponding to the sampled clip.

This allows to correlate the sampled clip with subtitles or just scroll into the video for debugging. Currently the returned `info` object just has `video_fps` property, `clip_idx` is not there. 

cc @pmeier @bjuncek"
"[proposal] Use self.flatten instead of torch.flatten and when becomes possible derive ResNet from nn.Sequential (scripting+quantization is blocker), would simplify model surgery in the most frequent cases",pytorch/vision,2021-01-31 09:50:39,23,needs discussion#module: models,3331,797671455,"Currently In https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L243: 
```python
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
```

If it instead used `x = self.flatten(x)`, then it would simplify model surgery: `del model.avgpool, model.flatten, model.fc`. Also in this case the class can just derive from Sequential and use OrderedDict to pass submodules (like in https://discuss.pytorch.org/t/ux-mix-of-nn-sequential-and-nn-moduledict/104724/2?u=vadimkantorov), this would preserve checkpoint compat as well. The method `forward` could then be removed"
guard AuxLogits call with torch.jit.is_scripting()?,pytorch/vision,2021-01-29 00:15:35,1,awaiting response#module: models#torchscript,3321,796473690,"Should we guard the check:
https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py#L174
with `torch.jit.is_scripting()`? since if `self.AuxLogits` is None we'll be compiling `aux = self.AuxLogits(x)` and jit will complain that None is used as a function.

I changed it to
```
       if not torch.jit.is_scripting() and aux_defined:
            aux = self.AuxLogits(x)
        else:
            aux = None
```
and it worked.
"
'download=True' condition for more than 1 SBD dataset stops code cause of shutil,pytorch/vision,2021-01-28 10:56:28,1,bug#module: datasets,3316,795896528,"'download=True' condition for more than 1 dataset stops code cause of shutil

I always code dataset and dataloader as below

![image](https://user-images.githubusercontent.com/49643709/106124243-38c39180-619e-11eb-8fbf-708cbb24952e.png)


But at this time, dealing with SBD dataset, I get stucked as below

![image](https://user-images.githubusercontent.com/49643709/106122572-36603800-619c-11eb-8dc3-dfd048528f3e.png)


I saw the torchvision dataset source code and documentation, I think it would be more helpful and friendly to other users if
- edit code that check the folders are in or not
- or just add information on the documentation that 'download=True' more than 1 can occur error in SBD dataset

![image](https://user-images.githubusercontent.com/49643709/106124365-5e509b00-619e-11eb-9a24-316731acb528.png)

![image](https://user-images.githubusercontent.com/49643709/106126827-3adb1f80-61a1-11eb-9a9d-12e5d7b6ec00.png)


I know it's trivial but hope someone else like me don't get suffered from :)


cc @pmeier"
Error Compiling from Source with GPU on MacOS,pytorch/vision,2021-01-28 00:19:46,2,module: ops#topic: build,3313,795557014,"## 🐛 Bug

Got the following error when build from source with GPU on MacOS
`torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu(1210): error: expression must have a constant value`

## To Reproduce

Steps to reproduce the behavior:

1. Use Mac GPU machine
1. Build the latest PyTorch from source
1. Follow the instruction to build torchvision from source

`MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install`


## Expected behavior

```bash
FAILED: /Users/hangzhang/git/vision/build/temp.macosx-10.7-x86_64-3.7/Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.o 
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/Users/hangzhang/git/vision/torchvision/csrc -I/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include -I/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include/TH -I/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include/THC -I/usr/local/cuda/include -I/Users/hangzhang/anaconda3/include/python3.7m -c -c /Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu -o /Users/hangzhang/git/vision/build/temp.macosx-10.7-x86_64-3.7/Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_clang""' '-DPYBIND11_STDLIB=""_libcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1002""' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -ccbin clang -std=c++14
/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include/c10/util/BFloat16.h(57): warning: calling a __host__ function from a __host__ __device__ function is not allowed

/Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu(1210): error: expression must have a constant value
/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(62): note: cannot call non-constexpr function ""__builtin_expect"" (declared implicitly)

/Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu(1209): error: more than one instance of overloaded function ""torch::Library::impl"" matches the argument list:
            function template ""torch::Library &torch::Library::impl(torch::detail::SelectiveStr<false>, Func &&) &""
            function template ""torch::Library &torch::Library::impl(torch::detail::SelectiveStr<true>, Func &&) &""
            argument types are: (torch::detail::SelectiveStr<<error-constant>>, c10::CompileTimeFunctionPointer<std::__1::remove_pointer_t<std::__1::remove_reference_t<at::Tensor (const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, bool)>>, vision::ops::<unnamed>::deform_conv2d_forward_kernel>)
            object type is: torch::Library

/Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu(1213): error: expression must have a constant value
/Users/hangzhang/anaconda3/lib/python3.7/site-packages/torch/include/ATen/core/op_registration/op_whitelist.h(62): note: cannot call non-constexpr function ""__builtin_expect"" (declared implicitly)

/Users/hangzhang/git/vision/torchvision/csrc/ops/cuda/deform_conv2d_kernel.cu(1212): error: more than one instance of overloaded function ""torch::Library::impl"" matches the argument list:
            function template ""torch::Library &torch::Library::impl(torch::detail::SelectiveStr<false>, Func &&) &""
            function template ""torch::Library &torch::Library::impl(torch::detail::SelectiveStr<true>, Func &&) &""
            argument types are: (torch::detail::SelectiveStr<<error-constant>>, c10::CompileTimeFunctionPointer<std::__1::remove_pointer_t<std::__1::remove_reference_t<std::__1::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor> (const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, bool)>>, vision::ops::<unnamed>::deform_conv2d_backward_kernel>)
            object type is: torch::Library
```


## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```
Collecting environment information...
PyTorch version: 1.8.0a0+eaf5ca0
Is debug build: False
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: macOS 10.13.6 (x86_64)
GCC version: Could not collect
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.105
GPU models and configuration: TITAN Xp
Nvidia driver version: 1.1.0
cuDNN version: Probably one of the following:
/usr/local/cuda/lib/libcudnn.7.dylib
/usr/local/cuda/lib/libcudnn.dylib
/usr/local/cuda/lib/libcudnn_static.a
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] autotorch==0.0.2
[pip3] numpy==1.16.2
[pip3] numpydoc==1.1.0
[pip3] torch==1.8.0a0+unknown
[pip3] torch-encoding==1.2.2b20210127
[conda] autotorch                 0.0.2                     dev_0    <develop>
[conda] blas                      1.0                         mkl  
[conda] mkl                       2019.4                      233  
[conda] mkl-include               2020.2                      260  
[conda] mkl-service               2.3.0            py37hfbe908c_0  
[conda] mkl_fft                   1.1.0            py37hc64f4ea_0  
[conda] mkl_random                1.1.1            py37h959d312_0  
[conda] numpy                     1.15.2                   pypi_0    pypi
[conda] numpy-base                1.16.2           py37h6575580_0  
[conda] numpydoc                  1.1.0                      py_0  
[conda] torch                     1.4.0a0+7404463          pypi_0    pypi
[conda] torch-encoding            1.2.2b20210127            dev_0    <develop>

"
"[docs] use `versionadded`, `versionchanged` and `deprecated` directive",pytorch/vision,2021-01-27 09:02:48,2,module: documentation,3305,794907487,"## 🚀 Feature
Use `.. versionadded::`, `.. versionchanged::` and `.. deprecated::` directive, so that user knows which features are added / changed / deprecated in which version and they can navigate the docs easily without changing the docs from version to version.

Ref: https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded
"
Affine Transform: why is translate a list[int] when the code suggests it could be floating point?,pytorch/vision,2021-01-26 07:14:08,3,question#module: transforms,3293,793995480,"https://github.com/pytorch/vision/blob/f16322b596c7dc9e9d67d3b40907694f29e16357/torchvision/transforms/functional.py#L956

cc @vfdev-5"
[feature request] transforms for object detection,pytorch/vision,2021-01-24 15:30:50,2,needs discussion#module: transforms#topic: object detection#new feature,3286,792821822,"## 🚀 Feature
I would like to start adding/supporting transforms (both functional and class) for object detection, I know I can take some of them from `references` folder. But, it would nice to have OOTB. Here are a few basic transforms I would like to add first - 
- `RandomHorizontalFlipWithBBox`
- `RandomVerticalFlipWithBBox`
- `LetterBox`

## Pitch
All of the above transforms will accept 2 arguments when they are called. This breaks the purpose of `Compose` and `nn.Sequential`, but currently aren't we writing custom `Compose` or `nn.Sequential`? So I think it's ok to start introducing necessary transforms taking 2 arguments for detection, segmentation, etc and let users write custom `Compose` or `nn.Sequential` the way they would to like to call the transforms.

## Additional context
Current code:
```py
class RandomHorizontalFlipWithBBox(nn.Module):
    def __init__(self, prob: float = 0.5):
        super().__init__()
        self.prob = prob

    def forward(self, img, target):
        if random.random() < self.prob:
            width = img.width
            xmin, xmax = target[..., 0], target[..., 2]
            diff = abs(xmax - xmin)
            target[..., 0] = width - xmin - diff
            target[..., 2] = width - xmax + diff
            return FT.hflip(img), target
        return img, target

    def __repr__(self):
        return self.__class__.__name__ + ""(p={})"".format(self.prob)
```
```py
class RandomVerticalFlipWithBBox(nn.Module):
    def __init__(self, prob: float = 0.5):
        super().__init__()
        self.prob = prob

    def forward(self, img, target):
        if random.random() < self.prob:
            height = img.height
            ymin, ymax = target[..., 1], target[..., 3]
            diff = abs(ymax - ymin)
            target[..., 1] = height - ymin - diff
            target[..., 3] = height - ymax + diff
            return FT.vflip(img), target
        return img, target

    def __repr__(self):
        return self.__class__.__name__ + ""(p={})"".format(self.prob)
```
```py
class LetterBox(nn.Module):
    """"""
    Make letter box transform to image and bounding box target.

    Args:
        size (int or tuple of int): the size of the transformed image.
    """"""

    def __init__(self, size: Union[int, Tuple[int]]):
        super().__init__()
        self.size = size
        if isinstance(size, int):
            self.size = (size, size)

    def forward(self, img: Image.Image, target: Union[np.ndarray, Tensor]):
        """"""
        Args:
            img (PIL Image): Image to be transformed.
            target (np.ndarray or Tensor): bounding box target to be transformed.

        Returns:
            tuple: (image, target)
        """"""
        old_width, old_height = img.size
        width, height = self.size

        ratio = min(width / old_width, height / old_height)
        new_width = int(old_width * ratio)
        new_height = int(old_height * ratio)
        img = T.functional.resize(img, (new_height, new_width))

        pad_x = (width - new_width) * 0.5
        pad_y = (height - new_height) * 0.5
        left, right = round(pad_x + 0.1), round(pad_x - 0.1)
        top, bottom = round(pad_y + 0.1), round(pad_y - 0.1)
        padding = (left, top, right, bottom)
        img = T.functional.pad(img, padding, 255 // 2)

        if isinstance(target, torch.Tensor):
            target[..., 0] = torch.round(ratio * target[..., 0]) + left
            target[..., 1] = torch.round(ratio * target[..., 1]) + top
            target[..., 2] = torch.round(ratio * target[..., 2]) + right
            target[..., 3] = torch.round(ratio * target[..., 3]) + bottom
        elif isinstance(target, np.ndarray):
            target[..., 0] = np.rint(ratio * target[..., 0]) + left
            target[..., 1] = np.rint(ratio * target[..., 1]) + top
            target[..., 2] = np.rint(ratio * target[..., 2]) + right
            target[..., 3] = np.rint(ratio * target[..., 3]) + bottom
        return img, target

    def __repr__(self):
        return self.__class__.__name__ + f""({self.size})""
```

Thank you!

cc @vfdev-5, @fmassa "
torchvision data downloader aborts if ipywidgets are not available,pytorch/vision,2021-01-24 10:13:33,10,module: datasets#dependency issue,3284,792762738,"## 🐛 Bug

I am testing some PyTorch code in [Deepnote](https://deepnote.com/project/0b12c588-a1de-4d66-bb0f-fa6b8f9b1b87) and the following call to torchvision aborts since the environment doesn't support ipywidgets

`test_data = torchvision.datasets.MNIST('../data', train=False, download=True, transform=local_transforms)`

## To Reproduce

Check the following notebook - https://deepnote.com/project/0b12c588-a1de-4d66-bb0f-fa6b8f9b1b87

```
ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-3-c62d245b04eb> in <module>
      5 local_transforms = torchvision.transforms.Compose([local_transform_1, local_transform_2])
      6 
----> 7 test_data = torchvision.datasets.MNIST('../data', train=False, download=True, transform=local_transforms)

/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/mnist.py in __init__(self, root, train, transform, target_transform, download)
     77 
     78         if download:
---> 79             self.download()
     80 
     81         if not self._check_exists():

/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/mnist.py in download(self)
    144         for url, md5 in self.resources:
    145             filename = url.rpartition('/')[2]
--> 146             download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5)
    147 
    148         # process and save as torch files

/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/utils.py in download_and_extract_archive(url, download_root, extract_root, filename, md5, remove_finished)
    254         filename = os.path.basename(url)
    255 
--> 256     download_url(url, download_root, filename, md5)
    257 
    258     archive = os.path.join(download_root, filename)

/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/utils.py in download_url(url, root, filename, md5)
     70             urllib.request.urlretrieve(
     71                 url, fpath,
---> 72                 reporthook=gen_bar_updater()
     73             )
     74         except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]

/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/datasets/utils.py in gen_bar_updater()
     13 
     14 def gen_bar_updater() -> Callable[[int, int, int], None]:
---> 15     pbar = tqdm(total=None)
     16 
     17     def bar_update(count, block_size, total_size):

/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs)
    246         unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1
    247         total = self.total * unit_scale if self.total else self.total
--> 248         self.container = self.status_printer(self.fp, total, self.desc, self.ncols)
    249         self.container.pbar = self
    250         if display_here:

/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)
    113         if IProgress is None:  # #187 #451 #558 #872
    114             raise ImportError(
--> 115                 ""IProgress not found. Please update jupyter and ipywidgets.""
    116                 "" See https://ipywidgets.readthedocs.io/en/stable""
    117                 ""/user_install.html"")

ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
```
## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

Maybe a check can be added that skips call to tqdm if it's not available?



cc @pmeier"
Multi-class focal loss,pytorch/vision,2021-01-13 22:46:15,20,new feature,3250,785511756,"## 🚀 Feature

Define an official multi-class focal loss function

## Motivation

Most object detectors handle more than 1 class, so a multi-class focal loss function would cover more use-cases than the existing [binary focal loss](https://github.com/pytorch/vision/blob/v0.8.0/torchvision/ops/focal_loss.py) released in v0.8.0

Additionally, there are many different implementations of multi-class focal loss floating around on the web (PyTorch forums, Github, etc). As the authors of the RetinaNet paper, Facebook AI Research should provide a definitive version to settle any existing debates

## Pitch

To the best of my understanding, this [version](https://discuss.pytorch.org/t/61289/2) by Thomas V. in the PyTorch forums seems correct. Please feel free to correct me if this is not the right approach

```python
import torch
import torch.nn.functional as F

batch_size = 8
num_classes = 5
logits = torch.randn(batch_size, num_classes)
targets = torch.randint(0, num_classes, (batch_size, ))

alpha = 0.25
gamma = 2
ce_loss = F.cross_entropy(logits, targets, reduction='none')
pt = torch.exp(-ce_loss)
focal_loss = alpha * (1 - pt) ** gamma * ce_loss
```

## Alternatives

Individual practitioners continue writing their own

## Additional context

The RetinaNet paper doesn't provide any equations to describe multi-class focal loss, so I think that's partially why people currently have varying implementations. In particular `alpha_t` is not defined, so I noticed Thomas and other users don't follow the same `alpha --> alpha_t` conversion used in torchvision's current binary focal loss implementation"
transforms.Pad returns error with tuple fill for tensors,pytorch/vision,2021-01-07 08:04:21,4,enhancement#module: transforms,3227,781117031,"## 🐛 Bug

Passing a 3-tuple, i.e. `Pad(5, fill=(117, 250, 87))` will return an error when when called on a tensor.

## To Reproduce

Issue seems to be here:
https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional_tensor.py#L408

tuple fills and Tensors don't seem to be supported; however the docs suggest that that they should be:
https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Pad

seems like support for this feature exists for PIL Images but not Tensors. Here is the check in `functional_pil.py`.
https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional_pil.py#L130



cc @vfdev-5"
Revisit ONNX-specific workarounds,pytorch/vision,2021-01-06 15:32:54,0,enhancement#module: models#module: onnx,3225,780639099,"## 🚀 Feature

As pointed out by @masahi in https://github.com/pytorch/vision/issues/3221#issuecomment-754924562, torchvision currently contain a few workarounds in order to support ONNX, many of which have been fixed upstream and could be cleaned up.

This issue is to track those potential improvements, so that we can have as little ONNX-specific codepaths as possible.

cc @neginraoof "
Current limitation on transforms,pytorch/vision,2021-01-06 13:20:46,2,enhancement#module: transforms,3224,780548165,"The torchvision transforms now have 2 backends (PIL and Tensor), here are some functional mismatch between them and some may-be-useful features that neither of them support. Details are listed in [transforms.py](https://github.com/pytorch/vision/blob/7b9d30eb7c4d92490d9ac038a140398e0a690db6/torchvision/transforms/transforms.py) and [functional.py](https://github.com/pytorch/vision/blob/7b9d30eb7c4d92490d9ac038a140398e0a690db6/torchvision/transforms/functional.py).

**Supported by PIL but not Tensor:**

1. Fill value for pad and random crop.
2. Tensor images do not support many modes due to lack of metadata (maybe not possible to address). For instance, the adjust_* functions and autoaugment related functions.
3. Tensors only support 3 interpolation modes (bilinear, linear, nearest).
4. Tensors only support transformations on RGB images

Crop with crop size larger than the original image. #3297  **Solved by #3333**

**Supported by Tensor but not PIL:**

1. Normalize (probably of no use for PIL images).
2. Erase.

**Supported by neither:**

1. adjust_gamma() and adjust_hue() do not support images with transparency.
2. Subpixel translations.  #3293 

**Not supported by torchscript (mostly not possible given the current jit support):**

1. single value inputs in Pad(fill), RandomCrop(padding), Resize(size), RandomResizedCrop(size).
2. PIL and Tensor conversions.
3. Compose, RandomOrder, RandomChoice.
4. Lambda.

It is just a draft, let me know if I forget anything. cc @vfdev-5 @datumbox "
torchvision.transforms.functional.adjust_hue does not respect alpha channel,pytorch/vision,2021-01-04 21:32:27,5,enhancement#help wanted#module: transforms#module: documentation,3219,778381621,"## 🐛 Bug

when working with RGBA images, the adjust_hue function does not seem to respect the alpha channel and imposes a max_value of 255 after the transform. Taking a quick look at the source code, it seems that the image is immediately converted to HSV without retaining the alpha channel. It should be a quick fix to retain the alpha channel and include it when merging back into RGBA. 

## To Reproduce

Steps to reproduce the behavior:

```python
img = Image.open('xyz.png')
img_ = adjust_hue(img, 0.1)
print(np.array(img.split()[-1]).mean())
print(np.array(img_.split()[-1]).mean())
```


cc @vfdev-5"
v0.8.X Build failure due to binding type error,pytorch/vision,2021-01-01 19:32:20,1,topic: build,3216,777339603,"## 🐛 Bug

Build fails due to a binding type error on v0.8.X.

## To Reproduce

Steps to reproduce the behavior:

1. Clone the repo.
    ```
    $ git clone git@github.com:pytorch/vision.git
    ```

2. Checkout any 0.8.X version.

    ```
    $ git checkout v0.8.2
    ```

3. Build and install from source (my setup requires a home directory installation).

    ```
   $ python setup.py install --home ""${HOME}""
    ```

Build failure error message:

```
[REDACTED_INSTALL_DIR]/torch/include/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:242:170: error: cannot bind non-const lvalue reference of type ‘at::Tensor&’ to an rvalue of type ‘at::Tensor’
  242 | move_cv_t<std::remove_reference_t<guts::typelist::element_t<ivalue_arg_indices, IValueArgTypes>>>, AllowDeprecatedTypes>::call(
      | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^

  243 | alue_arg_indices, num_ivalue_args))
      | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  244 |
      |

error: command 'gcc' failed with exit status 1
```

## Expected behavior

Build success

## Environment

PyTorch version: 1.6.0a0+b31f58d
Is debug build: False
CUDA used to build PyTorch: False
ROCM used to build PyTorch: N/A

OS: Manjaro ARM Linux (aarch64)
GCC version: (GCC) 10.2.0
Clang version: N/A
CMake version: version 3.19.1

Python version: 3.8 (64-bit runtime)
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] torch==1.6.0a0+b31f58d
[conda] N/A

## Additional context

- At first, I tried to install with the master branch, but I faced #3210. Thanks to @qyb, I was able to work around this by checking out a specific version (`git checkout v0.8.2`).
- I am able to successfully build and install with v0.7.0 (`git checkout v0.7.0; python setup.py install --home ""${HOME}""`).
"
More ResNet backbones for Faster R-CNN and Mask R-CNN,pytorch/vision,2020-12-28 04:11:16,7,enhancement#module: models,3213,775179249,"## 🚀 Feature
More ResNet backbones (e.g., resnet18, resnet34, and resnet101) for Object Detection and Instance Segmentation. 

## Motivation

Detectron2 provides a curated list of pre-trained detectors with having different backbones. However, I believe that the implementation of object detectors in `torchvision` is intriguingly easy to play around and plug into users' systems to solve their problems. Still, there is a limited choice of backbone for Faster R-CNN and Mask R-CNN. 

Is there any reason behind not supporting pre-trained models with such backbones or any future plan?

Many thanks for making educationally understandable and clean codebases, and I'm really looking forward to your reply.
"
SyncBatchNorm in detection train script,pytorch/vision,2020-12-27 19:44:57,0,module: reference scripts#topic: object detection,3211,775096854,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
I noticed that in the classification and segmentation train scripts SyncBatchNorm option is available but not in the detection train script. is it intentional?
## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

## Pitch

<!-- A clear and concise description of what you want to happen. -->
adding --sync-bn option to detection train script 
## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
"
Add batch-wise random erasing support?,pytorch/vision,2020-12-23 02:40:46,14,module: transforms#new feature,3206,773584391,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

## Motivation

The random erasing supported in torchvision now is implemented as image-wise. I need to use it after data loader collecting batches. But the current design cannot be used for a large batch because it is too slow to preprocess on CPU.

I use a for loop to process per-image in a batch, as shown below, but it is too slow:
```python
for batch_of_imgs, labels in my_data_loader:
    # I need apply random erasing here, not in transforms of data loader
    # This for-loop is too slow!!!
    for i, img in enumerate(batch_of_imgs):
        batch_of_imgs[i] = RandomErasing(img)
    # train model as usual
    preds = model(batch_of_imgs, labels)
```

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

## Pitch

If a batch-wise random erasing operation that could accelerate such preprocessing is available, it would be very helpful.

<!-- A clear and concise description of what you want to happen. -->

## Alternatives

I've tried 2 alternatives:
1. Use multi-threading. This does not help due to python GIL.
2. Use multi-processing. This saves around 50% preprocessing time per-iteration at the beginning of training but gradually becomes as slow as single-processing one. It provides around <10% improvements from the whole training procedure but requires more resources for processing management, and tricky hacks to modify training code.

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
The `erasing` procedure should be image-wise, not batch-wise. In other words, the erased region cannot be shared among images of same batch.

cc @SsnL @VitalyFedyunin @ejguan @fmassa @vfdev-5"
torchvision.io.read_video mirrors videos of BDD100k and sometimes rotates them 180 degrees,pytorch/vision,2020-12-22 09:52:46,5,module: io#module: video,3201,772808076,"## 🐛 Bug

The function `torchvision.io.read_video(path, start_pt, end_pt, pts_unit='sec')` flips videos horizontally and for around half of the videos, also rotates them 180 degrees. The videos in question are from the dataset BDD100k. I believe they have been collected with an iPhone, and are in .mov format.

## To Reproduce

Steps to reproduce the behavior:

```
import torch
import torchvision as tv
import matplotlib.pyplot as plt

vid1, _, _ = tv.io.read_video('data/bdd100k/bdd100k/videos/train/011f8481-b14725fa.mov', 9.5, 10.5, pts_unit='sec')
vid2, _, _ = tv.io.read_video('data/bdd100k/bdd100k/videos/train/019c8305-3a3be0d6.mov', 9.5, 10.5, pts_unit='sec')

# vid1.size() -> torch.Size([31, 1280, 720, 3])

plt.imshow(vid1[0].permute(1, 0, 2))
plt.show() # See first figure
plt.imshow(vid2[0].permute(1, 0, 2))
plt.show() # See second figure
```

Figures:
<img width=""638"" alt=""image"" src=""https://user-images.githubusercontent.com/24507535/102872249-c42f6180-443f-11eb-89e4-e47c80d332d2.png"">
<img width=""638"" alt=""image"" src=""https://user-images.githubusercontent.com/24507535/102872304-d14c5080-443f-11eb-8daf-deb0c55ee120.png"">

Videos when opened with QuickTime:
<img width=""609"" alt=""image"" src=""https://user-images.githubusercontent.com/24507535/102872560-2720f880-4440-11eb-8342-f4b3da2b599a.png"">
<img width=""831"" alt=""image"" src=""https://user-images.githubusercontent.com/24507535/102872619-415ad680-4440-11eb-9104-f361e3e2fe06.png"">

ffprobe provides:
```
> ffprobe data/bdd100k/bdd100k/videos/train/011f8481-b14725fa.mov
ffprobe version 4.3.1 Copyright (c) 2007-2020 the FFmpeg developers
  built with Apple clang version 12.0.0 (clang-1200.0.32.27)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/4.3.1_4 --enable-shared --enable-pthreads --enable-version3 --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librtmp --enable-libspeex --enable-libsoxr --enable-videotoolbox --disable-libjack --disable-indev=jack
  libavutil      56. 51.100 / 56. 51.100
  libavcodec     58. 91.100 / 58. 91.100
  libavformat    58. 45.100 / 58. 45.100
  libavdevice    58. 10.100 / 58. 10.100
  libavfilter     7. 85.100 /  7. 85.100
  libavresample   4.  0.  0 /  4.  0.  0
  libswscale      5.  7.100 /  5.  7.100
  libswresample   3.  7.100 /  3.  7.100
  libpostproc    55.  7.100 / 55.  7.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'data/bdd100k/bdd100k/videos/train/011f8481-b14725fa.mov':
  Metadata:
    major_brand     : qt
    minor_version   : 512
    compatible_brands: qt
    encoder         : Lavf57.71.100
  Duration: 00:00:40.09, start: 0.000000, bitrate: 3949 kb/s
    Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, smpte170m/bt709/bt709), 720x1280, 3946 kb/s, 30.06 fps, 60 tbr, 19200 tbn, 38400 tbc (default)
    Metadata:
      rotate          : 270
      handler_name    : VideoHandler
      encoder         : H.264
    Side data:
      displaymatrix: rotation of 90.00 degrees

> ffprobe data/bdd100k/bdd100k/videos/train/019c8305-3a3be0d6.mov
ffprobe version 4.3.1 Copyright (c) 2007-2020 the FFmpeg developers
  built with Apple clang version 12.0.0 (clang-1200.0.32.27)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/4.3.1_4 --enable-shared --enable-pthreads --enable-version3 --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librtmp --enable-libspeex --enable-libsoxr --enable-videotoolbox --disable-libjack --disable-indev=jack
  libavutil      56. 51.100 / 56. 51.100
  libavcodec     58. 91.100 / 58. 91.100
  libavformat    58. 45.100 / 58. 45.100
  libavdevice    58. 10.100 / 58. 10.100
  libavfilter     7. 85.100 /  7. 85.100
  libavresample   4.  0.  0 /  4.  0.  0
  libswscale      5.  7.100 /  5.  7.100
  libswresample   3.  7.100 /  3.  7.100
  libpostproc    55.  7.100 / 55.  7.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'data/bdd100k/bdd100k/videos/train/019c8305-3a3be0d6.mov':
  Metadata:
    major_brand     : qt
    minor_version   : 512
    compatible_brands: qt
    encoder         : Lavf57.71.100
  Duration: 00:00:40.16, start: 0.000000, bitrate: 4085 kb/s
    Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, smpte170m/bt709/bt709), 720x1280, 4082 kb/s, 30.03 fps, 29.97 tbr, 19200 tbn, 38400 tbc (default)
    Metadata:
      rotate          : 90
      handler_name    : VideoHandler
      encoder         : H.264
    Side data:
      displaymatrix: rotation of -90.00 degrees
```

## Expected behavior

I expect `tv.io.read_video` to provide images similar to what would be displayed in QuickTime.

## Environment

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): 1.7.1 / 0.8.2
 - OS (e.g., Linux): macOS Big Sur
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): Conda
 - Build command you used (if compiling from source): N/A
 - Python version: 3.8.5
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information:

Best regards,
Joakim


cc @bjuncek"
torch.cuda.amp issue with torchvision densenet121 when memory_efficient=True,pytorch/vision,2020-12-04 19:13:59,2,module: models,3122,757339331,"## 🐛 Bug

I have been using a torchvision densenet121 with PT1.4+ APEX/AMP for mixed precision training. Today, I tried to migrate my code to PT1.7 and to native mixed precision (amp). 

In short, when using a densenet121 with memory_efficient=True, I get an amp related error message: 'Input type (torch.cuda.HalfTensor) and weight type (torch.cuda.FloatTensor) should be the same'

When using the densenet121 with memory_efficient=False, everything works fine and in mixed precision mode.

## To Reproduce
```
import torchvision.models as models
(A) model=models.densenet121(pretrained=None, memory_efficient=True)
(B) model=models.densenet121(pretrained=None, memory_efficient=False)

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()

        with autocast():
            output = model(input)
            loss = loss_fn(output, target)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
```

How could the densenet(memory_efficient=True) be modified so that it becomes compatible with mixed precision training?

## Environment
PyTorch version: 1.7.0+cu101
Is debug build: True
CUDA used to build PyTorch: 10.1
OS: SUSE Linux Enterprise Server 12 SP5 (x86_64)
GCC version: (SUSE Linux) 4.8.5
Python version: 3.7 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.1.105
GPU models and configuration:
GPU 0: Tesla V100-SXM2-32GB
Nvidia driver version: 418.39
[pip] torch==1.7.0+cu101
[pip] torchaudio==0.7.0
[pip] torchtuples==0.2.0
[pip] torchvision==0.8.0
"
Why get_num_threads() and set_num_threads(1) in engine.py? Is it essential?,pytorch/vision,2020-12-04 09:13:35,3,module: reference scripts#topic: object detection,3115,756938667,"Hi there,

I'm wondering what's the function of these code? Can i remove them?

Thanks!

https://github.com/pytorch/vision/blob/dab475720f116c42fc80e437b25496dfc94d2a8a/references/detection/engine.py#L72:L74

https://github.com/pytorch/vision/blob/dab475720f116c42fc80e437b25496dfc94d2a8a/references/detection/engine.py#L108


"
about retrain shufflenetv2 question,pytorch/vision,2020-12-02 02:35:51,1,question,3090,754882195,"First of all, thanks for your perfect projects.

## Environments
pyhton: 3.7
pytorch: 1.7+cpu
torchvison: 0.8.1+cpu
system-os: ubuntu18.04

## Hyperparameters
lr: 0.001
momentum: 0.9
weights_decay: 0.0001
batch_size: 16

## Question introduction
Recently, I was learning the source code your provided in torchvision about shufflenetv2.
But when I was fine-training the network(only training fc layer), I had a problem that network convergence is very slow. like this:
```
[epoch 0] accuracy: 0.246
[epoch 1] accuracy: 0.253
[epoch 2] accuracy: 0.28
[epoch 3] accuracy: 0.305
[epoch 4] accuracy: 0.338
[epoch 5] accuracy: 0.353
```
I have read this document [https://pytorch.org/docs/stable/torchvision/models.html#classification](https://pytorch.org/docs/stable/torchvision/models.html#classification)
According to this document, I downloaded the weights [https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth](https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth), and use same preprocessing method.
```python
    data_transform = {
        ""train"": transforms.Compose([transforms.RandomResizedCrop(224),
                                     transforms.RandomHorizontalFlip(),
                                     transforms.ToTensor(),
                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),
        ""val"": transforms.Compose([transforms.Resize(256),
                                   transforms.CenterCrop(224),
                                   transforms.ToTensor(),
                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}
```
But with conditions unchanged, I just replace the model with resnet34 your provided in torchvision, and I can get great results. like this:
```
[epoch 0] accuracy: 0.968
```

Strangely, When fine-training shfflenetv2 if I change the learning rate from 0.001 to 0.1, I can get the following results:
```
[epoch 0] accuracy: 0.85
[epoch 1] accuracy: 0.848
.....
[epoch 29] accuracy: 0.899
```
Does fine-training shufflenet network need such a large learning rate?

I guess the preprocessing algorithm is not like that. Because if I use the mobilenetv2 network, I can get better results under the same conditions. Could you help me find out what's wrong? Thank you very much.


## Code
[https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/pytorch_classification/Test7_shufflenet/train.py](https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/blob/master/pytorch_classification/Test7_shufflenet/train.py)
"
Windows C++ library build issues,pytorch/vision,2020-12-02 00:18:05,0,windows#module: c++ frontend,3086,754824652,"## 🐛 Bug

Thanks for your work in putting up this great software. PyTorch and TorchVision are two amazing tools.

I encountered two different issues when trying to build the C++ library on Windows:

1. The debug build tries to link to the debug python library which most people do not have. 
2. The operators registration code was not included in consuming projects.

## To Reproduce

Steps to reproduce the behavior:

1. Download the latest LibTorch builds
2. Build and install the vision C++ library with CMake by giving it the proper CMake arguments, e.g. `Torch_DIR`, etc.
3. Build a small exectuable that lists the registered ops like this:

Credits to @zsef123 for the code below
```cpp
#include <iostream>

#include <torch/torch.h>
#include ""vision.h""

int main(int argc, char* argv[]) {
    auto& ops = torch::jit::getAllOperators();
    std::cout << ""torch jit operators\n"";
    for (auto& op: ops) {
        auto& name = op->schema().name();
        if (name.find(""torchvision"") != std::string::npos)
            std::cout << ""op : "" << op->schema().name() << ""\n"";
    }
    std::cout << ""\n"";
    return 0;
}
```

## Expected behavior

A debug build should be possible since it is almost always required on Windows.
Operators should register automatically in consuming binaries.

## Environment

 - PyTorch / torchvision Version (e.g., 1.7.0 / master):
 - OS: Windows 10:
 - Installed LibTorch from pytorch.org
 - cmake --build . 
 - Python version: 3.8
 - CUDA/cuDNN version: 10.2

## Additional context

- The first issue is easily fixed by ignoring the python debug library by adding the `/NODEFAULTLIB:python38_d.lib` linker flag.
- The second issue is in fact multiple issues. First, if I am not mistaken, since the `vision.h` file is never included in the `vision.cpp` the `cuda_version` definition is considered as a different symbol from the declaration in `vision.h`. An easy fix, is to add `VISION_API` in front the definition. However, this is more elegantly fixed by including the `vision.h` header and changing it so it builds correctly. Second, even with this symbol included in the consuming library, the linker strips the operators registration symbols in a Release build. This is fixed by using the `/OPT:NOREF` linker flags.

I will submit a PR that fixes all these above issues.



cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @smartcat2010 @mszhanyi"
Getting an error when modifying the faster_rcnn model to add inception_v3 backbone model,pytorch/vision,2020-12-01 20:19:44,0,question,3083,754775456,"I was following this tutorial  [Modifying the model to add a different backbone](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#modifying-the-model-to-add-a-different-backbone). When I replace the mobilenet_v2 model with inception_v3, the code does not work and  gives the following error:
```
  File ""/home/gpu-user/projects/building-outline-detection/src/models/faster_rcnn/vision/engine.py"", line 46, in train_one_epoch
    loss_dict = model(images, targets)
  File ""/home/gpu-user/miniconda3/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/gpu-user/miniconda3/envs/faster_rcnn/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py"", line 99, in forward
    proposals, proposal_losses = self.rpn(images, features, targets)
  File ""/home/gpu-user/miniconda3/envs/faster_rcnn/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/gpu-user/miniconda3/envs/faster_rcnn/lib/python3.8/site-packages/torchvision/models/detection/rpn.py"", line 330, in forward
    features = list(features.values())
AttributeError: 'InceptionOutputs' object has no attribute 'values'
```
I am using the following environment:

* Ubuntu 18.04.4 LTS
* CUDA Version: 10.2
* Python: 3.8.6
* Pytorch: 1.7.0

It will be great if someone can help me in resolving this issue.
Thanks"
test_write_video_with_audio is disabled on Windows,pytorch/vision,2020-12-01 17:12:54,0,windows#module: tests#module: io,3078,754575550,"With https://github.com/pytorch/vision/pull/2304, I had to disable Windows tests `test_write_video_with_audio`.
This is a workaround in order to move forward with fbcode sync.

We should re-enable those tests and fix what was wrong with Windows testing.

cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @smartcat2010 @mszhanyi"
How to solve this error? RuntimeError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend,pytorch/vision,2020-11-30 08:17:04,25,needs reproduction#module: ops,3058,753260921,"## ❓ Questions and Help
### Please note that this issue tracker is not a help form and this issue will be closed.

I'm beginner of ML and trying to use some solution based on pytorch (called detectron2)
When the solution inferred the image, I always got the below error.

RuntimeError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. 'torchvision::nms' is only available for these backends: [CPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, Tracer, Autocast, Batched, VmapMode].

Actually, I didn't get this error and couldn't search anything about this on google.
Is there anybody who knows the way to handle this?

Info:
I installed the CUDA v11.1 from https://developer.nvidia.com/cuda-downloads 
torch version: 1.7.0
torchvision version: 0.8.0"
torchvision.roi_align does not support TPU,pytorch/vision,2020-11-28 12:16:32,1,question,3056,752665281,"Hello. 
We are using TPU in GCP.

We are currently modifying the code to allow the TPU to return to Detectron2.
However, there is an error that roi_align in Torchvision is not supported by TPU.
Please check the bottom. Can you solve it for me?

`File ""/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/torchvision/ops/roi_align.py"", line 51, in roi_align
    return torch.ops.torchvision.roi_align(input, rois, spatial_scale, output_size[0], output_size[1], sampling_ratio, aligned)
RuntimeError: Could not run 'torchvision::roi_align' with arguments from the 'XLA' backend. 'torchvision::roi_align' is only available for these backends: [CPU, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].`"
Multi GPU support for vision model training in C++ with libtorch,pytorch/vision,2020-11-27 09:41:50,1,module: c++ frontend,3052,752117513,"## 🚀 Feature

Multi GPU support for vision model training in C++ with libtorch.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
More and more systems rely only on C++ for deep learning, including ours. But at the moment the C++ version of vision models are quite unpractical to use, and it is not possible to train them on multiple GPU when using large amount of data.

## Pitch

<!-- A clear and concise description of what you want to happen. -->
Make C++ models support multi GPU. This can be done by deriving the pytorch ICloneable class, but this might not be sufficient for optimal GPU training, as pointed out in [this](https://github.com/pytorch/pytorch/issues/40581) issue. The best option might be to implement the `clone` method so that `data_parallel` ([here](https://github.com/pytorch/pytorch/blob/7df84452423f44ebe1db40a2e3463066bf954f95/torch/csrc/api/include/torch/nn/parallel/data_parallel.h#L246)) can handle the models.
"
Faster/Mask RCNN (fasterrcnn_mobilenet_v2_fpn) model export to ONNX. Warnings.,pytorch/vision,2020-11-24 11:18:19,6,topic: object detection#module: onnx,3044,749616213,"Hello, 

I have a lot of 'UserWarnings' during exporting model to ONNX. Can someone clarify meanings of this warnings and help to eliminate then. I am creating model from [this finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and have no problem during training/torch evaluation. 
However encounter one 'RuntimeWarning' and about 7 'UserWarning' while exporting trained model to ONNX. There are also several warnings during evaluation ONNX model on onnxruntime.

# To Reproduce

``` python
import torch
import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

def create_faster_mobilenet_v2():
    backbone = torchvision.models.mobilenet_v2(pretrained=True).features
    backbone.out_channels = 1280
    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
                                       aspect_ratios=((0.5, 1.0, 2.0),))
    num_classes = 47
    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)
    model = FasterRCNN(backbone,
                       num_classes=num_classes,
                       rpn_anchor_generator=anchor_generator,
                       box_roi_pool=roi_pooler)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

# export to ONNX
batch_size = 1
# load model
model = create_faster_mobilenet_v2()

# Dummy input to the model
x = torch.randn(batch_size, 3, 800, 800)

# set the model to inference mode
model.eval()

# running dummy image
torch_out = model(x)

print('Model outputs: ', torch_out[0]['boxes'].shape, torch_out[0]['labels'].shape, torch_out[0]['scores'].shape)
print('Model output boxes: ', torch_out[0]['boxes'])
print('Model output labels: ', torch_out[0]['labels'])
print('Model output scores: ', torch_out[0]['scores'])

# Export the model
torch.onnx.export(model,  # model being run
                  x,  # model input (or a tuple for multiple inputs)
                  ""fasterRCNN-231120-694-46C-batch1.onnx"",  # model path + name
                  # export_params=True,  # store the trained parameter weights inside the model file
                  opset_version=11,  # the ONNX version to export the model to
                  # do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names=['input'],  # the model's input names
                  output_names=['output'],  # the model's output names
                  dynamic_axes={'input': {0: 'batch_size'},  # variable length axes
                                            'output': {0: 'batch_size'}
                                }
                  )
```
# Output during export:
``` terminal
Model outputs:  torch.Size([3, 4]) torch.Size([3]) torch.Size([3])

Model output boxes:  tensor([[2.6542e+00, 2.4133e-01, 1.1089e+02, 8.9261e+01],
        [2.1360e+00, 6.8483e+02, 1.0292e+02, 7.9821e+02],
        [9.6579e-01, 7.3022e+02, 1.3740e+02, 7.9860e+02]],
       grad_fn=<StackBackward>)

Model output labels:  tensor([3, 1, 1])

Model output scores:  tensor([0.0778, 0.0638, 0.0519], grad_fn=<IndexBackward>)

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torch\nn\functional.py:3123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requ
ires_grad_(True), rather than torch.tensor(sourceTensor).
  dtype=torch.float32)).float())) for i in range(dim)]

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torchvision\models\detection\anchor_utils.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.
clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torch\tensor.py:593: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iter
ations executed (and might lead to errors or silently give incorrect results).
  'incorrect results).', category=RuntimeWarning)

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torchvision\ops\boxes.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().req
uires_grad_(True), rather than torch.tensor(sourceTensor).
  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torchvision\ops\boxes.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().req
uires_grad_(True), rather than torch.tensor(sourceTensor).

  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))
D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torchvision\models\detection\transform.py:271: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clo
ne().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  for s, s_orig in zip(new_size, original_size)

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torch\onnx\symbolic_opset9.py:2378: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators,
 including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.
  ""If indices include negative values, the exported graph will produce incorrect results."")

D:\Projects\MachineLearning\fasterRCNN\train\venv\lib\site-packages\torch\onnx\symbolic_opset9.py:588: UserWarning: This model contains a squeeze operation on dimension 1 on an input with unknown shape. Note that if the size of dimensi
on 1 of the input is not 1, the ONNX model will return an error. Opset version 11 supports squeezing on non-singleton dimensions, it is recommended to export this model using opset version 11 or higher.
  ""version 11 or higher."")
```

# Environment

1. PyTorch Version : 1.7.0
2. Torchvision: 0.8.1
2. OS : Wndows 10
3. How you installed PyTorch (conda, pip, source): pip install torch===1.7.0 torchvision===0.8.1 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
4. Python version: 3.6.5
5. CUDA/cuDNN version: 10.2 / 11.0 (I have another machine - result is the same)
6. onnx: 1.7.0
7. onnxruntime: 1.5.2

With respect.
Bulat.

cc @neginraoof"
I am not able to obtain results with custom backbone,pytorch/vision,2020-11-20 15:45:08,1,question#module: documentation,3040,747590858,"_I am following the tutorial about FasterRCNN and I would like to test my network as backbone of the net:

UCapsNet return 512 features maps
I am training on VocPascal 2007_


FRCN_model = FasterRCNN(backbone_model.Ucapsnet, 21, rpn_anchor_generator=backbone_model.anchor_generator, box_roi_pool=backbone_model.roi_pooler)
FRCN_model = FRCN_model.to(device)

params = [p for p in FRCN_model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.02, momentum=0.9, weight_decay=1e-4)

pbar = tqdm(range(n_epochs))
for epoch in pbar:
    train_one_epoch(FRCN_model, optimizer, dataloaders['train'], device, epoch, print_freq=10)
    evaluate(FRCN_model, dataloaders['val'], device=device)


**I got**:
Averaged stats: model_time: 1605886336.0000 (1605886304.8101)  evaluator_time: 0.0275 (0.0285)
Accumulating evaluation results...
DONE (t=0.06s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000


In training, the loss is dropping slowly to 1.15 but in evaluation, i do not get anything. 

Please help me understand

cc @fmassa "
randomroate by some change,pytorch/vision,2020-11-19 14:22:27,0,question#module: transforms,3030,746628357,"```
def mapper(dataset_dict):
    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
    image = utils.read_image(dataset_dict[""file_name""], format=""BGR"")    
    transform_list = [
                  
                     T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice')
                     ,T.RandomRotation([10,15])
                
                      ]
    image, transforms = T.apply_transform_gens(transform_list, image)
    dataset_dict[""image""] = torch.as_tensor(image.transpose(2, 0, 1).astype(""float32""))

    
    #print('image_shape->',image.shape,image.shape[:2])

    annos = [
        utils.transform_instance_annotations(obj, transforms, image.shape[:2])
        for obj in dataset_dict.pop(""annotations"")
        if obj.get(""iscrowd"", 0) == 0
    ]

    instances = utils.annotations_to_instances(annos, image.shape[:2])
    dataset_dict[""instances""] = instances
    #dataset_dict[""instances""] = utils.filter_empty_instances(instances)
    return dataset_dict
```
this is my mapper for augmentation.
is T.RandomRotation([10,15]) happen every image? or by some change. 
if it apply to every images. how should I apply it by only some percentage?


cc @vfdev-5"
MaskRCNN Training on Images with no Annotations,pytorch/vision,2020-11-18 15:23:52,7,question#awaiting response#topic: object detection,3022,745755565,"Hi all,
I am working on a little MaskRCNN training program and ran into an issue. I know it is common practice to remove any images from the dataset that lack annotations upon initializing the dataset which I am doing. However, I am running a series of transforms using albumentations on my image and my mask. One of these transforms is a random crop and sometimes the resulting mask image no longer contains any instances. I was trying to find a way to pass in an empty tensor of some kind without much success. Would it be common practice just to remove it from the batch, and if so what happens if you had a batch size of 1 or an image that only had one annotation and the chances the random crop came across it are really low. I was able to create an empty tensor and pass it in but then received this error.

`RuntimeError: cannot reshape tensor of 0 elements into shape [0, -1] because the unspecified dimension size -1 can be any value and is ambiguous`

This is because my box tensor had a shape of 0, 4 which is what I want since there are no instances. I read some of the other issue reports and they talked about creating a background class and just making a small bounding box and having an empty segmentation mask but this seems a little hacky and I was wondering if there would be a better solution for my specific use case.
"
FFmpeg-based rescaling and frame rate,pytorch/vision,2020-11-17 12:51:35,4,needs discussion#module: io#module: video,3016,744709865,"## 🚀 Feature
Add support for (basic) FFmpeg filters for faster video pre-processing. In particular, rescaling and changing the frame rate would be useful when feeding in-the-wild videos through a trained model.

## Motivation

I am working on a video loader to feed video frames to a model trained on the Kinetics 400 dataset and obtain predictions. The model is trained at a fixed resolution, on videos with a frame rate of 15fps. To support making predictions on videos from various sources, I at least need to resample them at the correct resolution and frame rate.

The current public API only supports decoding of video frames and trimming, but not any other pre-processing, so I need to do any such pre-processing in Python/PyTorch. Such an approach is visibly slower when compared to an implementation based on `ffmpeg-python` – a wrapper around the command line `ffmpeg`. For some stats, see Additional context.

## Pitch

I would like to start a conversation on how best to bring such functionality to Torchvision. I imagine changing the resolution/fps is a common requirement for making predictions on videos, so I can see it as a useful feature of video I/O. Looking at the C++ code, there is already some support for requesting video frames of a certain resolution [[1]](https://github.com/pytorch/vision/blob/74de51d6d478e289135d9274e6af550a9bfba137/torchvision/csrc/cpu/decoder/defs.h#L47)[[2]](https://github.com/pytorch/vision/blob/74de51d6d478e289135d9274e6af550a9bfba137/torchvision/csrc/cpu/video_reader/VideoReader.cpp#L464), but this functionality is only exposed in `torch.ops.video_reader.read_video_from_file`, not the public API. I can’t find anything similar for requesting a certain frame rate.

Is this something that you would want to add to `torchvision.io.read_video`? What about to `torchvision.io.VideoReader`? More generally, is there a plan to add support for all FFmpeg filters in the future? What would that interface look like?

## Additional context

I’ve done some initial comparisons between `torchvision.io.VideoReader` + changing frame rate in Python + `torch` rescaling on batches of 16 frames versus a `ffmpeg-python` pipeline with `scale` and `fps` filters on a 854x480@30fps MP4 input video of ~261s. I’ve included the results below.

#### Decoding the first <N> seconds of a clip (output fps=15, output size=input size):
![clip-length](https://user-images.githubusercontent.com/971313/98943077-dcc66680-24e6-11eb-8738-d47a9bf69dd4.png)


#### Decoding 1s of video for given start time (output fps=15, output size=input size):
![start-time](https://user-images.githubusercontent.com/971313/98943414-58c0ae80-24e7-11eb-92a6-3458f18b5ad5.png)


#### Changing the framerate for the first 1s of video (output size=input size):
![framerate-1s](https://user-images.githubusercontent.com/971313/98943495-81e13f00-24e7-11eb-87c7-1e3cfaf44709.png)


#### Changing the framerate for the first 5s of video (output size=input size):
![framerate-5s](https://user-images.githubusercontent.com/971313/98943507-886fb680-24e7-11eb-84f7-e9ecfb537d36.png)


#### Rescaling the first 1s of video (output fps=15):
![scale-1s](https://user-images.githubusercontent.com/971313/98943605-accb9300-24e7-11eb-8522-abae167f9045.png)


#### Rescaling the first 1s of video with `bilinear-fast` FFMpeg algorithm (output fps=15):
![scale-1s-fast](https://user-images.githubusercontent.com/971313/98944582-29ab3c80-24e9-11eb-8395-a1cd670c4238.png)


#### Rescaling the first 5s of video (output fps=15):
![scale-5s](https://user-images.githubusercontent.com/971313/98943649-b7862800-24e7-11eb-9e3c-fd6dcad7f5f1.png)




cc @bjuncek"
Investigate why PIL and LibJPEG pixel values don't match for CMYK images,pytorch/vision,2020-11-16 17:38:05,0,module: io,3012,744030694,"Loading a CMYK image using libJpeg seems to yield different results than loading it with PIL. 

More specifically the pixel values of `img_ljpeg == 255 - img_pil`:
https://github.com/pytorch/vision/blob/ada56a2442282752ccb83c935e10cb4fbbc0c8d5/test/test_image.py#L55-L69 

We should investigate if that discrepancy is a bug on the way we use libjpeg."
"Error building torchvision from source. CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.",pytorch/vision,2020-11-11 20:38:29,2,topic: build,2991,741054574,"Hi, dear support team. I've faced the issue during installation process of torchvision from source on my Jetson Nano. 
I followed instructions from official Nvidia site and succeeded in installation Torch v1.7.0. I build it from source cause i have python3.8. 

The log file with the error could be found [here](https://github.com/Dyst/public/blob/main/torchvision_issue)

Here is the output of your environment data collection script:

PyTorch version: 1.7.0
Is debug build: True
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (aarch64)
GCC version: (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Probably one of the following:
/usr/lib/aarch64-linux-gnu/libcudnn.so.8.0.0
/usr/lib/aarch64-linux-gnu/libcudnn_adv_infer.so.8.0.0
/usr/lib/aarch64-linux-gnu/libcudnn_adv_train.so.8.0.0
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_infer.so.8.0.0
/usr/lib/aarch64-linux-gnu/libcudnn_cnn_train.so.8.0.0
/usr/lib/aarch64-linux-gnu/libcudnn_ops_infer.so.8.0.0
/usr/lib/aarch64-linux-gnu/libcudnn_ops_train.so.8.0.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.4
[pip3] torch==1.7.0
[conda] numpy                   1.19.4                  <pip>
[conda] torch                     1.7.0                     <pip>

Please, share the know! 

"
About converting PIL Image to PyTorch Tensor,pytorch/vision,2020-11-11 13:36:27,2,module: transforms,2989,740771996,"## About converting PIL Image to PyTorch Tensor 

I use PIL open an image:

```python
pic = Image.open(...).convert('RGB')
```
Then I want to convert it to tensor, I have read `torchvision.transforms.functional`, the function `to_tensor` use the following way：

```python
img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))
```

Why not use the following method directly:

```python
img = torch.from_numpy(np.array(pic))
```

I want to know what is the difference, is it because of the difference in efficiency？

The following is the code snippet for `to_tensor` to handle PIL Image

```python
# handle PIL Image
    if pic.mode == 'I':
        img = torch.from_numpy(np.array(pic, np.int32, copy=False))
    elif pic.mode == 'I;16':
        img = torch.from_numpy(np.array(pic, np.int16, copy=False))
    elif pic.mode == 'F':
        img = torch.from_numpy(np.array(pic, np.float32, copy=False))
    elif pic.mode == '1':
        img = 255 * torch.from_numpy(np.array(pic, np.uint8, copy=False))
    else:
        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))

    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))
    # put it from HWC to CHW format
    img = img.permute((2, 0, 1)).contiguous()
    if isinstance(img, torch.ByteTensor):
        return img.float().div(255)
    else:
        return img
```

cc @vfdev-5"
[discussion needed] [videoAPI][bc-breaking] custom class for video frames,pytorch/vision,2020-11-10 10:20:18,3,needs discussion#module: video,2981,739778948,"## 🚀 Feature
Design and implement a custom class to hold the return of a video frame.

## Motivation

At the moment the new [VideoReader API](https://github.com/pytorch/vision/issues/2660) returns a dictionary with a byte tensor ""data"" and a ""pts"" value. This works well for numerical data (such as audio and video), however, it may cause issues with CC and SUB streams that return strings. Additionally, if we ever want to expose additional fields and functionalities, this approach would make it easy to do from either C++ or python. 

## Pitch

I propose to have a custom registered `VideoFrame` class that would be the sole return value of the video reader API. 
For example, we could get a new frame:
```
frame = next(VideoReaderObject)
```
where the frame object would have a set type, and be able to return a value based on that return type. 
```
VideoReaderObject.set_current_stream(""video"")
frame = next(VideoReaderObject)
frame.type()     # returns ""video""
frame.data()     # retruns uint8 byte tensor with video frame
frame.pts().      # returns float with the current presentation timestamp

VideoReaderObject.set_current_stream(""subtitle"")
frame = next(VideoReaderObject)
frame.type()     # returns ""subtitle""
frame.data()     # returns a string
```


## Alternatives

We could keep the current API, and cast the pointer to the string to the byte tensor, and then cast it back in python.
Not sure if and how well this could work, but it would be less disruptive. We can always alter the behaviour of python API in the wrapper we have at the moment, but I'm not sure what sort of time overhead that would be adding.



cc @bjuncek"
[RFC] Loss Functions in Torchvision,pytorch/vision,2020-11-10 09:22:47,29,needs discussion#module: ops,2980,739736277,"## 🚀 Feature

A loss functions API in torchvision.

## Motivation

The request is simple, we have loss functions available in torchvision
E.g. `sigmoid_focal_loss` , `l1_loss`. But these are quite scattered and we have to use `torchvision.ops.sigmoid_focal_loss` etc.

In future, we might need to include further loss functions. E.g. `dice_loss`

Since loss functions are differentiable we can put them under `nn`.
We can have

`torchvision.nn.losses.sigmoid_focal_loss` and so on.

This keeps the scope of `nn` open for other differentiable functions such as layers, etc.

## Pitch

These losses are very specific and pertain to vision domain. These are really useful and in general not tied to any specific model.
Though the loss functions that we keep are usually in `torch`. If we keep under `nn` namespace, future migration stays simple.

instead of `torchvision.nn.sigmoid_focal_loss` it would be `torch.nn.sigmoid_focal_loss`.

This Pitch comes from the above issues. 
[More Loss Functions](https://github.com/pytorch/pytorch/issues/41142)

## Alternatives

Alternatively, this should go in torch. But if we keep the above idea, we can support them in torchvision and later deprecate and move to torch (when needed).

Currently, we include them under `ops` but it is actually not an `operation` it is a differentiable loss function. 

Whereas other ops are not differentiable and perform transformations / some manipulation over boxes/layers.

## Additional context

Here is a list of loss functions we would like to include.

- [x] [LabelSmoothing Loss](https://github.com/pytorch/pytorch/issues/7455) https://github.com/pytorch/pytorch/pull/63122
- [x] [SoftTarget CrossEntropy](https://github.com/pytorch/pytorch/issues/11959) https://github.com/pytorch/pytorch/pull/61044
- [x] [Huber Loss](https://github.com/pytorch/pytorch/issues/48595) https://github.com/pytorch/pytorch/pull/50553
- [ ] [Barron loss](https://arxiv.org/pdf/1701.03077.pdf) Implemented in classy vision
- [ ] [JSD Loss](https://arxiv.org/pdf/1912.02781.pdf)
- [ ] [Dice Loss](https://github.com/pytorch/pytorch/issues/1249)
- [ ] [Poly Loss](https://arxiv.org/abs/2204.12511)
- [x] [gIoU Loss](https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/giou_loss.py) Used in DETR.
- [x] Refactor Current [Focal Loss](https://github.com/pytorch/vision/blob/master/torchvision/ops/focal_loss.py) from ops to nn.
- [x] Refactor FRCNN [Smooth L1 Loss](https://github.com/pytorch/vision/blob/master/torchvision/models/detection/_utils.py#L342) to nn.
- [ ] [Super Loss](https://github.com/pytorch/pytorch/issues/49851) https://github.com/pytorch/pytorch/issues/49851
- [ ] [TripletMarginLoss](https://github.com/pytorch/vision/blob/master/references/similarity/loss.py#L9) This has similar issue to `LabelSmoothing`. `TripletMarginLoss` is supported in PyTorch but we use a variant of it in torchvision references for similarity search.
- [ ] [DeepLabCELoss](https://github.com/facebookresearch/detectron2/blob/60d7a1fd33cc48e58968659cd3301f3300b2786b/projects/DeepLab/deeplab/loss.py) This is implemented in Detectron2, but in torchvision references and model training we use `nn.CrossEntropy()` with a little modification to aux loss.
- [ ] [Multi Class Focal Loss](https://github.com/pytorch/vision/issues/3250)
- [ ] [PSNR Loss](https://github.com/kornia/kornia/blob/master/kornia/losses/psnr.py) Also PSNR as `torchvision.ops` will be nice.
- [x] [Distance-IoU & Complete-IoU loss](https://arxiv.org/abs/1911.08287) - [see here](https://github.com/facebookresearch/detectron2/blob/dfe8d368c8b7cc2be42c5c3faf9bdcc3c08257b1/detectron2/layers/losses.py#L66) - #5776 #5786
- [ ] [SioU loss](https://arxiv.org/abs/2205.12740)
- [ ] [Federated loss](https://arxiv.org/abs/2103.07461)
- [ ] [Poly Loss](https://arxiv.org/pdf/2204.12511.pdf) https://github.com/pytorch/pytorch/issues/76732
- [ ] [Tversky Loss](https://arxiv.org/abs/1706.05721)

## References

We can refer to [Kornia](https://github.com/kornia/kornia/), [Fvcore](https://github.com/facebookresearch/fvcore) and few PyTorch issues that need this feature.

"
detector as feature extractor,pytorch/vision,2020-11-04 22:49:19,0,question,2963,736474461,"Hello,

I am using mask rcnn for detection. So basically fine tuning. However I want extract feature for each object that is being detected. 
So possibly extracting feature vector just before last layer. How can I do that ? forward hooks ? 

I was also looking into https://github.com/pytorch/vision/blob/master/torchvision/models/_utils.py ? could not get it working. 

Also how to use jit for the same ?

Any leads would be helpful. @fmassa 

Cheers! 
"
Add pathlib.Path support to read_image,pytorch/vision,2020-11-04 17:52:06,5,enhancement#module: io,2962,736310884,"When I try to pass an object of `pathlib.Path` to `torchvision.io.read_image`, I get:
```
RuntimeError: image::read_file() Expected a value of type 'str' for argument '_0' but instead found type 'PosixPath'.
Position: 0
Value: PosixPath('<SOME PATH>')
Declaration: image::read_file(str _0) -> (Tensor _0)
Cast error details: Unable to cast Python instance to C++ type (compile in debug mode for details)
```

Casting the object to `str` fixes the problem. It would be nice to support the usual set of ""path-like"" types as input (str, bytes, pathlib.Path, ...)

```
python==3.8.3
torch==1.7.0
torchvision==0.8.1
```"
[RFC] How to handle BC breaking changes on Model weights or hyper-parameters,pytorch/vision,2020-11-03 12:10:36,3,needs discussion#version incompatibility,2955,735267803,"## 🚀 Feature
In order to fix bugs we are sometimes forced to introduce BC breaking changes. While the process of such introductions is clear when it comes to code changes, it's not when it comes to model weights or hyper-parameters. Thus we should define when, why and how to introduce BC-breaking changes when it comes to model weights or model hyper-parameters.

## Motivation

We have recently bumped to a few issues that motivate this. Here are a few examples:
- On #2326 we discovered a bug in the initialization of some weights of all detection models. If we fix the bug on code, we should probably retrain the models. What happens if their accuracy improves? How do we make them available to our users? 
- How do we handle cases such as #2599 where in order to fix a bug we need to update the hyper-parameters of the model?

## Approaches

There are quite a few different approaches for this:
1. Replace the old parameters and Inform the community about the BC breaking changes. Example: #2942
   - Reasonable approach when the accuracy improvement is substantial or the effect on the model behaviour is negligible.
   - Keeps the code-base clean from workarounds and minimizes the number of weights we provide.
   - Can potentially cause issues to users who use transfer learning.
2. Write code/workarounds to minimize the effect of the changes on existing models. Example: #2940
   - Reasonable approach when the changes lead to slight decrease in accuracy.
   - Minimizes the effects on users who used pre-trained models.
   - Introduces ugly workarounds on the code and increases the number of weights we provide.
3. Introduce versioning on model weights:
   - Appropriate when introducing significant changes on the models.
   - Keeps the code-base clean from workarounds.
   - Forces us to maintain multiple versions of weights and model config.

It's worth discussing whether we want to adapt our approach depending on the characteristics of the problem or if we want to go with one approach for all cases. Moreover it's worth investigating whether we need to handle differently changes on weights vs changes on hyper-parameters used on inference.

cc @fmassa @cpuhrsch @vfdev-5 @mthrok "
Imagenet Pre-trained model for other Depth Multiplier,pytorch/vision,2020-11-03 06:58:39,1,question#module: models,2951,735072722,"On the mnasnet model under mnasnet.py file, the link provided for imagenet pretrained model is only for two depth multiplier, as shown in the code below:

_MODEL_URLS = {
    ""mnasnet0_5"":
    ""https://download.pytorch.org/models/mnasnet0.5_top1_67.823-3ffadce67e.pth"",
    ""mnasnet0_75"": None,
    ""mnasnet1_0"":
    ""https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth"",
    ""mnasnet1_3"": None
}


Can you provide the link for Imagenet pre-trained model for  mnasnet0_75 and mnasnet1_3?

"
Preprocessing helper for Normalized and Denormalized Boxes,pytorch/vision,2020-10-29 18:54:23,0,topic: object detection#new feature,2936,732572234,"## 🚀 Feature

We recently had `box_convert` function which helps interconversions of boxes.
I missed the cases where bounding boxes are normalized.
People would like to Denormalize them as well as Normalize them back.

## Motivation

Motivation was same as `box_convert`. We need these quick helper functions which are often used through Albumentations, etc. This is boiler plate of most object detectino repos / models. Datasets come in all sort of formats.

## Pitch

This can be done in 3 ways. I am not sure which one is good.
1. Have 2 seperate utilites like `denormalize_boxes`, `normalize_boxes`. Just like other box ops such as `box_area`, `box_iou` 
2. Make this part of `box_convert`. We can have an extra argument `noramalize`. This seems hacky and can cause other torchscript errors. So not pretty sure if it would be nice.
3. Can this be transform? Like `T.NormalizeBox()` and `T.DenormalizeBox()` . I haven't worked with transforms closely so not sure. But can be an idea.

## Alternatives

Again this is utlity. No forcing. But something useful that bounding boxes often need.

## Additional context

Maybe there is a function in torchvision / workaround which I'm not aware of.
"
ImageFolder work-a-like for regression tasks,pytorch/vision,2020-10-28 22:14:10,4,needs discussion#module: datasets,2930,731826701,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->

The requested/proposed feature is a close analog of `torchvision.datasets.ImageFolder` for regression tasks. The target values could be in a dict or an external JSON file.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->

Thanks to `torchvision.models` and `torchvision.datasets`, the workflow for using transfer learning from alexnet, resnet, etc. to custom image classification tasks is very streamlined and polished. Datasets can be handled directly in the file system, which makes it easy to use a number of labeling tools that don't necessarily understand PyTorch,  but are able to place images in folders like `data/train/class1`, `data/val/class1`, etc. 

It's not as easy to do this for regression tasks, despite these being useful variations (e.g. for NSFW scoring rather than binary classification) of popular transfer learning tasks.

## Pitch

<!-- A clear and concise description of what you want to happen. -->

Ideally, there would be a drop-in replacement for `ImageFolder` accepting the same arguments (such as transform callables) *and* an additional `target_scores` variable accepting a JSON filename or a dict. The method `__getitem__` would return a (sample, score) pair.

To facilitate switching from classification to regression tasks, this `RegressionImageFolder` would ignore the directory structure that's used for target classes in `ImageFolder`.

## Alternatives

It's possible that there's no rationale for having `ImageFolder` and `RegressionImageFolder` as separate dataset loaders and the functionality can be folded into what already exists.


cc @pmeier"
[Feature Request] Support same_on_batch option for transforms,pytorch/vision,2020-10-28 21:03:26,2,module: transforms,2929,731789081,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
In the latest torchvison, transforms support tensor input (very nice feature, cheers!). 

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
As titled, `same_on_batch` option is not available for torchvision transforms. For example, `RandomResizedCrop` will crop at the same location for all images in the batch (which is ideal for video, but not for batched images).  

## Pitch

<!-- A clear and concise description of what you want to happen. -->
[Kornia](https://kornia.readthedocs.io/en/latest/augmentation.html) has this feature off the shelf. Maybe torchvision could follow the same API. 



cc @vfdev-5"
COVID-19 library in torchvision,pytorch/vision,2020-10-22 21:54:34,4,needs discussion#module: datasets#module: models,2877,727745972,"## 🚀 Feature
Library of models and dataset interfaces for COVID-19 models

## Motivation

There are quite a few models (feature extractors, mask segmentation, classifiers) for COVID-19, both in pytorch and tensorflow. They use different datasets, making it hard to scientists to compare results and extend their findings. It would be good to (re-implement) at least some models and dataset interfaces as a library in torchvision

## Pitch

Similar to the `models` and `datasets` in torchvision: `models` for the published models (see below), at least those that come with pretrained weights (e.g. COVIDNet-CT), and `datasets` for open-source labelled dataset interfaces: eg. CNCB-CT, UCSD, MedSeg, Zenodo, especially mask extraction.

## Alternatives

None that I know of

## Additional context

Some candidates include COVIDNet (x-rays), COVIDNet-CT (ct-scans), COVNet (ct-scans), JCS (ct-scans). 


cc @pmeier"
Make the resize anchor edge variable if size is scalar,pytorch/vision,2020-10-22 06:58:27,10,module: transforms#new feature,2868,727108565,"## 🚀 Feature

Make the resize anchor edge variable if size is scalar.

## Motivation

Right now `torchvision.transforms.Resize()` assumes that the smaller edge of the image should be resized to size if its an `int`. 

https://github.com/pytorch/vision/blob/d1e134c71b94a5975dcbf16927ed608dac54fdfe/torchvision/transforms/transforms.py#L239-L240

That can be cumbersome if the user wants for example an image that is not larger than x or wants a specific edge to be this size.

## Pitch

I propose we add a `edge` keyword argument which can be one of `""short""` (default), `""long""`, `""vert""`, and `""horz""`. Based on this we calculate the size.


cc @vfdev-5"
Mismatch in build_vision.sh environment variables,pytorch/vision,2020-10-21 20:44:12,5,windows#topic: binaries,2867,726850283,"The if-statement is checking **TORCHVISION**_BUILD_VERSION but the body uses **PYTORCH**_BUILD_VERSION. Same issue with `BUILD_NUMBER`.

https://github.com/pytorch/vision/blob/d1e134c71b94a5975dcbf16927ed608dac54fdfe/packaging/conda/build_vision.sh#L19-L22

cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @smartcat2010 @mszhanyi"
Relocate and package binary dependencies in conda distributions,pytorch/vision,2020-10-21 18:30:14,0,needs discussion,2866,726762972,"## 🚀 Feature
We should relocate all the binary dependencies of torchvision on the conda recipes. That means that all torchvision conda distributions should package the extension binaries, as well as their own dependencies (i.e., libpng, libjpeg-turbo, ffmpeg), similar to what is done for wheel packaging.

## Motivation
After PR https://github.com/pytorch/vision/pull/2777, we found that it is possible to have collisions between binary dependencies across some of the libraries that torchvision depends on, e.g., PyAV and torchvision's video_reader. This is very common on Windows, where the DLLs do not have an rpath, as opposed to ELF and MachO formats, which do have. 

Also it is possible to have some conflicts on Mac and Linux when users try to mix FFmpeg versions between conda-forge and other sources different from the PyTorch channel ffmpeg

## Pitch
The suggested relocation steps should occur as follows:

1. During build time (``build`` dependencies on meta.yaml), we list the binary packages that we require to build each of the torchvision extensions, i.e., libpng, libjpeg-turbo and ffmpeg (pytorch channel)
2. The recipe should compile torchvision as usual
3. Once the compilation is done, we proceed to relocate the required binaries into the python package itself by renaming the shared libraries and updating the rpaths on Mac and Linux and using https://github.com/njsmith/machomachomangler on Windows DLLs
4. We remove the already-packaged binaries from the runtime dependencies (``run`` in meta.yaml).

By doing this, we're preventing any possible clash that could occur between user-installed versions of any of the aforementioned libraries and other packages that might depend on them.

**Note:** If users want a different version of any of the packages that we're bundling, they should recompile torchvision from scratch themselves.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

* On Linux we can use patchelf
* On Mac we can take a look at the delocate mechanism
* On Windows, we need to use https://github.com/njsmith/machomachomangler, since it is the only tool available to patch DLLs
 
cc @fmassa @seemethere @malfet 
"
How to get corresponding feature regions of final detections from feature map of backbone?,pytorch/vision,2020-10-21 13:24:56,2,question#topic: object detection,2853,726483235,"Hi, 

For every output detection [x1, y1, x2, y2], I would like to extract its corresponding region in the feature map output of the backbone of Faster-RCNN. Similarly, I want to extract the corresponding region in the feature map for the target (groundtruth) bounding boxes.

Can you point me to how this should be done?

Thank you. 

"
Bad Performance with default_observer for quantization,pytorch/vision,2020-10-21 02:55:26,0,module: models.quantization,2849,726087858,"## ❓ Questions and Help

### Please note that this issue tracker is not a help form and this issue will be closed.

We have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:

- [Discussion Forum](https://discuss.pytorch.org/)


I tried to quantize mobilenet v2 from float model file.
I found defaut_observer is used for activation in QConfig.
https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/utils.py#L27

I got bad imagenet classification accuracy with this configuration.


https://pytorch.org/docs/stable/quantization.html
Here, the following configuration is recommended.
qconfig = torch.quantization.get_default_qconfig('qnnpack')

this uses HistogramObserver.
        qconfig = QConfig(activation=HistogramObserver.with_args(reduce_range=False),
                          weight=default_weight_observer)

When I changed default observer to HistogramObserver, I got much better accuracy.

I think the following configuration should be changed as recommended in official docs.
https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/utils.py#L27"
Could you please make it more obvious when imagefolder is loading hidden folders?,pytorch/vision,2020-10-18 23:11:03,5,module: datasets#new feature,2831,724154752,"Thanks for making this dataloader! I was using imagefolder as part of a dataloader, then I was trying to use the dcgan example on the same folder with the images, and I started getting mysterious errors about the last index number on my cnn... It turned out that something made .ipynb_checkpoints folders in the imagefolder (I suspect the dcgan example). While this is clearly an example of user error, it would have saved a couple of hours of debugging if the imagefolder dataloader gave a warning about using hidden folders as a dataclass... I really do enjoy this amazing dataloader...  

cc @pmeier"
warning: ignoring #pragma omp parallel,pytorch/vision,2020-10-10 06:23:15,16,module: ops#topic: build,2783,718545522,"## 🐛 Bug

I'm seeing a lot of `warning: ignoring #pragma omp parallel` like this:
<img width=""1085"" alt=""image"" src=""https://user-images.githubusercontent.com/5203025/95647645-b5553600-0b03-11eb-94e1-4ad6caf7215d.png"">

Maybe there should be a `-fopenmp`?

## Environment

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): master/master
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): cmake+ninja+gcc-8
 - Python version: 3.6
 - CUDA/cuDNN version: 11.0/8"
CI tests only run on conda,pytorch/vision,2020-10-08 13:21:33,2,triage review#high priority#module: ci,2776,717340190,"## 🚀 Feature

Run CI tests on pip as well.

## Motivation

We have traditionally built torchvision via both conda and pip, but we only perform tests on conda.

This has been generally fine because there was nothing which was specific to pip that could affect runtime and wouldn't be catch by a conda test.

This is not true anymore since the addition of libpng / libjpeg / ffmpeg as dependencies to torchvision, [which requires different manipulations for pip](https://github.com/pytorch/vision/blob/6756ed0292315e7dcae19e03362840c22c9034fb/packaging/build_wheel.sh#L14-L35).

This lack of coverage is illustrated in https://github.com/pytorch/vision/pull/2775, where only the conda binaries work, and not the pip packages.

cc @andfoy @seemethere "
[RFC] Rotated Bounding Boxes,pytorch/vision,2020-10-06 08:25:46,17,needs discussion#topic: object detection,2761,715456912,"## 🚀 Feature
A bit unsure about this feature.

Support Rotated Bounding Boxes in Torchvision.

## Motivation

There is recent research on Rotated Bounding boxes which provides better detection results. I am not able to find highly cited results but a few of them are
1. [Rotation Invariant Detector](https://arxiv.org/pdf/1711.09405v1.pdf)
2. [SCRDET](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf) ICCV 2019.

I'm not sure for more papers. I think this is slightly new topic, and needs a bit more research.

## Pitch

1. Support operations for Rotated Bounding boxes, NMS, ROI Align. IoU.
2. Support Augmentation (if needed) for Rotated Bounding boxes.
3. Support such Datasets (if any) that use Rotated Boxes. 

If possible we can also support Rotated models. In my opinion it might not be very feasible. 
Since it will take a lot of time and maintenance for these models.

## Alternatives

Currently, Detectron2 has great support for all the above. These operations are implemented in C++.

## Additional context

Looks tricky and challenging. I guess it might also be too early to think about this. I think it needs a bit more research and some baseline.

cc @pmeier "
High Level API for OCR tasks,pytorch/vision,2020-10-04 20:20:12,12,needs discussion#new feature,2753,714394261,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
Pytorch vision library has many high-level API for performing the tasks under the hood seamlessly if there can be a high-level API for OCR tasks then downloading lots of third party libraries could be avoided.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
When I was building one such ocr system I found that there is no generalized way to recognize and text from various image/document formats even due to slight change in the format structure of document/images the systems tend to fail, at times we have to resort to using many traditional machine learning techniques which are highly time-consuming to get the desired results.

## Pitch
OCR has high application in the industry millions of industry use this software on a daily basis and spends a lot of money in maintaining them they are used in medical, finance, delivery and many other domains for verification and data entry/storage purpose  but despite having a use case over a wide number of industries there is no single solution which everyone can use and keep improving over the years, instead they rely on the old methods of creating one's own solution for this purpose.

If such an API is integrated in PyTorch then many businesses can shift to digital platforms and can increase their productivity, apart from this it will also be useful to the school and university students in a wide range of tasks.

Lastly once integrated into Pytorch it will be freely available for everyone to use since OCR software are very expensive and it can be improved over the years to come.
<!-- A clear and concise description of what you want to happen. -->

## Alternatives

At present we only have tesseract which is capable of leveraging deep learning tasks for ocr but tesseract has its own set of problems and cannot be used everywhere.

and required a lot of preprocessing from other libraries to be done beforehand in order to use it

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->


<!-- Add any other context or screenshots about the feature request here. -->
"
LSUN bedroom_train data load error,pytorch/vision,2020-10-02 16:46:35,7,help wanted#module: datasets#new feature,2746,713760824,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
I am trying to load the lsun bedroom_train data from torchvision.dataset but it's returning an error.

## To Reproduce

Steps to reproduce the behavior:

1. import torch
2. from torchvision import transforms
3. from torchvision import datasets
4. import os
5. path = os.getcwd()
6. train_transform = transforms.Compose([
7.    transforms.ToTensor(),
8.   transforms.Normalize((0.5,0.5,0.5), (0.5, 0.5, 0.5))])
9. bedroom = datasets.LSUN(root = path, classes= ['bedroom_train'], transform=train_transform)
<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
---------------------------------------------------------------------------
Error                                     Traceback (most recent call last)
<ipython-input-4-0560a8b9293e> in <module>
----> 1 bedroom = datasets.LSUN(root = path, classes= ['bedroom_train'], transform=train_transform)

D:\Softwares\Anaconda\lib\site-packages\torchvision\datasets\lsun.py in __init__(self, root, classes, transform, target_transform)
     75             self.dbs.append(LSUNClass(
     76                 root=root + '/' + c + '_lmdb',
---> 77                 transform=transform))
     78 
     79         self.indices = []

D:\Softwares\Anaconda\lib\site-packages\torchvision\datasets\lsun.py in __init__(self, root, transform, target_transform)
     17 
     18         self.env = lmdb.open(root, max_readers=1, readonly=True, lock=False,
---> 19                              readahead=False, meminit=False)
     20         with self.env.begin(write=False) as txn:
     21             self.length = txn.stat()['entries']

Error: G:\DCGANs/bedroom_train_lmdb: No such file or directory



## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

The LSUN bedroom training dataset should be downloaded in specified path

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): 1.6.0/0.7.0
 - OS (e.g., Linux): Windows 10
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): conda
 - Build command you used (if compiling from source):
 - Python version: 3.7.7
 - CUDA/cuDNN version: 10.2
 - GPU models and configuration: NVIDIA GTX 1060 6GB
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @pmeier"
Are new models planned to be added?,pytorch/vision,2020-09-24 20:14:28,57,needs discussion#module: models#new feature,2707,708445867,"## 🚀 Feature
Adding new models to the models section.

## Motivation

Many new models have been proposed in the recent years and do not exist in the models module.
For example, the EfficientNets appear to provide with 8 models of different complexities that outperform everything else that exists at each complexity level.

## Pitch

See [Contributing to Torchvision - Models](https://github.com/pytorch/vision/blob/main/CONTRIBUTING_MODELS.md) for guidance on adding new models.

- [x] RetinaNet #1697 
- [x] Mobile Net v3 #3252
- [x] Mobile Net Backbones #2700.
- [x] Mobile Net Backbones for Detection #1999 
- [x] Mobile Net Backbones for Segmenetation #3276 
- [x] Single Shot Multi-Box (SSD) Detector #3760 #3403 
- [x] SSD Lite #3757 
- [x] Efficient Net (b0 to b7) #980
- [x] RegNet #2655. 
- [x] ViT #4594
- [x] [FCOS](https://arxiv.org/abs/1904.01355) #4961
- [x] [ConvNeXt](https://arxiv.org/pdf/2201.03545.pdf) #5197 #5253
- [x] [EfficientNetV2](https://arxiv.org/abs/2104.00298) #5450
- [x] [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) #5491
- [x] [Improved MViT](https://arxiv.org/pdf/2112.01526.pdf) #6198
- [ ] [Swin Transformer V2](https://arxiv.org/abs/2111.09883) - #6242 #6246
- [ ] [DeTR](https://arxiv.org/abs/2005.12872) #5922
- [ ] [DINO](https://arxiv.org/pdf/2203.03605v3.pdf)
- [ ] [Deformable DeTR](https://arxiv.org/abs/2010.04159)
- [ ] [EfficientDet](https://arxiv.org/abs/1911.09070) 
- [ ] [YOLO](https://arxiv.org/abs/1804.02767) #2074
- [ ] [Cascade RCNN](https://arxiv.org/pdf/1712.00726.pdf)
- [ ] [HTC](https://arxiv.org/pdf/1901.07518v2.pdf)
- [ ] [CondInst](https://arxiv.org/pdf/2003.05664.pdf)
- [ ] [SOLO](https://arxiv.org/pdf/1912.04488.pdf)
- [ ] DeepLabv3+ With Resnet #2689 
- [ ] SE-ResNet and SE-ResNeXt #2179
- [ ] Inception-ResNet #3899 and V2 #5036
- [ ] [NFNets](https://arxiv.org/abs/2102.06171)
- [ ] [ResNeSt](https://arxiv.org/abs/2004.08955)
- [ ] [ReXNet](https://arxiv.org/pdf/2007.00992.pdf)
- [ ] [FBNet](https://arxiv.org/abs/1812.03443)
- [ ] [CoAtNet](https://arxiv.org/abs/2106.04803)

Add pre-trained weights for the following variants:
- [x] Pretrained weights for ShuffleNetv2 1.5x and 2.0x + the Quantized versions. #5906 #3257 
- [x] Pretrained weights for MNasnet 0_75 and 1_3. #3722 #6019
- [x] Variant + Pretrained weights for Resnext101_64x4d depth. #5935 #3485
- [x] Variant + Pretrained weights for Resnext152_32x4d depth. #3485
"
Installing torchvision for ppc64le IBM architecture,pytorch/vision,2020-09-23 20:50:31,4,topic: binaries,2709,709006318,"I am in a weird scenario were I am forced to use torch `1.3.1` (due to hardware see: https://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda/#/). I read from the pytorch docs that it's corresponding version of torchvision is `0.4.1` (https://pypi.org/project/torchvision/):

> ## Installation
> 
> We recommend Anaconda as Python package management system. Please refer to [pytorch.org](https://pytorch.org/) for the detail of PyTorch (torch) installation. The following is the corresponding torchvision versions and supported Python versions.
> 
> ```
> Installation
> We recommend Anaconda as Python package management system. Please refer to pytorch.org for the detail of PyTorch (torch) installation. The following is the corresponding torchvision versions and supported Python versions.
> 
> torch	torchvision	python
> master / nightly	master / nightly	>=3.6
> 1.5.0	0.6.0	>=3.5
> 1.4.0	0.5.0	==2.7, >=3.5, <=3.8
> 1.3.1	0.4.2	==2.7, >=3.5, <=3.7
> 1.3.0	0.4.1	==2.7, >=3.5, <=3.7
> 1.2.0	0.4.0	==2.7, >=3.5, <=3.7
> 1.1.0	0.3.0	==2.7, >=3.5, <=3.7
> <=1.0.1	0.2.2	==2.7, >=3.5, <=3.7
> ```

but for some reason I have the wrong version of it:

```
torchvision 0.2.2 pypi_0 pypi
```

is there a way to install the right version of torchvision?

-----

What I've tried:

First I tried force installing the right version with conda. Conda couldn't find the version of torchvision that I need:

```
$ conda install torchvision==0.4.2
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - torchvision==0.4.2

Current channels:

  - https://repo.anaconda.com/pkgs/main/linux-ppc64le
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-ppc64le
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.

```

Then I proceeded to try to install it regardless with pip

```
$ pip install torchvision==0.4.2
Defaulting to user installation because normal site-packages is not writeable
ERROR: Could not find a version that satisfies the requirement torchvision==0.4.2 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3)
ERROR: No matching distribution found for torchvision==0.4.2
```

got an error too.

Is there anything else to try?

----

I tried but it failed:

```
conda install torchvision==0.4.2 -c pytorch

Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - torchvision==0.4.2

Current channels:

  - https://conda.anaconda.org/pytorch/linux-ppc64le
  - https://conda.anaconda.org/pytorch/noarch
  - https://repo.anaconda.com/pkgs/main/linux-ppc64le
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-ppc64le
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.

```

----

related:

- crossposted SO: https://discuss.pytorch.org/t/force-installing-torchvision/97279
- crossposted reddit pytorch: https://www.reddit.com/r/pytorch/comments/iyf2qn/force_installing_torchvision/
- crossposted reddit ibm: https://www.reddit.com/r/IBM/comments/iyhzex/force_installing_torchvision_042_when_i_am_forced/
- real problem is installing torchmeta: https://github.com/tristandeleu/pytorch-meta/issues/95
- https://www.ibm.com/mysupport/s/forumsquestion?id=0D50z00006gaxV9CAI
- quora: https://www.quora.com/unanswered/How-does-one-install-specific-Python-packages-in-Conda-from-IBM-architectures
- reddit ibm2: https://www.reddit.com/r/newIBM/comments/iyij10/force_installing_torchvision_042_when_i_am_forced/
- gitissue for ibm: https://github.com/IBM/powerai/issues/268

cc @fmassa @vfdev-5"
Building FFmpeg binaries for Windows,pytorch/vision,2020-09-18 18:32:44,3,windows,2688,704576085,"## Discussion
<!-- A clear and concise description of the feature proposal -->
### Context

Right now, as part of PRs https://github.com/pytorch/vision/pull/2683 and https://github.com/pytorch/vision/pull/2596, we intend to include the FFmpeg binaries on or wheel distributions. Due to the software licensing nature of FFmpeg (GPL), we started to build our own LGPL FFmpeg binaries on the [PyTorch builder repo](https://github.com/pytorch/builder/tree/master/ffmpeg), based off the [conda-forge recipe](https://github.com/conda-forge/ffmpeg-feedstock)   

### Problems
1. **Building and distributing FFmpeg**
While Linux and Mac builds can be done from source and they are working, Windows builds depend on a binary distribution repository for FFmpeg, namely, [Zeranoe FFmpeg builds](https://ffmpeg.zeranoe.com/builds/), which will be deprecated after the 18th of September of 2020 (The day of the opening of this discussion), that implies that we need to start compiling LGPL binaries for Windows.

2. **FFmpeg dependencies**
While compiling FFmpeg is feasible by using MinGW-w64 and MSYS2, some of the libraries required to compile it, such as OpenH264, iconv and others are not available for Windows in any Anaconda channel, both defaults nor conda-forge, therefore we need also to tackle those packages as well. 

## Solution options

1. **Cross compile FFmpeg and distribute the binaries (Zeranoe-like)** 
We can setup a new binary distribution repository where we can distribute LGPL binaries for Windows to replace the Zeranoe one. In order to compile the binaries, we can ""easily"" cross-compile FFmpeg and all of the downstream libraries on Linux and package them. I've built FFmpeg locally on linux by using [MXE](https://github.com/mxe/mxe), and we could set a Docker image to do so. I've done some tests using this repository: https://github.com/andfoy/ffmpeg-win-lgpl, for redistributing those packages.
**Note:** This solution fixes also the problem with the FFmpeg downstream binaries 

2. **Compile the downstream libraries and FFmpeg directly on conda-build**
While this is an adequate solution, we would need to update the recipes on conda-build for the dependencies that we need for compiling FFmpeg, such as OpenH264, libiconv and others, then ask the Anaconda team to update the corresponding packages on defaults and then update our FFmpeg build recipe for Windows in order to compile FFmpeg using MinGW-w64.

## Additional context
We need to take into account potential patent licensing issues regarding OpenH264 specifically. According to the [OpenH264 FAQ](http://www.openh264.org/faq.html) and the following conda-forge issues: https://github.com/conda-forge/openh264-feedstock/issues/5, https://github.com/conda-forge/ffmpeg-feedstock/issues/66. We should download Cisco OpenH264 binaries directly each time the library is installed, since the patent fee that Cisco pays can only apply when using their binaries instead of ours. 

Regarding the conda recipe, we can use a post-install script to handle this case, however, for the wheels, we need a mechanism to download Cisco OpenH264 when installing it. In Windows this shouldn't be a problem, since DLLs do not require an rpath and just require dependencies to be on the PATH, whereas on Linux and Mac this requires updating of the rpath of the `video_reader.so/dylib` libraries.

## Other resources
Here is the same issue regarding the Windows source compilation on conda-forge: https://github.com/conda-forge/ffmpeg-feedstock/issues/90

cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @smartcat2010 @mszhanyi @fmassa @seemethere "
ROCM version is slow,pytorch/vision,2020-09-17 11:20:03,7,help wanted#module: rocm,2682,703497077,"## 🐛 Bug

Inference speed is abysmal - way slower than cpu. It works as expected on NVidia. I'm not really sure whether this is torch vision or torch itself at fault.

## To Reproduce

Steps to reproduce the behavior:

1. Dockerfile: https://gist.github.com/boniek83/0bbf858a24961816557c522007a11c11
2. Lab: https://github.com/timesler/facenet-pytorch/blob/master/examples/infer.ipynb
3. Data for lab: https://github.com/timesler/facenet-pytorch/tree/master/data/test_images

## Expected behavior

Performance should be competitive.

## Environment

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0):  1.6.0 / 0.7.0
 - OS (e.g., Linux): CentOS 7.8
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): source
 - Build command you used (if compiling from source): see Dockerfile above
 - Python version: 3.6
 - GPU models and configuration: Radeon VII
 - Any other relevant information: ROCK 3.7 during runtime


cc @iotamudelta @ashishfarmer"
toy video datasets for generative models,pytorch/vision,2020-09-14 15:31:01,4,help wanted#module: datasets#new feature,2676,701208808,"## 🚀 Feature
Adding toy video datasets for generative models such as Moving MNIST and Robonet

## Motivation

The currently available video datasets are suitable for recognition and not synthesis. It would be nice if you add simple toy video datasets such as Moving MNIST.

cc @pmeier"
transforms.ColorJitter().get_params(...) does not support float inputs,pytorch/vision,2020-09-11 22:02:39,2,bug#module: transforms,2669,699793968,"## 🐛 Bug

The docstring for `transforms.ColorJitter().get_params(...)` states that it accepts float and tuple inputs, just as `ColorJitter`'s `__init__(...)`, however, the current implementation supports tuple inputs only. Unlike the constructor it does not support float inputs.

## To Reproduce

Steps to reproduce the behavior:

`from torchvision import transforms as T

t = T.ColorJitter()
t = t.get_params(0.4, 0.4, 0.4, 0.2)`

## Expected behavior

the `.get_params(...)` should take float inputs.


 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): 1.4.0
 - OS (e.g., Linux): Linux
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source): N/A
 - Python version: 3.8
 - CUDA/cuDNN version: 10.1
 - GPU models and configuration: GeForce 2080ti
 - Any other relevant information: N/A

cc @vfdev-5"
New video API Proposal,pytorch/vision,2020-09-09 09:46:52,5,module: video#new feature,2660,696664359,"## 🚀 Feature

We're proposing to add a lower level, more flexible, and equally robust API than the one currently existing in `torchvision`.
It would be implemented in C++ and compatible with torchscript. Following the merge of https://github.com/pytorch/vision/pull/2596 it would also be installable via pip or conda

## Motivation

Currently, our API supports returning a tensor of `(TxCxHxW)` via `read_video` (see here) abstraction. This can be prohibitive if a user wants to get a single frame, perform some operations on a per-frame basis. For example, I've ran into multiple issues where I'd want to return a single frame, iterate over frames, or (for example in EPIC Kitchens dataset) reduce the memory usage by transforming the elements before I save them to output tensors.

## Pitch

We propose the following style of API:
First we'd have a constructor that would be a part of torch registered C++ classes, and would take some basic inputs.
```
import torch.classes.torchvision as tvcls
vid = tvcls.Video(path, ""stream:stream_id"")
```

Returning a frame is as simple as calling `next` on the container [optionally, we can define stream from which we'd like to return the next frame from]. What a frame is will largely depend on the encoding of the video. For video, it is almost always an RGB image, whilst for audio it might be a 1024point sample. In most cases, same temporal timespan is covered with variable number of frames (1s of a video might contain 30 video frames and 40 audio frames), so returning the presentation timestamp of the returned frame allows for a more precise control of the resulting clip.
```
frame, timestamp = vid.next(optional: ""stream:stream_id"")
```

To get the exact frame that we want, a seek function can be exposed (with an optional stream definition). Seeking is done either to the closest keyframe before the requested timestamp, or to the exact frame if possible.
```
vid.seek(ts_in_seconds, any_frame=True)
```
For example if we seek into the 5s of a video container, following call to `next()` will return either 1) the last keyframe before 5s in the video (if `any_frame=False`), 2a) the frame with pts=5.0 (if `any_frame=True` and frame at 5s exist), or 2b) the first frame after 5s, e.g. with pts 5.03 (if  `any_frame=True` and frame at 5s doesn't exist). 

We plan to expose metadata getters, and add additional functionality down the line. 

## Alternatives

In the end, every video decoder library is a tradeoff between speed and flexibility. Libraries that support batch decoding such as decord offer greater speed (due to multithreaded Loader objects and/or GPU decoding) at the expense of dataloader compatibility, robustness (in terms of available formats), or flexibility. Other libraries that offer greater flexibility such as pyav, opencv, or decord (in sequential reading mode) can sacrifice either speed or ease of use.

We're aiming for this API to be as close in flexibility to pyav as possible, with the same (or better) per-frame decoding speed, all of which by being torch scriptable. 

## Additional context

Whilst technically, this would mean depreciating our current `read_video` API, during a transition period, we would actually support it through a simple function that would mimic the implementation of current read_video, with minimum to no performance impact. 

cc @bjuncek"
Pretrained GoogLeNet without batch norm,pytorch/vision,2020-08-31 22:03:14,9,module: models,2634,689539586,"## 🚀 Feature
Looking at the [commit history](https://github.com/pytorch/vision/commits/e1a3042792a3f15a8cae8cc563d54d2215a3e201/torchvision/models/googlenet.py) for `torchvision.models.googlenet`, I noticed that the squashed commit messages for the initial PR [#678](https://github.com/pytorch/vision/pull/678/commits/6c0d34d250bbc85a556f58f6aed8190ed14e79eb) mention a plain _and_ batch norm version of GoogLeNet. Currently, there is only one `googlenet` defined in that module's `__all__`. Is there a reason we can't offer both `googlenet` and `googlenet_bn` like before?

## Motivation

I am trying to use PyTorch to replicate a custom Caffe model which used GoogLeNet as one of its components. In both the Caffe and ONNX implementation GoogLeNet, batch norm is not used. In order to have the most direct comparison, I would like to have access to a PyTorch model which also does not use batch norm. 

## Pitch

Use the commit history of the linked PR to supply two GoogLeNet implementations - one with batch norm and one without

## Alternatives

As mentioned in the [forums](https://discuss.pytorch.org/t/how-to-delete-layer-in-pretrained-model/17648/2), I could use `nn.modules.Linear.Identity` to delete all batch norm layers from the current implementation. However, this would render the pre-trained weights ineffective and therefore require many more epochs to get an optimized backbone. With a smaller dataset, I am also not confident that the quality of the backbone would ever reach that of the pre-trained ImageNet model.

## Additional context

The official ONNX GoogLeNet can be downloaded from [here](https://github.com/onnx/models/tree/master/vision/classification/inception_and_googlenet/googlenet/model). Since I am using, PyTorch 1.4.0, I chose the ONNX opset 6. Following your conversion [docs](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html), I wrote the following script to save PyTorch's GoogLeNet as an ONNX file

```
from numpy.testing import assert_allclose
import onnx
import onnxruntime
import torch
from torchvision.models import googlenet
import torch.onnx


def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()


# Trace inputs through the PyTorch model to export
model = googlenet(pretrained=True)
model.eval()
dummy = (torch.randn(1, 3, 224, 224), )
torch_outs = model(*dummy)
path = 'googlenet-pytorch.onnx'
torch.onnx.export(
    model=model,
    args=dummy,
    f=path,
    do_constant_folding=True,
    input_names=['images'],
    output_names=['scores'],
    dynamic_axes={
        'images': {0: 'batch_size'},
        'scores': {0: 'batch_size'}})

# Load/check the ONNX model and run through Python API
onnx_model = onnx.load(path)
onnx.checker.check_model(onnx_model)
ort_session = onnxruntime.InferenceSession(path)
ort_inputs = {inp.name: to_numpy(dummy[i]) for i, inp in enumerate(ort_session.get_inputs())}
ort_outs = ort_session.run(None, ort_inputs)

# Confirm outputs match
assert_allclose(to_numpy(torch_outs), ort_outs[0], rtol=1e-03, atol=1e-05)
```

Loading both implementations into [Netron](https://github.com/lutzroeder/netron) for visualization, you can clearly see how the first inception block differs in regard to its use of batch norm

**PyTorch**

![image](https://user-images.githubusercontent.com/39926682/91772910-fec87200-eba2-11ea-8bff-450e0bb07af9.png)

**ONNX**

![image](https://user-images.githubusercontent.com/39926682/91772978-1ef83100-eba3-11ea-9640-5a9b549a3790.png)
"
some problems when install torchvision from source offline,pytorch/vision,2020-08-31 09:09:06,7,help wanted#topic: build,2632,689049352,"Hi, I have installed pytorch from source following the instruction in https://github.com/pytorch/pytorch. Then, I installed torchvison also from source, using this instruction ""python setup.py install"". And I got this message
> Installed /opt/conda/envs/pytorch_src/lib/python3.8/site-packages/torchvision-0.8.0a0+10d5a55-py3.8-linux-x86_64.egg
Processing dependencies for torchvision==0.8.0a0+10d5a55
Searching for torch
Reading https://pypi.org/simple/torch/
Downloading https://files.pythonhosted.org/packages/8c/5d/faf0d8ac260c7f1eda7d063001c137da5223be1c137658384d2d45dcd0d5/torch-1.6.0-cp38-cp38-manylinux1_x86_64.whl#sha256=5357873e243bcfa804c32dc341f564e9a4c12addfc9baae4ee857fcc09a0a216

I cannot connect to the internet. So what should I do next? Please give me a help. thanks."
none-any.whl  doesnt support new versions,pytorch/vision,2020-08-27 00:35:53,8,topic: binaries,2622,686852654,"## 🐛 Bug

the https://download.pytorch.org/whl/torch_stable.html  include older version of torchvision with none-any.whl:

torchvision-0.1.6-py2-none-any.whl
torchvision-0.1.6-py3-none-any.whl
torchvision-0.2.0-py2.py3-none-any.whl

these are old and dont match latest torch versions.

the problem that when perfoming pip3 install torchvision  no a cpu that is not supported, the old none-any wheel is installed instead of build from source


## To Reproduce:

on EC2 C6g server, ubuntu 20.04. as you can see, it is install v 0.2.2 because that's latest none-any.whl

ubuntu@ip-172-31-82-41:~$ uname -a
Linux ip-172-31-82-41 5.4.0-1021-aws #21-Ubuntu SMP Fri Jul 24 09:43:03 UTC 2020 aarch64 aarch64 aarch64 GNU/Linux

ubuntu@ip-172-31-82-41:~$ pip install --pre torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
Defaulting to user installation because normal site-packages is not writeable
Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
Collecting torchvision
Downloading torchvision-0.2.2.post3-py2.py3-none-any.whl (64 kB)
|████████████████████████████████| 64 kB 4.4 MB/s


## Expected behavior

remove all the none-any.whl options,  or keep them up to date with latest stable version

## Additional context

pip3 install torchvision  on AWS Graviton2 is installing old 0.2.0 that's not compatible with latest torch
"
get_coco_api_from_dataset returns wrong dataset,pytorch/vision,2020-08-25 18:42:31,3,module: reference scripts#topic: object detection,2619,685705942,"## 🐛 Bug

I tried to train a detector on a custom dataset (using https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and the training is stuck on 60% of precision. So I started looking at the source code and noticed that get_coco_api_from_dataset() is probably doing the wrong thing. In case the Subset is used for train and test datasets, it returns its original dataset. Problem is that it returns the entire dataset, not just the test dataset, which is the part of the entire dataset. This results in slow down when building a needless full Coco dataset for evaluation. It also updates coco_evaluator in evaluate() with wrong stats because of conflicting ""image_id"". According to the tutorial ""image_id"" is unique inside one data set and this is why the dataset ""index"" is used. This all results in invalid evaluation stats presented during training.

## To Reproduce

Steps to reproduce the behavior:
No reproducing is needed.

@torch.no_grad()
def evaluate(model, data_loader, device):
        ...
---> coco = get_coco_api_from_dataset(data_loader.dataset) # this returns full dataset
        ...
        res = {target[""image_id""].item(): output for target, output in zip(targets, outputs)}
        evaluator_time = time.time()
---> coco_evaluator.update(res) # invalid target is updated
        ...
        return coco_evaluator


## Expected behavior

Subset dataset should be used in get_coco_api_from_dataset()

## Environment

Collecting environment information...
PyTorch version: 1.6.0
Torchvision: 0.7.0
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Ubuntu 20.04 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
Clang version: Not used
CMake version: version 3.16.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: yes
CUDA runtime version: 11.0.2-1
GPU models and configuration: GPU 0: GeForce GTX 1080 Ti
Nvidia driver version: 450.51.05
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.4.2

Versions of relevant libraries:
[pip3] numpy==1.17.4
[conda] Not used
"
Ability to install all optional datasets requirements,pytorch/vision,2020-08-25 13:53:12,3,enhancement#needs discussion#module: datasets#topic: build,2618,685504962,"Some of the datasets in `torchvision` require some additional packages installed:

- `scipy`: [`Caltech101`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/caltech.py#L84), [`ImageNet`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/imagenet.py#L115), [`SBDataset`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/sbd.py#L63), [`SVHN`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/svhn.py#L67)
- `pycocotools`: [`CocoCaptions`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/coco.py#L58), [`CocoDetection`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/coco.py#L112)
- `lmdb`: [`LSUN`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/lsun.py#L18)
- `requests`: [`CelebA`](https://github.com/pytorch/vision/blob/3e0f5a6ff24c4be25cdaa6695bad421c99e257b9/torchvision/datasets/utils.py#L136)

Since these are all optional requirements, they are all imported lazily. Thus, the user is only informed at runtime (sometimes without a clear error message) that a package is missing.

It would be convenient to specify during installation that all soft requirements should also be installed. For `pip` this can be easily done with an extra:

```sh
pip install torchvision[datasets]
```

with 

```python
setup(
    ...
    extras_require={datasets: [""scipy"", ""pycocotools"", ""lmdb"", ""requests""]}
    ...
)
```

in `setup.py`. I don't know if something similar is possible for `conda` though.

cc @pmeier"
[Legal question] Is it legal to use pre-trained models for commercial purposes?,pytorch/vision,2020-08-20 03:27:09,7,module: models,2597,682378977,
`box_iou` doesn't work with degenerated boxes,pytorch/vision,2020-08-12 21:19:52,9,awaiting response#needs discussion#module: ops,2582,677986095,"## 🐛 Bug
[torchvision.ops.boxes.box_iou](https://github.com/pytorch/vision/blob/c547e5cbbf25e46870abb2e925b42c67c3367afc/torchvision/ops/boxes.py#L153) works only if (x1,y1) < (x2,y2) and calculates zero IoU otherwise
<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:
1. Consider a hypothetical bbox array:
```python
hboxes = torch.arange(5,5*25,5).view(-1,4).float(); hboxes
```
```
tensor([[  5.,  10.,  15.,  20.],
        [ 25.,  30.,  35.,  40.],
        [ 45.,  50.,  55.,  60.],
        [ 65.,  70.,  75.,  80.],
        [ 85.,  90.,  95., 100.],
        [105., 110., 115., 120.]])
```
As all the (x1,y1) are less than (x2,y2) in this case, the `box_iou` output with itself is a perfect identity matrix
```python
from torchvision.ops.boxes import box_iou

box_iou(hboxes,hboxes)
```
```
tensor([[1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 1.]])
```
2. However, if the said condition doesn't hold (x1y1 > x2y2)
```python
hboxes[[2,4],:] = torch.cat([hboxes[[2,4],2:],hboxes[[2,4],:2]],dim=1); hboxes
```
```
tensor([[  5.,  10.,  15.,  20.],
        [ 25.,  30.,  35.,  40.],
        [ 55.,  60.,  45.,  50.],
        [ 65.,  70.,  75.,  80.],
        [ 95., 100.,  85.,  90.],
        [105., 110., 115., 120.]])
```
_like the row 2nd and 4th in this case_

The `box_iou` output is 0
```python
box_iou(hboxes,hboxes)
```
```
tensor([[1., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1.]])
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

The coordinates should be handled beforehand and output for the 2nd case mentioned above should also be an Identity Matrix

## Environment

PyTorch version: 1.6.0
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Manjaro Linux
GCC version: (GCC) 10.1.0
Clang version: Could not collect
CMake version: version 3.17.3

Python version: 3.8 (64-bit runtime)
Is CUDA available: Yes
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: GeForce 940MX
Nvidia driver version: 440.100
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-lightning==0.7.6
[pip3] torch==1.6.0
[pip3] torchvision==0.7.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] mkl                       2020.1                      217  
[conda] mkl-service               2.3.0            py38he904b0f_0  
[conda] mkl_fft                   1.1.0            py38h23d657b_0  
[conda] mkl_random                1.1.1            py38h0573a6f_0  
[conda] numpy                     1.19.1           py38hbc911f0_0  
[conda] numpy-base                1.19.1           py38hfa32c7d_0  
[conda] pytorch                   1.6.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch
[conda] pytorch-lightning         0.7.6                    pypi_0    pypi
[conda] torchvision               0.7.0                py38_cu102    pytorch

## Willing to work

I'd be glad to fix this behaviour :slightly_smiling_face:"
 Python3.6Config.cmake     python3.6-config.cmake,pytorch/vision,2020-08-11 02:51:09,1,duplicate#module: c++ frontend,2573,676538939,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.cmake -DCMAKE_PREFIX_PATH=""/opt/libtorch-cxx11-abi-shared-with-deps-1.5.0/libtorch;/home/ts/opt/torchvision"" -DCMAKE_BUILD_TYPE=Release ..
2.CMake Warning at CMakeLists.txt:12 (find_package):
  By not providing ""FindPython3.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""Python3"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""Python3"" with any
  of the following names:

    Python3Config.cmake
    python3-config.cmake

  Add the installation prefix of ""Python3"" to CMAKE_PREFIX_PATH or set
  ""Python3_DIR"" to a directory containing one of the above files.  If
  ""Python3"" provides a separate development package or SDK, be sure it has
  been installed.


-- Looking for pthread.h
-- Looking for pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Found CUDA: /usr/local/cuda (found version ""10.2"") 
-- Caffe2: CUDA detected: 10.2
-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc
-- Caffe2: CUDA toolkit directory: /usr/local/cuda
-- Caffe2: Header version is: 10.2
-- Found CUDNN: /usr/local/cuda/lib64/libcudnn.so  
-- Found cuDNN: v7.6.5  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so)
-- Autodetected CUDA architecture(s):  6.1
-- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61
-- Found torch: /opt/libtorch-cxx11-abi-shared-with-deps-1.5.0/libtorch/lib/libtorch.so  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ts/project/vision/build



<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```

 - PyTorch / torchvision Version (1.5.0):
 - OS ( Linux):
 - How you installed PyTorch / torchvision (pip):
 - Build command you used (if compiling from source):
 - Python version:3.6.5
 - CUDA/cuDNN version:10.2/7.6.5
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
CMake Warning at CMakeLists.txt:12 (find_package):
  By not providing ""FindPython3.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""Python3"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""Python3"" with any
  of the following names:

    Python3Config.cmake
    python3-config.cmake

  Add the installation prefix of ""Python3"" to CMAKE_PREFIX_PATH or set
  ""Python3_DIR"" to a directory containing one of the above files.  If
  ""Python3"" provides a separate development package or SDK, be sure it has
  been installed.


-- Looking for pthread.h
-- Looking for pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE  
-- Found CUDA: /usr/local/cuda (found version ""10.2"") 
-- Caffe2: CUDA detected: 10.2
-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc
-- Caffe2: CUDA toolkit directory: /usr/local/cuda
-- Caffe2: Header version is: 10.2
-- Found CUDNN: /usr/local/cuda/lib64/libcudnn.so  
-- Found cuDNN: v7.6.5  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so)
-- Autodetected CUDA architecture(s):  6.1
-- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61
-- Found torch: /opt/libtorch-cxx11-abi-shared-with-deps-1.5.0/libtorch/lib/libtorch.so  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ts/project/vision/build
"
cmake torchvision出现的问题,pytorch/vision,2020-08-09 13:34:17,17,awaiting response#topic: build#module: c++ frontend,2565,675700949,"/home/ts/Downloads/vision-master/torchvision/csrc/ROIAlign.h: In function ‘at::Tensor roi_align(const at::Tensor&, const at::Tensor&, double, int64_t, int64_t, int64_t, bool)’:
/home/ts/Downloads/vision-master/torchvision/csrc/ROIAlign.h:29:25: error: ‘class c10::OperatorHandle’ has no member named ‘typed’
                        .typed<decltype(roi_align)>();
                         ^~~~~
/home/ts/Downloads/vision-master/torchvision/csrc/ROIAlign.h:29:31: error: expected primary-expression before ‘decltype’
                        .typed<decltype(roi_align)>();
                               ^~~~~~~~
/home/ts/Downloads/vision-master/torchvision/csrc/ROIAlign.h: In function ‘at::Tensor _roi_align_backward(const at::Tensor&, const at::Tensor&, double, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, bool)’:
/home/ts/Downloads/vision-master/torchvision/csrc/ROIAlign.h:77:12: error: ‘class c10::OperatorHandle’ has no member named ‘typed’
           .typed<decltype(_roi_align_backward)>();
            ^~~~~
/home/ts/Downloads/vision-master/torchvision/csrc/ROIAlign.h:77:18: error: expected primary-expression before ‘decltype’
           .typed<decltype(_roi_align_backward)>();
                  ^~~~~~~~
In file included from /home/ts/Downloads/vision-master/torchvision/csrc/vision.cpp:17:0:
/home/ts/Downloads/vision-master/torchvision/csrc/nms.h: In function ‘at::Tensor nms(const at::Tensor&, const at::Tensor&, double)’:
/home/ts/Downloads/vision-master/torchvision/csrc/nms.h:19:25: error: ‘class c10::OperatorHandle’ has no member named ‘typed’
                        .typed<decltype(nms)>();
                         ^~~~~
/home/ts/Downloads/vision-master/torchvision/csrc/nms.h:19:31: error: expected primary-expression before ‘decltype’
                        .typed<decltype(nms)>();
                               ^~~~~~~~
/home/ts/Downloads/vision-master/torchvision/csrc/vision.cpp: At global scope:
/home/ts/Downloads/vision-master/torchvision/csrc/vision.cpp:45:14: error: expected constructor, destructor, or type conversion before ‘(’ token
 TORCH_LIBRARY(torchvision, m) {
              ^
/home/ts/Downloads/vision-master/torchvision/csrc/vision.cpp:59:19: error: expected constructor, destructor, or type conversion before ‘(’ token
 TORCH_LIBRARY_IMPL(torchvision, CPU, m) {
                   ^
/home/ts/Downloads/vision-master/torchvision/csrc/vision.cpp:82:19: error: expected constructor, destructor, or type conversion before ‘(’ token
 TORCH_LIBRARY_IMPL(torchvision, Autograd, m) {
                   ^
CMakeFiles/torchvision.dir/build.make:518: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/vision.cpp.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/vision.cpp.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/torchvision.dir/all' failed
make[1]: *** [CMakeFiles/torchvision.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2

"
Add a new parameter to skip conversion to PIL Image for VisionDataset Loading ,pytorch/vision,2020-08-08 20:50:37,2,needs discussion#module: datasets#module: io,2564,675586550,"## 🚀 Feature
Add a new parameter to skip conversion to PIL Image for VisionDataset Loading 

## Motivation

The original slowness-related issue was brought up in https://github.com/pytorch/pytorch/issues/42405. This SO question: https://stackoverflow.com/questions/63202478/why-pytorch-is-slower-than-tensorflow-in-read-data/63204186#63204186 is a live example.

The investigation (https://github.com/pytorch/pytorch/issues/42405#issuecomment-670971616) pointed to the fact that we are converting from Tensor -> PIL Image -> Tensor (via a toTensor Transform), which seems unnecessary.

## Pitch
We want to allow VisionDataset to directly return Tensor without converting to PIL Image. This will be controlled by a parameter which for BC will be turned off by default.

## Alternatives


## Additional context

"
"Error(s) in loading state_dict in ..., unexpected keys",pytorch/vision,2020-08-07 17:52:39,3,module: models.quantization,2562,675175596,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

Hello, I am trying to quantize a model. I have done post training static quantization following the tutorial. During the conversion, I:

- define my model:

mymodel = model(cfg)

- load the state_dict

mymodel = load_state_dict(torch.load('weights.pt'))

- Prepare the model, calibrate it and convert it to its quantized version

- save the quantized version using 

torch.save(mymodel_q.state_dict(), 'weights_q.pt')

When I load it I use the same code as before, i.e. I define the model and then load it using load_state_dict

But having the quantized model information about the scale and zero_point it seems that they are missing from the original model definition. It was guessable since nobody has ever changed 'model(cfg)' after quantization. But how to include info about scale and zero_point?

Am I saving the quantized model in the wrong way?

Thank you!


 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): the last available version
 - OS (e.g., Linux): Ubuntu Linux
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.7
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
Add NMS version which can do confidence based thresholding #42370,pytorch/vision,2020-08-04 17:25:52,5,awaiting response#needs discussion#module: ops,2552,672955372,"## 🚀 Feature
Add NMS version which takes confidence threshold and prunes out low confidence scores from the entire prediction.

## Motivation
Motivation is that if Pytorch NMS requires only boxes which cross confidence threshold, it means that the filtering should be done apriori. This filtering usually results in an ONNX NONZERO operator, and if using GLOW, NONZERO can't be supported, owing to the dynamic shape of the output tensor. ONNX supports NonMaxSuppression node, which does exactly the same thing. If pytorch also supports this, conversion of Pytorch NMS -> ONNX NMS becomes much easier.

## Pitch

Add an additional input to NMS operator (confidence_threshold). New NMS operator takes entire model prediction as an input, and based on the confidence_threshold, filters out boxes lower than the threshold before moving to IOU.

"
Torchvision Object detection TPU Support,pytorch/vision,2020-07-17 18:47:07,9,question#topic: object detection#new feature,2486,659524611,"## ❓ Torchvision object detection models with TPU.

My doubt lies somewhere between feature request and question. hence posting here.

PyTorch supports TPU through torch_xla. It makes it possible to train models over TPU.
I guess most torchvision classification models can be used with transfer learning/training over TPU.

For torchvision object detection models, do they support TPU?
Some operations such as `NMS`, `rpn`, `roi_align` do not support TPU and hence I get an error as follows.

I was trying Faster R-CNN resnet50 fpn model for object detection.

```
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/generalized_rcnn.py"", line 70, in forward
    proposals, proposal_losses = self.rpn(images, features, targets)
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py"", line 493, in forward
    boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)
  File ""/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py"", line 416, in filter_proposals
    keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)
RuntimeError: Cannot access data pointer of Tensor that doesn't have storage
```
My doubts/concerns/feature request.
1. Do torchvision object detection models support TPU training?
2. Any Plans for TPU support in future releases for these models?
3. Are these ops only CUDA native and GPU/CPU specific? Is there a work-around to train object detection / segmentation models with TPU?

"
Install failing from the master:  Failed building wheel,pytorch/vision,2020-07-16 17:38:56,21,bug#topic: build#dependency issue,2480,658410068,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.`pip install -q git+https://github.com/pytorch/vision.git`

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
Build successfully without any error message
<!-- A clear and concise description of what you expected to happen. -->

## Environment

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): master
 - OS (e.g., Linux): ubuntu16.04
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source): `pip install -q git+https://github.com/pytorch/vision.git`
 - Python version: 3.6
 - CUDA/cuDNN version:
 - GPU models and configuration:
 - Any other relevant information: clang7

## Additional context

It occurs a week ago and keeps failing. The error message is as follows:
```
+ pip install -q git+https://github.com/pytorch/vision.git
  Failed building wheel for torchvision
Command ""/tmp/venv/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-req-build-7wrzlist/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-record-bbtn3535/install-record.txt --single-version-externally-managed --compile --install-headers /tmp/venv/include/site/python3.6/torchvision"" failed with error code 1 in /tmp/pip-req-build-7wrzlist/
```

I suppose it's a bug from the master branch? If so, when can it be fixed?
If not, how can I fix this?
Thanks.

<!-- Add any other context about the problem here. -->
"
VideoClips: audio clips do not correspond to video clips,pytorch/vision,2020-07-15 14:02:02,9,bug#module: video,2474,657375158,"## 🐛 Bug

The audio stream does not correspond to the visual stream when `torchvision.datasets.video_utils.VideoClips` is used.

## To Reproduce

Steps to reproduce the behavior:
1. Here are two videos I tested on [Archive.zip](https://github.com/pytorch/vision/files/7903650/Archive.zip)
2. The code to reproduce

```python
from torchvision.io import read_video
from torchvision.datasets.video_utils import VideoClips

VIDEO_PATH = './4fpkD4A_t1s_35000_45000.mp4'
VIDEO_PATH = './small.mp4'

if __name__ == ""__main__"":
    print(f'I am using: {VIDEO_PATH}')
    print(f'Output using torchvision.io.read_video:')
    visual, audio, info = read_video(VIDEO_PATH, pts_unit='sec')
    print('Visual:', visual.shape, 'Audio:', audio.shape, info)
    
    print(f'Output using torchvision.datasets.video_utils.VideoClips:')
    vclips = VideoClips([VIDEO_PATH], clip_length_in_frames=30, frames_between_clips=30)
    for i in range(vclips.num_clips()):
        visual, audio, info, vid_idx = vclips.get_clip(i)
        print(f'Clip #{i}', 'Visual:', visual.shape, 'Audio:', audio.shape, info)


```
3. The output I see
```
I am using: ./small.mp4
Output using torchvision.io.read_video:
Visual: torch.Size([166, 320, 560, 3]) Audio: torch.Size([1, 266240]) {'video_fps': 30.0, 'audio_fps': 48000}
Output using torchvision.datasets.video_utils.VideoClips:
/home/vladimir/miniconda3/envs/bug_report_video_clips/lib/python3.8/site-packages/torchvision/io/video.py:103: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  warnings.warn(
100.0%
Clip #0 Visual: torch.Size([30, 320, 560, 3]) Audio: torch.Size([1, 86016]) {'video_fps': 30.0, 'audio_fps': 48000}
Clip #1 Visual: torch.Size([30, 320, 560, 3]) Audio: torch.Size([1, 87142]) {'video_fps': 30.0, 'audio_fps': 48000}
Clip #2 Visual: torch.Size([30, 320, 560, 3]) Audio: torch.Size([1, 87255]) {'video_fps': 30.0, 'audio_fps': 48000}
Clip #3 Visual: torch.Size([30, 320, 560, 3]) Audio: torch.Size([1, 0]) {'video_fps': 30.0, 'audio_fps': 48000}
Clip #4 Visual: torch.Size([30, 320, 560, 3]) Audio: torch.Size([1, 0]) {'video_fps': 30.0, 'audio_fps': 48000}
```

```
I am using: ./4fpkD4A_t1s_35000_45000.mp4
Output using torchvision.io.read_video:
Visual: torch.Size([300, 720, 1280, 3]) Audio: torch.Size([2, 440320]) {'video_fps': 30.0, 'audio_fps': 44100}
Output using torchvision.datasets.video_utils.VideoClips:
/home/vladimir/miniconda3/envs/bug_report_video_clips/lib/python3.8/site-packages/torchvision/io/video.py:103: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.
  warnings.warn(
100.0%
Clip #0 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 14336]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #1 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #2 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #3 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #4 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #5 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #6 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #7 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #8 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
Clip #9 Visual: torch.Size([30, 720, 1280, 3]) Audio: torch.Size([2, 15360]) {'video_fps': 30.0, 'audio_fps': 44100}
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior
1. The output of `torchvision.io.read_video` is ok, and it is as expected. I provide it here for reference. Also, the visual streams returned from `torchvision.datasets.video_utils.VideoClips` are ok.
2. I expect the output of `torchvision.datasets.video_utils.VideoClips().get_clip()` to have a comparable number of samples, i.e. 48k or 44.1k for 1 second of 30 fps video. Instead, it outputs more samples than expected or just a fraction of it. Specifically, for `./small.mp4`, it outputs ~87k in the first three clips and 0 in the last two (expected 48k at each clip), while for `4fpkD4A_t1s_35000_45000.mp4` it outputs ~14k for the first one and 15k for the rest of them (expected 44.1k at each). The later one does not even reach the expected 440k samples for the whole 10s video. Similarly, the earlier one totals to `(86016 + 87142 + 87255) = 260413` which does not correspond to `266240` loaded in `torchvision.io.read_video`.

<!-- A clear and concise description of what you expected to happen. -->

## Environment
```
Collecting environment information...
PyTorch version: 1.5.1
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: Could not collect

Nvidia driver version: 440.44
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] torch==1.5.1
[pip3] torchvision==0.6.0a0+35d732a
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] mkl                       2020.1                      217  
[conda] mkl-service               2.3.0            py38he904b0f_0  
[conda] mkl_fft                   1.1.0            py38h23d657b_0  
[conda] mkl_random                1.1.1            py38h0573a6f_0  
[conda] numpy                     1.18.5           py38ha1c710e_0  
[conda] numpy-base                1.18.5           py38hde5b4d6_0  
[conda] pytorch                   1.5.1           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch
[conda] torchvision               0.6.1                py38_cu102    pytorch
```
## Additional context

Currently, `VideoClips` does not have a doc on the website. Therefore, my misunderstanding might arise from its absence."
Label tracking meta-issue (edit me to get automatically CC'ed on issues! cc bot),pytorch/vision,2020-07-09 20:53:25,2,,2447,654341168,"This issue is used by [pytorch-probot](https://github.com/pytorch/pytorch-probot) to manage subscriptions to labels.  To subscribe yourself to a label, add a line `* label @yourusername`, or add your username to an existing line (space separated) in the body of this issue. **DO NOT COMMENT, COMMENTS ARE NOT RESPECTED BY THE BOT.**

As a courtesy to others, please do not edit the subscriptions of users who are not you.

For more examples see (https://github.com/pytorch/pytorch/issues/24422).

* windows @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @mszhanyi
* module: datasets @pmeier
* module: transforms @vfdev-5 @datumbox
* module: ci @seemethere 
* module: build @seemethere 
* module: onnx @neginraoof 
* topic: video @bjuncek 
* topic: classification @vfdev-5 @datumbox 
* topic: semantic segmentation @vfdev-5 @datumbox
* topic: object detection @datumbox
* module: reference scripts @datumbox 
* module: models @datumbox 
* module: rocm @jeffdaily  @jithunnair-amd
* module: tests @pmeier
* prototype @bjuncek @pmeier"
3D NMS and RoiAlign for volumetric data,pytorch/vision,2020-07-07 14:58:53,2,needs discussion#module: ops,2402,652394788,"## 🚀 Feature
3D data gains more and more popularity inside the deep learning community. As a consequence it would be great to have a unified 3D NMS and 3D ROI Align for future and current projects like MONAI . 

## Motivation
[Information](https://github.com/pytorch/vision/pull/2337#issuecomment-654775782) added from @mjorgecardoso 
Medical imaging is a huge field of research, with conferences such as ISMRM (5k+ attendees), MICCAI (2.5k+), ISBI (1.5k+). Volumetric neural network operations (convolutions, pooling, etc), are common and supported in PyTorch (see here https://pytorch.org/docs/master/generated/torch.nn.Conv3d.html).

Spatial dimensions summarised:
N = batch size, C = channels, H = height, W = width, D = depth / T = time

Typically found in **2D**: [N, C, H, W]

Typically found in **2d + time (video)**: [N, C, T, H, W]
Expected behaviour: operations are only applied along the spatial dimensions (H, W) and NOT along T

Typically found in **3d (volumetric)**: [N, C, D, H, W] (sometimes also [N, C, H, W, D] as in [medicaldetectiontoolkit](https://github.com/MIC-DKFZ/medicaldetectiontoolkit))
Expected behaviour: operations are applied along all spatial dimensions (D,H,W)

## Pitch
Add support for NMS and RoiAlign for volumetric data and define the right conventions and proper documentation to make clear which function needs to be used in which case.

For backward compatibility nms and roialign should be kept as an alias for their plain 2d counterparts. Moving forward, there could be two functions nms2d and nms3d (like typically found in pytorch e.g. Conv2d and Conv3d). I'm not quite sure what the optimal way of handling/naming the video case is (maybe a flag inside the 3d versions?).

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context
https://github.com/pytorch/vision/pull/2337
https://github.com/pytorch/vision/issues/1678
@pfjaeger

"
Normalization for object detection,pytorch/vision,2020-07-06 14:38:58,6,module: models#module: documentation,2397,651593020,"Migrated from [discuss.pytorch.org](https://discuss.pytorch.org/t/input-format-for-pretrained-torchvision-models/48759/9). Requests were made by @mattans.

## 📚 Documentation

The reference implementations for classification, segmentation, and video classification all use a normalization transform. In contrast, object detection does not use any normalization.

> 1. Consider explaining why the pretrained detection models are the only ones that don’t require image normalization (I understand that the training set was not normalized. But again, why?)
> 2. Worth mentioning that no normalization is needed. The classification, segmentation and detection pretrained models are trained on ImageNet, so one may think all of them require ImageNet normalization, when in fact only the classification and segmentation models require normalization. Perhaps it’s best to put this info in a table, since the pretrained video models also have a normalization, but different.

"
Supporting more target_type for Cityscapes,pytorch/vision,2020-07-06 09:31:58,0,module: datasets,2395,651391677,"Hi,

as the Cityscapes dataset also provides extra training data e.g. disparity upon request, I was wondering if they should be added also to the `target_type` argument for ease of use as well?

one straightforward extension could be:
```
dataset = Cityscapes('./data/cityscapes', split='train', target_type='disparity')
```

cc @pmeier"
"subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1",pytorch/vision,2020-06-28 12:49:45,14,awaiting response#needs reproduction#topic: build,2360,646911899,"## 🐛 Bug

When I tried to install the vision from source I got the following error:

```
#include <libavcodec/avcodec.h>
          ^~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 1510, in _run_ninja_build
    env=env)
  File ""/home/ahmad/miniconda3/lib/python3.7/subprocess.py"", line 512, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""setup.py"", line 255, in <module>
    'clean': clean,
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 161, in setup
    return distutils.core.setup(**attrs)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
    self.run_command(cmd)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/install.py"", line 67, in run
    self.do_egg_install()
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/install.py"", line 109, in do_egg_install
    self.run_command('bdist_egg')
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/bdist_egg.py"", line 173, in run
    cmd = self.call_command('install_lib', warn_dir=0)
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/bdist_egg.py"", line 159, in call_command
    self.run_command(cmdname)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/install_lib.py"", line 11, in run
    self.build()
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/command/install_lib.py"", line 107, in build
    self.run_command('build_ext')
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/build_ext.py"", line 87, in run
    _build_ext.run(self)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/command/build_ext.py"", line 340, in run
    self.build_extensions()
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 644, in build_extensions
    build_ext.build_extensions(self)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/command/build_ext.py"", line 449, in build_extensions
    self._build_extensions_serial()
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/command/build_ext.py"", line 474, in _build_extensions_serial
    self.build_extension(ext)
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/setuptools/command/build_ext.py"", line 208, in build_extension
    _build_ext.build_extension(self, ext)
  File ""/home/ahmad/miniconda3/lib/python3.7/distutils/command/build_ext.py"", line 534, in build_extension
    depends=ext.depends)
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 473, in unix_wrap_ninja_compile
    with_cuda=with_cuda)
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 1228, in _write_ninja_file_and_compile_objects
    error_prefix='Error compiling objects for extension')
  File ""/home/ahmad/miniconda3/lib/python3.7/site-packages/torch/utils/cpp_extension.py"", line 1524, in _run_ninja_build
    raise RuntimeError(message)
RuntimeError: Error compiling objects for extension
```
** Steps to reproduce **



Steps to reproduce the behavior:

1. Installing Pytorch from source (with cuda 10.2, python 3.7)
1. Cloning vision
1. python setup.py install

## Environment
OS: Ubuntu 20.04 LTS
GCC version: (Ubuntu 8.4.0-3ubuntu2) 8.4.0
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: GPU 0: GeForce GT 740M
Nvidia driver version: 440.33.01
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] torch==1.7.0a0+502ec8f
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.0.130                      0  
[conda] magma-cuda102             2.5.2                         1    pytorch
[conda] mkl                       2020.1                      217  
[conda] mkl-include               2020.1                      217  
[conda] mkl-service               2.3.0            py37he904b0f_0  
[conda] mkl_fft                   1.1.0            py37h23d657b_0  
[conda] mkl_random                1.1.1            py37h0573a6f_0  
[conda] numpy                     1.18.5           py37ha1c710e_0  
[conda] numpy-base                1.18.5           py37hde5b4d6_0  
[conda] torch                     1.7.0a0+502ec8f          pypi_0    pypi
"
Source packages not published to PyPI,pytorch/vision,2020-06-28 10:22:19,3,topic: binaries,2359,646888736,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1. Use Python 3.9
1. pip install torchvision==0.6.0

 ERROR: Could not find a version that satisfies the requirement torchvision==0.6.0 ... (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3)

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

It installs correctly.

## Additional context

Because source packages are not being uploaded to PyPI, pip will go back in history until it finds a version with a source package. The solution is to uploaded source packages for all versions (and Python 3.9 wheels --- when it is released)."
[RFC] Use CMake to compile torchvision in setup.py,pytorch/vision,2020-06-24 13:31:26,10,needs discussion#topic: build,2349,644617822,"TorchVision uses setuptools to compile its C++ / CUDA dependencies, which has been working ok for now.

But with the addition of FFmpeg and libpng / libjpeg-turbo as dependencies, and in the interest to simplify the path from using torchvision ops in C++, it might be worth considering unifying the compilation steps in torchvision to aways go through CMake.
This could potentially simplify / make more robust to locate external libraries (like FFmpeg and libjpeg-turbo

This has been first discussed in https://github.com/pytorch/vision/pull/2253#issuecomment-648219377, and has been brought already in the past by @soumith 

Non-exhaustive list of what is left to do:
- [ ] Optional PNG and JPEG libs (they are required in cmake)
- [ ] Verify that FindPNG / FindJPEG work with our setup on CI
- [ ] Logic to compile independent libs for image and video
- [ ] torchvision_EXPORTS macro definition unconditionally for windows (shouldn't matter too much)
- [ ] rocm/hipify logic
- [ ] option to build tests for C++ models (can be postponed)

cc @bmanga @andfoy for discussion."
Function to_pil_image expects wrong dtype for 'I;16' mode.,pytorch/vision,2020-06-15 15:37:30,1,needs discussion#module: transforms,2322,638950212,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->
Function to_pil_image expects wrong dtype for 'I;16' mode.
According to Pillow [documentation](https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes) it should accept uint16 while torchvision expects int16 to set 'I;16' and don't support uint16 ndarray ([code](https://github.com/pytorch/vision/blob/c2e8a00885e68ae1200eb6440f540e181d9125de/torchvision/transforms/functional.py#L223))

## To Reproduce

Steps to reproduce the behavior:

1. Create uint16 ndarray with image inside
1. Try to convert it to pil image.
1. Get an error that input type is not supported

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
uint16 ndarray should generate proper pil image.

## Environment

<!-- Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```
-->

 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): 1.5/ 0.6.0
 - OS (e.g., Linux): Windows
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.7
 - CUDA/cuDNN version: 10
 - GPU models and configuration: titan rtx, titan v
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->
"
Please add 'keep_channels' flag to make_grid,pytorch/vision,2020-06-12 10:32:09,2,needs discussion#module: utils,2315,637654870,"## 🚀 Feature
For images with 1 channel, it would be useful to tell make_grid to not convert grayscale images to RGB

## Motivation

I just wanted to do a simple MNIST example but torchvision.utils.make_grid modified the data such that it became 3-dimensional RGB. That's cool for color images but I wish there were a simple way to keep the channels the same

## Pitch

if the user passes ""keep_channels=True"" then the number of channels doesn't change

## Alternatives
first i looked for other issues, and found others have the same problem
i tried using cv2 but it had some error,
i tried doing the y = 0.2989 * r + 0.5870 * g + 0.1140 * b math but it didn't look great, 

so eventually i settled on converting the tensor to PIL, 
converting the PIL to grayscale, 
then converting the grayscale image back to a tensor, 
which is a ton of extra code and compute, 
when we could just not convert images to RGB in the first place!

## Additional context

https://github.com/pytorch/vision/issues/863

Thank you for making torchvision"
ONNX export of MaskRCNN: dynamic axes seem broken (for batch size > 1),pytorch/vision,2020-06-10 13:39:52,6,bug#topic: object detection#module: onnx,2311,636255756,"## 🐛 Bug

At export to ONNX, dynamic axes were set and the inputs and outputs named properly. However, the output of inferred images is incorrect and wrongly named. Depending on different batch sizes used at export and inference, the behaviour varies as follows:
Supposing that batch size at export time is `n`, and batch size at inference time is `m`:
1. if `n`==`m`:
Output has length of `n*4`, so ex. if `n=m=3`, output has length of 12. In the onnx runtime session, it looks like in the following screenshot:
![image](https://user-images.githubusercontent.com/24386215/84271948-1278b500-ab2d-11ea-98d6-74ae4a590ee8.png)
So only output of the first image in batch is correctly named.
2. if `n`<`m`: 
Similar behaviour as in 1., but **output for only `n` first images in batch** is returned.
3. if `n`>`m`:
A ""SplitToSequence_XXXX"" error is returned, ex. that one:
```
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running SplitToSequence node. Name:'SplitToSequence_4001' Status Message: split_size_sum (57) != split_dim_size (40)
```
This exception is similar to behaviour listed in #2309 and seems connected.
## To Reproduce

Steps to reproduce the behavior:
1.
Load and export a pretrained MaskRCNN model using `input_tensor` of shape (n, 3, 1024, 1024), for example (4,3,1024,1024):
```
model = torchvision.models.detection.maskrcnn_resnet50_fpn(
            pretrained=False,
            min_size=1024, max_size=1024,
            pretrained_backbone=False,
            num_classes=num_classnames + 1,  # + background class
            image_mean=image_mean,
            image_std=image_std,
)
torch.onnx.export(
        model,
        input_tensor.float(),
        onnx_model_filepath,
        export_params=True,
        opset_version=12,
        do_constant_folding=False,
        input_names=[""images_tensors""],
        output_names=[""boxes"", ""labels"", ""scores"", ""masks""],
        dynamic_axes={""images_tensors"": [0, 1, 2, 3], ""boxes"": [0, 1], ""labels"": [0],
                      ""scores"": [0], ""masks"": [0, 1, 2, 3]},
)
```
2. Load and infer ONNX model on `input_tensor` of shape (m,3,1024,1024), where `m` corresponds to the value in the description above, and different `m` values (bigger, smaller or equal to `n`) will result in different behaviours.
```
input_array = input_tensor.cpu().numpy()
ort_session = onnxruntime.InferenceSession(onnx_model_filepath)
ort_inputs = {""images_tensors"": input_array}
ort_outs = ort_session.run(None, ort_inputs)
outputs = ort_session.get_outputs()
```
These outputs are presented in the screenshot above.
## Expected behavior

With dynamic_axes set properly, I expect:
1. Output length dependent on batch size of the **inferred tensor**, not the one used for export.
2. Output of shape similar to the torch model's output, which is a list (len == batch size) of dictionaries of `boxes`, `labels`, `scores` and `masks`. Also, all outputs correctly named, not like currently in the above screenshot.
3. No exceptions if inferred tensor is of smaller batch size than the one used for export.

## Environment
```
PyTorch version: 1.6.0.dev20200526+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 20.04 LTS
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
CMake version: version 3.16.3

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4

Versions of relevant libraries:
[pip3] numpy==1.18.4
[pip3] torch==1.6.0.dev20200526+cu101
[pip3] torchvision==0.7.0.dev20200526+cu101
[conda] Could not collect
```
Also:
```
ONNX_runtime and ONNX_runtime_gpu==1.3.0
ONNX==1.7.0
```
## Additional context

This seems connected to #2309 and #2251 "
ONNX export of MaskRCNN: inference fails when batch size > 1 and no detections,pytorch/vision,2020-06-10 12:28:42,5,bug#help wanted#topic: object detection#module: onnx,2309,636205414,"## 🐛 Bug

When running inference with MaskRCNN exported to ONNX with batch size bigger than one, an exception is thrown on images with no detections. 
```
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running SplitToSequence node. Name:'SplitToSequence_4427' Status Message: split_size_sum (75) != split_dim_size (72)
```

## To Reproduce

Steps to reproduce the behavior:
NOTE: `input_tensor` is of size (4,6,1024,1024) -> batch size is 4
1. Load and export a pretrained MaskRCNN model:
```
model = torchvision.models.detection.maskrcnn_resnet50_fpn(
            pretrained=False,
            min_size=1024, max_size=1024,
            pretrained_backbone=False,
            num_classes=num_classnames + 1,  # + background class
            image_mean=image_mean,
            image_std=image_std,
)
torch.onnx.export(
        model,
        input_tensor.float(),
        onnx_model_filepath,
        export_params=True,
        opset_version=12,
        do_constant_folding=False,
        input_names=[""images_tensors""],
        output_names=[""boxes"", ""labels"", ""scores"", ""masks""],
        dynamic_axes={""images_tensors"": [0, 1, 2, 3], ""boxes"": [0, 1], ""labels"": [0],
                      ""scores"": [0], ""masks"": [0, 1, 2, 3]},
)
```
2. Infer on an image that has detections (image's values are in range 0.0-1.0):
```
input_array = input_tensor.cpu().numpy()
ort_session = onnxruntime.InferenceSession(onnx_model_filepath)
ort_inputs = {""images_tensors"": input_array}
ort_outs = ort_session.run(None, ort_inputs)
```
Works correctly.

3. Infer on an image that will have no detections, ex. a random one or a black one (making sure that image's values are still in range 0.0-1.0):
```
random_tensor = torch.randn(input_tensor.shape) 
# also tried:
# random_tensor = torch.zeros(input_tensor.shape) 
random_array = random_tensor.cpu().numpy()
ort_session = onnxruntime.InferenceSession(onnx_model_filepath)
ort_inputs = {""images_tensors"": random_array}
ort_outs = ort_session.run(None, ort_inputs)
```
This throws the exception:
```
[E:onnxruntime:, sequential_executor.cc:281 Execute] Non-zero status code returned while running SplitToSequence node. Name:'SplitToSequence_4427' Status Message: split_size_sum (61) != split_dim_size (15)
Traceback (most recent call last):
  File ""/.../maskrcnn_deployment.py"", line 1087, in <module>
    main()
  File ""/.../maskrcnn_deployment.py"", line 948, in main
    onnx_prediction = infer_onnx_model_on_single_image(
  File ""/.../maskrcnn_deployment.py"", line 527, in infer_onnx_model_on_single_image
    ort_outs = ort_session.run(None, ort_inputs)
  File ""/.../python3.8/site-packages/onnxruntime/capi/session.py"", line 111, in run
    return self._sess.run(output_names, input_feed, run_options)
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running SplitToSequence node. Name:'SplitToSequence_4427' Status Message: split_size_sum (61) != split_dim_size (15)
```

## Expected behavior

I would expect analogical behaviour to the one with batch size = 1: empty prediction arrays returned and no exceptions. 

## Environment
```
PyTorch version: 1.6.0.dev20200526+cu101
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 20.04 LTS
GCC version: (Ubuntu 9.3.0-10ubuntu2) 9.3.0
CMake version: version 3.16.3

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4

Versions of relevant libraries:
[pip3] numpy==1.18.4
[pip3] torch==1.6.0.dev20200526+cu101
[pip3] torchvision==0.7.0.dev20200526+cu101
[conda] Could not collect
```
Also:
```
ONNX_runtime and ONNX_runtime_gpu==1.3.0
ONNX==1.7.0
```
## Additional context

With batch size = 1 no such error occurs.
Also, the number in SplitToSequence error varies depending on the batch size used, ex. if batch size was 2, the error would be:
```
onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running SplitToSequence node. Name:'SplitToSequence_3575' Status Message: split_size_sum (22) != split_dim_size (6)
```
Also, it's connected to my previous issue, #2251 "
ImageNet rogue PNG file,pytorch/vision,2020-06-06 06:11:11,1,module: datasets,2294,632238831,"Sorry if this has been addressed here and I can't find the discussion, but it's the same issue as here:
https://github.com/soumith/imagenet-multiGPU.torch/issues/43

The file `train/n02105855/n02105855_2933.JPEG` from ILSVRC 2012 is actually a PNG file. This is triggering the following warning from PIL, and I'm guessing it can't handle this type of error automatically (pillow==7.1.1, torchvision==0.5.0, pytorch=1.4.0).

```
/miniconda3/envs/pytorch/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: 
UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. 
```

Should this be handled in the ImageNet class? Or would you prefer we do it ourselves? 

Thank you!

cc @pmeier"
Add uint8 support for interpolate and grid-sample in PyTorch,pytorch/vision,2020-06-05 09:07:46,4,help wanted#module: transforms#high priority,2289,631437287,"## 🚀 Feature

With the addition of torch tensor support for the transforms (following https://github.com/pytorch/vision/issues/1375), there are two operators that will require extra attention in order for them to be as efficient as the PIL implementations.

Indeed, while we can implement `resize` via `torch.nn.functional.interpolate` and `rotate` via `torch.nn.functional.grid_sample`, those operators for now only support floating-point types, so for now we need to perform a `.float() -> interpolate() -> byte()` in order to maintain compatibility, which is wasteful.

It would be great if they could be extended to support `uint8` (and maybe other integer types) as well.

A first PR adding support for uint8 to `nearest` mode interpolate has been sent in https://github.com/pytorch/pytorch/pull/35029

Something to keep in mind: the interpolate function is under optimization in https://github.com/pytorch/pytorch/pull/34864, so this should be kept in mind to avoid conflicts."
Adding new datasets,pytorch/vision,2020-05-29 18:35:06,2,needs discussion#module: datasets,2271,627461465,"## 🚀 Feature
Are there any plans to add additional datasets that have clear splits between train/valid/test

## Motivation
Easy development and testing of models.

## Pitch

I'd like to see the following [dataset](https://www.tensorflow.org/datasets/catalog/patch_camelyon) in the torchvision datasets.


cc @pmeier"
to_tensor and pil_to_tensor inconsistency,pytorch/vision,2020-05-28 16:55:06,6,needs discussion#module: transforms,2269,626663311,"`to_tensor` uses `from_numpy` which never copies and always returns a CPU tensor. But `pil_to_tensor` uses `as_tensor`, which would return a CUDA tensor if the default tensor type is changed by the user (e.g., https://github.com/pytorch/pytorch/issues/39088). I think a discussion on what the correct behavior is is needed, followed with a patch to make the two functions consistent."
Please complete torchvision for libtorch c++,pytorch/vision,2020-05-27 09:58:32,1,,3017,744880351,"Please complete torchvision for libtorch c++.
like torchvision.ops ,torchvision.models.detection
It is best to provide a compiled torchvision.lib for libtorch.


cc @yf225 @glaringlee @fmassa"
Making `transformation` an optional parameter in FasterRCNN,pytorch/vision,2020-05-27 05:38:08,9,enhancement#help wanted#module: models#topic: object detection,2263,625393408,"Hello,


## 🚀 Feature
I think it would be more generic to have `transform`(https://github.com/pytorch/vision/blob/3d65fc6723f1e0709916f24d819d6e17a925b394/torchvision/models/detection/faster_rcnn.py#L231) as a function that can be modified by users rather than a default one.

## Motivation

I am applying transformations separately as a part of data augmentation, which includes cropping and resizing. Hence I would prefer to not do the twice while retraining `FasterRCNN`. 

## Pitch

I would like to have a fixed size input to be fed into the network for variable-sized images. At present, I do this by resizing the images separately as a part of DataLoader and adjust the parameters of `GeneralizedRCNNTransform` accordingly. 

## Alternatives

My present way of using `FasterRCNN` is an alternative. Since my set of transformations are pre-defined, I have to apply hacks such as setting mean to 0., std to 1. and altering min and max sizes to my default value(this would mean that `scale=1` and interpolation would return the same image.


## Additional context

While the input to the network is fixed size, I apply many other augmentations such as mirror, random cropping etc, inspired by SSD based networks. Hence I would prefer to do all augmentation in a separate place once instead of twice.

Thank you!

Edit : If you think this would be a meaningful change, I will be happy to send a Pull Request. "
Ability to add extra custom roi-heads to generalizedRCNN models,pytorch/vision,2020-05-18 15:05:09,4,needs discussion#module: models#topic: object detection,2229,620284357,"## 🚀 Feature
This feature would allow adding custom RoI heads to any existing GeneralizedRCNN model.

## Motivation

While the current functionalities of existing GeneralizedRCNN models are great, one might want to make extra predictions (like, e.g. the number of sides of an object) per detection, without having to alter the underlying torchvision code.

## Pitch

The idea would be to be able to provide an extensions class (inheriting from RoIHeads), that preserves all the current behaviour but also exposes in the forward pass all the necessary elements (proposals, matched_idxs, labels) for an extra head to compute its own predictions.

## Alternatives

#### EDIT
The alternative below was my initial idea. However, in the meantime I have a found a far simpler solution, which can be found on the first comment of this thread. As such, please feel free to ignore the alternative described here.
#### END OF EDIT

So far I have the current proposal:

- Allow for passing to the constructor of GeneralizedRCNN models (faster_rcnn, mask_rcnn, keypoint_rcnn) a custom transform (similar to GeneralizedRCNNTransform, probably inheriting from it) that handles any necessary transformations to be done to the extra heads' targets (this custom transform might not even be necessary, depending on the extra heads).
- Allow for passing to the constructor of GeneralizedRCNN models (faster_rcnn, mask_rcnn, keypoint_rcnn) an instance of a RoiHeadsExtensions, that would inherit from RoIHeads, preserving all its current behaviour but also exposing in the forward pass all the necessary elements (proposals, matched_idxs, labels) for an extra head to compute its own predictions.

Example (for faster_rcnn):
```python
def __init__(self, backbone, num_classes=None,
                 # transform parameters
                 min_size=800, max_size=1333,
                 image_mean=None, image_std=None,

                 transform=None, # NEW

                 # RPN parameters
                 rpn_anchor_generator=None, rpn_head=None,
                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,
                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,
                 rpn_nms_thresh=0.7,
                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,
                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,
                 # Box parameters
                 box_roi_pool=None, box_head=None, box_predictor=None,
                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,
                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,
                 box_batch_size_per_image=512, box_positive_fraction=0.25,
                 bbox_reg_weights=None,

                 # RoI heads extensions # NEW
                 roi_heads_extensions=None):
```

Adapting existing code to allow for a custom transform would be as simple as changing, in faster_rcnn, from:
```python
if image_mean is None:
    image_mean = [0.485, 0.456, 0.406]
if image_std is None:
    image_std = [0.229, 0.224, 0.225]
transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)

super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)
```
to:
```python
if transform is None:
            if image_mean is None:
                image_mean = [0.485, 0.456, 0.406]
            if image_std is None:
                image_std = [0.229, 0.224, 0.225]
            transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)

super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)
```

As for creating the RoiHeadsExtensions class it would be necessary to change the RoiHeads class in the following way:
- add, at construction time, an internal parameter that identifies if extensions exist, and by default is false.
```python
self.has_extensions = False
```
- change the return of forward from:
```python
return result, losses
```
to
```python
if self.has_extensions:
            return result, losses, (proposals, matched_idxs, labels)
return result, losses
```
thus allowing the extensions to access the proposals, matched_idxs and labels.


Now, the RoiHeadsExtensions class itself would simply hold the extra heads and mimick RoiHeads as much as possible. So far I had in mind something like:
```python
class RoIHeadsExtensions(RoIHeads):
    # Note that depending on your extensions, you might have to create your own GeneralizedRCNNTransform.

    def __init__(self, extensions):
        # type: (List[CustomRoIHead])
        self.extensions = extensions
        super(RoIHeads, self).__init__()


    def add_base(self, roi_heads):
        # type: (RoIHeads)

        self.has_extensions = True

        self.box_similarity   = roi_heads.box_similarity # ISSUE EDIT -> 'roi_heads.box_ops.box_iou' was wrong!
        self.proposal_matcher = roi_heads.proposal_matcher
        self.fg_bg_sampler    = roi_heads.fg_bg_sampler
        self.box_coder        = roi_heads.box_coder

        self.box_roi_pool  = roi_heads.box_roi_pool
        self.box_head      = roi_heads.box_head
        self.box_predictor = roi_heads.box_predictor

        self.score_thresh       = roi_heads.score_thresh
        self.nms_thresh         = roi_heads.nms_thresh
        self.detections_per_img = roi_heads.detections_per_img

        has_mask = roi_heads.has_mask()
        self.mask_roi_pool  = roi_heads.mask_roi_pool if has_mask else None
        self.mask_head      = roi_heads.mask_head if has_mask else None
        self.mask_predictor = roi_heads.mask_predictor if has_mask else None

        has_keypoint = roi_heads.has_keypoint()
        self.keypoint_roi_pool  = roi_heads.keypoint_roi_pool if has_keypoint else None
        self.keypoint_head      = roi_heads.keypoint_head if has_keypoint else None
        self.keypoint_predictor = roi_heads.keypoint_predictor if has_keypoint else None


    def forward(self, features, proposals, image_shapes, targets=None):
        # type: (Dict[str, Tensor], List[Tensor], List[Tuple[int, int]], Optional[List[Dict[str, Tensor]]])
        """"""
        Arguments:
            features (List[Tensor])
            proposals (List[Tensor[N, 4]])
            image_shapes (List[Tuple[H, W]])
            targets (List[Dict])
        """"""
        result, losses, values_for_extension = super(RoIHeadsExtensions, self).forward(features, proposals, image_shapes, targets)

        for extension in self.extensions:
            extension.forward(result, losses, features, image_shapes, targets, values_for_extension) # ISSUE EDIT -> was missing image_shapes!

        return result, losses
```

Which would get updated in fasterrcnn by simply adding
```python
if roi_heads_extensions:
    roi_heads_extensions.add_base(roi_heads)
    roi_heads = roi_heads_extensions
```

to the end of 
```python
roi_heads = RoIHeads(
    # Box
    box_roi_pool, box_head, box_predictor,
    box_fg_iou_thresh, box_bg_iou_thresh,
    box_batch_size_per_image, box_positive_fraction,
    bbox_reg_weights,
    box_score_thresh, box_nms_thresh, box_detections_per_img)
```
yielding:
```python
roi_heads = RoIHeads(
    # Box
    box_roi_pool, box_head, box_predictor,
    box_fg_iou_thresh, box_bg_iou_thresh,
    box_batch_size_per_image, box_positive_fraction,
    bbox_reg_weights,
    box_score_thresh, box_nms_thresh, box_detections_per_img)
if roi_heads_extensions:
    roi_heads_extensions.add_base(roi_heads)
    roi_heads = roi_heads_extensions
```

As far as I see, this would preserve the existing behaviour of all 3 models and would require minimal changes to mask_rcnn and keypoint_rcnn (just its own parameters and the call to super(), i.e. faster_rcnn), some also small, albeit larger, changes to faster_rcnn (that still preserve its current behaviour) and some more significant changes to roi_heads.py, that nonetheless still preserve its current behaviour.
"
Image augmentation with bounding boxes.,pytorch/vision,2020-05-14 10:39:00,0,module: transforms#topic: object detection,2213,618120985,"## 🚀 Feature

Image augmentation with bounding boxes.

## Motivation

Torchvision already has data augmentation for images, but I think it would be convinient to support images with bounding boxes as well.

Here is a jupyter notebook with an example: github.com/maximlopin/boxaug/blob/master/example.ipynb

## Implementation

One way to implement this is to add an optional argument `bboxes` to `__call__` methods, and some transforms will just ignore it (e.g. ColorJitter).

## Alternatives

Wihout having this built-in, you have to use and learn other libraries. Torchvision already has a simple interface for image augmentation, and adapting bounding boxes support to the same interface would be very convinient.

cc @vfdev-5"
Float PILImage not converted as writeable,pytorch/vision,2020-05-07 18:54:44,8,,2194,614272264,"## 🐛 Bug

When we have a float PIL-Image (e.g. mode='F'), e.g. for the purpose of applying transforms to it, and finally convert it with `ToTensor` then it will print the warning

> /opt/conda/conda-bld/pytorch_1587428094786/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.

The writeability of the numpy array got lost somewhere when converting a numpy array to PILImage with `Image.fromarray(numpy_array, mode='F')` and then after some transforms to a tensor with `ToTensor`.
This does not happen with PIL Images other than float (e.g. mode='RGB').

This warning is especially annoying since it gets printed every epoch.

## To Reproduce

Steps to reproduce the behavior:

```
    from torchvision.transforms import ToTensor
    import numpy as np
    from PIL import Image
    a = np.array([[1.0,0.5], [1.0,0.5]])
    print(a.flags.writeable)
    pilimg = Image.fromarray(a, mode='F')
    tensor = ToTensor()(pilimg)
    print(tensor.numpy().flags)

    b = np.asarray(pilimg)
    c = np.array(pilimg)
    print(b.flags)
    print(c.flags)
```

This code will print above warning.

Also note the following:

- the numpy array b is NOT writeable, the numpy array c is writeable.
- This suggests that the error is located in the conversion from numpy to PIL.
- But since I do not have the easy possibility to convert PIL to numpy and then to tensor within a transforms Compose and since numpy array c is writeable, I open this issue here.

As workaround I do the following:

```
class ToNumpy(object):
    def __call__(self, sample):
        return np.array(sample)

def fix_compose_transform(transform):
        if isinstance(transform.transforms[-1], torchvision.transforms.ToTensor):
            transform = torchvision.transforms.Compose([
                *transform.transforms[:-1],
                ToNumpy(),
                torchvision.transforms.ToTensor()
            ])
        return transform
```


## Expected behavior

Warning is not printed and ToTensor method can deal with the misbehaviour of PIL image.

## Environment

```
Collecting environment information...
PyTorch version: 1.5.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.10.2

Python version: 3.8
Is CUDA available: Yes
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce GTX 970
Nvidia driver version: 418.87.01
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.3
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.1.243             h6bb024c_0  
[conda] mkl                       2020.0                      166  
[conda] mkl-service               2.3.0            py38he904b0f_0  
[conda] mkl_fft                   1.0.15           py38ha843d7b_0  
[conda] mkl_random                1.1.0            py38h962f231_0  
[conda] numpy                     1.18.1           py38h4f9e942_0  
[conda] numpy-base                1.18.1           py38hde5b4d6_1  
[conda] numpydoc                  0.9.2                      py_0  
[conda] pytorch                   1.5.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] pytorch3d                 0.1.1                    pypi_0    pypi
[conda] torchvision               0.6.0                py38_cu101    pytorch

```"
"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)",pytorch/vision,2020-05-07 08:50:17,11,enhancement#help wanted#module: models#topic: object detection,2192,613885232,"When coco dataset has empty bbox annotation 
following error occured.
How should I handle the empty bbox dataset in cocodataset 

IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

> [{'boxes': tensor([], device='cuda:1'), 'labels': tensor([], device='cuda:1', dtype=torch.int64), 'image_id': tensor([311877], device='cuda:1'), 'area': tensor([], device='cuda:1'), 'iscrowd': tensor([], device='cuda:1', dtype=torch.int64)}]
> len_> 0

```
class CocoDataset(torch.utils.data.Dataset):
    def __init__(self, root, annotation, transforms=None):

        self.root = root
        self.transforms = transforms
        self.coco = COCO(annotation)
        self.ids = list(sorted(self.coco.imgs.keys()))

    def __getitem__(self, index):
        # Own coco file
        coco = self.coco
        # Image ID
        img_id = self.ids[index]
        # List: get annotation id from coco
        ann_ids = coco.getAnnIds(imgIds=img_id)
        # Dictionary: target coco_annotation file for an image
        coco_annotation = coco.loadAnns(ann_ids)
        # path for input image
        path = coco.loadImgs(img_id)[0]['file_name']
        # open the input image
        img = Image.open(os.path.join(self.root, path))

        # number of objects in the image
        num_objs = len(coco_annotation)

        # Bounding boxes for objects
        # In coco format, bbox = [xmin, ymin, width, height]
        # In pytorch, the input should be [xmin, ymin, xmax, ymax]
        boxes = []
        for i in range(num_objs):
            xmin = coco_annotation[i]['bbox'][0]
            ymin = coco_annotation[i]['bbox'][1]
            xmax = xmin + coco_annotation[i]['bbox'][2]
            ymax = ymin + coco_annotation[i]['bbox'][3]
            boxes.append([xmin, ymin, xmax, ymax])
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        # Labels (In my case, I only one class: target class or background)
        labels = torch.ones((num_objs,), dtype=torch.int64)
        # Tensorise img_id
        img_id = torch.tensor([img_id])
        # Size of bbox (Rectangular)
        areas = []
        for i in range(num_objs):
            areas.append(coco_annotation[i]['area'])
        areas = torch.as_tensor(areas, dtype=torch.float32)
        # Iscrowd
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)

        # Annotation is in dictionary format
        my_annotation = {}
        my_annotation[""boxes""] = boxes
        my_annotation[""labels""] = labels
        my_annotation[""image_id""] = img_id
        my_annotation[""area""] = areas
        my_annotation[""iscrowd""] = iscrowd

        if self.transforms is not None:
            img = self.transforms(img)
        return img, my_annotation


def resize_boxes(boxes, original_size, new_size):
    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(new_size, original_size))
    ratio_height, ratio_width = ratios
    #print(""============================================================="")
    #print('ratio_height->',ratio_height,'ratio_width->',ratio_width,'boxes->',boxes)
    print('len_>',len(boxes))

    xmin, ymin, xmax, ymax = boxes.unbind(1)
    xmin = xmin * ratio_width
    xmax = xmax * ratio_width
    ymin = ymin * ratio_height
    ymax = ymax * ratio_height

    return torch.stack((xmin, ymin, xmax, ymax), dim=1)
```"
"Implementing and training SE-ResNet and SE-ResNeXt and including them in ""torchvision.models""",pytorch/vision,2020-05-04 14:50:01,1,module: models#new feature,2179,611935078,"## 🚀 Feature
Inclusion of pretrained SE-ResNet and SE-ResNeXt models in torchvision.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
SE-ResNet and SE-ResNeXt outperform ResNet and ResNeXt respectively and have been released around 2 years ago already. 

## Pitch

<!-- A clear and concise description of what you want to happen. -->
Implementing SE blocks and creating ResNet and ResNeXt models that include them, which will be called SE-ResNet and SE-ResNeXt and will be available as a part of `torchvision.models`. The [SE paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf) has been published in CVPR 2018.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->

## Additional context

<!-- Add any other context or screenshots about the feature request here. -->
"
"windows maskrcnn error: unresolved external symbol ""class at::Tensor __cdecl nms_cpu(class at::Tensor const &,class at::Tensor const &,float)"" (?nms_cpu@@YA?AVTensor@at@@AEBV12@0M@Z)",pytorch/vision,2020-04-24 10:34:05,22,help wanted#windows#module: ops#module: c++ frontend,2139,606214389,"I built torchvision pulled from master using : 
```
cmake -DCMAKE_PREFIX_PATH=D:/Garage/onnx-deploy/libtorchd  -DCMAKE_INSTALL_PREFIX=D:/vision/Debug -DCMAKE_BUILD_TYPE=Debug ..
cmake --build . --config Debug 
cmake --install .
```
Then `torch.jit.script(model)` and saved  mask-rcnn model from torchvision.
Errors occured in my C++ project, firstly `Unknown builtin op: torchvision::_new_empty_tensor_op`, then I modified my code according to #1407,  added lines like `#include ""torchvision/nms.h""` and `torch::RegisterOperators().op(""torchvision::nms"", &nms)` , but it won't work.
The error became `unresolved external symbol ""class at::Tensor __cdecl nms_cpu(class at::Tensor const &,class at::Tensor const &,float)"" (?nms_cpu@@YA?AVTensor@at@@AEBV12@0M@Z)`.
The libtorch version is 1.5.0 and torchvision is pulled from master yesterday.
Any clue how I can get mask rcnn running? Similar issues are many but little to help,  I really get confused about how to modify CMakeList.txt of TorchVision. 



cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @smartcat2010 @mszhanyi"
Anchors not being properly distributed across feature maps?,pytorch/vision,2020-04-23 15:35:42,3,enhancement#help wanted#topic: object detection#module: documentation,2135,605638332,"*Edit: based on the rpn.py documentation, it sounds like this may be intended behavior for RPN - in which case this is really more of an issue in faster_rcnn, in the way that its default anchor sizes are specified. RPN docs note that ""sizes[i] and aspect_ratios[i] can have an arbitrary number of elements,"" but this doesn't seem to be totally true - it looks like sizes[i] must have the same number of elements for all feature maps.

https://github.com/pytorch/vision/blob/d6ee8757eca7b74b98e5f0d434a565eb7b1c410b/torchvision/models/detection/rpn.py#L122

This zip statement will result in anchors being split up across each feature map, as opposed to computing all anchors at each feature map. For example, if anchor sizes are defined as ((8,), (16,), (32,), (64,), (128,)), the first feature map will only be processed using anchors size 8, the second feature map size 16, etc. Specifying a number of anchor sizes that is different from the number of feature maps will result in a crash, since the number of anchors and output predictions will not match."
About RegisterOperators in C++,pytorch/vision,2020-04-23 10:11:23,23,help wanted#module: ops#module: c++ frontend#torchscript,2134,605409187,"## 🐛 Bug
I'm testing about jit in C++ ( libtorch )

My test cases
```
int main(int argc, const char* argv[]) {
    auto& ops = torch::jit::getAllOperators();
    std::cout << ""torch jit operators\n"";
    for (auto& op: ops) {
        auto& name = op->schema().name();
        if (name.find(""torchvision"") != std::string::npos)
            std::cout << ""op : "" << op->schema().name() << ""\n"";
    }
    std::cout << ""\n"";
    return 0;
}

>>> torch jit operators
```

Expected (I changed some codes in vision)
```
>>> torch jit operators
op : torchvision::_cuda_version
op : torchvision::deform_conv2d
op : torchvision::ps_roi_pool
op : torchvision::ps_roi_align
op : torchvision::_new_empty_tensor_op
op : torchvision::roi_pool
op : torchvision::roi_align
op : torchvision::nms
```

In 1st case, seems no jit opeators registry on libtorch.

Is this only occurs my env? or some codes are wrong?

Because I checked expected output when I changed srcs.

## Environment

Install command follows on `reamdme`

torch 1.5.0 and torchvision 0.6.0
"
NotImplementedError: Dilation > 1 not supported in BasicBlock on Resnet,pytorch/vision,2020-04-20 12:03:54,7,module: models#topic: classification,2121,603185581,"## 🐛 Bug

I tried to obtain a deeplabv3_resnet34 version based on the code that you are using for getting the resnet50 and resnet101 version

## To Reproduce

```
def deeplabv3_resnet34(pretrained=False, progress=True,
                       num_classes=21, aux_loss=None, **kwargs):
    """"""Constructs a DeepLabV3 model with a ResNet-34 backbone.

    Args:
        pretrained (bool): If True, returns a model pre-trained on COCO train2017 which
            contains the same classes as Pascal VOC
        progress (bool): If True, displays a progress bar of the download to stderr
    """"""
    return _load_model('deeplabv3', 'resnet34', pretrained, progress, num_classes, aux_loss, **kwargs)
```

## Expected behavior

I expected it to being created correctly. However, it throws the next error:

```
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-21-71ba71ca9be2> in <module>
----> 1 model=deeplabv3_resnet34(pretrained=False,num_classes=2)
      2 model.train()

~/Documents/TFG/seg/models/torchvision.py in deeplabv3_resnet34(pretrained, progress, num_classes, aux_loss, **kwargs)
     91         progress (bool): If True, displays a progress bar of the download to stderr
     92     """"""
---> 93     return _load_model('deeplabv3', 'resnet34', pretrained, progress, num_classes, aux_loss, **kwargs)
     94 
     95 def deeplabv3_resnet50(pretrained=False, progress=True,

~/Documents/TFG/seg/models/torchvision.py in _load_model(arch_type, backbone, pretrained, progress, num_classes, aux_loss, **kwargs)
     45     if pretrained:
     46         aux_loss = True
---> 47     model = _segm_resnet(arch_type, backbone, num_classes, aux_loss, **kwargs)
     48     if pretrained:
     49         arch = arch_type + '_' + backbone + '_coco'

~/Documents/TFG/seg/models/torchvision.py in _segm_resnet(name, backbone_name, num_classes, aux, pretrained_backbone)
     18     backbone = resnet.__dict__[backbone_name](
     19         pretrained=pretrained_backbone,
---> 20         replace_stride_with_dilation=[False, True, True])
     21 
     22     return_layers = {'layer4': 'out'}

~/anaconda3/envs/seg/lib/python3.7/site-packages/torchvision/models/resnet.py in resnet34(pretrained, progress, **kwargs)
    247     """"""
    248     return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,
--> 249                    **kwargs)
    250 
    251 

~/anaconda3/envs/seg/lib/python3.7/site-packages/torchvision/models/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)
    218 
    219 def _resnet(arch, block, layers, pretrained, progress, **kwargs):
--> 220     model = ResNet(block, layers, **kwargs)
    221     if pretrained:
    222         state_dict = load_state_dict_from_url(model_urls[arch],

~/anaconda3/envs/seg/lib/python3.7/site-packages/torchvision/models/resnet.py in __init__(self, block, layers, num_classes, zero_init_residual, groups, width_per_group, replace_stride_with_dilation, norm_layer)
    148                                        dilate=replace_stride_with_dilation[0])
    149         self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
--> 150                                        dilate=replace_stride_with_dilation[1])
    151         self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
    152                                        dilate=replace_stride_with_dilation[2])

~/anaconda3/envs/seg/lib/python3.7/site-packages/torchvision/models/resnet.py in _make_layer(self, block, planes, blocks, stride, dilate)
    191             layers.append(block(self.inplanes, planes, groups=self.groups,
    192                                 base_width=self.base_width, dilation=self.dilation,
--> 193                                 norm_layer=norm_layer))
    194 
    195         return nn.Sequential(*layers)

~/anaconda3/envs/seg/lib/python3.7/site-packages/torchvision/models/resnet.py in __init__(self, inplanes, planes, stride, downsample, groups, base_width, dilation, norm_layer)
     45             raise ValueError('BasicBlock only supports groups=1 and base_width=64')
     46         if dilation > 1:
---> 47             raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")
     48         # Both self.conv1 and self.downsample layers downsample the input when stride != 1
     49         self.conv1 = conv3x3(inplanes, planes, stride)

NotImplementedError: Dilation > 1 not supported in BasicBlock
```


cc @vfdev-5"
Resnet34 as backbone for segmentation models,pytorch/vision,2020-04-20 10:49:16,5,enhancement#help wanted#module: models#topic: semantic segmentation,2120,603140203,"## 🚀 Feature

I would like segmentation models with resnet34 backbone

## Motivation

They have added resnet50 backbone.
"
Minimum CMake version required to build.,pytorch/vision,2020-04-09 09:56:15,3,module: c++ frontend,2085,597165082,"It seems like there are a sizable number of issues being created where people can't successfully compile the C++ code due to old CMake (#2076, #2001, #2045 ).
#2076 and #2001 are due to cmake forwarding unknown compiler flags to nvcc, and #2045 because a required CMake module (FindPython3) is only available on CMake 3.12+.
#2076 is fixed, accoding to the author, by upgrading to CMake 3.13.2 so I would expect #2001 to also be fixed with that version.

However 3.13 seems like a fairly high version requirement for the project, as that is beyond the current Ubuntu LTS default CMake (which is 3.10), even though CMake itself is quite easy to upgrade (prebuilt binaries are available on their website, and packages are also available on pip and conda).

I have WIP backport of the FindPython3 module and with that I manage to successfully compile torchvision with CMake 3.10 - I can't personally reproduce the errors in #2076 and #2001.

My idea is then to land the FindPython3 Module backport and require the minimum CMake version to be 3.10, and to tell people that experience nvcc-related build failures to upgrade their version to at least 3.13.
@fmassa what do you think?
"
does deformable conv support export to onnx?,pytorch/vision,2020-04-06 09:03:05,3,enhancement#help wanted#module: ops#module: onnx,2066,594932878,Does deformable conv support export to onnx or not?
C++ API Build instructions result in an error,pytorch/vision,2020-04-02 00:11:46,5,bug#module: c++ frontend,2045,592275595,"## 🐛 Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Following the current instructions in the README to build the C++ API result in an error.

Repro:
```
mkdir build
cd build
# Add -DWITH_CUDA=on support for the CUDA if needed
cmake ..
make
make install
```

Resulting Error:
```
CMake Warning at CMakeLists.txt:13 (find_package):
  By not providing ""FindPython3.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""Python3"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""Python3"" with any
  of the following names:

    Python3Config.cmake
    python3-config.cmake

  Add the installation prefix of ""Python3"" to CMAKE_PREFIX_PATH or set
  ""Python3_DIR"" to a directory containing one of the above files.  If
  ""Python3"" provides a separate development package or SDK, be sure it has
  been installed.


CMake Error at CMakeLists.txt:14 (find_package):
  By not providing ""FindTorch.cmake"" in CMAKE_MODULE_PATH this project has
  asked CMake to find a package configuration file provided by ""Torch"", but
  CMake did not find one.

  Could not find a package configuration file provided by ""Torch"" with any of
  the following names:

    TorchConfig.cmake
    torch-config.cmake

  Add the installation prefix of ""Torch"" to CMAKE_PREFIX_PATH or set
  ""Torch_DIR"" to a directory containing one of the above files.  If ""Torch""
  provides a separate development package or SDK, be sure it has been
  installed.


-- Configuring incomplete, errors occurred!
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

Build completes successfully

## Environment

Please copy and paste the output from our
[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)
(or fill out the checklist below manually).

You can get the script and run it with:
```
wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
# For security purposes, please check the contents of collect_env.py before running it.
python collect_env.py
```
```
PyTorch version: 1.5.0
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.8
Is CUDA available: No
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 418.116.00
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.5.0
[conda] blas                      1.0                         mkl
[conda] mkl                       2020.0                      166
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.0.15           py38ha843d7b_0
[conda] mkl_random                1.1.0            py38h962f231_0
[conda] pytorch                   1.5.0           py3.8_cuda10.2.89_cudnn7.6.5_0    pytorch-test
```
## Additional context

<!-- Add any other context about the problem here. -->

cc @chauhang "
Reminder: bytes is a valid type for torch._six.string_classes,pytorch/vision,2020-04-01 05:37:27,1,enhancement#help wanted#needs discussion,2038,591639668,"This is a reminder for the following (from https://github.com/pytorch/vision/pull/2017#discussion_r400275068):

> [W]hy [do] we include `bytes` [in `torch._six.string_classes`] for [python3](https://github.com/pytorch/pytorch/blob/e3daf7018499fbcb52de4018543fd9a0306ccca6/torch/_six.py#L49)? For python2 `torch._six.string_classes` is simply [`basestring`](https://github.com/pytorch/pytorch/blob/e3daf7018499fbcb52de4018543fd9a0306ccca6/torch/_six.py#L47), which according to the [`six` documentation](https://six.readthedocs.io/index.html#six.string_types) is just `str` in python3.

@fmassa suggested:

> [...] it might be because we were dealing with output of a few IO functions in Python might be `str` or `bytes`, depending on how you opened the file.

---

Shouldn't this be an issue in [`torch`](/pytorch/pytorch) since it directly is related to `torch._six`?
"
One-to-many transforms for image datasets,pytorch/vision,2020-03-31 15:48:17,1,module: transforms,2043,591913125,"## 🚀 Feature
It would be convenient if the standard torchvision datasets could handle one-to-many transforms more seamlessly.

## Motivation
<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
Currently there are two built-in examples of this, FiveCrop and TenCrop, which take in a PIL image and return a tuple of images. The documentation recommends handling these by stacking them with a lambda function, i.e.
```
transform = Compose([
    FiveCrop(size), # this is a list of PIL Images
    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor
])
```
However this makes it more difficult to add further transformations to the Compose, and more significantly requires extra work in the train/test loop to reshape the batch.

## Pitch
<!-- A clear and concise description of what you want to happen. -->
I suggest a kind of ""flat transform"" which would work like a flatMap function in some functional languages, i.e. transform each input into a sequence and combine them into a single output sequence. There would be an extra parameter ""flat_transform"" on image datasets which would take such a function instead of (or maybe in addition to) a normal transform. There would also be a ""FlatCompose"" transform which would act like Compose but for flat transforms, i.e. it would apply the first transform to a single input, and each additional transform to each output, flattening each time. Iterating over the whole dataset would chain the flattened outputs from all inputs.

As a special case, transforms that do not return a list or tuple would be treated as returning a list of size one, so you can mix one-to-one and one-to-many transforms in FlatCompose.

An example could look like,
```
flat_transform = FlatCompose([
    Resize(256),
    FiveCrop(224),
    AddFlipped(), # Transform that returns both original and flipped image
    AddRotated(90), # Transform that returns original image and 90-degree rotated image
    ToTensor(),
    Normalize(mean, std),
])
```
One problem is that most datasets are random-access, which would be impossible if each image could be transformed into an arbitrary number of outputs. To solve this we could require flat transforms to have a constant ""expansion factor"" which would be passed to the dataset as well (maybe in a tuple to the flat_transform parameter or as a separate parameter). The example flat transform would have expansion factor 5\*2\*2=20. Then random access would be implemented as:
```
def __getitem__(self, index):
    return flat_transform(data[index//expansion_factor])[index % expansion_factor]
```


cc @vfdev-5"
Type annotations,pytorch/vision,2020-03-28 09:38:51,44,enhancement#help wanted,2025,589542663,"## 🚀 Feature

Type annotations.

## Motivation

Right now, if a project depends on `torchvision` and you want to static type check it, you have no choice but either ignore it or write your own stubs. `torch` already has partial support for type annotations. 

## Pitch

We could add type annotations for `torchvision` as well.

## Additional context

If we want this, I would take that up.
"
Use multiple processes when extracting ImageNet training archive,pytorch/vision,2020-03-27 06:17:42,0,module: datasets,2023,588913798,"## 🚀 Feature

Use multiple processes when extracting `ImageNet` training archive.

## Motivation

I recently extracting the `ImageNet` training archive with the code of `torchvision` and was suprised how long it took. I realised that after extracting the main archive, we only extract the subarchives one after another:

https://github.com/pytorch/vision/blob/3c254fb7af5f8af252c24e89949c54a3461ff0be/torchvision/datasets/imagenet.py#L183-L184

## Pitch

I think we can speed that up significantly by using multiple processes to do this simultaneously.  IMO doing this would have no drawbacks. 

## Additional context

If we want this feature, I could take it up, albeit with a low priority.


cc @pmeier"
nvcc fatal : unknown option 'Wall' during 'make' torchvision with -DWITH_CUDA=ON,pytorch/vision,2020-03-20 16:21:35,22,help wanted#topic: build#module: c++ frontend,2001,585182708,"Hey!

I try to make vision with CUDA but I get error during make procedure:
My cuda compilation tools: realease 10.0, V10.0.130

Make error output:
[ 25%] Building CUDA object CMakeFiles/torchvision.dir/torchvision/csrc/cuda/ROIAlign_cuda.cu.o
CMakeFiles/torchvision.dir/build.make:296: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/cuda/PSROIPool_cuda.cu.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/cuda/PSROIPool_cuda.cu.o] Error 1
make[2]: *** Waiting for unfinished jobs....
nvcc fatal   : Unknown option 'Wall'
nvcc fatal   : Unknown option 'Wall'
nvcc fatal   : Unknown option 'Wall'
CMakeFiles/torchvision.dir/build.make:270: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/cuda/DeformConv_cuda.cu.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/cuda/DeformConv_cuda.cu.o] Error 1
CMakeFiles/torchvision.dir/build.make:322: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/cuda/ROIPool_cuda.cu.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/cuda/ROIPool_cuda.cu.o] Error 1
CMakeFiles/torchvision.dir/build.make:335: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/cuda/nms_cuda.cu.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/cuda/nms_cuda.cu.o] Error 1
nvcc fatal   : Unknown option 'Wall'
CMakeFiles/torchvision.dir/build.make:283: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/cuda/PSROIAlign_cuda.cu.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/cuda/PSROIAlign_cuda.cu.o] Error 1
nvcc fatal   : Unknown option 'Wall'
CMakeFiles/torchvision.dir/build.make:309: recipe for target 'CMakeFiles/torchvision.dir/torchvision/csrc/cuda/ROIAlign_cuda.cu.o' failed
make[2]: *** [CMakeFiles/torchvision.dir/torchvision/csrc/cuda/ROIAlign_cuda.cu.o] Error 1
CMakeFiles/Makefile2:75: recipe for target 'CMakeFiles/torchvision.dir/all' failed
make[1]: *** [CMakeFiles/torchvision.dir/all] Error 2
Makefile:129: recipe for target 'all' failed
make: *** [all] Error 2

P.S.
I was able to fix this error after remove ""-Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp"" params in flags.make"
Unable to export detection model to ONNX,pytorch/vision,2020-03-19 11:28:12,9,bug#help wanted#topic: object detection#module: onnx#dependency issue,1995,584344846,"## 🐛 Bug

I was trying to convert the pretrained `resnet50_fpn` detection model to `onnx` but I got a runtime error while trying it. 

## To Reproduce
```python
import os
import numpy as np
import matplotlib.pyplot as plt

import torch
import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor

# load model
model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model.eval()

#converison
batch_size=1
x = torch.randn(batch_size, 3, 300, 300, requires_grad=True)
outputs = ['boxes', 'labels', 'scores', 'masks']
torch.onnx.export(model,                     # model being run
                  x,                         # model input (or a tuple for multiple inputs)
                  ""resnet50detection.onnx"",  # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=10,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = ['input'],   # the model's input names
                  output_names = outputs,    # the model's output names
                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes
                                'output' : {0 : 'batch_size'}})

```

## Expected behavior
An onnx format based converted model

## Environment
 - PyTorch / torchvision Version (e.g., 1.0 / 0.4.0): 1.4/0.5
 - OS (e.g., Linux): Linux
 - How you installed PyTorch / torchvision (`conda`, `pip`, source): conda 
 - Build command you used (if compiling from source):
 - Python version: 3.7
 - CUDA/cuDNN version: N/A
 - GPU models and configuration: N/A
 - Any other relevant information:

## Additional context
Here is the error I am getting
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-6-ce14949e7fa3> in <module>
     11                   output_names = outputs,    # the model's output names
     12                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes
---> 13                                 'output' : {0 : 'batch_size'}})

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
    146                         operator_export_type, opset_version, _retain_param_name,
    147                         do_constant_folding, example_outputs,
--> 148                         strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
    149 
    150 

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs)
     64             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,
     65             example_outputs=example_outputs, strip_doc_string=strip_doc_string,
---> 66             dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)
     67 
     68 

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size)
    414                                                         example_outputs, propagate,
    415                                                         _retain_param_name, do_constant_folding,
--> 416                                                         fixed_batch_size=fixed_batch_size)
    417 
    418         # TODO: Don't allocate a in-memory string for the protobuf

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size)
    294     graph = _optimize_graph(graph, operator_export_type,
    295                             _disable_torch_constant_prop=_disable_torch_constant_prop,
--> 296                             fixed_batch_size=fixed_batch_size, params_dict=params_dict)
    297 
    298     if isinstance(model, torch.jit.ScriptModule) or isinstance(model, torch.jit.ScriptFunction):

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict)
    133         torch._C._jit_pass_erase_number_types(graph)
    134 
--> 135         graph = torch._C._jit_pass_onnx(graph, operator_export_type)
    136         torch._C._jit_pass_lint(graph)
    137 

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/__init__.py in _run_symbolic_function(*args, **kwargs)
    177 def _run_symbolic_function(*args, **kwargs):
    178     from torch.onnx import utils
--> 179     return utils._run_symbolic_function(*args, **kwargs)
    180 
    181 

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, n, inputs, env, operator_export_type)
    655                                   .format(op_name, opset_version, op_name))
    656                 op_fn = sym_registry.get_registered_op(op_name, '', opset_version)
--> 657                 return op_fn(g, *inputs, **attrs)
    658 
    659         elif ns == ""prim"":

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py in wrapper(g, *args)
    126             # some args may be optional, so the length may be smaller
    127             assert len(arg_descriptors) >= len(args)
--> 128             args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]
    129             return fn(g, *args)
    130         # In Python 2 functools.wraps chokes on partially applied functions, so we need this as a workaround

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py in <listcomp>(.0)
    126             # some args may be optional, so the length may be smaller
    127             assert len(arg_descriptors) >= len(args)
--> 128             args = [_parse_arg(arg, arg_desc) for arg, arg_desc in zip(args, arg_descriptors)]
    129             return fn(g, *args)
    130         # In Python 2 functools.wraps chokes on partially applied functions, so we need this as a workaround

~/miniconda3/envs/torchenv/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py in _parse_arg(value, desc)
     79                 if v.node().kind() != 'onnx::Constant':
     80                     raise RuntimeError(""Failed to export an ONNX attribute '"" + v.node().kind() +
---> 81                                        ""', since it's not constant, please try to make ""
     82                                        ""things (e.g., kernel size) static if possible"")
     83             return [int(v.node()['value']) for v in value.node().inputs()]

RuntimeError: Failed to export an ONNX attribute 'onnx::Sub', since it's not constant, please try to make things (e.g., kernel size) static if possible
```"
Missing second order derivatives for operations like RoIAlign and DeformConv,pytorch/vision,2020-03-13 23:01:13,5,enhancement#help wanted#module: ops,1982,580917549,"## 🐛 Bug

The `RoIAlign` operation and `DeformConv2d` operation do not support second order derivatives right now. Thus when trying to compute higher order derivatives (a common case in meta learning), it raises an error. 


A snippet to demonstrate this:
```
import torch
import torch.nn as nn
import torch.autograd as autograd
from torchvision.ops import DeformConv2d

normal_conv = nn.Conv2d(3, 5, 3, 1, 1).cuda()
deform_conv = DeformConv2d(3, 5, 3, 1, 1).cuda()

input = torch.rand(1, 3, 10, 10, requires_grad=True).cuda()
offset = torch.rand(1, 2*1*3*3, 10, 10, requires_grad=True).cuda()

# verify gradient of gradient for normal conv
out1 = normal_conv(input)
grad = autograd.grad(out1.sum(), input, create_graph=True)
loss = sum(g.sum() for g in grad)
loss.backward() # this works fine
print('Succeed in computing second order derivative for normal convs')

# deform conv
out2 = deform_conv(input, offset)
grad = autograd.grad(out2.sum(), [input, offset], create_graph=True)
loss = sum(g.sum() for g in grad)
loss.backward() # an error happens here
```

## Expected behavior

Compute second order derivatives for these ops as well. 

## Environment

PyTorch version: 1.4.0
Is debug build: No
CUDA used to build PyTorch: 10.0

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: Yes
CUDA runtime version: 10.0.130
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 430.50
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.17.2
[conda] blas                      1.0                         mkl
[conda] mkl                       2020.0                      166
[conda] mkl-service               2.3.0            py37he904b0f_0
[conda] mkl_fft                   1.0.15           py37ha843d7b_0
[conda] mkl_random                1.1.0            py37hd6b4f25_0
[conda] pytorch                   1.4.0           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch
[conda] torchvision               0.5.0                py37_cu100    pytorch


## Additional context
I would be happy to help if someone can point out how to modify the code to support second order derivative. 
"
Error building C++ backend,pytorch/vision,2020-03-10 14:18:44,11,needs reproduction#topic: build#module: c++ frontend,1958,578613609,"Trying version 0.5.0 (but same with 0.4.2), first I get:

```
/usr/lib/gcc/x86_64-pc-linux-gnu/9.2.0/include/g++-v9/x86_64-pc-linux-gnu/bits/c++config.h:273:27: error: #if with no expression
  273 | #if _GLIBCXX_USE_CXX11_ABI
      |                           ^
```

if I fix it by manually changing makefile and replacing `-D_GLIBCXX_USE_CXX11_ABI=` with `_GLIBCXX_USE_CXX11_ABI=1` I get another errors:

```
/usr/include/ATen/core/dispatch/Dispatcher.h:211:80: error: redeclaration of ‘at::Tensor& args#0’
  211 |     return kernel.template callUnboxedOnly<Return, Args...>(std::forward<Args>(args)...);
      |                                 
```"
Pre-trained detection models for PyTorch Hub,pytorch/vision,2020-03-06 04:12:52,12,enhancement#help wanted#module: models#module: hub,1945,576686246,"## 🚀 Feature
First of all, thanks for the amazing work. 

I noticed currently there's no pre-trained detection models exposed via `hub_conf.py`. It looks like the building blocks are already there. Is there plan to support it? 

Thanks

## Motivation

Increase coverage of supported vision tasks via Torch Hub. 

## Pitch
- entrypoints for detection models are added to `hub_conf.py`
- some helpers e.g. for bbox parsing could also be added there
"
" Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at ../caffe2/serialize/inline_container.cc:132)",pytorch/vision,2020-03-05 13:06:40,22,needs discussion#module: models#topic: object detection#topic: mobile,1943,576240071,"I have created .pt file of keypoint_rcnn using torch=1.5 and vision=0.6.0 now trying to integrate it with my android app. The error I am getting is as below. 

> E/AndroidRuntime: FATAL EXCEPTION: main
    Process: org.pytorch.helloworld, PID: 15691
    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld/org.pytorch.helloworld.MainActivity}: com.facebook.jni.CppException: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at ../caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at ../caffe2/serialize/inline_container.cc:132)
    (no backtrace available)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3122)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3261)
        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)
        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)
        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1977)
        at android.os.Handler.dispatchMessage(Handler.java:106)
        at android.os.Looper.loop(Looper.java:193)
        at android.app.ActivityThread.main(ActivityThread.java:6923)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:870)
     Caused by: com.facebook.jni.CppException: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at ../caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at ../caffe2/serialize/inline_container.cc:132)
    (no backtrace available)
        at org.pytorch.NativePeer.initHybrid(Native Method)
        at org.pytorch.NativePeer.<init>(NativePeer.java:18)
        at org.pytorch.Module.load(Module.java:23)
        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:41)
        at android.app.Activity.performCreate(Activity.java:7148)
        at android.app.Activity.performCreate(Activity.java:7139)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1293)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3102)
        	... 11 more"
windows10 cmake torchvision cuda doesn't work,pytorch/vision,2020-02-21 05:17:23,6,,1903,568741932,"windows10+cuda10.1+vs2017
when i cmake torchvision with CUDA but it doesn't work.i change enable_language(CUDA) as find_package(CUDA) but it still only compile cpu file."
CUDA incorrectly parsed and invalid options forwarded by cmake,pytorch/vision,2020-02-10 17:32:56,15,windows#topic: build#module: c++ frontend,1865,562716051,"I am trying to build the C++ API on Windows 10 with CUDA 10.1 for Visual Studio 2019.

Part of cmake seems to indicate that it uses CUDA 10.2, then counter indicates it uses CUDA 10.1
(I am targeting 10.1, all my env is configured to point to it)
Output from cmake: 

```
  Environment variable CUDA_ROOT is set to:

    %DEVELOP%/cuda/cuda-10.1

  For compatibility, CMake is ignoring the variable.
Call Stack (most recent call first):
  E:/develop/torch/torch-1.5.0a-source-20200208/share/cmake/Caffe2/Caffe2Config.cmake:88 (include)
  E:/develop/torch/torch-1.5.0a-source-20200208/share/cmake/Torch/TorchConfig.cmake:40 (find_package)
  CMakeLists.txt:13 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.

Caffe2: CUDA detected: 10.2
Caffe2: CUDA nvcc is: E:/develop/cuda/cuda-10.1/bin/nvcc.exe
Caffe2: CUDA toolkit directory: E:/develop/cuda/cuda-10.1
Caffe2: Header version is: 10.1
CMake Warning (dev) at E:/develop/torch/torch-1.5.0a-source-20200208/share/cmake/Caffe2/public/cuda.cmake:106 (find_package):
  Policy CMP0074 is not set: find_package uses <PackageName>_ROOT variables.
  Run ""cmake --help-policy CMP0074"" for policy details.  Use the cmake_policy
  command to set the policy and suppress this warning.

  CMake variable CUDNN_ROOT is set to:

    E:/develop/cuda/cudnn-10.1

  Environment variable CUDNN_ROOT is set to:

    %DEVELOP%/cuda/cuda-10.1/../cudnn-10.1
```  

Resulting Visual Studio 2019 solution doesn't have the proper options forwarded : 
(for example, I should have `compute_61;sm_61`)

![image](https://user-images.githubusercontent.com/19194484/74173691-8bf4ab00-4c00-11ea-8c0d-18dcbf1822b1.png)

And although cmake seemed to indicate it correctly found my CUDA 10.1 in the end, it still points to CUDA 10.2 when trying to compile: 

![image](https://user-images.githubusercontent.com/19194484/74173788-c1999400-4c00-11ea-85b3-bf1c67b023c4.png)

I have managed to compile https://github.com/pytorch/pytorch without issue using CUDA 10.1, so I really think this is not a ENV/config issue of CUDA, but a cmake parsing problem.
"
[proposal] Add `instance` option to SBD dataset.,pytorch/vision,2020-02-10 14:35:12,0,enhancement#module: datasets,1863,562602054,"Hello,

Currently, the `SBDataset` does not have the option to return boundaries or segments of instances available on the original dataset, the function is fixed to class-specific only.

More info about the instances from the dataset README.

```txt
     o cls
     This directory contains category-specific segmentations and boundaries. There is one .mat file for
     each image. Each mat file contains a struct called GTcls with 3 fields:
         - GTcls.Segmentation is a single 2D image containing the segmentation. Pixels that belong to
         category k have value k, pixels that do not belong to any category have value 0.
         - GTcls.Boundaries is a cell array. GTcls.Boundaries{k} contains the boundaries of the k-th category.
         These have been stored as sparse arrays to conserve space, so make sure you convert them to full arrays
         when you want to use them/visualize them, eg : full(GTcls.Boundaries{15})
         - GTcls.CategoriesPresent is a list of the categories that are present.
 
     o inst
     This directory contains instance-specific segmentations and boundaries. There is one mat file for each
     image. Each mat file contains a struct called GTinst with 3 fields:
         - GTinst.Segmentation is a single 2D image containing the segmentation. Pixels belonging to the
         i-th instance have value i.
         - GTinst.Boundaries is a cell array. GTinst.Boundaries{i} contains the boundaries of the i-th instance.
         Again, these are sparse arrays.
         - GTinst.Categories is a vector with as many components as there are instances. GTinst.Categories(i) is
         the category label of the i-th instance.
```

Is this suggestion adequate?
I have already done these changes locally, if approved I will open a PR.


cc @pmeier"
[Feature proposal] RandomCrop without the borders of the image.,pytorch/vision,2020-02-10 13:29:16,2,module: transforms#new feature,1862,562560998,"Hello everyone, I thank you for your efforts for this beautiful library.

For a project I came to use a  RandomCrop without the borders of the image.
I know it's possible to do it in two steps like this:

![compose](https://user-images.githubusercontent.com/24889239/74153156-40e29400-4c10-11ea-97b0-35c7267e2f6f.png)

```python
import torchvision.transforms.functional as F
from torchvision.transforms import Compose, RandomCrop

class Crop(object):
    def __init__(self, w_border:int, h_border: int):
        self.w_border = w_border
        self.h_border = h_border
    
    def __call__(self, img):
        w, h = _get_image_size(img)
        return F.crop(img, self.h_border, self.w_border,h - h_border, w - w_border)

pipeline = Compose([
                    Crop(w_border=30, h_border=30),
                    RandomCrop(size=(256, 256))
])
```

However it is possible to do it in **one step** by slightly modifying the RandomCrop code by adding a new argument: 

![new_random_crop](https://user-images.githubusercontent.com/24889239/74153180-4fc94680-4c10-11ea-94d1-3da9436791d8.png)
```python

class RandomCrop(object):
    """"""Crop the given PIL Image at a random location.

    Args:
        size (sequence or int): Desired output size of the crop. If size is an
            int instead of sequence like (h, w), a square crop (size, size) is
            made.
        padding (int or sequence, optional): Optional padding on each border
            of the image. Default is None, i.e no padding. If a sequence of length
            4 is provided, it is used to pad left, top, right, bottom borders
            respectively. If a sequence of length 2 is provided, it is used to
            pad left/right, top/bottom borders, respectively.
        pad_if_needed (boolean): It will pad the image if smaller than the
            desired size to avoid raising an exception. Since cropping is done
            after padding, the padding seems to be done at a random offset.
        fill: Pixel fill value for constant fill. Default is 0. If a tuple of
            length 3, it is used to fill R, G, B channels respectively.
            This value is only used when the padding_mode is constant
        padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.
       border_size (int or sequence, optional): Border zone excluded from the crop.  If border_size is 
             an int instead of sequence like (h, w), a square border (border_size, border_size) is made.

             - constant: pads with a constant value, this value is specified with fill

             - edge: pads with the last value on the edge of the image

             - reflect: pads with reflection of image (without repeating the last value on the edge)

                padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode
                will result in [3, 2, 1, 2, 3, 4, 3, 2]

             - symmetric: pads with reflection of image (repeating the last value on the edge)

                padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode
                will result in [2, 1, 1, 2, 3, 4, 4, 3]

    """"""

    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant', border_size=0):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size
        # Modification HERE
        if isinstance(border_size, numbers.Number):
            self.border_size = (int(border_size), int(border_size))
        else:
            self.border_size = border_size
        self.padding = padding
        self.pad_if_needed = pad_if_needed
        self.fill = fill
        self.padding_mode = padding_mode

    @staticmethod
    def get_params(img, output_size, border_size):
        """"""Get parameters for ``crop`` for a random crop.

        Args:
            img (PIL Image): Image to be cropped.
            output_size (tuple): Expected output size of the crop.
            border_size (tuple):  Border size to be excluded.

        Returns:
            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.
        """"""
        w, h = _get_image_size(img)
        th, tw = output_size
        bh, bw = border_size # Modification HERE

        if w == tw and h == th:
            return 0, 0, h, w

        i = random.randint(bh, h - th) # Modification instead of (0, h - th)
        j = random.randint(bw, w - tw) # Modification instead of (0, w - tw)
        return i, j, th, tw

    def __call__(self, img):
        """"""
        Args:
            img (PIL Image): Image to be cropped.

        Returns:
            PIL Image: Cropped image.
        """"""
        if self.padding is not None:
            img = F.pad(img, self.padding, self.fill, self.padding_mode)

        # pad the width if needed
        if self.pad_if_needed and img.size[0] < self.size[1]:
            img = F.pad(img, (self.size[1] - img.size[0], 0), self.fill, self.padding_mode)
        # pad the height if needed
        if self.pad_if_needed and img.size[1] < self.size[0]:
            img = F.pad(img, (0, self.size[0] - img.size[1]), self.fill, self.padding_mode)

        i, j, h, w = self.get_params(img, self.size, self.border_size) # Modification: add  self.border_size.

        return F.crop(img, i, j, h, w)

    def __repr__(self):
        return self.__class__.__name__ + '(size={0}, border={1}, padding={2})'.format(self.size, self.border_sise, self.padding) # modification HERE
```
And we could do it this way: 

```python
your_image = ...
random_crop = RandomCrop(size=(256, 256), border_size=(30, 30))
```

I will be happy to discuss it with you and if there are smarter solutions, don't hesitate to let me know 😄 .

If it satisfies you I can make a pull request.

Thank you.

cc @vfdev-5"
View for use inside Sequential,pytorch/vision,2020-02-02 02:15:59,2,module: models,1844,558629788,"The previous suggestion to have an official View(shape) for use inside nn.Sequential was closed without explanation or sufficient discussion.

https://github.com/pytorch/vision/issues/720

I believe there are good reasons implement this upstream, including:
- it avoids users implementing their own and making errors
- being able to see the shapes of tensor inside the nn..Sequential list which defines other shape-sensitive functions is ergonomic coding!

The reason given for not doing this that the developers prefer us to code without using Sequential. This is only a preference. PyTorch should support both styles, and note that beginners and newcomers to coding prefer the simplicity of Sequential."
Windows nightly wheels build failed,pytorch/vision,2020-01-29 13:51:17,3,help wanted#windows#module: video,1830,556875404,"Error text:
```powershell
  C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.11.25503\bin\HostX64\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT -ID:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder -ID:\_work\1\s\packaging\windows\conda\envs\py35\Library\include -ID:\_work\1\s\packaging\windows\vision\torchvision\csrc -ID:\_work\1\s\packaging\windows\conda\envs\py35\lib\site-packages\torch\include -ID:\_work\1\s\packaging\windows\conda\envs\py35\lib\site-packages\torch\include\torch\csrc\api\include -ID:\_work\1\s\packaging\windows\conda\envs\py35\lib\site-packages\torch\include\TH -ID:\_work\1\s\packaging\windows\conda\envs\py35\lib\site-packages\torch\include\THC -ID:\_work\1\s\packaging\windows\conda\envs\py35\include -ID:\_work\1\s\packaging\windows\conda\envs\py35\include ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.11.25503\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.11.25503\include"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\8.1\include\\shared"" ""-IC:\Program Files (x86)\Windows Kits\8.1\include\\um"" ""-IC:\Program Files (x86)\Windows Kits\8.1\include\\winrt"" ""-IC:\Program Files (x86)\Windows Kits\8.1\include\\cppwinrt"" -ID:\_work\1\s\packaging\windows\conda\envs\py35\Library\include /EHsc /TpD:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\audio_sampler.cpp /Fobuild\temp.win-amd64-3.5\Release\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\audio_sampler.obj /MD /wd4819 /EHsc -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=base_decoder -D_GLIBCXX_USE_CXX11_ABI=0
  cl : Command line warning D9025 : overriding '/MT' with '/MD'
  cl : Command line warning D9002 : ignoring unknown option '-std=c++14'
  audio_sampler.cpp
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(30): error C3646: 'format': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(30): error C2059: syntax error: '{'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(30): error C2334: unexpected token(s) preceding '{'; skipping apparent function body
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(25): error C2039: 'format': is not a member of 'ffmpeg::AudioFormat'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(21): note: see declaration of 'ffmpeg::AudioFormat'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(25): error C2065: 'format': undeclared identifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(45): error C3646: 'format': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(45): error C2059: syntax error: '{'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(45): error C2334: unexpected token(s) preceding '{'; skipping apparent function body
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(40): error C2039: 'format': is not a member of 'ffmpeg::VideoFormat'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(36): note: see declaration of 'ffmpeg::VideoFormat'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(53): error C3646: 'type': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(53): error C2059: syntax error: '{'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(53): error C2334: unexpected token(s) preceding '{'; skipping apparent function body
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(97): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(99): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(101): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(103): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(106): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(112): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(118): error C2061: syntax error: identifier 'ssize_t'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(129): error C3646: 'stream': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(129): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(135): error C3646: 'num': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(135): error C2059: syntax error: '{'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(135): error C2334: unexpected token(s) preceding '{'; skipping apparent function body
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(137): error C3646: 'den': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(137): error C2059: syntax error: '{'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(137): error C2334: unexpected token(s) preceding '{'; skipping apparent function body
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(139): error C3646: 'duration': unknown override specifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(139): error C2059: syntax error: '{'
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(139): error C2334: unexpected token(s) preceding '{'; skipping apparent function body
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(98): error C2065: 's': undeclared identifier
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(97): error C2614: 'ffmpeg::MediaFormat': illegal member initialization: 'stream' is not a base or member
  d:\_work\1\s\packaging\windows\vision\torchvision\csrc\cpu\decoder\defs.h(97): fatal error C1903: unable to recover from previous error(s); stopping compilation
  Internal Compiler Error in C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.11.25503\bin\HostX64\x64\cl.exe.  You will be prompted to send an error report to Microsoft later.
  INTERNAL COMPILER ERROR in 'C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\VC\Tools\MSVC\14.11.25503\bin\HostX64\x64\cl.exe'
      Please choose the Technical Support command on the Visual C++
      Help menu, or open the Technical Support help file for more information
  error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\BuildTools\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\\cl.exe' failed with exit status 2
  
  ----------------------------------------
  Running setup.py clean for torchvision
Failed to build torchvision
ERROR: Failed to build one or more wheels
You are using pip version 10.0.1, however version 20.0.2 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.

```"
[models] Suggestion of GeneralizedRCNN forward output,pytorch/vision,2020-01-19 21:52:05,9,enhancement#help wanted#module: models#topic: object detection,1775,551991687,"## 🚀 Loss computation in eval mode

Even in `eval` mode, if the user passes targets to the `forward` method, return (or store) the loss dictionary in addition to the detections.

## Motivation

Recently, I made a quick training script for MaskRCNN on a different dataset. And since I usually look at training and validation loss evolution over epochs to spot potential overfit, I checked how I could get around the current `forward` method of `GeneralizedRCNN`.

In train mode, it returns only the loss dictionary (which is fined, there is little use to check detections).
Now, say in eval mode that I want to get the loss. There is no way to retrieve it currently.

## Pitch

Change the `forward` method implementations of  `RoiHeads`, `RegionProposalNetwork` and  `GeneralizedRCNN` so that when both an input and a target are passed to `GeneralizedRCNN.forward` it returns both the loss dictionary and the detections.

_Instead of checking  `self.training`, we could check `self.training or targets is not None`_


Happy to come up with a PR if you think that's a good idea, cheers!"
Pyramid layer,pytorch/vision,2020-01-10 15:44:20,2,question#module: models#topic: object detection,1737,548150014,"I want to extract the third layer of feature pyramid from 

features = self.backbone(images.tensors) in generalized_rcnn.py

any help please?"
quantization-aware training in classification,pytorch/vision,2020-01-08 13:45:27,6,module: models.quantization,1729,546869915,"Hi, I want to reproduce the result of the quantization awareness training of mobilenet_v2 using this [script](https://github.com/pytorch/vision/blob/master/references/classification/train_quantization.py). 

1: It seems that the script will raise launch error when using multi GPU. Do we support multi-GPU when quantization awareness training?
2: Also the readme says that 'Training converges at about 10 epochs.', it seems that after 10 epochs, the test result can not achieve 'acc@top1 71.6' as the pretrained model hosted in [hub](https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/mobilenet.py#L12). 
"
Inconsistency in inception_v3 'transform_input' default value,pytorch/vision,2019-12-31 22:41:26,2,enhancement#module: models#module: documentation,1709,544282762,"In the comments of `inception_v3` it is said that the default value of `transform_input` is `False`:
https://github.com/pytorch/vision/blob/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c/torchvision/models/inception.py#L42-L43
However, the default value is actually set to `True` when `pretrained` is true:
https://github.com/pytorch/vision/blob/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c/torchvision/models/inception.py#L45-L47
This change is not mentioned in the source code comments or the [official documentation](https://pytorch.org/docs/stable/torchvision/models.html#inception-v3). Sometimes people don't want the input to be transformed in the exact same way as `transform_input` did even when using pretrained model, or people just don't notice that `transform_input` is set to true when `pretrained` is enabled. Maybe it would be better to mention this change of default value of `transform_input` in the comments?
"
FasterRCNN to ONNX model,pytorch/vision,2019-12-30 09:10:41,29,bug#module: models#topic: object detection#module: onnx,1706,543804911,"Hi there,
I tried to convert a fasterrcnn model to onnx format, and followed the instruction from test/test_onnx.py [https://github.com/pytorch/vision/blob/master/test/test_onnx.py](url). 

Here is my code:
`model=models.detection.faster_rcnn.fasterrcnn_resnet50_fpn(pretrained=True,min_size=800,max_size=1333)`
`image=cv2.imread(""test.jpg"")  `
`image=cv2.resize(image,(1333,800)) `                                              
`image1 = Image.fromarray(cv2.cvtColor(image.copy(),cv2.COLOR_BGR2RGB))  `
`image_tensor=to_tensor(image1)`
`model.eval()`
`onnx_io = io.BytesIO()`
`torch.onnx.export(model, [image_tensor], ""faster_rcnn.onnx"",do_constant_folding=True, opset_version=_onnx_opset_version)`

I have succeed convert the model with the above code, however, when I tried to convert the tensor and model to cuda tensor with `.to(device)`, there is an error that is` line 359, in _get_top_n_idx r.append(top_n_idx + offset) RuntimeError: expected device cuda:0 but got device cpu`.
I don't know how to solve it.

Please help me with that.

Cheers!
"
Runtime error in torchvision nms in Linux. Windows works fine,pytorch/vision,2019-12-29 10:55:53,5,bug#help wanted#module: ops#triage review#high priority,1705,543356573,"Hi,

I'm getting an runtime error when running torchvision\ops\boxes.nms.
torchvision 0.4.0
pytorch 1.2.0 (GPU)

```
RuntimeError: Trying to create tensor with negative dimension -532064992: [-532064992] (check_size_nonnegative at /pytorch/aten/src/ATen/native/TensorFactories.h:64)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fd8e1d79273 in /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: at::native::empty_cuda(c10::ArrayRef, c10::TensorOptions const&, c10::optionalc10::MemoryFormat) + 0xb76 (0x7fd7d85e8fb6 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so)
frame #2: + 0x3f7da58 (0x7fd7d6f7fa58 in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so)
frame #3: torch::autograd::VariableType::empty(c10::ArrayRef, c10::TensorOptions const&, c10::optionalc10::MemoryFormat) + 0x3fa (0x7fd7d69fa75a in /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so)
frame #4: + 0x7f661 (0x7fd87b3fd661 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #5: nms_cuda(at::Tensor const&, at::Tensor const&, float) + 0x430 (0x7fd87b3fe042 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #6: nms(at::Tensor const&, at::Tensor const&, float) + 0x172 (0x7fd87b3c1cf9 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #7: + 0x65115 (0x7fd87b3e3115 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #8: + 0x62304 (0x7fd87b3e0304 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #9: + 0x5dc45 (0x7fd87b3dbc45 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #10: + 0x5ded2 (0x7fd87b3dbed2 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
frame #11: + 0x4f2e7 (0x7fd87b3cd2e7 in /opt/conda/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)
```

**To Reproduce**
code:
```
nms(transformed_anchors, scores, iou_threshold = 0.7)
transformed_anchors is [ 490698, 4]
scores is [ 490698]

transformed_anchors .min () = 0
transformed_anchors .max () = 2560
```

**Environment**
```
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.16.4
[pip] torch==1.2.0
[pip] torchtext==0.4.0
[pip] torchvision==0.4.0
[conda] magma-cuda100 2.1.0 5 local
[conda] mkl 2019.1 144
[conda] mkl-include 2019.1 144
[conda] torch 1.2.0 pypi_0 pypi
[conda] torchtext 0.4.0 pypi_0 pypi
[conda] torchvision 0.4.0 pypi_0 pypi
```

**Additional context**
same code on Windows runs as expected

[same issue opened in pytorch/pytorch](https://github.com/pytorch/pytorch/issues/31568
)"
I hope you can provide 3D align-pooling layer directly. Pray. ,pytorch/vision,2019-12-18 02:17:15,7,awaiting response#needs discussion#module: ops,1678,539411200,"I see that 2D align-pooling has been implemented, but there is no 3D one. I do target detection of 3D voxels. The input data format is (batch, channel, depth, height, w). I customized a 3D align-pooling Op. However, there is a big problem in the deployment. The custom OP needs to be registered in the torchscript. In this way, the model can be exported normally, and finally the model can be loaded on the C + + side.I tried for months, but it didn't work. I hope you can provide 3D align-pooling layer directly. Pray.
This is the function:
torchvision.ops.roi_align(input, boxes, output_size, spatial_scale=1.0, sampling_ratio=-1)"
I hope you can provide 3D align-pooling layer directly. Pray.,pytorch/vision,2019-12-14 08:05:26,4,module: ops#topic: object detection#new feature,2774,717093748,"I see that 2D align-pooling has been implemented, but there is no 3D one. I do target detection of 3D voxels. The input data format is (batch, channel, depth, height, w). I customized a 3D align-pooling Op.  However, there is a big problem in the deployment. The custom OP needs to be registered in the torchscript. In this way, the model can be exported normally, and finally the model can be loaded on the C + + side.I tried for months, but it didn't work.  I hope you can provide 3D align-pooling layer directly. Pray.

cc @fmassa @vfdev-5"
Provide the meta.bin file of the ImageNet dataset together with torchvision?,pytorch/vision,2019-12-07 12:50:02,5,module: datasets,1647,534398108,"@fmassa In the light of recent problems with the `meta.bin` file of the ImageNet dataset (#1645 #1646 ), I think it is reasonable to ask, if we can provide it together with `torchvision`. Especially now without official download links for the archives, I think it would be beneficial. With it users that switch to `torchvision` and only have the image archives do not need to download the devkit. "
process.wait(0) when traing faster_rcnn,pytorch/vision,2019-12-06 08:23:40,3,help wanted#awaiting response#needs discussion#needs reproduction#module: reference scripts#topic: object detection,1643,533815827,"<img width=""575"" alt=""662FE0EB-CD0B-4FED-B036-CB5964218D31"" src=""https://user-images.githubusercontent.com/12228641/70307884-10176480-1845-11ea-93d6-d0e6ccae08ca.png"">

I use the [provided code ](https://github.com/pytorch/vision/blob/master/references/detection/train.py) to train my data using faster_rcnn on 2 GPUs. The program always stops at epoch 27(just stop, no more output), and when I use `ctrl+c` to interrupt the program, the content is as shown in the pic."
VideoClips object needs documentation,pytorch/vision,2019-11-15 15:25:57,5,enhancement#help wanted#module: documentation,1582,523533137,"The recent 0.4.2 update on VideoClips seems to bring lots of new functionality with `video_reader` backend without many explanations. 

I think VideoClips need documentation, it's a very useful tool. Thanks!"
"Add an option for Dection Models to return cost in eval mode as well (validation, test loss)",pytorch/vision,2019-11-14 16:50:17,12,needs discussion#module: models#topic: object detection,1574,522981365,"We need to get the model cost in eval mode when we compute validation loss/cost for hyper parameter search.

At the moment eval mode only returns the detection without the loss/cost.
Which mean it is used as inference mode which is different than what people might want.

We can not use training mode to compute validation because of batchnorm and dropout.

The solution with the least code modification would be to add a new argument `inference_mode` in [forward][1] that would be true if we only need the detection and false if we want the loss/cost.
and [change `if self.training` to `if not inference_mode`][2].

Few classes through out the `torchvision.models.vision` would need to be updated like so

[1]: https://github.com/pytorch/vision/blob/681c6c1115035a9dfd2f05ede1a00620532e2f01/torchvision/models/detection/generalized_rcnn.py#L31

[2]: https://github.com/pytorch/vision/blob/681c6c1115035a9dfd2f05ede1a00620532e2f01/torchvision/models/detection/generalized_rcnn.py#L59"
Pad if needed when using TenCrop and FiveCrop,pytorch/vision,2019-11-14 06:10:55,1,,1573,522647207,"When using TenCrop and FiveCrop, if the source image is smaller than the aimed crop size, there will be an error like `ValueError: Requested crop size (256, 256) is bigger than input size (320, 240)`.

Can you pad the image if needed, just as what `torchvision.transforms.RandomCrop` do? "
quantized fuse_model should have an inplace argument,pytorch/vision,2019-11-12 21:46:58,1,module: models.quantization,1569,521822218,"Currently if the user wants to have the fused model while preserving the original, they have to do

```python
import copy
import torchvision.models.quantization as models

model = models.resnet18(pretrained=True, progress=True, quantize=False)
model_fused = copy.deepcopy(model)
model_fused.fuse_model()
```

Given that the quantization API provides an inplace variant for the `torch.quantization.fuse_modules`, we need to have an option for inplace in the model methods as well."
MaskRCNN crashes when reshaping an empty tensor rel_codes,pytorch/vision,2019-11-12 10:24:27,16,awaiting response#needs reproduction#module: models#topic: object detection,1568,521459728,"torchvision '0.4.0+cu92'


Traceback:

```
creating index...
index created!
Traceback (most recent call last):
  File ""./scratch_19.py"", line 1068, in <module>
    main()
  File ""./scratch_19.py"", line 1054, in main
    evaluate(model, data_loader_test, device=device)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 49, in decorate_no_grad
    return func(*args, **kwargs)
  File ""./scratch_19.py"", line 889, in evaluate
    outputs = model(image)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py"", line 52, in forward
    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/roi_heads.py"", line 550, in forward
    boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/roi_heads.py"", line 474, in postprocess_detections
    pred_boxes = self.box_coder.decode(box_regression, proposals)
  File ""/disk1/mattan/anaconda3/lib/python3.7/site-packages/torchvision/models/detection/_utils.py"", line 168, in decode
    rel_codes.reshape(sum(boxes_per_image), -1), concat_boxes
RuntimeError: cannot reshape tensor of 0 elements into shape [0, -1] because the unspecified dimension size -1 can be any value and is ambiguous

```

It appears that during a forward pass rel_codes is empty which crashes the reshape operator."
Improve error message when number of anchors mismatch,pytorch/vision,2019-10-30 15:31:09,0,enhancement#help wanted#module: models#topic: object detection,1539,514765999,"See discussion in https://github.com/pytorch/vision/issues/1308#issuecomment-547964507

We should add checks in https://github.com/pytorch/vision/blob/067704986d92a3dff4ccf4a686080b3d31390402/torchvision/models/detection/rpn.py#L440 so that it's clearer to users what they need to change if they face such an error in the future."
[RFC] Abstractions for segmentation / detection transforms,pytorch/vision,2019-10-02 14:12:11,15,needs discussion#module: transforms,1406,501515676,"This is a proposal. I'm not sure yet it's the best way to achieve this, so I'm putting this up for discussion.

## tl;dr

Have specific tensor subclasses for `BoxList` / `SegmentationMask`, `CombinedObjects` , etc, which inherits from `torch.Tensor` and overrides methods to properly dispatch to the relevant implementations. Depends on `__torch_function__` from https://github.com/pytorch/pytorch/issues/22402 (implemented in https://github.com/pytorch/pytorch/pull/27064)

## Background

For more than 2 years now, users have asked for ways of performing transformations of multiple inputs at the same time, for example for semantic segmentation or object detection https://github.com/pytorch/vision/issues/9

The recommended solution is to use the functional transforms in this case https://github.com/pytorch/vision/issues/230 https://github.com/pytorch/vision/issues/1169 , but for simple cases, this is a bit verbose.

## Requirements

Ideally, we would want the following to be possible:
1. work with a `Compose` style interface for simple cases
2. support more than a single input of each type (for example, two images and one segmentation mask)
3. support joint rotations / rescale with different hyperparameters for different input types (images can do bilinear interpolation, segmentation maps should do nearest interpolation)
4. be simple and modular

# Proposed solution

We define new classes for each type of object, which should all inherit from `torch.Tensor`, and implement / override a few specific methods. It might depend on `__torch_function__` from https://github.com/pytorch/pytorch/issues/22402 (implemented in https://github.com/pytorch/pytorch/pull/27064) 

### Work with `Compose`-style
We propose to define a `CombinedObjects` (better names welcome), which is a collection of arbitrary objects (potentially named, but that's not a requirement).
Calling any of the methods in it should dispatch to the corresponding methods of its constituents. A basic example is below, I'll mention a few more points about it afterwards):
```python
class CombinedObjects(object):
    def __init__(self, **kwargs):
        self.kwargs = kwargs

    def hflip(self):
        result = {}
        for name, value in self.kwargs.items():
            result[name] = value.hflip()
        return type(self)(**result)
```

In this way, if the underlying objects follows the same protocol (i.e., implement the required functions), then this should allow to combine an arbitrary number of objects with a `Compose` API via
```python
# example for flip
class RandomFlip(object):
    def __call__(self, x):
        # implementation stays almost the same
        # but now, `x` can be an Image, or a CombinedObject
        if random.random() > 0.5:
            x = x.hflip()
        return x

transforms = Compose([
    Resize(300),
    RandomFlip(),
    RandomColorAugment()
])

inputs = CombinedObjects(img1= x, img2=y, mask=z)
output = transforms(inputs)
```
which satisfies point 1 and 2 above, and part of point 3 (except for the different transformation hyperparameters for image / segmentation mask, which I'll cover next).

## Different behavior for mask / boxes / images

In the same vein as the `CombinedObject` approach from the previous section, we would have subclasses of `torch.Tensor` for `BoxList` / `SegmentationMask` / etc which would override the behavior of specific functions so that they work as expected.

For example (and using code snippets from https://github.com/pytorch/pytorch/pull/25629), we can define a class for segmentation masks where rotation / interpolation / grid sample always behave with nearest interpolation:
```python
HANDLED_FUNCTIONS = {}

def implements(torch_function):
    ""Register an implementation of a torch function for a Tensor-like object.""
    def decorator(func):
        HANDLED_FUNCTIONS[torch_function] = func
        return func
    return decorator

class SegmentationMask(torch.Tensor):
    def __torch_function__(self, func, types, args, kwargs):
        if func not in HANDLED_FUNCTIONS:
            return NotImplemented
        # Note: this allows subclasses that don't override
        # __torch_function__ to handle DiagonalTensor objects.
        if not all(issubclass(t, self.__class__) for t in types):
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)

@implements(torch.nn.functional.interpolate)
def interpolate(...):
    # force nearest interpolation
    return torch.nn.functional.interpolate(..., mode='nearest')
```
and we can also give custom implementations for bounding boxes:
```python
class BoxList(torch.Tensor):
    # need to define height and width somewhere as an attribute
    ...

@implements(torch.nn.functional.interpolate)
def interpolate(...):
    return box * scale
```

This would allow to cover the remaining of point 3. Because they are subclasses of `torch.Tensor`, they behave as `Tensor` except in some particular cases where we override the behavior.
This would be used as follows:
```python
boxes = BoxList(torch.rand(10, 4), ...)
masks = SegmentationMask(torch.rand(1, 100, 200))
image = torch.rand(3, 100, 200)
# basically a dict of inputs
x = CombinedObject(image=image, boxes=boxes, masks=masks)
transforms = Compose([
    RandomResizeCrop(224),
    RandomFlip(),
    ColorAugment(),
])
out = transforms(x)
# have an API for getting the elements back
image = out.get('image')
# or something like that
```

### Be simple and modular

This is up for discussion. The fact that we are implementing subclasses that do not behave *exactly* like tensors can be confusing and misleading. But it does seem to simplify a number of things, and makes it possible for users to leverage the same abstractions in torchvision for their own custom types, without having to modify anything in torchvision, which is nice.

## Related discussions

Some other proposals have been discussed in https://github.com/pytorch/vision/issues/230 https://github.com/pytorch/vision/issues/1169 and many other places.

cc @Noiredd @SebastienEske @pmeier for discussion
"
"How to Crop single image before calling torchvision.utils.save_image, If I am using PIL lib Image.crop(....) method then image quality degrade.",pytorch/vision,2019-09-30 20:35:12,6,module: utils,1395,500509727,"
     vutils.save_image(fixed_fake.data,outputpath , normalize=True)
    print(""output path"",outputpath)
    img = Image.open(outputpath)
    noOfRow = 5
    noOfColumn = 8
    x1 = 2
    y1 = 2
    x2 = 130
    y2 = 130
    folder = file_batch

    for i in range(0, noOfColumn):
        dest_dir = file_batch[i].split(""/"")[7]
        if not os.path.exists(outf+""/""+dest_dir):
            os.mkdir(outf+""/""+dest_dir)
        for j in range(1, noOfRow + 1):
            area = (x1, y1, x2, y2)
            cropped_img = img.crop(area)
            imgName = ""{}{}"".format(i, j)
            cropped_img.save(os.path.join(outf+dest_dir,filename))
            y1 = y1 + 130
            y2 = y2 + 130
        x1 = x1 + 130
        x2 = x2 + 130
       y1 = 2
        y2 = 130"
Error while using RandomResizedCropVideo,pytorch/vision,2019-09-29 17:51:37,4,bug#module: transforms#module: video,1385,499948106,"I started using the new video transformations by downloading the video transformations source code (seems like it doesn't appear when trying to upgrade via pip).

The transformation I'm trying to use is:
```
def build_transforms():
    mean = [0.485, 0.456, 0.406]
    std = [0.229, 0.224, 0.225]
    res = transforms.Compose([transforms_video.ToTensorVideo(),
                              transforms_video.RandomResizedCropVideo(224),
                              transforms_video.RandomHorizontalFlipVideo(),
                              transforms_video.NormalizeVideo(mean=mean, std=std)
                              ])

    return res
```
This composition raises an error while trying to transform a video clip (It doesn't happen if I remove  transforms_video.RandomResizedCropVideo(224)):

```
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/home/ekosman/anaconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop
    data = fetcher.fetch(index)
  File ""/home/ekosman/anaconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/ekosman/anaconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/workdisk/action_start_detection/loaders/torch_data_loader_no_bg.py"", line 284, in __getitem__
    video = self.transform_clip(video)
  File ""/workdisk/action_start_detection/loaders/torch_data_loader_no_bg.py"", line 157, in transform_clip
    clip = self.transform(clip)
  File ""/home/ekosman/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/transforms/transforms.py"", line 61, in __call__
    img = t(img)
  File ""/workdisk/action_start_detection/utils/transforms_video.py"", line 78, in __call__
    i, j, h, w = self.get_params(clip, self.scale, self.ratio)
  File ""/home/ekosman/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/transforms/transforms.py"", line 638, in get_params
    area = img.size[0] * img.size[1]
TypeError: 'builtin_function_or_method' object is not subscriptable
```"
Change and visualize bounding boxes for Mask R-CNN,pytorch/vision,2019-09-24 01:40:19,1,question#awaiting response#module: models#topic: object detection,1364,497407948,"I'm interested in implementing rotated Mask R-CNN similar to https://github.com/mrlooi/rotated_maskrcnn.

How would I go about accessing and visualizing the bounding boxes? Any help regarding this would be highly appreciated!"
[fyi][video] Video clip creation fails on YFCC,pytorch/vision,2019-09-11 15:12:18,5,bug#help wanted#module: io#module: video,1327,492301221,"# Culprit:
known bug in H264 decoding

For certain filetypes - specifically some related to YFCC100m dataset - there is an error whilst creating the `VideoClips` object. Not 100% sure what is the best way of fixing the issue (other than try-catch in `read_video_timestamps`.

Even if the fix is not critical, we should make sure that is caught in C++ or future versions. 

---

## Sample YFCC video in question:
`27b66931edb88a5f7e6c756ea65fd8.mp4`
there are at least 50 of them, probably more.

## Minimal repro example
```
video_list = ['~/yfcc100m/mp4/27b/669/27b66931edb88a5f7e6c756ea65fd8.mp4']
VideoClips(video_list, 8, 1)
```

or even simpler
```
import av
container = av.open('~/yfcc100m/mp4/27b/669/27b66931edb88a5f7e6c756ea65fd8.mp4')
for idx, frame in enumerate(container.decode({'video': 0})):
    pass
```

## Output error stack:
```
Invalid NAL unit size (34173 > 22144).
Error splitting the input into NAL units.
stream 1, offset 0x16baf66: partial file
Invalid NAL unit size (24747 > 6180).
Error splitting the input into NAL units.
``` 

"
standardize dataset naming conventions,pytorch/vision,2019-09-07 00:27:38,2,module: datasets,1305,490568613,"Please standardize at least the naming convention for all datasets when using DataLoader so that data are in `loader.dataset.data` and labels in `loader.dataset.targets`.
MWE:
```
loader_cifar = torch.utils.DataLoader(""CIFAR10"")
loader_cifar.dataset.data <-- contains the data
loader_cifar.dataset.targets <-- contains targets

loader_svhn = torch.utils.DataLoader(""SVHN"")
loader_svhn.dataset.data <-- contains the data
loader_svhn.dataset.labels <-- contains targets
```
This can be really confusing and error prone when developing code to run on multiple datasets.



cc @pmeier"
Add DetNet as a backbone for object detection,pytorch/vision,2019-09-01 05:32:29,1,needs discussion#module: models,1288,487844183,"**DetNet** currently is the state of the art backbone for object detection .... is there any hope for adding it to the detection models in `torchvision`.

https://github.com/guoruoqian/DetNet_pytorch"
GoogleNet and Inception return different types depending on train / eval mode,pytorch/vision,2019-08-29 13:21:22,3,needs discussion#module: models#topic: classification#topic: feature extraction,1273,486954910,"Googlenet and Inception have two different return types, depending on train / eval mode.

This is necessary only during training, because the original models have auxiliary classifiers, which are not used for inference.

The current structure of the models might be seen as an artifact on how they were trained.
One other way of seeing it would be that, if we could return arbitrary intermediate outputs, then during training we could plug the auxiliary classifiers outside of the model, and perform training there, but the model by itself doesn't have those auxiliary classifiers.

We should consider if we would be willing to change this, which would be a BC-breaking change.

## Advantages:
- simplifies torchscript support for those models
- unifies the return value of the model during training and testing

## Disadvantages:
- BC-breaking
- training those models would require custom code to add the auxiliary heads. But this is not a real disadvantage because we haven't reproduced those models with the current training code anyway."
[FYI] Bug in R2+1D implementation,pytorch/vision,2019-08-27 11:15:01,9,bug#needs discussion#module: models#module: video,1265,485746951,"I found that there is a lack of clarity in the original R2+1D paper and official code implementation for models utilizing BottleNeck layers, which makes it impossible to transfer weights from large pretrained models in C2 to the models implemented in torchvision. 

For background info see the [question in their repo](https://github.com/facebookresearch/VMZ/issues/82).

The fix is very straightforward (change bottleneck midplanes computation), but the question is whether we _should_ do it, which I suspect should be based on author's answer. I'm leaving this here just so that people are aware of it."
Improve testing for models,pytorch/vision,2019-08-12 11:43:05,15,enhancement#module: models,1234,479600718,"We currently have very limited testing for the models in `torchvision`.

In most cases, we only test for the output shape to be correct, see
https://github.com/pytorch/vision/blob/8635be94d1216f10fb8302da89233bd86445e449/test/test_models.py#L29-L59
While this allows to catch bugs such as Py2-Py3 differences, it is not enough to ensure that refactorings of the models keep the behavior the same (and that pre-trained weights are still valid).

We should come up with a better testing strategy.

Ideally, we would not want to download the pre-trained weights, because this can make the tests prone to failure due to IO issues.

One possible approach would be to fix the random seed and initialize the models in a particular way and compare the output we get for a particular input with an expected output, ensuring that the output is non-trivial (such as all-zeros or empty, which could happen due to ReLU or for detection models). Something in the lines of
```python
torch.manual_seed(42)
m = torchvision.models.resnet18(num_classes=2)
torch.manual_seed(42)
i = torch.rand(2, 3, 224, 224)
output = m(i)
expected_output = torch.tensor([[0.0002, 0.3], [0.245, 0.001]])
assert output.equals(expected_output)
```

The problem with this approach is that we do not guarantee that the RNG is the same between different versions of PyTorch, which means that this way of testing the model would lead to failures if we change PyTorch's RNG.

This issue will be particularly important to be addressed soon, because we will be adding some slight modifications to the model implementations in order for them to be traceable / ONNX-exportable. @lara-hdr is currently looking into making the torchvision models ONNX-ready.

cc @fbbradheintz , who showed interest in addressing this issue."
Several tests fail on Windows with 0.4.0,pytorch/vision,2019-08-11 06:18:59,8,bug#windows#module: models#module: c++ frontend,1229,479337249,"Test log:
```
============================= test session starts =============================
platform win32 -- Python 3.7.4, pytest-5.0.1, py-1.8.0, pluggy-0.12.0
rootdir: C:\w\2\s\packaging\windows\vision
collected 187 items

test\test_backbone_utils.py ..                                           [  1%]
test\test_cpp_models.py FFFFF..FFFF........FFFFFFFFFF..                  [ 17%]
test\test_datasets.py ..F......                                          [ 22%]
test\test_datasets_transforms.py ..                                      [ 23%]
test\test_datasets_utils.py .....FFF.                                    [ 28%]
test\test_datasets_video_utils.py ..FFss                                 [ 31%]
test\test_io.py .FFFFF                                                   [ 34%]
test\test_models.py ................................................     [ 60%]
test\test_ops.py ..s..s.s.s.s.s.s.s.s                                    [ 71%]
test\test_transforms.py ..........sss................................... [ 96%]
..                                                                       [ 97%]
test\test_utils.py ..FF                                                  [100%]

================================== FAILURES ===================================
_____________________________ Tester.test_alexnet _____________________________

self = <test_cpp_models.Tester testMethod=test_alexnet>

    def test_alexnet(self):
>       process_model(models.alexnet(self.pretrained), self.image, _C_tests.forward_alexnet, 'Alexnet')

test\test_cpp_models.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1)...ures=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_alexnet of PyCapsule object at 0x000000323C5E9450>
name = 'Alexnet'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_densenet121 ___________________________

self = <test_cpp_models.Tester testMethod=test_densenet121>

    def test_densenet121(self):
>       process_model(models.densenet121(self.pretrained), self.image, _C_tests.forward_densenet121, 'Densenet121')

test\test_cpp_models.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DenseNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias....1, affine=True, track_running_stats=True)
  )
  (classifier): Linear(in_features=1024, out_features=1000, bias=True)
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_densenet121 of PyCapsule object at 0x000000323C6005A0>
name = 'Densenet121'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_densenet161 ___________________________

self = <test_cpp_models.Tester testMethod=test_densenet161>

    def test_densenet161(self):
>       process_model(models.densenet161(self.pretrained), self.image, _C_tests.forward_densenet161, 'Densenet161')

test\test_cpp_models.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DenseNet(
  (features): Sequential(
    (conv0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias....1, affine=True, track_running_stats=True)
  )
  (classifier): Linear(in_features=2208, out_features=1000, bias=True)
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_densenet161 of PyCapsule object at 0x000000323C600BD0>
name = 'Densenet161'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_densenet169 ___________________________

self = <test_cpp_models.Tester testMethod=test_densenet169>

    def test_densenet169(self):
>       process_model(models.densenet169(self.pretrained), self.image, _C_tests.forward_densenet169, 'Densenet169')

test\test_cpp_models.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DenseNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias....1, affine=True, track_running_stats=True)
  )
  (classifier): Linear(in_features=1664, out_features=1000, bias=True)
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_densenet169 of PyCapsule object at 0x000000323C6008D0>
name = 'Densenet169'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_densenet201 ___________________________

self = <test_cpp_models.Tester testMethod=test_densenet201>

    def test_densenet201(self):
>       process_model(models.densenet201(self.pretrained), self.image, _C_tests.forward_densenet201, 'Densenet201')

test\test_cpp_models.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = DenseNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias....1, affine=True, track_running_stats=True)
  )
  (classifier): Linear(in_features=1920, out_features=1000, bias=True)
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_densenet201 of PyCapsule object at 0x000000323C600B70>
name = 'Densenet201'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_mnasnet0_5 ____________________________

self = <test_cpp_models.Tester testMethod=test_mnasnet0_5>

    def test_mnasnet0_5(self):
>       process_model(models.mnasnet0_5(self.pretrained), self.image, _C_tests.forward_mnasnet0_5, 'MNASNet0_5')

test\test_cpp_models.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = MNASNet(
  (layers): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)...Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_mnasnet0_5 of PyCapsule object at 0x000000323C6009F0>
name = 'MNASNet0_5'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_mnasnet0_75 ___________________________

self = <test_cpp_models.Tester testMethod=test_mnasnet0_75>

    def test_mnasnet0_75(self):
>       process_model(models.mnasnet0_75(self.pretrained), self.image, _C_tests.forward_mnasnet0_75, 'MNASNet0_75')

test\test_cpp_models.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = MNASNet(
  (layers): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)...Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_mnasnet0_75 of PyCapsule object at 0x000000323C600A80>
name = 'MNASNet0_75'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_mnasnet1_0 ____________________________

self = <test_cpp_models.Tester testMethod=test_mnasnet1_0>

    def test_mnasnet1_0(self):
>       process_model(models.mnasnet1_0(self.pretrained), self.image, _C_tests.forward_mnasnet1_0, 'MNASNet1_0')

test\test_cpp_models.py:129: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = MNASNet(
  (layers): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)...Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_mnasnet1_0 of PyCapsule object at 0x000000323C8A9570>
name = 'MNASNet1_0'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_mnasnet1_3 ____________________________

self = <test_cpp_models.Tester testMethod=test_mnasnet1_3>

    def test_mnasnet1_3(self):
>       process_model(models.mnasnet1_3(self.pretrained), self.image, _C_tests.forward_mnasnet1_3, 'MNASNet1_3')

test\test_cpp_models.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = MNASNet(
  (layers): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)...Sequential(
    (0): Dropout(p=0.2, inplace=True)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_mnasnet1_3 of PyCapsule object at 0x000000323C8A9060>
name = 'MNASNet1_3'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
__________________________ Tester.test_squeezenet1_0 __________________________

self = <test_cpp_models.Tester testMethod=test_squeezenet1_0>

    def test_squeezenet1_0(self):
        process_model(models.squeezenet1_0(self.pretrained), self.image,
>                     _C_tests.forward_squeezenet1_0, 'Squeezenet1.0')

test\test_cpp_models.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = SqueezeNet(
  (features): Sequential(
    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))
    (1): ReLU(inplace=...00, kernel_size=(1, 1), stride=(1, 1))
    (2): ReLU(inplace=True)
    (3): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_squeezenet1_0 of PyCapsule object at 0x000000323C600F60>
name = 'Squeezenet1.0'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
__________________________ Tester.test_squeezenet1_1 __________________________

self = <test_cpp_models.Tester testMethod=test_squeezenet1_1>

    def test_squeezenet1_1(self):
        process_model(models.squeezenet1_1(self.pretrained), self.image,
>                     _C_tests.forward_squeezenet1_1, 'Squeezenet1.1')

test\test_cpp_models.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = SqueezeNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))
    (1): ReLU(inplace=...00, kernel_size=(1, 1), stride=(1, 1))
    (2): ReLU(inplace=True)
    (3): AdaptiveAvgPool2d(output_size=(1, 1))
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_squeezenet1_1 of PyCapsule object at 0x000000323C600D50>
name = 'Squeezenet1.1'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
______________________________ Tester.test_vgg11 ______________________________

self = <test_cpp_models.Tester testMethod=test_vgg11>

    def test_vgg11(self):
>       process_model(models.vgg11(self.pretrained), self.image, _C_tests.forward_vgg11, 'VGG11')

test\test_cpp_models.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg11 of PyCapsule object at 0x000000323C5E97B0>
name = 'VGG11'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
____________________________ Tester.test_vgg11_bn _____________________________

self = <test_cpp_models.Tester testMethod=test_vgg11_bn>

    def test_vgg11_bn(self):
>       process_model(models.vgg11_bn(self.pretrained), self.image, _C_tests.forward_vgg11bn, 'VGG11BN')

test\test_cpp_models.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Batc...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg11bn of PyCapsule object at 0x000000323C5B5F30>
name = 'VGG11BN'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torch\include\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
______________________________ Tester.test_vgg13 ______________________________

self = <test_cpp_models.Tester testMethod=test_vgg13>

    def test_vgg13(self):
>       process_model(models.vgg13(self.pretrained), self.image, _C_tests.forward_vgg13, 'VGG13')

test\test_cpp_models.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg13 of PyCapsule object at 0x000000323C5E97E0>
name = 'VGG13'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
____________________________ Tester.test_vgg13_bn _____________________________

self = <test_cpp_models.Tester testMethod=test_vgg13_bn>

    def test_vgg13_bn(self):
>       process_model(models.vgg13_bn(self.pretrained), self.image, _C_tests.forward_vgg13bn, 'VGG13BN')

test\test_cpp_models.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Batc...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg13bn of PyCapsule object at 0x000000323C5B5E40>
name = 'VGG13BN'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
______________________________ Tester.test_vgg16 ______________________________

self = <test_cpp_models.Tester testMethod=test_vgg16>

    def test_vgg16(self):
>       process_model(models.vgg16(self.pretrained), self.image, _C_tests.forward_vgg16, 'VGG16')

test\test_cpp_models.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg16 of PyCapsule object at 0x000000323C5E9990>
name = 'VGG16'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
____________________________ Tester.test_vgg16_bn _____________________________

self = <test_cpp_models.Tester testMethod=test_vgg16_bn>

    def test_vgg16_bn(self):
>       process_model(models.vgg16_bn(self.pretrained), self.image, _C_tests.forward_vgg16bn, 'VGG16BN')

test\test_cpp_models.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Batc...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg16bn of PyCapsule object at 0x000000323C5B5EA0>
name = 'VGG16BN'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
______________________________ Tester.test_vgg19 ______________________________

self = <test_cpp_models.Tester testMethod=test_vgg19>

    def test_vgg19(self):
>       process_model(models.vgg19(self.pretrained), self.image, _C_tests.forward_vgg19, 'VGG19')

test\test_cpp_models.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg19 of PyCapsule object at 0x000000323C5B5ED0>
name = 'VGG19'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
____________________________ Tester.test_vgg19_bn _____________________________

self = <test_cpp_models.Tester testMethod=test_vgg19_bn>

    def test_vgg19_bn(self):
>       process_model(models.vgg19_bn(self.pretrained), self.image, _C_tests.forward_vgg19bn, 'VGG19BN')

test\test_cpp_models.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

model = VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Batc...lace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
tensor = tensor([[[[0.0902, 0.1098, 0.1216,  ..., 0.2824, 0.2314, 0.2392],
          [0.0980, 0.0863, 0.1020,  ..., 0.3333, 0.3...20, 0.1059, 0.0980,  ..., 0.0667, 0.0784, 0.0706],
          [0.1059, 0.0941, 0.0980,  ..., 0.0588, 0.0667, 0.0667]]]])
func = <built-in method forward_vgg19bn of PyCapsule object at 0x000000323C5B5C90>
name = 'VGG19BN'

    def process_model(model, tensor, func, name):
        model.eval()
        traced_script_module = torch.jit.trace(model, tensor)
        traced_script_module.save(""model.pt"")
    
        py_output = model.forward(tensor)
>       cpp_output = func(""model.pt"", tensor)
E       RuntimeError: undefined Tensor (infer_is_variable at C:\w\1\s\windows\pytorch\build\aten\src\ATen/Functions.h:1149)
E       (no backtrace available)

test\test_cpp_models.py:16: RuntimeError
___________________________ Tester.test_cityscapes ____________________________

self = <test_datasets.Tester testMethod=test_cityscapes>

    def test_cityscapes(self):
        with cityscapes_root() as root:
    
            for mode in ['coarse', 'fine']:
    
                if mode == 'coarse':
                    splits = ['train', 'train_extra', 'val']
                else:
                    splits = ['train', 'val', 'test']
    
                for split in splits:
                    for target_type in ['semantic', 'instance']:
                        dataset = torchvision.datasets.Cityscapes(root, split=split,
                                                                  target_type=target_type, mode=mode)
                        self.generic_segmentation_dataset_test(dataset, num_images=2)
    
                    color_dataset = torchvision.datasets.Cityscapes(root, split=split,
                                                                    target_type='color', mode=mode)
                    color_img, color_target = color_dataset[0]
                    self.assertTrue(isinstance(color_img, PIL.Image.Image))
                    self.assertTrue(np.array(color_target).shape[2] == 4)
    
                    polygon_dataset = torchvision.datasets.Cityscapes(root, split=split,
                                                                      target_type='polygon', mode=mode)
                    polygon_img, polygon_target = polygon_dataset[0]
                    self.assertTrue(isinstance(polygon_img, PIL.Image.Image))
                    self.assertTrue(isinstance(polygon_target, dict))
                    self.assertTrue(isinstance(polygon_target['imgHeight'], int))
                    self.assertTrue(isinstance(polygon_target['objects'], list))
    
                    # Test multiple target types
                    targets_combo = ['semantic', 'polygon', 'color']
                    multiple_types_dataset = torchvision.datasets.Cityscapes(root, split=split,
                                                                             target_type=targets_combo,
                                                                             mode=mode)
                    output = multiple_types_dataset[0]
                    self.assertTrue(isinstance(output, tuple))
                    self.assertTrue(len(output) == 2)
                    self.assertTrue(isinstance(output[0], PIL.Image.Image))
                    self.assertTrue(isinstance(output[1], tuple))
                    self.assertTrue(len(output[1]) == 3)
                    self.assertTrue(isinstance(output[1][0], PIL.Image.Image))  # semantic
                    self.assertTrue(isinstance(output[1][1], dict))  # polygon
>                   self.assertTrue(isinstance(output[1][2], PIL.Image.Image))  # color

test\test_datasets.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\contextlib.py:119: in __exit__
    next(self.gen)
test\fakedata_generation.py:243: in cityscapes_root
    yield tmp_dir
..\conda\envs\py37\lib\contextlib.py:119: in __exit__
    next(self.gen)
test\common_utils.py:16: in get_tmp_dir
    shutil.rmtree(tmp_dir)
..\conda\envs\py37\lib\shutil.py:516: in rmtree
    return _rmtree_unsafe(path, onerror)
..\conda\envs\py37\lib\shutil.py:395: in _rmtree_unsafe
    _rmtree_unsafe(fullname, onerror)
..\conda\envs\py37\lib\shutil.py:395: in _rmtree_unsafe
    _rmtree_unsafe(fullname, onerror)
..\conda\envs\py37\lib\shutil.py:395: in _rmtree_unsafe
    _rmtree_unsafe(fullname, onerror)
..\conda\envs\py37\lib\shutil.py:400: in _rmtree_unsafe
    onerror(os.unlink, fullname, sys.exc_info())
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

path = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp5etnebcf\\gtFine\\test\\bochum'
onerror = <function rmtree.<locals>.onerror at 0x000000323F3BFDC8>

    def _rmtree_unsafe(path, onerror):
        try:
            with os.scandir(path) as scandir_it:
                entries = list(scandir_it)
        except OSError:
            onerror(os.scandir, path, sys.exc_info())
            entries = []
        for entry in entries:
            fullname = entry.path
            try:
                is_dir = entry.is_dir(follow_symlinks=False)
            except OSError:
                is_dir = False
            if is_dir:
                try:
                    if entry.is_symlink():
                        # This can only happen if someone replaces
                        # a directory with a symlink after the call to
                        # os.scandir or entry.is_dir above.
                        raise OSError(""Cannot call rmtree on a symbolic link"")
                except OSError:
                    onerror(os.path.islink, fullname, sys.exc_info())
                    continue
                _rmtree_unsafe(fullname, onerror)
            else:
                try:
>                   os.unlink(fullname)
E                   PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp5etnebcf\\gtFine\\test\\bochum\\bochum_000000_000000_gtFine_color.png'

..\conda\envs\py37\lib\shutil.py:398: PermissionError
__________________________ Tester.test_extract_gzip ___________________________

self = <test_datasets_utils.Tester testMethod=test_extract_gzip>

    def test_extract_gzip(self):
        with get_tmp_dir() as temp_dir:
            with tempfile.NamedTemporaryFile(suffix='.gz') as f:
>               with gzip.GzipFile(f.name, 'wb') as zf:

test\test_datasets_utils.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <[AttributeError(""'GzipFile' object has no attribute 'fileobj'"") raised in repr()] GzipFile object at 0x32007d28c8>
filename = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpc1wq6shu.gz'
mode = 'wb', compresslevel = 9, fileobj = None, mtime = None

    def __init__(self, filename=None, mode=None,
                 compresslevel=9, fileobj=None, mtime=None):
        """"""Constructor for the GzipFile class.
    
        At least one of fileobj and filename must be given a
        non-trivial value.
    
        The new class instance is based on fileobj, which can be a regular
        file, an io.BytesIO object, or any other object which simulates a file.
        It defaults to None, in which case filename is opened to provide
        a file object.
    
        When fileobj is not None, the filename argument is only used to be
        included in the gzip file header, which may include the original
        filename of the uncompressed file.  It defaults to the filename of
        fileobj, if discernible; otherwise, it defaults to the empty string,
        and in this case the original filename is not included in the header.
    
        The mode argument can be any of 'r', 'rb', 'a', 'ab', 'w', 'wb', 'x', or
        'xb' depending on whether the file will be read or written.  The default
        is the mode of fileobj if discernible; otherwise, the default is 'rb'.
        A mode of 'r' is equivalent to one of 'rb', and similarly for 'w' and
        'wb', 'a' and 'ab', and 'x' and 'xb'.
    
        The compresslevel argument is an integer from 0 to 9 controlling the
        level of compression; 1 is fastest and produces the least compression,
        and 9 is slowest and produces the most compression. 0 is no compression
        at all. The default is 9.
    
        The mtime argument is an optional numeric timestamp to be written
        to the last modification time field in the stream when compressing.
        If omitted or None, the current time is used.
    
        """"""
    
        if mode and ('t' in mode or 'U' in mode):
            raise ValueError(""Invalid mode: {!r}"".format(mode))
        if mode and 'b' not in mode:
            mode += 'b'
        if fileobj is None:
>           fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')
E           PermissionError: [Errno 13] Permission denied: 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpc1wq6shu.gz'

..\conda\envs\py37\lib\gzip.py:163: PermissionError
___________________________ Tester.test_extract_tar ___________________________

self = <test_datasets_utils.Tester testMethod=test_extract_tar>

    def test_extract_tar(self):
        for ext, mode in zip(['.tar', '.tar.gz'], ['w', 'w:gz']):
            with get_tmp_dir() as temp_dir:
                with tempfile.NamedTemporaryFile() as bf:
                    bf.write(""this is the content"".encode())
                    bf.seek(0)
                    with tempfile.NamedTemporaryFile(suffix=ext) as f:
>                       with tarfile.open(f.name, mode=mode) as zf:

test\test_datasets_utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\tarfile.py:1611: in open
    return cls.taropen(name, mode, fileobj, **kwargs)
..\conda\envs\py37\lib\tarfile.py:1621: in taropen
    return cls(name, mode, fileobj, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tarfile.TarFile object at 0x0000003200A91AC8>
name = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmplby3znrd.tar', mode = 'w'
fileobj = None, format = None, tarinfo = None, dereference = None
ignore_zeros = None, encoding = None, errors = 'surrogateescape'
pax_headers = None, debug = None, errorlevel = None, copybufsize = None

    def __init__(self, name=None, mode=""r"", fileobj=None, format=None,
            tarinfo=None, dereference=None, ignore_zeros=None, encoding=None,
            errors=""surrogateescape"", pax_headers=None, debug=None,
            errorlevel=None, copybufsize=None):
        """"""Open an (uncompressed) tar archive `name'. `mode' is either 'r' to
           read from an existing archive, 'a' to append data to an existing
           file or 'w' to create a new file overwriting an existing one. `mode'
           defaults to 'r'.
           If `fileobj' is given, it is used for reading or writing data. If it
           can be determined, `mode' is overridden by `fileobj's mode.
           `fileobj' is not closed, when TarFile is closed.
        """"""
        modes = {""r"": ""rb"", ""a"": ""r+b"", ""w"": ""wb"", ""x"": ""xb""}
        if mode not in modes:
            raise ValueError(""mode must be 'r', 'a', 'w' or 'x'"")
        self.mode = mode
        self._mode = modes[mode]
    
        if not fileobj:
            if self.mode == ""a"" and not os.path.exists(name):
                # Create nonexistent files in append mode.
                self.mode = ""w""
                self._mode = ""wb""
>           fileobj = bltn_open(name, self._mode)
E           PermissionError: [Errno 13] Permission denied: 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmplby3znrd.tar'

..\conda\envs\py37\lib\tarfile.py:1436: PermissionError
___________________________ Tester.test_extract_zip ___________________________

self = <test_datasets_utils.Tester testMethod=test_extract_zip>

    def test_extract_zip(self):
        with get_tmp_dir() as temp_dir:
            with tempfile.NamedTemporaryFile(suffix='.zip') as f:
                with zipfile.ZipFile(f, 'w') as zf:
                    zf.writestr('file.tst', 'this is the content')
>               utils.extract_archive(f.name, temp_dir)

test\test_datasets_utils.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\site-packages\torchvision\datasets\utils.py:231: in extract_archive
    with zipfile.ZipFile(from_path, 'r') as z:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <zipfile.ZipFile [closed]>
file = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpiwmc4x4z.zip', mode = 'r'
compression = 0, allowZip64 = True, compresslevel = None

    def __init__(self, file, mode=""r"", compression=ZIP_STORED, allowZip64=True,
                 compresslevel=None):
        """"""Open the ZIP file with mode read 'r', write 'w', exclusive create 'x',
        or append 'a'.""""""
        if mode not in ('r', 'w', 'x', 'a'):
            raise ValueError(""ZipFile requires mode 'r', 'w', 'x', or 'a'"")
    
        _check_compression(compression)
    
        self._allowZip64 = allowZip64
        self._didModify = False
        self.debug = 0  # Level of printing: 0 through 3
        self.NameToInfo = {}    # Find file info given name
        self.filelist = []      # List of ZipInfo instances for archive
        self.compression = compression  # Method of compression
        self.compresslevel = compresslevel
        self.mode = mode
        self.pwd = None
        self._comment = b''
    
        # Check if we were passed a file-like object
        if isinstance(file, os.PathLike):
            file = os.fspath(file)
        if isinstance(file, str):
            # No, it's a filename
            self._filePassed = 0
            self.filename = file
            modeDict = {'r' : 'rb', 'w': 'w+b', 'x': 'x+b', 'a' : 'r+b',
                        'r+b': 'w+b', 'w+b': 'wb', 'x+b': 'xb'}
            filemode = modeDict[mode]
            while True:
                try:
>                   self.fp = io.open(file, filemode)
E                   PermissionError: [Errno 13] Permission denied: 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpiwmc4x4z.zip'

..\conda\envs\py37\lib\zipfile.py:1207: PermissionError
___________________________ Tester.test_video_clips ___________________________

self = <test_datasets_video_utils.Tester testMethod=test_video_clips>

    def test_video_clips(self):
        with get_list_of_videos(num_videos=3) as video_list:
>           video_clips = VideoClips(video_list, 5, 5)

test\test_datasets_video_utils.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\site-packages\torchvision\datasets\video_utils.py:55: in __init__
    self._compute_frame_pts()
..\conda\envs\py37\lib\site-packages\torchvision\datasets\video_utils.py:84: in _compute_frame_pts
    for batch in dl:
..\conda\envs\py37\lib\site-packages\torch\utils\data\dataloader.py:278: in __iter__
    return _MultiProcessingDataLoaderIter(self)
..\conda\envs\py37\lib\site-packages\torch\utils\data\dataloader.py:682: in __init__
    w.start()
..\conda\envs\py37\lib\multiprocessing\process.py:112: in start
    self._popen = self._Popen(self)
..\conda\envs\py37\lib\multiprocessing\context.py:223: in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
..\conda\envs\py37\lib\multiprocessing\context.py:322: in _Popen
    return Popen(process_obj)
..\conda\envs\py37\lib\multiprocessing\popen_spawn_win32.py:89: in __init__
    reduction.dump(process_obj, to_child)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <Process(Process-1, initial daemon)>, file = <_io.BufferedWriter name=10>
protocol = None

    def dump(obj, file, protocol=None):
        '''Replacement for pickle.dump() using ForkingPickler.'''
>       ForkingPickler(file, protocol).dump(obj)
E       AttributeError: Can't pickle local object 'VideoClips._compute_frame_pts.<locals>.DS'

..\conda\envs\py37\lib\multiprocessing\reduction.py:60: AttributeError
---------------------------- Captured stderr call -----------------------------

_____________________ Tester.test_video_clips_custom_fps ______________________

self = <test_datasets_video_utils.Tester testMethod=test_video_clips_custom_fps>

    def test_video_clips_custom_fps(self):
        with get_list_of_videos(num_videos=3, sizes=[12, 12, 12], fps=[3, 4, 6]) as video_list:
            num_frames = 4
            for fps in [1, 3, 4, 10]:
>               video_clips = VideoClips(video_list, num_frames, num_frames, fps)

test\test_datasets_video_utils.py:117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\site-packages\torchvision\datasets\video_utils.py:55: in __init__
    self._compute_frame_pts()
..\conda\envs\py37\lib\site-packages\torchvision\datasets\video_utils.py:84: in _compute_frame_pts
    for batch in dl:
..\conda\envs\py37\lib\site-packages\torch\utils\data\dataloader.py:278: in __iter__
    return _MultiProcessingDataLoaderIter(self)
..\conda\envs\py37\lib\site-packages\torch\utils\data\dataloader.py:682: in __init__
    w.start()
..\conda\envs\py37\lib\multiprocessing\process.py:112: in start
    self._popen = self._Popen(self)
..\conda\envs\py37\lib\multiprocessing\context.py:223: in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
..\conda\envs\py37\lib\multiprocessing\context.py:322: in _Popen
    return Popen(process_obj)
..\conda\envs\py37\lib\multiprocessing\popen_spawn_win32.py:89: in __init__
    reduction.dump(process_obj, to_child)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = <Process(Process-2, initial daemon)>, file = <_io.BufferedWriter name=10>
protocol = None

    def dump(obj, file, protocol=None):
        '''Replacement for pickle.dump() using ForkingPickler.'''
>       ForkingPickler(file, protocol).dump(obj)
E       AttributeError: Can't pickle local object 'VideoClips._compute_frame_pts.<locals>.DS'

..\conda\envs\py37\lib\multiprocessing\reduction.py:60: AttributeError
---------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):

  File ""<string>"", line 1, in <module>

  File ""c:\w\2\s\packaging\windows\conda\envs\py37\lib\multiprocessing\spawn.py"", line 105, in spawn_main

    exitcode = _main(fd)

  File ""c:\w\2\s\packaging\windows\conda\envs\py37\lib\multiprocessing\spawn.py"", line 115, in _main

    self = reduction.pickle.load(from_parent)

EOFError: Ran out of input


-------------------------- Captured stderr teardown ---------------------------
Traceback (most recent call last):

  File ""<string>"", line 1, in <module>

  File ""c:\w\2\s\packaging\windows\conda\envs\py37\lib\multiprocessing\spawn.py"", line 105, in spawn_main

    exitcode = _main(fd)

  File ""c:\w\2\s\packaging\windows\conda\envs\py37\lib\multiprocessing\spawn.py"", line 115, in _main

    self = reduction.pickle.load(from_parent)

EOFError: Ran out of input

_______________________ Tester.test_read_partial_video ________________________

self = <test_io.Tester testMethod=test_read_partial_video>

    def test_read_partial_video(self):
>       with temp_video(10, 300, 300, 5, lossless=True) as (f_name, data):

test\test_io.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\contextlib.py:112: in __enter__
    return next(self.gen)
test\test_io.py:51: in temp_video
    io.write_video(f.name, data, fps=fps, video_codec=video_codec, options=options)
..\conda\envs\py37\lib\site-packages\torchvision\io\video.py:55: in write_video
    container.mux(packet)
av/container/output.pyx:198: in av.container.output.OutputContainer.mux
    ???
av/container/output.pyx:204: in av.container.output.OutputContainer.mux_one
    ???
av/container/output.pyx:166: in av.container.output.OutputContainer.start_encoding
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   av.AVError: [Errno 13] Permission denied

av/utils.pyx:109: AVError
___________________ Tester.test_read_partial_video_bframes ____________________

self = <test_io.Tester testMethod=test_read_partial_video_bframes>

    def test_read_partial_video_bframes(self):
        # do not use lossless encoding, to test the presence of B-frames
        options = {'bframes': '16', 'keyint': '10', 'min-keyint': '4'}
>       with temp_video(100, 300, 300, 5, options=options) as (f_name, data):

test\test_io.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\contextlib.py:112: in __enter__
    return next(self.gen)
test\test_io.py:51: in temp_video
    io.write_video(f.name, data, fps=fps, video_codec=video_codec, options=options)
..\conda\envs\py37\lib\site-packages\torchvision\io\video.py:55: in write_video
    container.mux(packet)
av/container/output.pyx:198: in av.container.output.OutputContainer.mux
    ???
av/container/output.pyx:204: in av.container.output.OutputContainer.mux_one
    ???
av/container/output.pyx:166: in av.container.output.OutputContainer.start_encoding
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   av.AVError: [Errno 13] Permission denied

av/utils.pyx:109: AVError
_________________________ Tester.test_read_timestamps _________________________

self = <test_io.Tester testMethod=test_read_timestamps>

    def test_read_timestamps(self):
>       with temp_video(10, 300, 300, 5) as (f_name, data):

test\test_io.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\contextlib.py:112: in __enter__
    return next(self.gen)
test\test_io.py:51: in temp_video
    io.write_video(f.name, data, fps=fps, video_codec=video_codec, options=options)
..\conda\envs\py37\lib\site-packages\torchvision\io\video.py:59: in write_video
    container.mux(packet)
av/container/output.pyx:198: in av.container.output.OutputContainer.mux
    ???
av/container/output.pyx:204: in av.container.output.OutputContainer.mux_one
    ???
av/container/output.pyx:166: in av.container.output.OutputContainer.start_encoding
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   av.AVError: [Errno 13] Permission denied

av/utils.pyx:109: AVError
___________________ Tester.test_read_timestamps_from_packet ___________________

self = <test_io.Tester testMethod=test_read_timestamps_from_packet>

    def test_read_timestamps_from_packet(self):
>       with temp_video(10, 300, 300, 5, video_codec='mpeg4') as (f_name, data):

test\test_io.py:129: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\contextlib.py:112: in __enter__
    return next(self.gen)
test\test_io.py:51: in temp_video
    io.write_video(f.name, data, fps=fps, video_codec=video_codec, options=options)
..\conda\envs\py37\lib\site-packages\torchvision\io\video.py:55: in write_video
    container.mux(packet)
av/container/output.pyx:198: in av.container.output.OutputContainer.mux
    ???
av/container/output.pyx:204: in av.container.output.OutputContainer.mux_one
    ???
av/container/output.pyx:166: in av.container.output.OutputContainer.start_encoding
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   av.AVError: [Errno 13] Permission denied

av/utils.pyx:109: AVError
________________________ Tester.test_write_read_video _________________________

self = <test_io.Tester testMethod=test_write_read_video>

    def test_write_read_video(self):
>       with temp_video(10, 300, 300, 5, lossless=True) as (f_name, data):

test\test_io.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\contextlib.py:112: in __enter__
    return next(self.gen)
test\test_io.py:51: in temp_video
    io.write_video(f.name, data, fps=fps, video_codec=video_codec, options=options)
..\conda\envs\py37\lib\site-packages\torchvision\io\video.py:55: in write_video
    container.mux(packet)
av/container/output.pyx:198: in av.container.output.OutputContainer.mux
    ???
av/container/output.pyx:204: in av.container.output.OutputContainer.mux_one
    ???
av/container/output.pyx:166: in av.container.output.OutputContainer.start_encoding
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   av.AVError: [Errno 13] Permission denied

av/utils.pyx:109: AVError
___________________________ Tester.test_save_image ____________________________

self = <test_utils.Tester testMethod=test_save_image>

    def test_save_image(self):
        with tempfile.NamedTemporaryFile(suffix='.png') as f:
            t = torch.rand(2, 3, 64, 64)
>           utils.save_image(t, f.name)

test\test_utils.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\site-packages\torchvision\utils.py:105: in save_image
    im.save(filename)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <PIL.Image.Image image mode=RGB size=134x68 at 0x323F3755C8>
fp = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpm0s9rq8o.png'
format = 'PNG', params = {}
filename = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpm0s9rq8o.png'
open_fp = True, save_all = False, ext = '.png'
save_handler = <function _save at 0x000000323CA1FA68>

    def save(self, fp, format=None, **params):
        """"""
        Saves this image under the given filename.  If no format is
        specified, the format to use is determined from the filename
        extension, if possible.
    
        Keyword options can be used to provide additional instructions
        to the writer. If a writer doesn't recognise an option, it is
        silently ignored. The available options are described in the
        :doc:`image format documentation
        <../handbook/image-file-formats>` for each writer.
    
        You can use a file object instead of a filename. In this case,
        you must always specify the format. The file object must
        implement the ``seek``, ``tell``, and ``write``
        methods, and be opened in binary mode.
    
        :param fp: A filename (string), pathlib.Path object or file object.
        :param format: Optional format override.  If omitted, the
           format to use is determined from the filename extension.
           If a file object was used instead of a filename, this
           parameter should always be used.
        :param params: Extra parameters to the image writer.
        :returns: None
        :exception ValueError: If the output format could not be determined
           from the file name.  Use the format option to solve this.
        :exception IOError: If the file could not be written.  The file
           may have been created, and may contain partial data.
        """"""
    
        filename = """"
        open_fp = False
        if isPath(fp):
            filename = fp
            open_fp = True
        elif HAS_PATHLIB and isinstance(fp, Path):
            filename = str(fp)
            open_fp = True
        if not filename and hasattr(fp, ""name"") and isPath(fp.name):
            # only set the name for metadata purposes
            filename = fp.name
    
        # may mutate self!
        self._ensure_mutable()
    
        save_all = params.pop(""save_all"", False)
        self.encoderinfo = params
        self.encoderconfig = ()
    
        preinit()
    
        ext = os.path.splitext(filename)[1].lower()
    
        if not format:
            if ext not in EXTENSION:
                init()
            try:
                format = EXTENSION[ext]
            except KeyError:
                raise ValueError(""unknown file extension: {}"".format(ext))
    
        if format.upper() not in SAVE:
            init()
        if save_all:
            save_handler = SAVE_ALL[format.upper()]
        else:
            save_handler = SAVE[format.upper()]
    
        if open_fp:
            if params.get(""append"", False):
                fp = builtins.open(filename, ""r+b"")
            else:
                # Open also for reading (""+""), because TIFF save_all
                # writer needs to go back and edit the written data.
>               fp = builtins.open(filename, ""w+b"")
E               PermissionError: [Errno 13] Permission denied: 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpm0s9rq8o.png'

..\conda\envs\py37\lib\site-packages\PIL\Image.py:2085: PermissionError
_____________________ Tester.test_save_image_single_pixel _____________________

self = <test_utils.Tester testMethod=test_save_image_single_pixel>

    def test_save_image_single_pixel(self):
        with tempfile.NamedTemporaryFile(suffix='.png') as f:
            t = torch.rand(1, 3, 1, 1)
>           utils.save_image(t, f.name)

test\test_utils.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\conda\envs\py37\lib\site-packages\torchvision\utils.py:105: in save_image
    im.save(filename)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <PIL.Image.Image image mode=RGB size=1x1 at 0x323F678748>
fp = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpn5hd8b_0.png'
format = 'PNG', params = {}
filename = 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpn5hd8b_0.png'
open_fp = True, save_all = False, ext = '.png'
save_handler = <function _save at 0x000000323CA1FA68>

    def save(self, fp, format=None, **params):
        """"""
        Saves this image under the given filename.  If no format is
        specified, the format to use is determined from the filename
        extension, if possible.
    
        Keyword options can be used to provide additional instructions
        to the writer. If a writer doesn't recognise an option, it is
        silently ignored. The available options are described in the
        :doc:`image format documentation
        <../handbook/image-file-formats>` for each writer.
    
        You can use a file object instead of a filename. In this case,
        you must always specify the format. The file object must
        implement the ``seek``, ``tell``, and ``write``
        methods, and be opened in binary mode.
    
        :param fp: A filename (string), pathlib.Path object or file object.
        :param format: Optional format override.  If omitted, the
           format to use is determined from the filename extension.
           If a file object was used instead of a filename, this
           parameter should always be used.
        :param params: Extra parameters to the image writer.
        :returns: None
        :exception ValueError: If the output format could not be determined
           from the file name.  Use the format option to solve this.
        :exception IOError: If the file could not be written.  The file
           may have been created, and may contain partial data.
        """"""
    
        filename = """"
        open_fp = False
        if isPath(fp):
            filename = fp
            open_fp = True
        elif HAS_PATHLIB and isinstance(fp, Path):
            filename = str(fp)
            open_fp = True
        if not filename and hasattr(fp, ""name"") and isPath(fp.name):
            # only set the name for metadata purposes
            filename = fp.name
    
        # may mutate self!
        self._ensure_mutable()
    
        save_all = params.pop(""save_all"", False)
        self.encoderinfo = params
        self.encoderconfig = ()
    
        preinit()
    
        ext = os.path.splitext(filename)[1].lower()
    
        if not format:
            if ext not in EXTENSION:
                init()
            try:
                format = EXTENSION[ext]
            except KeyError:
                raise ValueError(""unknown file extension: {}"".format(ext))
    
        if format.upper() not in SAVE:
            init()
        if save_all:
            save_handler = SAVE_ALL[format.upper()]
        else:
            save_handler = SAVE[format.upper()]
    
        if open_fp:
            if params.get(""append"", False):
                fp = builtins.open(filename, ""r+b"")
            else:
                # Open also for reading (""+""), because TIFF save_all
                # writer needs to go back and edit the written data.
>               fp = builtins.open(filename, ""w+b"")
E               PermissionError: [Errno 13] Permission denied: 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpn5hd8b_0.png'

..\conda\envs\py37\lib\site-packages\PIL\Image.py:2085: PermissionError
============================== warnings summary ===============================
c:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torchvision\datasets\lsun.py:8
  c:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torchvision\datasets\lsun.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Iterable

c:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\av\container\__init__.py:1
  c:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\av\container\__init__.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from .core import Container, open

test/test_datasets.py::Tester::test_imagenet
  c:\w\2\s\packaging\windows\conda\envs\py37\lib\importlib\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
    return f(*args, **kwds)

test/test_transforms.py::Tester::test_randomperspective
  c:\w\2\s\packaging\windows\conda\envs\py37\lib\site-packages\torchvision\transforms\functional.py:440: UserWarning: torch.gels is deprecated in favour of torch.lstsq and will be removed in the next release. Please use torch.lstsq instead.
    res = torch.gels(B, A)[0]

-- Docs: https://docs.pytest.org/en/latest/warnings.html
======= 32 failed, 141 passed, 14 skipped, 4 warnings in 407.68 seconds =======
```

cc @peterjc123 @nbcsm @guyang3532 @maxluk @gunandrose4u @smartcat2010 @mszhanyi"
Consistent argument names for video datasets and VideoClips class,pytorch/vision,2019-08-10 14:04:37,5,enhancement#help wanted#module: io#module: video,1227,479270654,"First of all, the new support for Video data is amazing. Really useful. 😃 

Just when I was going through the code of the video datasets and utilities, I found this a bit confusing.

Sample Dataset:
https://github.com/pytorch/vision/blob/8635be94d1216f10fb8302da89233bd86445e449/torchvision/datasets/ucf101.py#L27-L36

`VideoClips` class: 
https://github.com/pytorch/vision/blob/8635be94d1216f10fb8302da89233bd86445e449/torchvision/datasets/video_utils.py#L43-L49

From what I understand, the argument `frames_per_clip` from the dataset is the same as `clip_length_in_frames` from the `VideoClips` class. 

Similarly, `step_between_clips` <-> `frames_between_clips`.

Is there a reason for this difference. Or did I understand it wrong?

Would it be better if both, the video datasets and the `VideoClips` class had consistent arguments? 

In my opinion, `frames_per_clip` looks better than `clip_length_in_frames`.
And `frames_between_clips` can be favored over `step_between_clips` since the former feels more intuitive.

Any suggestions? @fmassa "
[Don't Close] Replacing PIL and OpenCV,pytorch/vision,2019-08-10 11:26:37,2,,1226,479257516,"Here is a TODO list of the replacing process.
+ [ ] Reading image files into memory
+ [ ] Reading PNG images from memory
+ [ ] Reading JPG images from memory
+ [ ] Having a single function that reads the implemented image types.
+ [ ] Having a function that reads the implemented image types and returns a fixed type and format and ```CHW```.
+ [ ] Replacing ```PIL```"
Add stream flag to io.read_video,pytorch/vision,2019-08-08 21:45:05,2,module: io,1220,478701836,"The `torchvision.io.read_video` function does not have an option to exclude reading certain streams. 

I would like to be able to load the frames of a video without loading audio.  In order to do this I had to use internal functions like so:
```python
container = av.open(fileath, metadata_errors='ignore')

info = {}
video_frames = []
video_frames = torchvision.io.video._read_from_stream(container, start, end,
                                      container.streams.video[0], {'video': 0})
info[""video_fps""] = float(container.streams.video[0].average_rate)
````

Obviously this exposes the PyAV backend, therefore I envision two options:
- functions for video only, audio only, and both. 
- optional parameters added to the `read_video` function.
"
Add a requirements.txt for testing dependency,pytorch/vision,2019-08-05 21:27:41,1,enhancement#module: tests,1200,477066075,"There are bunch of non-required deps which are needed to get full test coverage. It would be helpful if they were all collected somewhere, e.g., in a requirements.txt"
Ton of DEPRECATE_MESSAGE warnings,pytorch/vision,2019-07-25 15:57:47,14,help wanted#module: models,1173,472946250,"Sample:

```
gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wformat -fPIC -I/remote/wheel/vision/test -I/remote/wheel/vision/torchvision/csrc/models -I/opt/python/cp37-cp37m/lib/python3.7/site-packages/torch/include -I/opt/python/cp37-cp37m/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/opt/python/cp37-cp37m/lib/python3.7/site-packages/torch/include/TH -I/opt/python/cp37-cp37m/lib/python3.7/site-packages/torch/include/THC -I/opt/python/cp37-cp37m/include/python3.7m -c /remote/wheel/vision/torchvision/csrc/models/shufflenetv2.cpp -o build/temp.linux-x86_64-3.7/remote/wheel/vision/torchvision/csrc/models/shufflenetv2.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=_C_tests -D_GLIBCXX_USE_CXX11_ABI
=0 -std=c++11          #warning DEPRECATE_MESSAGE [-Wcpp]                            :
 #  warning DEPRECATE_MESSAGEthon3.7/site-packages/torch/include/torch/csrc/api/include/torch/    ^


```"
[Feature proposal] Allow processing multiple images with transforms.Compose,pytorch/vision,2019-07-25 14:32:15,4,module: transforms,1169,472901121,"### Proposal
Following discussions started in #9, #230, #240, and most recently in #610, I would like to propose the following change to `transforms.Compose` (and the transform classes) that would allow easy **processing of multiple images with the same parameters** using existing infrastructure. I think this would be very useful for segmentation tasks, where both the image and label need to be transformed exactly the same way.

### Details
Currently the problem is that each transform, when called, implicitly generates randomized parameters (if it is a random transform) right before computing the transformation. In my opinion, it doesn't have to be so - parameter generation (`get_params`) is already separated from the actual image operation (which relies on the functional backend). My idea comes in two parts: first, completely decouple parameter generation from transformation; then allow `Compose` to generate parameters once and apply transformations multiple times.

**Step 1**, on the example of [`RandomResizedCrop`](https://github.com/pytorch/vision/blob/3483342733673c3182bd5f8a4de3723a74ce5845/torchvision/transforms/transforms.py#L598):

1. Add a `generate_params` method, to access the existing `get_params` but without the need to pass specific arguments. This function would look exactly the same for every transform that needs any random parameters. Passing specific arguments to `get_params` will be implementation-dependent.

```python
def generate_params(self, image):
    return self.get_params(image, self.scale, self.ratio)
```

2. Allow `__call__` to *optionally* accept a tuple of pre-generated params:

```python
def __call__(self, img, params=None):
    if params:
        i, j, h, w = params
    else:
        i, j, h, w = self.get_params(img, self.scale, self.ratio)
    return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)
```

**Step 2** is enabling this functionality in [`Compose`](https://github.com/pytorch/vision/blob/3483342733673c3182bd5f8a4de3723a74ce5845/torchvision/transforms/transforms.py#L43) by changing `__call__` to accept iterables. Alternatively, we could subclass it entirely, which I will do in this example:

```python
class MultiCompose(Compose):
    def __call__(self, imgs):
        for t in self.transforms:
            try:
                params = t.generate_params()
            except AttributeError:
                params = None
            imgs = tuple(t(img, params) for img in imgs)
        return imgs
```

Subclassing offers some advantages, for example interpolation methods could be bound to iterable indices at `__init__`, so we could interpolate the first item bilinearly, and the second with nearest-neighbour (ideal for segmentation).

### Alternative approach
Instead of doing try/except in the `Compose` subclass, all transforms could be changed to inherit from a new `BaseTransform` abstract class, which could define `generate_params` as a trivial function returning `None`. Then we could just do:

```python
class MultiCompose(Compose):
    def __call__(self, imgs):
        for t in self.transforms:
            params = t.generate_params()
            imgs = tuple(t(img, params) for img in imgs)
        return imgs
```

because static transforms like `Pad` would simply return None, while any random transforms would need to define `generate_params` accordingly.

Yes I do realize that this requires a slight refactoring of e.g. [`RandomHorizontalFlip`](https://github.com/pytorch/vision/blob/3483342733673c3182bd5f8a4de3723a74ce5845/torchvision/transforms/transforms.py#L482)

### Usage
The user could subclass `Dataset` to yield (image, label) tuples. This change would allow them to apply custom preprocessing/augmentation transformations separately, instead of hard-coding them in the dataset implementation using functional backend. It would look sort of like this:

```python
data_transform = transforms.MultiCompose([
    transforms.RandomCrop(256),
    transforms.ToTensor()
])
data_source = datasets.SegmentationDataset( # user class
    root='/path/to/data/,
    transform=data_transform
)
loader = torch.utils.data.DataLoader(data_source, batch_size=4, num_workers=8)
```

I think this would be a significantly more convenient way of doing this.

Let me know if you think this is worth pursuing. I will have some free time next week so if this would be useful and has a chance of being merged, I'd happily implement it myself. If you see any potential pitfalls or backwards-compatibility ruining caveats - please tell me as well.

---

### Addenda

Later I have found PR #611, but it seems to have been abandoned by now, having encountered some issues that I think my plan of attack can overcome.

Some deal of the problems with this stem from the fact that `get_params`, since their introduction in #311, do not share an interface between classes. Instead, getting params for each transform is a completely different call. This feels anti-OOP and counter-intuitive to me; are there any reasons why this has been made this way? @alykhantejani?

cc @vfdev-5"
mnasnet training schedule,pytorch/vision,2019-07-25 00:41:30,7,help wanted#awaiting response#module: models#topic: classification,1163,472612154,"What is the training schedule for MNASNet 1.0 that gives 73.5 top-1? I tried the mobilenet schedule but it not giving the same accuracy as mentioned in torchvision page. 

On a side note, mnasnet paper reports 74% top1 but torchvision model is 73.5% top-1."
Make `to_pil_image` do an implicit clamp,pytorch/vision,2019-07-24 19:16:26,2,enhancement#module: transforms,1162,472460745,"Related to https://github.com/pytorch/pytorch/issues/21044. As suggested by @ppwwyyxx , we probably should add a type conversion in https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py#L104 when the mode is variant of `int8`. 
Right now if there're negative values in tensor, we'll just keep it as negative, which doesn't align with int8 range, and cause visual overshoot. 

Thoughts? @soumith @fmassa 
example script: 
```
import torch
from torch.nn.functional import interpolate
from PIL import Image
from torchvision.transforms.functional import to_tensor, to_pil_image
import numpy as np
import pdb

image_path = './dog.jpg'
scale_factor = 2.0

image = Image.open(image_path)

# PyTorch
image_tensor = to_tensor(image)
image_tensor.unsqueeze_(0)
image_tensor = interpolate(image_tensor, scale_factor=scale_factor, mode='bicubic', align_corners=False)
image_tensor.squeeze_(0)
# image_tensor contains negative values
to_pil_image(image_tensor, mode='RGB').save('pt.png')
```"
cuda extension build shouldn't link against cudart.so.9.0 ,pytorch/vision,2019-07-24 16:41:56,0,,1159,472386688,"right now it links against something like "".9.0"", it should either not link against cudart.so or link against a generic name"
Inquiry about memory usage compare to maskrcnn_benchmark repository.,pytorch/vision,2019-07-22 09:12:22,7,awaiting response#module: ops,1150,470981508,"I cross checked that FasterRCNN in torchvision and in https://github.com/facebookresearch/maskrcnn-benchmark/ are almost identical.

I tried both repository and compared memory usage. What I found is that maskrcnn_benchmark repository use less GPU. I am using Titan xp pascal 1 GPU, and I could run it with 5 batch images for maskrcnn repo, but for torchvision, 3 batch was maximum.
Can I know why?"
Segmentation reference train script,pytorch/vision,2019-07-21 20:49:00,4,awaiting response#needs reproduction#module: models#module: reference scripts#topic: semantic segmentation,1148,470823193,"I am training deeplabv3 model from torchvision for 91 classes of coco. I am using training script from references. I use following command to train the model. 
`python -m torch.distributed.launch --nproc_per_node=4 --use_env train.py --dataset coco --model deeplabv3_resnet101 --output-dir output/ -b 2 -j 4 `

However the training gets stuck at with no error or anything
_Epoch: [6]  [ 8160/14525]  eta: 2:14:49  lr: 0.008007956757936691  loss: 0.3471 (0.5539)  time: 1.2725  data: 0.0021  max mem: 4783_

"
VGGFace2 dataset,pytorch/vision,2019-07-19 13:01:01,3,enhancement#help wanted#module: datasets,1139,470312679,"@fmassa how would you like to go about this?

Currently, I preprocess VGGFace2 by cropping out the face from each image using the provided bounding box csv. After doing that, I load it as an ImageFolder dataset.

VGGFace2 also has a bunch of other CSVs that annotate gender, age group, and facial landmarks.

I propose we allow users to specify the targets they want (such as ID, gender, age, bounding box etc.) and have a `crop_bb=False` if they only want the face.

What do you think?"
Similarity learning reference code is not documented in the website,pytorch/vision,2019-07-19 11:41:16,2,module: reference scripts#module: documentation,1138,470282032,@dakshjotwani @fmassa 
Keypoint transform,pytorch/vision,2019-07-16 18:03:01,5,enhancement#needs discussion#module: transforms,1131,468790227,"Hi Pytorch community 😄 

I started working on keypoint transformation (as was requested in #523).
I worked on it in the context of data augmentation for object detection tasks.

I submitted a proposal in PR #1118, but as @fmassa pointed out, that's not something we can merge without reviewing the design choices.

I've implemented the functionality by changing the signature of the transform.__call()__ method from: ```def __call__(self, img):``` to ```def run(self, img, keypoints):``` so that every transform can work on a list of keypoints in addition to the image itself.

I've been with keypoints as a point is the most basic element, bounding boxes are defined as points, segmentation mask can be defined as points, facial landmarks are keypoints ...
If we have the ability to transform a point, we have the ability to transform anything.

My goal with that design was to make the data augmentation as straitforward as possible.
I added a wrapper class to transform the XML annotaion from VOCDetection to a keypoint list and fed then to the transform pipeline.

```python
class TransformWrapper(object):
    def __init__(self, transforms):
        super(TransformWrapper, self).__init__()
        self.transforms = transforms
        pass

    def __call__(self, img, anno):

        print(img, anno)

        keypoints = []
        objs = anno['annotation']['object']
        if not isinstance(objs, list):
            objs = [objs]
        for o in objs:
            b = o['bndbox']
            x1 = int(b['xmin'])
            x2 = int(b['xmax'])
            y1 = int(b['ymin'])
            y2 = int(b['ymax'])
            keypoints.append([x1, x2])
            keypoints.append([y1, y2])
        img, keypoints = self.transforms(img, keypoints)
        for o in objs:
            b = o['bndbox']
            x = keypoints.pop(0)
            b['xmin'] = str(int(x[0]))
            b['xmax'] = str(int(x[1]))
            y = keypoints.pop(0)
            b['ymin'] = str(int(y[0]))
            b['ymax'] = str(int(y[1]))
        return img, anno
```
This allows for an usage as simple as 
```python
transform = transformWrapper.TransformWrapper(torchvision.transforms.Compose([torchvision.transforms.Resize(600), torchvision.transforms.ToTensor()]))
vocloader = torchvision.datasets.voc.VOCDetection(""/home/wilmot_p/DATA/"", transforms=transform)
```
And the annotations comes out with values corresponding to the resized image.

The aim of this thread is to bring up other usecases of keypoint transformation that I may not have though of and that may be imcompatible with this design, so that we can make a sensible design decision that works for everyone. So if you have an oppinion on this matter, please share 😄 

Curently, one of the drawbacks of my design is that I broke the interface for Lambda, it use to take only the image as input parameter, it now takes the image and the keypoint list, and that break retro-compatibility."
Add drop_last option to VideoClips,pytorch/vision,2019-07-12 08:56:27,0,enhancement#module: io#module: video,1113,467299600,See comments from @LowikC in https://github.com/pytorch/vision/pull/1077/files#r302533135
Standardization of the datasets,pytorch/vision,2019-07-03 12:30:46,23,needs discussion#module: datasets,1080,463725405,"This is a discussion issue which was kicked of by #1067. Some PRs that contain ideas are #1015 and #1025. I will update this comment regularly with the achieved consensus during the discussion.

Disclaimer: I have never worked with segmentation or detection datasets. If I make same wrong assumption regarding them, feel free to correct me. Furthermore, please help me to fill in the gaps.

---
### Proposed Structure

This issues presents the idea to standardize the `torchvision.datasets`. This could be done by adding parameters to the `VisionDataset` (`split`) or by subclassing it and add task specific parameters (`classes` or `class_to_idx`) to the new `class`es. I imagine it something like this:

```python
import torch.utils.data as data

class VisionDataset(data.Dataset):
    pass

class ClassificationDataset(VisionDataset):
    pass

class SegmentationDataset(VisionDataset):
    pass

class DetectionDataset(VisionDataset):
    pass
```

For our tests we could then have a `generic_*_dataset_test` as is already implement for `ClassificationDataset`s.

---
### `VisionDataset`

- As discussed in #1067 we could unify the argument that selects different parts of the dataset. IMO `split` as a `str` is the most general, but still clear term for this. I would implement this as positional argument within the constructor. This should work for all datasets, since in order to be useful each dataset should have at least a training and a test split. Exceptions to this are the `Fakedata` and `ImageFolder` datasets, which will be discussed separately.

-  IMO every dataset should have a `_download` method in order to be useful for every user of this package. We could have the constructor have `download=True` as keyword argument and call the `download` method within it. As above, the  `Fakedata` and `ImageFolder` datasets will be discussed below.

---
### `Fakedata` and `ImageFolder`

What makes these two datasets special, is that there is nothing to download and they are not splitted in any way. IMO they are not special enough to not generalise the `VisionDataset` as stated above. I propose that we simply remove the `split` and `download` argument from their constructor and raise an exception if someone calls the `download` method.

Furthermore the `Fakedata` dataset is currently a `ClassificationDataset`. We should also create a `FakeSegmentationData` and a  `FakeDetectionData` dataset.

---
### `ClassificationDataset`

The following datasets belong to this category: `CIFAR*`, `ImageNet`, `*MNIST`, `SVHN`, `LSUN`, `SEMEION`, `STL10`, `USPS`, `Caltech*`

- Each dataset should return `PIL.Image, int` if indexed
- Each dataset should have a `classes` parameter, which is a `tuple` with all available classes in human-readable form
- Currently, some datasets have a  `class_to_idx` parameter, which is dictionary that maps the human-readable class to its index used as target. I propose to change the direction, i.e. create a `idx_to_class` parameter, since IMO this is the far more common transformation.

---
### `SegmentationDataset`

The following datasets belong to this category: `VOCSegmentation`

---
### `DetectionDataset`

The following datasets belong to this category:  `CocoDetection`, `VOCDetection`

---
### ToDo

- The following datasets need sorting into the three categories: ~`Caltech101`~, ~`Caltech256`~, 
`CelebA`, `CityScapes`, `Cococaptions`, `Flickr8k`, `Flickr30k`, ~`LSUN`~, `Omniglot`, `PhotoTour`, `SBDataset` (shouldn't this be just called `SBD`?), `SBU`, ~`SEMEION`~, ~`STL10`~, and ~`USPS`~
- ~Add some common arguments / parameters for the `SegmentationDataset` and `DetectionDataset`~

Thoughts and suggestions?"
Standardisation of Dataset API split argument name,pytorch/vision,2019-06-29 14:41:07,3,enhancement#needs discussion#module: datasets,1067,462308693,"I've noticed some naming inconsistencies across the torchvision datasets when it comes to specifying how to split the dataset (train/val/test). We currently have:

- Team `classes`: [LSUN](https://github.com/pytorch/vision/blob/master/torchvision/datasets/lsun.py)
- Team `image_set`: [SBD](https://github.com/pytorch/vision/blob/master/torchvision/datasets/sbd.py), [VOC](https://github.com/pytorch/vision/blob/master/torchvision/datasets/voc.py)
- Team `split` : [CelebA](https://github.com/pytorch/vision/blob/master/torchvision/datasets/celeba.py), [CityScapes](https://github.com/pytorch/vision/blob/master/torchvision/datasets/cityscapes.py), [ImageNet](https://github.com/pytorch/vision/blob/master/torchvision/datasets/imagenet.py), [STL10](https://github.com/pytorch/vision/blob/master/torchvision/datasets/stl10.py), [SVHN](https://github.com/pytorch/vision/blob/master/torchvision/datasets/svhn.py)
- Team `train` : [CIFAR10](https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py), [MNIST](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py), [USPS](https://github.com/pytorch/vision/blob/master/torchvision/datasets/usps.py)

The rest are unspecified - but you can effectively choose the split in them by choosing the root folder (e.g. for COCO).

Is there a reason for different naming conventions for each? If not, is there a case for standardising the argument name to one of the above so it's consistent?"
Accounting for class_id in cityscapes dataset,pytorch/vision,2019-06-28 12:54:12,6,enhancement#help wanted#needs discussion#module: datasets,1064,462021003,"Hi, 

In the dataset class for `Cityscapes`, the classes with their meta-data is mentioned as a namedtuple. However, this information is not used anywhere. Especially for semantic segmenation, the train_id should be used to convert the pixel value (of the label image) to the training label. 

Is this planned to be added anytime soon? 

Edit: There seems to have been a commit that does something like this and that hasn't been merged.
https://github.com/pytorch/vision/pull/1025/commits/39a7a2fb6015792b6c32fd7ee3eeac9774f712a4
 "
inception_v3 of vision 0.3.0 does not fit in DataParallel of torch 1.1.0,pytorch/vision,2019-06-25 00:41:27,9,bug#needs discussion#module: models#topic: classification,1048,460154694,"Environment:
Python 3.5
torch 1.1.0
torchvision 0.3.0

Reproducible example:
`import torch`
`import torchvision`
`model = torchvision.models.inception_v3().cuda()`
`model = torch.nn.DataParallel(model, [0, 1])`
`x = torch.rand((8, 3, 299, 299)).cuda()`
`model.forward(x)`

Error:
> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""env/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 493, in __call__
>     result = self.forward(*input, **kwargs)
>   File ""env/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in forward
>     return self.gather(outputs, self.output_device)
>   File ""/env/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py"", line 165, in gather
>     return gather(outputs, output_device, dim=self.dim)
>   File ""/env/lib/python3.5/site-packages/torch/nn/parallel/scatter_gather.py"", line 67, in gather
>     return gather_map(outputs)
>   File ""env/lib/python3.5/site-packages/torch/nn/parallel/scatter_gather.py"", line 62, in gather_map
>     return type(out)(map(gather_map, zip(*outputs)))
> TypeError: __new__() missing 1 required positional argument: 'aux_logits'

I guess the error occurs because the output of inception_v3 was changed from tuple to namedtuple."
CMake config file to link target,pytorch/vision,2019-06-24 16:29:53,10,enhancement#help wanted#module: models,1046,459986214,"There is no TorchvisionConfig.cmake file generated, or FindTorchvision.cmake, so that we can link the target from CMake. Should there be some, or the library is supposed to be linked in some other way?
If there should be one, I could work on a PR to add it."
"Mini imagenet, tiered imagenet",pytorch/vision,2019-06-16 05:33:14,4,enhancement#help wanted#module: datasets,1028,456604405,"Hello,

Is it possible to add omniglot, mini imagenet and tiered imagenet datasets to torchvision.

And is it possible to upload these datasets to google drive, and share that link, so that people using google colab do not need to download these datasets. they access them directly by adding the dataset to their drives.

This would reduce the time taken to download datasets, as they would be directly available through google drive in untarred manner, making life easy.

Thanks"
Train pretrained faster RCNN with resnet101 FPN as backbone,pytorch/vision,2019-06-05 12:13:12,6,enhancement#help wanted#module: models,993,452467666,"I want to use pretrain faster rcnn of resnet101 FPN to train with my own dataset. 
But, it fails to download pretrain faster-rcnn model of resnet101 FPN. 
This is my code of model part:
backbone =torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101','False')
model_urls ={'resnet101_fpn':'https://download.pytorch.org/models/fasterrcnn_resnet101_fpn_coco-258fb6c6.pth'}
state_dict = load_state_dict_from_url(model_urls['resnet101_fpn'])
model = FasterRCNN(backbone,num_classes=2)

The error looks like this:

Downloading: ""https://download.pytorch.org/models/resnet101-5d3b4d8f.pth"" to /home/user/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth
100%|###############################################################| 178728960/178728960 [00:07<00:00, 23374182.30it/s]
Downloading: ""https://download.pytorch.org/models/fasterrcnn_resnet101_fpn_coco-258fb6c6.pth"" to /home/user/.cache/torch/checkpoints/fasterrcnn_resnet101_fpn_coco-258fb6c6.pth
Traceback (most recent call last):
  File ""detect.py"", line 59, in <module>
    state_dict = load_state_dict_from_url(model_urls['resnet101_fpn'])
  File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/hub.py"", line 433, in load_state_dict_from_url
    _download_url_to_file(url, cached_file, hash_prefix, progress=progress)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/hub.py"", line 349, in _download_url_to_file
    u = urlopen(url)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/urllib/request.py"", line 532, in open
    response = meth(req, response)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/urllib/request.py"", line 642, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/urllib/request.py"", line 570, in error
    return self._call_chain(*args)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/home/user/miniconda/envs/py36/lib/python3.6/urllib/request.py"", line 650, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden

"
HRNet models,pytorch/vision,2019-06-04 00:18:56,2,module: models,987,451742664,"HRNet family for different tasks.

PyTorch reference impl:
https://github.com/HRNet"
MobileNetV2 export to ONNX fails,pytorch/vision,2019-06-03 04:22:33,1,,982,451286935,#https://github.com/pytorch/pytorch/issues/20516
"torchvision[cuda10] installs default pytorch build, instead of cuda10 build",pytorch/vision,2019-05-23 08:36:37,6,bug,949,447519426,"torchvision and pytorch are required to be built in similar environment.
Currently, when calling pip-install on cuda10 build of torchvision, (if torch is not already installed) it implicitly installs the default Pytorch build, which - as of today - is built against cuda9, resulting in raising an error during run-time.

e.g.
`pip install torchvision[cuda10_package] torch[cuda10_package]` - will work
but:
``pip install torchvision[cuda10_package];
pip install torch[cuda10_package]``
or similarily, ``pip install torch[cuda10_package] torchvision; pip install --force torchvision[cuda10_package]``
will not. (order matters)

Can the requirements of each build be specified in such a way that pip will automatically look for the matching build of pytorch?
Alternatively, is it possible for pip to independently decide the correct build based on environment variables?"
Read COCO dataset from ZIP file?,pytorch/vision,2019-05-23 08:01:30,3,module: datasets,947,447505637,"For large datasets on e.g. university clusters, where your data storage is an NFS mount, reading individual files can be slow. It also doesn't support reading ahead. In the cloud, you typically have SSD storage, but unzipping the dataset still takes time.

Would you be open to receiving a pull request that reads the COCO dataset from its zipped version? It adds around 10 lines in the COCO Detection class, and adds another Python file for reading ZIP files in a fork-safe manner (so it works with distributed training)."
Inceptionv3 weight is not consistent with tensorflow,pytorch/vision,2019-05-20 14:53:37,6,bug,926,446161980,"I am trying to reproduce the Frechet Inception Distance with Pytorch. The original code uses InceptionV3 trained with Tensorflow. But using the pretrained weight provided by Pytorch, the behavior is much different with Tensorflow version.

After some research I found that the weight is totally different. For example, the first convolution kernel data of Pytorch is:

```
> dic['Conv2d_1a_3x3.conv.weight'][0,0]
tensor([[-0.2103, -0.3441, -0.0344],
        [-0.1420, -0.2520, -0.0280],
        [ 0.0736,  0.0183,  0.0381]])
```

While tensorflow is

```
> x=f['weights'].value
> x.shape
Out[15]: (3, 3, 3, 32)
> x[:,:,0,0]
array([[ 0.01260555, -0.00162022,  0.09091024],
       [-0.10557341, -0.15358226, -0.04656353],
       [-0.16552085, -0.17688492, -0.10908931]], dtype=float32)
```

I wonder how the weight of Pytorch is obtained. And is there a way of perfectly copying tensorflow's weight to Pytorch?

I noticed there are repo for converting Inceptionv3 to Torch (https://github.com/Moodstocks/inception-v3.torch), but it does not work for Pytorch. Can anyone help me? Thank you very much."
[Feature Request] LMDB Dataset for ImageNet,pytorch/vision,2019-05-19 14:05:54,9,awaiting response#needs discussion#module: datasets,915,445823808,"Is it possible to support LMDB for ImageNet as this one `https://github.com/pytorch/vision/blob/master/torchvision/datasets/lsun.py#L22`. One benefit is that we do not need to save 1.28 M small images on the disk, instead, we can save the whole ImageNet into one single file (maybe several files)."
ColorJitter's get_params doesn't have same arguments as __init__,pytorch/vision,2019-05-13 12:11:12,0,,905,443363946,"According to [the docstring](https://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py#L876), `ColorJitter.get_params` has arguments `same as that of __init__`.

However, while `ColorJitter.__init__` can accept tuples *or* floats as parameters, `get_params` only accepts tuples:

```
In [2]: from torchvision.transforms import ColorJitter

In [3]: ColorJitter((0.8, 0.9), (0.8, 0.9), (0.7, 0.9), (0.1, 0.2))
Out[3]: ColorJitter(brightness=(0.8, 0.9), contrast=(0.8, 0.9), saturation=(0.7, 0.9), hue=(0.1, 0.2))

In [4]: ColorJitter(0.9, 0.9, 0.9, 0.1)
Out[4]: ColorJitter(brightness=[0.09999999999999998, 1.9], contrast=[0.09999999999999998, 1.9], saturation=[0.09999999999999998, 1.9], hue=[-0.1, 0.1])

In [5]: ColorJitter.get_params((0.8, 0.9), (0.8, 0.9), (0.7, 0.9), (0.1, 0.2))
Out[5]:
Compose(
    Lambda()
    Lambda()
    Lambda()
    Lambda()
)

In [6]: ColorJitter.get_params(0.9, 0.9, 0.9, 0.1)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-32bc75c83828> in <module>
----> 1 ColorJitter.get_params(0.9, 0.9, 0.9, 0.1)

~/anaconda3/envs/pytorch1/lib/python3.6/site-packages/torchvision/transforms/transforms.py in get_params(brightness, contrast, saturation, hue)
    805
    806         if brightness is not None:
--> 807             brightness_factor = random.uniform(brightness[0], brightness[1])
    808             transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))
    809

TypeError: 'float' object is not subscriptable

```

This bug means that [workarounds](https://github.com/nianticlabs/monodepth2/blob/master/datasets/mono_dataset.py#L67) are required to support earlier and later torchvision versions.

I feel that `get_params` should call `self._check_input` on the input arguments, just like `__init__`. I don't however know if this would have any knock-on input on anything else.

(This issue I think was introduced around [here](https://github.com/pytorch/vision/issues/545).)"
Potential Resnet Inefficiency,pytorch/vision,2019-05-12 16:19:18,1,needs discussion,902,443121465,"Hi,
I noticed a potential inefficiency with the Resnet implementation in the ordering of maxpool / ReLU operations:
https://github.com/pytorch/vision/blob/50d54a82d1479ffb6dd7469ed05fccdf290a1d84/torchvision/models/resnet.py#L189-L193

This is my thinking:
Consider a 2x2 pooling kernel. If you activate first then maxpool, you're activating 4 values then pooling for a total of 5 operations per kernel operation. However, the position of the max element in any given kernel remains the same regardless of its activation -- the value ordering relationship of elements in a kernel is maintained through activation operations). Conversely, if you maxpool first then activate, it's only 2 operations. 

That is, `maxpool(relu(x)) == relu(maxpool(x))` but the latter costs less computation.

Some quick testing...
Setup:
```python
relu_first = nn.Sequential(
    nn.Conv2d(3, 512, 5),
    nn.ReLU(),
    nn.MaxPool2d(2, 2)
)

maxpool_first = nn.Sequential(
    nn.Conv2d(3, 512, 5),
    nn.MaxPool2d(2, 2),
    nn.ReLU()
)
```

Testing result:
<img width=""384"" alt=""test"" src=""https://user-images.githubusercontent.com/21148987/57584897-8b728c80-7495-11e9-8f57-ab33ba4a11ea.png"">

Consequently, I think there would be an improvement to computation without any changes to functionality simply by reordering the forward pass operations to:
```python
def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.maxpool(x) # Changes here
    x = self.relu(x)    # Changes here

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)

    x = self.avgpool(x)
    x = x.reshape(x.size(0), -1)
    x = self.fc(x)
    return x
```
Maybe there is a good reason for the choice of activation first or you haven't noticed a significant difference in real-time usage. I could be entirely missing a very important piece of thinking...! Please let me know if I am incorrect in any way.

Thanks for all your excellent work, and for your time in considering my thoughts!"
[feature proposal] TwentyCrop 💥,pytorch/vision,2019-04-27 09:10:28,0,module: transforms,877,437923956,"Hi,

I decided to implement TwentyCrop for one of my project. Compared to TenCrop, it flips image BOTH vertically AND horizontally. 💥
The implementation is very easy starting from TenCrop's code! You can see it [here (Google Colab)](https://colab.research.google.com/github/sebastienlange/dermatologist-ai/blob/master/dermatologist_ai.ipynb#scrollTo=a2Tv1HWfOJSt) or [here (GitHub)](https://github.com/sebastienlange/dermatologist-ai/blob/master/dermatologist_ai.ipynb) (search for TwentyCrop).

For this project, I have trained multiple models, and have tested them with 1, 5, 10 and 20-crop.
The best scores can be found [here (Google Colab)](https://colab.research.google.com/github/sebastienlange/dermatologist-ai/blob/master/dermatologist_ai.ipynb#scrollTo=bKZuxEW1w0ag).

Theses ""statistics"" show that (at least on my particular project), almost half the time 20-crop does better than 10-crop, so (again at least on my particular project) TwentyCrop is worthwhile.

Do you think it's worthwile to implement TwentyCrop in PyTorch? If yes, I can fork it and make a pull request.

cc @vfdev-5"
functional : to_tensor() doesn't accommodate for  PIL image mode 'I:16B'.,pytorch/vision,2019-04-16 12:44:07,0,module: transforms,856,433759349,"I am trying to convert a TIFF image, read in through PIL, to Pytorch tensor. I am doing this through `transorms.ToTensor()`. I get the following error:

`RuntimeError: shape '[1460, 1936, 5]' is invalid for input of size 5653120`

The reason for the error is that the function is mistakenly extracting the number of channels as `5`. 

 The `TiffImage.mode` is ` ""I;16B"" `. Since `to_tensor` hasn't got a hard coded rule for `""I;16B""` mode, it gets the number of channels using `len(TiffImage.mode)` which returns the length of the string. 

cc @vfdev-5"
To extend torchvision for video,pytorch/vision,2019-04-16 07:48:18,16,enhancement#module: transforms#module: video,855,433632875,"# Motivation  
I've realized that the way torchvision is coded it's not possible to store a transformation to be applied several times. Video requires the same transformation to be applied to the whole sequence.  
# Proposed changes  
I propose to restructure the code with minor changes such that:  

A base transformation  class (template) were created, providing get_params and reset_params method:

```
class BaseTransformation(object):  
    def get_params(self):  
        pass  
    def reset_params(self):  
        pass  
```
get_params would provide needed parameters if necessary meanwhile reset_params would act as param initilizer + reseter.  

To modify compose class  to deal with list/tuples of frames such that when the list were exhausted, paramters would be reset:  
```
class Compose(object):
    """"""Composes several transforms together.

    Args:
        transforms (list of ``Transform`` objects): list of transforms to compose.

    Example:
        >>> transforms.Compose([
        >>>     transforms.CenterCrop(10),
        >>>     transforms.ToTensor(),
        >>> ])
    """"""

    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, inpt):
        if isinstance(inpt,(list,tuple)):
            return self.apply_sequence(inpt)
        else:
            return self.apply_img(inpt)
    def apply_img(self,img):
        for t in self.transforms:
            img = t(img)
        return img
    def apply_sequence(self,seq):
        output = list(map(self.apply_img,seq))
        for t in self.transforms:
            t.reset_params()
        return output
    def __repr__(self):
        format_string = self.__class__.__name__ + '('
        for t in self.transforms:
            format_string += '\n'
            format_string += '    {0}'.format(t)
        format_string += '\n)'
        return format_string
 ```
To set random parameters and image parameters as object attributes. As some parameters requires image features to be computed, parameters would be initialized as None and computed/stored with the 1st frame:  
Example 1:
```
class RandomHorizontalFlip(object):
    """"""Horizontally flip the given PIL Image randomly with a given probability.

    Args:
        p (float): probability of the image being flipped. Default value is 0.5
    """"""

    def __init__(self, p=0.5):
        self.p = p

    def __call__(self, img):
        """"""
        Args:
            img (PIL Image): Image to be flipped.

        Returns:
            PIL Image: Randomly flipped image.
        """"""
        if self.flag is None: #This was initially if random.random() < self.p: so it was not possible
                                       #to apply the same transformation to another frame
            self.get_paramters()
        if self.flag:
            return F.hflip(img)
        return img

    def __repr__(self):
        return self.__class__.__name__ + '(p={})'.format(self.p)
    def get_paramters(self):
        self.flag = random.random() < self.p
    def reset_params(self):
        self.flag = None
```
Example 2:
```
class RandomResizedCrop(BaseTransformation):
    """"""Crop the given PIL Image to random size and aspect ratio.

    A crop of random size (default: of 0.08 to 1.0) of the original size and a random
    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop
    is finally resized to given size.
    This is popularly used to train the Inception networks.

    Args:
        size: expected output size of each edge
        scale: range of size of the origin size cropped
        ratio: range of aspect ratio of the origin aspect ratio cropped
        interpolation: Default: PIL.Image.BILINEAR
    """"""

    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR):
        if isinstance(size, tuple):
            self.size = size
        else:
            self.size = (size, size)
        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):
            warnings.warn(""range should be of kind (min, max)"")

        self.interpolation = interpolation
        self.scale = scale
        self.ratio = ratio
        self.reset_params()

    def get_params(self,img, scale, ratio):
        """"""Get parameters for ``crop`` for a random sized crop.

        Args:
            img (PIL Image): Image to be cropped.
            scale (tuple): range of size of the origin size cropped
            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped

        Returns:
            tuple: params (i, j, h, w) to be passed to ``crop`` for a random
                sized crop.
        """"""
        area = img.size[0] * img.size[1]

        for attempt in range(10):
            target_area = random.uniform(*scale) * area
            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))

            w = int(round(math.sqrt(target_area * aspect_ratio)))
            h = int(round(math.sqrt(target_area / aspect_ratio)))

            if w <= img.size[0] and h <= img.size[1]:
                i = random.randint(0, img.size[1] - h)
                j = random.randint(0, img.size[0] - w)
                return i, j, h, w

        # Fallback to central crop
        in_ratio = img.size[0] / img.size[1]
        if (in_ratio < min(ratio)):
            w = img.size[0]
            h = w / min(ratio)
        elif (in_ratio > max(ratio)):
            h = img.size[1]
            w = h * max(ratio)
        else:  # whole image
            w = img.size[0]
            h = img.size[1]
        self.i = (img.size[1] - h) // 2
        self.j = (img.size[0] - w) // 2
        self.h = h
        self.w = w
        
    def reset_params(self):
        self.i = None
        self.j = None
        self.h = None
        self.w = None    
        
    def __call__(self, img):
        """"""
        Args:
            img (PIL Image): Image to be cropped and resized.

        Returns:
            PIL Image: Randomly cropped and resized image.
        """"""
        if self.i is None:
            assert self.i == self.h == self.j == self.w 
            self.get_params(img, self.size)

        return F.resized_crop(img, self.i, self.j, self.h,
                              self.w, self.size, self.interpolation)

    def __repr__(self):
        interpolate_str = _pil_interpolation_to_str[self.interpolation]
        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)
        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))
        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))
        format_string += ', interpolation={0})'.format(interpolate_str)
        return format_string
```

"
Denormalize option in torchvision.utils.save_image(),pytorch/vision,2019-04-11 14:31:34,4,needs discussion#module: transforms#module: utils,848,432056946,"## 🚀 Feature
<!-- A clear and concise description of the feature proposal -->
A function to denormalize an image based on mean and standard deviation.

## Motivation

<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->
When working with images on NN's trained on a specific dataset (for example ImageNet), an image is first normalized to the mean and standard deviation of that dataset. When we want to save such an image later in the process we can use the function *torchvision.utils.save_image()*. However the image is still normalized and will have a different mean and standard deviation compared to the original image. There is no option to *denormalize* such an image such that the initial normalization is undone and the saved image has the same mean and std.

## Pitch

<!-- A clear and concise description of what you want to happen. -->
A extra parameter to the *torchvision.utils.save_image()* function to denormalize an image based on a mean and standardization array.

## Alternatives

<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->
One way to tackle the problem currently is to use the *transforms.Normalize()* function. My current implementation is shown below. One flaw of this implementation is that the image has to be clipped to keep the values between 0 and 1. Thus some information is lost. I am not sure how to do this operation lossless.

```python
def img_denorm(img, mean, std):
    #for ImageNet the mean and std are:
    #mean = np.asarray([ 0.485, 0.456, 0.406 ])
    #std = np.asarray([ 0.229, 0.224, 0.225 ])

    denormalize = transforms.Normalize((-1 * mean / std), (1.0 / std))

    res = img.squeeze(0)
    res = denormalize(res)

    #Image needs to be clipped since the denormalize function will map some
    #values below 0 and above 1
    res = torch.clamp(res, 0, 1)
    
return(res)
```



cc @vfdev-5"
[feature proposal] U-Nets with pretrained torchvision backbones,pytorch/vision,2019-04-04 09:18:20,6,enhancement,834,429169120,"I was thinking about extending torchvision models with an U-Net builder for segmentation, that takes pre-trained torchvision classification models as backbone architectures in the encoder path of the U-Net, and builds a decoder on top of it, using features from specified layers of the backbone model. 

I already implemented this for ResNet, DenseNet and VGG models in a separate module:
https://github.com/mkisantal/backboned-unet

Now I'm thinking about integrating it directly with torchvision. Do you think it would be a useful new feature?

It's not an addition to the available torchvision models in the traditional sense, as it just transforms the available models, does not work out of the box but requires training. But it can make torchvision easier to use for segmentation problems."
LSUN caching is ugly,pytorch/vision,2019-03-29 18:48:24,2,enhancement#help wanted,825,427130905,"The cache for the LSUN dataset (136MB just for bedrooms) gets dumped into the same directory as the script you're running, with a filename that's a function of the path to the dataset itself. Generating the cache can take several hours, eg on a standard EC2 server with EBS disk. If you want to run a script in a different folder, you have to regenerate the cache (hours) or copy the cache file to the new folder. If you move the dataset to a different folder, you either regenerate the cache or try to modify the cache filename to match the new path. No matter what you have cache files side by side with your code that need special gitignore directives (they don't have an extension to use wildcards on).

It should be possible to put the cache file in the data directory or something to fix these issues."
Inconsistency in inception,pytorch/vision,2019-01-23 20:46:54,1,bug,723,402422843,"## 🐛 Bug

Hi
I was going through the torchvision code to implement c++ API and I noticed an inconsistency.

In line 298 of inception.py stddev of a class of type BasicConv2d is set to 0.01:

``` python
self.conv1.stddev = 0.01
```

But in line 60 it checks if it's conv2d or linear which a BasicConv2d is none:

``` python
if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
   import scipy.stats as stats
   stddev = m.stddev if hasattr(m, 'stddev') else 0.1
```

I put this code inside the previous if statement:

``` python
if hasattr(m, 'stddev'):
    print(m)
```

and it only printed:

```
Linear(in_features=768, out_features=1000, bias=True)
```

so convs inside conv1 BasicConv2d of InceptionAux get initialized with stddev=0.1

is this an error or is it intentional?"
a small bug in resnet model _make_layer() implementation,pytorch/vision,2019-01-03 21:08:50,2,awaiting response,706,395725021,"Hi guys,

I think there is a small bug in the ""_make_layer(self, block, planes, blocks, stride=1)"" function (in charge of generating residual blocks at certain resolution) in the ResNet model.  This function will just simply discard the odd number of rows and columns in the feature maps in the identity path at the first residual block in a resolution stage when the stride=2 (the case where feature maps get downsampled by 2).  

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                #this 1x1 conv with stride 2 will simply discard the odd number of rows and columns in the feature maps in the identity path
                conv1x1(self.inplanes, planes * block.expansion, stride), 
                nn.BatchNorm2d(planes * block.expansion),
            )

Please let me know what do you think. 

"
Squeezenet model ONNX export,pytorch/vision,2018-12-17 22:05:28,7,awaiting response,690,391909244,"Hi,
I have some questions regarding the squeeznet model.

I failed at exporting the model to ONNX with the following error ""ONNX export failed : Couldn't export operator aten:max_pool2d_with_indices"".
I noticed that in /torch/onnx/symbolic.py , the use of ""ceil_mode"" is not supported in max_pool2d_with_indices.
After setting ""ceil_mode"" to False for all occurrences of ""MaxPool2d"", the export succeeds.

I also noticed that, when using the exported model mentioned above, the Pytorch results do not match the ONNX results, the reason being the use of ""nn.Dropout(0.5)"".


My questions are :

     1- Is there a particular reason ""ceil_mode"" is being used? and would there be a concern with setting it to False for the export to be possible? I could submit a PR with this change if no concern is noted.

     2- Why is ""Dropout"" used in the model outside of training? Could we remove it? 
 
 Thanks

"
TypeError: can't pickle Environment objects when num_workers > 0 for LSUN,pytorch/vision,2018-12-17 15:13:01,15,wontfix#module: datasets,689,391756091,"The program fails to create an iterator for a DataLoader object when the used dataset is LSUN and the amount of workers is greater than zero. I do not have such an error when work with other datasets. Something tells me that the issue might be caused by lmdb. I run on Windows 10, CUDA 10.


Code:
```python
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms

dataset = dset.LSUN(root='D:/bedroom_train_lmdb', classes=['bedroom_train'],
                            transform=transforms.Compose([
                                transforms.Resize((64, 64)),
                                transforms.ToTensor(),
                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                            ]))

dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,
                                             shuffle=True, num_workers=4)

for data in dataloader:
    print(data)
```
Error:
```python
Traceback (most recent call last):
  File ""C:/Users/x/.PyCharm2018.3/config/scratches/scratch.py"", line 15, in <module>
    for data in dataloader:
  File ""C:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 819, in __iter__
    return _DataLoaderIter(self)
  File ""C:\Anaconda3\lib\site-packages\torch\utils\data\dataloader.py"", line 560, in __init__
    w.start()
  File ""C:\Anaconda3\lib\multiprocessing\process.py"", line 112, in start
    self._popen = self._Popen(self)
  File ""C:\Anaconda3\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Anaconda3\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Anaconda3\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Anaconda3\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle Environment objects
```
"
[Master Issue] Add more models to torchvision,pytorch/vision,2018-10-30 10:27:49,46,enhancement,645,375417347,"This is a master issue to track requests for adding new pre-trained models to torchvision.

Here is the (potentially incomplete) list I compiled:
- [x] ResNext https://github.com/pytorch/vision/issues/154 https://github.com/pytorch/vision/issues/543 https://github.com/pytorch/vision/issues/154
- [x] ResNet / ResNext with Group Norm https://github.com/pytorch/vision/issues/631
- [x] mobilenet https://github.com/pytorch/vision/issues/625
- [x] Inception family https://github.com/pytorch/vision/issues/490 https://github.com/pytorch/vision/issues/537
- [x] ~NasNet https://github.com/pytorch/vision/issues/321~ Implemented as MNasNet
- [ ] SENet https://github.com/pytorch/vision/issues/260
- [x] ShuffleNet

@Cadene has already implemented a number of those models in his fantastic https://github.com/Cadene/pretrained-models.pytorch . I'll start from there and try to get models trained using `pytorch/examples/imagenet`, so that the models are reproducible.

---
### Requirements

- python implementation to live in `vision/models`
- pre-trained weights using the same mean / std normalization as [in the imagenet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L138-L139)
- the script used to train the models, or the command-line arguments used if the script was exactly the one from `examples/imagenet`."
Optional random number generation for get_params methods in image transforms,pytorch/vision,2018-10-24 17:57:13,5,awaiting response#needs discussion,638,373604671,"In order to make the process of mini batch generation more deterministic, it would be nice if the `get_params` method could take as an optional parameter a random number generator."
How to load indexed images (color maps) using dataset loader ,pytorch/vision,2018-10-09 05:02:32,5,,622,368041746,"The dataset loader returns image loaded as rgb. Which is great for all other purposes, but loading an indexed image for example segmentation mask is a pain. 


This is the line : 
https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py#L161 


The PIL module does load images by default if they are just color maps, can we not just return without conversion to rgb or pass a flag to convert to rgb ?  

OR is there any other better way of creating a dataloader with the masks folder ?  "
[Feature Request] Optionally save ImageFolder information,pytorch/vision,2018-10-04 01:31:22,0,,617,366589020,"Currently ImageFolder indexes a directory when the object is created. For some datasets (e.g., Places365, ImageNet, etc.) this can take several minutes. In the case of ImageNet-22K, this can take over 30 minutes on an SSD (albeit on an NFS system). Torch saved this information, but PyTorch requires re-indexing every run. I request the option to save and restore the indexing information."
[feature request] Add optional 'directory/path' parameter to save available pretrained models.,pytorch/vision,2018-10-03 13:29:47,6,enhancement#needs discussion#module: models,616,366339637,"I may be wrong but I could not find any simple way of specifying where the pre-trained models from **torchvision.models** are saved. 

By default, they are saved in the `...\.torch\models\` directory.
It would be very helpful to have an optional parameter to set the download path for the pre-trained models. (something like `model_dir` parameter in `torch.utils.model_zoo.load_url()`)

If there is an existing way to do this already, please correct me in the comments."
DALI support,pytorch/vision,2018-09-20 02:32:17,13,needs discussion,608,361995297,"Hi, any plan to integrate DALI (https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/index.html) to `torchvision` for faster preprocessing? I found `chainer` tries to integrate it (https://github.com/chainer/chainer/pull/5067)."
Strange behavior trying to install in conda with PyTorch built from source,pytorch/vision,2018-09-12 23:38:03,3,,603,359690273,"I'm running on a machine where I built PyTorch from source on CUDA 9.2 cuDNN 7.2.1, with Ubuntu 18.04. I built PyTorch from source exactly following directions, but when I went to install conda vision (`conda install torchvision`) I got this message:

```
justinkterry@Station:~$ conda install torchvision
Solving environment: done

## Package Plan ##

  environment location: /home/justinkterry/miniconda3

  added / updated specs: 
    - torchvision


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    cudatoolkit-9.0            |       h13b8566_0       340.4 MB
    libtiff-4.0.9              |       he85c1e1_1         566 KB
    nccl-1.3.5                 |        cuda9.0_0         8.6 MB
    torchvision-0.2.1          |           py36_0          75 KB
    pillow-5.2.0               |   py36heded4f4_0         586 KB
    ninja-1.8.2                |   py36h6bb024c_1         1.3 MB
    olefile-0.46               |           py36_0          48 KB
    pytorch-0.4.1              |   py36ha74772b_0       215.8 MB
    cudnn-7.1.2                |        cuda9.0_0       367.8 MB
    ------------------------------------------------------------
                                           Total:       935.1 MB

The following NEW packages will be INSTALLED:

    cudatoolkit: 9.0-h13b8566_0      
    cudnn:       7.1.2-cuda9.0_0     
    libtiff:     4.0.9-he85c1e1_1    
    nccl:        1.3.5-cuda9.0_0     
    ninja:       1.8.2-py36h6bb024c_1
    olefile:     0.46-py36_0         
    pillow:      5.2.0-py36heded4f4_0
    pytorch:     0.4.1-py36ha74772b_0

```

The fact that torchvision is trying to install pytorch again seems to imply that the instructions to build from source don't properly integrate it into conda, or that the dependencies in torchvision are wrong. It appears to be torch vision, because of this:

```
justinkterry@Station:~$ conda list
# packages in environment at /home/justinkterry/miniconda3:
#
# Name                    Version                   Build  Channel
[...]
torch                     0.5.0a0+c8b246a           <pip>
[...]

```

Additionally, why is installing cuDNN and CUDA the default with `conda install torchvision`? Using torchivison doesn't require them, and accidentally adding a CUDA installation to a workstation can cause a huge number of issues."
[feature request] Image Histogram Transformation,pytorch/vision,2018-09-10 05:35:44,10,enhancement#help wanted,598,358474425,"It is often useful (especially in the field of astronomy) to transform the histogram of images. I would like to suggest an image histogram transformation function (under torchvision.transforms) that transforms the histogram of an image to match that of a template image as closely as possible. For instance, consider the following function:

```
def match_histogram(source, template):

    source   = np.asanyarray(source)
    template = np.asanyarray(template)
    oldshape = source.shape
    source   = source.ravel()
    template = template.ravel()

    # get the set of unique pixel values and their corresponding indices and
    # counts
    s_values, bin_idx, s_counts = np.unique(source, return_inverse=True,
                                            return_counts=True)
    t_values, t_counts = np.unique(template, return_counts=True)

    # take the cumsum of the counts and normalize by the number of pixels to
    # get the empirical cumulative distribution functions for the source and
    # template images (maps pixel value --> quantile)
    s_quantiles  = np.cumsum(s_counts).astype(np.float32)
    s_quantiles /= s_quantiles[-1]
    t_quantiles  = np.cumsum(t_counts).astype(np.float32)
    t_quantiles /= t_quantiles[-1]

    # interpolate linearly to find the pixel values in the template image
    # that corresponds most closely to the quantiles in the source image
    interp_t_values = np.interp(s_quantiles, t_quantiles, t_values)

    return interp_t_values[bin_idx].reshape(oldshape)
```

The function above is not optimal since it has to recalculate template image information. It is not discretized for float type images. It only performs for highly discretized images such as png (0-255 bins). It also performs poorly when the number of diverse pixels is too low which might be fixed by adding small noise.
"
conda install installs PyTorch even if already present,pytorch/vision,2018-08-30 06:28:24,6,,590,355442151,"Running `conda install torchvision -c pytorch` results in PyTorch  `0.4.1-py36_py35_py27__9.0.176_7.1.2_2` being installed for me, although I have a newer version compiled from source installed.

The requirements should be configured in a way that newer PyTorch versions are also accepted.
At a minimum, a note should be added to the Readme to install PyTorch _after_ installing `torchvision`. "
torchvision.transforms cannot handle certain transformations (ResNet example),pytorch/vision,2018-08-21 11:44:18,3,enhancement,583,352502424,"I'm trying to replicate the ResNet paper, so I need the following transforms

- Subtract per-pixel-mean from all pixels in all channels (this can lead to negative values) I can do this with `Lambda`
- `Pad(4)`
- `RandomHorizontalFlip()`
- `RandomCrop([32, 32])`
- `ToTensor()`

The problem is that transforms 2-4 only work on `PIL.Image`, while transform 1 must happen on `np.array` or `torch.Tensor` because of the negative values. I can't convert back and forth inside of the pipeline since that would cut off the negative values, so I'm stuck.

It would be great if there were versions of at least the spatial transforms which work on numpy arrays. As far as I can see, some of them convert the PIL image to array anyway (e.g. `Pad`)."
Rotation transformation,pytorch/vision,2018-08-03 08:48:46,15,enhancement#needs discussion#module: transforms,566,347312138,"I noticed that RandomRotation  range only from (min, max); however it is more realistic to rotate from the set (angle1, angle2, angle3) randomly?"
BUG: The torchvision.models.alexnet is incorrect,pytorch/vision,2018-07-16 10:09:55,7,wontfix#module: models#topic: classification,549,341459260,"In [self.classifier](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py#L32), the correct order of layers should be fc6-relu-dropout-fc7-relu-dropout-fc8, which means that dropout layer should be after the fc and relu layers.

This does not matter when inference. However, when you fine tune AlexNet model on other datasets, the incorrect layer sequence will result in an accuracy drop of 2 percents or more.

Can I make a PR to fix it? The pre-trained model URL need to update too."
"transforms.ToTensor() for numpy float array in the range of [0.0, 255.0]",pytorch/vision,2018-07-13 17:19:32,6,enhancement#help wanted#module: transforms,546,341096992,"I had come across a debugging scenario where the ToTensor() didn't convert the numpy float array in the range of [0.0, 255.0] to the range [0.0 to 1.0] due to following lines:
https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py#L50-L53

Basically, this API assumes that all the float arrays will already be in range [0.0 to 1.0].
Do you think we have to change this behavior?"
Request to port Torch-Signal to Pytorch,pytorch/vision,2018-07-09 16:16:14,6,,3240,783534473,"Hi @soumith, As discussed today researchers dealing with 1-D signals (Biomedical mainly) would find it immensely helpful if we can get something similar to the Torch-Signal for applying cuda optimized transforms like Wavelet/FFT to 1D signals. Presently we have to apply transforms on scipy or numpy or use libraries like [PytorchWavelet](https://github.com/tomrunia/PyTorchWavelets) which too rely on cpu based libraries to apply the transform and just wrap around PyTorch's Conv1D function. It would be amazing to get a library to do the same in CUDA allowing us to integrate wavelet transforms in our training pipeline in runtime as opposed to us applying the tranform on hours long signals.

Thank you for hearing me out and thanks for the amazing presentation today."
"ImageFolder dataset: ""too many open files"" error",pytorch/vision,2018-07-02 09:34:34,11,module: datasets,539,337440438,"Hi,
I am using `ImageFolder` dataset to train on imagenet. 
I repeatedly get ""too many open files"" OSError after training for several hours.  
I suspect the issue comes from [here][1]:
```
def pil_loader(path):
    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGB')  # <--
```
The `return` is inside `with ...` clause, thus (I suspect) the file handle `f` is not closed properly when the function returns. 
Shouldn't the function be
```
def pil_loader(path):
    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)
    with open(path, 'rb') as f:
        img = Image.open(f)
    return img.convert('RGB')  # <--  not indented
```

Thanks,
-Shai
[1]: https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py#L156

cc @pmeier"
[Feature Request] Target Transforms for keypoints in Image,pytorch/vision,2018-06-03 17:29:53,8,enhancement,523,328846397,"Torchvision's functional API allows user to explicitly specify angles to rotate the image or points to crop the image. Thus one can apply sane augmentation on target masks or bounding boxes.
But in cases where we have the output as keypoints in the image, for example say human body joint coordinates in given image, if I rotate the image I'll have to find corresponding joint locations in image (via elementary geometry). I felt that this effort off applying corresponding keypoints in augmented image could be handled in pytorch backend and would be very helpful.
Do you think it's worth it to add in torchvision package?
This could come handy in object detection, human joint annotations and maybe even more place"
Support for customized channel TIF input image,pytorch/vision,2018-05-24 03:27:42,7,enhancement#awaiting response,514,325950911,"I am currently working on medical images, where one field of view could have more than 3 channels (up to 50+ channels). Current loader does not support customized channel input. If you give it a stacked TIF image, it use convert 'RGB' function to make it something else. If I change the loader and make it output the n-d array of the input image, it gives future error because it assumes PIL image format as input. Any solutions? Thanks."
to_tensor doesn't respect default dtype,pytorch/vision,2018-05-22 05:59:09,3,,509,325150766,
What is the relationship between the output label of pretrained model in model zoo and wordnet synset id? ,pytorch/vision,2018-05-02 07:23:46,8,enhancement,484,319437425,"we can easily access pytorch pre-trained model like VGG, AlexNet and SqueezeNet by

    import torchvision 
    torchvision.models.vgg16(pretrained=True)

can anyone point out what's the relationship between the output label(index of maximum output value) and the actual category?

i downloaded ILSVRC2012_devkit_t12 and got the imagenet id and other metainfo provided by meta.mat, however it seems pre-trained model have some different id. because when i evaluate the network with ILSVRC2012 validation set, it reports 100% error."
Image Augmentations on GPU Tests,pytorch/vision,2018-05-01 17:09:50,10,awaiting response#needs discussion,483,319260284,"Hello Pytorch vision people !  
  
I am currently working on a project that requires lots of image augmentations  
to perform better.  And I believe this is not only my case. When reading  
about topics such a domain randomization, we see that big variations on images  
leads to much more generalization.  

I saw that pytorch does not seem to provide a way to perform  
any image augmentation on GPU as comment in #45 . In some posts I saw people  
not encouraging to do it (https://discuss.pytorch.org/t/preprocess-images-on-gpu/5096)  but i really disagree, specially for the cases where several  augmentations are applied.  
To show this point I provide a gist code showing an example illustrating the
possible speed up gains on a multiplication operation ( brightness augmentation ? )

https://gist.github.com/felipecode/f3531e2d04e846da99053aff16b06028

On the gist, i show a GPU augmentation  interface is working as following:

	no_aug_trans = transforms.Compose( [transforms.RandomResizedCrop(224), transforms.ToTensor()])  
	dataset = datasets.ImageFolder(data_path, transform=no_aug_trans)
	multply_gpu = transforms.Compose([ToGPU()] + [Multiply(1.01)] * number_of_multiplications)
	for data in data_loader:  
	    image, labels = data  
	    result = multiply_gpu(image)

Unfortunately the GPU augmentation could not be smoothly interfaced with the dataloader without
sacrificing the multi threading for data reading. However, the speed ups obtained seems promising
The following plot shows up when running the gist code with a TITAN Xp and Intel(R) Xeon(R) CPU E5-1620 v3 @ 3.50GHz CPU, note that I remove the loading time when plotting.

![multiplications](https://user-images.githubusercontent.com/4934673/39483508-eb90fea8-4d72-11e8-81bb-12ab66ff6227.png)


The plot shows the time to compute in function of number of multiplications. For this test, on each data point about 500  RGB images  of 224x224 are multiplied by a constant.

Of course, there is no clear reason on why should one do 60 multiplications.
However,  I implemented an small library where I used imgaug library as reference
and implemented more functions in GPU. For the following augmentation set 
used in my project I obtained about 3-4 times speed up. 

	transforms.Compose( [ToGPU(), Add((-5, 5)), Multiply((0.9, 1.1)), Dropout(0.2),AdditiveGaussianNoise(0.10*255),GaussianBlur(sigma=(0.0, 3.0)),ContrastNormalization((0.5, 1.5))] )

This speed up is even higher if more augmentations are added. 

So, how can I improve this API ?  How could something like this fit in a pull request  ?
How can this be more smoothly merged inside the dataloader , but keeping the
multithreading for data reading ? 
I still have to test the training time for the full system, but I don't believe there will
be any overhead since images have to be copyed to GPU anyway.
	

	




"
[feature proposal] Composing transformations with __add__ magic method,pytorch/vision,2018-03-23 23:40:48,11,enhancement#needs discussion,456,308207654,"TLDR: `transform = Resize((299, 299)) + ToTensor()`

## Motivation

Consider the common use case of building an overall-common-but-differing-at-one-point transformation pipeline for train and test sets, for instance applying augmentation transforms only to train set:
```python
train_transform = transforms.Compose([
    Resize((299, 299), interpolation=1),
    RandomCrop(299, padding=4),
    RandomHorizontalFlip(),
    RandomGrayscale(),
    ToTensor(), 
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
    Resize((299, 299), interpolation=1), 
    ToTensor(), 
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

A way of expressing of `train_transform` builds upon `test_transform` would be handy. What you could do now is to recursively use `transforms.Compose` (i.e. `Compose([Compose(...resizing...), Compose(...augmentation...], Normalize(...)])`), but I don't find this particularly clean (and intuitively, final transforms objects should be sequential objects with only one level of depth).

## Proposed behavior
```python
common_transforms = ToTensor() + Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
augmentation = RandomCrop(299, padding=4) + RandomHorizontalFlip() + RandomGrayscale()
train_transforms =  Resize((299, 299)) + augmentation + common_transforms
test_transforms = Resize((299, 299)) + common_transforms
```

Note how this mimics the behavior of Python lists (and other iterables):
```python
>>> [1, 2] + [3, 4]
[1, 2, 3, 4]
```

## Implementation sketch

```python
class Transform(object):
    # Right now all transforms directly inherit from object. 
    # Let Transform be a superclass for all transformations

    def __call__(self, pic):
        raise NotImplementedError('Each subclass should implement this method')

    def __repr__(self):
        raise NotImplementedError('Each subclass should implement this method')

    def __add__(self, other):
        if not isinstance(other, Transform):
            raise TypeError('Only transformations can be added')
        if isinstance(self, Compose) and isinstance(other, Compose):
            return Compose(self.transforms + other.transforms)
        if not isinstance(self, Compose) and isinstance(other, Compose):
            other.transforms = self + other.transforms
            return other
        if isinstance(self, Compose) and not isinstance(other, Compose):
            self.transforms = self.transforms + other
            return self
        if not isinstance(self, Compose) and not isinstance(other, Compose):
            return Compose([self, other])
```

Comments are most welcome!
"
NASNet-Mobile is ported,pytorch/vision,2018-03-21 19:44:00,1,needs discussion,454,307392025,"Hi!
NASNet-A-Mobile has been successfully ported to PyTorch!
https://github.com/veronikayurchuk/pretrained-models.pytorch
The results are:
**Acc@1 74.080, acc@5 91.740**

Hope, it would be useful!
"
Bug: torchvision/transforms/functional/to_pil_image always converts 1-channel (gray) FloatTensor images to 8-bit unsigned int,pytorch/vision,2018-03-19 14:46:25,16,bug#help wanted#module: documentation,448,306493505,"- OS: Ubuntu 16.04.4 LTS x64
- PyTorch version: 0.3.0
- Torchvision version: 0.2.0
- How you installed PyTorch (conda, pip, source): conda
- Python version: 3
- CUDA/cuDNN version: 8.0
- GPU models and configuration: Titan X (Maxwell)

ERROR:
**ValueError: Incorrect mode (<class 'float'>) supplied for input type <class 'numpy.dtype'>. Should be L**

The *torchvision* transform **ToPILImage(mode=float)** will always break for input of type *torch.FloatTensor*
**ToPILImage()** uses the internal function **to_pil_image** found in *torchvision/transforms/functional.py*

In *https://github.com/pytorch/vision/blob/master/torchvision/transforms/functional.py*:
Line 104 checks if the input is of type *torch.FloatTensor*
If so, line 105 scales the input by 255, but then converts it to **byte**
Lines 113-127 check if the user-specified mode is the expected mode, and throws an error if not.
The expected mode is assigned by **npimg.dtype**, which return **np.uint8** if line 105 is executed 

I believe the bug can be fixed by changing line 105 from:
**pic = pic.mul(255).byte()**
-to-
**pic = pic.mul(255)**

**Test script:**
import torch
from torchvision import transforms
a = torch.FloatTensor(1,64,64)
tform = transforms.Compose([transforms.ToPILImage(mode='F')])
b = tform(a)

Please let me know if I am in error. 
Thank you.

"
[proposal] Move dataset tar/zip extraction and integrity checking of multiple files to utils.py,pytorch/vision,2018-03-06 17:13:52,6,enhancement#help wanted#needs discussion,441,302788877,"Datasets such as CIFAR / MNIST / etc have download / integrity logic that is useful for reimplementing custom user datasets (such as https://github.com/vadimkantorov/metriclearningbench/blob/master/cars196.py, https://github.com/vadimkantorov/metriclearningbench/blob/master/cub2011.py, https://github.com/vadimkantorov/metriclearningbench/blob/master/stanford_online_products.py)

I propose moving it to `torchvision/datasets/utils.py` functions like downloading and extracting tarballs / zipfiles / plain files; checking integrity by md5 of a file list if it is provided.

Currently avoiding duplication leads to quirky subclassing of ImageFolder, Cifar10 etc."
Improve documentation for torchvision.models,pytorch/vision,2018-02-28 18:16:07,6,needs discussion,437,301129005,"Every model's architecture (as implemented in pytorch), number and names of layers (in the implementation of models done in pytorch, not the original model from the paper), and other important details that might help users (devs) should be included in documentation. Right now, the [docs](http://pytorch.org/docs/master/torchvision/models.html) only list names of available models."
Attribute for size of classes in vision/torchvision/datasets/folder.py,pytorch/vision,2018-02-25 01:15:22,3,module: datasets,431,299987723,"It would be great if you can provide one method inside ImageFolder class or Dataset class which can provide number of samples for each loaded class/label.

Though we can do this outside ImageFolder class, but as we always use ImageFolder for loading image dataset hence a class_size attribute will quickly tell us how many samples of each class/label are loaded.   
   
Counting num of samples outside ImageFolder class:
   from collections import Counter   
    image_datasets['train'] = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'))
    class_counts['train'] = dict(Counter(sample_tup[1] for sample_tup in image_datasets['train''].imgs))


cc @pmeier"
[RFC] Extend the scope of ImageFolder for segmentation data,pytorch/vision,2018-02-16 17:57:19,4,enhancement#help wanted,424,297870287,"I am starting to work on segmentation problem.

[ImageFolder](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder) is great but seems to be dedicated for classification.

Would it make sense to modify it such that we can load input/target which can be both list of images as in the case of CamVid dataset? 
It is also true that segmentation can also be organized with a list of images and a list of annotations which does not fit the previous requirement.

So I was wondering what the core devs are thinking about it.

"
Feature proposal: Cutout,pytorch/vision,2018-02-13 13:50:48,5,,420,296743692,"[Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/abs/1708.04552) proposes a simple and effective method for regularization. I think it would fit greatly in the `torchvision.transforms` toolbox, and am willing to contribute an implementation with a proper test, if that's okay."
Local feature CNNs in models?,pytorch/vision,2018-02-09 15:27:44,8,enhancement#needs discussion,413,295908075,"Hi,

Are the state-of-the-art local patch descriptors welcomed in models? E.g. HardNet  https://github.com/DagnyT/hardnet 
They are trained in pytorch + torchvision and are potentially useful for low-level vision tasks. However, I understand that they are less transferable than ImageNet-trained ones. "
CapsNet Model,pytorch/vision,2017-12-23 08:30:13,1,enhancement,375,284294434,"Recently _Geoffrey Hinton_ and his team introduced a new interesting deep neural network model, called [**CapsNet**](http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules). 
I think the implementation of this Net on PyTorch and train it on ImageNet Data set, and after that add its pre-trained model to TorchVision Models would be great for the PyTorch community.  
Thanks to the PyTorch team

"
Dataset transforms to sample a set from data,pytorch/vision,2017-11-21 19:34:26,2,,338,275833870,"NOTE: I am creating this issue as a discussion ground for the proposal.

## Requirements

Given a dataset, we must be able to sample instance sets under certain constraints. For instance, given a dataset of images and their class labels, consider the following two samplings.

Sampling 1 - Sample a pair of images from two distinct classes or a pair of images from the same class.

Sampling 2 - Sample a set of `k` images from the dataset along with another image to test this k-subset against (I'll spare what exactly what ""testing"" against means). The constraint applicable here is that the test image should be from a class which exists in the initially sampled k-subset. An alternative view would be to sample `k+1` images from the dataset such that at least 2 images are from the same class and use one of those images as the test image.

If you are not convinced why the above kinds of samplings might be needed, I can provide references to representative literature.

## Approach

Borrowing the idea from @fmassa 's comment at #323 , in similar spirit of the `ConcatDataset` class, we must have another wrapper say `MultiDataset`.

## Tricky Parts

The above higher-order abstraction is a good approach, but a few challenges to generalize such a dataset are the following. Since, we would want to wrap around an existing dataset, we will require
standardization of member fields of the dataset classes. Especially for tasks where labels are involved. Or perhaps the dataset classes must also implement `get_labels()` method which returns a list of labels and a `get_label_instances()` which allows accessing instances for a particular label.

This seems like a not-so-clean approach and really looking for cleaner ideas. Perhaps I am missing something to cleanly implement this?"
NASNet Model,pytorch/vision,2017-11-03 20:24:09,15,enhancement#help wanted,321,271108332,"Recently the Google Brain Team released a fantastic CNN model, [NASNet](https://arxiv.org/abs/1707.07012), in [TF-slim](https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet), which achieved the state-of-the-art Top-1 Accuracy on ImageNet by **82.7 %**. I want to know that the PyTorch team has any plan for implement or porting this model into the PyTorch Offifcial Models (_i.e.,_ torchvision models)?
"
[Feature Request] Random Spatial Transforms,pytorch/vision,2017-10-10 16:38:08,6,,287,264296508,"I just saw the refactored transform and it looks much better. I had issues with the PIL backend of the previous transform and used to completely avoid the torchvision transform and implement the transforms I want locally using mostly opencv. 

I also saw that color transforms are added in #275. These are all great! Is there a plan to add spatial transforms such as translation, rotation, shear etc.. (in general warping) augmentations. They are crucial in case of limited dataset training such as in attribute prediction, person re-id, extreme classification, etc... "
Is SENet (and new architectures) welcome to `models`?,pytorch/vision,2017-09-16 03:49:15,12,enhancement#help wanted,260,258205590,"Hi, now `torchvision.models` contains some models such as ResNet and they are very helpful as baselines.

Recently I implemented [SENet](https://github.com/moskomule/senet.pytorch), which is the winner of ILSVR 2017's classification task. Can I send a PR of SENet to `models`? 

Plus I'd like to know which models are welcome to `models`.

Thank you."
can anyone add pretrained vgg-m to this repository,pytorch/vision,2017-09-04 13:23:16,0,enhancement#help wanted,242,255041380,see [here](http://www.vlfeat.org/matconvnet/pretrained/)
[Feature_Request] datasets class_to_idx for all datasets,pytorch/vision,2017-09-01 13:16:05,3,,237,254637501,"So I noticed that in torchvision.datasets only the ImageFolder class actually has the member called
class_to_idx.

Is there a reason why this isn’t just hard-coded for the other datasets like MNIST, CIFAR and so on, so that one could readily use it for actually getting a class-string for all sorts of visualizations + confusion matrices?

It does seem a little inconvenient to go and manually define the list of all 100 classes for e.g. CIFAR-100.
Do you think this is something that could be added?"
pre-activation ResNet,pytorch/vision,2017-07-20 04:45:05,4,,205,244244265,"Is there any interest in implementing the ""pre-activation"" version of ResNet, as described in 

https://arxiv.org/pdf/1603.05027.pdf

and which has been show to have better performance on CIFAR 10/100 and Imagenet?  It's a relatively minor modification of the ResNet version that's already implemented -- just changing the order of some of the operations in the residual block.

I'm happy to implement and submit a PR, though I don't really have the resources to train an Imagenet model ATM."
The  implementation of ResNet is different from official implementation in Caffe,pytorch/vision,2017-06-26 02:51:21,11,wontfix#module: models#topic: classification,191,238429759,"The `downsample` part in each block/layer (not the skip connection part), the PyTorch do it in conv3x3 using stride=2, but official caffe version in conv1x1 with stride=2 

```
conv1x1 -> caffe do it in here
conv3x3 -> pytorch do it in here
conv1x1
```

[Here in Bottleneck]():
```
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=1, bias=False)

  (layer2): Sequential (
    (0): Bottleneck (
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
       ...
```

[but in caffe](https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-101-deploy.prototxt#L531)

```

layer {
	bottom: ""res2c""
	top: ""res3a_branch2a""
	name: ""res3a_branch2a""
	type: ""Convolution""
	convolution_param {
		num_output: 128
		kernel_size: 1
		pad: 0
		stride: 2
		bias_term: false
	}
}
```"
Unit tests for Vision,pytorch/vision,2017-03-17 20:53:44,8,help wanted,110,215119593,"This issue fleshes out the full details and scope of the unit tests needed for torchvision.

There are very limited unit tests under test/ which dont cover the transform outputs themselves, but are limited to dimension and shape checks.

First, let's start with quantitative tests on known results.

We need to have a set of 10 test images, and then do each transformation of vision.transforms on these 10 images, and compare them pixel-wise with known results. We then compare the known results with the computed result from the transforms, and if they are within some threshold, we pass the test.

Some of the transforms such as Horizontal / Vertical flip can also have exact numerical unit-tests.

The test images:
- 2 monochrome images
- 2 3-channel images
- 2 4-channel PNG images with an Alpha component

Can find some on Wikipedia that are freely licensed.

The tests need to cover all transforms under: https://github.com/pytorch/vision#transforms

For similar testing, you can have a look at: 

https://github.com/torch/image/blob/master/test/test.lua#L258-L646"
CamVid dataset,pytorch/vision,2017-02-15 08:59:53,2,needs discussion#module: datasets,60,207743224,"Hi, I am interested in adding CamVid Dataset for Image Segmentation.

The Repository hosting the dataset is here- https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid

cc @pmeier"
Embed LR schedule and initialization with the model,pytorch/vision,2017-01-20 18:43:57,11,needs discussion,36,202212847,"I tried to implement [SqueezeNet](https://github.com/DeepScale/SqueezeNet) as a `torchvision` model and train it via [ImageNet example](https://github.com/pytorch/examples/tree/master/imagenet), and found that it doesn't converge as is. The reference code differs in two aspect:

- All but the last convolutions are initialized with Xavier Glorot initializer, the last is normal with stdev 0.01
- The learning rate is linearly decreased (polynomial schedule with power=1).

In PyTorch these aspects are hard-coded inside the ImageNet example, but I think it makes sense to make them part of the model definition in `torch.vision`. What's your position on it?"
details about the output shape from function extract_feature,leduoyang/depth_estimation_MCCNN,2019-07-24 12:16:31,0,,1,472249957,"In your main.py line 195 and 196, you seem to reshape the output to (h,w,feaure_dim), which w and h is the size of the input image, but the output from extract_feature is supposed to be (batch_size, feature_dim), I don't understand how this is a valid reshape operation. If you can help me understand your code which I'll be really grateful."
Already In Use ERROR,damien911224/theWorldInSafety,2017-12-11 07:55:46,0,,8,280924223,"Already In Used error시

ps aux | grep python 후

해당 하는 process id를 찾은 후

kill -9 process-id를 하면 해결된다!"
[Raspberry pi] port 22 Error,damien911224/theWorldInSafety,2017-11-26 14:06:58,0,,7,276827347,파일명이 ssh인 파일 하나 만들어서 boot에 넣어주면 해결됩니다.
[darkflow] libcublas.so.X.0 ERROR,damien911224/theWorldInSafety,2017-11-06 11:51:54,0,,6,271449456,"1. cudnn 8.0 미설치의 경우 설치하고  cuda (ex - /usr/local/cuda/lib64)로 cp

2. 설치한 경우 Error발생시,  -> sudo ldconfig /usr/local/cuda/lib64"
Git Unexpectedly hung up Push Problem,damien911224/theWorldInSafety,2017-10-02 11:53:28,0,,5,262060661,"git config http.postBuffer 104857600

위 명령어를 Git repository에서 쳐주면 해결 된다."
Git Commit 되돌리는 법,damien911224/theWorldInSafety,2017-09-14 13:18:19,0,,4,257716294,"Git Commit 되돌리는법

git log를 해당 git repository에서 터미널에 입력 한다.
>
git log
>

엔터를 누르며 아래로 목록을 읽어가며,
돌아가고 싶은 상태의 커밋 옆 해쉬코드를 복사 한다.
>
git reset --hard (commit hash)
>
((commit hash) 전체 부분을 해쉬코드로 채우면 된다 괄호 까지 모두)"
pip 설치시 에러,damien911224/theWorldInSafety,2017-09-14 13:11:34,0,,3,257714229,"pip 설치시 아래와 같은 에러가 나면..........
```python
E: Could not get lock /var/lib/dpkg/lock - open (11 Resource temporarily unavailable)
E: Unable to lock the administration directory (/var/lib/dpkg/) is another process using it? 
```
다음과 같이 하면 됩니다.
```python
sudo rm /var/lib/apt/lists/lock
sudo rm /var/cache/apt/archives/lock
sudo rm /var/lib/dpkg/lock
```"
How to push files with larger than 100MB,damien911224/theWorldInSafety,2017-09-14 12:00:40,0,,2,257693634,"See the blog

https://medium.com/@stargt/github에-100mb-이상의-파일을-올리는-방법-9d9e6e3b94ef"
Images for README,damien911224/theWorldInSafety,2017-09-14 11:23:45,0,,1,257684460,"<img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/28569107/30427378-8e49bf7e-998a-11e7-9c13-6629586edb11.png"">
"
Jaccard Distance values look off - over 90% are 1s,zhunzhong07/person-re-ranking,2022-09-20 18:26:14,0,,31,1379827120,"With no reranking, distance values are distributed evenly across the whole 0-1 range. But with reranking, although the accuracy improves ~2%, the distance histogram gets compressed into narrow range (0.7-0.9) with several distance values > 1. This looks incorrect. I studied the problem further and found that this is due to Jaccard distance that has over 90% values as 1s. 

## How I got this issue:

Use custom dataset (unable to share the dataset, 4k images, 200 identities) with reranking (k1=54, k2=6, lambda=0.3. - this gave the best overall `fscr`). Plot histograms of 

- original distance - looks right - distributed evenly across the whole range 0-1
- Jaccard distance - looks way off - over 85% values are 1s. 
- final distance - looks off due to Jaccard distance. Distributed in narrow window - 0.7-0.9. Many values are > 1!

## Expected behavior:

Final distance must be evenly distributed across the whole 0-1 range. 

## Questions:

1. What is [2-temp_min here](https://github.com/zhunzhong07/person-re-ranking/blob/master/python-version/re_ranking_ranklist.py#:~:text=temp_min/(-,2.%2Dtemp_min,-))? Is it same as the max operation - eq. 10 in the [paper](https://arxiv.org/pdf/1701.08398.pdf)?
2. Does `k1` depend on the number of images in the dataset? V is of size NxN (total num images). When `k1 << N` V has many 0s even after qe. Could this be why my Jaccard distances are all 1s? 
3. Is this expected of the distance distribution? If yes, why and if not, any suggestions to fix?"
请问目前有关于该数据集上的跨域表现吗,zhunzhong07/person-re-ranking,2020-09-30 04:07:20,0,,30,711615316,
Some question about cuhk03-labeled dataset under  the new training/testing protocol,zhunzhong07/person-re-ranking,2020-05-29 10:17:13,0,,29,627160202,"First：download cuhk-03.mat and the new training/testing protocol split
cuhk03_new_protocol_config_detected.mat cuhk03_new_protocol_config_labeled.mat, place all in the directory '/data/person-reid/cuhk03/'
Second: I use my own code to generate the bounding_box_train query bounding_box_test, and Is it OK?

The question is how to generate the bounding_box_train query bounding_box_test from the raw data cuhk-03.mat. Do all methods evaluate on the new training/testing protocol split."
Question about the usage of q_q_dist,zhunzhong07/person-re-ranking,2020-03-17 15:45:49,2,,28,583110112,"Hello! your reranking is a good job! However, there is a problem about your usage of q_q_dist. Without q_q_dist, it seems that your reranking would drop a lot.  I do the experiments below
| mAP of No reranking | mAP of reranking with q_q_dist | mAP of reranking without q_q_dist |
| ------------------- | ------------------------------ | --------------------------------- |
| 70.3                | 85.7                           | 71.9                              |
| 23.9                | 34.7                           | 25.5                              |
| 24.0                | 35.2                           | 25.5                              |

It is so weird. Could you explain it? "
关于re-ranking代码的疑问,zhunzhong07/person-re-ranking,2019-11-22 03:08:03,0,,27,526960455,"你好，感谢作者的算法思想，但我对您的算法实现有些疑问
（最开始我用的是罗浩的python版本，他告诉我完全按您的matlab版本复现的）
https://github.com/zhunzhong07/person-re-ranking/blob/master/evaluation/utils/re_ranking.m

1. 代码第10行
` original_dist = original_dist./ repmat(max(original_dist, [], 2), 1, size(original_dist, 2));`
对距离矩阵如此处理的目的是什么？处理后，距离矩阵已经不是对称的了，并且转置后每一行的元素放缩的scale都不同

2. 代码6行
`[~, initial_rank] = sort(original_dist, 2, 'ascend');`
对整个query和gallery进行排序，会出现查询样本的k1互近邻中出现查询集样本，这和论文并不一致
另外我试了一下按论文思想，每次只传入1个query进行reranking的方法，这样可能会影响了query expansion，但结果似乎大部分情况都会比现在稍好一些（只是太慢）

3. 代码第31行
`V(i, k_reciprocal_expansion_index) = weight/sum(weight); `
求每个样本的reci-feature, 最终除以了负指数的和，这和论文中也不相同

4. 代码第63行
`jaccard_dist(i, :) = bsxfun(@minus, 1, temp_min./(2 - temp_min)); `
这个距离的计算，好似和论文中计算方法并不等价？


想问一下，这几个地方算法实现的思路是什么？"
"after using re-ranking, the map and rank-1 both dropped? why did it happened? ",zhunzhong07/person-re-ranking,2019-11-05 02:51:20,0,,26,517518566,
about CMC curve ,zhunzhong07/person-re-ranking,2019-04-02 07:35:52,0,,23,428068668,Thank you very much for your outstanding work. I have reproduced your code in the past few days. The accuracy is one or two percentage points lower than the one mentioned in your paper. I also want to ask how to draw the CMC curve or PR curve. Do you have any code?  Thank you very much. @zhunzhong07 
batchsize,zhunzhong07/person-re-ranking,2019-03-29 01:10:23,1,,22,426770500,"@zhunzhong07 Hi，i want to know your batchsize, but i cannot find it. Can you tell me the value of batchsize or where i can find it. Thank you very much."
Rank2~Rank10 dropped significantly ?,zhunzhong07/person-re-ranking,2019-03-07 06:55:45,5,,21,418156873,"Hello, when I ran the code of this article, I found that Rank1 really improved, but why is Rank2~Rank10 dropped significantly ?"
 initial_rank = np.argsort(original_dist).astype(np.int32),zhunzhong07/person-re-ranking,2018-11-12 03:19:42,1,,18,379605390,"Hello!
When I run the example ResNet-50 + Global Loss on Market1501, I met the error about Memory Error when it came to the Re-Ranking. Here is the link https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch. The memory is 8G of my ubuntu. Could you give some tips to solve the Memory Error.Thank you very much!
![2018-11-09 16-01-59](https://user-images.githubusercontent.com/7524271/48324640-b9a11c00-e66c-11e8-9912-99d49cf97aeb.png)
"
How to generate Figure 7,zhunzhong07/person-re-ranking,2018-06-27 01:45:10,2,,13,336048327,"Hi, can you please share the code to generate Figure 7 in your paper?


"
time,zhunzhong07/person-re-ranking,2017-11-09 06:17:05,0,,6,272451961,"This is just a query... how much time did it take for you to complete the entire re-ranking as described in your esteemed paper?
Because I believe that a nested for loop is computationally very expensive.
Kindly advise.

Thank you "
"For more information about source code, please read chinese blog: https://www.jianshu.com/p/ab0a10c2e710",alexshuang/TGS_Salt,2022-06-04 08:29:33,0,,1,1260685312,"For more information about source code, please read chinese blog: https://www.jianshu.com/p/ab0a10c2e710
你的简书博客被删除啦
"
" raise AttributeError('Layer %s has no keyword argument %s=%s' % (param_type, k, v))",jacobandreas/nmn2,2017-07-21 06:13:38,3,,23,244573712,"Hi,I meet an error,,,,
2017-07-21 12:17:32,576 DEBUG    [root] prepared indices
2017-07-21 12:55:22,728 DEBUG    [root] computed image feature normalizers
2017-07-21 12:55:22,789 DEBUG    [root] using cvpr chooser
2017-07-21 12:57:36,539 INFO     [root] TRAIN2014, VAL2014:
2017-07-21 12:57:36,540 INFO     [root] 369861 items
2017-07-21 12:57:36,540 INFO     [root] 2002 answers
2017-07-21 12:57:36,541 INFO     [root] 877 predicates
2017-07-21 12:57:36,541 INFO     [root] 3591 words
2017-07-21 12:57:36,541 INFO     [root] 
2017-07-21 12:57:56,250 INFO     [root] TEST-DEV2015:
2017-07-21 12:57:56,250 INFO     [root] 60864 items
2017-07-21 12:57:56,250 INFO     [root] 2002 answers
2017-07-21 12:57:56,251 INFO     [root] 877 predicates
2017-07-21 12:57:56,251 INFO     [root] 3591 words
2017-07-21 12:57:56,251 INFO     [root] 
2017-07-21 12:59:02,813 INFO     [root] TEST2015:
2017-07-21 12:59:02,813 INFO     [root] 244302 items
2017-07-21 12:59:02,814 INFO     [root] 2002 answers
2017-07-21 12:59:02,814 INFO     [root] 877 predicates
2017-07-21 12:59:02,814 INFO     [root] 3591 words
2017-07-21 12:59:02,814 INFO     [root] 
Traceback (most recent call last):
  File ""main.py"", line 255, in <module>
    main()
  File ""main.py"", line 31, in main
    do_iter(task.train, model, config, train=True)
  File ""main.py"", line 97, in do_iter
    batch_data, model, config, train, vis)
  File ""main.py"", line 118, in do_batch
    predictions = forward(data, model, config, train, vis)
  File ""main.py"", line 170, in forward
    dropout=(train and config.opt.dropout), deterministic=not train)
  File ""/disk3/hbliu/vqa/nmn2/models/nmn.py"", line 440, in forward
    question_hidden = self.forward_question(question_data, dropout)
  File ""/disk3/hbliu/vqa/nmn2/models/nmn.py"", line 648, in forward_question
    bottoms=[word], param_names=[wordvec_param]))
  File ""/home/hbliu/disk3/vqa/nmn2/apollocaffe/python/apollocaffe/layers/caffe_layers.py"", line 203, in __init__
    super(Wordvec, self).__init__(self, name, kwargs)
  File ""/home/hbliu/disk3/vqa/nmn2/apollocaffe/python/apollocaffe/layers/layer_headers.py"", line 6, in __init__
    self.parse(sublayer, name, kwargs)
  File ""/home/hbliu/disk3/vqa/nmn2/apollocaffe/python/apollocaffe/layers/layer_headers.py"", line 57, in parse
    raise AttributeError('Layer %s has no keyword argument %s=%s' % (param_type, k, v))
AttributeError: Layer Wordvec has no keyword argument vocab_size=3591"
How to visualize the attention map,jacobandreas/nmn2,2017-04-17 18:03:17,1,,21,222187971,"I am attempting to visualize results, which is _mostly_ handled by main.visualize(). However, the code to get the attention map has been commented out, and replaced with np.zeros. 

My general question is what is the intuition behind the commented out code? Some specifics:

- What is i_datum?
- What is mod_layout_choice?
- Why is att_blob_name created the way it is?

This will be helpful to understand, as we are also attempting to connect an additional model to the final attention map, pre softmax activation. 
Thanks."
fatal error: Could not connect to the endpoint URL: ....,EscVM/OIDv4_ToolKit,2022-08-09 10:14:48,1,,107,1333031518,"<img width=""1583"" alt=""Screen Shot 2022-08-09 at 13 14 33"" src=""https://user-images.githubusercontent.com/72133828/183624100-4f68279f-3ae9-4726-b26b-3bfda3311c3a.png"">
"
Process killed while downloading training images,EscVM/OIDv4_ToolKit,2022-05-16 15:25:57,0,,104,1237328110,"Hi

I'm experiencing an issue regarding the download of the training set of several classes.
I'm using the folllowing command: 

python3 main.py downloader --classes Car --type_csv train

But after about 10 seconds of hanging (nothing appears after ""[INFO] | Downloading Car"") the process is killed and no image is downloaded. The validation and testing sets can be downloaded.

Do you know why this is happening to this specific set?

Thanks!"
What is the class name for Kitchen Knife?,EscVM/OIDv4_ToolKit,2022-01-04 04:18:07,1,,103,1092996937,"What is the class name for Kitchen Knife? Is it:
Kitchen Knife
Kitchen-Knife
Kitchen_Knife
etc.
Thanks!"
Add argument for downloading normalised annotations,EscVM/OIDv4_ToolKit,2021-07-27 13:56:08,0,,101,953911400,If we could include an argument that would let the user decide if they wish to download the normalised annotations. Many of the object detection tasks require using normalised annotations. 
How to specify type of image to download?,EscVM/OIDv4_ToolKit,2021-05-12 17:58:49,1,,100,890349053,"Open Images Dataset now has 3 types available - Detection, Segmentation and Relationships.
**I want to download images of type - Relationships** for the class 'drink', How shall I do it?

![image](https://user-images.githubusercontent.com/51288316/118022052-8b097680-b379-11eb-8857-2c3eda99e166.png)

- Images of classes under each type are different, also some classes are present in one type but absent in others."
visualizer IndexError: list index out of range ,EscVM/OIDv4_ToolKit,2021-05-05 14:23:13,0,,99,876506656,"Hello, i am using this toolkit in win10 with python 3.7.9.

I have downloaded the class Cat and now want to use the virtualizer to view the images. 

But i get the following error:

```
Traceback (most recent call last):
  File ""main.py"", line 37, in <module>
    bounding_boxes_images(args, DEFAULT_OID_DIR)
  File ""C:\Users\MS-23\Documents\GitHub\OIDv4_ToolKit\modules\bounding_boxes.py"", line 192, in bounding_boxes_images
    show(class_name, download_dir, label_dir,len(os.listdir(download_dir))-1, index)
  File ""C:\Users\MS-23\Documents\GitHub\OIDv4_ToolKit\modules\show.py"", line 39, in show
    img_file = os.listdir(download_dir)[index]
IndexError: list index out of range
```
"
OSError: The handle is Invalid,EscVM/OIDv4_ToolKit,2021-01-16 13:41:35,4,,95,787468784,"Using the Toolkit to download a single class of images on Windows 10, Python version 3.7.6

Running the downloader commands results in a WinError 6. The traceback points towards Lines 25 of the downloader file
```
Traceback (most recent call last):
  File ""C:\Users\James\Documents\OIDv4_ToolKit\modules\downloader.py"", line 25, in download
    columns, rows = os.get_terminal_size(0)
OSError: [WinError 6] The handle is invalid

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""OIDv4_ToolKit/main.py"", line 37, in <module>
    bounding_boxes_images(args, DEFAULT_OID_DIR)
  File ""C:\Users\James\Documents\OIDv4_ToolKit\modules\bounding_boxes.py"", line 62, in bounding_boxes_images
    download(args, df_val, folder[0], dataset_dir, class_name, class_code)
  File ""C:\Users\James\Documents\OIDv4_ToolKit\modules\downloader.py"", line 27, in download
    columns, rows = os.get_terminal_size(1)
OSError: [WinError 6] The handle is invalid
```"
No module named 'tqdm'. Add tqdm module,EscVM/OIDv4_ToolKit,2020-11-23 07:17:53,0,,90,748540608,"Dear EscVM,
Installing tqdm is quite difficult, especially on Windows. So if you add source of tqdm to this repository, it will be better. Many people also encounter bug: No module named 'tqdm'.
Thank you!"
V6 update,EscVM/OIDv4_ToolKit,2020-10-20 15:15:14,4,,89,725698358,"Hello,

congrats for the hard work of creating this.

Is the downloader updated to the most recent v6?"
Question on downloading image-level labels: how are negative examples handled?,EscVM/OIDv4_ToolKit,2020-09-30 04:12:41,0,,87,711617010,"First of all, thank you guys for your work! I am trying to download some images for an image-classification task and I'm wondering whether this tool handles negative examples. According to the OID website:

> Confidence: Labels that are human-verified to be present in an image have confidence = 1 (positive labels). Labels that are human-verified to be absent from an image have confidence = 0 (negative labels). Machine-generated labels have fractional confidences, generally >= 0.5. The higher the confidence, the smaller the chance for the label to be a false positive.

However I can't find any point in the code where the confidence value is checked. I would like to tinker with it so that the negative examples are downloaded into their own directory.
"
[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed,EscVM/OIDv4_ToolKit,2020-09-10 13:51:15,1,,86,697995106,"Not able to download images, attaching screenshot of the error 
![image](https://user-images.githubusercontent.com/7257675/92739770-dad31080-f39a-11ea-9037-aa0a1f9e32d7.png)
"
 can't open file 'convert_annotations.py',EscVM/OIDv4_ToolKit,2020-08-30 07:31:57,2,,85,688670712,"Hi,  is there a file called 'convert_annotations.py' to convert the annotation into yolo form?  I didn't find it. 
Thank you:)"
error when running python main.py downloader,EscVM/OIDv4_ToolKit,2020-07-29 05:52:35,3,,84,667569988,"CMD : python main.py downloader --classes Football --type_csv train --limit 100

Error: Traceback (most recent call last):
  File ""main.py"", line 37, in <module>
    bounding_boxes_images(args, DEFAULT_OID_DIR)
  File ""C:\OIDv4_ToolKit\modules\bounding_boxes.py"", line 60, in bounding_boxes_images
    df_val = TTV(csv_dir, name_file, args.yes)
  File ""C:\OIDv4_ToolKit\modules\csv_downloader.py"", line 21, in TTV
    df_val = pd.read_csv(CSV)
  File ""C:\Users\haoji\Python38\lib\site-packages\pandas\io\parsers.py"", line 686, in read_csv
    return _read(filepath_or_buffer, kwds)
  File ""C:\Users\haoji\Python38\lib\site-packages\pandas\io\parsers.py"", line 458, in _read
    data = parser.read(nrows)
  File ""C:\Users\haoji\Python38\lib\site-packages\pandas\io\parsers.py"", line 1186, in read
    ret = self._engine.read(nrows)
  File ""C:\Users\haoji\Python38\lib\site-packages\pandas\io\parsers.py"", line 2145, in read
    data = self._reader.read(nrows)
  File ""pandas\_libs\parsers.pyx"", line 826, in pandas._libs.parsers.TextReader.read
  File ""pandas\_libs\parsers.pyx"", line 869, in pandas._libs.parsers.TextReader._read_low_memory
  File ""pandas\_libs\parsers.pyx"", line 2053, in pandas._libs.parsers._concatenate_chunks
  File ""<__array_function__ internals>"", line 5, in concatenate
MemoryError: Unable to allocate 55.7 MiB for an array with shape (14610229,) and data type object

Please help, I followed the The AI Guy youtube video
https://www.youtube.com/watch?v=_4A9inxGqRM

"
Images Not Downloaded,EscVM/OIDv4_ToolKit,2020-07-08 08:18:43,1,,82,653091882,"![Screenshot 2020-07-08 13 33 05](https://user-images.githubusercontent.com/46680993/86894880-4f7e9a00-c121-11ea-8c4e-b67eed84a1d1.png)

That script didn't download even a single image but consumed 1138MB of data pack."
Thank you guys!,EscVM/OIDv4_ToolKit,2020-06-16 14:25:18,0,,80,639711889,This tool is amazing!
Error downloading images,EscVM/OIDv4_ToolKit,2020-05-22 22:53:23,0,,79,623510882,"I run: python main.py downloader --classes Weapon Airplane Vehicle_registration_plate --type_csv train --limit 500 and get the following error: [INFO] | Downloading Weapon.
Traceback (most recent call last):
  File ""C:\Users\123\Repos\OIDv4_ToolKit\modules\downloader.py"", line 25, in download
    columns, rows = os.get_terminal_size(0)
OSError: [WinError 6] The handle is invalid

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 37, in <module>
    bounding_boxes_images(args, DEFAULT_OID_DIR)
  File ""C:\Users\123\Repos\OIDv4_ToolKit\modules\bounding_boxes.py"", line 62, in bounding_boxes_images
    download(args, df_val, folder[0], dataset_dir, class_name, class_code)
  File ""C:\Users\123\Repos\OIDv4_ToolKit\modules\downloader.py"", line 27, in download
    columns, rows = os.get_terminal_size(1)
OSError: [WinError 6] The handle is invalid
Running this on windows 10 with python 3.5 but I also have 3.8
Sorry for the inconvenience and thank you for you time.
"
Memory Error while downloading for classification,EscVM/OIDv4_ToolKit,2020-05-20 06:19:43,0,,78,621491917,"![Screenshot (22)](https://user-images.githubusercontent.com/31354773/82411651-c9829300-9a8f-11ea-9e9b-3b4746d0b77c.png)
"
Cant exit the viewer,EscVM/OIDv4_ToolKit,2020-03-21 21:49:59,0,,75,585592006,The viewer works fine except when i try to exit it. Pressing 'q' doesnt seem to work and have to close the terminal. But everything else is awesome!
OIDv4 download nothing only error,EscVM/OIDv4_ToolKit,2020-02-25 06:40:37,3,,74,570347055,"I follow all the steps:
install all the 3rd library:pandas/numpy/awscli/urllib3/tqdm/opencv-python
but when I use: python main.py  downloader --classes Apple --type_csv validation

Here is what I get
 [INFO] | Downloading Apple.
Traceback (most recent call last):
  File ""E:\OpenImageV4\ss\OIDv4_ToolKit\modules\downloader.py"", line 25, in download
    columns, rows = os.get_terminal_size(0)
OSError: [WinError 6] ▒▒▒▒▒Ч▒▒

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 37, in <module>
    bounding_boxes_images(args, DEFAULT_OID_DIR)
  File ""E:\OpenImageV4\ss\OIDv4_ToolKit\modules\bounding_boxes.py"", line 70, in bounding_boxes_images
    download(args, df_val, folder[1], dataset_dir, class_name, class_code)
  File ""E:\OpenImageV4\ss\OIDv4_ToolKit\modules\downloader.py"", line 27, in download
    columns, rows = os.get_terminal_size(1)
OSError: [WinError 6] ▒▒▒▒▒Ч▒▒

It seems that the the error is caused by  function of  os.get_terminal_size. I try to solve it but fail.
Someone say Ubuntu system instead Windows may help. I try but still fail.

Hope some advice.
"
Download part of super subset and give all the same label,EscVM/OIDv4_ToolKit,2020-02-20 14:51:45,0,,73,568354116,"Hi
Thanks for the Greate Job Realy usefull!

In the OpenImages there are subsets of classes,
Such as Vehicle contains Land Vehicle,Aerial vehicle, etc

how can I choose to download all category but exclude some subset, such as Aerial vehicle but exclude Rocket.
and lable all in same super lable.

Is this possible?

I don't want to download each sub category, but instead download all category, and remove the ones that I don't need.

Thanks

![image](https://user-images.githubusercontent.com/17617515/74945812-08f20280-5401-11ea-8477-2c590e243b63.png)
"
sh: 1: aws: not foundaws: not found,EscVM/OIDv4_ToolKit,2020-02-10 10:36:49,7,,71,562469985,"[INFO] | Downloading Apple.

---------------------------------------------------------------------------------------------------Apple---------------------------------------------------------------------------------------------------
    [INFO] | Downloading validation images.
    [INFO] | [INFO] Found 46 online images for validation.
    [INFO] | Download of 46 images in validation.
sh: 1: sh: 1: sh: 1: aws: not foundaws: not found
aws: not found

sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 326460.21it/s]
    [INFO] | Done!
    [INFO] | Creating labels for Apple of validation.
    [INFO] | Labels creation completed.
    [INFO] | Downloading Orange.

---------------------------------------------------------------------------------------------------Orange---------------------------------------------------------------------------------------------------
    [INFO] | Downloading validation images.
    [INFO] | [INFO] Found 61 online images for validation.
    [INFO] | Download of 61 images in validation.
  0%|                                                                                                                                                                                | 0/61 [00:00<?, ?it/s]sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: sh: 1: aws: not found
aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: sh: 1: aws: not found
aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: sh: 1: aws: not found
aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
sh: 1: aws: not found
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:00<00:00, 1163.82it/s]
    [INFO] | Done!
    [INFO] | Creating labels for Orange of validation.
    [INFO] | Labels creation completed.
"
Memory error,EscVM/OIDv4_ToolKit,2020-01-25 15:34:22,1,,70,555098720,"`pandas.errors.ParserError: Error tokenizing data. C error: out of memory. `
What do I do Please provide a fix!!"
Preparing label for training YOLOV3,EscVM/OIDv4_ToolKit,2019-12-29 16:42:03,2,,69,543471926,"Thank you very much for your fantastic work. Can you please let me know which part of your program makes the label text file? I want to update that part to prepare the annotation text file for using it with YOLOv3.
"
Multiple classes present in the same image,EscVM/OIDv4_ToolKit,2019-12-09 09:13:42,6,,67,534790272,"Hi,
I have a problem regarding labeling of images containing instances from multiple classes I want to detect.

Say that I want to train a model to detect ""guitars"" and ""human faces"" in pictures.
I download images specifying the 2 classes, and I got (say 1K) images for each class, with the corresponding annotations for bounding boxes.
The problem is that, for every folder of images, only one class is annotated.

So, in the ""human face"" folder, I have 1000 images, and faces are labelled in every picture.
On the other hand, in the ""guitar"" folder, I have 1000 images with labelled guitars...however, as you can imagine, in most of the pictures of guitars there are also people, but their faces are not annotated.

At this point, I can't use them for training as they are, because all the faces in the ""guitar"" images are not labeled as such, and it would provide lots of false negatives to the model.
I tried using the multiclasses argument:

`python3 main.py downloader --classes Guitar Human_face --type_csv train --limit 1000 --multiclasses 1`

and it's the same, the images and annotations are downloaded in the same folder, but they are always separated per class.

Am I doing anything wrong here? Is there a way to download images with labels for all the classes I'm interested in?"
Only 20% of images with image-level labels downloadable through toolkit,EscVM/OIDv4_ToolKit,2019-11-26 17:03:53,2,,66,528857579,"I noticed that the image-level labeled images downloader couldn't find a lot of images in the bucket for image-level labeled images. On further inspection, I've realized that it downloads them from the same bucket as the bounding boxes, meaning that only around 2 million of the 10 million images are available. I appreciate that there is no bucket for the image-level labels, so I'm assuming this was intentional, but I think this limitation should be emphasized in the documentation for transparency."
Create custom model for 230.000 images using darknet,EscVM/OIDv4_ToolKit,2019-11-21 14:28:45,1,,65,526629782,"I have jpg files that contains 230.000 car plate images for graduate project and i'm new for this topic.
Can i convert this images to yolo format using this repo ? "
Crop out the bounding box,EscVM/OIDv4_ToolKit,2019-11-08 12:21:00,0,,63,519985507,"How can i get the part of the image which is only in the bounding box? So just the bounding box, not the surrounding of it.

Thank you!"
Fatal error using downloader_ill,EscVM/OIDv4_ToolKit,2019-11-08 09:32:51,0,,62,519907595,"I will check this issue but it's not due to us; it seems that the image `0d72ff3e2601d71c.jpg` is present on the csv file but it's not on the OIDv4 server.

_Originally posted by @keldrom in https://github.com/EscVM/OIDv4_ToolKit/issues/30#issuecomment-480489271_

Hello, I would like to know if this is solved?"
visualize the labeled images does not work,EscVM/OIDv4_ToolKit,2019-11-07 22:58:20,0,,61,519561199,"!python3 OIDv4_ToolKit/main.py visualizer

does not work on Google colab. 
Error: : cannot connect to X server "
Feature Request: Kitti Labels Format?,EscVM/OIDv4_ToolKit,2019-10-06 05:58:16,0,,59,503056747,"Not so much a bug or an error report, but more of a feature request. If it's possible to have the labels in the kitti labels format? A reference exists here:

https://github.com/NVIDIA/DIGITS/tree/master/digits/extensions/data/objectDetection#label-format"
Unable to download from OIDv5,EscVM/OIDv4_ToolKit,2019-10-02 21:20:12,4,,58,501732811,"Hi!
With the new version of OID it is impossible to download the images, thinking that this memory error is due to this incompatibility?

WIN10
PYTHON 3.7.4


`    [INFO] | Downloading Handgun.
Traceback (most recent call last):
  File ""main.py"", line 37, in <module>
    bounding_boxes_images(args, DEFAULT_OID_DIR)
  File ""C:\Users\aless\Desktop\YOLOv3_GUN\Tool\OIDv4_ToolKit-master\modules\bounding_boxes.py"", line 60, in bounding_boxes_images
    df_val = TTV(csv_dir, name_file, args.yes)
  File ""C:\Users\aless\Desktop\YOLOv3_GUN\Tool\OIDv4_ToolKit-master\modules\csv_downloader.py"", line 21, in TTV
    df_val = pd.read_csv(CSV)
  File ""C:\Users\aless\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\io\parsers.py"", line 685, in parser_f
    return _read(filepath_or_buffer, kwds)
  File ""C:\Users\aless\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\io\parsers.py"", line 463, in _read
    data = parser.read(nrows)
  File ""C:\Users\aless\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\io\parsers.py"", line 1154, in read
    ret = self._engine.read(nrows)
  File ""C:\Users\aless\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\io\parsers.py"", line 2059, in read
    data = self._reader.read(nrows)
  File ""pandas\_libs\parsers.pyx"", line 881, in pandas._libs.parsers.TextReader.read
  File ""pandas\_libs\parsers.pyx"", line 924, in pandas._libs.parsers.TextReader._read_low_memory
  File ""pandas\_libs\parsers.pyx"", line 2165, in pandas._libs.parsers._concatenate_chunks
  File ""<__array_function__ internals>"", line 6, in concatenate
MemoryError: Unable to allocate array with shape (14610229,) and data type object`"
Multi-word class names,EscVM/OIDv4_ToolKit,2019-10-02 19:45:56,3,,57,501688901,"When label files are created it would be nice if multi-word class names like ""adhesive tape"" and ""brown bear"" be in quotes or space gets replaced with underscore. Otherwise it's little bit problematic to process those files.

PS.
The readme suggests using underscore in classes file but such classes (e.g. adhesive_tape) aren't found. But it happily accepts natural names so i don't know what's going on."
OIDv5?,EscVM/OIDv4_ToolKit,2019-07-25 18:20:21,12,,48,473005431,"Hello! 

Any plans to release a version for OIDv5 anytime soon?"
instance segmentation,EscVM/OIDv4_ToolKit,2019-05-14 16:19:35,3,,42,444007493,"Thank you for your wonderful work. The open Images Dataset V5 was just released, which contained instance segmentation. Would you mind update the package to support download of instance mask?

Thank you again for your work."
Argument files,syyeung/frameglimpses,2017-07-20 13:49:11,4,,3,244369040,"Hi,

Could you please provide the files: train_data_file, train_meta_file, val_data_file, val_meta_file, val_vids_file, class_mapping_file. My interest is only for academmic research purposes. Thanks in advance.
"
About hdf5 files,syyeung/frameglimpses,2016-09-10 08:58:43,7,,1,176169565,"Hi， can you please provide some scripts to generate the hdf5 files such as train_data_file? thx~
"
gpu：rtx3090 ， show error    MemoryError: std::bad_alloc: cudaErrorMemoryAllocation: out of memory ,js1010/cuhnsw,2022-04-10 06:19:18,0,,30,1198900646,"File ""/media/data/senith_e/anaconda3/lib/python3.9/site-packages/cuhnsw-0.0.8-py3.9-linux-x86_64.egg/cuhnsw/pyhnsw.py"", line 63, in build
    self.obj.build_graph()

when in gpu ：titan xp，can right run
so why in rtx3090 can’t"
Some APIs are avaiable?,js1010/cuhnsw,2021-04-20 12:34:01,0,,29,862770352,"Hi, I created a image matching system using the `hnswlib`.
I am going to replace the `hnswlib` with the `cuhnsw` to use GPUs.

Do some APIs of the `hnswlib` exist in the `cuhnsw` too?
For example,
- `hnswlib.add_items(data, data_labels)` for adding custom id scheme and APPENDING(not setting) new data
- `hnswlib.get_items(ids)`

Thanks"
EdgeBoxesVOC2007test.mat was not found.,hbilen/WSDDN,2019-04-26 07:28:40,3,,20,437536600,The requested URL /hbilen-data/WSDDN/EdgeBoxesVOC2007test.mat was not found on this server.
only get 32.9 mAp,hbilen/WSDDN,2019-01-15 01:00:51,1,,18,399142647,"
Hello, we use your source code, but only got the map of 32.99 (vgg16), is this a normal deviation? We have not made any changes to the source configuration."
lossTopB: NaN,hbilen/WSDDN,2018-12-12 08:20:22,0,,17,390111973,"Hello, when I run the wsddn_train.m, it shows:
train: epoch 01: 146/5011: 2.6 (2.2) Hz lossTopB: NaN mAP: 12.229 objective: NaN"
Keyerror verbosity,jiny2001/deeply-recursive-cnn-tf,2021-04-28 11:07:53,0,,15,869824134,"Duplicate flags error, keyerror verbosity
on windows 10 platform for spyder
"
I cannot see where you apply transpose convolution,jiny2001/deeply-recursive-cnn-tf,2020-04-03 09:04:30,0,,14,593232938,"Hi,

if I understand the paper correctly, we input a small image and output a large image with good resolution.

So I read through your code and I dont know where you apply transposed convolution(upscaling) to get larger outputs. 

from the paper it must be in re-construct layer but I just cannot find in your code.

Can you explain a bit about this point?"
Is it recursive?,jiny2001/deeply-recursive-cnn-tf,2019-10-02 02:47:04,0,,13,501244897,"In the paper, the inference network only contains a single conv layer, and it is conducted with a D-times loop. In this implementation, it seems construct D conv layers with same W and B. Thus, this version cannot accomplish reduction on parameters and memory, with no recursion, right?"
The problem of not fount files during testing,jiny2001/deeply-recursive-cnn-tf,2019-03-06 10:49:11,2,,11,417739597,"When I test with a trained model, I use the command: python test.py --dataSet14 --inference_depth 9 --feature_num 96 has the following error

Features:96 Inference Depth:9 Initial LR:0.00100 [model_F96_D9_LR0.001000]
(3, 3, 1, 96)-864, (96,)-96, (3, 3, 96, 96)-82944, (96,)-96, (3, 3, 96, 96)-82944, (96,)-96, (3, 3, 96, 96)-82944, (96,)-96, (3, 3, 97, 1)-873, (1,)-1, (9,)-9,
Total 11 variables, 250,963 params
Model restored.
Traceback (most recent call last):
  File ""test.py"", line 82, in <module>
    if __name__ == '__main__':
  File ""C:\Users\lenovo\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""test.py"", line 78, in main
    model.init_all_variables(load_initial_data=FLAGS.load_model)
  File ""E:\程序\deeply-recursive-cnn-tf-master\super_resolution.py"", line 446, in do_super_resolution
    org_image = util.load_image(file_path)
  File ""E:\程序\deeply-recursive-cnn-tf-master\super_resolution_utilty.py"", line 205, in load_image
    raise LoadError(""File not found [%s]"" % filename)
super_resolution_utilty.LoadError: File not found []"
X4 model,jiny2001/deeply-recursive-cnn-tf,2018-10-26 09:17:32,0,,10,374296169,Hello there! Which is your X4 model? I changed the scale to 4 and the result was very poor.
PSNR,Nhat-Thanh/VDSR-Pytorch,2022-07-27 15:31:17,0,,1,1319738695,"I am trying to train it with T91 and the PSNR is remaining 16.9neven after 50 Epochs. What am I doing wrong?
"
请问wiindows为什么无法下载处理好的RGB\FLOW图啊,feichtenhofer/twostreamfusion,2021-12-10 01:54:53,1,,61,1076332591,
How to load pre-trained resnet-50 weights?,feichtenhofer/twostreamfusion,2021-07-01 16:27:34,0,,59,935017414,"Hello @abursuc and @feichtenhofer, can you tell how the dictionary is structured in the saved .mat weight file? I tried loading them as usual by simply referencing the keys, but the output does'nt make any sense.

It would be great if you could give some pointers on how to load the dictionary properly so that I can create a model instance and load them. "
Speed is too low,feichtenhofer/twostreamfusion,2019-11-14 15:25:43,2,,57,522929816,"My speed is ""train: epoch 01:   1/994: lr: 1e-03, 1.2 Hz "" with a GTX 1060,it's too low and doesn't make sense. Anyone have the same question?"
"when I try to unzip the UCF101 RBG file, a problem occured",feichtenhofer/twostreamfusion,2019-07-31 16:58:23,2,,56,475255599,"After download the dataset and run 'cat ucf101_jpegs_256.zip* > ucf101_jpegs_256.zip', I try to unzip the file, use the command 'unzip ucf101_jpegs_256.zip', but a problem occured, 'jpegs_256/v_TrampolineJumping_g09_c05/frame000020.jpg  bad CRC 5860d5eb  (should be b9b7fa4a)', is the picture damaged?"
How Can I get a PYTORCH VERSION?,feichtenhofer/twostreamfusion,2019-06-21 15:50:57,0,,55,459263412,"Hello, I would like to know whether there is a version which is based on Pytorch, and if there is, could that be possible to be sharing to me !
Thanks a lot!"
the dataset of flow are damaged files,feichtenhofer/twostreamfusion,2019-03-30 08:04:33,0,,54,427257107,"there are many "".bin"" files in 'U' folder,can you upload the dataset again, thanks"
Pre-computed RGB images cannot be merged,feichtenhofer/twostreamfusion,2018-11-18 02:33:32,1,,52,381921727,"I have downloaded your pre-computed RGB images, but I have some problems in merging the three parts (ucf101_jpegs_256.zip.001, ucf101_jpegs_256.zip.002, ucf101_jpegs_256.zip.003) into one .zip file. I wonder if some files (e.g. ucf101_jpegs_256.zip) are missing?"
Recommended arguments for optical flow computation,feichtenhofer/twostreamfusion,2018-11-05 05:14:44,0,,51,377253312,"Hi @feichtenhofer,

Thanks for sharing this amazing work. For my work, I also wanted to calculate flow for Olympic Sports dataset also by using the _**gpu_flow**_ library that you have shared. Can you please share with me the recommended values for the following options, so that the flow are coherent with the precomputed flows that you shared. 

1) skip
2) stride
3) clipFlow 
4) MIN_SZ 
5) OUT_SZ 

I would be happy to share the pre-computed flow so that others can also benefit and save some time."
Pre-computed optical flow components ,feichtenhofer/twostreamfusion,2018-10-02 14:35:12,5,,50,365928139,"Hi @feichtenhofer ,


Thank you for your work and for the pretrained models and processed data.

l just downloded your precomputed optical flow images (part1, part2, part3) 
 https://github.com/feichtenhofer/twostreamfusion#data

After unzipping the file l get the following directory :  

`/precomputed_flow/tvl1_flow` which is composed of   **u** and **v** sub-directories.

Do **u** and  **v** stands respectively for horizontal components and vertical components ? 

Thank you for your answer 


"
 Scores of the FV-encoded IDT descriptors,feichtenhofer/twostreamfusion,2018-03-29 13:56:32,0,,48,309764804,"Hi,

Could you please provide the SVM scores of the FV-encoded IDT descriptors that you used late fusion in the paper? My interest is purely for academic purposes.

Thank you in advance."
run cnn_ucf101_fusion with half of the videos,feichtenhofer/twostreamfusion,2018-03-08 06:10:54,0,,47,303367802,"Hello, I want to test the twostream method by just providing half of each video in UCF101. However, I found that the numbers of frames of each video were written into the pretrained models. Does anyone know which part should I changed to make that work?"
run cnn_ucf101_spatial.m  problem occured,feichtenhofer/twostreamfusion,2017-12-20 21:50:53,0,,46,283705826,"When I run ""**cnn_ucf101_spatial.m**"",I got this:


`**error dagnn.Layer/load (line 191)
        obj.(f) = s.(f) 
error dagnn.DagNN.loadobj (line 27)
    block.load(struct(s.layers(l).block)) ;
error cnn_ucf101_spatial (line 112)
  net = dagnn.DagNN.loadobj(net);**
`

How can i fix this problem?
"
pretrained model,feichtenhofer/twostreamfusion,2017-12-18 15:35:13,1,,45,282920484,"The link to pretrained model for ResNet is not available, anyone else encounter this situation?"
No public property dilate exists for class dagnn.Conv when running cnn_ucf101_spatial.m,feichtenhofer/twostreamfusion,2017-10-24 01:50:24,1,,44,267871135,"No public property dilate exists for class dagnn.Conv
Error in dagnn.Layer/load(line 191)
obj.(f)=s.(f)
error in dagnn.dagnn.loadobj(lin27)
block.load(struct(s.layers(l).block))
error in cnn_ucf101_spatial(line112)
net=dagnn.DagNN.loadobj(net)
Another question:your dataset has being framed?And if I train on my own dataset,do I only need to use cnn_ucf101_spatial.m,cnn_ucf101_temporal.m and cnn_ucf101_fusion.m??Do i need to change some configuration in other .m files?
Thanks for you attention "
Use optical flow extraction  on MSVD Youtube dataset,feichtenhofer/twostreamfusion,2017-10-22 06:08:27,0,,43,267439117,"I followed your installation steps,but I got errors like this:
CMake Error at CMakeLists.txt:12 (find_package):
  By not providing ""FindQt5Widgets.cmake"" in CMAKE_MODULE_PATH this project
  has asked CMake to find a package configuration file provided by
  ""Qt5Widgets"", but CMake did not find one.

  Could not find a package configuration file provided by ""Qt5Widgets"" with
  any of the following names:

    Qt5WidgetsConfig.cmake
    qt5widgets-config.cmake

  Add the installation prefix of ""Qt5Widgets"" to CMAKE_PREFIX_PATH or set
  ""Qt5Widgets_DIR"" to a directory containing one of the above files.  If
  ""Qt5Widgets"" provides a separate development package or SDK, be sure it has
  been installed.


-- Configuring incomplete, errors occurred!
Do I need to install Qt5.7?? I don't know how to add path to cmake.txt,would you give me some advice??
Thank you very much "
Subscripted assignment dimension mismatch,feichtenhofer/twostreamfusion,2017-10-11 08:45:39,1,,42,264503027,"cnn_ucf101_get_im_flow_batch.m(258)
if flip
          sx = fliplr(sx) ;
          imo(:,:,:,i,si) = imt(sy,sx,:) ;
          imo(:,:,1:2:nStack,i,si) = -imt(sy,sx,1:2:nStack) + 255; %invert u if we flip
else
          imo(:,:,:,i,si) = imt(sy,sx,:) ;
I get the error ""Subscripted assignment dimension mismatch"", so is the code wrong?"
CNN Accuracy,feichtenhofer/twostreamfusion,2017-10-09 07:56:14,0,,41,263810319,"Hello, I am very interested in your work and I am doing some reproduction work based on your work.
Now I have two questions which make me a little confused. May I ask about them?
Is accurary in your paper only use the Validation set ?
would you use test set for your cnn model?"
error in cnn_ucf101_get_im_flow_batch.m,feichtenhofer/twostreamfusion,2017-09-26 14:31:01,5,,40,260646317,"When I run cnn_ucf101_fusion.m, I get the error about 
""Subscript indices must either be real positive integers or logicals"" 
at the code
 ""last_frame = min(frameSamples(end), max(nFrames - nStack/4 - opts.nFrameStack,nStack/4 ));""
in file ""cnn_ucf101_get_im_flow_batch.m"".
It seems ""framSamples"" is empty. Does it mean that I didn't load the data successfully?
"
incompatibility,feichtenhofer/twostreamfusion,2017-09-07 13:33:42,2,,39,255938477,"Hi Feichtenhofer

I have the following version incompatibity problem as follow:

replace classifier layer of fc1000_bias
No public property dilate exists for class dagnn.Conv.

I use the matconv 23 version and the two-stream-matconv+dagnn. thanks a lot!"
out of memory,feichtenhofer/twostreamfusion,2017-07-17 06:25:19,2,,38,243305369,
Error using load,feichtenhofer/twostreamfusion,2017-07-11 12:08:48,0,,37,242024762,"No read file ""\ucf101-img-vgg16-split1-dr0.85.mat"".
There is no file or directory. Thanks ."
"thank you~I’m learning the code of two-steam CNN,could you give me your Contact information.",feichtenhofer/twostreamfusion,2017-07-01 12:48:41,0,,36,239947469,
a parameter about the two-stream CNN,feichtenhofer/twostreamfusion,2017-07-01 12:12:34,0,,35,239945845,opts.frameList = NaN; I have a questions about this parameter.What does it mean?
Question about learning rate setting,feichtenhofer/twostreamfusion,2017-06-16 12:22:28,0,,34,236467595,"Hi, I haven't used the matconvnet framework, so I don't understand well about the code of the lr setting. 
      opts.train.learningRate =  [1e-3*ones(1, 3) 5e-4*ones(1, 5) 5e-5*ones(1,2) 5e-6*ones(1,2)]  ;
It confuses me, could you tell me the detail about the lr setting? thx"
HMDB51 weights,feichtenhofer/twostreamfusion,2017-05-16 01:51:52,0,,32,228893161,Is there any reason why the weights for the HMDB51 dataset is not available? I would like to do use the trained weights on the HMDB51 for some experiments. Thanks.
How long does cnn_ucf101_fusion.m take?,feichtenhofer/twostreamfusion,2017-04-19 03:38:39,6,,30,222608124,"I have run the fusion.m.And found that this code may need very long tme.I ran about 10 hours and it still in epoch 01 : 600/994. The number of epoch seems to be 300.So this program need about 150 days????Is there something wrong with me?I ran this in one gpu:Tesla k40c. And according to other joiners,muti-gpu can't work?  "
Version Incompatibility issue,feichtenhofer/twostreamfusion,2017-02-27 06:15:35,20,,27,210396609,"Hi @feichtenhofer ,
I think there is some issue with matconvnet version used in this code. 
When I am using the matconvnet uploaded to this repo, I get the error about ""No public property dilate exists for class dagnn.Conv"". 
I tried using the latest version of matconvnet, but I get this error: ""The class dagnn.DagNN has no Constant property or Static method named 'setLrWd'. "" 
So, apparretnly, none of these versions work. I was wondering if you happen to see this issue? Could you please help me with this?
Thank you,
Best. "
Could you please tell me how to implement your fusion with caffe,feichtenhofer/twostreamfusion,2017-01-10 14:20:30,0,,26,199833298,"Thanks for your sharing!
Could you please tell me how to implement your fusion between two models(rgb vgg and optical flow vgg) with caffe？And I only want to fuse the fc8 layer of both models."
The UCF101 flow dataset cannot be download.,feichtenhofer/twostreamfusion,2016-12-19 00:23:39,4,,25,196309834,"@feichtenhofer Hi, The dataset cannot be download from the link you give. Could you please check the link. Thanks for your attention."
imresize error,feichtenhofer/twostreamfusion,2016-12-15 09:19:22,17,,24,195753792,"hello, I got following error:
Expected input number 1, A, to be nonempty.
Error in imresize>parsePreMethodArgs (line 333)
validateattributes(A, {'numeric', 'logical'}, {'nonsparse', 'nonempty'}, mfilename, 'A', 1);
Error in imresize>parseInputs (line 248)
[params.A, params.map, params.scale, params.output_size] = ...
Error in imresize (line 141)
params = parseInputs(varargin{:});
Error in cnn_ucf101_get_im_flow_batch (line 248)
            imt =   imresize(gather(imt(dy:sz(1)+dy-1,dx:sz(2)+dx-1,:)), [opts.imageSize(1:2)]);
Error in getBatchWrapper_ucf101_rgbflow>getBatch (line 52)
im = cnn_ucf101_get_im_flow_batch(images, opts, ...
Error in getBatchWrapper_ucf101_rgbflow>@(imdb,batch,moreopts)getBatch(imdb,batch,opts,numThreads,trainopts,moreopts) (line 3)
fn = @(imdb,batch, moreopts) getBatch(imdb,batch,opts,numThreads, trainopts, moreopts) ;
Error in cnn_train_dag>process_epoch (line 306)
    [inputs] = state.getBatch(state.imdb, batch, moreopts) ;
Error in cnn_train_dag (line 108)
    s_train = process_epoch(net, state, opts, 'train');
Error in cnn_ucf101_fusion (line 349)
[info] = cnn_train_dag(net, imdb, fn, opts.train) ;
"
utility used at test time,feichtenhofer/twostreamfusion,2016-11-16 04:28:03,2,,23,189584684,Can you please guide me that which code one can use at test time.
could not use more than one gpu,feichtenhofer/twostreamfusion,2016-11-09 02:54:23,7,,22,188157121,"hi, when I use one gpu, the code works well.
But when I change opts.train.gpus = 1; (in cnn_ucf101_temporal.m) to opts.train.gpus = [1,2]; it was fail:

Error using cnn_train_dag (line 120)
Error detected on worker 1.

Error in cnn_ucf101_temporal (line 231)
[info] = cnn_train_dag(net, imdb, fn, opts.train) ;

Caused by:
    Error using cnn_train_dag>map_gradients (line 503)
    Invalid file identifier. Use fopen to generate a valid file identifier.
 

I've tested  opts.train.gpus = [1,2] on the mnisit in Matconvnet example, it works well. anyone could help me?
"
Do you use the val error_1 of the last val epoch as the final result?,feichtenhofer/twostreamfusion,2016-11-03 13:01:40,2,,21,187053462,"Hi, thank you a lot for your great work.
I ran your code for several iterations. And I noticed that the error_1(val) for each iteration, is just the same as the error_1(val) of the last val epoch of the iteration.
Shouldn't it be the average of every epoch?

"
Error in running compile.m,feichtenhofer/twostreamfusion,2016-11-02 11:16:11,3,,20,186773378,"Hi,
I followed the README guide, and run the compile.m, but met the following error: 
incorrect use make_all>check_clpath (line 385)
Unable to find cl.exe

error make_all (line 207)
      check_clpath(); % check whether cl.exe in path

error run (line 96)
evalin('caller', [script ';']);

error compile (line 23)
run(fullfile(fileparts(mfilename('fullpath')), "
About the implementation in HMDB51 case.,feichtenhofer/twostreamfusion,2016-11-02 06:29:00,1,,19,186722495,"Hello I am very appreciate for your work. And I now I start to try to reproduce your work in HMDB51 case by your code. But I noticed some parts for HMDB51 case is nor included in your webpage, such as 'cnn_hmdb51_setup_data.m', and baseline networks for hmdb51. 

I have just run the training and testing in HMDB51 case by adapting from your UCF case (e.g. adapt your ucf101 baseline networks into networks for 51 categories).

 But my result is a little weird. Could you please kindly upload your HMDB51 part code and network models?

Thank you very much for your precious time."
How to extract feature from fusion net?,feichtenhofer/twostreamfusion,2016-10-07 03:41:01,3,,15,181579019,"Hi, I would like to directly extract feature from the pretrain nets. How can I modify the code since cnn_ucf101_fusion.m is supposed to train the nets. 
"
the first paper proposed subpixel convolution?,atriumlts/subpixel,2021-07-03 06:54:19,0,,66,936190043,"Hello, thanks for sharing the work. I want to know the first paper that proposed subpixel convolution, not efficient subpixel convolution that you discussed in the work. Please answer me."
Is this the same as tf depth_to_space?,atriumlts/subpixel,2021-01-09 05:58:24,0,,65,782520918,As title
the project requestments.txt is needed.,atriumlts/subpixel,2020-07-08 10:39:05,0,,64,653186653,"Amazing to find this project and thank you for your work! I want to realize the project ,would you please tell me the request.txt?thanks! "
"ValueError: Shape (32, 64, 32, 4, 4) must have rank 0",atriumlts/subpixel,2019-11-22 08:26:57,0,,63,527056445,
Evaluation,atriumlts/subpixel,2019-11-03 07:01:19,1,,62,516789859,Do you release the quantitative evaluation code?
Pre -Trained model,atriumlts/subpixel,2019-03-04 04:12:54,1,,61,416622059,"thank you very much for this work 
but  would you release the pre-trained model ,thanks"
Keras SubPixel 3D,atriumlts/subpixel,2018-12-01 04:00:32,0,,60,386419528,"Hi, Thank you for the Subpixel 2D implementation in Keras, I am able to run the 2D implementation.
I am currently working on a 3D Convolution and trying to implement Subpixel for 3D convolution. I was wondering if there was an implementation for 3D Convolution and / or is there any specific parameters that I need to take into account when implementing Subpixel 3D"
What to do in grayscale images?,atriumlts/subpixel,2018-05-04 20:04:11,0,,56,320410487,"Error trying in grayscale images
ValueError: Error when checking target: expected SubPixel to have shape (250, 250, 3) but got array with shape (250, 250, 1)
Thank you in advance !!!!!!!!!!!"
How subpixel handle odd image size？,atriumlts/subpixel,2018-02-04 09:36:55,0,,55,294184000,"such as an input images with size (?, 501, 301, ?).
considering a process:
 images-> (501, 301)
pool ->(251, 151)
subpixel->(502, 302)

I think it might add a tf.image.resize_bilinear function at each subpixel convolution?

"
"Please confirm PS(I,r) ",atriumlts/subpixel,2018-01-01 06:27:17,1,,54,285290970,"Please confirm that the function 
```python
def PS(I, r):
  assert len(I.shape) == 3
  assert r>0
  r = int(r)
  O = np.zeros((I.shape[0]*r, I.shape[1]*r, I.shape[2]/(r*2)))
  for x in range(O.shape[0]):
    for y in range(O.shape[1]):
      for c in range(O.shape[2]):
        c += 1
        a = np.floor(x/r).astype(""int"")
        b = np.floor(y/r).astype(""int"")
        d = c*r*(y%r) + c*(x%r)
        print a, b, d
        O[x, y, c-1] = I[a, b, d]
  return O
```
does not implement the `PS(T)_{x,y,c}` :
<img src=""https://user-images.githubusercontent.com/20728373/34465875-3cfe661a-ee76-11e7-8b83-e49b5a76b699.png"">

In particular, the line in `PS(I,r)`
```python
d = c*r*(y%r) + c*(x%r)
```
does not implement the index in `PS(T)_{x,y,c}`
```latex
C * r * mod(y,r) + C * mod(x,r) + c
``` 
where  capital letter `C` and  small letter `c` are two different things."
Testing the model,atriumlts/subpixel,2017-12-07 18:32:35,0,,53,280233254,"After training the model with the following line
`python main.py --dataset celebA --is_train true --is_crop true`
how can I test the model?

I've tried `python main.py --dataset celebA --is_train false` but it just print this 
`
{'batch_size': 64,
 'beta1': 0.5,
 'checkpoint_dir': 'checkpoint',
 'dataset': 'celebA',
 'epoch': 25,
 'gpu': '0',
 'image_size': 128,
 'is_crop': True,
 'is_train': False,
 'learning_rate': 0.0002,
 'sample_dir': 'samples',
 'train_size': inf,
 'visualize': False}
2017-12-07 20:30:13.309369: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2017-12-07 20:30:13.415852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-12-07 20:30:13.416285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce 840M major: 5 minor: 0 memoryClockRate(GHz): 1.124
pciBusID: 0000:08:00.0
totalMemory: 1.96GiB freeMemory: 1.55GiB
2017-12-07 20:30:13.416310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 840M, pci bus id: 0000:08:00.0, compute capability: 5.0)
 [*] Reading checkpoints...
`
then it terminats."
Query trained model,atriumlts/subpixel,2017-11-05 15:43:45,0,,52,271289101,"HI, first a big thank you for publishing this work.
I am trying to use a trained model and query it with a new probe image.
It seems to me a very imprtant functionality , after all that is what you train the network for, right? 
But I couldn't find it anywhere. I tried writing something, but I get poor results.
here is what I came up with:
any insights would be most appreciated. 
thanks,
Omer

```
import os
import cv2
import numpy as np
from model import DCGAN
from utils import get_image, image_save, save_images
import tensorflow as tf
from scipy.misc import imresize

flags = tf.app.flags
flags.DEFINE_integer(""batch_size"", 64, ""The size of batch images [64]"")
flags.DEFINE_integer(""image_size"", 128, ""The size of image to use"")
flags.DEFINE_string(""checkpoint_dir"", ""/home/omer/work/sub_pixel/models"",
                    ""Directory name to read the checkpoints [checkpoint]"")
flags.DEFINE_string(""test_image_dir"", ""/home/omer/work/sub_pixel/data/celebA/valid"",
                    ""Directory name of the images to evaluate"")
flags.DEFINE_string(""out_dir"", ""/home/omer/work/sub_pixel/out"", ""Directory name of to save results in"")

FLAGS = flags.FLAGS


def doresize(x, shape):
    x = np.copy((x + 1.) * 127.5).astype(""uint8"")
    y = imresize(x, shape)
    return y


def main():
    with tf.Session() as sess:
        dcgan = DCGAN(sess, image_size=FLAGS.image_size, image_shape=[FLAGS.image_size, FLAGS.image_size, 3],
                      batch_size=FLAGS.batch_size,
                      dataset_name='celebA', is_crop=False, checkpoint_dir=FLAGS.checkpoint_dir)
        res = dcgan.load(FLAGS.checkpoint_dir)
        if not res:
            print (""failed loading model from path:"" + FLAGS.checkpoint_dir)
            return

        i = 0
        files = []
        num_batches = len(os.listdir(FLAGS.test_image_dir)) / FLAGS.batch_size
        completed_batches = 0
        input_images = np.zeros(shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3))
        for f in os.listdir(FLAGS.test_image_dir):
            try:
                img_path = os.path.join(FLAGS.test_image_dir, f)
                if os.path.isdir(img_path):
                    i += 1
                    continue
                img = get_image(img_path, FLAGS.image_size, False)
                files.append(f)
                input_images[i] = img

                if i == FLAGS.batch_size - 1 or i == len(os.listdir(FLAGS.test_image_dir)) - 1:
                    batch_ready(dcgan, input_images, sess, files)

                    i = 0
                    input_images = np.zeros(shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3))
                    files = []
                    completed_batches += 1
                    print('done batch {0} out of {1}'.format(completed_batches, num_batches))
                else:
                    i += 1
            except Exception as e:
                print(""problem working on:"" + f)
                print (str(e))
                i += 1


def batch_ready(dcgan, input_images, sess, files):
    input_resized = [doresize(xx, (32, 32, 3)) for xx in input_images]
    sample_input_resized = np.array(input_resized).astype(np.float32)
    sample_input_images = np.array(input_images).astype(np.float32)
    output_images = sess.run(fetches=[dcgan.G],
                             feed_dict={dcgan.inputs: sample_input_resized, dcgan.images: sample_input_images})
    save_results(output_images, files)


def save_results(output_images, files):
    for k in range(0, len(files)):
        out_path = os.path.join(FLAGS.out_dir, files[k] + '_.png')
        out_img = output_images[0][k]

        # out_correct = ((out_img + 1) * 127.5).astype(np.uint8)
        # out_correct = cv2.cvtColor(out_correct, cv2.COLOR_RGB2BGR)
        # cv2.imshow('image', out_correct)
        # cv2.waitKey(0)

        image_save(out_img, out_path)


if __name__ == '__main__':
    main()


```"
"problem when run main.py with  mnist, Is there anyone have the same problem?or you know how this happen?please give me some advice!Thanks",atriumlts/subpixel,2017-11-01 07:57:13,0,,51,270226594,"  File ""main.py"", line 67, in <module>
    tf.app.run()
  File ""/home/fyh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 51, in main
    dcgan.train(FLAGS)
  File ""/home/fyh/ESPCN/subpixel-master/model.py"", line 141, in train
    save_images(sample_input_images, [8, 8], os.path.join(self.sample_dir, ""inputs_small.png""))
  File ""/home/fyh/ESPCN/subpixel-master/utils.py"", line 22, in save_images
    return imsave(inverse_transform(images[:num_im]), size, image_path)
  File ""/home/fyh/ESPCN/subpixel-master/utils.py"", line 41, in imsave
    return scipy.misc.imsave(path, merge(images, size))
  File ""/home/fyh/ESPCN/subpixel-master/utils.py"", line 31, in merge
    h, w = images.shape[1], images.shape[2]
"
Not work on tensorflow 1.3,atriumlts/subpixel,2017-08-30 06:54:58,2,,47,253896378,"tf.split and tf.concat API changed from earlier version.

"
Magic number,atriumlts/subpixel,2017-08-25 04:09:27,0,,46,252793738,Could you please tell me why do you use 32 in [here](https://github.com/tetrachrome/subpixel/blob/master/model.py#L160) and use 16*3 in [here](https://github.com/tetrachrome/subpixel/blob/master/model.py#L166) and use r=4 in [here](https://github.com/tetrachrome/subpixel/blob/master/model.py#L167)
Testing our dataset,atriumlts/subpixel,2017-07-05 10:41:56,1,,43,240610105,"Hi,
I have trained the model using celebA dataset and now I'm going to test my own data. How should I do it. Where the super-resolution test data (the output) will be saved? In which file exactly?

Thanks for your help. :)"
"Dynamic image size [batch, None, None, 3] error.",atriumlts/subpixel,2017-06-25 15:05:21,11,,41,238386051,"Hi thank you for releasing this implementation.

I try to evaluate the model on images with different size, so I would like to define a placeholder as:

```python
t_image = tf.placeholder('float32', [1, None, None, 3], name='input_image')
```

then when I use the subpixel layer, I will get:
```
    X = tf.reshape(I, (bsize, a, b, r, r))
  File ""/ssd2/Workspace/env3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2451, in reshape
    name=name)
  File ""/ssd2/Workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 508, in apply_op
    (input_name, err))
ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.
```

Do you have any idea to solve this problem? if we cannot use dynamic size, we need to define different inference for different image, which is resource and time consuming.

Many thanks in advance."
Implementation for keras,atriumlts/subpixel,2017-06-15 17:23:04,1,,38,236260101,"Hi, Thanks for the nice work. I was wondering if you are planning release keras code for subpixel layer nearby? I am currently working on a problem where I am using keras APIs. Thanks!

Thanks!"
the output picture is filled with grid ,atriumlts/subpixel,2017-05-12 07:14:37,0,,37,228207073,"When I implement the subpixel layer in DCGAN, the output image is very strange. it full of grid. (sorry,I can't upload the image somehow)




"
Why  it can't run on cpu?,atriumlts/subpixel,2017-05-02 05:58:37,1,,34,225597594,"Hi, thanks for your sharing, I try to run the code on cpu but failed. could you help me out?
I changed the link for installing tensorflow (on cpu)"
Why subpixel rather than deconv?,atriumlts/subpixel,2017-04-18 16:27:03,4,,33,222472981,"It seems like you highly recommend to use subpixel to replace deconv. But as introduced by Wenzhe Shi, subpixel upsampling is identical  to deconv upsampling. So I don't understand why you do this while deconv layer has been well developed in existed deep learning tools."
Difference from tf.depth_to_space,atriumlts/subpixel,2017-04-11 12:16:09,6,,32,220931968,"Hi,

Was amazed by this great idea compared to conv2_transpose. Looking at tensorflow documentation I found tf.depth_to_space, which seems to do something similar (at least the non-color version), am I mistaken? What's the difference? What's the reason you did not use this op?"
Trained model provided?,atriumlts/subpixel,2017-03-19 16:49:50,0,,31,215276970,"Hi:
Good work for the implementation and could you please provide the trained model?"
main.py  error,atriumlts/subpixel,2017-03-16 09:49:17,1,,30,214647969,"when I do the command which is ""python main.py --dataset mnist --is_train True --is_crop True"",there are some errors.

 tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""main.py"", line 36, in main
    dataset_name=FLAGS.dataset, is_crop=FLAGS.is_crop, checkpoint_dir=FLAGS.checkpoint_dir)
  File ""/home/qyq/q2015work/Image_Reconstruction/subpixel-master/model.py"", line 58, in __init__
    self.build_model()
  File ""/home/qyq/q2015work/Image_Reconstruction/subpixel-master/model.py"", line 75, in build_model
    self.G = self.generator(self.inputs)
  File ""/home/qyq/q2015work/Image_Reconstruction/subpixel-master/model.py"", line 167, in generator
    h2 = PS(h2, 4, color=True)
  File ""/home/qyq/q2015work/Image_Reconstruction/subpixel-master/subpixel.py"", line 21, in PS
    X = tf.concat(3, [_phase_shift(x, r) for x in Xc])
  File ""/home/qyq/q2015work/Image_Reconstruction/subpixel-master/subpixel.py"", line 12, in _phase_shift
    X = tf.concat(2, [tf.squeeze(x, axis=1) for x in X])  # bsize, b, a*r, r
TypeError: squeeze() got an unexpected keyword argument 'axis'

I'll let you know if make progress as well--thanks for the help!"
How to save the model run on android?,atriumlts/subpixel,2017-03-13 08:48:09,0,,29,213704084,"I write the model ,and transfer to android ,but the output is dark."
Reason for using transposed convolutions in the generator,atriumlts/subpixel,2017-02-06 04:09:15,0,,28,205484336,Is there any specific reason you would prefer a transposed convolution to a normal convolution in the generator ?
Inference - model and code,atriumlts/subpixel,2017-02-01 23:55:09,0,,27,204742359,"Could you provide the model data and sample code for doing inference, i.e. generating supper-rosultion images that you provide as examples? Thank you."
Do you have any suggestion on implementing this in caffe,atriumlts/subpixel,2017-01-11 06:49:24,7,,25,200016629,
error in ipython script,atriumlts/subpixel,2017-01-11 06:29:37,0,,24,200013894,"Your script :

1 def pony2(I, r):
2    a, b, c = I.shape
3    B = I.reshape(a/r, b/r, r, r).transpose(0, 1, 3, 2)
4    B = np.concatenate([B[i] for i in range(8)], axis=1)
5    B = B.transpose(1, 2, 0)
6    B = np.concatenate([B[:, :, i] for i in range(8)], axis=1)
7    return B

changed code :
1 def pony2(I, r):
2    a, b, c = I.shape
3    B = I.reshape(a, b, r, r).transpose(0, 1, 3, 2)
4    B = np.concatenate([B[i] for i in range(8)], axis=1)
5    B = B.transpose(1, 2, 0)
6    B = np.concatenate([B[:, :, i] for i in range(8)], axis=1)
7    return B

The code in your third is a error. Please change it."
when loss down?,atriumlts/subpixel,2016-12-09 08:36:42,0,,21,194540416,"Hi,i have some questions.
1.in the  1 epoch ,the loss gets lower from 0.1 to 0.05 . after 25 epoch ,no obvious change ..  
it is normal?  
2. the samples generated during training are like this:


![image](https://cloud.githubusercontent.com/assets/14009379/21042718/3cd8b2cc-be2e-11e6-9a6f-f75a438aee1e.png)
some lines can be seen nearly the edge of images . it is something wrong?

btw, how to use trained model  to generate sr image ?   




thank you "
Bug in Phase Shift?,atriumlts/subpixel,2016-12-07 20:26:40,3,,20,194159728,"As an example
```
        x3 = np.arange(10*528*528*48).reshape(10, 528, 528, 48)
        X3 = tf.placeholder(""float32"", shape=(10, 528, 528, 48), name=""X"")# tf.Variable(x, name=""X"")
        Y3 = PS(X3, 3, color=True)
        y3 = sess.run(Y3, feed_dict={X3: x3})
```
this works fine when the second arg to PS is, say, 4, but it chokes when it's 3

```
  File ""/home/user/Dev/subpixel/model.py"", line 75, in build_model
    self.G = self.generator(self.inputs)
  File ""/home/user/Dev/subpixel/model.py"", line 191, in generator
    h2 = PS(h2, 3, color=True)
  File ""/home/user/Dev/subpixel/subpixel.py"", line 21, in PS
    Xc = tf.split(3, 3, X)
  File ""/home/user/Dev/subpixel/subpixel.py"", line 9, in _phase_shift
    X = tf.reshape(I, (bsize, a, b, r, r))
  File ""/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2166, in reshape
    name=name)
  File ""/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 748, in apply_op
    op_def=op_def)
  File ""/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2405, in create_op
    set_shapes_for_outputs(ret)
  File ""/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1790, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 1696, in _ReshapeShape
    (num_elements, known_elements))
ValueError: input has 44605440 elements, which isn't divisible by 2509056

```

Any ideas?"
AttributeError: 'DCGAN' object has no attribute 'g_bn0,atriumlts/subpixel,2016-11-29 16:57:20,1,,19,192338658,"When I specify `--visualize True` as an arg, I get the following

```markb@dvsn:~/Dev/subpixel$ python main.py --dataset celebC --is_crop True --is_train True --epoch 2 --visualize True
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:116] successfully opened CUDA library libcurand.so.8.0 locally
{'batch_size': 64,
 'beta1': 0.5,
 'checkpoint_dir': 'checkpoint',
 'dataset': 'celebC',
 'epoch': 2,
 'image_size': 400,
 'is_crop': True,
 'is_train': True,
 'learning_rate': 0.0002,
 'sample_dir': 'samples',
 'train_size': inf,
 'visualize': True}
I tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:01:00.0
Total memory: 11.90GiB
Free memory: 11.35GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0)

 [*] Reading checkpoints...
 [*] Load SUCCESS
Epoch: [ 0] [   0/   3] time: 3.6289, g_loss: 0.38425747
WARNING:tensorflow:*******************************************************
WARNING:tensorflow:TensorFlow's V1 checkpoint format is deprecated; V2 will become the default shortly after 10/31/2016.
WARNING:tensorflow:Consider switching to the more efficient V2 format now:
WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`
WARNING:tensorflow:to prevent breakage.
WARNING:tensorflow:*******************************************************
Epoch: [ 0] [   1/   3] time: 8.5043, g_loss: 0.34239292
Epoch: [ 0] [   2/   3] time: 10.3694, g_loss: 0.28749919
Epoch: [ 1] [   0/   3] time: 12.3451, g_loss: 0.35043076
Epoch: [ 1] [   1/   3] time: 14.3204, g_loss: 0.29150483
Epoch: [ 1] [   2/   3] time: 16.3195, g_loss: 0.23925565
Traceback (most recent call last):
  File ""main.py"", line 60, in <module>
    tf.app.run()
  File ""/home/markb/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 49, in main
    to_json(""./web/js/layers.js"", [dcgan.h0_w, dcgan.h0_b, dcgan.g_bn0],
AttributeError: 'DCGAN' object has no attribute 'g_bn0'
```
Further, when I disable this call in hopes of getting sampler images to save, I get the following
```
Traceback (most recent call last):
  File ""main.py"", line 60, in <module>
    tf.app.run()
  File ""/home/markb/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 30, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""main.py"", line 57, in main
    visualize(sess, dcgan, FLAGS, OPTION)
  File ""/home/markb/Dev/subpixel/utils.py"", line 148, in visualize
    if option == 0:
AttributeError: 'DCGAN' object has no attribute 'sampler'
```
I can't find any mention of the dcgan's sampler in the code. Is there an easy way of fixing this?

Thanks!
Mark"
When probably the keras implementation could be released?,atriumlts/subpixel,2016-11-09 23:03:22,2,,18,188375605,"Hi,

I really appreciate this work, the tensorflow code is nice and clean. Great job！
I'm wondering if there will be a keras version for this project in the future?

Thanks"
how to use the trained model to generate super-res images,atriumlts/subpixel,2016-10-26 09:23:42,10,,16,185339860,"Probably a noob question but I have trained the model on the celebA dataset. How should I use this model to generate super-res versions of arbitrary images?

In main.py if the training mode is False, the model is simply loaded from checkpoints. How do I run the inference stage/forward pass of the loaded model?
"
tf.depth_to_space / tf.space_to_depth,atriumlts/subpixel,2016-10-03 05:28:33,1,,4,180565716,"Have you looked at implementing this using built-in functions in TensorFlow?  Should be much more efficient.
"
Where can I find the implement of  global loss  of pairwise similarity siamese network? ,vijaykbg/deep-patchmatch,2019-07-22 06:20:53,0,,2,470913414,Where can I find the implement of  global loss  of pairwise similarity siamese network? Is there a pytorch version?
How exaclty is the global triplet loss function implemented? Can you provide an implementation of it?,vijaykbg/deep-patchmatch,2018-02-04 18:02:49,3,,1,294216673,
small implementation error in torchattacks.DeepFool,LTS4/DeepFool,2021-12-06 14:04:00,0,,10,1072189442,"I currently use DeepFool as an estimate for the distance to the decision boundary of a classifier, as suggested in the paper: https://arxiv.org/abs/2002.01810v1

I used the implementation from torchattacks because I found the implementation from the LTS4/DeepFool repo hard to understand and work without throwing errors. In the implementation from torchattacks I found an implementation error, that returned the original images as perturbation when the classifier already misclassified the image. https://github.com/Harry24k/adversarial-attacks-pytorch/issues/51

This might also be the case in this repo, but I havent checked, just wanted to let you know."
Issue with test_deepfool.py,LTS4/DeepFool,2021-05-01 18:22:10,0,,9,873738195,"Instead of these lines 
tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=map(lambda x: 1 / x, std)),
                        transforms.Normalize(mean=map(lambda x: -x, mean), std=[1, 1, 1]),
                        transforms.Lambda(clip),
                        transforms.ToPILImage(),
                        transforms.CenterCrop(224)])

it should be like 

tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=**list(map(lambda x: 1 / x, std))**),
                            transforms.Normalize(mean=**list(map(lambda x: -x, mean))**, std=[1, 1, 1]),
                            transforms.Lambda(clip),
                            transforms.ToPILImage(),
                            transforms.CenterCrop(224)])
i
"
There may be a little mistake in test_deepfool.py,LTS4/DeepFool,2020-05-27 11:35:08,0,,8,625617993,"In line 51, that is:
`clip = lambda x: clip_tensor(x, 0, 255)`
should be:
`clip = lambda x: clip_tensor(x, 0, 1)`

In tf,` transforms.Lambda()` is in front, and there is no multiplication by 255 which will be operated by `transforms.ToPILImage() `.

But this should not affect the final result."
"Deepfool generated perturbation pictures, but the categories have not changed",LTS4/DeepFool,2019-12-06 07:13:59,2,,5,533790189,"Use test_deepfool.py to test on two test pictures. When the generated disturbed picture enters the network again for forward calculation, the category has not changed.
The images directly generated by deepfool have relatively large changes. After performing some normalization on the disturbed images generated by deepfool in the code, it is obviously closer to the original image, but the category of the generated image has not changed.
for example:
For the picture test_im1.jpg: the original category is macaw, and the deepfool shows that the disturbed picture category is flamingo (the picture directly output by the network has changed dramatically).
But after a series of other operations, it is very similar to the original image, but at the same time, the category is still macaw."
When running code on binary classification ,LTS4/DeepFool,2019-07-14 10:48:15,0,,4,467821828,"When running code on binary classification, does it mean that we only need to modify the num_classes = 2 instead of 10 as default?

When I tried it, I found a lot of perturbed samples as NaN value. Have you ever found this before?"
Cannot download the pre-trained model.,eborboihuc/Deep360Pilot-CVPR17,2022-02-26 15:10:15,0,,9,1151858075,"The parameter link provided in your readme is lost.

https://github.com/eborboihuc/Deep360Pilot-CVPR17#pre-trained-model
"
can't find some data,eborboihuc/Deep360Pilot-CVPR17,2018-06-07 14:22:43,2,,6,330295587,"where are the frame_bmx, frame_skate etc. in the clip based dataset so that to use Deep360Pilot-optical-flow code for other domains like bmx etc. for converting the input.
After testing with existing models I am trying prediction on feature_bmx_16boxes/zZ6FlZRLvek_6 but missing divide_area_pruned_boxes0001.npy"
Key error 10 while running the training code,SeungjunNah/DeepDeblur-PyTorch,2022-10-12 04:32:24,0,,49,1405548008,"The training code is stopped automatically at epoch 10 due to **key error 10**. I'm not exactly getting why this error occurs.
Could you help me to solve this error.
```Traceback (most recent call last):
  File ""main.py"", line 67, in <module>
    main()
  File ""main.py"", line 64, in main
    main_worker(args.rank, args)
  File ""main.py"", line 54, in main_worker
    trainer.test(epoch)
  File ""/home/DeepDeblur-PyTorch-master/src/train.py"", line 196, in test
    self.evaluate(epoch, 'test')
  File ""/home/DeepDeblur-PyTorch-master/src/train.py"", line 182, in evaluate
    tq.set_description(self.criterion.get_loss_desc())
  File ""/home/DeepDeblur-PyTorch-master/src/loss/__init__.py"", line 311, in geoss_desc
    loss = self.loss_stat[self.mode]['Total'][self.epoch]
KeyError: 10 

"
Test on my device,SeungjunNah/DeepDeblur-PyTorch,2022-10-03 01:13:23,4,,47,1393976832,"Hi @SeungjunNah !
I'm so appreciated you provide us with your work to learn more.I want to test the work on my device .As I know , the current result shows that everything loads from the downloaded files,including the loss ,psnr and ssim.What I want to do is that after loading the model weight that you provide, then test on my device and result new loss, psnr and ssim.I want to know is it can be done? If so ,could you help me?"
how to train it on custom dataset?,SeungjunNah/DeepDeblur-PyTorch,2022-03-09 10:46:44,3,,41,1163776375,"Hi, I've been trying to run this code on my custom dataset but couldn't understand the directory structure. Can someone help"
Running Demo (MultiSaver) on Windows,SeungjunNah/DeepDeblur-PyTorch,2020-09-29 13:18:40,6,,18,711104844,"I spent many hours trying to get this to work under Windows. I managed to get it to work now, so this is probably useful to others.

## Setup
The first obstacle is the `readline` Python package, which seems to be default on Unix systems, but not on Windows. For this, simply install the `pyreadline`  package, which is a Windows port of readline.

## Understanding the command-line
Example command: `python main.py --save_dir REDS_L1 --demo_input_dir d:/datasets/motion47set/noise_only --demo_output_dir ../results/motion47set`. 
Explanation: specifying `--demo_input_dir` (or `--demo true`) will run an evaluation, using a pretrained model as specified in `--save_dir`. Every image of my motion47set will be evaluated. The results will be saved alongside the folders `src` and `experiments` at the project root, in a folder `results/motion47set`.
Note that even getting this far is not very intuitive, as others have already pointed out. Usually there is a separate python script for just evaluation/testing/inference. Next, the term demo is a bit unusual, at first I was expecting some interactive demonstration of some form. The `save_dir` I had at first used as what `demo_output_dir` does.
Another word of caution, if the output path is given without any `.`, it somehow ends up saving the results at `d:/results/motion47set`, which again took me a while to figure out, i.e. on the root of the same drive that the project is located at. I suggest printing out the absolute output dir with `os.path.abspath` to the user at some point, for clarity.

## Bug
Running the above command will produce the following output:
```
===> Loading demo dataset: Demo
Loading model from ../experiment\REDS_L1\models\model-200.pt
Loading optimizer from ../experiment\REDS_L1\optim\optim-200.pt
Loss function: 1*L1
Metrics: PSNR,SSIM
Loading loss record from ../experiment\REDS_L1\loss.pt
===> Initializing trainer
results are saved in ../results/motion47set
|                                                        | 0/90 [00:00<?, ?it/s]Can't pickle local object 'MultiSaver.begin_background.<locals>.t'
|██▏                                             | 4/90 [00:06<02:14,  1.56s/it]Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""d:\Program Files\Anaconda3\envs\torch gpu\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""d:\Program Files\Anaconda3\envs\torch gpu\lib\multiprocessing\spawn.py"", line 115, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
|█████▋                                         | 11/90 [00:10<01:12,  1.09it/s]forrtl: error (200): program aborting due to control-C event
```
Also note that ctrl+c takes a really long time to terminate for me, and even slows down my entire machine for several seconds.

This is difficult to debug, because there is no fatal exception, and everything seems to run normally, ignoring the errors, which might also just be warnings, for all we know. I did not realize for a while that MultiSaver is a file of this project, which is why there is not much help online in regards to this error/warning. Second, the only that that gives a little stronger hint that this is an error, and not a warning, is the `EOFError`, which I still don't know why or where it even happens. A large part of debugging time was me assuming these were just warnings, and trying to fix the command-line arguments instead, since that is easy to get wrong.

What is actually happening is that the MultiSaver code runs clean on the main thread, but then each spawned thread/process will fail, without the main thread being aware. As a result, the program runs through, attempts to save the output images, which all do nothing since the threads/processes already died. I'm not sure how to to achieve this, but it would be nice if the program stops running when it is unable to save output images (at least in demo mode, where that's about the only purpose).

The keywords to locate the actual issue here are `pickle` and `multiprocessing`. Going into `utils.py` and looking at the class `MultiSaver` shows us a method `begin_background`, with a method-local variable `t` (another method). Defining that method works, however (under Windows) that variable has to be pickled/serialized to hand it over to the `mp.Process`, which will run it in a different thread/process. This fails because pickle does not support local objects.
I tried various ways to change the scope of `t`:
- put `global t` before the definition of `t` (no change)
- move `t` to the outermost scope of the file utils.py, i.e. same level as MutliSaver (can pickle the method, but later fails at a different point)
- the solution that works is putting `t` on the same scope as MultiSaver, and annotating it with `@staticmethod`. The annotation avoids the first method parameter to be used as `self`.

So my modification looks like this
```python
class MultiSaver():
...

    @staticmethod
    def t(queue):
        ...

    def begin_background(self):
        self.queue = mp.Queue()
        
        worker = lambda: mp.Process(target=MultiSaver.t, args=(self.queue,), daemon=False)
    ...
...
```

After this change, everything works as expected. I haven't tested it, but I suspect this will still work under Unix as well.
I'm not sure if this will work if multiple instances of `MultiSaver` are created, and maybe this would give the same result as putting `t` to the outermost scope, i.e. fail again."
Steps to run this project on personal device is so complecated,SeungjunNah/DeepDeblur-PyTorch,2020-09-20 10:21:12,1,,17,705083487,"Hi DeepDeblur Team,
Thanks for your great work

I am really sorry to say that, but the steps to run this project on personal device is so complected. I have spent around 4 hours looping around, modifying and testing, still not able to run the demo. inputs are out of numbers and totally unorganized

I want to ask if there is any modification or restructuring is going to be implemented in this regards (as adding some configuration files for the input parameters)

Also, I noted that it doesn't work without GPU, is there is any intention to implement it for both cases where GPU is available and not?

"
Data preprocessing,kyusbok/Video2ShopExactMatching,2022-09-25 07:49:28,0,,1,1384934430,"Hello author, what do the following annotation（ e['match_desc']:） information refer to in the code? Can you provide the annotated JSON file or Deepfashion preprocessing script?

e = self.coco.imgs[i]
            for x in e['match_desc']:"
median error,vislearn/dsacstar,2022-10-12 02:54:19,0,,22,1405476448,
Questions on Pre-training and pose matrix h,vislearn/dsacstar,2022-06-20 22:09:24,3,,20,1277428652,"Hi Eric
Thanks for your wonderful work on pose estimation.
(1). I was checking the performance improvement on the test dataset in the Cambridge_GreatCourt scene starting with your provided initial network and then the trained network in an end-to-end fashion. Surprisingly, the error rates are pretty the same before and after end-to-end learning. In particular, I'm running test.py by
python test.py Cambridge_GreatCourt models\rgb\cambridge\Cambridge_GreatCourt.net --mode 1 
and the results are
![image](https://user-images.githubusercontent.com/101369948/174682693-3a9a8c24-c696-4ff5-87fc-396eaad921b3.png)
However when I train the network for a few epochs in the second phase, by
python train_e2e.py Cambridge_GreatCourt models\rgb\cambridge\Cambridge_GreatCourt.net output\output.net --mode 1
the results are 
![image](https://user-images.githubusercontent.com/101369948/174682828-cefbf841-941f-42d5-9adb-a33f4ff95a25.png)
which means there is no benefit in the end-to-end learning stage.  Is that what you observed before? Or I'm doing something wrong.

(2). I'm a bit confused about the notations in the paper and what has been implemented in the code. In the paper, it was defined that h (4*4 matrix) is the pose matrix from the camera to the 3d scene coordinate. However, the ground truth pose is the matrix transforming from the environment to the camera coordinate. So direct differencing is not correct, right? I do not know within the c++ code whether you are computing the inverse or not, but I can see at the end, you are saving the inverse pose in the output text file. Can you please explain this?

Best regards
Aref



"
Problems with run mode 2 with my data,vislearn/dsacstar,2022-05-17 07:15:38,1,,18,1238183023,"Hello, I try to train DSAC* on my data captured by kinect v2 and I have some questions:
1. Because the intrinstics of RGB camera and depth camera are different, some depth pixels do not have corresponding RGB pixels(I use white color instead). The input RGB image may like this:
![2639](https://user-images.githubusercontent.com/80031171/168749903-063e8e43-6e25-40c5-a28a-94cf3d43c244.jpg)
Can the network handle with such case?
2. I train model by using my RGBD images. The training process is successed but when I run test.py in mode 2, I meet the errors:
![image](https://user-images.githubusercontent.com/80031171/168751195-62f016f0-0c22-42da-a07b-b6fe7d5e1784.png)
It seems that dsacstar.forward_rgbd failed on some frames. And when I run test.py in mode 1 everything is OK.

"
Problem with testing on mode 2,vislearn/dsacstar,2021-09-24 08:51:19,1,,12,1006234464,"Hello there, 

I have a problem with testing on mode 2. I can train models by using rendered depth maps with mode 2. But when I try to test them it gives results as below. To be sure I tried your pre-trained model it also gives the same results. When I test in mode 1, everything seems correct for both models. 

![image](https://user-images.githubusercontent.com/19689052/134644947-8a961519-6f9c-48dd-a076-4ff4a4d8b1f5.png)

Additionally, in rendered depth maps in seq04 frames between 450-500 does not match with the original images.
![scm_002475](https://user-images.githubusercontent.com/19689052/134646905-c5cb27a6-9de1-4c3e-b0c5-8f435bbaba59.png)
![frame-000475 color](https://user-images.githubusercontent.com/19689052/134646986-6f78afb4-ada5-47fc-93a6-a561e0044d2e.png)


Thanks for sharing your work. 


"
Problems executing setup.py,vislearn/dsacstar,2021-09-02 11:49:29,6,,11,986605427,"Hi, I made environment in conda with all the packages that you mentioned with correct versions, but I am constantly getting this error while running setup.py. 


dsacstar/dsacstar/dsacstar_util.h:193:5: error: ‘SOLVEPNP_P3P’ is not a member of ‘cv’

I am using OpenCV 3.4.2 and I manually set the paths for header and library files for for OpenCV in setup.py.  "
"bpy.data.objects['Cube'].select=True,AttributeError: 'Object' object has no attribute 'select'",gulvarol/surreal,2022-08-17 13:06:44,0,,63,1341738114,
Center of origin?,gulvarol/surreal,2022-06-08 15:01:44,0,,61,1264896421,Which key point is the center of origin (root joint) in surreal? Like pelvis is the center of origin for human3.6m
Low res textures,gulvarol/surreal,2022-05-05 12:06:17,2,,60,1226584347,"Why are textures provided so low res, are there higher-res ones also available, something near 2k like in the Seizer dataset. Is there a way to convert this res to higher res?"
download_smpl_data.sh not working,gulvarol/surreal,2022-03-17 14:35:23,5,,59,1172444313,"Hello Dear Author,

I am having a problem with executing the abovementioned script. I have all the log in credentials and executing it as said. Is there another way to download it. Thank you.
"
wrong ground-truth ,gulvarol/surreal,2021-06-14 12:44:59,0,,57,920375809,"Hi @gulvarol,
   I have worries on the data using code here https://github.com/gulvarol/surreal/blob/master/datageneration/misc/smpl_relations/smpl_relations.py.  It seems the head is twisted 180 degree in the  ground-truth(second one), 
![image](https://user-images.githubusercontent.com/6660254/121893804-aff95c80-cd1e-11eb-85f5-8ae3dbbdfb0e.png)

Here is the original data file: ``` cmu/train/run0/01_01/01_01_c0001_info.mat frame 0```
"
3D Pose in Info.mat,gulvarol/surreal,2021-02-22 17:12:16,0,,54,813693224,"Hello,

I learned that the 3D pose in Info.mat has a meter unit. Then, where is its origin!?

Thanks! "
Incorrect global rotation,gulvarol/surreal,2020-07-17 01:44:45,1,,49,658741122,"Hello, thank you for your excellent work！
I use surreal dataset to train the model，When reading smpl parameter data，I have done the preprocessing according to your method`pose[0:3] = rotateBody(RzBody, pose[0:3])`，However, during the training process, I found that the global rotation of the model was still biased (not too large)
But in the test program of dataset，Model rendering is correct (but left and right seem to be opposite)，
Different from the following program, I used orthogonal projection in my training program. Is there any error here?
`proj_smpl_vertices = project_vertices(smpl_vertices, intrinsic, extrinsic)` 

Thank you!"
cost too much memory,gulvarol/surreal,2019-08-12 03:14:20,0,,41,479451805,"i try to render data by myself,and i set clipsize to 500,but it cost too much memory!
is there has some function to release memory after render a image?

bpy.ops.render.render(animation=False,write_still=True,use_viewport=False,)
"
smpl texture,gulvarol/surreal,2019-07-11 07:17:09,0,,36,466707639,"The texture files present in Surreal Data-set are 512x512 packed texture images.
How to convert these texture files into a newer format of texture format currently being used in smpl texturing, which are 1080x1080.

![grey_female_0019](https://user-images.githubusercontent.com/13464149/61030332-c8e29000-a3da-11e9-8528-5d4b100024bc.jpg)
surreal texture map

![sample_texture_2](https://user-images.githubusercontent.com/13464149/61030359-d435bb80-a3da-11e9-8767-0cfbe23f7142.jpg)
uvtexture map found in new repositories like octopus https://github.com/thmoa/octopus 
"
Flow x-channel,gulvarol/surreal,2019-05-24 08:17:01,0,,35,448038061,"@gulvarol 
HI,

Thanks a lot for your works:) I found in an issue that after data generation, x channel of the optical flow should be multiplied by -1. I am a little bit confused about that. Does it mean that the direction of x-channel in ""RenderLayer.Vector"" which could be activated by ""use_vector_pass"" is the reverse direction of the motion in reality?

Thanks a lot for your response.

Best regards,
Haozhou"
How to generate the SMPL texture,gulvarol/surreal,2019-04-09 07:24:05,4,,30,430807598,"How can I use my image to generate the SMPL texture image like this?
![grey_male_0927](https://user-images.githubusercontent.com/45211872/55780849-7ba10a80-5adb-11e9-9af8-020b6d471ca0.jpg)
"
Strange values after converting 3D joint positions from world to camera coordinates,gulvarol/surreal,2019-04-08 11:05:19,2,,29,430388471,"Hi, thanks for the great dataset! I used some of the code you provided (e.g. for retrieving the camera extrinsic matrix) to convert 3D joint positions from world to camera coordinates. However, the values of joint positions in camera coordinates seem a bit strange to me. Here is the code:

```
def get_extrinsic_matrix(T):
	# Return the extrinsic camera matrix for SURREAL images
	# Script based on:
	# https://blender.stackexchange.com/questions/38009/3x4-camera-matrix-from-blender-camera
	# Take the first 3 columns of the matrix_world in Blender and transpose.
	# This is hard-coded since all images in SURREAL use the same.
	R_world2bcam = np.array([[0, 0, 1], [0, -1, 0], [-1, 0, 0]]).transpose()
	# *cam_ob.matrix_world = Matrix(((0., 0., 1, params['camera_distance']),
	#                               (0., -1, 0., -1.0),
	#                               (-1., 0., 0., 0.),
	#                               (0.0, 0.0, 0.0, 1.0)))

	# Convert camera location to translation vector used 
	# in coordinate changes
	T_world2bcam = -1 * np.dot(R_world2bcam, T)

	# Following is needed to convert Blender camera to 
	# computer vision camera
	R_bcam2cv = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])

	# Build the coordinate transform matrix from world to 
	# computer vision camera
	R_world2cv = np.dot(R_bcam2cv, R_world2bcam)
	T_world2cv = np.dot(R_bcam2cv, T_world2bcam)

	# Put into 3x4 matrix
	RT = np.concatenate([R_world2cv, T_world2cv], axis=1)
	return RT, R_world2cv, T_world2cv

def world_to_camera(RT,p_w):
        # Get a set of n points in world coordinates (p_w) and
	# convert them to camera coordinates (p_c)
        # Args:
        # p_w (numpy array): points in world coordinates, shape (n, 3, 1)
        # RT  (numpy array) : 4x4 camera extrinsic matrix, shape (4,4)
	n_points = p_w.shape[0]
	ones = np.ones([n_points, 1, 1])
	p_w = np.concatenate((p_w, ones), axis=1)
	
	p_c = np.dot(RT, p_w[0])
	for p in p_w[1:]:
		p_c = np.concatenate((p_c, np.dot(RT, p)))
		
	return p_c.reshape(n_points,3,1)

# Read annotation file
mat = scipy.io.loadmat(""./01_06_c0001_info.mat"")

# Get camera position in world coordinates
camera_pos = mat['camLoc']

# Get camera extrinsic matrix
extrinsic, _, _ = get_extrinsic_matrix(camera_pos)

# Frame number
frame_id = 10

# Get joints positions in world coordinates
joints3d = mat['joints3D'][:, :, frame_id].T # shape (24, 3)
joints3d = joints3d.reshape([
					joints3d.shape[0],
					joints3d.shape[1],
					1]
			) # shape (24, 3, 1)

# Convert 3D joint positions from world to camera coords
joints3d_cam = world_to_camera(extrinsic, joints3d)

# Reshape to restore previous shape, and permute
joints3d_cam = joints3d_cam.reshape(joints3d_cam.shape[0], 
						joints3d_cam.shape[1]) # shape (24, 3)
joints3d_cam = np.moveaxis(joints3d_cam, 0, 1) # shape (3, 24)

# Swap Y and Z axes
joints3d_cam[[0,1,2]] = joints3d_cam[[0,2,1]]
```

When plotting the joints in 3D I get strange depth values (see Y axis in the figure below). For example, in the image below the subject appears very close to the camera, however it's position on the Y axis (computed with the above code) is about 6 meters, which seems quite unrealistic to me:
![Screenshot from 2019-04-08 11-12-46](https://user-images.githubusercontent.com/30569186/55719443-83d74800-59fe-11e9-9be9-e9fe2b68fc30.png)

Do you have any idea why this is happening? Thanks "
How to get 3D coordinates from depth maps,gulvarol/surreal,2019-02-02 04:47:04,14,,27,405947788,"Hi, thank you so much for this dataset! 
But I have a question. I want to transform depth maps to 3D point clouds, i.e. calculate the 3D coordinates according to depth maps.
I try to inverse the process in https://github.com/gulvarol/surreal/tree/master/datageneration/misc/3Dto2D
 but the result seems wrong, especially for the x coordinates. The image below shows some of my results, joint3D is calculated from depth maps and real3D is the ground truth.
![image](https://user-images.githubusercontent.com/47238203/52159972-dfc3de80-26e6-11e9-84a3-f9c6aeea94d8.png)

How can I get the right 3D coordinates from depth maps? 
Thanks a lot!"
Data generator doesn't work on GPU mode??,gulvarol/surreal,2018-06-08 23:54:11,4,,12,330832905,"Hi gulvarol,

Thank you for providing such a good tool, when I run main_part1.py, it seems too slow to render one frame on CPU mode, so I modify your code by adding:

    bpy.context.user_preferences.addons['cycles'].preferences.compute_device_type = ""CUDA""
    bpy.context.user_preferences.addons['cycles'].preferences.devices[0].use = True
    scene.cycles.device = 'GPU'

Now it works on GPU mode, but those images rendered on GPU mode lose body texture.  
![image](https://user-images.githubusercontent.com/24106976/41185346-3c67fc3c-6b3c-11e8-9c37-fe427b7d57e4.png)

Any ideas? Thank you very much:)

Sicong
"
Material segments vertices loaded from pickle file may contains gaps,gulvarol/surreal,2018-02-11 08:37:35,1,,7,296172607,"Thanks for making this available! I'm an amateur at both ML and Blender, but your project is fascinating, so I'm digging into it from top to bottom. I've begun generating synthetic data according to your scripts and instructions, and I noticed this:

![image](https://user-images.githubusercontent.com/4377719/36071352-81e62d86-0ed2-11e8-92eb-6f9ba0d12153.png)

This is the raw synthetic data, with `Material 1 (head)` vertices selected. The back is worse:

![image](https://user-images.githubusercontent.com/4377719/36071371-cf203204-0ed2-11e8-808b-695c15c0df03.png)

**Update**: From what I can tell, there are gaps in the faces between spine2 and head, and between hips and left/rightUpperLeg. This seems to be because the first sorted key in the list of parts doesn't actually have a segment created from the material, but whatever is left over among unassigned vertices is applied to the first material group. So although it *seems* that the head is being properly assigned, it actually is given anything that is left over. 

I'm not sure this actually makes a difference once training is done on the synthetic data, as it seems that you join together various discrete segments into larger segments. So perhaps the gaps disappear. However, when using your pre-trained models to segment images, I do recall occasionally seeing odd parts near the hips and upper body that were mis-segmented as the head, so I think this might be a real bug. I am running Blender 2.79, so if you're not seeing the same thing, perhaps that's the culprit.

Thanks again for sharing this!

```python
## My hack added to main_part_1.py - create_segmentation(). It seems like there ought to be a way to configure Blender to just assign these automatically. 

vsegm['rightUpLeg'].extend([4353, 4418])
vsegm['leftUpLeg'].extend([869, 932])
vsegm['spine2'].extend([709, 712, 734, 1236, 1535, 1840, 1847, 1899, 2903, 2938, 2940, 2949, 4195, 4200, 4222, 4719, 5006, 5301, 5308, 5360, 6362, 6397, 6399, 6408])
```

![image](https://user-images.githubusercontent.com/4377719/36071454-6513905c-0ed4-11e8-9922-d5c1fd14324c.png)
"
Are there stereo images？,SoonminHwang/rgbt-ped-detection,2022-05-12 03:36:13,0,,25,1233402032,"In your paper, this dataset also appears to contain left and right stereo RGB images and ground truth depth maps, but the downloaded data only contains aligned color-thermal pairs. Where can I download unaligned images and depth maps?"
No bounding box annotations with downloaded dataset,SoonminHwang/rgbt-ped-detection,2022-03-10 18:17:34,0,,24,1165563786,"Dear developers. After following the download instructions, the xml files contained in the annotation folders do not show the bounding box annotations, as you can see in my picture attached. How can these annotations be downloaded? Many thanks
![image](https://user-images.githubusercontent.com/83634753/157728733-b1b4fa0c-506a-484f-a71b-97b6d0683991.png)
"
数据集的目录树，test20.txt啥的格式都没给，这还怎么复现啊。。,SoonminHwang/rgbt-ped-detection,2021-12-17 09:22:04,0,,23,1083048682,
Do you provide the intrinsic and extrinsic of the camera?,SoonminHwang/rgbt-ped-detection,2021-12-12 10:54:59,0,,22,1077775397,
Is the set_counter logic valid in data/scripts/kaist_to_voc_format.py ?,SoonminHwang/rgbt-ped-detection,2021-08-03 01:16:02,0,,21,958627933,"Hi,
In data/scripts/kaist_to_voc_format.py, I see the following:
```
# all train images but 20% from test
if set_ == 'set06' and set_counter == 901:  # all = 1161, person only = 901
```
But, in the annotations **set06 folder**, I see 5562 text files that have 'person'. 20% of 5562 comes to 1112. But the above logic is checking the counter for 901. 

Can someone clarify? 
Thank you."
Camera intrinsic parameters,SoonminHwang/rgbt-ped-detection,2020-10-29 06:47:55,1,,20,732052584,"It seems that no camera intrinsic parameters are provided in this repo. Though I know this is a dataset for detection, I wonder if intrinsic parameters of the RGB camera are avaliable. Many thanks."
@MuhammadAsadJaved  I can send you the missing data set and share some results about the data set. I wonder if you have time to help me answer a few questions about experimenting on this data set,SoonminHwang/rgbt-ped-detection,2020-08-12 08:25:07,4,,19,677500331,"@MuhammadAsadJaved 
Thanks for Sharing! Unfortunately your page seems to be not working either. It has the same dead links from this https://sites.google.com/site/pedestrianbenchmark/home 
Do you download from this page recently? 
Is it convenient for you to share any subset from day and night? For example set01 and set04.

_Originally posted by @diciembre-noche in https://github.com/SoonminHwang/rgbt-ped-detection/issues/18#issuecomment-671988931_"
Explanation of the .txt annotations ,SoonminHwang/rgbt-ped-detection,2020-07-21 08:16:40,7,,18,662732769,"Thank you for this explanation. It's very helpful. I have a few more questions about the dataset.

1 - There are 11 sets in total with several subsets.
Some .txt annotation contain only "" % bbGt version=3"" this tag and no bounding box values. What does this mean? Should we remove these annotations?

2- Can you explain the values in the .txt file? for example
annotation set00/V000/I02165 contains these values. 

% bbGt version=3
person 427 243 27 66 0 0 0 0 0 0 0

T
![kaist1](https://user-images.githubusercontent.com/28862708/88030174-81561e80-cb6d-11ea-8289-374f3f4d4e06.png)
![kaist2](https://user-images.githubusercontent.com/28862708/88030178-81eeb500-cb6d-11ea-9f00-777ecf8fcab7.png)

he first one is class, next 4 are bounding box coordinates ,what about other values? Do we need these in the training ?

3- How we divide these annotation to 9 categories? Reasonable all, Reasonable day, Reasonable night, Near scale, Far scare, Medium scare, No occlusion, Partial occlusion, and Heavy occlusion? Anyone write any scripts to categories images and .txt files?
Please also see attached images for Question 1 and 2.

Thank you very much."
utils/run_KAIST_labeler.m not working,SoonminHwang/rgbt-ped-detection,2020-01-23 09:37:02,0,,16,554041564,"I tried to reuse the run_KAIST_labeler.m script to label my own dataset. But the file vbbLabeler2.m, which is needed for the labeler to work, is not included in the repository. After a quick google search I found that the file should be located in the folder ""piotr-toolbox-3.40/vbbLabeler2.m"". Maybe this was a mistake to not include the changes you made on the repository of https://github.com/pdollar/toolbox.

Could you please add those missing files?
"
mean and variance value of this dataset,SoonminHwang/rgbt-ped-detection,2018-12-10 01:51:29,3,,7,389099983,where can I get the mean and variance value of this dataset ? or suggested value? I need them to normalize images before training. 
Missing imageSets/train20.txt,SoonminHwang/rgbt-ped-detection,2018-12-02 00:23:16,3,,6,386512294,"Can't find these .txt files within the dataset.
That the textscan function error in line 50 imgNms =bbGt2('getSubsetFiles',{imgDir, imgDir, gtDir}, testSet);
I'm not sure if that problem happens when there's no train20.txt?"
"Wrong MD5 checksum for ""imageSets.zip""",SoonminHwang/rgbt-ped-detection,2018-09-17 14:20:25,0,,4,360892198,"Got this error message:
`[1/43] Download imageSets.zip
Check md5 checksum for imageSets.zip...
[Error] md5 checksum is not correct. (imageSets.zip)
`

Even if the imageSets.zip file can be unzipped without any problem
The downloaded file has the following md5 hash:
`
md5sum imageSets.zip
9c9324b676b90a1e2beab79f961688e0  imageSets.zip
`"
Where can I find the reasonable subset?,SoonminHwang/rgbt-ped-detection,2018-07-30 03:47:12,3,,2,345601431,"Hi, I just downloaded the dataset but I could not find the _reasonable_ subset mentioned in your paper. 
Is it one of the image lists in the _imageSets_ subdirectory? 
Also, could you tell me the relation of the benchmark mentioned in your paper with the lists in the _imageSets_ directory? Thank you very much!"
How to use this acfDemoKAIST?,SoonminHwang/rgbt-ped-detection,2018-05-03 08:06:58,2,,1,319822790,"You say we just need to download this Dataset and your package.But this file don't work.Maybe I should transfer vbb files into txt files firstly,or I should do some preparatory work?"
"Hi, I have a question that why the number of test videos in 'msvd_test_sequencelabel.h5' is 470?",ylqi/gl-rg,2022-10-16 03:28:37,9,,6,1410359871,"I remember the the standard split contains 1.2K training videos, 100 validation videos, and **_670_** test videos, so why you did not use the other 200 test videos? Thank u!"
"Hi, I run the test.sh and something unexpected happened.",ylqi/gl-rg,2022-10-12 03:41:45,1,,5,1405508331,"```
Traceback (most recent call last):
  File ""test.py"", line 130, in <module>
    assert opt.feat_dims == test_loader.get_feat_dims()
AssertionError
```
I did not make any change before this."
关于data文件,ylqi/gl-rg,2022-07-16 13:16:32,1,,4,1306805005,data文件缺少train相关的文件，请问可以提供完整train的数据吗？非常感谢！！
请求训练代码,ylqi/gl-rg,2022-07-13 09:12:59,1,,3,1303142479,您好，非常感谢您的工作！请问训练的主函数代码可以开源吗？train.py只是定义了一些方法而没有程序入口和主函数。
fatal error: google/protobuf/port_def.inc: No such file or directory,kalov/ShapePFCN,2020-06-08 12:44:17,0,,15,634579447,"Hi,

I have followed installation instructions. while running `make -j32` I am getting below error - 
```
.build_release/src/caffe/proto/caffe.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
compilation terminated.
Makefile:572: recipe for target '.build_release/src/caffe/util/blocking_queue.o' failed
make: *** [.build_release/src/caffe/util/blocking_queue.o] Error 1
In file included from ./include/caffe/util/cudnn.hpp:8:0,
                 from ./include/caffe/util/device_alternate.hpp:40,
                 from ./include/caffe/common.hpp:19,
                 from src/caffe/data_reader.cpp:6:
.build_release/src/caffe/proto/caffe.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
compilation terminated.
Makefile:572: recipe for target '.build_release/src/caffe/data_reader.o' failed
make: *** [.build_release/src/caffe/data_reader.o] Error 1
In file included from ./include/caffe/util/cudnn.hpp:8:0,
                 from ./include/caffe/util/device_alternate.hpp:40,
                 from ./include/caffe/common.hpp:19,
                 from src/caffe/util/math_functions.cpp:6:
.build_release/src/caffe/proto/caffe.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
compilation terminated.
Makefile:572: recipe for target '.build_release/src/caffe/util/math_functions.o' failed
make: *** [.build_release/src/caffe/util/math_functions.o] Error 1
In file included from ./include/caffe/util/cudnn.hpp:8:0,
                 from ./include/caffe/util/device_alternate.hpp:40,
                 from ./include/caffe/common.hpp:19,
                 from src/caffe/util/benchmark.cpp:3:
.build_release/src/caffe/proto/caffe.pb.h:10:40: fatal error: google/protobuf/port_def.inc: No such file or directory
compilation terminated.
Makefile:572: recipe for target '.build_release/src/caffe/util/benchmark.o' failed
make: *** [.build_release/src/caffe/util/benchmark.o] Error 1

```

I installed ptorobuf through below commands - 
```
sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler
sudo apt-get install --no-install-recommends libboost-all-dev
```

But still i am facing issue.  I found that we need to add `include` & `lib` dir in `Makefile.config` file but i don't find `include` & `lib` dir.

I ran 

```
protoc --version
```
got     `libprotoc 3.11.4`

**Could anyone please help/guide**"
how to skip the Surface-based CRF layer,kalov/ShapePFCN,2020-06-04 03:19:19,0,,13,630479444,"Hi, thanks for this perfect work. And i want to ask how to skip the Surface-based CRF layer if I only want to get the Per-label confidence maps (on surface)? Is there a command that can complete this task? Thanks very much"
Dependencies of complete package,kalov/ShapePFCN,2019-06-29 12:50:48,0,,12,462299721,"Hi,
I am relatively new to the field of shape segmentation and machine learning. Also, I am not a professional programmer since I am an engineering student. I am interested in finding an application case for such methods. However, I am not able to get the package to run. 
The main problem is that the most of the packages are Linux-related. I tried Ubuntu WSL to get it down but apparently, the WSL can not have a GPU relation. 

Has anyone achieved to get it to run on windows yet? Is it even possible?

Sorry for that newbie question:)"
Academic research on vulnerable c++ code snippet,kalov/ShapePFCN,2019-06-09 14:33:04,1,,11,453903631,"Dear Vangelis, 
We are a group of Academic researchers. We are analyzing vulnerable C++ code snippets migrated from StackOverflow to GitHub. Our research will be published in Academic publications and will not be used in any Industrial application. 
We noted a vulnerable code snippet in your repository that was most likely copied from Stack Overflow. The vulnerability exists in this source code [file](https://raw.githubusercontent.com/kalov/ShapePFCN/master/Thea/Code/Source/AlignedAllocator.hpp) of your repository.

Please verify our report here with regards to the above vulnerability to assist you.
**[Link to report](https://docs.google.com/forms/d/e/1FAIpQLScyzcHYnBBzXxGmAJV09-qyyOJws7aMGJFmvsJUFX6xvMGvOA/viewform?entry.793621839=2263)** with four questions for you related to the vulnerability (should not take more than 5 minutes to answer).

Here is a summary of the vulnerable code snippet:

Description:
===
This answer implements an custom allocator and hacks standard `vector` class. It's not a good practice to modify standard C++ headers ( or in any other programming language) since it'll result in compilation error in other platforms, undefined behaviours and bugs, problems in newer language compiler and ... .  
 
Mitigation:
===
```
#include <vector>
#include <boost/align/aligned_allocator.hpp>
template <typename T>
using aligned_vector = std::vector<T, boost::alignment::aligned_allocator<T, 16>>;
```
From this [answer](https://stackoverflow.com/a/45833474/3686236)
        
---
Please verify our report here with regards to the above vulnerability to assist you.
**[Link to report](https://docs.google.com/forms/d/e/1FAIpQLScyzcHYnBBzXxGmAJV09-qyyOJws7aMGJFmvsJUFX6xvMGvOA/viewform?entry.793621839=2263)** with four questions for you related to the vulnerability (should not take more than 5 minutes to answer).

Sincerely yours,
Morteza Verdi, Shiraz university, E-mail: m.verdi@shirazu.ac.ir
Jafar Akhondali, Shiraz university, E-mail: jafar.akhondali@yahoo.com
Ashkan Sami, Shiraz university, E-mail: ashkan.sami@gmail.com
Foutse Khomh, Polytechnique Montreal, E-mail: foutse.khomh@polymtl.ca, website: http://www.khomh.net/
Gias Uddin,  Polytechnique Montreal, E-mail: gias98@gmail.com, website: https://giasuddin.github.io
Alireza Karami motlagh, Shahid Chamran University, E-mail: alireza.karami.m@gmail.com
"
How to speed up the rendering?,kalov/ShapePFCN,2018-08-18 19:30:03,0,,9,351845347,"I am trying to perform the tests on the labeled shapenet core dataset from your project page. The first step of rendering takes lots of time and memory. Is there a way to speed up that step? I am also not certain about how pretraining-batch-splits, pretraining-batch-size, pretraining-num-epochs can effect the trining?"
evaluation,kalov/ShapePFCN,2018-08-12 03:21:28,0,,8,349780934,"hi, could you provide some hints how to compute the accuracy of psb and coseg dataset for the face area weighting?
"
Output Visualization,kalov/ShapePFCN,2018-07-31 00:55:47,0,,7,345994638,I am able to run the training and testing but I need some help in terms of visualisation of testing. 
opencv fault,kalov/ShapePFCN,2018-07-02 14:43:02,4,,6,337548209,"when i compile ShapePFCN, after the ""make -j32"" command, it returns the following error

/usr/bin/ld: warning: libopencv_core.so.3.4, needed by ./caffe-ours/build/lib/libcaffe.so, may conflict with libopencv_core.so.2.4
/usr/bin/ld: build_release/./MeshProcessor.o: undefined reference to symbol '_ZN2cv6String10deallocateEv'
//usr/local/lib/libopencv_core.so.3.4: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
Makefile:156: recipe for target 'build_release/mvfcn.bin' failed
make: *** [build_release/mvfcn.bin] Error 1

what should i do to solve it ?

thank you for your attention!"
Expected Training Time,kalov/ShapePFCN,2018-05-07 00:00:55,4,,5,320640859,"Hi Kalov, thanks for this wonderful work, and I just spent hours setting up all the dependencies to run your code, and currently in training mode for psbAirplane1.

It is currently in iteration 1700, LR=0.001, loss = 0.0786143, and been running for 10 hours on 8GB GPU.

I am pretty new to machine learning, and still trying to digest all these terms. Can I know how much iteration is expected for that sample?

Thanks."
ErrorinitializingOpenGL,kalov/ShapePFCN,2017-11-14 10:43:01,2,,3,273744751,"I have meet a problem when I run the code ''./build_release/mvfcn.bin --skip-testing --do-only-rendering --train-meshes-path  /home/qiuchao/Codes/labeled_meshes/PSB_COSEG_MESHES/cosegGoblets --pretraining-batch-splits 4''
![default](https://user-images.githubusercontent.com/15049009/32775850-977af480-c96b-11e7-976d-53837463a0ca.png)
"
The shape of running_mean,DmitryUlyanov/texture_nets,2021-10-14 08:44:12,0,,98,1026127668,"Why the shape of running_mean in Instancenorm is [C] not [N,C] ?
the code "" self.running_var:copy(self.bn.running_var:view(input:size(1),self.nOutput):mean(1))""
I don't the reason running_mean/var view to (N,C) and mean(1)? So it similar to Batchnorm?
I find the cuda-kernel of Instancenorm used batchnorm's kernel ,the shape of running_men in Bn is [C], Instance's also [ C]too?"
What is the real difference between this model and Johnson's model?,DmitryUlyanov/texture_nets,2019-03-19 08:54:32,0,,96,422609617,"Dear all,
Does anybody know the real difference of the model(net) written in the code between this repository and Johnson's fast-neural-style? I mean, in this repo, the default model parameter is 'johnson', If I use the 'johnson' mode, and since the Johnson's newest version has implement 'InstanceNormalization' yet, What is the real difference? and the output is different too. Anybody knows? I'll appreciate any reply.
this repo's output:
![image](https://user-images.githubusercontent.com/21032349/54592438-59314980-4a67-11e9-85a7-c6c467fbbf77.png)
Johnson's papers output:
![image](https://user-images.githubusercontent.com/21032349/54592487-7534eb00-4a67-11e9-8a26-3a4d23a23862.png)
"
Broken link in README,DmitryUlyanov/texture_nets,2019-03-09 12:06:28,0,,95,419064984,"Link to a demo in README is broken
https://riseml.com/DmitryUlyanov/texture_nets"
CUDNN is not installed in CUDA directory by default,DmitryUlyanov/texture_nets,2018-05-02 21:23:29,0,,91,319717292,"I'm new to torch and have a fresh install just to test out fast style transfer.

Installing cudnn from deb packages installs it in `/usr/lib/x86_64-linux-gnu/` as referenced [here](https://github.com/NVIDIA/nvidia-docker/issues/172) and in this comment [here](https://github.com/NVIDIA/nvidia-docker/issues/168#issuecomment-238924455).

I think support for that in this code is missing. I would add it myself but am quite new to torch and so adding an issue.
```
$ th train.lua -data ./datasets/ -style_image ./data/textures/862.png -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2
/home/user/torch/install/bin/luajit: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: /home/user/torch/install/share/lua/5.1/cudnn/ffi.lua:1603: 'libcudnn (R5) not found in library path.
Please install CuDNN from https://developer.nvidia.com/cuDNN
Then make sure files named as libcudnn.so.5 or libcudnn.5.dylib are placed in
your library load path (for example /usr/local/lib , or manually add a path to LD_LIBRARY_PATH)

Alternatively, set the path to libcudnn.so.5 or libcudnn.5.dylib
to the environment variable CUDNN_PATH and rerun torch.
For example: export CUDNN_PATH=""/usr/local/cuda/lib64/libcudnn.so.5""

stack traceback:
	[C]: in function 'error'
	...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	train.lua:5: in main chunk
	[C]: in function 'dofile'
	...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50

```
Solution being : 
`export CUDNN_PATH=""/usr/lib/x86_64-linux-gnu/libcudnn.so.5""`"
training parameter,DmitryUlyanov/texture_nets,2018-03-19 11:45:02,0,,89,306429094,"I found default transfer parameter is 
cmd:option('-content_weight', 1)
cmd:option('-style_weight', 1)
cmd:option('-tv_weight', 0, 'Total variation weight.')

seems not same with common option(Gatys set 1000:1), is it your pretrained option? and do you try transfer result with and without tv loss?"
Only plain blank color images rendering,DmitryUlyanov/texture_nets,2018-01-18 13:22:37,1,,88,289626570,"I have tried this now on many different configurations and different batch sizes and models. I am not managing to get any stylized images but plain color outputs (mostly black, sometimes green).

The loss seems to be going up most of the time as well in training time.  Any clue what could be wrong?

Is there a ""frozen"" configuration somewhere that I can access with torch + rocks instances that worked at the time?

Thanks!"
Blank image and low quality style models ,DmitryUlyanov/texture_nets,2017-10-30 22:59:26,1,,85,269781601,"Hi,
 i am trying to train models using Johnson model , bu what ever i try low images are appearing 
- i have tried to make the image width multiple of 32
- i have tried to decrease learning rate 


what is the best solution to get good results in my case

`th train.lua -data dataset -style_image ./AdobeStock_90768625.jpeg -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2 -gpu 0
`
 
![ballon](https://user-images.githubusercontent.com/25582838/32208084-d0dce88c-be08-11e7-9681-287e26fc42d9.jpg)

"
SpatialConvolution : assertion failed!,DmitryUlyanov/texture_nets,2017-10-20 12:48:41,0,,84,267164837,"I am using the following command to train the model 

**`th train.lua -data dataset -style_image ./AdobeStock_90768625.jpeg -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2 -cpu`**


but this error is raised :-
```
/home/src/torch/install/bin/luajit: ...erspace/src/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
...aperspace/src/torch/install/share/lua/5.1/cudnn/init.lua:171: assertion failed!
stack traceback:
	[C]: in function 'assert'
	...aperspace/src/torch/install/share/lua/5.1/cudnn/init.lua:171: in function 'toDescriptor'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:123: in function 'createIODescriptors'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:188: in function <...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:186>
	[C]: in function 'xpcall'
	...erspace/src/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	...rspace/src/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:212: in function 'opfunc'
	...aperspace/src/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:241: in main chunk
	[C]: in function 'dofile'
	.../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
	[C]: in function 'error'
	...erspace/src/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
	...rspace/src/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:212: in function 'opfunc'
	...aperspace/src/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:241: in main chunk
	[C]: in function 'dofile'
	.../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

```"
Printing pretrained torch model options ,DmitryUlyanov/texture_nets,2017-09-26 09:45:36,4,,82,260559740,"I am trying to print the torch model options of training, I have used the provided model **model.t7** 

I am using this code to print the option 
```
require 'torch'
require 'nn'

require 'InstanceNormalization'


--[[
Prints the options that were used to train a a feedforward model.
--]]


local cmd = torch.CmdLine()
cmd:option('-model', 'models/instance_norm/candy.t7')
local opt = cmd:parse(arg)

print('Loading model from ' .. opt.model)
local checkpoint = torch.load(opt.model)

for k, v in pairs(checkpoint.opt) do
  if type(v) == 'table' then
    v = table.concat(v, ',')
  end
  print(string.format('%s: %s', k, v))
end




```



but I am getting this error:
```
/src/torch/install/bin/luajit: print_options.lua:22: bad argument #1 to 'pairs' (table expected, got nil)
stack traceback:
	[C]: in function 'pairs'
	print_options.lua:22: in main chunk
	[C]: in function 'dofile'
	.../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

```"
Update Readme,DmitryUlyanov/texture_nets,2017-09-08 06:26:54,0,,81,256152938,"Hi, 
It would be great if you can update the readme. The current readme follows a stale branch."
Why no content image input for style transfer during training?,DmitryUlyanov/texture_nets,2017-09-07 20:45:47,0,,80,256065128,"@DmitryUlyanov  first, thanks for your great work which is really helpful to my research. I have a question after reading the paper. To apply your method to style transfer, do we need to input content image during training phase? If not, the content loss is generated for what? Based on my understanding, the content loss is used for preserving the spatial structure of the content image. But there is no content image option in train.lua."
"Help,getting error while training !",DmitryUlyanov/texture_nets,2017-08-02 03:52:36,1,,79,247265880,"I followed the basic command and try to  train the  dataset, an error occurred.
Following is the command:
`th train.lua -data  dataset  -style_image wave.jpg  -cpu`
Following is the output on the stdout:
`=> Generating list of images	
 | finding all validation images	
 | finding all training images	
 | saving list of images to /home/jiehang/texture_nets/gen/style.t7	
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 574671192
Successfully loaded data/pretrained/VGG_ILSVRC_19_layers.caffemodel
conv1_1: 64 3 3 3
conv1_2: 64 64 3 3
conv2_1: 128 64 3 3
conv2_2: 128 128 3 3
conv3_1: 256 128 3 3
conv3_2: 256 256 3 3
conv3_3: 256 256 3 3
conv3_4: 256 256 3 3
conv4_1: 512 256 3 3
conv4_2: 512 512 3 3
conv4_3: 512 512 3 3
conv4_4: 512 512 3 3
conv5_1: 512 512 3 3
conv5_2: 512 512 3 3
conv5_3: 512 512 3 3
conv5_4: 512 512 3 3
fc6: 1 1 25088 4096
fc7: 1 1 4096 4096
fc8: 1 1 4096 1000
Using TV loss with weight 	0	
Setting up texture layer  	2	:	relu1_1	
Setting up texture layer  	7	:	relu2_1	
Setting up texture layer  	12	:	relu3_1	
Setting up texture layer  	21	:	relu4_1	
Setting up content layer	23	:	relu4_2	
        Optimize        	
/home/jiehang/torch/install/bin/luajit: /home/jiehang/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
/home/jiehang/torch/install/share/lua/5.1/cudnn/init.lua:171: assertion failed!
stack traceback:
	[C]: in function 'assert'
	/home/jiehang/torch/install/share/lua/5.1/cudnn/init.lua:171: in function 'toDescriptor'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:123: in function 'createIODescriptors'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:188: in function <...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:186>
	[C]: in function 'xpcall'
	/home/jiehang/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	/home/jiehang/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:211: in function 'opfunc'
	/home/jiehang/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:240: in main chunk
	[C]: in function 'dofile'
	...hang/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
	[C]: in function 'error'
	/home/jiehang/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
	/home/jiehang/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:211: in function 'opfunc'
	/home/jiehang/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:240: in main chunk
	[C]: in function 'dofile'
	...hang/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50
`
"
Transfer model to apple core ml,DmitryUlyanov/texture_nets,2017-07-05 12:09:30,2,,76,240630188,"Is there any way I can transfer the .t7 model to apple machine learning model using the latest iOS 11 core ml? In that case, I can call the model in iPhone without renting a server"
Generate smooth sky from an original image with unsmooth sky,DmitryUlyanov/texture_nets,2017-06-25 05:18:25,6,,75,238360810,"I have an original image shown as follows. Due to the moon, the lower half of the sky is brighter than the upper half. The part of the sky around the moon is particularly bright.
 
![shanghai_moon](https://user-images.githubusercontent.com/23154573/27513794-ad1b4da0-5939-11e7-852d-65b0a5e99be8.jpg)

When I use the ""Times"" filter in the Vinci app, the output shows an uneven sky as follows, which reflects the unsmooth sky in the original image.
![shanghai_moon_times](https://user-images.githubusercontent.com/23154573/27513817-dcf448dc-593a-11e7-9e70-c70896ad1bab.JPG)

However, when I use a different filter, i.e., ""Harvest"" filter, the output shows an amazing smooth sky as follows.
![shanghai_moon_harvest](https://user-images.githubusercontent.com/23154573/27513821-0764858c-593b-11e7-8d70-3a0c719a70e9.JPG)

I also tried to use texture_nets to stylize the original image using different style images, the output always had the uneven sky like the one with the ""Times"" filter. I have tried to use different combinations of layers for styles and contents, changed the style sizes and image sizes, no luck so far.

Any one has idea how to generate a smooth sky like the one produced by the ""Harvest"" filter? Thanks. 



"
invalid device ordinal,DmitryUlyanov/texture_nets,2017-06-22 02:58:15,6,,74,237718808,"**I have installed all dependencies that this repository need,but something gose wrong when running the command bellow:**

th test.lua -input_image /data/artwork/content/huaban.jpeg -model_t7 data/checkpoints/model.t7 -gpu 0

THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-3022/cutorch/init.c line=734 error=10 : 
**_invalid device ordinal_**
/root/AI/torch/install/bin/luajit: test.lua:26: cuda runtime error (10) : invalid device ordinal at /tmp/luarocks_cutorch-scm-1-3022/cutorch/init.c:734
stack traceback:
	[C]: in function 'setDevice'
	test.lua:26: in main chunk
	[C]: in function 'dofile'
	...t/AI/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670
`
**bellow is my GPU info**

`+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P5000        Off  | 0000:03:00.0      On |                  Off |
| 26%   39C    P8     8W / 180W |    110MiB / 16264MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1387    G   /usr/lib/xorg/Xorg                             108MiB |
+-----------------------------------------------------------------------------+
`
I really have no idea where the problem is."
"Where is the code for implementing the term ""lamda"" in Equation (10) of the paper ""Improved Texture Networks""?",DmitryUlyanov/texture_nets,2017-05-25 20:40:32,5,,72,231453177,"I tried to find the code to reflect the term ""lamda"" in Equation (10), which is on Page 4 of the paper ""Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis"". But I couldn't find it.

Anyone can tell me in which .lua file Equation (10) is implemented? Thanks."
unknown Torch class <nn.SpatialReplicationPadding>,DmitryUlyanov/texture_nets,2017-04-19 18:57:15,1,,69,222836726,"I train on my laptop with Geforce 940m 8gb RAM 
My Settings
echo $PATH
/home/alex/torch-cl/install/bin:/usr/local/cuda-8.0/bin:/home/alex/torch/install/bin:/home/alex/torch-cl/install/bin:/home/alex/torch/install/bin:/home/alex/bin:/home/alex/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
echo $LD_LIBRARY_PATH
/home/alex/torch-cl/install/lib:/usr/local/cuda-8.0/lib64:/home/alex/torch/install/lib:/home/alex/torch-cl/install/lib:/home/alex/torch/install/lib:

My calling
test.lua -input_image karya.jpg -model_t7 model.t7
/home/alex/torch-cl/install/bin/luajit: /home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:334: unknown Torch class <nn.SpatialReplicationPadding>
stack traceback:
	[C]: in function 'error'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:334: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:360: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:360: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/nn/Module.lua:154: in function 'read'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:342: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:391: in function 'load'
	test.lua:17: in main chunk
	[C]: in function 'dofile'
	...x/torch-cl/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00405e90
"
Help to trim the dataset,DmitryUlyanov/texture_nets,2017-04-04 18:58:33,9,,68,219349547,"Please, help me. Describing my problem - i try to trim the train and value datasets, because my laptop cannot work with this huge sets (write,that i have little RAM (cpu calcukating) or have little memory on harddisk (GPU) ). I have ubuntu and try copy a part of datasets like this (sort by name and copy first 1000 items)
 find ./datasets/train2014  -maxdepth 1 -type f | sort |head -1000 |xargs cp -t ./datasets/train/dummy
find ./datasets/val2014  -maxdepth 1 -type f | sort | head -1000|xargs cp -t ./datasets/val/dummy
this work good, but when i try to teach network, i get this error 

Optimize        	
/home/alex/torch/install/bin/lua: ...alex/torch/install/share/lua/5.1/threads/threads.lua:183: [thread 1 callback] ./datasets/style.lua:53: Error reading: /home/alex/Desktop/texture_nets/datasets/train/dummy/COCO_train2014_000000286564.jpg
stack traceback:
	[C]: in function 'assert'
	./datasets/style.lua:53: in function '_loadImage'
	./datasets/style.lua:32: in function 'get'
	./dataloader.lua:92: in function <./dataloader.lua:84>
	(tail call): ?
	[C]: in function 'xpcall'
	...alex/torch/install/share/lua/5.1/threads/threads.lua:234: in function 'callback'
	...e/alex/torch/install/share/lua/5.1/threads/queue.lua:65: in function <...e/alex/torch/install/share/lua/5.1/threads/queue.lua:41>
	[C]: in function 'pcall'
	...e/alex/torch/install/share/lua/5.1/threads/queue.lua:40: in function 'dojob'
	[string ""  local Queue = require 'threads.queue'...""]:15: in main chunk
stack traceback:
	[C]: in function 'error'
	...alex/torch/install/share/lua/5.1/threads/threads.lua:183: in function 'dojob'
	./dataloader.lua:143: in function 'loop'
	./dataloader.lua:62: in function 'get'
	train.lua:137: in function 'opfunc'
	...home/alex/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:174: in main chunk
	[C]: in function 'dofile'
	.../torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: ?


why style.lua tryes to read COCO_train2014_000000286564.jpg (i dont have this file in train directory, last image in train - COCO_train2014_000000007510.jpg in train , and in value - COCO_val2014_000000014226.jpg)

How to trim train and value directory CORRECT ?"
Encounter error when trying to use ResNet-50 to replace VGG-19 as the loss network,DmitryUlyanov/texture_nets,2017-03-11 05:20:40,2,,67,213506754,"I tried to replace the VGG-19 with ReSNet-50 for the loss network used in the training.
I downloaded the prototxt and caffemodel files from https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777, which is given on https://github.com/KaimingHe/deep-residual-networks

Then I tried to launch the training as following:
 th train.lua -style_image style/face_bottle_o.png -style_size 512 -image_size 256 -style_layers res2b_relu,res3b_relu,res4b_relu,res5b_relu -content_layers res4b_relu -style_weight 20 -content_weight 1 -proto_file data/pretrained/ResNet-50-deploy.prototxt -model_file data/pretrained/ResNet-50-model.caffemodel -checkpoints_path checkpoint/ -checkpoints_name face_bottle_o.s512i256.sL2b3b4b5b.cL4b.sw20.cw1.ResNet50.4x -num_iterations 10000 -batch_size 4 -save_every 2000

I encountered the following errors.
torch.display not found. unable to plot
Using TV loss with weight       1e-06
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 26:26: Message type ""caffe.LayerParameter"" has no field named ""batch_norm_param"".
Successfully loaded data/pretrained/ResNet-50-model.caffemodel
warning: module 'bn_conv1 [type BatchNorm]' not found
warning: module 'scale_conv1 [type Scale]' not found
warning: module 'pool1_pool1_0_split [type Split]' not found
warning: module 'bn2a_branch1 [type BatchNorm]' not found
warning: module 'scale2a_branch1 [type Scale]' not found
warning: module 'bn2a_branch2a [type BatchNorm]' not found
warning: module 'scale2a_branch2a [type Scale]' not found
..........(similar warning messages)
warning: module 'bn5c_branch2b [type BatchNorm]' not found
warning: module 'scale5c_branch2b [type Scale]' not found
warning: module 'bn5c_branch2c [type BatchNorm]' not found
warning: module 'scale5c_branch2c [type Scale]' not found
warning: module 'res5c [type Eltwise]' not found
conv1: 64 3 7 7
res2a_branch1: 256 64 1 1
Segmentation fault (core dumped)

Any idea how to resolve this issue? Thanks.
"
Bad results – what parameters to fix?,DmitryUlyanov/texture_nets,2017-02-22 20:19:09,1,,66,209568769,"Hi, I know this has been asked but I've been getting odd results with my style transfer.

With the default parameters, I used this drawing:
http://i.imgur.com/TpbblRf.jpg

And got this result:
http://imgur.com/a/B8PcY

I'd really like something more styled according to the original drawing, but I'm not sure what parameters to use? I can't find an explanation of what the parameters do. I'm going to try increasing style-size from 600 to 960 – will this help? Both the style image and the images I'm styling are very large.

Thank you."
Assign different weights to different style layers,DmitryUlyanov/texture_nets,2017-01-19 22:03:21,4,,65,201988681,"@DmitryUlyanov Currently a single weight is assigned to all style layers during the training process. Is it possible to assign different weights to different style layers?

For example, if ""style_layers"" = ""relu1_2,relu2_2,relu3_2,relu4_2"", is it possible to use ""style_weight"" = ""2,4,5,3"" so that the weights of the four layers (relu1_2,relu2_2,relu3_2,relu4_2) are 2, 4, 5, 3, respectively, in training? Thanks."
GPU utilization is 0% when the image_size=768 in training,DmitryUlyanov/texture_nets,2016-12-25 19:49:41,0,,63,197513202,"When using MSCOCO dataset for training, I tried to set the image_size > 600, say 768. Then I found that the GPU utilization stayed at 0%. Following was the output of the command nvidia-smi:

Sun Dec 25 13:45:10 2016       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.55                 Driver Version: 367.55                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40c          On   | 0000:01:00.0     Off |                    0 |
| 29%   60C    P0    62W / 235W |    968MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40c          On   | 0000:02:00.0     Off |                    0 |
| 23%   39C    P8    23W / 235W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

The command was as follows.

nohup th train.lua -style_image style/transverse.jpg -style_size 768 -image_size 768 -checkpoints_path checkpoint/ -checkpoints_name transverse.s768i768.sw10.1x -style_weight 10 -num_iterations 40000 -batch_size 1 > train_0.out &

I was using the johnson model. When the image_size <=600, there was no issue."
"what different between model value,""pyramid"" and ""johnson""?",DmitryUlyanov/texture_nets,2016-10-21 08:03:03,1,,59,184422947,"The model set to"" pyramid"" by default,when change it to johnson,get different results.
what different between them?
"
Great result from new parameters,DmitryUlyanov/texture_nets,2016-09-28 15:28:38,0,,58,179806437,"This is not a issue, just want to say that the new parameter in README generate really good result. Compare to previous default parameter, I used to get image with a lot of broken glasses image, or a lot of fragments. Now it's smooth, even with 1 batch. Any idea why you change style_layers from ""relu1_1,relu2_1,relu3_1,relu4_1"" to ""relu1_2,relu2_2,relu3_2,relu4_2"" ?  Thanks for the improvement.
"
Why not calculating TVloss during forward pass?,DmitryUlyanov/texture_nets,2016-09-26 05:30:40,1,,57,179142585,"Hi. Comparing TVLoss:updateOutput(input) with TextureLoss:updateOutput(input),  I found that the code didn't calculate TVLoss during forward pass but only calculate the gradient of TVLoss during backward pass. Any idea why?
"
Faster on 8*4GHz / 32G Mem than 32*2GHz / 64G Mem ?,DmitryUlyanov/texture_nets,2016-09-19 12:41:14,10,,54,177773974,"I've tested these on a computer with AMD FX(tm)-8350 Eight-Core Processor, with 32G RAM installed, with test.lua, I can get a stylized.jpg in 6 seconds. After reading this post https://github.com/DmitryUlyanov/texture_nets/issues/41
I think it's possible to generate high resolution images if I have larger RAM.
I got a VM running on XenServer, with 32 CPU core(Intel(R) Xeon(R) CPU E7- 4820  @ 2.00GHz) and 64G RAM, but when I ran test.lua with regular params, it was extreamly slow than the former computer.
It costs over 15 minutes to generate one single image. What's wrong with that?
I noticed the script runs on only single one CPU core, is that normal?

This is what I used:
th test.lua -input_image images/forbidden_city.jpg -model_t7 data/checkpoints/model.t7 -cpu

Any body have experience on this?

Thanks.
"
Resolution Limitation,DmitryUlyanov/texture_nets,2016-09-07 21:05:36,2,,49,175606384,"I see that the pretrained model you provided runs at higher resolutions compared to the models I trained for a few style images. What might be the reason for this?Did you use VGG19 or normalised vgg? 
"
Unable to get good results,DmitryUlyanov/texture_nets,2016-08-31 18:02:31,3,,47,174338896,"Ive been trying to train the network for the Van Gogh starry night style by modifying the style_weight and tv_weight (0.001). I am not able to get good results. 
![stylized_v2](https://cloud.githubusercontent.com/assets/14928283/18140421/517407b2-6f6a-11e6-83be-c895ac3f7aeb.jpg)

Any suggestions on how to improve this? Has anyone tried using Van Gogh starry night? How can i reduce the blocky appearance?
"
Blank Images using Pyramid model,DmitryUlyanov/texture_nets,2016-08-29 20:33:40,3,,45,173865723,"I am attempting to train a model using the default pyramid model with this command (also tried using all defaults):

```
th train.lua -style_weight 5   -tv_weight 0.000085  -style_size 512 -model pyramid  -image_size 512 -num_iterations 50000  -batch_size 1 -data ../coco2014 -style_image ../candy_512.jpg  
```

When I try to generate an image with it, I am getting black output, even though the input image's dimension is a multiple of 32:

```
th test.lua  -image_size 1024  -input_image ../Input_square1568.jpg  -model_t7 pyramid/model_20000.t7  -save_path stylized.jpg
```

The issue occurs if I don't use the image_size flag as well. The weird thing is, it seemed to work a couple times, but now all I get is black output. Using johnson's model does not create the issue, but skip_unpool model also creates blank output.

I have tried creating smaller images to 256, 512 etc. but they are all blank.

I am just trying to figure out how to train the model to achieve results similar to the default model. As others have posted, it doesn't seem possible with the current code. It seems like others have experienced the black output as well. Please advise! 
"
train model seems not OK?,DmitryUlyanov/texture_nets,2016-08-29 13:42:01,3,,44,173777288,"I use 

th train.lua -data <path to any image dataset>  -style_image path/to/img.jpg

it gen 
model_1000.t7
model_2000.t7
model_3000.t7
......
model_49000.t7
model_50000.t7

but train model_50000.t7 to gen new picture seems not ok, is any wrong from my operate?

th test.lua -input_image tubingen.jpg -model_t7 data/checkpoints/rio.jpg/model_50000.t7

[rio.zip](https://github.com/DmitryUlyanov/texture_nets/files/442499/rio.zip)

thanks
"
train models without GPU,DmitryUlyanov/texture_nets,2016-08-25 10:46:02,5,,43,173171037,"is it possible to train models without GPU?
always get error:  module 'cutorch' not found
"
Is it possible to scale style when generating image after training(using test.lua),DmitryUlyanov/texture_nets,2016-08-24 03:29:54,25,,41,172856812,"Hi, i find i can't tune the style scale in result using test.lua, also i can't find any option about it or is it possible in theory to scale style after the training procedure?

recently i trained a style, and created the following results on a photo with size 668 x 448 and with its original size 4272 × 2848:

![700px](https://cloud.githubusercontent.com/assets/465136/17917134/1d0608c8-69ec-11e6-8a2d-cbf4fa6c5d2f.jpg)
668 x 448

![12m](https://cloud.githubusercontent.com/assets/465136/17917141/26c5cee8-69ec-11e6-9863-faedab7a85f3.jpg)
4272 x 2848

i can understand both of the results are right, and the style is applied correctly. but if i can tune the style scale on large size image as artist using large brush, i think it will be better to user to see consistency between thumbnail(he get this quickly) and the final size result(he get this after a long time). 

sorry i don't know if this is mentioned on paper or there is any progress in recently research?
"
"Run Test.lua and Train.lua, lots of errors",DmitryUlyanov/texture_nets,2016-08-23 06:07:52,17,,40,172619446,"![trainlua](https://cloud.githubusercontent.com/assets/20099577/17881827/f0f1724c-693a-11e6-974e-13367e8876ec.png)
![testlua](https://cloud.githubusercontent.com/assets/20099577/17881830/f882134a-693a-11e6-948b-67e154ca1369.png)
"
More parameter about the trainning model,DmitryUlyanov/texture_nets,2016-08-19 22:07:45,3,,39,172229736,"Hi,
 I have used the model by your download link, the result look nice. But can you provide more input how you train this model, for example the content and style weight and also is it possible to output original ratio size photo rather than complete square output? Also if you can provide more models for test would be appreciated! Anyway, awesome job on the feed forward method.

Thanks
"
is it possible to resume training progress from checkpoint obtained before(snapshot),DmitryUlyanov/texture_nets,2016-08-18 05:49:52,2,,37,171822951,"Hi guys, i meet this certain situation, i confirm an amazing effect on some checkpoint but the training progress is stopped before. so i think if i can continue the training instead of discarding those intermediate result(for example have been trained 8K+ iters) and starting an new one(this will consume more unnecessary time), this will be very helpful to save our time.

or if i missed something in train.lua? 

thanks in advance. 
"
Parameter used in Figure 3?,DmitryUlyanov/texture_nets,2016-08-09 13:03:38,4,,30,170160377,"Figure 3 in ""Texture Networks: Feed-forward Synthesis of Texture and Stylized Images"" is fantastic. For style of Picasso's painting (first row of figure 3), I trained the model with default parameter. But the result with style of Picasso's painting is bad. 
Can you share the parameter setting for style of Picasso's painting ,or the trained model? 
Thank you very much!
"
All brushes same size,DmitryUlyanov/texture_nets,2016-08-08 14:57:59,60,,28,169944041,"Hi! It looks like there is a problem where all the ""brush strokes"" in a style are all the same size. You can really see it here: 
![karya-oil_07-stylweight3-stylesize512-learning0-001-model 40000](https://cloud.githubusercontent.com/assets/10135842/17484147/b0449174-5d56-11e6-9ed1-1b3039c60714.jpg)
This was the style image I used:
![oil_07](https://cloud.githubusercontent.com/assets/10135842/17484178/c5f2c4a0-5d56-11e6-843b-598b65897a97.jpg)

40,000 iterations of the microsoft coco dataset. styleweight of 3, stylesize of 512, image size of 512, learning set to .001, batch of 1. I got the same results with different style weights and learning sizes. Unfortunately this looks like a big limitation :(
"
What parameters used in paper's fig.4? ,DmitryUlyanov/texture_nets,2016-08-05 10:31:08,2,,25,169578776,"The result in paper's fig.4 was excellent. But I trained the model by the code with the default parameter, the result was not so good. 
"
Bad argument #1 to 'copy' (sizes do not match) at model:forward(input:cuda()) ,DmitryUlyanov/texture_nets,2016-08-05 08:21:13,10,,24,169554187,"Hi I have this error at line 33 of test.lua : 

```
torch/install/share/lua/5.1/nn/Concat.lua:25: bad argument #1 to 'copy' (sizes do not match at /tmp/luarocks_cutorch-scm-1-9130/cutorch/lib/THC/THCTensorCopy.cu:30)
stack traceback:
    [C]: in function 'copy'

```

at the line 33 of test.lua when doing `model:forward(input:cuda())`
It's doing that with my own model I have pretrained with train.lua, it's not doing that with the pretrained model downloaded from the readme...

How do I use my own pretrained styles ?

Thanks in advance :) .
"
Training the model failed,DmitryUlyanov/texture_nets,2016-08-03 11:39:55,5,,22,169111355,"Hi,
I tried to train my model based on the imagenet validation set which contains 50k images. At the beginning, i.e. the iteration step is smaller than some small numbers like 8000, I can get reasonable test result using the trained model. But as the training going on, I get a black image, where is output of the pixel value is NaN!  All the parameters used for training are unchanged, and the training image is the "" cezanne.jpg"" which is included in the branch of texture_nets_v1. 
Shoud I change the learning late? Could you please give me some advice about this problem? 
Thanks!
"
How to Set Up Training Data,DmitryUlyanov/texture_nets,2016-08-03 07:35:16,5,,19,169067012,"I'm trying to train a style image using the MS COCO dataset but am having trouble setting up the directories in the proper way. I tried to follow your instructions in the README and have the following structure:

```
image-training/
image-training/train
image-training/train/dummy (images are in this folder)
image-training/val
image-training/val/dummy (images are in this folder)
```

When I run this command `th train.lua -data image-training/  -style_image data/textures/cezanne.jpg`, I get the following error:

`/root/torch/install/bin/luajit: /root/texture_nets/datasets/style-gen.lua:67: class not found: val`

Do you know if the structure of my `image-training` folder is incorrect?
"
Stylization train params,DmitryUlyanov/texture_nets,2016-07-14 07:51:34,1,,12,165499091,"Exemple:
th stylization_train.lua -style_image data/textures/cezanne.jpg -train_hdf5 <path/to/generated/hdf5> -noise_depth 3 -model_name pyramid -normalize_gradients true -train_images_path <path/to/ILSVRC2012> -content_weight 0.8

Can you please explain every param:
-style_image - path to style image (.jpg, .png(??))
-train_hdf5 - path to generated hdf5 file, generated by this command (th scripts/extract4_2.lua -images_path <path/ILSVRC2012>) ???
-train_images_path - ???
"
ValueError: The passed save_path is not a valid checkpoint: NCCmodel/model/NCCResnetModel,kyrs/NCC-experiments,2019-12-18 11:52:14,3,,4,539634466,"Could you please help me out with this?

I tried making a new directory ""NCCmode"". After I run the script, two new directories are created as per the script, model and summary which has summaryWriter. The summary writer has tf events files but the model directory remains empty."
Update Readme,kyrs/NCC-experiments,2019-12-17 15:50:06,2,,3,539151784,Could you please update the readme so that I can reuse your code
About the code schedule,kyrs/NCC-experiments,2019-08-14 07:07:46,1,,1,480516074,"Hi, kyrs. It's a very wonderful code and it really help me.
However I want to ask that what part of NCC have you completed?
THanks a lot~"
The BFM_exp_idx.mat file,nabeel3133/3D-texture-fitting,2021-02-26 03:23:02,0,,4,817004677,"Hi, Nabeel!Thank you so much for your great work!When I run it, and found that the BFM_exp_idx.mat is required, I cannot download from the Basel Face Model 2009 web.Can you share it, or send it to me(yygbruce@gmail.com)?
Thank you very much!"
Paper results,dnddnjs/pytorch-cifar10,2019-05-29 23:44:49,0,,14,450066592,"Did you manage to obtain the same results as reported in the papers you referenced?
E.g., 97.69% accuracy for ShakeDrop and such?
"
"When shakedrop run in eval model, shake shake take over?",dnddnjs/pytorch-cifar10,2019-02-22 12:00:10,0,,13,413373459,"In 73th line of shakedrop/model.py:
`  out = self.shake_shake(out, alpha, drop_factor)`
I don't now why u use the shake_shake here?"
dead readme links,dnddnjs/pytorch-cifar10,2019-01-17 00:46:17,0,,12,400062911,I think some of the paper/post links in the readme might be dead
questions about shakedrop,dnddnjs/pytorch-cifar10,2018-12-26 02:14:31,1,,11,394065333,"In the original paper, shakedrop is given as **G(x) = x + (bl + a + bl*a)F(x)**, but your application makes **out = (drop_factor + alpha + alpha * (1 - drop_factor)) * input**. Is that a slip of a pen？"
image link,dnddnjs/pytorch-cifar10,2018-10-11 01:29:39,5,,1,368916003,"![cifar1](https://user-images.githubusercontent.com/16641054/46775076-8b17e480-cd40-11e8-9501-89c6fbca36bd.jpg)
"
help me please !!!,yjhong89/Domain-Adaptation,2019-11-26 15:29:10,0,,1,528799767,"Hello, I want to set the coefficient lamda to a variable of 0-1. Is there any good way to achieve this?"
Evaluate early prediction and taxonomy prediction?,StanfordVL/feedback-networks,2018-06-04 09:48:41,3,,5,328983982,"what parameter need to be passed to evaluate early prediction and  taxonomy prediction for fbnet,resnet,vgg."
"why code is throwing error for feedback depth 12,8,4?",StanfordVL/feedback-networks,2018-05-22 07:29:54,0,,4,325171743,"In the research paper, feedback Net performance is computed for depth 12,8 and 4 but this code is not running for that given depth."
rnn module deprecated in favour of https://github.com/torch/rnn,StanfordVL/feedback-networks,2017-11-02 08:09:31,0,,3,270565794,Do I still need to use the 'rnn' module from Element-Research or using the 'rnn' module provided by torch works fine?
Paper issue: don't understand the proxy tasks' function?,gustavla/self-supervision,2019-09-09 15:38:20,0,,8,491173319,"So colorization as a proxy task in self-supervised learning just means that we pretrained the vgg16 or ResNet network for image colorization, and then transfer the weights in the pretrained model to the downstream task?
What about features that we get?"
Missing `test` function in `evaluate/voc_segmentation.py`,gustavla/self-supervision,2017-10-31 21:28:11,0,,7,270124923,The `test()` function is missing. Also if you have code to finetune vgg-16 and alexnet models for semantic segmentation that will be very useful to make a fair comparison against your published results.
Converting colorization-pretrained model to RGB,gustavla/self-supervision,2017-10-11 16:20:30,1,,6,264650994,"Hi Gustav,

I downloaded the pretrained VGG-16 model from http://people.cs.uchicago.edu/~larsson/color-proxy/models/vgg16.caffemodel.h5 and tried to fine tune it for pascal classification using the script in `selfsup/evaluate/__main__.py` . Unfortunately, the pretrained model's `conv1_1` filters are meant for single channel grayscale inputs but the model defined by `voc_classification.py` is for colour images. This results in the following asserting failure 

```
(tensorflow3) aravindh@gnodeb1:~/projects/self-supervision$ CUDA_VISIBLE_DEVICES=0 python3 selfsup/evaluate/ voc2007-classification /users/aravindh/scratch/autocolorize/vgg16.caffemodel.h5 -n vgg16 --output /users/aravindh/scratch/self_supervision/gustavia/voc_vgg16/classification/ --limit 100 
Traceback (most recent call last):
  File ""/usr/lib64/python3.4/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.4/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""selfsup/evaluate/__main__.py"", line 26, in <module>
    main()
  File ""selfsup/evaluate/__main__.py"", line 23, in main
    time_limit=args.limit, iterations=args.iterations, network_type=args.network)
  File ""/users/aravindh/projects/self-supervision/selfsup/evaluate/voc_classification.py"", line 476, in train_and_test
    train(*args, **kwargs)
  File ""/users/aravindh/projects/self-supervision/selfsup/evaluate/voc_classification.py"", line 191, in train
    network_type=network_type)
  File ""/users/aravindh/projects/self-supervision/selfsup/evaluate/voc_classification.py"", line 91, in build_network
    use_dropout=True)
  File ""/users/aravindh/projects/self-supervision/selfsup/model/vgg16.py"", line 362, in build_network
    z = conv(z, 64, name='conv1_1')
  File ""/users/aravindh/projects/self-supervision/selfsup/model/vgg16.py"", line 303, in conv
    return vgg_conv(z, num(ch), **kwargs)
  File ""/users/aravindh/projects/self-supervision/selfsup/model/vgg16.py"", line 164, in vgg_conv
    assert W_shape is None or tuple(W_shape) == tuple(shape), ""Incorrect weights shape for {} (file: {}, spec: {})"".format(name, W_shape, shape)
AssertionError: Incorrect weights shape for conv1_1 (file: (3, 3, 1, 64), spec: [3, 3, 3, 64])
```

In order to replicate the results in column 1 of table 1 in your paper (http://arxiv.org/pdf/1703.04044.pdf), please let me know what should be changed.

Best wishes,
Aravindh Mahendran"
Converting the output of net to a colorized image,gustavla/self-supervision,2017-10-03 22:43:49,0,,5,262614116,"Hi Gustav,

I got the output from  the neural net but its a (1,32) vector of h and c values whereas the input image had dimensions (514,514). I compared it with the output of the default neural net in autocolorize  and it generates (1,32,512,512) output. Could you please see and provide any changes to the model needed to get a colorized output image?

Regards,
Abhay"
Tensorflow code not responding,gustavla/self-supervision,2017-09-10 16:06:46,0,,4,256520298,"Hi @gustavla,

I tried running the Tensorflow code that you have uploaded using the following instructions that you mentioned:

```
import deepdish as dd
import selfsup.model.vgg16

data = dd.io.load('vgg16.caffemodel.h5')
x = tf.placeholder(tf.float32, shape=[1, 224, 224, 3], name='x')
phase_test = tf.placeholder(tf.bool, name='phase_test')
z = selfsup.model.vgg16.build_network(x, parameters=data, phase_test=phase_test)
```

The code gets stuck somewhere. I did a preliminary debug and it seems that  it occurs on calls to `vgg_conv()` in line 302 of vgg16.py. I ran it on a titanx gpu for 4 hours and it was not able to return from `build_network()`. Can you please see if you can fix it?

Regards,
Abhay"
Looking forward for the code,gustavla/self-supervision,2017-05-10 07:30:16,0,,1,227590165,The paper inspires me a lot. Great work! I am wondering when the code will be released. Looking forward to it.  Thanks a lot.
About torch version,mys007/ecc,2022-06-27 09:59:19,0,,9,1285555874,"Torch version 0.3 is too old. If I use version 1.0 or above, what changes should I make？
Thanks for your code😄"
Frequent Error,mys007/ecc,2019-09-11 13:55:16,1,,7,492254159,"Config:

python 3.6.4
torch 1.2.0

This error is frequently triggered stopping the training.

Traceback (most recent call last):
  File ""learning/main.py"", line 607, in <module>
    main()
  File ""learning/main.py"", line 455, in main
    train_metrics, _ = train()
  File ""learning/main.py"", line 296, in train
    outputs = model.ecc(embeddings[0], clouds_data[4:6])
  File ""/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/graphnet.py"", line 145, in forward
    input = module(input)
  File ""/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/modules.py"", line 88, in forward
    input = ecc.GraphConvFunction(nc, nc, idxn, idxe, degs, degs_gpu, self._edge_mem_limit)(hx, weights)
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/ecc/GraphConvModule.py"", line 67, in forward
    cuda_kernels.conv_aggregate_fw(output.narrow(0,startd,numd), products.view(-1,self._out_channels), self._degs_gpu.narrow(0,startd,numd))
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/ecc/cuda_kernels.py"", line 123, in conv_aggregate_fw
    csdegs = torch.cumsum(degs,0)
RuntimeError: scan failed to synchronize: an illegal memory access was encountered

Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 193, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 82, in cupy.cuda.driver.check_status
TypeError: 'NoneType' object is not callable
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 193, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 82, in cupy.cuda.driver.check_status
TypeError: 'NoneT"
cuda_kernels help,mys007/ecc,2019-08-06 19:06:51,5,,6,477546261,"Hey,

I am working deeply on your code.
I would like to ask you a favor, and if you could please help me to understand the cuda kernels.
My email adress is thomasc@helix.re

I have benchmark your code vs pytorch geometric own implementation and I would say yours is approximately 3 times faster to train.

Best,"
MNIST Classification,mys007/ecc,2019-07-11 15:49:26,6,,5,466973912,"I am trying to replicate the MNIST classification experiment through ECC by following the model configuration reported in your paper. There are some issues that I was not able to solve:
1 - the described network configuration (  C(16)-MP(2,3.4)-C(32)-MP(4,6.8)-C(64)-MP(8,30)-C(128)-D(0.5)-FC(10) ) does not return the proper output dimensions. It seems there is some pooling layer(s) missing. Could you confirm that?
Maybe I am missing something in the following description (network configuration in Section 4.4), when a 4x4 to 1 points map is quoted, how should I get points down-sampling with convolutional layer in ECC?
2 -  Could you provide further details about data augmentation and other parameters settings for the same MNIST experiment? "
pytorch_scatter could maybe be faster for features aggregation,mys007/ecc,2019-06-12 14:37:54,3,,4,455252402,https://github.com/rusty1s/pytorch_scatter
Backpropagation of graph convolution,mys007/ecc,2019-03-14 16:23:31,0,,3,421119183,"Hi,
I would like to ask if it is possible to you to explain the backpropagation step of the graph convolution. I would like to know if you can give the equations and their correspondence to the code. I am trying to understand how is the derivative calculated, but I am not able to do it. 

Sorry if it is a silly question.."
Issues executing examples. CUDA_ERROR_ILLEGAL_ADDRESS and torch.bmm received an invalid combination of arguments,mys007/ecc,2018-10-01 06:28:04,36,,1,365324335,"Hi,

I have some issues executing your code.  First, I tried to execute your example with modelnet 10 using the command provided. It seemed to work but an advanced epoch the code crash with this error:


```
Traceback (most recent call last):
  File ""main.py"", line 315, in <module>
    main()
  File ""main.py"", line 217, in main
    acc_train, loss, t_loader, t_trainer = train(epoch)
  File ""main.py"", line 155, in train
    loss_meter.add(loss.data[0])
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/torch/lib/THC/generic/THCStorage.c:32

Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
=
```
I executed the code several times and the error appears randomly, it is not always in the same epoch, also it is not appearing in the same part of the code, here you can see an other example of the error:

```
File ""main.py"", line 315, in <module>
    main()
  File ""main.py"", line 217, in main
    acc_train, loss, t_loader, t_trainer = train(epoch)
  File ""main.py"", line 152, in train
    loss.backward()
  File ""/projects/env/ecc/lib/python3.6/site-packages/torch/autograd/variable.py"", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File ""/projects/env/ecc/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cublas runtime error : an internal operation failed at /pytorch/torch/lib/THC/THCBlas.cu:247

Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
```


I tried different versions of pytorch: 0.2 0.3 0.4. The three versions was installed using pip, and also I tried to execute the code with a compiled from source version (0.2) the same error appears. I am using a machine with: 60gb of ram, Intel Xeon and a titan X with 12gb of ram. Moreover I tried to use different versions of open3d: 0.2.0 and 0.3.0. Finally I modified your sample command and I add edge_mem_limit in order to limit the memory used on the gpu without success.

Also I tested the code using the Sydney Urban Objects example, but in this case, this error is appearing at the begging of the execution:

```
File ""main.py"", line 315, in <module>
    main()
  File ""main.py"", line 217, in main
    acc_train, loss, t_loader, t_trainer = train(epoch)
  File ""main.py"", line 148, in train
    outputs = model(inputs)
  File ""/project/env/ecc/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/project/code/ecc/models.py"", line 103, in forward
    input = module(input)
  File ""/project/env/ecc/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/project/code/ecc/ecc/GraphConvModule.py"", line 171, in forward
    return GraphConvFunction(self._in_channels, self._out_channels, idxn, idxe, degs, degs_gpu, self._edge_mem_limit)(input, weights)
  File ""/project/code/ecc/ecc/GraphConvModule.py"", line 63, in forward
    self._multiply(sel_input, sel_weights, products, lambda a: a.unsqueeze(1))
  File ""/project/code/ecc/ecc/GraphConvModule.py"", line 36, in _multiply
    torch.bmm(f_a(a) if f_a else a, f_b(b) if f_b else b, out=out)
TypeError: torch.bmm received an invalid combination of arguments - got (torch.DoubleTensor, torch.FloatTensor, out=torch.DoubleTensor), but expected (torch.DoubleTensor source, torch.DoubleTensor mat2, *, torch.DoubleTensor out)
```

Please can you give me some hint in order to solve the issues?

Thanks,"
Predict captions for existing bounding boxes,linjieyangsc/densecap,2019-03-27 17:16:53,1,,5,426086576,"Hi, 

we would like to use densecap to predict caption for already computed bounding boxes. 
I tried using the `im_detect()` function in `/lib/fast_rcnn/test.py` which has a `boxes` argument. 

I would expect the function to output the same number of boxes as i put in, which does not happen. Instead,  it looks like the network predicts new boxes in the RPN. 

I tried setting the `cfg.TEST.HAS_RPN` parameter to `False` in order to load the rois blobs in the `_get_blobs()` function -> The boxes are loaded into `blobs`, but this has no effect on the outcome. Are they used at all in this case?

Do i need to adjust the feature_net (vgg_region_global_feature.prototxt) in some way or set some other parameters in order for the network to work as expected? Or did I miss something else? 

Thanks 
"
Unable to run demo.py due to Caffe error,linjieyangsc/densecap,2019-03-14 06:58:01,2,,4,420859442,"Hey,

I've been trying to get the system to work for a couple of days now, but keep on running into trouble with Caffe.

When running the `demo.py` in lib/tools, I get the  following error message:

```
[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 426:18: Message type ""caffe.LayerParameter"" has no field named ""reshape_param"".

WARNING: Logging before InitGoogleLogging() is written to STDERR
F0314 06:48:26.013850 25088 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: models/dense_cap/vgg_region_global_feature.prototxt
```

Could you provide a link to the specific Caffe version / fork that you currently use for the project? I understand that you need Ross Girschick's fork for Fast R-CNN to enable ROI pooling. My system has CUDA 8.0 and CuDNN 7.1."
"Unable to find train.txt, val.txt, test.txt in corresponding folder",linjieyangsc/densecap,2019-01-19 06:19:21,1,,3,400966461,"I want to reproduce your code and want to run it on Visual Genome1.4 dataset. However, I cannot find the corresponding TXT file for train, val, and test when loading the dataset. Can you put these three files on GitHub?"
Where to get json files for training and testing,surajdakua/Crowd-Counting-Using-Pytorch,2022-04-01 17:11:51,0,,8,1190070922,"Dataset does not contain any json files, how to generate them "
predict error[transform is not defined],surajdakua/Crowd-Counting-Using-Pytorch,2021-04-16 02:40:26,0,,6,859404621,"hello!

I'm getting the following error when predicting.

> Traceback (most recent call last):
  File ""predict.py"", line 3, in <module>
    img = transform(Image.open('part_A/test_data/images/IMG_100.jpg').convert('RGB')).cuda()
NameError: name 'transform' is not defined

I just ran **python predict.py.**
How can I make a prediction?"
train error [different to the input size],surajdakua/Crowd-Counting-Using-Pytorch,2021-04-16 02:07:33,0,,5,859392740,"hello!

At the training stage, I get the following error.

> UserWarning: Using a target size (torch.Size([1, 1, 768, 1024])) that is different to the input size (torch.Size([1, 1, 96, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size
...
...
...
RuntimeError: The size of tensor a (128) must match the size of tensor b (1024) at non-singleton dimension 3

Do I need to resize using any python code before training?

And what is the **TASK** of the fourth argument when executing the training code?

thank you."
train_error!,surajdakua/Crowd-Counting-Using-Pytorch,2020-04-28 09:41:13,0,,4,608173634,"Using a target size (torch.Size([1, 1, 768, 1024])) that is different to the input size (torch.Size([1, 1, 96, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
Traceback (most recent call last):
  File ""train.py"", line 214, in <module>
    main()        
  File ""train.py"", line 84, in main
    train(train_list, model, criterion, optimizer, epoch)
  File ""train.py"", line 136, in train
    loss = criterion(output, target)
  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py"", line 493, in __call__
    result = self.forward(*input, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py"", line 443, in forward
    return F.mse_loss(input, target, reduction=self.reduction)
  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py"", line 2256, in mse_loss
    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
  File ""/usr/local/lib/python3.5/dist-packages/torch/functional.py"", line 62, in broadcast_tensors
    return torch._C._VariableFunctions.broadcast_tensors(tensors)
RuntimeError: The size of tensor a (128) must match the size of tensor b (1024) at non-singleton dimension 3
"
error train,surajdakua/Crowd-Counting-Using-Pytorch,2020-03-31 13:35:46,4,,3,591115642,"so i should use all the arg , if yes ? what's the id task and GPU ?
please when i try to train the model what's arg should i put ? and thank you mr SURAJDAKIA

"
error generate goundntruth,surajdakua/Crowd-Counting-Using-Pytorch,2020-03-29 17:34:27,1,,2,589842676,"(base) C:\Users\larguet\Desktop\Crowd-Counting-Using-Pytorch-master>python GenerateGroudnTruth.py
C:/Users/larguet/Desktop/Crowd-Counting-Using-Pytorch-master/shanghaitech_part_B/train_data/images\IMG_1.jpg
Traceback (most recent call last):
  File ""C:\Users\larguet\Anaconda3.1\lib\site-packages\scipy\io\matlab\mio.py"", line 39, in _open_file
    return open(file_like, mode), True
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/larguet/Desktop/Crowd-Counting-Using-Pytorch-master/shanghaitech_part_B/train_data/ground-truth\\GT_IMG_1.mat'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""GenerateGroudnTruth.py"", line 65, in <module>
    mat = io.loadmat(img_path.replace('.jpg','.mat').replace('images','ground-truth').replace('IMG_','GT_IMG_'))
  File ""C:\Users\larguet\Anaconda3.1\lib\site-packages\scipy\io\matlab\mio.py"", line 216, in loadmat
    with _open_file_context(file_name, appendmat) as f:
  File ""C:\Users\larguet\Anaconda3.1\lib\contextlib.py"", line 112, in __enter__
    return next(self.gen)
  File ""C:\Users\larguet\Anaconda3.1\lib\site-packages\scipy\io\matlab\mio.py"", line 19, in _open_file_context
    f, opened = _open_file(file_like, appendmat, mode)
  File ""C:\Users\larguet\Anaconda3.1\lib\site-packages\scipy\io\matlab\mio.py"", line 45, in _open_file
    return open(file_like, mode), True
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/larguet/Desktop/Crowd-Counting-Using-Pytorch-master/shanghaitech_part_B/train_data/ground-truth\\GT_IMG_1.mat'



help ? "
Shanghai Dataset hyperlink is not reachable,surajdakua/Crowd-Counting-Using-Pytorch,2020-03-13 15:49:03,3,,1,580688377,"Hello,

I would like to test this project but there is an http error 404 when I click on the link  provided (https://www.dropbox.com/s/fipgjqxl7uj8hd5/ShanghaiTech.zip?dl=0). Is this dataset is available elsewhere?"
The background of illumination transfer result is strange...,senguptaumd/SfSNet,2022-01-06 12:22:50,0,,8,1095270336,How can I get a illumination transfer result image with nice background ?? The background of result image without mask is so strange... thanks
how to render normal map?,senguptaumd/SfSNet,2021-02-28 12:34:32,0,,7,818196481,
"Missing explanation on how to train skip connection network to produce albedo,normal,light on real data",senguptaumd/SfSNet,2019-10-22 09:02:24,0,,6,510520539,"Hello,

Can you provide explanation on how to train the network to produce the real data's albedo, normal and light estimation?"
Model and Weights error result in Matlab crash,senguptaumd/SfSNet,2019-09-05 06:30:05,3,,5,489553347,"Hi, I have some problem when running a test in your code. Apparently I keep getting crash when I tried to run your code and I manage to track which part that gives the error which is when trying to load the model. So I tried to check if the problem persist when I tried to open other model, I tried to open other model and I was able to print out the Net. So can you help me fix this?

I have put below the part that has error on it.

> clc; clear;
> % %% TO DO: Add your Matcaffe path as $PATH_TO_CAFFE/matlab
> PATH_TO_CAFFE_MATLAB='/home/cgal/caffe/matlab/';
> addpath(genpath(PATH_TO_CAFFE_MATLAB));
> 
> addpath(genpath('functions'));
> 
>  model = '/home/cgal/SfSNet/SfSNet_deploy.prototxt';
>  weights='/home/cgal/SfSNet/SfSNet.caffemodel.h5';
> 
> %model = '/home/cgal/caffe/models/bvlc_reference_caffenet/deploy.prototxt';
> %weights = '/home/cgal/caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel';
> 
> GPU_ID=0; %Set your GPU ID
> caffe.set_mode_gpu();
> caffe.set_device(GPU_ID);
> net = caffe.Net(model, weights, 'test');"
How to generate 3d face mesh?,senguptaumd/SfSNet,2019-06-10 09:07:54,0,,4,454070535,"Can I use the CNN result to generate 3d face mesh ?  How ?
"
Wrong Doc,verlab/SemanticFastForward_CVPR_2018,2018-11-21 18:15:54,0,,2,383240778,Documentation on `LLC/accelerate_video_LLC.m` does not match with the function's arguments.
Dimensions of matrices being concatenated are not consistent,verlab/SemanticFastForward_CVPR_2018,2018-09-25 20:21:58,0,bug,1,363750577,"Error on video_features_frames file on function video_features_frames( videoFilename , ofFilename , semanticFilename , yoloDescFilename).

===============================================================
Error using vertcat
Dimensions of matrices being concatenated are not consistent.

Error in video_features_frames (line 53)
     videoFeatures = [movementFeatures; appearanceFeatures; semanticFeatures ];

Error in accelerate_video_LLC (line 140)
        [ videoFeaturesFile.descriptors, videoFeaturesFile.OF ] = video_features_frames(path.in.video, path.in.opticalFlowCSV, path.in.semanticExtracted,
        path.in.YoloDesc);

"
Question about evaluation,jiangzhongshi/SurfaceNetworks,2022-08-04 06:37:24,0,,9,1328142423,"Hello again.

So, i've launched ""as_rigid_as_possible"" experiment, trained a model, it saved in the pts folder. But what's next? How to use it to draw something?"
Issues running mesh_mnist/main.py,jiangzhongshi/SurfaceNetworks,2022-08-01 07:31:36,5,,8,1323962627,"Hello and thanks for your code.

Don't know if i'll get any answers, but anyway.

I'm trying to launch main.py (with default settings) in mesh_mnist folder (i've downloaded mesh mnist of course from the google drive links). Training crashes on the line:
`outputs = model(inputs, laplacian, mask)`

If we'll look deeper, it crashes here:
`xs = [x, torch.mm(L,x.view(-1, feat)).view(batch, node, feat)]` (167 line of src/utils/utils_pt.py file).

It says `The expanded size of the tensor (64) must match the existing size (503) at non-singleton dimension 0.  Target sizes: [64, 64].  Tensor sizes: [503, 64]`

It is torch.mm failing because L is (64,503,503) and x.view(-1, feat) is (32192, 64). By the way, as i know, torch.mm uses 2D tensors as input, no? Well, does anyone launched that and could advice something?"
Update pyigl to new igl python binding,jiangzhongshi/SurfaceNetworks,2021-01-13 14:48:40,0,,7,785168537,
Could you share the preprocessing code?,jiangzhongshi/SurfaceNetworks,2019-07-02 19:29:28,9,,4,463397993,"I would like to try running ModelNet40 classification task and hard to find preprocessing the data.
Could you share the code for the preprocessing?
Thank you. "
Issues running main.py in mesh_mnist and main.py in dense_correspondence,jiangzhongshi/SurfaceNetworks,2019-05-28 01:29:36,1,,3,449026378,"In line 104 of main.py in mesh_mnist:
Di.append(samples[ind]['Di'])
It has errors as KeyError. But I couldn't understand where is 'Di' in samples which are training dataset. 

In line 255~259 of main.py in dense_correspondence. It also has errors as log doesn't exist. I wonder how can I run the code without existing log files?"
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_NOT_INITIALIZED: initialization error,jiangzhongshi/SurfaceNetworks,2019-04-13 08:18:21,3,,2,432822720,"Thank you for the great code!
I have a problem. When i run the program on gpu,  the output is as follows:

`Load data
Preprocess Dataset
100% (60000 of 60000) |####################| Elapsed Time: 0:00:20 Time: 0:00:20
100% (10000 of 10000) |####################| Elapsed Time: 0:00:03 Time: 0:00:03
Num parameters 90314
N/A% (0 of 937) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--Traceback (most recent call last):
  File ""main.py"", line 213, in <module>
    main()
  File ""main.py"", line 155, in main
    outputs = model(inputs, laplacian, mask)
  File ""/home/jiang/work/ping/local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/jiang/work/ping/SurfaceNetworks/src/mesh_mnist/models.py"", line 43, in forward
    x = self._modules['rn{}'.format(i)](L, mask, x)
  File ""/home/jiang/work/ping/local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/jiang/work/ping/SurfaceNetworks/src/utils/utils_pt.py"", line 125, in forward
    xs = [x, SparseBMMFunc()(L, x)]
  File ""/home/jiang/work/ping/SurfaceNetworks/src/utils/cuda/sparse_bmm_func.py"", line 39, in forward
    col_ind, col_ptr = batch_csr(matrix1._indices(), matrix1.size())
  File ""/home/jiang/work/ping/SurfaceNetworks/src/utils/cuda/batch_csr.py"", line 39, in __call__
    m.load(bytes(ptx.encode()))
  File ""cupy/cuda/function.pyx"", line 175, in cupy.cuda.function.Module.load
  File ""cupy/cuda/function.pyx"", line 176, in cupy.cuda.function.Module.load
  File ""cupy/cuda/driver.pyx"", line 141, in cupy.cuda.driver.moduleLoadData
  File ""cupy/cuda/driver.pyx"", line 72, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_NOT_INITIALIZED: initialization error
100% (937 of 937) |########################| Elapsed Time: 0:00:00 Time: 0:00:00`

Is there any problem with my operation?

**System information**

- Python version: 2.7.15
- CUDA/cuDNN version: 10.0.130 / 7.5.0
- GPU model and memory: Nvidia GeForce GTX 980
- Nvidia driver version: 410.48
- Linux Ubuntu 18.04"
 Sampling kernel_size,jianzhangcs/ISTA-Net-PyTorch,2022-05-30 08:05:22,0,,6,1252352656,"The size of the convolution kernel in the sampling process is 33*33,
Could this size be modified?"
Sampling matrix,jianzhangcs/ISTA-Net-PyTorch,2022-01-02 08:39:27,0,,4,1091975555,How is the sampling matrix obtained? which algorithm is used? Thanks a lot!
关于对比算法,jianzhangcs/ISTA-Net-PyTorch,2021-11-02 09:04:43,0,,3,1042047170,您能分享对比算法中TVAL3 和D-AMP的代码么？谢谢
Test-CS-MRI,jianzhangcs/ISTA-Net-PyTorch,2021-04-15 09:38:23,2,,2,858696504,"
Hi, when I run the test ISTA-Net+  for MRI_CS I get the flowing result:
MRI CS Reconstruction Start
CS ratio is 50, Avg Initial  PSNR/SSIM for Brainimages_test is nan/nan
CS ratio is 50, Avg Proposed PSNR/SSIM for Brainimages_test is nan/nan, Epoch number of model is 200
MRI CS Reconstruction End
/home/amax/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/amax/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in true_divide
  ret = ret.dtype.type(ret / rcount)


Any suggestion thank you in advance.

"
Inter-human score,richardaecn/cvpr18-caption-eval,2020-12-16 09:18:58,2,,4,768620379,"I can not find the code to calculate the Inter-human score. I use ""kendalltau"" provided by ""scipy.stats"" to get the mean value(0.739) of AB and AC which is not matched with yours. (ABC is assumed as three scorers in Flickr8k)
However, if I keep 2 decimal places, the result is 0.73 which matched with the metric ""SPICE"".
Wish for your reply~"
./download.sh has one invalid link,richardaecn/cvpr18-caption-eval,2020-11-18 08:34:35,0,,3,745450624,"The ./download.sh doesn't download the required data from the given aws link.
Is there any way to get those relevant data files."
"HTTP request sent, 403 forbidden",richardaecn/cvpr18-caption-eval,2019-12-01 09:18:34,2,,2,530699738,"Hello,
Thank you for sharing your code. It is very helpful. However, in preparation part, I encounter a problem during downloading cvpr18-caption-eval.zip. It returns ""HTTP request sent, awaiting response 403 forbidden"" error. It is possible that you could give a hint to solve this problem or is there anywhere else to download this zip file ? "
How to train your model or where to download a trained model?,richardaecn/cvpr18-caption-eval,2019-05-30 10:09:11,0,,1,450230824,"Since it is a learnable model, so how to train it or to run the code where should I download the trained model?"
Providing checkpoint for calculating domain similarities,richardaecn/cvpr18-inaturalist-transfer,2022-09-12 09:14:12,0,,9,1369542565,"Hi, is it possible to provide the JFT-pretrained ResNet-101 checkpoint so we could calculate the domain similarity for other custom datasets?"
How to get the subset A/B in Table 6?,richardaecn/cvpr18-inaturalist-transfer,2019-10-10 12:42:22,0,,8,505245127,How to get the subset A/B in Table 6?  Can you share the subset A and subset B ?
Can't get expected nabirds dataset,richardaecn/cvpr18-inaturalist-transfer,2019-09-11 18:52:46,0,,7,492408279,"NABIRDS datasets are showing very bad result in terms of accuracy (31.75%). Any suggestion, why? "
exploitation of ckpt file,richardaecn/cvpr18-inaturalist-transfer,2019-09-01 23:09:01,0,,6,487941050,"Hi,
Thanks for your work. Do you have the tf.SavedModel version of the pretrained models you show ?
If not, is there a way to transform a ckpt file in a tf.SavedModel easily ? I would like to use your model in keras at the very end if possible.

Amaury"
code question,richardaecn/cvpr18-inaturalist-transfer,2019-03-26 04:56:55,0,,4,425227266,Is there a code that can reproduce the results in the author's paper?
Query image in LinearClassifierDemo.ipynb,richardaecn/cvpr18-inaturalist-transfer,2019-03-20 13:43:27,1,,3,423258179,"Hi, I Have a couple of questions:

1. In LinearClassifierDemo.ipynb, you gave the query image and it extracts similar images from training data. What is the criteria that extract those five images? Like is there any feature matching criteria?

2. The features we got by using pretrained models, these pretrained models are available publicly or you trained these by yourself and which pretrained model train on which dataset? "
data (google drive) cannot open,richardaecn/cvpr18-inaturalist-transfer,2019-03-20 03:42:49,1,,2,423052309,https://drive.google.com/file/d/1vOHKuqt7XgROo9t5cblJvGf0kGpiFaF1/edit cannot open ?is it right?
When would you release the pre-trained model in Tensorflow Hub?,richardaecn/cvpr18-inaturalist-transfer,2018-09-18 09:06:04,4,,1,361205911,I have been stay tuned for months.
test set,charliememory/Disentangled-Person-Image-Generation,2020-11-19 07:00:59,1,,14,746306452,"Hi. Impressive work product.
I want to test on my own deep fashion data. But when I run  'run_convert_DF.sh' ,it says I don't have the file named 'all_peaks_dic_DeepFashion.p'.Can you tell me how can I make this file? Thanks! "
Modify the log_dir and log_dir_pretrain,charliememory/Disentangled-Person-Image-Generation,2020-11-01 14:24:33,1,,13,733979047,What file should to put in the path of log_dir and  log_dir_pretrain
error test,charliememory/Disentangled-Person-Image-Generation,2020-10-31 18:16:24,1,,12,733778707,"hello,
when I run the test.py I have this error : (in tflib/inception_score.py)
ValueError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [2048], [2048,1008].
can you help me please?"
I am not sure you used the generated pose in tester.py,charliememory/Disentangled-Person-Image-Generation,2018-07-29 01:01:53,0,,7,345490973,"I am not sure you are using the generated pose in your test phase.
In tester.py #497, I think the self.pose_embs should be replaced by self.G_pose_embs, or the pose will be sampled directly from the dataset."
"OutOfRangeError (see above for traceback): FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 16, current size 0)",charliememory/Disentangled-Person-Image-Generation,2018-07-13 12:34:01,0,,6,341002027,"Hi,

I get the following error when attempting to train the Fg Bg reconstruction model, using tensorflow-gpu1.9, python2.7. Thank you for help.

```shell
gpu=0
D_arch='DCGAN'
log_dir='./logs'

####################### Stage-I: reconstruction #####################
## Fg Bg reconstruction
model_dir=${log_dir}'/MODEL1_Encoder_GAN_BodyROI7'
python main.py --dataset=Market_train_data \
             --use_gpu=True --img_H=128  --img_W=64 \
             --batch_size=16 --max_step=120000 \
             --d_lr=0.00002  --g_lr=0.00002 \
             --lr_update_step=50000 \
             --model=1 \
             --D_arch=${D_arch} \
             --gpu=${gpu} \
             --z_num=64 \
             --model_dir=${model_dir} \
```

error message:
/home/ryu/sourcecode/Disentangled-Person-Image-Generation/tflib/plot.py:4: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

The backend was *originally* set to 'TkAgg' by the following code:
  File ""main.py"", line 4, in <module>
    from trainer import *
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py"", line 20, in <module>
    import models
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/models.py"", line 6, in <module>
    from utils import *
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/utils.py"", line 492, in <module>
    import matplotlib.pyplot as plt
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 71, in <module>
    from matplotlib.backends import pylab_setup
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/matplotlib/backends/__init__.py"", line 16, in <module>
    line for line in traceback.format_stack()


  matplotlib.use('Agg')
load pn_pairs_num ......
######coord2channel_simple#####
######coord2channel_simple#####
Uppercase local vars:
        BATCH_SIZE: 16
        DATA_DIR: 
        DIM: 64
        G_OUTPUT_DIM: 24576
        IMG_H: 128
        IMG_W: 64
        ITERS: 200000
        LAMBDA: 10
        MODE: dcgan
WARNING:tensorflow:From /home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py:199: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-07-13 10:59:03,319:WARNING::From /home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py:199: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-07-13 10:59:06.651736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:02:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-07-13 10:59:06.651792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-07-13 10:59:07.038518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-07-13 10:59:07.038577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 
2018-07-13 10:59:07.038589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N 
2018-07-13 10:59:07.038950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10411 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)
[*] MODEL dir: ./logs/MODEL1_Encoder_GAN_BodyROI7
[*] PARAM path: ./logs/MODEL1_Encoder_GAN_BodyROI7/params.json
2018-07-13 10:59:21.620532: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: part_bbox_1.  Can't parse serialized Example.
2018-07-13 10:59:21.620573: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: part_bbox_1.  Can't parse serialized Example.
2018-07-13 10:59:21.621055: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: part_bbox_1.  Can't parse serialized Example.
2018-07-13 10:59:21.621579: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:240 : Invalid argument: Key: part_bbox_1.  Can't parse serialized Example.
Traceback (most recent call last):
  File ""main.py"", line 90, in <module>
    main(config)
  File ""main.py"", line 82, in main
    trainer.train()
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py"", line 328, in train
    part_bbox_target_fixed, part_vis_fixed, part_vis_target_fixed = self.get_image_from_loader()
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py"", line 530, in get_image_from_loader
    self.mask_r6, self.mask_r6_target, self.part_bbox, self.part_bbox_target, self.part_vis, self.part_vis_target])
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1135, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1316, in _do_run
    run_metadata)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 16, current size 0)
         [[Node: batch = QueueDequeueManyV2[component_types=[DT_UINT8, DT_UINT8, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](batch/fifo_queue, batch/n)]]

Caused by op u'batch', defined at:
  File ""main.py"", line 90, in <module>
    main(config)
  File ""main.py"", line 23, in main
    trainer = DPIG_Encoder_GAN_BodyROI_FgBg(config)
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py"", line 42, in __init__
    self.part_bbox, self.part_bbox_target, self.part_vis, self.part_vis_target = self._load_batch_pair_pose(self.dataset_obj)
  File ""/home/ryu/sourcecode/Disentangled-Person-Image-Generation/trainer.py"", line 555, in _load_batch_pair_pose
    batch_size=self.batch_size, num_threads=self.num_threads, capacity=self.capacityCoff * self.batch_size)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 988, in batch
    name=name)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/training/input.py"", line 762, in _batch
    dequeued = queue.dequeue_many(batch_size, name=name)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py"", line 483, in dequeue_many
    self._queue_ref, n=n, component_types=self._dtypes, name=name)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 3480, in queue_dequeue_many_v2
    component_types=component_types, timeout_ms=timeout_ms, name=name)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/home/ryu/anaconda3/envs/pose/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

OutOfRangeError (see above for traceback): FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 16, current size 0)
         [[Node: batch = QueueDequeueManyV2[component_types=[DT_UINT8, DT_UINT8, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_INT64], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](batch/fifo_queue, batch/n)]]
"
Limit to GPU memory,charliememory/Disentangled-Person-Image-Generation,2018-06-07 04:34:47,0,,3,330114188,"Hi,

I run your training code for DeepFashion. 
And OOM error happens when i set batch size as 6 for MODEL101_DF_Encoder_GAN_BodyROI7.
Here i only can set batch size as 1, and i will take almost 8.5G memory per GPU.
So how to use 6 batch size for training?

Thanks in advance"
关于检测结果,jylins/core-text,2022-03-24 02:02:09,0,,5,1178849123,"作者您好，最近正在研究您的论文，并用自己的数据测试了代码，但是检测到的文本框只能是水平的，请问这是怎么回事啊，代码本身就不能做倾斜文本检测嘛
![1648087309(1)](https://user-images.githubusercontent.com/59427046/159827147-f2aea91e-fa70-4ada-b98a-ee73eec99c45.png)
![1648087322(1)](https://user-images.githubusercontent.com/59427046/159827169-67dce18e-929f-45c8-aa19-7cfba35315e4.png)
"
How can I eval in the same format of train,jylins/core-text,2021-12-09 11:44:42,0,,4,1075478448,"Thanh you for your excellent work.
I am successfull in training custom dataset and get checkpoint file with Icdar 2015 format dataset. I want to evaluate testset with the same format of train dataset. When I run tools/test.py, I can get results file but get errors when evaluate in testset
How can I fix it. Thank you very much!

"
Questions of specific constants,ceciliavision/perceptual-reflection-removal,2022-04-24 12:21:27,0,,23,1213639424,"What do the 2.6, 4.8, 3.7, etc. mean? 

https://github.com/ceciliavision/perceptual-reflection-removal/blob/1d2449463ab9e436443204f3a00eb75740e9a8f9/main.py#L189"
 asking about improving reflection removal results,ceciliavision/perceptual-reflection-removal,2022-04-19 14:26:09,0,,22,1208448243,"Hi,
I am trying the code with sample data (attached is the result), but I still can find the reflection. 
How to improve and control the reflection removal result, which parameters could tune the loss and enhance the following results.
https://drive.google.com/drive/u/2/folders/1_QIIfaLXXElNv9t0ayE2sc2ODpdty2J6

Thanks in advance."
std::bad_alloc with pre-trained test,ceciliavision/perceptual-reflection-removal,2022-03-13 11:43:09,0,,21,1167556629,"Hi, I've tried to test reflection removal with the pre-trained model on my images. I've launched this command:

`python3 main.py --task pre-trained --is_training 0`

However, the process crashes throwing  an instance of `std::bad_alloc`

I've run this command on a WSL instance with an i7-10875H / 32GB RAM / RTX 3070 Laptop. I've compiled kernel with NUMA support :

```
CONFIG_NUMA=y
CONFIG_K8_NUMA=y
CONFIG_X86_64_ACPI_NUMA=y
CONFIG_ACPI_NUMA=y
```

Process log

```
python3 main.py --task pre-trained --is_training 0
[i] Loaded pre-trained vgg19 parameters
[i] Hypercolumn ON, building hypercolumn features ...
conda activate perceptual-reflection-removalListing trainable variables ...
<tf.Variable 'g_conv0/weights:0' shape=(1, 1, 1475, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv0/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv0/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv0/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv1/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv1/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv1/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv1/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv2/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv2/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv2/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv2/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv3/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv3/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv3/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv3/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv4/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv4/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv4/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv4/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv5/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv5/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv5/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv5/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv6/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv6/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv6/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv6/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv7/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv7/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv7/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv7/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv9/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv9/w0:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv9/w1:0' shape=() dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv9/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv_last/weights:0' shape=(1, 1, 64, 6) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'g_conv_last/biases:0' shape=(6,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_1/conv/filter:0' shape=(4, 4, 6, 64) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_2/conv/filter:0' shape=(4, 4, 64, 128) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_2/batchnorm/offset:0' shape=(128,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_2/batchnorm/scale:0' shape=(128,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_3/conv/filter:0' shape=(4, 4, 128, 256) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_3/batchnorm/offset:0' shape=(256,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_3/batchnorm/scale:0' shape=(256,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_4/conv/filter:0' shape=(4, 4, 256, 512) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_4/batchnorm/offset:0' shape=(512,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_4/batchnorm/scale:0' shape=(512,) dtype=float32_ref>
Listing trainable variables ...
<tf.Variable 'discriminator/layer_5/conv/filter:0' shape=(4, 4, 512, 1) dtype=float32_ref>
2022-03-13 12:26:11.065267: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2022-03-13 12:26:13.884531: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:950] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2022-03-13 12:26:13.884731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:
name: NVIDIA GeForce RTX 3070 Laptop GPU major: 8 minor: 6 memoryClockRate(GHz): 1.56
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.98GiB
2022-03-13 12:26:13.884883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2022-03-13 12:26:18.254634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-03-13 12:26:18.254792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0
2022-03-13 12:26:18.254833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N
2022-03-13 12:26:18.255209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1193] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted
```"
Training Dataset,ceciliavision/perceptual-reflection-removal,2022-03-10 10:38:24,0,,20,1165047722,"Could you please provide your training dataset names，including 5000 synthetic images and 90 real images？
And how to set parameters 'w1' 'learning rate_g' 'learning rate_d' in main.py, are they fixed values？"
exclusion loss,ceciliavision/perceptual-reflection-removal,2020-05-23 12:21:46,0,,19,623648974,"according to the formulation of Exclusion loss in your paper, 
you set the normalization factors **Lambda_T** and **Lambda_R**.

I guess function ""compute_exclusion_loss"" in main.py computes the exclusion loss.

In that function,
It seems that alphax and alphay are calculated for the normalization.

however, after that line ,
I think they are used only for **Lambda_R**.
where is **Lambda_T** for normalizing the absolute value of T's gradient?
"
Test results are not correct,ceciliavision/perceptual-reflection-removal,2019-07-29 10:42:43,6,,17,473979579,"I'm testing the code by using pre-trained model.
i have executed following command
`python3 main.py --task pre-trained --is_training 0`
But result's images do not make any sense.
here is screenshot 
![result](https://user-images.githubusercontent.com/51105008/62042396-e2455380-b1b2-11e9-90e5-7014a1dc490d.PNG)
"
missing data,ceciliavision/perceptual-reflection-removal,2018-12-03 10:16:50,0,,11,386746519,"There  are one image named `34.jpg` is missing on `real` data of your provided Google Driver download link. According to your paper, there should have 110 images, but the downloaded data only contains 109 datas."
Generating cloth masks for new clothes,Aliasgarsaifee/HackInIndia,2020-11-30 04:10:46,0,,2,753124954,"Hi,
Amazing work! I was wondering how can we generate new cloth masks for new clothes that we might want to add?

Thanks"
Data links,Jianbo-Lab/LBIE,2019-06-14 20:29:22,0,,2,456421822,"Thanks for providing the code. :) 

Could you also provide the links of the dataset that you used in the paper. "
Is there a code for this?,Jianbo-Lab/LBIE,2018-11-18 23:27:03,0,,1,382012973,Is there a code for this?
 f30k-caption?,ruotianluo/DiscCaptioning,2020-03-11 06:35:46,4,,21,579040825,"hello,where can I download the f30k-caption  in your annFile = 'f30k-caption/annotations/dataset_flickr30k.json'?when I use the flickr30,the eval code takes wrong.Is there the eval code for flickr30?"
"when bash run_att_d.sh ,it broke ",ruotianluo/DiscCaptioning,2020-03-01 15:36:13,13,,20,573566954,
Training curve of reinforcement learning,ruotianluo/DiscCaptioning,2019-11-29 06:37:59,0,,19,530179741,"When I train the model with RL using run_att_d.sh, the CIDEr score got a significant drop,
I saw that you have provided the curve of training VSE model#11,
I wonder would you like to provide the training curve of RL stage?
Thank you very much :D"
关于每张图片使用几个句子,ruotianluo/DiscCaptioning,2019-07-15 07:07:57,1,,14,467971630,您好！我在跑您的代码时候，发现opt文件中默认每张图片使用一个句子 ，但是在最后联合训练的脚本中，又专门指定了每张图片使用一个句子，但是其他两个脚本并没有指定。我对此感到很困惑，请问您在实现的时候，训练自检索模型、预训练caption模型和最后的联合训练每张图片分别用了几个句子呢？
Similar work,ruotianluo/DiscCaptioning,2019-05-08 11:46:49,0,,13,441694014,"I think your work is very similar to ""Deep Reinforcement Learning-based Image Captioning with Embedding Reward"".  I wonder what is the difference with them?"
att_masks,ruotianluo/DiscCaptioning,2019-05-08 02:34:04,2,,12,441523298,"excuse me, would you mind explaining   the function about the att_mask?  "
the retrieval loss doesn't converge well,ruotianluo/DiscCaptioning,2019-02-26 06:36:54,11,,11,414454400,"Hello, luo
when I pretrain the VSEFCmodel,  the vse_loss doesn't converge well , just around 51.2. is there some mistakes in my experiments, how about your vse_loss when you pretrain VSEFCmodel?"
Video Segment as timestamp,ycxioooong/MovieSynopsisAssociation,2022-01-07 01:47:43,0,,5,1095903020,"Hi,
I understand the clips cannot be shared due to copyrights. 
Is it possible to just share the _timestamp_ of the segment associated with the paragraph in the movie? 
Thanks :) 
"
More information about training and testing,ycxioooong/MovieSynopsisAssociation,2021-08-01 13:34:21,0,,4,957494199,"Thank you for sharing the code .

I was wondering ，after I have trained the appr and action embedding networks , how can I proceed to the next step, how to conduct test experiments, and what are the three other types of datasets mentioned in em.py、efm_appr.py、and graph.py ?
Thanks a lot!

"
Dataset release,ycxioooong/MovieSynopsisAssociation,2021-07-18 03:00:35,0,,3,946940144,"Thank you for sharing the code. I was wondering when the dataset will be released. Thank you.

Best,
Huaizu"
How many keyframes were used to represent a shot?,ycxioooong/MovieSynopsisAssociation,2021-04-10 04:54:17,0,,2,854981504,MovieNet offers 3 keyframes per shot; I was wondering if in your work you used all three keyframes to represent a shot? Or did you only use 1 keyframe?
Out of memory exception. ,liviniuk/DORN_depth_estimation_Pytorch,2021-04-19 12:09:39,1,,6,861241756,"Hi, I found really usefull for implementaion and i am facing an out of memory exception when i learn in a bigger Dataset. I did not change the model implementation and i tracked the memory over the batches. 
![image](https://user-images.githubusercontent.com/4273938/115233828-a980a700-a118-11eb-9e26-969da89fd38c.png)
Since you are more familiar with the code, where do u think that a memory leak could happen? I will investigate this if i find something I will post it here. "
Depth results on NYUv2,liviniuk/DORN_depth_estimation_Pytorch,2021-04-16 07:58:56,0,,5,859570383,"
First of all, Thank you very much for sharing this code. It is an improved version of the former implementation.

I have tested the NYUv2 values, and I got the results you publish but for TRAINING, not for testing. Is that right? In the DORN paper, these values refer to the TESTING set. 

Is it that correct? 
As you mention the amount of images can be a reason of the difference. 
![Selection_003](https://user-images.githubusercontent.com/4273938/114991647-1e927900-9e9a-11eb-81dc-8b82b51f576f.png)
"
"D1, D2, D3 gradually become smaller",liviniuk/DORN_depth_estimation_Pytorch,2020-12-14 02:31:23,0,,4,765794830,"Thank you for your code, it's very useful to me. But in the training process, D1, D2, D3 gradually become smaller. What's the problem?"
The question of Subjective effect,liviniuk/DORN_depth_estimation_Pytorch,2020-11-23 05:42:30,1,,3,748486016,"Hello，your code help me a lot,thank u very much
I have a question about subjective effect.The RMSE is great,but the prediect depth has a block effect.Do you konw how to solve this effect?"
The Peformance on KITTI,liviniuk/DORN_depth_estimation_Pytorch,2020-06-15 11:04:24,1,,2,638760042,"Hi,

Thank you for providing this PyTorch implementation.

Have you tested the accuracy performance of this code on KITTI?

Many thanks"
someting wrong ,liviniuk/DORN_depth_estimation_Pytorch,2020-06-12 13:26:50,1,,1,637749668,"Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
Traceback (most recent call last):
  File ""train.py"", line 56, in <module>
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    for i, (input, target) in enumerate(train_loader):
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\site-packages\torch\utils\data\dataloader.py"", line 278, in __iter__
    exitcode = _main(fd)
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\spawn.py"", line 114, in _main
    return _MultiProcessingDataLoaderIter(self)
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\site-packages\torch\utils\data\dataloader.py"", line 682, in __init__
    prepare(preparation_data)
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\spawn.py"", line 225, in prepare
        _fixup_main_from_path(data['init_main_from_path'])
w.start()  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\spawn.py"", line 277, in _fixup_main_from_path

    run_name=""__mp_main__"")
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\process.py"", line 112, in start
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\runpy.py"", line 263, in run_path
        pkg_name=pkg_name, script_name=fname)
self._popen = self._Popen(self)  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\runpy.py"", line 96, in _run_module_code

    mod_name, mod_spec, pkg_name, script_name)
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\context.py"", line 223, in _Popen
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\runpy.py"", line 85, in _run_code
        exec(code, run_globals)
return _default_context.get_context().Process._Popen(process_obj)  File ""C:\DORN_depth_estimation_Pytorch-master\train.py"", line 18, in <module>

    parser = argparse.ArgumentParser(description='DORN ')
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\context.py"", line 322, in _Popen
NameError: name 'argparse' is not defined
    return Popen(process_obj)
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\popen_spawn_win32.py"", line 89, in __init__
    reduction.dump(process_obj, to_child)
  File ""D:\Users\Administrator\Anaconda3\envs\VNL\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
"
Would you like to provide any chinese-available links of Youtube-Boundingboxes dataset?,guanfuchen/video_obj,2019-09-17 06:58:40,0,,3,494438806,"I could not download YoutubeBoundingBox dataset due to poor network.  
Does there any optional download link of this dataset that is available in China?  
Thanks in advance :)"
你这是什么代码?怎么用?,guanfuchen/video_obj,2019-04-29 05:31:35,0,,2,438155161,
Test results of official trained model(download_PieAPPv0.1_PT_weights.sh) not same with paper,prashnani/PerceptualImageError,2022-05-28 10:54:48,0,,13,1251593193,"Hi, I have tested the trained model(download_PieAPPv0.1_PT_weights.sh) that you released, but the performance is lower than the result in your paper by about 2% in both PLCC, SRCC, and KRCC. Also, the reimplement is also lower than the result in your paper by about 2%. Can I ask for your testing and training details?

Thanks a lot."
PieAPPv0.1.exe ignores --gpu_id parameter,prashnani/PerceptualImageError,2022-03-29 12:38:19,0,,12,1184803804,"Even with --gpu_id 0 only CPU is used when using the windows executable, any workaround for that?"
Train with Custom dataset?,prashnani/PerceptualImageError,2022-02-26 20:43:28,0,,11,1152274106,"Sorry for my noob question, but Is possible train this network with my dataset (images from one class) and get same results???"
torch cant use numpy??,prashnani/PerceptualImageError,2022-01-17 12:59:23,0,,10,1105824907,"![image](https://user-images.githubusercontent.com/73474866/149773334-5fdf02a2-3f72-4c1e-b698-9a23db654056.png)

how can i use numpy in pytorch?"
Size mismatch when loading the pretrained pytorch model.,prashnani/PerceptualImageError,2020-06-04 15:35:58,1,,6,630937777,"Below is the error message:
""size mismatch for ref_score_subtract.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1, 1]).""

Need to manually adjust the size of 'ref_score_subtract.weight'."
Available training code,prashnani/PerceptualImageError,2020-02-20 10:09:39,3,todo,5,568188144,Hi! Will the training code be uploaded any time soon? Am excited to try and train with this metric.
About visdom version,juefeix/pnn.pytorch,2018-07-04 07:22:35,0,,4,338158039,"The latest visdom version gives an error.

`AttributeError: 'Visdom' object has no attribute 'updateTrace'`

When I installed visdom version of 0.17, no error occurred."
Trained models,juefeix/pnn.pytorch,2018-06-26 19:02:25,0,,3,335951701,"Thanks for releasing your code! Any chance you might be able to release some of the trained models reported in your paper, especially the model trained on ImageNet?"
How to control fan-out ratio?,juefeix/pnn.pytorch,2018-06-15 04:09:01,1,,2,332640298,"Dear @juefeix ,
   I could not figure out how to control fan-out ratio using your code. Would you please to make this clear for me?"
Inference time compare with CNN？,juefeix/pnn.pytorch,2018-06-14 16:45:30,0,,1,332481777,"Firstly, Thank you for the amazing work. The inference speed is my main concern for now. Can you please compare it with CNN, especially when m/p=256？"
Question about 'distortion_to_minimize',fab-jul/imgcomp-cvpr,2021-05-22 07:40:06,3,,32,898745063,"hello!

**If i want to optimized with mse, What should I change to mse?**
I thought
""imgcomp-cvpr/code/ae_configs/cvpr/base
distortion_to_minimize=ms_ssim
-> distortion_to_minimize=mse"" to optimize to mse.
Is it right???

Thank you."
about soft  and hard quantization,fab-jul/imgcomp-cvpr,2021-04-09 09:16:29,2,,30,854339328,"hello, I don't figure out that when testing the model , why to use the hardout for the input of the P network .In my opinions ,the hardout  and the symbols are equal in a sense ."
VQA-CP v2 dataset download ,AishwaryaAgrawal/GVQA,2020-10-08 08:59:08,1,,1,717156347,"Could anyone tell me where I can download the VQA-CP v2 dataset? The link in the README file is dead.
Thanks!"
No convergence while estimating H with W fixed,rhgao/separating-object-sounds,2018-12-23 15:09:34,0,,4,393757251,"Hi rhgao,

Thanks for releasing your code. When I used your network to seperate audio sources, to get some `.wav` files as output, I realized a NMF algorithm estimating `H` with the `W` fixed by myself. But I found my program didn‘t seem to converge. So I want to know how did you compute `H` with `W` fixed? Do you know of any related libraries? I didn't found it in `sklearn` you mentioned in #1. Or  did you realize it by yourself?  And could you share your code? 

Thank you very much! Waiting for your reply."
"how to get the video level ""weak"" label",rhgao/separating-object-sounds,2018-11-13 04:33:28,5,,3,380055715,"Dear Mr. Gao
   Thank you so much for the great work. However, I met some problems when  I  implemented this code. 
   As described in you article, ""For the visual frames, we use an ImageNet pre-trained ResNet-152 network [34] to make object category predictions, and we max-pool over predictions of all frames to obtain a video-level prediction. The top labels (with class probability larger than a threshold = 0.3) are used as weak \labels"" for the unlabeled video."" 
  However, when I use the pre-trained-152 network, I can  get the only one category prediction lager than the threshold. How can I get multi-labels through the pre-trained-152 network.
  Should  I  train a object detection network or a multi-classes multi-labels network or some other solutions. Thank you for your assistance
  Best regards!"
can you provide a final well-trained model?,rhgao/separating-object-sounds,2018-10-27 16:48:06,0,,2,374664842,"hi, I want to use this model to   test some data, can you provide the well trained weight file for us?  thanks"
result ,He-jerry/CRRN,2020-08-07 06:22:25,0,,1,674789772,"Hello, could you please share the result images?"
Unable to recreate the results,johnwlambert/dlupi-heteroscedastic-dropout,2020-02-29 20:39:53,6,,3,573407571,"Hi John,
Thank you for providing the code for a lot of methods. While I was trying to recreate the results for the model dropout_fn_of_xstar, it tries to load the dlupi trained model. If I load the untrained model, the loss diverges and goes to Nan. Could you please look into it. Thank you."
Re-Creating the dataset,johnwlambert/dlupi-heteroscedastic-dropout,2018-11-26 17:27:20,13,,2,384434337,"https://github.com/johnwlambert/dlupi-heteroscedastic-dropout/blob/da71881506a550d19a0879fc625d4ff609ac11e3/cnns/imagenet/create_bbox_dataset.py#L160-L172

I'm trying to re-create the exact dataset. I feel like all the shutil.copy lines here should be uncommented because otherwise, I'm just iterating over the data and not creating anything. "
Pytorch Version,marcoancona/dasp,2022-03-07 08:03:05,0,,5,1161035114,"Hi,
Thanks a lot for your great work, do you have a pytorch version of DASP? "
ValueError,marcoancona/dasp,2021-01-06 18:03:23,2,,4,780744970,"Hello,

thank you for providing your code!
I tried running the provided example (without changes) for the parkinson dataset and there seems to be an error. So the training is fine but wenn  dasp = DASP(model) is executed, I get the following message:

ValueError: Layer dense_1 expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'Placeholder:0' shape=(None, 18) dtype=float32>, <tf.Tensor 'Placeholder_1:0' shape=(None, 18) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(None, 1) dtype=float32>]

The problem appears to be while executing the line:

first_layer_output = ProbDenseInput(l.units, activation=l.activation, name=l.name)(self.inputs)

from the dasp.py file. Do you know, what might be the problem? Thanks in advance!"
Outputs are `nan` given a big window shape in iterator,marcoancona/dasp,2020-08-10 05:19:46,3,,3,675870036,"When I run DASP on a convolutional neural network (mnist dataset), I found something weird.
If I use a `ImagePlayerIterator` with a big `window_shape` (like 1/4 of the image size), then `y1` and `y2` will produce `nan`.

`y1, y2 = self.dasp_model.predict(inputs, batch_size=batch_size)`
`y1 = y1.reshape(len(ks), x.shape[0], -1, 2)`
`y2 = y2.reshape(len(ks), x.shape[0], -1, 2)`

I tried to check if something like delta (0.0001) is missing in denominator or divisor, but it seems like no problem in codes of all lpdn or dasp layers.
Have you any idea about this? Thanks in advance."
OSError when opening provided data file,Vijetha1/WDHT,2021-08-13 02:14:04,0,,7,969938780,"When running the 12bits_NUS.py file, running into the following issue:

 OSError: Unable to open file (truncated file: eof = 108228769296, sblock->base_addr = 0, stored_eof = 115503994024)

I downloaded the dataset and the pretrained weights. 
python version is 3.8, and ubuntu is 20.04. Am I supposed to use python2.7 and ubuntu 14.04? Anyone has some experience with other dependencies?
"
"Hi, I can't download this data set completely because of my network. Can you provide a method to generate this hdf5 data set?",Vijetha1/WDHT,2021-04-12 06:08:25,1,,6,855586846,"Hi, I can't download this data set completely because of my network. Can you provide a method to generate this hdf5 data set?"
The vectorLoss and MSE loss,Vijetha1/WDHT,2019-11-29 07:57:17,0,,4,530205636,"Hi,

I found the loss function in your code did not consistent with the ones in your paper, such as the MSE loss term in the code is ((h_i-h_j)^2-w_i^Tw_j), while in the paper is ((h_i-h_j)^2-0.5*(1-w_i^Tw_j)),

And the vectorLoss ia also wrong.

I wanna to know why you utiliz different losses from your paper?"
HI ，I cannot find the dataset and pretrain weight ，can you help me?,Vijetha1/WDHT,2019-09-20 12:55:57,0,,3,496337239,
mAP on 12bit is only 0.3269,Vijetha1/WDHT,2019-04-22 06:20:40,11,,2,435614472,"I used all the pre-training models and data you provided to test. But on 12 bits, the mAP is only 0.3269. Why is the result so different from that of the paper?

![image](https://user-images.githubusercontent.com/20432947/56486725-3c2ae300-650b-11e9-8667-e88a6286229b.png)
"
Submodule ref does not exist,LPMP/LPMP,2022-05-10 18:46:14,1,,38,1231556948,"Pip fails with the following error:

```
Pip subprocess error:
  Running command git clone -q https://github.com/LPMP/LPMP.git /tmp/pip-req-build-btoiy_7y
  Running command git submodule update --init --recursive -q
  fatal: remote error: upload-pack: not our ref 6df61c7d478016053cffa2f998e0ef4fdf57be8c
  fatal: The remote end hung up unexpectedly
  Fetched in submodule path 'external/DD_ILP', but it did not contain 6df61c7d478016053cffa2f998e0ef4fdf57be8c. Direct fetching of that commit failed.

```

The commit `6df61c7d478016053cffa2f998e0ef4fdf57be8c` does not exist in DD_ILP repo, but is pointed to in `.gitmodules`. Please fix."
"Failed to install the graph matching solver as used in ""Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers""",LPMP/LPMP,2021-11-26 07:22:28,0,,37,1064172214,"I am trying to install the precise version of graph matching as used in ""Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers."". I used the following command to install it, which you gave in README.md.
 `python3 -m pip install git+https://github.com/lpmp/LPMP.git@keypiont_submission` 

However, I meet a problem as follow:
```
Collecting git+https://github.com/lpmp/LPMP.git@keypiont_submission
  Cloning https://github.com/lpmp/LPMP.git (to revision keypiont_submission) to /tmp/pip-req-build-8vg9x9xr
  Running command git clone -q https://github.com/lpmp/LPMP.git /tmp/pip-req-build-8vg9x9xr
  WARNING: Did not find branch or tag 'keypiont_submission', assuming revision or ref.
  Running command git checkout -q keypiont_submission
  error: pathspec 'keypiont_submission' did not match any file(s) known to git.
WARNING: Discarding git+https://github.com/lpmp/LPMP.git@keypiont_submission. Command errored out with exit status 1: git checkout -q keypiont_submission Check the logs for full command output.
ERROR: Command errored out with exit status 1: git checkout -q keypiont_submission Check the logs for full command output.
```

Did the ""keypiont_submission"" branch removed ? How can I get this graph matching solver ?

"
Failed to install LPMP on Ubuntu 18.04,LPMP/LPMP,2021-07-01 13:12:42,4,,34,934831999,"Hi, when I run the command
`PACKAGES=""gm"" python3 -m pip install git+https://github.com/AndreaHor/LPMP.git`
I get an error during installing collected packages: lpmp-py. I try to install LPMP on Ubuntu 18.04 with gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~18.04). The full error message is below. Thank you for your time and attention.

```
  Cloning https://github.com/AndreaHor/LPMP.git to /tmp/pip-aw_xn3gm-build
Installing collected packages: lpmp-py
  Running setup.py install for lpmp-py ... error
    Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-aw_xn3gm-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-vi1j_bdf-record/install-record.txt --single-version-externally-managed --compile --user --prefix=:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/lpmp_py
    copying lpmp_py/raw_solvers.py -> build/lib.linux-x86_64-3.6/lpmp_py
    copying lpmp_py/__init__.py -> build/lib.linux-x86_64-3.6/lpmp_py
    running build_ext
    extension name: graph_matching_py
    Testing gcc...
    ...has version 9.4.0
    Found suitable gcc/g++ version gcc g++
    -- The C compiler identification is GNU 9.4.0
    -- The CXX compiler identification is GNU 9.4.0
    -- Check for working C compiler: /usr/bin/gcc
    -- Check for working C compiler: /usr/bin/gcc -- works
    -- Detecting C compiler ABI info
    -- Detecting C compiler ABI info - done
    -- Detecting C compile features
    -- Detecting C compile features - done
    -- Check for working CXX compiler: /usr/bin/g++
    -- Check for working CXX compiler: /usr/bin/g++ -- works
    -- Detecting CXX compiler ABI info
    -- Detecting CXX compiler ABI info - done
    -- Detecting CXX compile features
    -- Detecting CXX compile features - done
    -- Found OpenMP_C: -fopenmp (found version ""4.5"")
    -- Found OpenMP_CXX: -fopenmp (found version ""4.5"")
    -- Found OpenMP: TRUE (found version ""4.5"")
    -- HDF5: Using hdf5 compiler wrapper to determine CXX configuration
    -- Found HDF5: /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_cpp.so;/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so;/usr/lib/x86_64-linux-gnu/libpthread.so;/usr/lib/x86_64-linux-gnu/libsz.so;/usr/lib/x86_64-linux-gnu/libz.so;/usr/lib/x86_64-linux-gnu/libdl.so;/usr/lib/x86_64-linux-gnu/libm.so (found version ""1.10.0.1"") found components:  CXX
    -- /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_cpp.so/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so/usr/lib/x86_64-linux-gnu/libpthread.so/usr/lib/x86_64-linux-gnu/libsz.so/usr/lib/x86_64-linux-gnu/libz.so/usr/lib/x86_64-linux-gnu/libdl.so/usr/lib/x86_64-linux-gnu/libm.so
    -- /usr/include/hdf5/serial
    --
    -- pybind11 v2.6.2 dev1
    CMake Warning at external/pybind11/tools/pybind11Common.cmake:174 (message):
      USE -DCMAKE_CXX_STANDARD=17 instead of PYBIND11_CPP_STANDARD
    Call Stack (most recent call first):
      external/pybind11/CMakeLists.txt:169 (include)
    
    
    -- Found PythonInterp: /usr/bin/python3 (found version ""3.6.9"")
    -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.6m.so
    -- Performing Test HAS_FLTO
    -- Performing Test HAS_FLTO - Success
    -- Found OpenMP_C: -fopenmp (found version ""4.5"")
    -- Found OpenMP_CXX: -fopenmp (found version ""4.5"")
    -- Configuring done
    CMake Warning (dev) at src/asymmetric_multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""asymmetric_multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    CMake Warning (dev) at src/multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    CMake Warning (dev) at src/asymmetric_multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""asymmetric_multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    CMake Warning (dev) at src/multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    -- Generating done
    -- Build files have been written to: /tmp/pip-aw_xn3gm-build/build/temp.linux-x86_64-3.6
    Scanning dependencies of target lglmain
    Scanning dependencies of target lgl
    [ 25%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lglmain.dir/lglmain.c.o
    [ 25%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lglib.c.o
    [ 25%] Linking C static library liblglmain.a
    [ 25%] Built target lglmain
    [ 25%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lglopts.c.o
    [ 50%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lgldimacs.c.o
    /tmp/pip-aw_xn3gm-build/external/DD_ILP/external/lingeling/lglib.c: In function ‘lglstampall’:
    /tmp/pip-aw_xn3gm-build/external/DD_ILP/external/lingeling/lglib.c:19233:2: warning: this ‘if’ clause does not guard... [-Wmisleading-indentation]
    19233 |  if (rootsonly) noimpls++; goto CONTINUE;
          |  ^~
    /tmp/pip-aw_xn3gm-build/external/DD_ILP/external/lingeling/lglib.c:19233:28: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘if’
    19233 |  if (rootsonly) noimpls++; goto CONTINUE;
          |                            ^~~~
    [ 50%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lglbnr.c.o
    [ 50%] Linking C static library liblgl.a
    [ 50%] Built target lgl
    Scanning dependencies of target graph_matching_frank_wolfe
    Scanning dependencies of target MRF_factors
    [ 50%] Building CXX object src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/graph_matching_frank_wolfe.cpp.o
    [ 75%] Building CXX object src/mrf/CMakeFiles/MRF_factors.dir/pairwise_simplex_factor.cpp.o
    g++: fatal error: Killed signal terminated program cc1plus
    compilation terminated.
    src/mrf/CMakeFiles/MRF_factors.dir/build.make:62: recipe for target 'src/mrf/CMakeFiles/MRF_factors.dir/pairwise_simplex_factor.cpp.o' failed
    make[3]: *** [src/mrf/CMakeFiles/MRF_factors.dir/pairwise_simplex_factor.cpp.o] Error 1
    CMakeFiles/Makefile2:1181: recipe for target 'src/mrf/CMakeFiles/MRF_factors.dir/all' failed
    make[2]: *** [src/mrf/CMakeFiles/MRF_factors.dir/all] Error 2
    make[2]: *** Waiting for unfinished jobs....
    g++: fatal error: Killed signal terminated program cc1plus
    compilation terminated.
    src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/build.make:62: recipe for target 'src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/graph_matching_frank_wolfe.cpp.o' failed
    make[3]: *** [src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/graph_matching_frank_wolfe.cpp.o] Error 1
    CMakeFiles/Makefile2:2199: recipe for target 'src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/all' failed
    make[2]: *** [src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/all] Error 2
    CMakeFiles/Makefile2:2294: recipe for target 'src/graph_matching/CMakeFiles/graph_matching_py.dir/rule' failed
    make[1]: *** [src/graph_matching/CMakeFiles/graph_matching_py.dir/rule] Error 2
    Makefile:786: recipe for target 'graph_matching_py' failed
    make: *** [graph_matching_py] Error 2
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-aw_xn3gm-build/setup.py"", line 137, in <module>
        zip_safe=False,
      File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 129, in setup
        return distutils.core.setup(**attrs)
      File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/usr/lib/python3.6/distutils/command/install.py"", line 589, in run
        self.run_command('build')
      File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/lib/python3.6/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/pip-aw_xn3gm-build/setup.py"", line 35, in run
        self.build_extension(ext)
      File ""/tmp/pip-aw_xn3gm-build/setup.py"", line 112, in build_extension
        subprocess.check_call(['cmake', '--build', '.', '--target', ext.name] + build_args, cwd=self.build_temp)
      File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call
        raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'graph_matching_py', '--config', 'Release', '--', '-j2']' returned non-zero exit status 2.
    
    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-aw_xn3gm-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-vi1j_bdf-record/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/pip-aw_xn3gm-build/
```"
Both pip install commands are down,LPMP/LPMP,2021-04-12 05:09:20,0,,30,855548641,Hi there! I found that currently both of the two pip install commands are down. Do you have any plans to fix this?
Multicut solvers,LPMP/LPMP,2021-03-11 17:13:22,0,,29,829350618,"Thanks for sharing this repository, which is quite complete !

I am particularly interested in the multicut problem. If I am not wrong, there are 2 LP-based solvers:
- [Iterative Cycle Packing (ICP)](http://proceedings.mlr.press/v80/lange18a/lange18a.pdf). The corresponding files are `src/multicut/multicut_<CONSTRAINTS>_packing*`, e.g. [multicut_cycle_packing](https://github.com/LPMP/LPMP/blob/master/src/multicut/multicut_cycle_packing.cpp).
- [Message Passing (MP)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Swoboda_A_Message_Passing_CVPR_2017_paper.pdf). The corresponding files are `src/multicut/multicut_message_passing*`, e.g. [multicut_message_passing_text_input_parallel](https://github.com/LPMP/LPMP/blob/master/src/multicut/multicut_message_passing_text_input_parallel.cpp).

On top of them, I wonder if there is another solver for multicut, whose the corresponding files are `multicut_<CONSTRAINTS>_text_input.*`, e.g. [multicut_cycle_text_input](https://github.com/LPMP/LPMP/blob/master/src/multicut/multicut_cycle_text_input.cpp). Is this right? If so, could you point me to its reference article ?

Moreover, the third solver has many parameters. I wonder how to call this method for a good performance. I suppose that you have already explored this aspect in some way. I saw the following call in the `mulitcut/eval` directory: `./multicut_odd_bicycle_wheel_text_input -i INPUT_FILE.txt -o OUTPUT_FILE.txt --standardReparametrization anisotropic --roundingReparametrization uniform:0.5 --tightenReparametrization uniform:0.5 --tightenInterval 50 --tightenIteration 1 --tightenConstraintsPercentage 0.05  --primalComputationStart 1 --primalComputationInterval 10 -v 2 --lowerBoundComputationInterval 10 --tighten`. Can we rely on this ?

By the way, is there any multicut ILP solver in this repository (based on the cutting plane approach)?

Thanks in advance.
"
bindings.graph_matching_py not found,LPMP/LPMP,2020-11-17 18:03:55,0,,22,744962285,"I have pip installed the forked version by  mrolinek which is the only one that really builds. But I get the following error when running the train_eval.py from blackbox-deep-graph-matching. Within the python package lpmp_py/raw_solvers.py tries to import the **bindings.graph_matching_py**  but there is no graph_matching_py in the bindings package.

ModuleNotFoundError: No module named 'bindings.graph_matching_py'.


Is this really working? I have tried everything."
failure to build ,LPMP/LPMP,2020-07-08 04:32:01,8,,15,652941152,"Hi, when I run the installation instruction 

    python3 -m pip install git+https://github.com/lpmp/LPMP.git


I get an error during building lpmp-py wheel causing the installation to fail. I am running Mac 10.14 and Clang 11. The full error message is below. Thanks for your attention. 
```
~ simon$ python3 -m pip install git+https://github.com/lpmp/LPMP.git
Collecting git+https://github.com/lpmp/LPMP.git
  Cloning https://github.com/lpmp/LPMP.git to /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g
  Running command git clone -q https://github.com/lpmp/LPMP.git /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g
  Running command git submodule update --init --recursive -q
Building wheels for collected packages: lpmp-py
  Building wheel for lpmp-py (setup.py) ... error
  ERROR: Complete output from command /Users/Simon/miniconda3/bin/python3 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-wheel-nhuiet1t --python-tag cp37:
  ERROR: running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.7
  creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py
  copying lpmp_py/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
  copying lpmp_py/raw_solvers.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
  creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/graph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/multigraph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/utils.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  running build_ext
  xargs: illegal option -- d
  usage: xargs [-0opt] [-E eofstr] [-I replstr [-R replacements]] [-J replstr]
               [-L number] [-n number [-x]] [-P maxprocs] [-s size]
               [utility [argument ...]]
  Traceback (most recent call last):
    File ""<string>"", line 1, in <module>
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 108, in <module>
      zip_safe=False,
    File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 161, in setup
      return distutils.core.setup(**attrs)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/core.py"", line 148, in setup
      dist.run_commands()
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
      self.run_command(cmd)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py"", line 192, in run
      self.run_command('build')
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 32, in run
      self.build_extension(ext)
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 87, in build_extension
      self._prepare_environment()
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 62, in _prepare_environment
      gcc, gpp = self._find_suitable_gcc_gpp()
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 50, in _find_suitable_gcc_gpp
      all_gccs = subprocess.check_output(cmd_for_all_gccs, shell=True).decode(""utf-8"").rstrip().split(""\n"")
    File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 395, in check_output
      **kwargs).stdout
    File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 487, in run
      output=stdout, stderr=stderr)
  subprocess.CalledProcessError: Command 'echo -n $PATH | xargs -d : -I {} find -H {} -maxdepth 1 -perm -o=x -type l,f -printf '%P
  ' | grep '^gcc-[0-9].\?.\?.\?'' returned non-zero exit status 1.
  ----------------------------------------
  ERROR: Failed building wheel for lpmp-py
  Running setup.py clean for lpmp-py
Failed to build lpmp-py
Installing collected packages: lpmp-py
  Running setup.py install for lpmp-py ... error
    ERROR: Complete output from command /Users/Simon/miniconda3/bin/python3 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-record-0gw1stpo/install-record.txt --single-version-externally-managed --compile:
    ERROR: running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.7-x86_64-3.7
    creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py
    copying lpmp_py/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
    copying lpmp_py/raw_solvers.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
    creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/graph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/multigraph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/utils.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    running build_ext
    xargs: illegal option -- d
    usage: xargs [-0opt] [-E eofstr] [-I replstr [-R replacements]] [-J replstr]
                 [-L number] [-n number [-x]] [-P maxprocs] [-s size]
                 [utility [argument ...]]
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 108, in <module>
        zip_safe=False,
      File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 161, in setup
        return distutils.core.setup(**attrs)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
        self.run_command(cmd)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/command/install.py"", line 545, in run
        self.run_command('build')
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 32, in run
        self.build_extension(ext)
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 87, in build_extension
        self._prepare_environment()
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 62, in _prepare_environment
        gcc, gpp = self._find_suitable_gcc_gpp()
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 50, in _find_suitable_gcc_gpp
        all_gccs = subprocess.check_output(cmd_for_all_gccs, shell=True).decode(""utf-8"").rstrip().split(""\n"")
      File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 395, in check_output
        **kwargs).stdout
      File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 487, in run
        output=stdout, stderr=stderr)
    subprocess.CalledProcessError: Command 'echo -n $PATH | xargs -d : -I {} find -H {} -maxdepth 1 -perm -o=x -type l,f -printf '%P
    ' | grep '^gcc-[0-9].\?.\?.\?'' returned non-zero exit status 1.
    ----------------------------------------
ERROR: Command ""/Users/Simon/miniconda3/bin/python3 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-record-0gw1stpo/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/
```"
target specific option mismatch,LPMP/LPMP,2019-03-26 15:12:14,2,,4,425478496,"When compiling, I get the following error:

```
In file included from /net/hciserver03/storage/jschnell/software/gccbin/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/immintrin.h:41,
                 from /export/home/jschnell/masterarbeit/code/LPMP/external/libsimdpp/simdpp/setup_arch.h:281,
                 from /export/home/jschnell/masterarbeit/code/LPMP/external/libsimdpp/simdpp/simd.h:14,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/config.hxx:15,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/memory_allocator.hxx:9,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/vector.hxx:4,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/mrf/pairwise_simplex_factor.h:4,
                 from /export/home/jschnell/masterarbeit/code/LPMP/src/mrf/pairwise_simplex_factor.cpp:1:
/net/hciserver03/storage/jschnell/software/gccbin/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/avxintrin.h: In function ‘void simdpp::arch_avx2::detail::insn::i_load(simdpp::arch_avx2::float64x4&, const char*)’:
/net/hciserver03/storage/jschnell/software/gccbin/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/avxintrin.h:859:1: error: inlining failed in call to always_inline ‘__m256d _mm256_load_pd(const double*)’: target specific option mismatch
 _mm256_load_pd (double const *__P)
```

This can be fixed by adding
```
add_definitions(-mavx)
```
in the CMakeLists.txt

However, when trying to execute one of the resulting binaries, for example `multigraph_matching_tightening_mp`, I get an `Illegal instruction (core dumped)`.
I have no clue on how to debug this and am glad about any help I can get :))"
A confusion about other image transformations,cihangxie/DI-2-FGSM,2021-05-13 14:47:45,0,,9,891106220,"Hi, i have a confusion when running this paper’s experiments.
when i set the transformation to flip or rotation, the gradient of the adversarial examples is none. 
What should i do? Thank you so much!"
Questions about evaluation on awa2,cyvius96/DGP,2021-03-25 09:26:45,3,,17,840733777,Q1：If i want to evaluate on awa2，should i still train on imagenet？
RuntimeError: Caught RuntimeError in replica 1 on device 1.,anuragranj/cc,2021-05-30 13:30:18,0,,27,906756955," python3 train.py /media/disk1/xgl/cc-pil/formatted --dispnet DispResNet6 --posenet PoseNetB6   --masknet MaskNet6 --flownet Back2Future --pretrained-disp /media/disk1/xgl/cc-pil/geometry/dispnet_k.pth.tar   --pretrained-pose /media/disk1/xgl/cc-pil/geometry/posenet.pth.tar --pretrained-flow /media/disk1/xgl/cc-pil/geometry/back2future.pth.tar   --pretrained-mask /media/disk1/xgl/cc-pil/geometry/masknet.pth.tar -b4 -m0.1 -pf 0.5 -pc 1.0 -s0.1 -c0.3   --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997 --with-flow-gt   --with-depth-gt --epochs 100 --smoothness-type edgeaware  --fix-masknet --fix-flownet   --log-terminal --name EXPERIMENT_NAME
=> will save everything to checkpoints/EXPERIMENT_NAME
=> fetching scenes in '/media/disk1/xgl/cc-pil/formatted'
588 samples found in 5 train scenes
154 samples found in 1 valid scenes
=> creating model
=> using pre-trained weights for explainabilty and pose net
=> using pre-trained weights for explainabilty and pose net
=> using pre-trained weights from /media/disk1/xgl/cc-pil/geometry/dispnet_k.pth.tar
=> using pre-trained weights for FlowNet
=> setting adam solver


N/A% (0 of 100) |                                                                                                                                                     | Elapsed Time: 0:00:00 ETA:  --:--:--


N/A% (0 of 147) |                                                                                                                                                     | Elapsed Time: 0:00:00 ETA:  --:--:--


N/A% (0 of 38) |                                                                                                                                                      | Elapsed Time: 0:00:00 ETA:  --:--:--

/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:2941: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn(""nn.functional.upsample is deprecated. Use nn.functional.interpolate instead."")
/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  ""See the documentation of nn.Upsample for details."".format(mode))
/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn(""nn.functional.sigmoid is deprecated. Use torch.sigmoid instead."")
/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:3384: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(""Default grid_sample and affine_grid behavior has changed ""
Traceback (most recent call last):
  File ""train.py"", line 784, in <module>
    main()
  File ""train.py"", line 353, in main
    train_loss = train(train_loader, disp_net, pose_net, mask_net, flow_net, optimizer, args.epoch_size, logger, training_writer)
  File ""train.py"", line 463, in train
    flow_fwd, flow_bwd, _ = flow_net(tgt_img_var, ref_imgs_var[1:3])
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/_utils.py"", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
100% (100 of 100) |###################################################################################################################################################| Elapsed Time: 0:00:04 Time:  0:00:04
    output = module(*input, **kwargs)
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
100% (147 of 147) |###################################################################################################################################################| Elapsed Time: 0:00:04 Time:  0:00:04
  File ""/media/disk1/xgl/cc-pil/models/back2future.py"", line 174, in forward
    corr6_fwd = corr6_fwd.index_select(1,self.idx_fwd)
100% (38 of 38) |#####################################################################################################################################################| Elapsed Time: 0:00:04 Time:  0:00:04




Excuse me, can you help me solve this problem? thank you very much."
about log-terminal,anuragranj/cc,2020-07-09 01:32:04,0,,26,653695844,"When I set --log-terminal=True, the following error is occur

self.epoch_bar = progressbar.ProgressBar(maxval=n_epochs, fd=Writer(self.t, (0, h-s+e)))

TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'

and I find that in logger.py line16,   h = self.t.height,  then the h is None

Could you help me with this? Thank you very much"
Large EPE for optical flow evaluation,anuragranj/cc,2020-02-04 00:03:31,0,,23,559408092,"Hey Anurag, thanks for your wonderful work!

I got the following results when running test_flow.py:

| | epe_total        | epe_sp           | epe_mv  | Fl  | epe_total_gt_mask | epe_sp_gt_mask | epe_mv_gt_mask | Fl_gt_mask |
| ------------- |:-------------:|:-----:|:-------------:|:-------------:|:-----:|:-----:|:-----:|:-----:|
|Errors | 81.6802 |  6.4460 | 174.5231 | 0.6040 | 36.2934 | 6.3183| 183.9449 | 0.4385 |

This seems pretty large, especially the EPE. 

I run the evaluation in the same way as mentioned in the readme, with the following script:
```
model_dir=/path/to/cc_models/geometry
cd /path/to/cc/
python test_flow.py \
    --pretrained-disp=""$model_dir/dispnet_cs_k.pth.tar"" \
    --pretrained-pose=""$model_dir/posenet.pth.tar"" \
    --pretrained-mask=""$model_dir/masknet.pth.tar"" \
    --pretrained-flow=""$model_dir/back2future.pth.tar"" \
    --kitti-dir=/path/to/kitti_data/
```
All other settings are set as default. My PyTorch and cuda version are 1.4.0 and 10.0.

Is there something wrong? Or I miss something?

Thanks in advance!
Best,

"
Runing test_flow on a real-world video ,anuragranj/cc,2020-01-14 02:45:51,1,,21,549297994,"Hi,

Is there any demo script for producing optical flow on a test video? Specifically, I am interested in running test_flow.py on a video but the current code reads data (tgt_img, ref_imgs, intrinsics, intrinsics_inv) from a validation dataset loader. How do I extract these information from a test video?

Thanks"
" viz3 = np.vstack((255*tgt_img_viz, 255*depth_viz, 255*mask_viz...   in test_mask",anuragranj/cc,2019-10-27 09:41:14,3,,17,512943313,"Hello, there seems something wrong with ""vstack"". The codes reports error as follows:

viz3_im = Image.fromarray(viz3.astype('uint8'))
TypeError: Cannot handle this data type

Can you help me with this, thanks!"
Versions of torch and torchvision in the experiment implementation,anuragranj/cc,2019-10-24 01:25:52,3,,16,511651197,"Hi anuragranj,

The requirements.txt of your code denotes the necessary packages for your code's implementation. However I encoutered problems when installing the spatial-correlation-sampler, which may be a result of version conflict of spatial-correlation-sampler v0.2.1 and torch v0.4.1.

So I wonder your package versions of your spatial-correlation-sampler, torch and torchvision. I want to specify them to successfully install them. 

Thanks!"
How to train my own dataset based on your pre-training model?,anuragranj/cc,2019-10-11 09:22:38,0,,15,505739276,"I have an indoor stereo image dataset with a total of 8,000 images. There is a salient mobile robot in the dataset scene that has been moving within the camera field. I have resized my dataset image to the size required by your code. I am training my model with the parameters in your open source code based on your pre-training model. I want to get a model that can perform good indoor monocular depth estimation and can segment the moving robot. But the model obtained is very poor and I can't see anything visually. Do you have any suggestions?"
scipy.misc.imresize removed in scipy 1.3.0.,anuragranj/cc,2019-07-28 20:08:44,7,,14,473780700,"imresize is no longer provided by scipy. If the intention is to support 1.3.0 and above, I suggest to switch to Pillow or something else."
the output images' format should be changed from CHW to HWC,anuragranj/cc,2019-06-13 20:01:32,8,,10,455927343,"Tensors in pytorch are formatted in CHW(BCHW) by default, so if you wanna output the results of depth,flow and mask, you should change them into HWC format.
such as:
 test_flow.py line 180
>row1_viz_im = Image.fromarray((255*row1_viz).astype('uint8'))
row2_viz_im = Image.fromarray((row2_viz).astype('uint8'))

this will raise TypeError(""Cannot handle this data type"")
you should transpose/permute the format into HWC like this below:

>row1_viz_im = Image.fromarray((255*row1_viz).astype('uint8').transpose((1,2,0)))
row2_viz_im = Image.fromarray((row2_viz).astype('uint8').transpose((1,2,0)))
"
【验收相关ISSUE】,ELKYang/2s-AGCN-paddle,2022-04-24 06:31:06,1,,1,1213561761,1. 方便创建一个AIStudio项目吗，便于这边验收
When the code and model can be released ?,anshulbshah/Blurred-Image-to-Video,2020-08-18 03:27:48,0,,2,680659791,
Great Work! Will the code and model be released,anshulbshah/Blurred-Image-to-Video,2019-04-20 09:45:32,4,,1,435378243,I read your paper and I think this is really inspiring work. I really hope to try the model and test some ideas. So I'm wondering whether/when will the code or model described in paper be released?
Is this algorithm suitable for find SIM3 matrix from 3D points pairs?,danini/magsac,2022-10-21 08:34:39,0,,25,1417970405,"hii, thank for your sharing, 
Many of the strategies for optimizing result  of the transformation estimation in the image problem.
the algorithm suitable for find SIM3 matrix from two group 3D points pair in coordinate transforamton problem?
"
Is it using for Roust Line Regression?,danini/magsac,2021-10-29 11:21:04,0,,24,1039461489,
Make Error!!!    Missing header file!!,danini/magsac,2021-09-15 01:00:00,5,,23,996555809,"Hello,everyone,Why the project I downloaded is missing some header files????????????  Can someone post me? like this:

-- The CXX compiler identification is GNU 7.5.0
-- Check for working CXX compiler: /usr/local/bin/c++
-- Check for working CXX compiler: /usr/local/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
CMake Warning (dev) at CMakeLists.txt:9 (option):
  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake
  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to
  set the policy and suppress this warning.

  For compatibility with older versions of CMake, option is clearing the
  normal variable 'CREATE_SAMPLE_PROJECT'.
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found OpenCV: /opt/ros/kinetic (found version ""3.3.1"") 
Found Gflags 2.2.2
-- > GFLAGS_INCLUDE_DIR:   /usr/local/include
-- > GFLAGS_LIBRARIES:   gflags_shared
Glog library found.
-- Found OpenMP_CXX: -fopenmp (found version ""4.5"") 
-- Found OpenMP: TRUE (found version ""4.5"")  
-- The C compiler identification is GNU 7.5.0
-- Check for working C compiler: /usr/local/bin/gcc
-- Check for working C compiler: /usr/local/bin/gcc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Found PythonInterp: /usr/bin/python3.5 (found version ""3.5.1"") 
-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.5m.so
-- pybind11 v2.1.1
-- Performing Test HAS_CPP14_FLAG
-- Performing Test HAS_CPP14_FLAG - Success
-- Performing Test HAS_CPP11_FLAG
-- Performing Test HAS_CPP11_FLAG - Success
-- Performing Test HAS_FLTO
-- Performing Test HAS_FLTO - Success
-- LTO enabled
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jjb/envirement/magsac/build
jjb@jjb:~/envirement/magsac/build$ make
Scanning dependencies of target GraphCutRANSAC
[  5%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/GCoptimization.cpp.o
[ 10%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/LinkedBlockList.cpp.o
[ 15%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/gamma_values.cpp.o
/home/jjb/envirement/magsac/graph-cut-ransac/src/pygcransac/include/gamma_values.cpp:1:9: 警告：#pragma once 出现在主文件中
 #pragma once
         ^~~~
[ 21%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/graph.cpp.o
[ 26%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/maxflow.cpp.o
[ 31%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/relative_pose/bundle.cpp.o
[ 36%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/relative_pose/colmap_models.cpp.o
[ 42%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/relative_pose/essential.cpp.o
[ 47%] Linking CXX static library libGraphCutRANSAC.a
[ 47%] Built target GraphCutRANSAC
Scanning dependencies of target pymagsac
[ 52%] Building CXX object CMakeFiles/pymagsac.dir/src/pymagsac/src/bindings.cpp.o
[ 57%] Building CXX object CMakeFiles/pymagsac.dir/src/pymagsac/src/magsac_python.cpp.o
**/home/jjb/envirement/magsac/src/pymagsac/src/magsac_python.cpp:3:10: 致命错误：fundamental_estimator.h：没有那个文件或目录
 #include ""fundamental_estimator.h""**
"
Magsac package for windows10 please,danini/magsac,2021-06-11 18:27:37,0,,21,919071754,"I tried building it for windows10 using VC-2015. But ran in to issues because of gflags.
It would be great if author can release windows package or even package installable via pip .
"
Essential matrix example code fails,danini/magsac,2021-04-29 18:51:41,0,,20,871336322,"Hi,

When I'm running the example code in example_essential_matrix.ipynb i receive the following error 

![image](https://user-images.githubusercontent.com/42467164/116602743-69959d00-a92c-11eb-8102-3137e6217d92.png)

I have experimented a bit with trying to find the correct arguments, but with no luck so I am posting it here. 

Thanks in advance for any help!"
Very verbose MAGSAC++ - is it possible to turn it off?,danini/magsac,2021-01-27 01:30:35,3,,17,794689746,"Hello. Whenever I run `pymagsac.findHomography' with `use_magsac_plus_plus=True` I always get really verbose prints:

```
Setting the core number for MAGSAC++ is deprecated.
Setting the partition number for MAGSAC++ is deprecated.
```

Is there a way to suppress this? I believe the prints are coming from [here](https://github.com/danini/magsac/blob/master/src/pymagsac/include/magsac.h#L100), but it's been hard-coded [here](https://github.com/danini/magsac/blob/master/src/pymagsac/src/magsac_python.cpp#L33).

Perhaps the following lines (L33-34) can be deleted if `use_magsac_plus_plus` is false?

```
    magsac->setCoreNumber(1); // The number of cores used to speed up sigma-consensus
    magsac->setPartitionNumber(partition_num); // The number partitions used for speeding up sigma consensus. As the value grows, the algorithm become slower and, usually, more accurate.
```

I'd be happy to create a PR if you think it'd be useful."
Make failed due to C++17 standard thing ....,danini/magsac,2020-11-04 08:00:16,2,,15,735898014,"Hi, @danini and @ducha-aiki , 

Thanks for sharing your great work! 

Unfortunatelly, issues raised when I compile it on linux machine with gcc 5.2.0. I am stuck and any advice would be appreciated!  

Thanks! 

Appended is the error: 

`$ make -j4`

[ 10%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/GCoptimization.cpp.o
[ 20%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/graph.cpp.o
[ 30%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/LinkedBlockList.cpp.o
[ 40%] Building CXX object CMakeFiles/MAGSAC.dir/include/gamma_values.cpp.o
[ 50%] Building CXX object CMakeFiles/MAGSAC.dir/include/magsac.cpp.o
[ 60%] Building CXX object CMakeFiles/MAGSAC.dir/include/model_score.cpp.o
[ 70%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/maxflow.cpp.o
[ 80%] Building CXX object CMakeFiles/MAGSAC.dir/src/main.cpp.o
[ 90%] Linking CXX static library libGraphCutRANSAC.a
[ 90%] Built target GraphCutRANSAC
In file included from /home/Code/magsac/include/magsac.cpp:1:0:
/home/Code/magsac/include/magsac.h: In member function ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensusPlusPlus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&)’:
/home/Code/magsac/include/magsac.h:743:20: error: expected unqualified-id before ‘[’ token
   for (const auto &[residual, idx] : residuals)
                    ^
/home/Code/magsac/include/magsac.h:743:20: error: expected ‘;’ before ‘[’ token
/home/Code/magsac/include/magsac.h:743:21: error: ‘residual’ was not declared in this scope
   for (const auto &[residual, idx] : residuals)
                     ^
/home/Code/magsac/include/magsac.h:743:31: error: ‘idx’ was not declared in this scope
   for (const auto &[residual, idx] : residuals)
                               ^
/home/Code/magsac/include/magsac.h: In lambda function:
/home/Code/magsac/include/magsac.h:743:36: error: expected ‘{’ before ‘:’ token
   for (const auto &[residual, idx] : residuals)
                                    ^
/home/Code/magsac/include/magsac.h: In member function ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensusPlusPlus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&)’:
/home/Code/magsac/include/magsac.h:743:36: error: expected ‘;’ before ‘:’ token
/home/Code/magsac/include/magsac.h:743:36: error: expected primary-expression before ‘:’ token
/home/Code/magsac/include/magsac.h:743:36: error: expected ‘)’ before ‘:’ token
/home/Code/magsac/include/magsac.h:743:36: error: expected primary-expression before ‘:’ token
make[2]: *** [CMakeFiles/MAGSAC.dir/include/magsac.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/Code/magsac/graph-cut-ransac/src/pygcransac/include/fundamental_estimator.h:52:0,
                 from /home/Code/magsac/graph-cut-ransac/src/pygcransac/include/types.h:39,
                 from /home/Code/magsac/graph-cut-ransac/src/pygcransac/include/utils.h:48,
                 from /home/Code/magsac/src/main.cpp:16:
/home/Code/magsac/graph-cut-ransac/src/pygcransac/include/GCRANSAC.h: In member function ‘void gcransac::GCRANSAC<_ModelEstimator, _NeighborhoodGraph, _ScoringFunction, _PreemptiveModelVerification>::run(const cv::Mat&, const _ModelEstimator&, gcransac::sampler::Sampler<cv::Mat, long unsigned int>*, gcransac::sampler::Sampler<cv::Mat, long unsigned int>*, const _NeighborhoodGraph*, gcransac::Model&, _PreemptiveModelVerification&)’:
/home/Code/magsac/graph-cut-ransac/src/pygcransac/include/GCRANSAC.h:305:8: error: expected ‘(’ before ‘constexpr’
     if constexpr (!std::is_same<preemption::EmptyPreemptiveVerfication<_ModelEstimator>, _PreemptiveModelVerification>())
        ^
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
 }
 ^
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
/home/Code/magsac/src/main.cpp: At global scope:
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
make[2]: *** [CMakeFiles/MAGSAC.dir/src/main.cpp.o] Error 1
make[1]: *** [CMakeFiles/MAGSAC.dir/all] Error 2
make: *** [all] Error 2"
Difference between paper and code,danini/magsac,2020-11-03 09:02:57,0,,14,735142440,"Hi,

I have been studying your paper, thanks for uploading your code with it !

I was wondering, when you apply SigmaConsensus, in the paper you use the formula (6) to compute weights, but in the code you only use the exponential part of this formula. Why is the factor \sigma^{-\rho} D(\theta, \sigma)^{\rho - 1} ignored in the computation of the weights ?

Best regards,
Clément Riu."
Rigid Transformation,danini/magsac,2020-10-09 12:40:44,0,,13,718114273,"Thanks a lot for your great work and code, do you have a plan to add support for rigid transformation estimation? 

Best,
Xuyang"
Essential Matrix estimation for equi-rectangular projection cameras,danini/magsac,2020-09-02 22:26:08,1,,12,691476258,"Is there a way to estimate Essential Matrix using DefaultEssentialMatrixEstimator() for cameras which don't have camera intrinsic matrices (for eg, 360 degree cameras)"
Just out of curiosity,danini/magsac,2019-06-30 16:02:27,0,,2,462420288,"Hi,
your paper is very interesting, thanks for sharing the code!

Just out of curiosity. In some of my old experiments (see ""[noRANSAC for fundamental matrix estimation](http://www.bmva.org/bmvc/2011/proceedings/paper98/paper98.pdf)"", Table 5), I found that the geometric epipolar error (i.e. sqrt(e12^2+e21^2)) is usually more robust than the symmetric epipolar error. Did you also try some kind of similar error with MAGSAC?

Thanks again,
Fabio "
Code to reflect results from Fig. 5 from paper,zihaoliu123/DeepN-Jpeg,2020-07-01 20:45:29,1,,1,649308515,"Hi, 

Thanks for sharing the code for delta calculation, could you also share the code for getting Fig.5 results from paper, I implemented the same, however I got different results. "
paddle inference demo 无法运行,ZhaoJ9014/face.evoLVe,2022-10-24 06:19:08,1,,188,1420283242,"![Snip20221024_11](https://user-images.githubusercontent.com/7626261/197460230-823c48ee-98da-4aa8-94c7-6829a359fed7.png)
"
Private Asia Face Data,ZhaoJ9014/face.evoLVe,2022-08-27 04:07:44,0,,187,1352934555,"Hi @ZhaoJ9014,
Thank your project.
I want to train IR50 with ""Private Asia Face Data"" from scratch. 
So, can you provide Private Asia Face Data for me?
Thank you."
How to implement the triple loss?,ZhaoJ9014/face.evoLVe,2022-06-19 13:16:34,1,,186,1276059581,"The README.md wrote:

> This repo provides a comprehensive face recognition library for face related analytics \& applications, including face alignment (detection, landmark localization, affine transformation, *etc.*), data processing (*e.g.*, augmentation, data balancing, normalization, *etc.*), various backbones (*e.g.*, [ResNet](https://arxiv.org/pdf/1512.03385.pdf), [IR](https://arxiv.org/pdf/1512.03385.pdf), [IR-SE](https://arxiv.org/pdf/1709.01507.pdf), ResNeXt, SE-ResNeXt, DenseNet, [LightCNN](https://arxiv.org/pdf/1511.02683.pdf), MobileNet, ShuffleNet, DPN, *etc.*), various losses (*e.g.*, Softmax, [Focal](https://arxiv.org/pdf/1708.02002.pdf), Center, [SphereFace](https://arxiv.org/pdf/1704.08063.pdf), [CosFace](https://arxiv.org/pdf/1801.09414.pdf), [AmSoftmax](https://arxiv.org/pdf/1801.05599.pdf), [ArcFace](https://arxiv.org/pdf/1801.07698.pdf), `Triplet`, *etc.*)

However, we found no triple loss in the repo. Could you tell us how to implement the triple loss?

Thanks."
how to do inference on webcam?,ZhaoJ9014/face.evoLVe,2022-06-09 13:46:11,0,,185,1266160905,"Hi,

I have trained custom model but i dont know how to do inference on web cam. Can anyone share snippet to do inference on webcam ?

thanks advance"
CFP aligned dataset,ZhaoJ9014/face.evoLVe,2022-05-09 17:08:20,1,,184,1230009658,WARNING! Last 12 images of dataset are just gray pixels...
Could you please help with pretrained model of IR_SE_50? ,ZhaoJ9014/face.evoLVe,2022-03-14 15:13:41,0,,183,1168523396,"Thanks a lot for the wonderful project. Could you please help with a pre-trained model of IR_SE_50 (and if possible 152) as well? I see only IR_50 made available in the model zoo. If not available, a short explanation of why only some models are made available and some not could help for my learning purposes.

Thanks,"
戴口罩的人脸识别有做嘛?,ZhaoJ9014/face.evoLVe,2022-02-17 06:27:32,0,,182,1140928395,"随着疫情,戴口罩人脸识别的应用场景越来越多了.你们有没有相关研究,去年10月份有进行比赛,可是比赛都没有开源."
convert model to tensorflow lite,ZhaoJ9014/face.evoLVe,2021-12-29 02:27:27,0,,181,1090227399,"Hi @ZhaoJ9014,
Thank your great project
I want to convert model IR50 to TensorFlow lite (dynamic quantize) and use it in mobile. I tried but failed, maybe the model was not supported. Have you converted it yet? Please, help me.
Thank you."
Mistake in the focal loss implementation?,ZhaoJ9014/face.evoLVe,2021-12-15 12:33:07,0,,180,1080986838,"Hi, thank you very much for releasing this codebase -- it has been very useful to my project.

I'm wondering if there is a mistake in the Focal Loss implementation. The code in `loss/focal.py` first calculates CrossEntropy loss, **averages it over all samples**, and then applies a modulating factor `loss = (1 - p) ** self.gamma * logp`. If I understand the original [Focal Loss paper](https://arxiv.org/pdf/1708.02002.pdf) correctly, they propose to calculate CrossEntropy, apply the modulating factor, and **only then** average the result over all samples. 
I wonder, is the order changed on purpose in this repository? I think this way it might be losing the idea of Focal loss...

If it's actually a mistake, a simple fix in the line https://github.com/ZhaoJ9014/face.evoLVe/blob/63520924167efb9ef53dcceed0a15cf739cad1c9/loss/focal.py#L13

to `self.ce = nn.CrossEntropyLoss(reduction='none')` will suffice.

Other implementations also seem to be having a different order, e.g. see [1](https://github.com/clcarwin/focal_loss_pytorch/blob/e11e75bad957aecf641db6998a1016204722c1bb/focalloss.py#L34) and [2](https://amaarora.github.io/2020/06/29/FocalLoss.html). "
人脸识别的PyTorch版本的使用案例可以有吗,ZhaoJ9014/face.evoLVe,2021-11-24 13:24:43,0,,179,1062426914,只能找到人脸特征点的案例，但是找不到人脸识别的案例，人脸比对，
UserWarning: Implicit dimension choice for softmax has been deprecated,ZhaoJ9014/face.evoLVe,2021-11-15 09:01:54,0,,178,1053380857,"face.evoLVe/applications/align/get_nets.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  a = F.softmax(a)

specifically in this case, how many output neuron number of this model.
I using Onet, Pnet and Rnet
Btw, i don't see any flatten layer or linear layer in Onet, Pnet or Rnet
Thanks you
"
Can you provide Head Resume of IR50 asian model?,ZhaoJ9014/face.evoLVe,2021-10-14 11:05:28,0,,177,1026261841,"Thanks for your project.
I want to train model IR-50 on my own dataset use the pre-trained model (IR-50 on Asia face data). But I cannot find the file head resume.
Can you provide it?
Thank you."
batchsize for detect and align,ZhaoJ9014/face.evoLVe,2021-10-09 08:37:42,0,,176,1021648455,I'd like to know whether the detection and location of human face can only be processed with a single image. How can we improve efficiency?
cannot run main.py in paddle project,ZhaoJ9014/face.evoLVe,2021-09-19 19:33:16,0,,175,1000398911,"Hi, 

when trying to run the main.py, I am getting the following error:

```
Traceback (most recent call last):
  File ""main.py"", line 157, in <module>
    test = FaceEval()
  File ""main.py"", line 13, in __init__
    self.mtcnn = MTCNN()
  File ""/../face.evoLVe.PyTorch/paddle/PaddleInference-demo/utils.py"", line 23, in __init__
    self.pnet = init_predictor('../model/PNet')
  File ""/../face_recognition/face.evoLVe.PyTorch/paddle/PaddleInference-demo/utils.py"", line 17, in init_predictor
    predictor = inference.create_predictor(config)
RuntimeError: (NotFound) Cannot open file ../model/PNet.pdmodel, please confirm whether the file is normal.
  [Hint: Expected static_cast<bool>(fin.is_open()) == true, but received static_cast<bool>(fin.is_open()):0 != true:1.] (at /paddle/paddle/fluid/inference/api/analysis_predictor.cc:915)
```

anyone having the same issue?
thanks in advance"
How to do inference using head on same faces?,ZhaoJ9014/face.evoLVe,2021-09-16 06:58:04,0,,174,997844957,"`def forward(self, input, label).`

During inference time, we won't know the value of the label. How to make inference in such cases?
Should we only return the value of `cosine = F.linear(F.normalize(input), F.normalize(self.weight))`"
Can not  load model  in paddle project,ZhaoJ9014/face.evoLVe,2021-09-14 07:51:13,1,,173,995704874,"thanks for your nice work!
i have some trouble which  happen in load_model module  when run paddle code of finetuning process. 
For example, i saved 10 mid-models during training. there maybe 8 models cannot be load.
waitting for your reply!"
mult_gpu_training.py报错,ZhaoJ9014/face.evoLVe,2021-09-08 09:46:59,0,,172,990946975,"File ""mult_gpu_training.py"", line 51, in <module>
    CLIP = cfg['CLIP']
KeyError: 'CLIP'

应该是config里没有定义"
paddle/align缺少first_stage.py文件,ZhaoJ9014/face.evoLVe,2021-09-06 10:29:21,0,,171,989025886,paddle/align缺少first_stage.py文件，导致运行face_align报错
how to download head_arcface.pth,ZhaoJ9014/face.evoLVe,2021-08-09 02:07:53,0,,169,963584380,
rgb conversion in perform_val,ZhaoJ9014/face.evoLVe,2021-08-07 17:02:25,0,,168,963266977,"Isn't it right to add `[:, [2, 1, 0], :, :]` at the 199th line in utils.py?

https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/a33a9121198ed354eb6b0d7c214443f09908ccc1/util/utils.py#L188
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/a33a9121198ed354eb6b0d7c214443f09908ccc1/util/utils.py#L199

I think rgb conversion is also needed to the tensors not included in the batch"
please！help me！how  to get head_arcface.pth and it's backbone is IR_50? baidu.drive and google drive link not including the file,ZhaoJ9014/face.evoLVe,2021-08-04 08:51:45,0,,167,960089260,"<img width=""674"" alt=""dfcefa7b73dba0de033bf15b84aebe5"" src=""https://user-images.githubusercontent.com/61956857/128151518-afc6599b-0bbf-4353-b5eb-f717f3fdea5f.png"">


I want to resume the model IR_50
"
casia surf test set password,ZhaoJ9014/face.evoLVe,2021-07-22 10:10:48,0,,166,950500942,"I have downloaded the casia surf dataset and also managed to unzip the train and val datasets using the password provided in https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/issues/81 but I am unable to unzip the test set. I tried `casia-cefa-test-572121-usd89f` but it says incorrect password.

I would greatly appreciate if someone can provide the password for test.zip

thank you.

rachel
"
How to test the performance?,ZhaoJ9014/face.evoLVe,2021-07-07 05:56:58,0,,164,938515955,"Hi @ZhaoJ9014, I am not familiar with this task, just want to attack the model, could you provide a test code with your provided checkpoint? Thanks!"
Pair of FGNET and MORPH,ZhaoJ9014/face.evoLVe,2021-06-16 16:03:10,0,,160,922804023,I have both datasets and I want to check the performance. Please let me know how I can make face pairs and .bin files?
What is this model for? What should i do if i want to use it to determine persons' identities in one picture?,ZhaoJ9014/face.evoLVe,2021-06-10 12:22:39,0,,157,917283623,
face_align.py not consider multiple face bbox,ZhaoJ9014/face.evoLVe,2021-06-04 01:04:10,0,,156,910977126,"I can not get the same result on LFW 112x112 or 112x96 aligned dataset  when I use face_align.py,  about 96% or 97%。 Then I found the face_align.py only use the first face bbox when MTCNN found multiple face， which results in error face label in generated LFW dataset。 

After processing multiple face (get the max bbox arear one), I get ACC: 99.2 on LFW 112x112 dataset using Mobilenetface"
How to continue to train from scratch?,ZhaoJ9014/face.evoLVe,2021-06-01 08:04:36,8,,155,908022119,"Can someone teach me? I just started to learn face recognition. 
Issues: I have already trained the model for  171 epochs. And I want to continue to train the model based on the latest model. How do I set the 
      BACKBONE_RESUME_ROOT = './', # the root to resume training from a saved checkpoint
      HEAD_RESUME_ROOT = './', # the root to resume training from a saved checkpoint
in the file of the config.py.


                                                                                                                                                                             Thank you very much"
How do i apply the model i obtained by training,ZhaoJ9014/face.evoLVe,2021-05-25 03:18:22,2,,154,900217978,"I'm new to machine learning, so it would be a great help if anyone can tell me where the model is stored and how can i apply this model, say, use one image and get a picture with faces illustrated and marked with names?
Great thanks!"
how to generate the data like lfw-aligned-112x112 ,ZhaoJ9014/face.evoLVe,2021-04-25 02:57:53,0,,150,866894702,
Best threshold meaning,ZhaoJ9014/face.evoLVe,2021-03-10 11:13:18,0,,149,827560389,"Hi.

I don't really understand the meaning of the ""best threshold"" metric when I visualize results in Tensorboard. What exactly does it mean? Can I convert it somehow to optimal cosine similarity threshold for model inference? 

Thanks for your answers."
where is the code for MPnet,ZhaoJ9014/face.evoLVe,2021-02-28 13:49:55,2,,148,818210874,where is the code for MPnet(<<Multi-Prototype Networks for Unconstrained Set-based Face Recognition>>)?
How you clean IJB-A dataset? ,ZhaoJ9014/face.evoLVe,2021-02-19 08:25:36,1,,147,811813194,"Thanks for your great work, would  you please tell me how you cleaned the IJB-A dataset?"
请问有SiW的数据(anti-spoofing)吗？,ZhaoJ9014/face.evoLVe,2021-02-15 09:20:52,3,,146,808353940,"数据：http://cvlab.cse.msu.edu/siw-spoof-in-the-wild-database.html
SiW: Spoofing in the Wild Database
真心希望可以分享一下这份anti-spoofing数据，找了很久都找不到
有资源的请联系我 121458737@qq.com, 有偿"
Do you plan to share a mobilefacenet-arcface model?,ZhaoJ9014/face.evoLVe,2021-02-05 12:42:53,0,,145,802134625,"Your trained resnet50 model with ""private asian dataset""  is just amazing, I can only guess the dataset you used must be huge.

Do you have any plans to share a mobilefacenet backboned pretrained model (with that same dataset)?"
FileNotFoundError: [Errno 2] No such file or directory: '/home/dataset/lfw/meta/sizes',ZhaoJ9014/face.evoLVe,2021-01-29 01:12:44,2,,144,796495435,"Hello, I meet the error about utility.py as i've seen few people metcthe same. 
My error message is FileNotFoundError: [Errno 2] No such file or directory: '/home/dataset/lfw/meta/sizes', but i just download the data from your link and ensure its right data syntax.
Can someone help me with this question? Thanks a lot!"
Maxpool shortcut is redundant,ZhaoJ9014/face.evoLVe,2020-12-31 06:19:44,0,,143,776811595,"In the Bottleneck_IR_SE , the shortcut layer of Maxpool is always with kernel=1 and stride=1. So the layer is useless.
Am I missing something?"
Use of option tta for pre-trained networks,ZhaoJ9014/face.evoLVe,2020-12-10 18:51:14,2,,142,761513334,"In your code (for instance extract_feature), you use a parameter tta. If it is set, the embeddings of both the imag and its flipped version are computed and added. 
`if tta:
            emb_batch = backbone(ccropped.to(device)).cpu() + backbone(flipped.to(device)).cpu()`

Have the pre-trained models beed trained on this variant? Should I expect an improved accuracy when using this option?  "
CASIA-SURF unzipping password,ZhaoJ9014/face.evoLVe,2020-12-08 17:12:31,1,,141,759625040,I just downloaded the CASIA-SURF dataset from the Baidu link that you provided. It requires a password to unzip the files. I was hoping you could also provide the password for unzipping the files.
Anything about Verification?,ZhaoJ9014/face.evoLVe,2020-11-10 03:30:53,0,,140,739539587,"Hi, every contributor on this work
Thanks for your work on face recognition, I have some issues on verification and need your help.
From your code on verification.py, it seems that ""actual_issamne"" is a prerequisite. Meanwhile, the definition ""get_val_pair"" in util.py  appears ""issame"". How do you generate the ""{}/{}_list.npy"" file? Could you provide me with an example img_list for paired images?
I'm looking forward to your reply, thanks."
IR-152 on Google Drive,ZhaoJ9014/face.evoLVe,2020-11-09 10:38:57,4,,139,738908680,"Dear authors,

First of all, thanks for distributing your models!

Would it be possible to distribute the IR-152 model on Google Drive?
Downloading it from Baidu outside of China is not straightforward.

Thank you!"
"How do I prepare my input data? I have a folder of pngs, what do I do with them?",ZhaoJ9014/face.evoLVe,2020-11-07 16:41:26,10,,138,738275580,"I tried putting the images directly in the `data/` directory as instructed on the README.md page, but this just leads to the following error:

    FileNotFoundError: [Errno 2] No such file or directory: './data/faces_emore/lfw/meta/sizes'

Someone in [this issue](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/issues/20#issuecomment-522906481) suggested to use `prepare_data.py` from the following repository (which btw also is in this repo under backup/):

https://github.com/TreB1eN/InsightFace_Pytorch#323-prepare-dataset--for-training

But that seems to be unable to work with just `.png`s either, it seems to be looking for some sort of `.rec` file:

    mxnet.base.MXNetError: [17:27:25] src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open ""data/faces_emore/train.rec"": No such file or directory

Any advice? Thanks for your attention."
Embeddings1 and embeddings2 order,ZhaoJ9014/face.evoLVe,2020-10-23 17:49:09,0,,137,728405375,"Thanks for great repo,

In your code, you defined
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/d5e31893f7e30c0f82262e701463fd83d9725381/util/verification.py#L165
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/d5e31893f7e30c0f82262e701463fd83d9725381/util/verification.py#L166

Could you tell me which part corresponds to embeddings1 (index 0, 2, 4, ..., in lwf image) and which part is for embeddings2 (index 1,3,5...). In my opinion, the part is for embeddings1
https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/d5e31893f7e30c0f82262e701463fd83d9725381/util/utils.py#L188

and the part is for embeddings2

https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/d5e31893f7e30c0f82262e701463fd83d9725381/util/utils.py#L199


"
ResNet Implementation wrong layer dimension for INPUT_SIZE=224,ZhaoJ9014/face.evoLVe,2020-10-17 08:08:02,0,,136,723701788,"Hi, thanks for sharing your repo. 
When you define the ResNet model in [model_resnet.py](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/master/backbone/model_resnet.py#L91), you define the last layer of the Backbone as follows:

```python
if input_size[0] == 112:
    self.fc = Linear(2048 * 4 * 4, 512)
else:
    self.fc = Linear(2048 * 8 * 8, 512)
```
I notice that when the input_size is set to 224, it raises an error about the layer dimensions that I fixed setting:
``` python
self.fc = Linear(2048 * 7 * 7, 512)
```

Please let me know if it’s correct or if it may require further fixing. 
Regards"
Backup.metric's ArcFace vs head.metric ArcFace,ZhaoJ9014/face.evoLVe,2020-10-16 11:53:41,0,,135,723152851,"I noticed that there are two implementations of ArcFace head. One is in head/metric.py, and the other one is in backup/metric.py. It seems that the former one split the weight to multiple GPUS and the latter one didn't.  I want to know if the two implementations of training will bring difference in accurcay.
In my experiment, the backup's ArcFace can speed up the training, but its accuracy seems not as well as the head.metric's ArcFace.  Did anyone try the two different head implementations? "
FileNotFoundError: [Errno 2] No such file or directory: '/home/dataset/lfw/meta/sizes',ZhaoJ9014/face.evoLVe,2020-10-14 07:45:02,4,,134,721225561,"你好，我这边报了这个错误。

在https://github.com/ZhaoJ9014/face.evoLVe.PyTorch#Data-Zoo 这里下载了很多数据集试了都不能直接用，看了这个数据集的格式：
./data/db_name/
        -> id1/
            -> 1.jpg
            -> ...
        -> id2/
            -> 1.jpg
            -> ...
好像并没有问题，不知道需要如何修改，期待大佬的回复，感谢

FileNotFoundError: [Errno 2] No such file or directory: '/home/dataset/lfw/meta/sizes'"
添加DeepGlint的百度云链接？,ZhaoJ9014/face.evoLVe,2020-09-13 02:06:31,0,,132,700455171,如题，@ZhaoJ9014 你好，是否可以添加DeepGlint的百度云链接??
IDE,ZhaoJ9014/face.evoLVe,2020-07-29 06:44:47,0,,131,667595099,"Hi
Which IDE for your code?
Best regards,
PeterPham"
IJB-A dataset,ZhaoJ9014/face.evoLVe,2020-07-27 02:18:39,0,,129,665926462,Can anyone explain the 'clean' IJB-A dataset that provided in this git? The meaning of 'clean' and What are the changes compared to the raw data?
accuracy remains 0?,ZhaoJ9014/face.evoLVe,2020-07-20 11:31:39,0,,127,661712948,"First thanks for sharing your code!
But I met some problem!
When I was training the network on casia-web datasets, the  accuracy remained 0 on lfw datasets, how can I solve it?
Maybe I have not choose the right arcface param s(norm of input feature)? How to choose this parameter?
"
Unable to download datasets using wget,ZhaoJ9014/face.evoLVe,2020-07-15 11:32:29,0,,126,657279224,"Hi Team

Thanks for this wonderful repo. Really useful

We have been trying to download msceleb/vgg2 using wget. However, unable to download . From a GUI, this download is working.

Is there a way we can do this using Command line. We have a linux machine without GUI

Thanks

Rohit"
num_epoch = 125?,ZhaoJ9014/face.evoLVe,2020-07-14 03:06:39,0,,125,656287980,
Database Formatting - Urgent Help!,ZhaoJ9014/face.evoLVe,2020-06-25 21:23:49,2,,123,645859895,"Can anyone show me how to correctly format the database folder because I keep running into errors. 

```
Traceback (most recent call last):
  File ""train.py"", line 70, in <module>
    dataset_train = datasets.ImageFolder(os.path.join(DATA_ROOT, 'imgs'), train_transform)
  File ""/Users/royce.moon/evoLVeTest/lib/python3.7/site-packages/torchvision/datasets/folder.py"", line 206, in __init__
    is_valid_file=is_valid_file)
  File ""/Users/royce.moon/evoLVeTest/lib/python3.7/site-packages/torchvision/datasets/folder.py"", line 94, in __init__
    classes, class_to_idx = self._find_classes(self.root)
  File ""/Users/royce.moon/evoLVeTest/lib/python3.7/site-packages/torchvision/datasets/folder.py"", line 121, in _find_classes
    classes = [d.name for d in os.scandir(dir) if d.is_dir()]
FileNotFoundError: [Errno 2] No such file or directory: '/Volumes/storage/""Deep Learning""/data/imgs'
```"
Is redistribution of CASIA NIR-VIS 2.0 allowed ?,ZhaoJ9014/face.evoLVe,2020-06-02 05:37:33,1,,121,628915470,"Hi, I found CASIA NIR-VIS 2.0 dataset in your Data Zoo.
I wonder that redistribution of the dataset allowed or not, because the RELEASE AGREEMENT of the dataset says,

> 1. Redistribution: Without prior approval from Prof. Stan Z. Li, the creator of the database, the CASIA NIR-VIS 2.0 Database, either entirely or partly, should not be further distributed, published, copied, or disseminated in any way or form, no matter for profitable use or not, including further distribution to a different department or organization in the same system.
> (c.f. http://www.cbsr.ia.ac.cn/english/HFB_Agreement/NIR-VIS-2.0_agreements.pdf)

Have you get the approval from Prof. Stan Z. Li ?
Thank you.
"
The angle distances of different people are always between 0.4-0.6 ?,ZhaoJ9014/face.evoLVe,2020-05-19 03:02:54,0,,119,620639954,"I trained a network based on mobilenetv3 backbone using arcface header. When I loaded the weights from header and calculated the angle distances of different feature vectors, I found that the distances ranged from 0.4 to 0.6. What made me strange was that the max distance was not 1.0, and it seems that the high dimensional space was not fully occupied. What's more, I also do the experiment on provided IR152 pertained model, and I met the same situation. Does anyone meet this question? And how to make the distribution of embeddings sparse?
<img width=""495"" alt=""屏幕快照 2020-05-19 上午11 01 52"" src=""https://user-images.githubusercontent.com/28386207/82279927-40e8f180-99c0-11ea-94e3-9b639d51cee7.png"">"
The purpose of backup folder. ,ZhaoJ9014/face.evoLVe,2020-05-07 16:50:15,0,,118,614201461,"Hi,

Could you please let know the purpose of the backup folder? I see some interesting implementation there (such as dataparallel for HEAD etc). But why then is it not a part of the main code? Are we allowed to run that as well to check the performance etc?

Let me end with thanking you for the repo. Its really structured. However I am facing issues of slow training etc. Hence would help to answer of above (as hopefully I can speed up HEAD calculation as well).

Thanks,"
Training loss becomes nan after a few thousands iterations,ZhaoJ9014/face.evoLVe,2020-05-07 03:27:34,4,,117,613746969,"Thanks for your impressive contribution. I ran the 'train.py' script to directly train a ResNet50 model with 'msceleb_ align_112' training set.  The loss functions I applied are ArcFace and Focal loss. 
However, after about 3000 iterations, the value of loss suddenly turned to nan. I have no idea where the bug may exist. Hope for your help!

Here is my soft and hard enviroment information:
Python version: 3.6.5
PyTorch version: 1.3.0
cuda version: cuda10

GPU: TITAN X (Pascal)
 "
how to create your lfw aligned 112x112 dataset?,ZhaoJ9014/face.evoLVe,2020-04-10 10:35:10,5,,116,597822497,There are some images that contain more than 1 face. How did you filter out unwanted faces?
"plz, Can someone tell me how to test with 1 image ??",ZhaoJ9014/face.evoLVe,2020-04-06 06:54:29,2,,115,594851261,
how to set data,ZhaoJ9014/face.evoLVe,2020-04-05 07:44:49,1,,114,594313293,"i use lfw data,and i resize for images to 112*112,but when i use python train ,error:FileNotFoundError: [Errno 2] No such file or directory: './data/new_lfw/imgs',
what should i do?"
about data_pipe.py issues,ZhaoJ9014/face.evoLVe,2020-03-26 09:35:56,4,,113,588279518,"In data_pipe.py ,**load_mx_rec** function
        header, img = mx.recordio.unpack_img(img_info)### **img is BGR**
        label = int(header.label)
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) ### **img is RGB**
        img = Image.fromarray(img)
so train data is RGB.
But ,in **load_bin** function
        img = mx.image.imdecode(_bin).asnumpy() ### **img is RGB**
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) ### **img is BGR**
so test data is BGR.

Training data and test data formats are different. "
Training speed is too slow,ZhaoJ9014/face.evoLVe,2020-03-19 18:03:30,3,,111,584598934,"I got 1.3s/it+ when training. My bacth size is 400 and use 4 GPU and disabled ""dispaly training loss & acc every DISP_FREQ"".
I think 1.3s/it is too slow. Is there a solution？Thank you very much！
我的训练速度只有1.3s/it （tqbm显示）使用了4块1080Ti GPU，而且已经关闭了""dispaly training loss & acc every DISP_FREQ""（即每个iter更新准确率的功能）。但是仍然速度很慢只有1.3s/it。想问一下是我的操作有问题吗？有什么解决方法吗？谢谢您～"
BatchNorm1d in last layer of backbone cause tensorRT conversion failed,ZhaoJ9014/face.evoLVe,2020-01-09 03:27:14,1,,107,547225942," I am trying to convert model to tensorRT. In my exported onnx model, BatchNorm1d always comes along with unsqueeze before it, which is not supported by tensorRT5.
I noticed there is BatchNorm1d in output layer of backbones. From my understanding, it is not a must to get face embedding. While the loss failed to converge once it is removed. Could anyone explain it a little bit? "
sampler and focalLoss,ZhaoJ9014/face.evoLVe,2020-01-06 11:23:18,0,,106,545677827,I notice there is make_weights_for_balanced_classes to generate samplers to balance classes. While we can also make the balance with focal loss. So my question is do we really need both of them when training? Will focal loss be enough?
About custom dataset,ZhaoJ9014/face.evoLVe,2019-11-19 08:52:07,0,,102,524861579,Can anyone show me how to train on a custom dataset 
A possible typo in train.py,ZhaoJ9014/face.evoLVe,2019-11-07 17:35:08,0,,100,519411882,"In `train.py`, Line 61 to 68:
```python
    train_transform = transforms.Compose([ # refer to https://pytorch.org/docs/stable/torchvision/transforms.html for more build-in online data augmentation
        transforms.Resize([int(128 * INPUT_SIZE[0] / 112), int(128 * INPUT_SIZE[0] / 112)]), # smaller side resized
        transforms.RandomCrop([INPUT_SIZE[0], INPUT_SIZE[1]]),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean = RGB_MEAN,
                             std = RGB_STD),
    ])
```
In Line 62, `transforms.Resize([int(128 * INPUT_SIZE[0] / 112), int(128 * INPUT_SIZE[0] / 112)])` should be `transforms.Resize([int(128 * INPUT_SIZE[0] / 112), int(128 * INPUT_SIZE[1] / 112)])`."
No progress on the learning,ZhaoJ9014/face.evoLVe,2019-11-06 08:25:23,0,,99,518314377,"![stuck](https://user-images.githubusercontent.com/32535528/68281035-bf7be280-00b1-11ea-839b-2f1422cdd5e1.png)

I have running the code for few hours. But, it seems to stuck on here. 
![image](https://user-images.githubusercontent.com/32535528/68281105-ea663680-00b1-11ea-8e4b-727973afc180.png)

It does running and use my resources. 

"
Deprecated module,ZhaoJ9014/face.evoLVe,2019-10-31 03:12:23,0,,98,515128239,"Some modules are deprecated now, like PIL that you used in face_align."
BUAA-VisNir data are not aligned,ZhaoJ9014/face.evoLVe,2019-10-21 12:34:11,4,,96,509933227,"Hi, I downloaded the BUAA-VisNir data from the given link ([Baidu Drive](https://pan.baidu.com/s/1MWiqNhHQmSKHmWY7oLsFbw)). But I found that the data in the compressed file are still raw images with a shape of 640x480.
Could you please upload an aligned version of this dataset? Thank you!"
How do I create a data zoo,ZhaoJ9014/face.evoLVe,2019-10-02 07:19:54,1,,94,501320143,"
I want to ask how to create a file like lfw or ageDB like data zoo from original data."
Why the training prec@1/5 is always 0.00000?,ZhaoJ9014/face.evoLVe,2019-09-26 05:23:04,15,,93,498665302,"When I run the train.py using the MS1M as training dataset, the Training Prec@1 and Training Prec@5 are always **0.000000**. Could anyone tell me why?
![image](https://user-images.githubusercontent.com/40749976/65660222-956de400-e060-11e9-8735-b36d4820c142.png)
 "
About the IJB evaluation,ZhaoJ9014/face.evoLVe,2019-09-09 09:12:20,0,,92,490975901,"I noticed that you have evaluated the model on IJB C dataset, could you please share the info about this?  how to evaluate the model on IJB dataset?
"
Any head weight for inference?,ZhaoJ9014/face.evoLVe,2019-08-24 12:04:47,0,,90,484810358,"Hi, Zhao, 

First, thanks for your wonderful work. 

I find that in the model zoo, you only provide the weight of backbone but no head weights. 
However, in 'train.py', both the backbone and the head weights are saved.  

I'm wondering where could I get the head weight. I just want to do inference using your code. 

Thanks for your time."
Distance + API- clarification,ZhaoJ9014/face.evoLVe,2019-08-01 08:02:32,1,,86,475534243,"Hey guys!

Thanks again for the great work.
So looking into the code here's what I will love to know:

Does taking the normalized output and then doing the L2 distance between the normalized outputs is almost always the right thing to do? (I'm asking because that's what's being done in verification.py@evaluate which calls verification.py@calculate_roc which does l2 distance, and the embeddings are always normalized as well)?

Also does the following code snippet represent the way that you intended the user code of face recognition to be?


```
def l2_norm(input, axis = 1):
    norm = torch.norm(input, 2, axis, True)
    output = torch.div(input, norm)
    return output

net=IR_50(input_size=[112,112]).cuda()
net.load_state_dict(torch.load('./backbone_ir50_asia.pth'))


def get_embeddings(net,image):
        import torchvision.transforms as transforms
        from torch.autograd import Variable
        import torch.nn
        PIC_SIZE=112
        transform=transforms.Compose([transforms.Resize((PIC_SIZE,PIC_SIZE)),transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])
        transformed_image=transform(image)
        the_image=Variable(transformed_image).cuda().unsqueeze(0)
        net.eval()
        outputs=net.forward(the_image)
        outputs=l2_norm(outputs)
        embeddings=outputs.detach().cpu().numpy()
        return embeddings

def calc_distance(embeddings1, embeddings2):
    diff = np.subtract(embeddings1, embeddings2)
    dist = np.sum(np.square(diff), 1)
    return dist
brad1=Image.open('aligned/Brad Pitt/Brad Pitt/1.jpg')
brad2=Image.open('aligned/Brad Pitt/Brad Pitt/2.jpg')
jennifer1= Image.open('aligned/Brad Pitt/Jennifer Aniston/1.jpg')
jennifer2= Image.open('aligned/Brad Pitt/Jennifer Aniston/1.jpg')
brad1_emb=get_embeddings(net,brad1)
brad2_emb=get_embeddings(net,brad2)
jennifer1_emb=get_embeddings(net,jennifer1)
jennifer2_emb=get_embeddings(net,jennifer2)
#And now just calc_distance between any two embeddings
```"
pretrained model with c++ inference,ZhaoJ9014/face.evoLVe,2019-07-26 01:36:07,12,,82,473136638,"Hi @ZhaoJ9014 , first, thank you for your great work!!!
I have a problem on c++ inference, maybe it is not related with your repo, however, I really appreciate if you could give some advice.
I loaded your pretrained model on baidu with c++ like this:
std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(""Head_ArcFace_Epoch_112_Batch_2547328_Time_2019-07-13-02-59_checkpoint.pth"");

but it generated this error:
An unhandled exception of type 'System.Runtime.InteropServices.SEHException' occurred in .......

My application based on C#, this error always means memory error, I compiled a wrapper with libtorch into a dll which can be used for C# applications, there is no problem as I used mxnet and other frameworks before, however, when I used libtorch to load pth or even pt file,  it always failed.
"
RetinaFace implement in pytorch ？,ZhaoJ9014/face.evoLVe,2019-05-21 06:03:08,1,,55,446431206,RetinaFace is used In insightface to detect and align faces. Any one re-implement it in pytorch?
"Can we convert 3d model image(i.e., .obj file) into the 2d image ?",ZhaoJ9014/face.evoLVe,2019-05-06 09:54:27,0,,51,440622951,"We are every much impressed with you work, We need a small help from you. Can you help us converting 3d model into 2d image."
Performance Issue,ZhaoJ9014/face.evoLVe,2019-03-19 21:16:36,0,,35,422957897,"https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/master/align/detector.py#L24

    # LOAD MODELS
    pnet = PNet()
    rnet = RNet()
    onet = ONet()
    onet.eval()

Model are loaded on detect_faces. 

So in face_align.py
    for subfolder in tqdm(os.listdir(source_root)):
        if not os.path.isdir(os.path.join(dest_root, subfolder)):
            os.mkdir(os.path.join(dest_root, subfolder))
        for image_name in os.listdir(os.path.join(source_root, subfolder)):
            print(""Processing\t{}"".format(os.path.join(source_root, subfolder, image_name)))
            img = Image.open(os.path.join(source_root, subfolder, image_name))
            try: # Handle exception
_, landmarks = detect_faces(img)

Models are going to be loaded every time.
They should be put outside of the function"
The network detects 18 different (farther than 0.99) me when moving my head.,ZhaoJ9014/face.evoLVe,2019-03-04 22:40:42,2,,27,417026122,"Hi.

I have developed a little app that when detecting a face tries to find it in the database and if it's not there, it created a new record.
As I move my head the app thinks it's a new user (distance >= 0.99) and creates a new entry in the database.
Up to 18 records have been created.

I was thinking of developing a professional app for controlling access to places, etc. 
How can I do to filter (or any other way) these extra records created with different head poses?

Thanks."
About model parallel for weight matrix in the head,ZhaoJ9014/face.evoLVe,2019-03-01 14:28:46,2,,21,416127252,"Thank you very much for your high performance repo! 

By splitting large matrix(refer to [here](https://github.com/ZhaoJ9014/face.evoLVe.PyTorch/blob/f8b8a982b5fe92f8d91d111f43a77071c115c66f/head/metrics.py#L107)), 
the memory consumption is more balanced, however the training process seems not speed up. I guess the bottleneck lies in the communication of transferring head matrix from device to device. 

May I ask how to implement [parallel_module_local_v1.py](https://github.com/deepinsight/insightface/blob/74ff9c4d6a52df43034a0b44020d921daedae30c/recognition/parall_module_local_v1.py) in insightface efficiently? 

Looking forward for your suggestion! Thank you!"
Face alignment speed up and GPU usage,ZhaoJ9014/face.evoLVe,2019-02-26 04:44:27,12,,10,414428869,"To whom it may concern,

This repo provided really amazing tools. Thanks for the great work.
I tried face alignment, extract features by using this lib. I found the face alignment may cost 1.3s to process an image. After reading the code, I realized the `mtcnn` is not running on GPU. A a little bit changes were made, e.g., `torch.FloatTensor => torch.cuda.FloatTensor`, `Pnet() =>Pnet().cuda()`, etc.

This increased the face alignment speed per image from 1.3 to 0.8s. It works, however, the result does not make me satisfied. Is there a way to make the face detection/alignment run faster?

There is another thing make me confused. The GPU usage is very low, 1%~2%. Please see the attachments.

<img width=""605"" alt=""screen shot 2019-02-26 at 12 22 32"" src=""https://user-images.githubusercontent.com/17941167/53387362-8be4a680-39c1-11e9-8f0a-999d4c1a9130.png"">

I'm not sure if this is due to I didn't configured the GPU properly or it is just one of the advantages of this library.
The installed CUDA version is 9.2, Cudnn version is 7.4. Graphic card is RTX 2070. It reports an error after I run the python code. Can anyone tell me how to fix it?
<img width=""716"" alt=""screen shot 2019-02-26 at 12 23 37"" src=""https://user-images.githubusercontent.com/17941167/53387364-8be4a680-39c1-11e9-87b6-fa3d52d587be.png"">

Again, many thanks for the great work!
"
I am trying to extract faces and train,ZhaoJ9014/face.evoLVe,2019-02-11 21:08:04,0,,5,408976494,"Hello,
I have live feeds streaming of stores I want to extract each face of the person automatic and save it.
Currently, I am doing that manually cropping face and saving it in a folder with a unique id 
even I tried opencv's modle but it was not accurate than i used ddn I got decent accuracy but some time i am getting blur faces may be because of movment of people fast or camera issue. So any knows how can i save detected face with quniue id inside a unique folder. The script is given blow I have searched a lot bet did not find any thing like auto face extracting from video.

I am using [this git project uses knn but its not scalable ](https://github.com/ageitgey/face_recognition) 
What i am trying to do is [this](https://www.youtube.com/watch?v=z4CpMfMIeU8) extract face and keep the faces with unique id folders each face should have unique id.

My Questions Are 
1. Is there any way to solve above problem
2. How many images do i need to give of a person face to train the modle using your git lib
3. Do i need to lable my data like bonding box or keeping the photos in a different folder. If my first problem is solved 
DDN Code
```
# USAGE
# python object_tracker.py --prototxt deploy.prototxt --model res10_300x300_ssd_iter_140000.caffemodel

# import the necessary packages
from data.centroidtracker import CentroidTracker
from imutils.video import VideoStream
import numpy as np
import argparse
import imutils
import time
import cv2

# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument(""-p"", ""--prototxt"", required=True,
	help=""path to Caffe 'deploy' prototxt file"")
ap.add_argument(""-m"", ""--model"", required=True,
	help=""path to Caffe pre-trained model"")
ap.add_argument(""-c"", ""--confidence"", type=float, default=0.5,
	help=""minimum probability to filter weak detections"")
args = vars(ap.parse_args())

# initialize our centroid tracker and frame dimensions
ct = CentroidTracker()
(H, W) = (None, None)

# load our serialized model from disk
print(""[INFO] loading model..."")
net = cv2.dnn.readNetFromCaffe(args[""prototxt""], args[""model""])

# initialize the video stream and allow the camera sensor to warmup
print(""[INFO] starting video stream..."")
vs = VideoStream(src=0).start()
time.sleep(2.0)

# loop over the frames from the video stream
while True:
	# read the next frame from the video stream and resize it
	frame = vs.read()
	frame = imutils.resize(frame, width=400)

	# if the frame dimensions are None, grab them
	if W is None or H is None:
		(H, W) = frame.shape[:2]

	# construct a blob from the frame, pass it through the network,
	# obtain our output predictions, and initialize the list of
	# bounding box rectangles
	blob = cv2.dnn.blobFromImage(frame, 1.0, (W, H),
		(104.0, 177.0, 123.0))
	net.setInput(blob)
	detections = net.forward()
	rects = []

	# loop over the detections
	for i in range(0, detections.shape[2]):
		# filter out weak detections by ensuring the predicted
		# probability is greater than a minimum threshold
		if detections[0, 0, i, 2] > args[""confidence""]:
			# compute the (x, y)-coordinates of the bounding box for
			# the object, then update the bounding box rectangles list
			box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])
			rects.append(box.astype(""int""))

			# draw a bounding box surrounding the object so we can
			# visualize it
			(startX, startY, endX, endY) = box.astype(""int"")
			cv2.rectangle(frame, (startX, startY), (endX, endY),
				(0, 255, 0), 2)

	# update our centroid tracker using the computed set of bounding
	# box rectangles
	objects = ct.update(rects)

	# loop over the tracked objects
	for (objectID, centroid) in objects.items():
		# draw both the ID of the object and the centroid of the
		# object on the output frame
		text = ""ID {}"".format(objectID)
		cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),
			cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
		cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)

	# show the output frame
	cv2.imshow(""Frame"", frame)
	key = cv2.waitKey(1) & 0xFF

	# if the `q` key was pressed, break from the loop
	if key == ord(""q""):
		break

# do a bit of cleanup
cv2.destroyAllWindows()
vs.stop()

```"
support bdd100k data set,RuochenFan/S4Net,2020-06-26 15:37:58,0,,9,646346789,"Hi,

Can you please elaborate how to train your model using bdd100k data set. It would be great help if you can provide script as provided for ms coco in the code. Thank you in advance for you help and support.

Thanks,"
the inference time,RuochenFan/S4Net,2020-02-09 04:56:45,1,,8,562122260,"Hello,
I use the code you provide to measure the inference time with a GTX 1080 Ti GPU as same as you. The backbone I use is ResNet50. But the _t['im_detect'].average_time got is about 0.050 which is not same as provided in your paper. Do you use other method to measure the inference time?"
About datasets,RuochenFan/S4Net,2019-12-16 03:24:04,0,,7,538163498,"I use pascal voc2012 dataset and generated .pkl
There was nothing problems when i taining the network,but when i test,i got this:

img = self.imgs[self.pos%len(self.imgs)]
ZeroDivisionError: integer division or modulo by zero
"
About the test_seg.py,RuochenFan/S4Net,2019-12-03 12:11:39,2,,6,531940883,"I can run the .py successfully but it didn't save the .jpg images,and i don't know why.
if args.show
.....
cv.imwrite('.jpg')
.....
not running in the code
"
"The model you given, that the result maybe is not the best compared to your paper.",RuochenFan/S4Net,2019-11-10 03:00:31,1,,5,520545758,
What is the dataset?,RuochenFan/S4Net,2019-06-21 04:44:36,2,,3,459005674,From readme I can't find the dataset setting. How does it generated? Could you please make it clear?
Good job! When will you release codes?,RuochenFan/S4Net,2019-05-16 03:41:32,0,,2,444739815,
Do you plan to release the code ?,RuochenFan/S4Net,2018-08-10 20:46:43,0,,1,349652335,
model for detection,xilaili/AOGNet,2019-08-27 04:46:19,0,,6,485580044,"https://github.com/iVMCL/AOGNets/issues/3#issue-485579661
I meet a problem described in the link. Can you help me?
Thank you very much!"
error when training by cifar10,xilaili/AOGNet,2019-04-10 02:47:48,1,,4,431278985,"Hi，I try to rerun this code to test this model's performance by using the 'python3.6 main.py --cfg cfgs/cifar10/aognet_cifar10_ps_4_bottleneck_1M.yaml --gpus 1,2'.At first everything seemed to be going smoothly，however，when it comes to epoch 280，it is stoped by an error：
Traceback (most recent call last):
  File ""main.py"", line 133, in <module>
    main()
  File ""main.py"", line 120, in main
    epoch_end_callback = checkpoint)
  File ""/home/amax/anaconda3/lib/python3.6/site-packages/mxnet/module/base_module.py"", line 575, in fit
    callback(epoch, self.symbol, arg_params, aux_params)
  File ""/home/amax/anaconda3/lib/python3.6/site-packages/mxnet/callback.py"", line 89, in _callback
    save_checkpoint(prefix, iter_no + 1, sym, arg, aux)
  File ""/home/amax/anaconda3/lib/python3.6/site-packages/mxnet/model.py"", line 409, in save_checkpoint
    nd.save(param_name, save_dict)
  File ""/home/amax/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/utils.py"", line 273, in save
    keys))
  File ""/home/amax/anaconda3/lib/python3.6/site-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [09:52:25] src/io/local_filesys.cc:39: Check failed: std::fwrite(ptr, 1, size, fp_) == size FileStream.Write incomplete
I can't find the suitable solution to deal with this problem.So could you please tell me how to solve this problem?"
TirtaDharma,xilaili/AOGNet,2019-04-07 15:17:12,0,,3,430156887,
PyTorch 1.0 version,xilaili/AOGNet,2019-04-06 08:55:03,3,,2,430013574,"Hi, It is a very exciting work for combing aog and deep learning. 
When do you plan to release the code of PyTorch version? I am really looking forward to it."
contribute into gluoncv,xilaili/AOGNet,2019-04-02 06:46:03,2,,1,428051218,Very interesting work! I’m wonder if you are interested in reimplement the symbol codes in gluon and contribute into gluoncv https://gluon-cv.mxnet.io ?
How can the Object 'bin_op' of Class 'BinOp' change the parameters 'weight' of network 'model' in your code?,XinDongol/BENN-PyTorch,2020-04-05 10:09:24,1,,3,594372227,"Hi, thank you for your impressing work. I have some questions about this code.
What is the role of the object 'bin_op'? It seems that it can not change the parameters 'weight' of the network 'model'? In the training process, it seems that all the parameters in network are not binary. All the calculated binary parameters are stored in 'bin_op.target_modules', but they are not used in the training process of the network 'model'. It looks like the training process only uses binary input but real weight. Do I misunderstand something? Thanks for your answer."
pretrained models,XinDongol/BENN-PyTorch,2020-01-16 02:39:12,0,,2,550539129,"Hi
Thanks for the nice project.
But could you please help to provide the pretrained models to facilitate more future studies?
"
checkpoints link is not working,aimagelab/novelty-detection,2022-04-17 14:01:12,0,,16,1206383514,"Hi, I am unable to download checkpoints, the link is not working. Could you please check and update the link for download
"
Why not providing a train.py,aimagelab/novelty-detection,2022-01-18 10:36:14,0,,15,1106739460,"How can I train your method on a dataset you didn't consider?

I want to compare your method with mine. Is there a way to replicate exactly your training on my dataset?"
ValueError: need at least one array to concatenate,aimagelab/novelty-detection,2021-05-07 01:59:50,0,,14,878320964,"what is wrong with it？
best wishes"
Sampling,aimagelab/novelty-detection,2020-01-30 07:36:39,10,,13,557316963,"Hi, thank you for sharing your code about the LSA model. Having trained the full model, I'm wondering how I can sample from the LSA? Can I just draw a uniform random vector as z and put it into the CPD estimator?"
Loss increases during the traning ,aimagelab/novelty-detection,2019-09-05 02:16:09,4,,10,489490656,"I am trying to do training but the loss function increases rather than decrease 

I have attached the following training part
______________________________________________________code start___________________
 for cl_idx, video_id in enumerate(dataset.train_videos):
        
            # Run the train video 
            dataset.train(video_id)
            loader = DataLoader(dataset, collate_fn=dataset.collate_fn)

            # Build score containers
            #sample_llk = np.zeros(shape=(len(loader) + t - 1,))
            #sample_rec = np.zeros(shape=(len(loader) + t - 1,))
            ##uploading the ground truth
            #sample_y = dataset.load_test_sequence_gt(video_id)
            for i, (x, y) in tqdm(enumerate(loader), desc=f'Computing scores for {dataset}'):
                optimizer.zero_grad()
                x = x.to('cuda')
                # Forward pass, get our logits then backward pass, then update weights
                x_r, z, z_dist = model(x)
                 # Calculate the joint loss 
                loss=criterion(x, x_r, z, z_dist)
                #print(loss)
                ## do backward and the update
                loss.backward()
                optimizer.step()
                running_loss += loss.item()  
                print (running_loss)"
About trainning.,aimagelab/novelty-detection,2019-09-03 08:22:49,1,,9,488460673,"Hello, thank you for your work, can you share your training codes?"
Question about Shanghaitech,aimagelab/novelty-detection,2019-08-09 08:16:57,3,,7,478858882,"Hi,  thanks for your work. Could you give more the following details about the training on ShanghaiTech?
"
train ucsd,aimagelab/novelty-detection,2019-07-31 03:59:30,8,,6,474923427,"Hi，thanks for your work。When I train it ,I get this error。

Traceback (most recent call last):
  File ""train.py"", line 201, in <module>
    main()
  File ""train.py"", line 192, in main
    train_ucsdped2()
  File ""train.py"", line 128, in train_ucsdped2
    x_r, z, z_dist = model(x)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/LSA_ucsd.py"", line 189, in forward
    z = self.encoder(h)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/LSA_ucsd.py"", line 62, in forward
    h = self.conv(h)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/blocks_3d.py"", line 133, in forward
    activation_fn=self._activation_fn
  File ""/home/dl/VSST/dm/novelty-detection-master/models/blocks_3d.py"", line 33, in residual_op
    ha = f1(ha)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/layers/mconv3d.py"", line 29, in forward
    return super(MaskedConv3d, self).forward(x)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 421, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected 5-dimensional input for 5-dimensional weight [8, 1, 3, 3, 3], but got input of size [105, 690, 1, 8, 32, 32] instead
"
Loss exploding after few steps,aimagelab/novelty-detection,2019-07-17 13:46:31,22,,5,469203460,"`from functools import reduce
from operator import mul
from typing import Tuple
import numpy as np
import torch
import torchvision
import torch.nn as nn
from models.loss_functions.lsaloss import LSALoss
from models.base import BaseModule
from models.blocks_2d import DownsampleBlock
from models.blocks_2d import ResidualBlock
from models.blocks_2d import UpsampleBlock
from models.estimator_1D import Estimator1D
import cv2

class Encoder(BaseModule):
    """"""
    CIFAR10 model encoder.
    """"""
    def __init__(self, input_shape, code_length):
        # type: (Tuple[int, int, int], int) -> None
        """"""
        Class constructor:

        :param input_shape: the shape of CIFAR10 samples.
        :param code_length: the dimensionality of latent vectors.
        """"""
        super(Encoder, self).__init__()

        self.input_shape = input_shape
        self.code_length = code_length

        c, h, w = input_shape

        print (c,h,w)

        activation_fn = nn.LeakyReLU()

        # Convolutional network
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=3, bias=False),
            activation_fn,
            ResidualBlock(channel_in=32, channel_out=32, activation_fn=activation_fn),
            DownsampleBlock(channel_in=32, channel_out=64, activation_fn=activation_fn),
            DownsampleBlock(channel_in=64, channel_out=128, activation_fn=activation_fn),
            DownsampleBlock(channel_in=128, channel_out=256, activation_fn=activation_fn),
        )
        self.deepest_shape = (256, h // 8, w // 8)

        # FC network
        self.fc = nn.Sequential(
            nn.Linear(in_features=reduce(mul, self.deepest_shape), out_features=256),
            nn.BatchNorm1d(num_features=256),
            activation_fn,
            nn.Linear(in_features=256, out_features=code_length),
            nn.Sigmoid()
        )

    def forward(self, x):
        # types: (torch.Tensor) -> torch.Tensor
        """"""
        Forward propagation.

        :param x: the input batch of images.
        :return: the batch of latent vectors.
        """"""
        h = x
        print (type(h))
        h = self.conv(h)
        h = h.view(len(h), -1)
        o = self.fc(h)

        return o


class Decoder(BaseModule):
    """"""
    CIFAR10 model decoder.
    """"""
    def __init__(self, code_length, deepest_shape, output_shape):
        # type: (int, Tuple[int, int, int], Tuple[int, int, int]) -> None
        """"""
        Class constructor.

        :param code_length: the dimensionality of latent vectors.
        :param deepest_shape: the dimensionality of the encoder's deepest convolutional map.
        :param output_shape: the shape of CIFAR10 samples.
        """"""
        super(Decoder, self).__init__()

        self.code_length = code_length
        self.deepest_shape = deepest_shape
        self.output_shape = output_shape

        print (self.output_shape,""--"")

        activation_fn = nn.LeakyReLU()

        # FC network
        self.fc = nn.Sequential(
            nn.Linear(in_features=code_length, out_features=256),
            nn.BatchNorm1d(num_features=256),
            activation_fn,
            nn.Linear(in_features=256, out_features=reduce(mul, deepest_shape)),
            nn.BatchNorm1d(num_features=reduce(mul, deepest_shape)),
            activation_fn
        )

        # Convolutional network
        self.conv = nn.Sequential(
            UpsampleBlock(channel_in=256, channel_out=128, activation_fn=activation_fn),
            UpsampleBlock(channel_in=128, channel_out=64, activation_fn=activation_fn),
            UpsampleBlock(channel_in=64, channel_out=32, activation_fn=activation_fn),
            ResidualBlock(channel_in=32, channel_out=32, activation_fn=activation_fn),
            nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1, bias=False)
        )

    def forward(self, x):
        # types: (torch.Tensor) -> torch.Tensor
        """"""
        Forward propagation.

        :param x: the batch of latent vectors.
        :return: the batch of reconstructions.
        """"""
        h = x
        h = self.fc(h)
        h = h.view(len(h), *self.deepest_shape)
        h = self.conv(h)
        o = h

        return o


class LSACIFAR10(BaseModule):
    """"""
    LSA model for CIFAR10 one-class classification.
    """"""
    def __init__(self,  input_shape, code_length, cpd_channels):
        # type: (Tuple[int, int, int], int, int) -> None
        """"""
        Class constructor.

        :param input_shape: the shape of CIFAR10 samples.
        :param code_length: the dimensionality of latent vectors.
        :param cpd_channels: number of bins in which the multinomial works.
        """"""
        super(LSACIFAR10, self).__init__()

        self.input_shape = input_shape
        self.code_length = code_length

        # Build encoder
        self.encoder = Encoder(
            input_shape=input_shape,
            code_length=code_length
        )

        # Build decoder
        self.decoder = Decoder(
            code_length=code_length,
            deepest_shape=self.encoder.deepest_shape,
            output_shape=input_shape
        )

        # Build estimator
        self.estimator = Estimator1D(
            code_length=code_length,
            fm_list=[32, 32, 32, 32],
            cpd_channels=cpd_channels
        )

    def forward(self, x):
        # type: (torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
        """"""
        Forward propagation.

        :param x: the input batch of images.
        :return: a tuple of torch.Tensors holding reconstructions, latent vectors and CPD estimates.
        """"""
        h = x

        # Produce representations
        z = self.encoder(h)

        # Estimate CPDs with autoregression
        z_dist = self.estimator(z)

        # Reconstruct x
        x_r = self.decoder(z)
        # print (x_r.shape)
        x_r = x_r.view(-1, *self.input_shape)

        return x_r, z, z_dist


def load_dataset(data_path=""/home/jbmai/Downloads/Defect Images-20190705T133320Z-001""):
    # data_path = 'data/train/'
# torchvision.transforms.Grayscale(num_output_channels=1)
    trainTransform  = torchvision.transforms.Compose([
                                    torchvision.transforms.Resize(size=(128,128), interpolation=2),
                                    torchvision.transforms.ToTensor(), 
                                    ])




    train_dataset = torchvision.datasets.ImageFolder(
        root=data_path,
        transform=trainTransform)
    

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=64,
        num_workers=0,
        shuffle=True
    )
    return train_loader


net = LSACIFAR10(input_shape=[3,128,128],code_length = 32,cpd_channels =100)
lossFunction = LSALoss(cpd_channels=100)
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)




try:

    checkpoint = torch.load(""savedWeights/enc.pth"")
    net.encoder.load_state_dict(checkpoint)

    checkpoint = torch.load(""savedWeights/est.pth"")
    net.estimator.load_state_dict(checkpoint)

    checkpoint = torch.load(""savedWeights/dec.pth"")
    net.decoder.load_state_dict(checkpoint)


except Exception as e:
    print (e)


for epoch in range(1000):  # loop over the dataset multiple times

    running_loss = 0.0
    d = load_dataset()
    for i, (data,l) in enumerate(d):
        # get the inputs; data is a list of [inputs, labels]
        
        # print (data.shape)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        # print (data)
        x_r,z,z_dist = net.forward(data)
        # print (x_r.shape)
        # print(data.shape)
        loss = lossFunction(data,x_r,z,z_dist)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 5 == 0:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 5))
            running_loss = 0.0
    if (epoch % 5)== 0 :
        # print (""--------------------{} epoch-----------"".format(epoch))
        # net.encoder.eval()
        # net.estimator.eval()
        # net.decoder.eval()

        # z = net.encoder(data)
        # z_dist = net.estimator(z)
        # x_r = net.decoder(z).permute(0,2,3,1).detach().numpy()
        # out =x_r
        # print (type(out))
        # for i in range(out.shape[0]):
        #     # print (out.shape)
        #     # # out.permute(0,2,3,1)
        #     # print (out.shape)
        #     cv2.imwrite(""constructedImages/outDec{}_{}.jpg"".format(epoch,i),out[i,:,:,:]*255)
        #     # cv2.waitKey(0)
           
        #     # cv2.
        # net.encoder.train()
        # net.estimator.train()
        # net.decoder.eval()
        torch.save(net.encoder.state_dict(),(""savedWeights/enc.pth""))
        torch.save(net.estimator.state_dict(),(""savedWeights/est.pth""))
        torch.save(net.decoder.state_dict(),(""savedWeights/dec.pth""))

print('Finished Training')`

Output : 

<class 'torch.Tensor'>
[1,     1] loss: 727109273.600
<class 'torch.Tensor'>
[2,     1] loss: 2495627954514337382531072.000



Hi can you help me rectify the issue."
Pretrained model,DTaoo/Simplified_DMC,2021-12-06 10:18:58,0,,2,1071972846,Could you please kindly share the pretrained model? Thank you.
Regarding code for DMF experiments,DTaoo/Simplified_DMC,2020-06-02 10:39:23,0,,1,629095845,"Hi Di Hu,

Sorry, this is not an issue about this code. I am writing here because when I sent you an email, it bounced back.

I am currently doing some experiments where I am comparing with your ICASSP 2019 paper, Dense Multimodal Fusion. Could you please share the code of that paper for a fair comparison? My email id is v.rajan@qmul.ac.uk 

Thanks,
"
Some questions about training PNGAN,caiyuanhao1998/PNGAN,2022-10-10 03:37:28,0,,5,1402546365,"I trained PNGAN（https://github.com/GarrickZ2/Image-Denoising/tree/master/PNGAN）, and the result of the generated noisy image is very poor. I don't know where I neglected or made a mistake. Please help me to find out what went wrong. Among them, the loss values of both the generator and the discriminator are high.

Can I take a look at your training log? I will refer to it. Because I want to reproduce it first, all parameters and data sets are used by default (such as the default optimal lambda_p=6e-3, lambda_ra=8e-4 in the article and code). Once I reproduce the effect in the schematic you gave, I will tune the hyperparameters to get better results in other applications.

This is the noisy image I generated
![image](https://user-images.githubusercontent.com/24237829/194795798-8781e406-4a01-4745-b472-8b77f63f4787.png)

Here is my training log
Configuration:
Namespace(act='relu', batch_size=8, benchmark_noise=False, beta1=0.9, beta2=0.9999, chop=False, cpu=True, data_test='SIDD', data_train='SIDD', debug=False, decay_type='step', dir_data='/home/shendi_mcj/datasets/SIDD_128/Datasets', epochs=100, epsilon=1e-08, ext='sep_reset', extend='.', gamma=0.5, gan_k=1, generate=False, load='.', load_best=False, load_dir='.', load_epoch=27, load_models=True, loss='1*L1', lr=0.0002, lr_decay_step=100000.0, lr_min=1e-07, model='RIDNET', momentum=0.9, n_GPUs=1, n_colors=3, n_feats=64, n_threads=8, n_train=20000, n_val=2000, noise=50, noise_g=[1], optimizer='ADAM', partial_data=True, patch_size=128, pre_train='experiment/ridnet.pt', precision='single', predict_patch_size=800, print_every=100, print_model=False, reduction=16, res_scale=1, reset=False, resume=0, rgb_range=255, save='./', save_models=True, save_results=False, savepath='./save', seed=1, self_ensemble=False, shift_mean=True, skip_threshold=1000000.0, split_batch=1, template='.', test_every=1000, test_only=False, testpath='./test', timestamp='1665173019', weight_decay=0.8)
Making model...
Loading model from experiment/ridnet.pt
Load Model from epoch: 27
Epoch 28:g_trn_l=4260.0166,d_trn_l=-8.6347: 100%|█| 2500/2500 [18:04<00:00, 2.3
val_d_loss=-16553.590091705322, val_g_loss=1456043.0610351562: 100%|█| 250/250 [
Epoch 29: val_d_loss=-8.276795045852662, val_g_loss=728.0215305175781
Epoch 29:g_trn_l=612.9119,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:27<00:00, 2.39
val_d_loss=-16508.084072113037, val_g_loss=1441245.7890625: 100%|█| 250/250 [00:
Epoch 30: val_d_loss=-8.254042036056518, val_g_loss=720.62289453125
Epoch 30:g_trn_l=569.982,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:29<00:00, 2.38i
val_d_loss=-16534.407222747803, val_g_loss=1445840.658203125: 100%|█| 250/250 [0
Epoch 31: val_d_loss=-8.267203611373901, val_g_loss=722.9203291015625
Epoch 31:g_trn_l=642.1012,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:27<00:00, 2.39
val_d_loss=-16530.938148498535, val_g_loss=1451748.1240234375: 100%|█| 250/250 [
Epoch 32: val_d_loss=-8.265469074249268, val_g_loss=725.8740620117187
Epoch 32:g_trn_l=1922.5544,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:29<00:00, 2.3
val_d_loss=-16461.000728607178, val_g_loss=1447958.69140625: 100%|█| 250/250 [00
Epoch 33: val_d_loss=-8.23050036430359, val_g_loss=723.979345703125
Epoch 33:g_trn_l=596.3593,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:28<00:00, 2.38
val_d_loss=-16396.410007476807, val_g_loss=1444418.5522460938: 100%|█| 250/250 [
Epoch 34: val_d_loss=-8.198205003738403, val_g_loss=722.2092761230468
Epoch 34:g_trn_l=4094.3552,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:33<00:00, 2.3
val_d_loss=-16325.490299224854, val_g_loss=1430286.7797851562: 100%|█| 250/250 [
Epoch 35: val_d_loss=-8.162745149612427, val_g_loss=715.1433898925782
Epoch 35:g_trn_l=562.7994,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:31<00:00, 2.38
val_d_loss=-16497.081832885742, val_g_loss=1441190.5700683594: 100%|█| 250/250 [
Epoch 36: val_d_loss=-8.24854091644287, val_g_loss=720.5952850341797
Epoch 36:g_trn_l=549.0154,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:32<00:00, 2.38
val_d_loss=-16452.982189178467, val_g_loss=1446414.1437988281: 100%|█| 250/250 [
Epoch 37: val_d_loss=-8.226491094589234, val_g_loss=723.207071899414
Epoch 37:g_trn_l=1389.9574,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:27<00:00, 2.3
val_d_loss=-16494.09930419922, val_g_loss=1442426.9399414062: 100%|█| 250/250 [0
Epoch 38: val_d_loss=-8.247049652099609, val_g_loss=721.2134699707032
Epoch 38:g_trn_l=1731.5358,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:22<00:00, 2.4
val_d_loss=-16414.167991638184, val_g_loss=1442114.4387207031: 100%|█| 250/250 [
Epoch 39: val_d_loss=-8.207083995819092, val_g_loss=721.0572193603516
Epoch 39:g_trn_l=1161.1187,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:25<00:00, 2.3
val_d_loss=-16374.823741912842, val_g_loss=1435748.1032714844: 100%|█| 250/250 [
Epoch 40: val_d_loss=-8.18741187095642, val_g_loss=717.8740516357421
Epoch 40:g_trn_l=1218.5015,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:33<00:00, 2.3
val_d_loss=-16411.774070739746, val_g_loss=1437521.4143066406: 100%|█| 250/250 [
Epoch 41: val_d_loss=-8.205887035369873, val_g_loss=718.7607071533204
Epoch 41:g_trn_l=585.713,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:30<00:00, 2.38i
val_d_loss=-16421.3918800354, val_g_loss=1435851.421875: 100%|█| 250/250 [00:51<
Epoch 42: val_d_loss=-8.2106959400177, val_g_loss=717.9257109375
Epoch 42:g_trn_l=3450.4092,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:28<00:00, 2.3
val_d_loss=-16342.977878570557, val_g_loss=1436181.6176757812: 100%|█| 250/250 [
Epoch 43: val_d_loss=-8.171488939285279, val_g_loss=718.0908088378907
Epoch 43:g_trn_l=1209.9535,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:36<00:00, 2.3
val_d_loss=-16354.145488739014, val_g_loss=1442767.9860839844: 100%|█| 250/250 [
Epoch 44: val_d_loss=-8.177072744369507, val_g_loss=721.3839930419922
Epoch 44:g_trn_l=1001.378,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:33<00:00, 2.37
val_d_loss=-16319.988140106201, val_g_loss=1442307.275390625: 100%|█| 250/250 [0
Epoch 45: val_d_loss=-8.1599940700531, val_g_loss=721.1536376953125
Epoch 45:g_trn_l=4854.5229,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00, 2.3
val_d_loss=-16327.855934143066, val_g_loss=1427248.0895996094: 100%|█| 250/250 [
Epoch 46: val_d_loss=-8.163927967071533, val_g_loss=713.6240447998047
Epoch 46:g_trn_l=1594.9669,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00, 2.3
val_d_loss=-16331.874336242676, val_g_loss=1439591.0356445312: 100%|█| 250/250 [
Epoch 47: val_d_loss=-8.165937168121339, val_g_loss=719.7955178222657
Epoch 47:g_trn_l=1727.7327,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00, 2.3
val_d_loss=-16301.70266342163, val_g_loss=1430494.6499023438: 100%|█| 250/250 [0
Epoch 48: val_d_loss=-8.150851331710815, val_g_loss=715.2473249511719
Epoch 48:g_trn_l=662.2933,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:29<00:00, 2.38
val_d_loss=-16289.168937683105, val_g_loss=1445139.4096679688: 100%|█| 250/250 [
Epoch 49: val_d_loss=-8.144584468841552, val_g_loss=722.5697048339844
Epoch 49:g_trn_l=638.9958,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00, 2.37
val_d_loss=-16676.23886871338, val_g_loss=1443492.8112792969: 100%|█| 250/250 [0
Epoch 50: val_d_loss=-8.33811943435669, val_g_loss=721.7464056396484
Epoch 50:g_trn_l=449.2137,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00, 2.37
val_d_loss=-16709.131187438965, val_g_loss=1430881.7004394531: 100%|█| 250/250 [
Epoch 51: val_d_loss=-8.354565593719482, val_g_loss=715.4408502197266
Epoch 51:g_trn_l=1438.2651,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:35<00:00, 2.3
val_d_loss=-16641.16509246826, val_g_loss=1435487.1213378906: 100%|█| 250/250 [0
Epoch 52: val_d_loss=-8.320582546234132, val_g_loss=717.7435606689453
Epoch 52:g_trn_l=597.417,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:32<00:00, 2.38i
val_d_loss=-16499.351203918457, val_g_loss=1440687.0793457031: 100%|█| 250/250 [
Epoch 53: val_d_loss=-8.249675601959229, val_g_loss=720.3435396728515
Epoch 53:g_trn_l=569.0695,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:31<00:00, 2.38
val_d_loss=-16576.316123962402, val_g_loss=1441669.537109375: 100%|█| 250/250 [0
Epoch 54: val_d_loss=-8.288158061981202, val_g_loss=720.8347685546875
Epoch 54:g_trn_l=1885.5017,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:36<00:00, 2.3
val_d_loss=-16543.578399658203, val_g_loss=1450123.9528808594: 100%|█| 250/250 [
Epoch 55: val_d_loss=-8.2717891998291, val_g_loss=725.0619764404297
Epoch 55:g_trn_l=976.4413,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:33<00:00, 2.37
val_d_loss=-16528.236602783203, val_g_loss=1476003.5375976562: 100%|█| 250/250 [
Epoch 56: val_d_loss=-8.264118301391601, val_g_loss=738.0017687988282
Epoch 56:g_trn_l=872.3619,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:35<00:00, 2.37
val_d_loss=-16424.807899475098, val_g_loss=1428827.9528808594: 100%|█| 250/250 [
Epoch 57: val_d_loss=-8.21240394973755, val_g_loss=714.4139764404297
Epoch 57:g_trn_l=58598960922624.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:0
val_d_loss=3051.3138489723206, val_g_loss=6.354772355868262e+16: 100%|█| 250/250
Epoch 58: val_d_loss=1.5256569244861602, val_g_loss=31773861779341.312
Epoch 58:g_trn_l=3881488875520.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00
val_d_loss=7193.342471122742, val_g_loss=5.553211802701005e+16: 100%|█| 250/250
Epoch 59: val_d_loss=3.596671235561371, val_g_loss=27766059013505.023
Epoch 59:g_trn_l=67181735837696.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:0
val_d_loss=8914.092770576477, val_g_loss=1.2930694204778086e+17: 100%|█| 250/250
Epoch 60: val_d_loss=4.457046385288239, val_g_loss=64653471023890.43
Epoch 60:g_trn_l=54223576236032.0,d_trn_l=-8.6331: 100%|█| 2500/2500 [17:27<00:0
val_d_loss=16679.991931915283, val_g_loss=1.2193291783530086e+17: 100%|█| 250/25
Epoch 61: val_d_loss=8.339995965957641, val_g_loss=60966458917650.43
Epoch 61:g_trn_l=38286747762688.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:26<00:0
val_d_loss=7301.153503417969, val_g_loss=7.05499680955433e+16: 100%|█| 250/250 [
Epoch 62: val_d_loss=3.6505767517089844, val_g_loss=35274984047771.65
Epoch 62:g_trn_l=1624666144768.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:24<00:00
val_d_loss=10444.144174575806, val_g_loss=4079118023655424.0: 100%|█| 250/250 [0
Epoch 63: val_d_loss=5.222072087287903, val_g_loss=2039559011827.712
Epoch 63:g_trn_l=3864946802688.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:24<00:00
val_d_loss=10191.777889251709, val_g_loss=5970656558579712.0: 100%|█| 250/250 [0
Epoch 64: val_d_loss=5.095888944625854, val_g_loss=2985328279289.856
Epoch 64:g_trn_l=19007925125120.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:21<00:0
val_d_loss=14041.098752975464, val_g_loss=6.728756782484685e+16: 100%|█| 250/250
Epoch 65: val_d_loss=7.020549376487732, val_g_loss=33643783912423.426
Epoch 65:g_trn_l=41974014935040.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:24<00:0
val_d_loss=15734.480403900146, val_g_loss=9.42083598796718e+16: 100%|█| 250/250
Epoch 66: val_d_loss=7.867240201950073, val_g_loss=47104179939835.91
Epoch 66:g_trn_l=4564226932736.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:25<00:00
val_d_loss=-8555.47631263733, val_g_loss=1.115331718283264e+16: 100%|█| 250/250
Epoch 67: val_d_loss=-4.277738156318665, val_g_loss=5576658591416.32
Epoch 67:g_trn_l=2083470442496.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:32<00:00
val_d_loss=-10110.21891784668, val_g_loss=5013995595497472.0: 100%|█| 250/250 [0
Epoch 68: val_d_loss=-5.05510945892334, val_g_loss=2506997797748.736
Epoch 68:g_trn_l=2037396799488.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:21<00:00
val_d_loss=-6897.837250709534, val_g_loss=6761572339810304.0: 100%|█| 250/250 [0
Epoch 69: val_d_loss=-3.448918625354767, val_g_loss=3380786169905.152
Epoch 69:g_trn_l=19517893771264.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:31<00:0
val_d_loss=-4325.360991001129, val_g_loss=2.0603421086449664e+16: 100%|█| 250/25
Epoch 70: val_d_loss=-2.1626804955005645, val_g_loss=10301710543224.832
Epoch 70:g_trn_l=2508975767552.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:28<00:00
val_d_loss=-9451.344500541687, val_g_loss=3735847221329920.0: 100%|█| 250/250 [0
Epoch 71: val_d_loss=-4.7256722502708435, val_g_loss=1867923610664.96
Epoch 71:g_trn_l=1847262969856.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:31<00:00
val_d_loss=-5519.979823589325, val_g_loss=3760646322651136.0: 100%|█| 250/250 [0
Epoch 72: val_d_loss=-2.7599899117946625, val_g_loss=1880323161325.568
Epoch 72:g_trn_l=520190787584.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:30<00:00,
val_d_loss=-6887.619359970093, val_g_loss=1009447890845696.0: 100%|█| 250/250 [0
Epoch 73: val_d_loss=-3.4438096799850464, val_g_loss=504723945422.848
Epoch 73:g_trn_l=226598469632.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:27<00:00,
val_d_loss=-15223.686599731445, val_g_loss=439739038236672.0: 100%|█| 250/250 [0
Epoch 74: val_d_loss=-7.611843299865723, val_g_loss=219869519118.336
Epoch 74:g_trn_l=176480763904.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:29<00:00,
val_d_loss=-4537.72459936142, val_g_loss=388233302310912.0: 100%|█| 250/250 [00:
Epoch 75: val_d_loss=-2.26886229968071, val_g_loss=194116651155.456
Epoch 75:g_trn_l=111401664512.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:27<00:00,
val_d_loss=-2710.772897720337, val_g_loss=248896543784960.0: 100%|█| 250/250 [00
Epoch 76: val_d_loss=-1.3553864488601683, val_g_loss=124448271892.48
Epoch 76:g_trn_l=223202803712.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:25<00:00,
val_d_loss=275.7913808822632, val_g_loss=495202342076416.0: 100%|█| 250/250 [00:
Epoch 77: val_d_loss=0.1378956904411316, val_g_loss=247601171038.208
Epoch 77:g_trn_l=145705172992.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:32<00:00,
val_d_loss=8066.014381408691, val_g_loss=424581223809024.0: 100%|█| 250/250 [00:
Epoch 78: val_d_loss=4.033007190704346, val_g_loss=212290611904.512
Epoch 78:g_trn_l=1403458289664.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:28<00:00
val_d_loss=3504.658802986145, val_g_loss=2283288738398208.0: 100%|█| 250/250 [00
Epoch 79: val_d_loss=1.7523294014930726, val_g_loss=1141644369199.104
Epoch 79:g_trn_l=233238986752.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34<00:00,
val_d_loss=6305.426145553589, val_g_loss=489256021917696.0: 100%|█| 250/250 [00:
Epoch 80: val_d_loss=3.1527130727767942, val_g_loss=244628010958.848
Epoch 80:g_trn_l=8642871427072.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:33<00:00
val_d_loss=-7856.273780822754, val_g_loss=1.2819610196770816e+16: 100%|█| 250/25
Epoch 81: val_d_loss=-3.928136890411377, val_g_loss=6409805098385.408
Epoch 81:g_trn_l=91949588480.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:36<00:00,
val_d_loss=-411.3753271102905, val_g_loss=437178517553152.0: 100%|█| 250/250 [00
Epoch 82: val_d_loss=-0.20568766355514526, val_g_loss=218589258776.576
Epoch 82:g_trn_l=125667479912448.0,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:35<00:
val_d_loss=-2535.5058431625366, val_g_loss=2.571578478784676e+17: 100%|█| 250/25
Epoch 83: val_d_loss=-1.2677529215812684, val_g_loss=128578923939233.8
Epoch 83:g_trn_l=2.7062537286275625e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:3
val_d_loss=-11571.54788017273, val_g_loss=1.6632423230309563e+27: 100%|█| 250/25
Epoch 84: val_d_loss=-5.785773940086365, val_g_loss=8.316211615154782e+23
Epoch 84:g_trn_l=3.415223712623123e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:37
val_d_loss=-10394.510999679565, val_g_loss=1.6684358548098048e+27: 100%|█| 250/2
Epoch 85: val_d_loss=-5.197255499839783, val_g_loss=8.342179274049023e+23
Epoch 85:g_trn_l=2.7823559154188393e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:2
val_d_loss=-11055.715322494507, val_g_loss=1.622653395616832e+27: 100%|█| 250/25
Epoch 86: val_d_loss=-5.527857661247253, val_g_loss=8.113266978084161e+23
Epoch 86:g_trn_l=4.9750231057087425e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:2
val_d_loss=-9373.261775970459, val_g_loss=1.617555301238983e+27: 100%|█| 250/250
Epoch 87: val_d_loss=-4.686630887985229, val_g_loss=8.087776506194916e+23
Epoch 87:g_trn_l=2.3236702020938654e+24,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:3
val_d_loss=-5703.948229789734, val_g_loss=1.6296847968279857e+27: 100%|█| 250/25
Epoch 88: val_d_loss=-2.8519741148948667, val_g_loss=8.148423984139929e+23
Epoch 88:g_trn_l=3.810222546436773e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:31
val_d_loss=-10268.105506896973, val_g_loss=1.6421163303251944e+27: 100%|█| 250/2
Epoch 89: val_d_loss=-5.134052753448486, val_g_loss=8.210581651625973e+23
Epoch 89:g_trn_l=1.5643003265820106e+24,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:2
val_d_loss=-7419.597512245178, val_g_loss=1.6674274361631297e+27: 100%|█| 250/25
Epoch 90: val_d_loss=-3.709798756122589, val_g_loss=8.337137180815649e+23
Epoch 90:g_trn_l=7.42748494410314e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:32<
val_d_loss=-10104.679119110107, val_g_loss=1.62342235217877e+27: 100%|█| 250/250
Epoch 91: val_d_loss=-5.052339559555054, val_g_loss=8.117111760893849e+23
Epoch 91:g_trn_l=1.0870287259692399e+24,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:3
val_d_loss=-8483.54418373108, val_g_loss=1.5998357661512992e+27: 100%|█| 250/250
Epoch 92: val_d_loss=-4.241772091865539, val_g_loss=7.999178830756496e+23
Epoch 92:g_trn_l=1.1925134775933924e+24,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:3
val_d_loss=-8301.296371459961, val_g_loss=1.6230275357947843e+27: 100%|█| 250/25
Epoch 93: val_d_loss=-4.15064818572998, val_g_loss=8.115137678973922e+23
Epoch 93:g_trn_l=2.9071117499924953e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:3
val_d_loss=-10424.478651046753, val_g_loss=1.6232417481779947e+27: 100%|█| 250/2
Epoch 94: val_d_loss=-5.2122393255233765, val_g_loss=8.116208740889973e+23
Epoch 94:g_trn_l=4.567849420806444e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:38
val_d_loss=-8979.812364578247, val_g_loss=1.6686692055458765e+27: 100%|█| 250/25
Epoch 95: val_d_loss=-4.489906182289124, val_g_loss=8.343346027729382e+23
Epoch 95:g_trn_l=4.156632224014708e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:36
val_d_loss=-10392.455976486206, val_g_loss=1.631881676505223e+27: 100%|█| 250/25
Epoch 96: val_d_loss=-5.196227988243103, val_g_loss=8.159408382526115e+23
Epoch 96:g_trn_l=5.133901453651089e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34
val_d_loss=-7938.717601776123, val_g_loss=1.6561539731179674e+27: 100%|█| 250/25
Epoch 97: val_d_loss=-3.9693588008880614, val_g_loss=8.280769865589838e+23
Epoch 97:g_trn_l=5.745301492151623e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:33
val_d_loss=-7501.854158401489, val_g_loss=1.63275133052841e+27: 100%|█| 250/250
Epoch 98: val_d_loss=-3.7509270792007445, val_g_loss=8.16375665264205e+23
Epoch 98:g_trn_l=8.744793599716533e+23,d_trn_l=-8.6347: 100%|█| 2500/2500 [17:34
val_d_loss=-10151.39899635315, val_g_loss=1.6066991155063287e+27: 100%|█| 250/25
Epoch 99: val_d_loss=-5.075699498176575, val_g_loss=8.033495577531644e+23

![image](https://user-images.githubusercontent.com/24237829/194795764-dc6e653b-a1e6-4d21-9a6d-f5cfcab9e7a3.png)
"
Log Files from Training,caiyuanhao1998/PNGAN,2022-09-26 14:28:11,1,,4,1386182856,"Thank you for your awesome code!

I am hoping you might open-source the log files you have from training. Maybe the training and validation loss as a function of epoch
(and/or batch) with an estimate of the runtime?"
Dose the test reuslts on several images rather than whole videos represent the performance of video semantic segmentation methods?,SamvitJ/Accel,2022-10-09 07:28:51,0,,7,1402183066,"The task of video semantic segmentation is to segment each frame of videos. But only several frames are labeled in the test set, the test performance in experiments is on several images rather than whole videos. I think it can not represent the performance of video semantic segmentation methods. 
Did I misunderstand something here?"
exact video name of example,SamvitJ/Accel,2021-03-08 07:47:03,0,,6,824281738,"I want to make a comparison of the example you used to exam my own model. Could you please tell me the source of that? I mean, I have already extrated it from Cityscape sequence but I don't know the frame id and which video they are from.
![image](https://user-images.githubusercontent.com/46511939/110290236-80eb9480-8025-11eb-8cd0-3b2db01a2936.png)
"
MXNet version,SamvitJ/Accel,2020-10-10 11:39:06,1,,5,718601061,"README.md refers to [MXNet@(commit 62ecb60)](https://github.com/apache/incubator-mxnet/tree/62ecb60) version 0.9.3~0.9.5. But I had to use version 0.12.0 to run the code.

It doesn't seem like DeformableConvolution is accessible from `mx.contrib.symbol`. I get this error when I run 

`python ./dff_deeplab/demo.py --version 18 --interval 5 --num_ex 10`

```
Traceback (most recent call last):
  File ""./dff_deeplab/demo.py"", line 312, in <module>
    main()
  File ""./dff_deeplab/demo.py"", line 136, in main
    key_sym = sym_instance.get_key_test_symbol(config)
  File ""/home/daeyun/git/Accel/dff_deeplab/symbols/accel_18.py"", line 133, in get_key_test_symbol
    conv_feat = self.get_resnet_dcn(data)
  File ""/home/daeyun/git/Accel/dff_deeplab/symbols/resnet_v1_101_flownet_deeplab.py"", line 1235, in get_resnet_dcn
    res5a_branch2b = mx.contrib.symbol.DeformableConvolution(name='res5a_branch2b', data=res5a_branch2a_relu, offset=res5a_branch2b_offset,
AttributeError: 'module' object has no attribute 'DeformableConvolution'
```

When I search the codebase using `grep -iR ""deformable"" mxnet`, I see that there is an implementation in `./mxnet/3rdparty/tvm`. Is there something I need to do when I build mxnet to expose this?"
flownet result return Convolution5 * 2.5?,SamvitJ/Accel,2019-11-20 06:35:55,0,,4,525605384,Why do the results of flownet need to be multiplied by 2.5? 
The inference time,SamvitJ/Accel,2019-09-29 11:05:40,1,,3,499900840,"Hi, great work!
I just want to ask about the inference time.
You report the inference time /per frame for baseline DL-18 and AL-18 and DFF.
How to calculate this time?
Is this time including the data processing time? Or just the model forward time?
I want to compare the results with the accuracy and inference time with your paper.
Therefore, I just want to make sure how to make a fair comparison.

Yifan"
What is perturbation used during training for CIFAR-10,xuanqing94/RobGAN,2022-07-27 00:42:10,1,,7,1318889467,"What is the perturbation used for training? Are all the columns in the results, which displays testing at different perturbation, are for a single model? i.e, for all columns the model has same adversarial perturbation (what is it?)? 

Results for CIFAR-10 as in Table1 of paper (at: https://web.cs.ucla.edu/~chohsieh/papers/RobustGAN_CVPR_new.pdf) is:
<table>
<tr><th>perturbation</th>    <td>0 (nat acc)</td>   <td>0.02</td>    <td>0.04</td>           <td>0.08</td></tr>
<tr><th>Rob-GAN (w/FT)</th> <td>81.1%</td>      <td>70.41%</td>  <td>57.43%</td>    <td>30.25%</td></tr>
</table>

A bit more detail, about my Q:
- Paper mentions: δ max ∈ np.arange(0, 0.1, 0.01). What does this mean? Does it mean maximum perturbation is 0.1 and step size is 0.01? 
- Code seems to limit perturbation to ε. [[line 24](https://github.com/xuanqing94/RobGAN/blob/8a478cf3435387753baee2d3a82d039236cc4fab/miscs/pgd.py#L10):] <pre>diff.clamp_(-epsilon, epsilon)</pre> The github page says to set ε to 0.03125, but this would effectively make it half as for this code input is in range [-1, 1] not [0, 1]."
关于acc under attack.py,xuanqing94/RobGAN,2022-03-29 07:38:27,0,,6,1184443155,我想从这几个测试模型里加一个vgg19的模型看看他的准确率，我想请问一下具体该怎么做
some questions about training epoch,xuanqing94/RobGAN,2019-12-16 03:42:25,13,,4,538167757,can you tell me the training epoch number before finetuning for imagenet 143 category and cifar datasets
loss function,xuanqing94/RobGAN,2019-08-25 02:35:41,1,,3,484882777,"Thank you very much for your excellent work. When I ran the code you provided, I encountered some problems, several of which were not solved. I'd like to consult you.
1: What are the specific meanings of ""positive"" and ""negative"" in train.py?
2: In the paper, D_Loss = Ls + Lc1, G_Loss = Lc2 - Ls; but in the program, what I see is
Los_g = λL1 + (1-λ) L2, loss_d =λL1 + (1-λ) L2. Is the definition of loss function in the paper consistent with that in the program?
3: When I validate your program with other data sets, at the step of saving real images, the last four pictures are always black,such as 
![image](https://user-images.githubusercontent.com/46865749/63644819-f3d33a00-c723-11e9-8c05-26c1a6d11857.png). I can't find the reason. I hope you can give me some suggestions.
Thank you again for your help!"
annotation don't match images ,bdd100k/bdd100k,2022-09-19 17:14:40,0,,275,1378274293,"I am trying to train yolo7 seg model on lane marking , I downloaded the 100k images then labels from ""Lane Marking"" in json will be exported as txt for yolo , I tried many things but I am not sure, which images and labels exactly should I download to get started for training a lane marking model , thank you . "
whereis removed area? I cannot find them,bdd100k/bdd100k,2022-07-14 10:39:43,0,,261,1304597070,"![b](https://user-images.githubusercontent.com/9405676/178963960-66fb1a2f-5dc3-4e56-8a3a-ba01fc363d1c.jpg)
I find I miss some important information！ where is removed area's labels?"
Questions about calculation of mAP,bdd100k/bdd100k,2022-07-13 07:45:04,0,,260,1303039920,"![image](https://user-images.githubusercontent.com/40333030/178679205-51f420e5-f440-4ee6-8802-d3fe1f8486af.png)
It seems mAP is not on the benchmark. How do you calcualate it. Thank you!

#https://github.com/SysCV/pcan/issues/19#issue-1301743798"
Semantic Segmentation JSON File has images with labels of categories which are listed under Panoptic Segmentation!,bdd100k/bdd100k,2022-06-28 20:12:34,0,,259,1287830157,"Hi Guys!!
I am currently using the sem_seg_train.json which I downloaded by clicking on ""segmentation"" on [this](https://bdd-data.berkeley.edu/portal.html#download) website. According to the [documentation](https://doc.bdd100k.com/format.html#semantic-segmentation) the semantic segmentation is evaluated on the following 19 categories:

```
0:  road
1:  sidewalk
2:  building
3:  wall
4:  fence
5:  pole
6:  traffic light
7:  traffic sign
8:  vegetation
9:  terrain
10: sky
11: person
12: rider
13: car
14: truck
15: bus
16: train
17: motorcycle
18: bicycle
```

But unfortunately when I am reading the json file and trying to convert to another format which I am going to use for training I come across image inforation with labels having categories fro the [Panoptic Segmentation](https://doc.bdd100k.com/format.html#panoptic-segmentation) task.

![image](https://user-images.githubusercontent.com/66687588/176275458-57e68451-426f-4d26-9c83-3984afbdb9d9.png)


My current list of categories for Semaantic Segmentation looks something like this:
![image](https://user-images.githubusercontent.com/66687588/176275590-664b5083-3b8c-4005-a3d8-e8576510acea.png)

### Notice the last three categories, ""static, dynamic and street light"" they belong to the Panoptic Segmentation Task. 
I had to add these to the list because of the KeyError.

Do you suggest I should directly use all categories i.e categories for Panoptic Segmentation for both semantic and panoptic tasks? or is there some mistake from my side? Some feedback would be very helpful and highly appreciateds. Thanks in advance :)

"
Unable to download 100k images,bdd100k/bdd100k,2022-06-15 02:42:09,0,,255,1271592817,"### Discussed in https://github.com/bdd100k/bdd100k/discussions/254

<div type='discussions-op-text'>

<sup>Originally posted by **alabsihuda** June 15, 2022</sup>
Hi. I am getting the following error while trying to download the 100k images.
![100k image](https://user-images.githubusercontent.com/89980562/173724646-e6f36c01-d94a-4737-8877-185a28d3d2ce.PNG)
</div>"
eval.ai is not working,bdd100k/bdd100k,2022-05-30 06:55:01,4,,247,1252278626,"My submission to eval.ai has been in the status of SUBMITTED for about one day without any feedback. Execution time is N/A and result/stdout/stderr are all None.

My submission was to BDD100K Multiple Object Tracking / Testing Phase.
We follow instructions on [BDD100K->Evaluation->Multiple Object Tracking](https://doc.bdd100k.com/evaluate.html#multiple-object-tracking) and use the following zip command to create ""a zip file of a folder that contains JSON files of each video.""
```
zip test_bdd.zip data/c*
```

Could you please kindly help us to figure out whether it is a submission format or an eval server issue?"
Lane line instance segmentation,bdd100k/bdd100k,2022-05-09 09:00:43,1,,237,1229398721,"In this dataset, there is no lane line instance segmentation dataset. that is, lane line classification. How to generate lane line classification instance segmentation dataset?"
"""MOT 2020 Labels"" and ""MOT 2020 Images""",bdd100k/bdd100k,2022-04-10 05:43:26,3,,221,1198889143,"### Discussed in https://github.com/bdd100k/bdd100k/discussions/204

<div type='discussions-op-text'>


Hi,
I am not able to download the MOT 2020 Images from the given links of .zip files. I have tried using chrome as well as firefox browsers but I am getting the same issue. The download stops after downloading some 400-500 MBs. I have even tried downloading it using terminal (wget command) but facing the same problem.
Can anyone suggest some solution to this?

Thanks</div>"
No annotations for 137 images in train,bdd100k/bdd100k,2022-02-15 21:55:09,2,,210,1139253227,"I found 137 images in the train dataset have no annotation, because they are absent from the annotation .json file. 
Here is the list, tell me if the problem is reproducible

[differences_137.txt](https://github.com/bdd100k/bdd100k/files/8074969/differences_137.txt)
 "
"TypeError: expected str, bytes or os.PathLike object, not NoneType",bdd100k/bdd100k,2021-11-11 05:58:12,1,,191,1050603190,"Excuse me ，i want Convert ""ins_seg_train.json"" to coco，i used this command ""python -m bdd100k.label.to_coco -m ins_seg -i ins_seg_train.json -o ins_seg_train_coco.json"".
but the program error：
[2021-11-11 13:53:21,270 to_coco.py:537 main] Loading annotations...
[2021-11-11 13:53:31,594 to_coco.py:546 main] Start format converting...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:00<00:00, 46697.44it/s]
[2021-11-11 13:53:31,746 to_coco.py:321 bdd100k2coco_ins_seg] Collecting annotations...
  0%|                                                                                                                                                    | 0/7000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File ""/home/stan/anaconda3/envs/labelme/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/stan/anaconda3/envs/labelme/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/stan/wkstion/labeltool/bdd100k/bdd100k/label/to_coco.py"", line 557, in <module>
    main()
  File ""/home/stan/wkstion/labeltool/bdd100k/bdd100k/label/to_coco.py"", line 548, in main
    coco = convert_func(frames=frames, config=bdd100k_config.scalabel)
  File ""/home/stan/wkstion/labeltool/bdd100k/bdd100k/label/to_coco.py"", line 344, in bdd100k2coco_ins_seg
    image_anns.name.replace("".jpg"", "".png""),
  File ""/home/stan/anaconda3/envs/labelme/lib/python3.7/posixpath.py"", line 80, in join
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType

What should I do ~！thanks!
"
About code bases,msraig/pfcnn,2022-03-06 08:24:16,0,,5,1160549776,"Hi，authors,I cant't find the same 'GeometricTools/WildMagic5/SDK' and 'ann_1.1.2_built'  code base on the Internet, can you share these two code bases?"
can this method be used for part segmentation,msraig/pfcnn,2021-03-30 14:56:12,2,,4,844688575,"hi, I want to ask a question, can this be used in part-segmentation, such as shapenet mesh segmentation?"
About the extension to point clouds,msraig/pfcnn,2021-02-26 09:22:26,2,,3,817186879,"Hello, authors. I saw in the paper that PFCNN can be extended to point clouds easily, but I don't know how to implement it. Can you give me some advice? Thank you very much."
pySurfaceHierarchy/include/DirectionFields.hpp:12:24: fatal error: Wm5Matrix3.h: no such file or directory ,msraig/pfcnn,2020-12-18 07:47:23,4,,2,770670230,"Hi, authors, thank you very much to publish you source code, but when I try to compile the pySurfaceHierarchy with cmake, I got this error ""pySurfaceHierarchy/include/DirectionFields.hpp:12:24: fatal error: Wm5Matrix3.h: no such file or directory "", and I cannot found the Wm5Matrix3.h, if you can give me some advice I will be very grateful.

BTW,If someone met the error and fix it, could you pls help me with it, thank you very much. "
about the code,msraig/pfcnn,2020-06-25 05:19:16,0,,1,645160491,"Hi, @haopan ,

I'm interested in your work and would like to study it further. When will the code be released?

THX!"
When will you release the code?,benyv/uncord,2021-12-09 08:44:17,1,,4,1075303738,
HUMBI dataset website can no longer login,zhixuany/HUMBI,2022-04-20 08:23:41,0,,30,1209342370,"Hi,

Thank you for your great contribution to our society. Currently, I am trying to download data from the official website https://humbi-data.net/. However, neither could I register a new account nor could I use my teammate's account to login(Displaying 'Login Failed, Please Try Again""). Would you mind to check whether the website is still supported? Looking forward to your reply.

Best,
S.F"
Question about clothing models,zhixuany/HUMBI,2021-11-23 12:35:35,0,,27,1061236409,"Hi. Thanks for publishing this useful dataset. I have a question about the clothing model in the dataset. As far as I understand, the method you used to obtain clothes (ClothCap) deforms the SMPL body into clothes. However, in the dataset, the clothing meshes have different numbers of vertices. Could you please let me know how I can make SMPL ""dress"" or how to match the vertices of clothing models back to SMPL?
Thank you very much!"
human mask ,zhixuany/HUMBI,2021-10-20 02:34:04,0,,26,1030912041,"As the noisy background in the image, is there the human mask available ?"
broken links to point cloud data and github repo,zhixuany/HUMBI,2021-09-27 10:40:27,1,,25,1007986404,"Hi folks,

just wanted to bring your attention to the fact that the links to point cloud data on [the download page](https://www.humbi-data.net/05-body-clothing-datasets/) seem to be broken. To fix them, one should cut everything after **?**:

https://......./pointcloud/subject_1_80.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-.....

->

https://......./pointcloud/subject_1_80.zip

Also, the Software link [on the main page](https://www.humbi-data.net/) is broken.

Thanks for the great work!

~Sergey
"
Questions about projection of provided 3D keypoints,zhixuany/HUMBI,2021-04-22 13:05:27,0,,20,864927937,"I want to see 2D keypoints of certain frame on images so i did projection processing. 
but i got wrong result. what's the matter?? 
As i know about getting 2D points is {2D keypoints = homogeneous(projection matrix * 3D keypoints)}
Could you tell me what's the problem...??
More details about my code:
i want to see image0000000.jpg & projected 2D keypoints each frame.

```
import torch
import numpy as np
from lib.models.smpl import SMPL
import os
import pdb
sub_path = '/Body_1_80_updat/subject_1/body/'
frames = os.listdir(sub_path)[:-4]
proj = open(sub_path+'project.txt','r').readlines()

projection = np.zeros((107,3,4))
extrinsic = np.zeros((107,3,4))
intrinsic = np.zeros((107,3,3))
cam_KR = np.zeros((107,3,4))
for i in range(len(projection)):
    projection[i,0] = np.array(proj[4+4*i].split(),dtype=np.float32)
    projection[i, 1] = np.array(proj[5 + 4*i].split(),dtype=np.float32)
    projection[i, 2] = np.array(proj[6 + 4*i].split(),dtype=np.float32)
    

import cv2

imglist = list(range(107))
for frame in frames:
    img_path =  '/Body_1_80_updat/subject_1/body/%s/image/'%frame
    img_list = os.listdir(img_path)
    keypoints_path ='/Body_1_80_updat/subject_1/body/%s/reconstruction/keypoints.txt'%frame
    keypoints_3d= np.loadtxt(keypoints_path).reshape(-1,3)
    dummy = np.ones((len(keypoints_3d), 1))
    kpts = np.concatenate((keypoints_3d, dummy), axis=1)
    kpts = np.matmul(kpts, projection[0].T)
    kpts = kpts[:,:2]/kpts[:,2].reshape(-1,1)


    img = cv2.imread(img_path+img_list[0])
    for joint in kpts:
        cv2.circle(img, (int(joint[0]),int(joint[1])),10,(255,0,0),-1)

    img = cv2.resize(img, (512,512))
    cv2.imshow('img', img)
    k = cv2.waitKey()
    if k == 27:
        cv2.destroyAllWindows()
```
![image](https://user-images.githubusercontent.com/54057384/115719341-f4b7e580-a3b6-11eb-93f7-a228f2ae12f9.png)
"
关于论文的问题,LTH14/FSKD,2021-08-13 11:04:21,3,,2,970297722,这里我有个问题：裁剪后的模型（剩余层）学习教师模型保留的参数（也是剩余层），无非两个一样的层之间多了个1×1卷积，应该没有区别的吗？
What would be the reason of missing detection in front of velodyne 32c?,princeton-computational-imaging/SeeingThroughFog,2022-11-02 09:44:37,2,,60,1432811125,"Hello STF team, 

First of all, thank you for your contribution, this is a amazing dataset. But I have a question, I'm wondering why there is missing detection in front of the vlp32c in many frames, I'm expecting unwanted detections of snowflakes and road reflection. 

![Screenshot from 2022-11-02 10-35-08](https://user-images.githubusercontent.com/20644522/199456252-1872cb0a-b841-406d-80c2-f8624b0f850b.png)

Best regards"
How do you calculate entropy?,princeton-computational-imaging/SeeingThroughFog,2021-09-03 11:16:16,1,,47,987668094,"The equation 1 in the article, what function is δ？"
Training object detector on lidar,princeton-computational-imaging/SeeingThroughFog,2021-05-19 20:27:57,9,,45,895855024,"Has anyone successfully trained an object detector (yolov3, ssd, etc...) on just lidar_hdl64_last_stereo_left or lidar_hdl64_strong_stereo_left? I'm getting pretty trash results, which I believe is due to the sparsity of the points. Anyone got tips for training? "
Foggification lost points,princeton-computational-imaging/SeeingThroughFog,2021-05-16 14:07:11,0,,44,892678754,"Hi,

to better understand the proposed foggification I had a look at the code but couldn't find where the lost points are actually discarded. The probabilities are computed but then only used for selecting points that are scattered.
To be precise, I'm talking about this method:
https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/094b95f64e1a1ed8a57e556984be16a8f8482cbc/tools/DatasetFoggification/lidar_foggification.py#L35-L112

In the end, `old_points` are returned although only the points with a distance larger than `dmax` were removed. Or am I misunderstanding the algorithm?

I also found this related issue: #21"
Network Architecture Code,princeton-computational-imaging/SeeingThroughFog,2021-03-19 01:15:35,0,,34,835424083,"Hi,
First of all, I have to say your work is really fascinating and the data you've provided is so wealthy and useful.
Moreover, I am highly interested in working on a topic relevant to this paper. So, the network architecture of this paper would be extremely helpful for its progress. I saw other comments with the same request and your response about improving it as well. I believe it would be so great if you share the network architecture code even before your intended improvement for many interested researchers.
Thanks!"
lidar foggification ralationed question,princeton-computational-imaging/SeeingThroughFog,2020-11-02 03:00:53,1,,21,734141589,"I hava review the supplement documents and  relevant code。but I have some question :in algorithm1 ,just shows the area where the red box is located,may be somecode is lost .
![Snipaste_2020-11-02_10-57-31](https://user-images.githubusercontent.com/33310310/97825702-6792ae80-1cfa-11eb-8519-100b98885ed1.png)
"
Request for the Detailed Network Script or Architecture,princeton-computational-imaging/SeeingThroughFog,2020-10-02 07:47:58,0,,13,713423306,"Hello, 

I read your paper and would like to reproduce it. However, the details of the architecture in Figure 4 are not clear. Could you please release the network script? If not, could you please provide a table of detailed network layers? 

Thank you. "
h5py version (integrity failure),prasunroy/stefann,2021-06-25 00:29:06,1,bug#enhancement,20,929715645,"I followed the installation instructions, but kept getting the following message.


`[DEBUG] Loading application... integrity failure`


There was no problem with my CUDA settings, and tensorflow was picking up my GPU just fine, so I tried taking stuff out of try/exceptions and found out that keras was failing on loading the models.


    original_keras_version = f.attrs['keras_version'].decode('utf8')
AttributeError: 'str' object has no attribute 'decode'


Googled it, and found that it works at h5py version 2.10.0.
I think it would be nice if h5py versions were also included in conda settings.

Thank you anyways for your awesome work.
I'll try it out now since my bug is fixed.
"
Questions about the input of FANnet.,prasunroy/stefann,2021-05-26 03:35:34,0,question,19,901700735,"  Hello, after reading your paper, I have some questions about the input of FANnet.
  In the paper, one of the inputs to FANnet is ""**_a one-hot encoding v of length 26 of the target character_**"", where 26 represents 26 capital letters. Have you ever tried to change 26 to 62 (uppercase letters + lowercase letters + numbers), because the same font theoretically has some commonalities, so they should also be able to complete the font style transfer. I am interested in this question. Do you have any comments or suggestions?"
SSIM calculation in fannet,prasunroy/stefann,2021-05-07 10:01:20,0,question,18,878707600,"I appreciate your excellent work on text editing. I tried to run FANnet with pretrained model on your datasets.  So I downloaded the pretrained weights from [here](https://drive.google.com/drive/folders/16-mq3MOR1zmOsxNgegRmGDeVRyeyQ0_H) and datasets from [here](https://drive.google.com/drive/folders/1dOl4_yk2x-LTHwgKBykxHQpmqDvqlkab) following README.

To generate results using the valid set as the input, I modified `fannet.py` and  ran the following code
```python
from skimage.metrics import structural_similarity as ssim
for data in valid_datagen.flow():
    [x, onehot], y = data
    out = fannet.predict([x, onehot]) 
    n = x.shape[0]
    for i in range(n):
        _x = x[i].reshape(64, 64)
        _gt = y[i].reshape(64, 64)
        _out = out[i].reshape(64, 64)
        _, _out_bin = cv2.threshold(_out,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
        sv = ssim(_gt, _out_bin, data_range=255, gaussian_weights=True, sigma=1.5, use_sample_covariance=False)
        print(sv)
```
 But the SSIM value `sv` is far from what was claimed in the paper. I also tried to calculate the average SSIM w.r.t different source characters. There is also a large gap. So I am wondering if there exists some mistakes when running the model or just the SSIM calculation. 
"
Performance of colornet,prasunroy/stefann,2021-04-24 17:37:41,1,question,17,866798624,"Hello, I noticed that in the stefann.py file, you have provided three methods for style transfer, transfer_color_pal, transfer_color_max, and the colornet implementation. I uncommented the lines from 480-487 in the stefann.py file to use the colornet model. When I tested the application on images from 'sample_images' folder using the colornet method with the provided pretrained weights, it did not work very well and produced blurry and inconsistent results. The results of colornet did not match the results given in the 'editing_examples' folder.

Given Result (Left - Original Image, Right - stefann generated image)
![08](https://user-images.githubusercontent.com/35864383/115967328-b275dc00-a54f-11eb-8850-0c14e2e71b0e.jpg)
Output using colornet
![08_colornet](https://user-images.githubusercontent.com/35864383/115967267-514e0880-a54f-11eb-8634-d6ef9ad633b6.jpg)

There are many more cases where the colornet model is not performing as expected. Could you please help me with this? "
Support for Chinese Language,prasunroy/stefann,2021-01-14 21:52:25,1,enhancement#help wanted,14,786366916,"Hi, thanks for your hard work on this project. It's really cool!
I've seen issue #7  but I still have some doubts. I would like to try to replace english text with it's corresponding chinese translation, but how can I do so if characters are stored in jpg file named as ASCII numbers? Chinese it's not included in ASCII.
Another question regarding chinese is, do I also need to generate new images, one for each character, in the colornet directory?
Your help would be much appreciated!"
How to edit numbers?,prasunroy/stefann,2021-01-08 08:19:39,1,question,13,781938974,"Hi,

Thanks for sharing your awesome work! I am wondering if you have a pretrained model to edit numbers? Thanks."
Making word longer?,prasunroy/stefann,2020-08-20 19:26:28,3,question,9,683041863,"Now I'm playing around with Stefann a bit. Your [readme](https://github.com/prasunroy/stefann/blob/master/release/readme.html) was really helpful here! But I guess there's more knobs and dials to it than I expected. Was still wondering is there e.g. a way to make the word you're replacing longer? Also, your examples seem way more crips than my quick shot at it. Got any tips?

Original:
![backspace orig](https://user-images.githubusercontent.com/2487783/90815259-b4393100-e32a-11ea-80d6-192b58bd8d9e.jpg)
Edited
![backspace-button_2020-08-20-21-15-52](https://user-images.githubusercontent.com/2487783/90815382-e77bc000-e32a-11ea-8f33-c7301550daf3.jpg)
"
Can't get it to run due to 'integrity failure'.,prasunroy/stefann,2020-08-16 09:37:16,9,bug#help wanted,8,679723168,"Hi,

I was trying out your project but I'm stuck installing dependencies. I tried the conda installation method but it keeps failing (tried cpu, gpu and osx yamls). 
`[DEBUG] Loading application... integrity failure`

I also think the yaml file here 
https://github.com/prasunroy/stefann/blob/acac8fec6985e5e651313472738dbf36bbc27199/release/env_cpu.yml#L12
and here is faulty:
https://github.com/prasunroy/stefann/blob/acac8fec6985e5e651313472738dbf36bbc27199/release/env_gpu.yml#L12

I guess it should look more like the OSX version with a double `==` and tensorflow below pip.
https://github.com/prasunroy/stefann/blob/acac8fec6985e5e651313472738dbf36bbc27199/release/env_osx.yml#L10-L15
However even when I edit the env_cpu.yaml that way it seems to fail.

It could be related to this issue: https://github.com/tensorflow/tensorflow/issues/37316
but when I tried `tensorflow>=2.1.0 ` it also failed.

Any idea how to solve this? Or is there an alternative way to install it? A docker container maybe?"
MSG-cycleGAN,akanimax/BMSG-GAN,2022-03-17 12:10:43,2,,47,1172298907,"I try to implement msg way to cycleGAN for two weeks.
The generator : Unet-withSkipConnection and downsample(k=4, s=2 conv) upsample(Deconv) 8x.

But the D_loss, both D_A and D_B always decreased near to zero  and G_loss(lsgan) always stuck in 0.01 ...

What I want to know is whether this combination will work? msg + cycleGAN? 
Anybody tried before?

"
Question about projecting generated images back to latent space,akanimax/BMSG-GAN,2021-08-02 12:58:02,0,,46,958116342,Is there a way we can project back the images to get the latent vectors just like it is available in StyleGan2 repo?
How to process the resolution of Flower dataset,akanimax/BMSG-GAN,2021-05-10 20:47:48,0,,44,885078612,"Hi,

Thank you for sharing your work. It seems that the resolution of Flower dataset is not 256x256. How do you process this dataset, resize of crop? Thanks."
Error(s) in loading state_dict for DataParallel in generating_samples.py,akanimax/BMSG-GAN,2021-03-29 09:45:24,0,,43,843191145,"After a whole training with

$ python3 sourcecode/train.py --depth=7 --latent_size=256 --images_dir=DATASET --sample_dir=sampledir/exp2 --model_dir=modeldir/exp2

when launching 

$ python3 sourcecode/generate_samples.py --generator_file=modeldir/exp2/GAN_GEN_100.pth --depth=7 --out_dir=outputdir

I got: 

'' 
Creating generator object ...
Loading the generator weights from: modeldir/exp2/GAN_GEN_100.pth
Traceback (most recent call last):
  File ""sourcecode/generate_samples.py"", line 134, in <module>
    main(parse_arguments())
  File ""sourcecode/generate_samples.py"", line 105, in main
    gen.load_state_dict(
  File ""~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1051, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for DataParallel:
        Missing key(s) in state_dict: ""module.layers.7.conv_1.weight"", ""module.layers.7.conv_1.bias"", ""module.layers.7.conv_2.weight"", ""module.layers.7.conv_2.bias"", ""module.layers.8.conv_1.weight"", ""module.layers.8.conv_1.bias"", ""module.layers.8.conv_2.weight"", ""module.layers.8.conv_2.bias"", ""module.rgb_converters.7.weight"", ""module.rgb_converters.7.bias"", ""module.rgb_converters.8.weight"", ""module.rgb_converters.8.bias"".
''

Both training and generation are executed on the same machine with multiple GPUs enabled and ready."
WGAN-GP loss: averaging penalties or not?,akanimax/BMSG-GAN,2020-11-30 11:12:56,0,,41,753386088,"Hi @akanimax 

First of all many thanks for this amazing contribution. 

In the paper, it is stated `we modified the gradient penalty to be the average of the penalties over each input`.
I didn't see any of that in the original MSG-GAN tensorflow implementation, neither in the BMSG-GAN implementation: it seems rather like only the gradient penalty of the higher resolution image is used.

Did I miss something, or is it a slight variation between the paper and the implementation?

Cheers

edit: related to #35 "
 when I train the model ? The  d_loss is always 0.0 ? Is there some problems ? ,akanimax/BMSG-GAN,2020-09-28 07:46:07,4,,40,710031665,"
Elapsed [0:05:20.807550] batch: 252  d_loss: 0.000000  g_loss: 9.195433
Elapsed [0:05:32.021947] batch: 261  d_loss: 0.000000  g_loss: 9.544848
Elapsed [0:05:43.313830] batch: 270  d_loss: 0.000000  g_loss: 8.533874
Elapsed [0:05:54.605321] batch: 279  d_loss: 0.000000  g_loss: 8.899170
Elapsed [0:06:06.190011] batch: 288  d_loss: 0.000000  g_loss: 18.478725
Elapsed [0:06:17.612306] batch: 297  d_loss: 0.000000  g_loss: 7.378197
"
"Why smaller batch size results  better results?  With the epochs increased, the G loss went up?",akanimax/BMSG-GAN,2020-09-01 09:34:24,1,,39,689987683,"There are 8k+ 256*256 images in my datasets, I set batchsize=32 and trained with 4 12GB GPUs, but it's not as good as I set batchsize = 4 and trained with one GPU,neither the training speed nor the image quality. Why a smaller batch size results  better results? Is there a best batchsize?
Beside,I set batchsize=4, when epoch>70, g-loss went up obviously,and qualities of generated images went worse.Why did this happen? (Training process was interrupted when epoch = 54, I reloaded weight files and optimer states from epoch 53 )
Thanks!"
Model collapse,akanimax/BMSG-GAN,2020-06-05 14:48:37,5,,38,631655995,"Why are all the images generated by me using the trained network exactly the same?
I observed the training process, and the images generated using different noises are still somewhat different, but gradually they become the same image. What is the reason? Is my training data too small?"
demo.py,akanimax/BMSG-GAN,2020-05-27 12:24:30,0,,36,625649955,"I trained the model  with:
python train.py --depth=7 --latent_size=512 --images_dir=../data/flowers --sample_dir=samples/exp_1 --model_dir=models/exp_1

and I test with:
python demo.py --generator_file=models/exp_1/GAN_DIS_3.pth --depth=7 --latent_size=512

but some error:


Missing key(s) in state_dict: ""module.layers.0.conv_1.weight"", ""module.layers.0.conv_1.bias"", ""module.layers.0.conv_2.weight"", ""module.layers.0.conv_2.bias"", ""module.layers.1.conv_1.weight"", ""module.layers.1.conv_1.bias"", ""module.layers.1.conv_2.weight"", ""module.layers.1.conv_2.bias"", ""module.layers.2.conv_1.weight"", ""module.layers.2.conv_1.bias"", ""module.layers.2.conv_2.weight"", ""module.layers.2.conv_2.bias"", ""module.layers.3.conv_1.weight"", ""module.layers.3.conv_1.bias"", ""module.layers.3.conv_2.weight"", ""module.layers.3.conv_2.bias"", ""module.layers.4.conv_1.weight"", ""module.layers.4.conv_1.bias"", ""module.layers.4.conv_2.weight"", ""module.layers.4.conv_2.bias"", ""module.layers.5.conv_1.weight"", ""module.layers.5.conv_1.bias"", ""module.layers.5.conv_2.weight"", ""module.layers.5.conv_2.bias"", ""module.layers.6.conv_1.weight"", ""module.layers.6.conv_1.bias"", ""module.layers.6.conv_2.weight"", ""module.layers.6.conv_2.bias"", ""module.rgb_converters.0.weight"", ""module.rgb_converters.0.bias"", ""module.rgb_converters.1.weight"", ""module.rgb_converters.1.bias"", ""module.rgb_converters.2.weight"", ""module.rgb_converters.2.bias"", ""module.rgb_converters.3.weight"", ""module.rgb_converters.3.bias"", ""module.rgb_converters.4.weight"", ""module.rgb_converters.4.bias"", ""module.rgb_converters.5.weight"", ""module.rgb_converters.5.bias"", ""module.rgb_converters.6.weight"", ""module.rgb_converters.6.bias"". 
	Unexpected key(s) in state_dict: ""rgb_to_features.0.weight"", ""rgb_to_features.0.bias"", ""rgb_to_features.1.weight"", ""rgb_to_features.1.bias"", ""rgb_to_features.2.weight"", ""rgb_to_features.2.bias"", ""rgb_to_features.3.weight"", ""rgb_to_features.3.bias"", ""rgb_to_features.4.weight"", ""rgb_to_features.4.bias"", ""rgb_to_features.5.weight"", ""rgb_to_features.5.bias"", ""final_converter.weight"", ""final_converter.bias"", ""layers.0.conv_1.weight"", ""layers.0.conv_1.bias"", ""layers.0.conv_2.weight"", ""layers.0.conv_2.bias"", ""layers.1.conv_1.weight"", ""layers.1.conv_1.bias"", ""layers.1.conv_2.weight"", ""layers.1.conv_2.bias"", ""layers.2.conv_1.weight"", ""layers.2.conv_1.bias"", ""layers.2.conv_2.weight"", ""layers.2.conv_2.bias"", ""layers.3.conv_1.weight"", ""layers.3.conv_1.bias"", ""layers.3.conv_2.weight"", ""layers.3.conv_2.bias"", ""layers.4.conv_1.weight"", ""layers.4.conv_1.bias"", ""layers.4.conv_2.weight"", ""layers.4.conv_2.bias"", ""layers.5.conv_1.weight"", ""layers.5.conv_1.bias"", ""layers.5.conv_2.weight"", ""layers.5.conv_2.bias"", ""final_block.conv_1.weight"", ""final_block.conv_1.bias"", ""final_block.conv_2.weight"", ""final_block.conv_2.bias"", ""final_block.conv_3.weight"", ""final_block.conv_3.bias"". 


how to solve it?"
WGAN-GP for Training MSG_ProGAN is failed,akanimax/BMSG-GAN,2020-03-26 03:51:38,3,,34,588135187,"Hi, I read your newest version paper (v3) in arxiv, and i saw that add MSG_styleGAN information, I'm glad to see it , but I have some question about paper : 

1. I think you paper said the result of showing in paper is trained MSG_ProGAN by using WGAN-GP, and trained MSG_styleGAN by using Non-saturating GAN loss with 1-sided gp.  This repo is for training MSG_ProGAN but default GAN loss is **relativistic-hinge** and then I changed it to wgan-gp it failed ,because of not support for mult-scale Discriminator , Can you update this function code. (B.T.W) I implemented by myself but I don't right or false)

2. why default using  **relativistic-hinge** not **WGAN-gp**"
Why output of torch.cat() is still C channels??,akanimax/BMSG-GAN,2020-03-24 03:12:23,0,,33,586659495,"https://github.com/akanimax/BMSG-GAN/blob/d06316974d1d84bd2077f8c558ebaf9d967205df/sourcecode/MSG_GAN/CustomLayers.py#L342

I think input feature x , the shape of **x** is (BS, C, H, W)
and the shape of **y** is (BS, 1, H, W)
so the output of concate them should be (BS, C+1, H, W), but (BS, C, H, W) in code ?"
I want to stop and resume the training continuously.,akanimax/BMSG-GAN,2020-03-13 08:12:18,3,,31,580439286,"Hi, @markstrefford @owang @sridharmahadevan @akanimax @huangzh13 

I want to stop and resume the training continuously with saved model.

My resuming commad : ""$ python3 train.py --start 300""  # I stop at 350 epoch

But, My Training sample is started from the scratch. --; 

What's wrong to me ??

Thanks.
Best,
@bemoregt."
pretrained models,akanimax/BMSG-GAN,2020-02-22 02:17:26,0,,30,569265908,Could you please share the 128*128 pretrained models on celebA?
About the performance of BMSG-GAN,akanimax/BMSG-GAN,2019-11-23 03:20:15,3,,25,527502187,"Hi, you published BBMSG-GAN. And In order to match the performance of ProGANs, you increased the size of the Discriminator (the one here in BMSG-GAN is technically half that of ProGAN) and use tricks like Batch Spoofing. So, do you mean that the performance of Pro-GANs is better than BMSG-GAN? But BBMSG-GAN is better than Pro-GANs? Of course, the number of parameters is bigger?"
How do you generate samples on a trained sagemaker model?,akanimax/BMSG-GAN,2019-07-22 13:42:13,3,,18,471102823,"Hi,

Thanks for a cool implementation. Your notebook ends at fitting - now I am a little bit confused on how to actually generate samples on the fit sagemaker model? 

Can you just deploy and predict? 

Thanks!"
deprecated function call in generate_samples,akanimax/BMSG-GAN,2019-07-03 22:44:57,1,,16,463983568,"I am a bit lame with GitHub pull requests so here is a note that a call is deprecated:

generate_samples.py:126: DeprecationWarning: `imsave` is deprecated!
`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.
Use ``imageio.imwrite`` instead.
  ss_image.squeeze(0).permute(1, 2, 0).cpu())"
Sagemaker environment vars should not be required,akanimax/BMSG-GAN,2019-05-29 08:36:46,0,,13,449670985,USE_SAGEMAKER is checked at the beginning of train.py but not in the arg code which includes a few sagemaker variables. Easy enough to comment out but would be nice just to pull the USE_SAGEMAKER through completely in the code for all variables.
A runtime error occurred while running 'train.py'...,akanimax/BMSG-GAN,2019-05-17 14:33:54,2,,10,445472864,"I encountered this error when I was training in the following configuration：

python train.py --images_dir='data/bird' --folder_distributed=True --sample_dir='samples/exp_1' --model_dir='models/exp_1'

I changed default options in train.py where
 ""default='os.environ['SM_MODEL_DIR'],""  to   “samples/exp_1',” ""default=os.environ['SM_CHANNEL_TRAINING'],“  to   ""default='data/bird',"",
""default='os.environ['SM_MODEL_DIR'],""    to   “samples/exp_1',”

The dataset is loaded correctly.
but it failed in training with this error :

Traceback (most recent call last):
  File ""train.py"", line 267, in <module>
    main(parse_arguments())
  File ""train.py"", line 261, in main
    start=args.start
  File ""/data/user/BMSG-GAN/sourcecode/MSG_GAN/GAN.py"", line 417, in train
    for (i, batch) in enumerate(data, 1):
  File ""/data/user/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 582, in __next__
    return self._process_next_batch(batch)
  File ""/data/user/.local/lib/python3.5/site-packages/torch/utils/data/dataloader.py"", line 608, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
RuntimeError: Traceback (most recent call last):
  File ""/data/user/.local/lib/python3.5/site-packages/torch/utils/data/_utils/worker.py"", line 99, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File ""/data/user/.local/lib/python3.5/site-packages/torch/utils/data/_utils/worker.py"", line 99, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File ""/data/user/BMSG-GAN/sourcecode/data_processing/DataLoader.py"", line 132, in __getitem__
    img = self.transform(img)
  File ""/data/user/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py"", line 60, in __call__
    img = t(img)
  File ""/data/user/.local/lib/python3.5/site-packages/torchvision/transforms/transforms.py"", line 163, in __call__
    return F.normalize(tensor, self.mean, self.std, self.inplace)
  File ""/data/user/.local/lib/python3.5/site-packages/torchvision/transforms/functional.py"", line 208, in normalize
    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])
**RuntimeError: output with shape [1, 128, 128] doesn't match the broadcast shape [3, 128, 128]**

I think this error has nothing to do with my modification.
Could you please tell me what caused this error?

![runtimeerror](https://user-images.githubusercontent.com/33981458/57935189-d48e6a80-78f3-11e9-9d49-3c43f6da2fc5.PNG)

"
I have met this error when run train.py ...,akanimax/BMSG-GAN,2019-04-23 08:50:11,21,,5,436071032,"Hi, @owang @sridharmahadevan @akanimax @huangzh13 

I have met this error when run train.py ...
What's wrong to me?

oem@sgi:~/BMSG-GAN/sourcecode$ python3 train.py --depth=7 --latent_size=128 --images_dir='../data/celebJapan/train' --sample_dir=samples/exp_2 --model_dir=models/exp_2
Total number of images in the dataset: 6604

error message -
Starting the training process ... 

Epoch: 1
Elapsed [0:00:04.581270] batch: 1  d_loss: 4.346926  g_loss: 6.674685
Traceback (most recent call last):
  File ""train.py"", line 254, in <module>
    main(parse_arguments())
  File ""train.py"", line 248, in main
    start=args.start
  File ""/home/oem/BMSG-GAN/sourcecode/MSG_GAN/GAN.py"", line 482, in train
    gen_img_files)
  File ""/home/oem/BMSG-GAN/sourcecode/MSG_GAN/GAN.py"", line 345, in create_grid
    samples = [Generator.adjust_dynamic_range(sample) for sample in samples]
  File ""/home/oem/BMSG-GAN/sourcecode/MSG_GAN/GAN.py"", line 345, in <listcomp>
    samples = [Generator.adjust_dynamic_range(sample) for sample in samples]
  File ""/home/oem/BMSG-GAN/sourcecode/MSG_GAN/GAN.py"", line 96, in adjust_dynamic_range
    data = data * scale + bias
TypeError: mul() received an invalid combination of arguments - got (numpy.float32), but expected one of:
 * (Tensor other)
      didn't match because some of the arguments have invalid types: (numpy.float32)
 * (Number other)
      didn't match because some of the arguments have invalid types: (numpy.float32)

Thanks in advance ~"
Training at half-/mixed-precision?,akanimax/BMSG-GAN,2019-04-21 13:25:44,4,,4,435509651,"Seeing very interesting results so far with the code in this repo. I'm wondering if @akanimax or anyone else has had any luck speeding up training with this repo by using half-precision or mixed-precision?

"
The use of the loss function.,akanimax/BMSG-GAN,2019-04-02 06:49:53,30,,2,428052420,"When I use **wgan-gp** as a loss function, the training fails. Any explanation?"
Do you have any pretrained models for celeba-hq or ,akanimax/BMSG-GAN,2019-04-01 03:48:31,0,,1,427499014,Nice work. It helps me a lot. Can you please provide any pretrained model for celeba-hq or ffhq (the new released dataset in stylegan paper)
is it a bug in HyperbolicMLR?,KhrulkovV/hyperbolic-image-embeddings,2022-09-05 10:07:27,0,,15,1361704256,"Thanks a lot for sharing your repo. I am new to hyperbolic neural networks, and I learned a lot from your repo.
While i am reading the code of examples/mnist.py, i am curious about why `ToPoincare` is only applied for x which is calculated by [a neural network](https://github.com/leymir/hyperbolic-image-embeddings/blob/6633edbbeffd6d90271f0963852a046c64f407d6/examples/mnist.py#L34), whereas, `ToPoincare` is not applied for p_val in [HyperbolicMLR](https://github.com/leymir/hyperbolic-image-embeddings/blob/6633edbbeffd6d90271f0963852a046c64f407d6/hyptorch/nn.py#L30).

https://github.com/leymir/hyperbolic-image-embeddings/blob/6633edbbeffd6d90271f0963852a046c64f407d6/hyptorch/nn.py#L30

consider that they are both need a `expmap0` to map them from $R^2$ to $D^2$, won't p_val get a wrong gradient?
"
Visualization of the feature embeddings in the hyperbolic space,KhrulkovV/hyperbolic-image-embeddings,2022-07-18 12:57:01,2,,14,1307919976,"Hi Valentin,

Thanks for your excellent work and codes. I am very interested in the hyperbolic space, and would like to know how to visualize feature embeddings in the hyperbolic space (as shown in the Fig. 1 of your paper). I didn't find the codes for the feature visualization. Can you please release the codes?

Thanks a lot. 

Xiaohan
"
mnist example get loss nan,KhrulkovV/hyperbolic-image-embeddings,2022-04-12 09:01:25,5,,13,1201468727,"when i run the mnist example i get the loss is nan, is the ToPoincare cause result?"
could you release the code for reid？,KhrulkovV/hyperbolic-image-embeddings,2020-10-12 09:08:59,0,,8,719209223,
Reproducing Results,KhrulkovV/hyperbolic-image-embeddings,2020-09-25 15:39:13,0,,7,709034336,"Hello authors,

Sorry to add another issue about this -- I've read everything in the existing (closed) issues, but I'm still having problems reproducing the results. 

Here are is what I ran and what I got for CUB 1s5w (conv4):
```bash
# CUB 1s5w (conv4) 0.6402 +- 0.002
python train_protonet.py \
  --dataset CUB \
  --shot 1 \
  --lr 0.001 \
  --step 50 \
  --gamma 0.8 \
  --c 0.05 \
  --model convnet \
  --hyperbolic \
  --not-riemannian \
  --dim 1600

### OUTPUT ###
batch 9998: 62.48(49.33)
batch 9999: 62.47(54.67)
batch 10000: 62.47(54.67)
Val Best Acc 0.6809, Test Acc 0.6247
Test Acc 0.6247 + 0.0024
```

Here are is what I ran and what I got for MiniImageNet 1s5w (conv4): 
```bash
# MiniImageNet 1s5w (conv4) 0.5443 +- 0.002
python train_protonet.py \
  --dataset MiniImageNet \
  --way 30 \
  --shot 1 \
  --lr 0.005 \
  --step 80 \
  --gamma 0.5 \
  --c 0.01 \
  --model convnet \
  --hyperbolic \
  --not-riemannian \
  --dim 1600

### OUTPUT ###
batch 9998: 52.15(44.00)
batch 9999: 52.15(60.00)
batch 10000: 52.15(42.67)
Val Best Acc 0.5362, Test Acc 0.5215
Test Acc 0.5215 + 0.0020
```"
Poincare distance to origin,KhrulkovV/hyperbolic-image-embeddings,2020-06-21 00:12:14,3,,5,642467604,"Thanks a lot for sharing the repo! In your paper, you presented a nice visualization showing the distribution of hyperbolic distances of embeddings (MNIST vs. Omniglot). I'm wondering from which stage did you obtain the embeddings to calculate those distances?

My guess is the output of the line below. Can you please confirm this is indeed the case?

https://github.com/leymir/hyperbolic-image-embeddings/blob/05b104e7b84b4b01e0a82640363f9dc3981b8adb/examples/mnist.py#L34

Thanks!"
图像可视化展示,microsoft/Relation-Aware-Global-Attention-Networks,2022-07-15 09:35:37,0,,29,1305809621,请问能提供下图像可视化程序吗？
Pre-trained weights for Market-1501,microsoft/Relation-Aware-Global-Attention-Networks,2021-10-14 14:00:02,0,,27,1026437048,"Hello everyone :) 

can you provide pre-trained weights for the Market-1501 dataset? Would be great! :)

Best, 
Jenny "
IndexError: min(): Expected reduction dim 1 to have non-zero size.,microsoft/Relation-Aware-Global-Attention-Networks,2021-10-07 08:07:13,1,,26,1019743632,"After I ran the 'bash ./scripts/run_rgasc_cuhk03.sh' order, the log reports error like below:

Traceback (most recent call last):
  File ""C:\Users\24282\Desktop\RAG\main_imgreid.py"", line 261, in <module>
    main(parser.parse_args())
  File ""C:\Users\24282\Desktop\RAG\main_imgreid.py"", line 178, in main
    trainer.train(epoch, train_loader, optimizer, random_erasing=args.random_erasing, empty_cache=args.empty_cache)
  File ""C:\Users\24282\Desktop\RAG\reid\img_trainers.py"", line 43, in train
    loss, all_loss, prec1 = self._forward(ori_inputs, targets)
  File ""C:\Users\24282\Desktop\RAG\reid\img_trainers.py"", line 94, in _forward
    loss_tri = self.criterion[1](outputs[0], targets)
  File ""C:\Users\24282\Desktop\RAG\reid\loss\loss_set.py"", line 133, in __call__
    dist_ap, dist_an = hard_example_mining(
  File ""C:\Users\24282\Desktop\RAG\reid\loss\loss_set.py"", line 86, in hard_example_mining
    dist_an, relative_n_inds = torch.min(
IndexError: min(): Expected reduction dim 1 to have non-zero size.

I checked the 'loss_set.py', the logic is reasonable. I couldn't figure out why there is such an 'IndexError', have any one met the same problem before? Please Help!"
报错,microsoft/Relation-Aware-Global-Attention-Networks,2021-09-28 05:08:45,10,,25,1009244599,"D:\PyCode\Detection-PyTorch-Notebook-master\venv\Scripts\python.exe D:/PyCode/PersonReID/Relation-Aware-Global-Attention-Networks/main_imgreid.py
Traceback (most recent call last):
  File ""D:/PyCode/PersonReID/Relation-Aware-Global-Attention-Networks/main_imgreid.py"", line 299, in <module>
    main(parser.parse_args())
  File ""D:/PyCode/PersonReID/Relation-Aware-Global-Attention-Networks/main_imgreid.py"", line 138, in main
    args.combine_trainval, args.rerank)
  File ""D:/PyCode/PersonReID/Relation-Aware-Global-Attention-Networks/main_imgreid.py"", line 89, in get_data
    num_classes = dataset.num_train_pids
UnboundLocalError: local variable 'dataset' referenced before assignment

Process finished with exit code 1"
CrossEntropyLoss & TripletLoss issue,microsoft/Relation-Aware-Global-Attention-Networks,2021-08-31 09:42:23,0,,24,983657919,"When I trained this model, I have set up Lr=0.0008, weight_decay=0.0005 and I have checked the dim of  feat_ and cls_feat, all of them are correct. But the CrossEntropyLoss fluctuates around 6 and it didn't reduce with epochs increasing. And the TripletLoss is 0.00000 or 0.45000 and in most cases it is 0.0000. And the prec is very low which fluctuates between 0 and 1. 
Does anyone know the reason for this porblem? May be we can discuss about it. Thanks."
"ys = torch.cat((g_xs, Gs_joint), 1  this line of code give Cuda out of memory error .how to fix it ?",microsoft/Relation-Aware-Global-Attention-Networks,2021-06-30 14:01:50,2,,23,933743437,
is it possible to run your model with single GPU?,microsoft/Relation-Aware-Global-Attention-Networks,2021-06-30 14:00:48,0,,22,933742293,
about embedding function shared,microsoft/Relation-Aware-Global-Attention-Networks,2021-06-24 07:58:33,0,,20,928954214,"From this paper, we can see that the parameters of all the modules including spatial and channel are shared, which means it is same weight in a certain position for all training images.

But if the distribution of images changes such as occlusion, can the performance be maintained?"
please look into the traceback . what could be the problem here,microsoft/Relation-Aware-Global-Attention-Networks,2021-06-23 11:26:27,0,,19,928153773,"Traceback (most recent call last):
  File ""train.py"", line 106, in <module>
    main(opt)
  File ""train.py"", line 51, in main
    num_classes=opt.num_classes, branch_name='rgasc')
  File ""/content/FairMOT/src/lib/models/__init__.py"", line 16, in create
    return __factory[name](*args, **kwargs)
  File ""/content/FairMOT/src/lib/models/rga_model.py"", line 112, in resnet50_rga
    return ResNet50_RGA_Model(*args, **kwargs)
  File ""/content/FairMOT/src/lib/models/rga_model.py"", line 84, in __init__
    s_ratio=scale, c_ratio=scale, d_ratio=d_scale, model_path=model_path)
  File ""/content/FairMOT/src/lib/models/models_utils/rga_branches.py"", line 107, in __init__
    self.rga_att1 = RGA_Module(256, (height//4)*(height/4), use_spatial=spa_on, use_channel=cha_on,
TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'"
about Ablation experiment in other datasets,microsoft/Relation-Aware-Global-Attention-Networks,2021-06-23 01:21:39,0,,18,927766966,"how can I repeat this project in other datasets such as Market1501, MSMT17 ?  this work only provides the version of CUHK03

In other word, how to rectify  the configuration of processing dataset.

Can anyone help me?"
What's the difference between RGA and ordinary attention module?,microsoft/Relation-Aware-Global-Attention-Networks,2021-02-20 02:36:58,0,,16,812492420,"It seems like RGA module is different from ordinary attention module, like the one used in ABD-Net.
In your article, the main advantage of RGA is that we can extract global relation information with less computational costs. However, spatial attention module and channel attention module in ABD-Net seem to be more efficient since there's no extra computational costs on feature embedding.
Could you tell me what's the advantage of RGA over ordinary attention module?
<img width=""906"" alt=""att"" src=""https://user-images.githubusercontent.com/34710660/108580682-854a5900-7367-11eb-8533-0953983ceb59.png"">
![20200811201340578](https://user-images.githubusercontent.com/34710660/108580683-867b8600-7367-11eb-8033-c9e10c4a8a2b.png)

"
with cuhk03 dataset,microsoft/Relation-Aware-Global-Attention-Networks,2020-11-27 02:24:02,4,,13,751936735,"i have not rectified any hyper parameters.
use_weight_eopch: 200
The test result is

Evaluated with ""feat_"" features and ""cosine"" metric:
Mean AP: 36.5%
CMC Scores
  top-1          41.7%
  top-5          63.1%
  top-10         72.2%
Evaluated with ""feat"" features and ""cosine"" metric:
Mean AP: 37.6%
CMC Scores
  top-1          42.3%
  top-5          62.8%
  top-10         73.6% 

da lao men
Could you help me ?"
How to compatible the different size of images?,microsoft/Relation-Aware-Global-Attention-Networks,2020-11-05 06:54:20,3,,12,736666958,"Thanks for your sharing. I have a questions about this work. If this work can compatible different size of image(width and height)?
Thanks for your reply."
./logs/RGA-SC/cuhk03labeled_b64f2048 not exists!!!,microsoft/Relation-Aware-Global-Attention-Networks,2020-08-08 04:58:51,7,,10,675430656,"root@405762d7523b:/home/RGA# bash ./scripts/run_rgasc_cuhk03.sh
./logs/RGA-SC/cuhk03labeled_b64f2048 not exists!!!
Begin to train.
Begin to test.
root@405762d7523b:/home/RGA#

这个文件是存在的呀？为什么出错了？怎么解决？
希望得到您们的有效答案，非常感谢！
"
performance gap with the reported results using cuhk03 (L),microsoft/Relation-Aware-Global-Attention-Networks,2020-08-03 07:13:46,10,,9,671854539,"Hi, i run the exact script `run_rgasc_cuhk03.sh` but the results are really different compared with the one you reported in the paper. I also had to skip `layer1.0.bn1.num_batches_tracked` and `bn1.num_batches_tracked` when loading the specific parameters. not sure if this could be the problem.

```
=> CUHK03 (labeled) loaded
Dataset statistics:
  ------------------------------
  subset   | # ids | # images
  ------------------------------
  train    |   767 |     7368
  query    |   700 |     1400
  gallery  |   700 |     5328
  ------------------------------
  total    |  1467 |     8768
  ------------------------------
Num of features: 2048.
Use_Spatial_Att: True;	Use_Channel_Att: True.
Use_Spatial_Att: True;	Use_Channel_Att: True.
Use_Spatial_Att: True;	Use_Channel_Att: True.
Use_Spatial_Att: True;	Use_Channel_Att: True.
=> Loaded checkpoint './logs/RGA-SC/cuhk03labeled_b64f2048/checkpoint_600.pth.tar'
```
Here are my results
```
Evaluated with ""feat_"" features and ""cosine"" metric:
Mean AP: 74.8%
CMC Scores
  top-1          77.0%
  top-5          89.9%
  top-10         94.4%
Evaluated with ""feat"" features and ""cosine"" metric:
Mean AP: 74.0%
CMC Scores
  top-1          75.9%
  top-5          89.1%
  top-10         93.4%
```
The paper reports a rank-1 accuracy of `81.1` while I get `77.0`. This is a huge gap (`4.1`)

![image](https://user-images.githubusercontent.com/1701705/89155277-18be6700-d59b-11ea-8c3e-4907b9e13d68.png)
"
（height//4）*（width//4）是不是相当于在空间上做池化呢？,microsoft/Relation-Aware-Global-Attention-Networks,2020-07-30 12:52:17,1,,8,668691565,考虑到减少计算量，所以做整除处理，不知道我这样理解对不对
为什么一个dataset愣是能写得这么复杂,microsoft/Relation-Aware-Global-Attention-Networks,2020-07-29 06:27:23,2,,7,667586645,感觉正常1/5的代码量足够了，代码简单易懂，容易修改你的star就上去了
with marker dataset ,microsoft/Relation-Aware-Global-Attention-Networks,2020-07-28 01:24:17,3,,6,666683843,"Evaluated with ""feat_"" features and ""cosine"" metric:
Mean AP: 0.4%
CMC Scores
  top-1           0.1%
  top-5           0.4%
  top-10          0.5%
Evaluated with ""feat"" features and ""cosine"" metric:
Mean AP: 0.3%
CMC Scores
  top-1           0.1%
  top-5           0.2%
  top-10          0.5%

market.py as fellow
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import glob
import re
import sys
import urllib
import tarfile
import zipfile
import os.path as osp
from scipy.io import loadmat
import numpy as np
import h5py
from scipy.misc import imsave

from ..utils.osutils import mkdir_if_missing
from ..utils.serialization import write_json, read_json


class Market(object):
    """"""
    Args:
        split_id (int): split index (default: 0)
        cuhk03_labeled (bool): whether to load labeled images; if false, detected images are loaded (default: False)
    """"""
    dataset_dir = 'cuhk03'
    # dataset_dir = 'CUHK03_New'

    def __init__(self, root='data', split_id=0, cuhk03_labeled=False, cuhk03_classic_split=False, verbose=True, **kwargs):
        super(Market, self).__init__()
        self.dataset_name = ""market""
        self.num_train_pids = 0
        self.num_train_imgs = 0
        self.num_query_pids = 0
        self.num_query_imgs = 0
        self.num_gallery_pids = 0
        self.num_gallery_imgs = 0

        ##############################################################
        # 以下代码是自己的
        data_dir = ""/data/server77_data/rrjia/Market-1501-v15.09.15""
        # data_dir = ""Z:\\rrjia\\Market-1501-v15.09.15""
        if osp.isdir(data_dir):
            self.data_dir = data_dir

        self.train_dir = osp.join(self.data_dir, 'bounding_box_train')
        self.query_dir = osp.join(self.data_dir, 'query')
        self.gallery_dir = osp.join(self.data_dir, 'bounding_box_test')

        required_files = [
            self.data_dir,
            self.train_dir,
            self.query_dir,
            self.gallery_dir,
        ]
        self.check_before_run(required_files)

        train = self.process_dir(self.train_dir)
        query = self.process_dir(self.query_dir, is_train=False)
        gallery = self.process_dir(self.gallery_dir, is_train=False)

        self.train = train
        self.query = query
        self.gallery = gallery

        if verbose:
            print(""Dataset statistics:"")
            print(""  ------------------------------"")
            print(""  subset   | # ids | # images"")
            print(""  ------------------------------"")
            print(""  train    | {:5d} | {:8d}"".format(self.num_train_pids, self.num_train_imgs))
            print(""  query    | {:5d} | {:8d}"".format(self.num_query_pids, self.num_query_imgs))
            print(""  gallery  | {:5d} | {:8d}"".format(self.num_gallery_pids, self.num_gallery_imgs))
            print(""  ------------------------------"")

    def check_before_run(self, required_files):
        """"""Checks if required files exist before going deeper.
        Args:
            required_files (str or list): string file name(s).
        """"""
        if isinstance(required_files, str):
            required_files = [required_files]

        for fpath in required_files:
            if not os.path.exists(fpath):
                raise RuntimeError('""{}"" is not found'.format(fpath))

    def process_dir(self, dir_path, is_train=True):
        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))
        pattern = re.compile(r'([-\d]+)_c(\d)')

        data = []
        pid_set = set()
        image_num = 0
        pid_list = list()
        for img_path in img_paths:
            pid, camid = map(int, pattern.search(img_path).groups())
            if pid == -1:
                continue  # junk images are just ignored
            assert 0 <= pid <= 1501  # pid == 0 means background
            assert 1 <= camid <= 6
            camid -= 1  # index starts from 0
            if pid in pid_list:
                pass
            else:
                pid_list.append(pid)
            new_pid = pid_list.index(pid)
            pid_set.add(new_pid)
            image_num += 1
            data.append((img_path, new_pid, camid))

        if dir_path.endswith(""bounding_box_train""):
            self.num_train_pids = len(pid_set)
            self.num_train_imgs = image_num
        elif dir_path.endswith(""query""):
            self.num_query_pids = len(pid_set)
            self.num_query_imgs = image_num
        else:
            self.num_gallery_pids = len(pid_set)
            self.num_gallery_imgs = image_num
        return data


"
关于non-local 和 RGA连接权重的区别,microsoft/Relation-Aware-Global-Attention-Networks,2020-07-15 01:56:06,1,,5,657003140,"hi 您好：
最近在精读您的文章，但是一直有一个疑问，暂时没有想通，RGA如何学得基于位置变化的一个连接权重？看了下non-local,主要区别是一个是直接加，在这里进行了一个stack操作，还望大佬解惑！"
KeyError: 'bn1.num_batches_tracked',microsoft/Relation-Aware-Global-Attention-Networks,2020-06-22 03:15:33,11,,4,642717017,"  这段代码state_dict[i].copy_(param_dict[key])出错，使用的是torchvision 0.4.0
KeyError: 'bn1.num_batches_tracked'"
can you provide the marker1501.py,microsoft/Relation-Aware-Global-Attention-Networks,2020-06-20 00:34:13,2,,3,642274312,can you provide the market1501.py and other py files of different datasets in the folder data_manager of your project? Thank you?
Other datasets,microsoft/Relation-Aware-Global-Attention-Networks,2020-06-18 11:37:10,0,,2,641136270,"How to run the experiments on other datasets? (e.g., market, duke, msmt17)"
Slow Training Speed compared to Yolov5,open-mmlab/mmdetection,2022-11-08 10:11:43,0,,9279,1439888033,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

/opt/conda/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
fatal: detected dubious ownership in repository at '/home/jovyan/train-volume-big/mmdetection'
To add an exception for this directory, call:

        git config --global --add safe.directory /home/jovyan/train-volume-big/mmdetection
sys.platform: linux
Python: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) [GCC 10.3.0]
CUDA available: True
GPU 0: Tesla V100-SXM2-16GB
CUDA_HOME: None
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.8.1+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.1+cu111
OpenCV: 4.6.0
MMCV: 1.7.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.25.3

### Reproduces the problem - code sample

Train the YOLOX-s model. 
Compare the time per epoch to that of YOLOv5s model.
MMdetection YOLOX-s: 20min/epoch on V100
YOLOv5-s: 7:30min/epoch on V100
I know that those are not the exact same models, but they are kind of similar in sizes.
Why is the training speed so different?


### Reproduces the problem - command or script

Run YOLOX training and YOLOv5 training on same dataset

### Reproduces the problem - error message

- 

### Additional information

Training speed should be somewhat similar"
ImportError: cannot import name 'SelfAttentionBlock' from 'mmdet.models.utils' (H:\SoftWare\Anaconda\envs\mmdt\lib\site-packages\mmdet-2.25.2-py3.8.egg\mmdet\models\utils\__init__.py),open-mmlab/mmdetection,2022-11-08 02:42:32,1,,9262,1439363460,"### What's the feature?

Hello, I encountered an error while training faster mcnn with mmdetection. All the files in mmdet\models\utils could not find the module 'SelfAttentionBlock' in the error

### Any other context?

_No response_"
[Bug] Missing argument --show-score-thr for tools/test.py if visualized.,open-mmlab/mmdetection,2022-11-07 16:17:51,2,Doc#v-3.x,9258,1438625452,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I have modified the scripts/configs, or I'm working on my own tasks/models/datasets.

### Branch

3.x branch https://github.com/open-mmlab/mmdetection/tree/3.x

### Environment

From `pip freeze`,  according to '[Bug] collect_env.py fails due to circular import  #9194':
```
addict==2.4.0
brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1648854175163/work
certifi==2022.9.24
cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1656782821535/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work
click==8.1.3
colorama==0.4.5
commonmark==0.9.1
contourpy==1.0.5
cryptography @ file:///tmp/build/80754af9/cryptography_1652083738073/work
cycler==0.11.0
fonttools==4.37.4
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work
importlib-metadata==5.0.0
kiwisolver==1.4.4
Markdown==3.4.1
matplotlib==3.6.1
mmcls==1.0.0rc2
mmcv==2.0.0rc1
# Editable install with no version control (mmdet==3.0.0rc1)
-e /home/users/jiaqi.ma/mmdetection
mmengine==0.1.0
# Editable install with no version control (mmsegmentation==1.0.0rc1)
-e /home/users/jiaqi.ma/mmsegmentation
model-index==0.1.11
numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1666092554508/work
opencv-python==4.6.0.66
openmim==0.3.2
ordered-set==4.1.0
packaging==21.3
pandas==1.5.1
Pillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1660385857893/work
prettytable==3.5.0
pycocotools==2.0.5
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work
Pygments==2.13.0
pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work
pyparsing==3.0.9
PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work
python-dateutil==2.8.2
pytz==2022.5
PyYAML==6.0
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1661872987712/work
rich==12.6.0
six==1.16.0
tabulate==0.9.0
termcolor==2.0.1
terminaltables==3.1.10
torch==1.12.1.post201
torchvision @ file:///home/conda/feedstock_root/build_artifacts/torchvision-split_1658664666016/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1665144421445/work
urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1658789158161/work
wcwidth==0.2.5
yapf==0.32.0
zipp==3.9.0
```

### Reproduces the problem - code sample

`tools/test.py`

### Reproduces the problem - command or script

`python tools/test.py configs/faster_rcnn/faster-rcnn_r50_fpn_2x_coco.py  ./epoch_xx.pth  --show-dir ./xxx --show-score-thr 0.5`

### Reproduces the problem - error message

```
usage: test.py [-h] [--work-dir WORK_DIR] [--out OUT] [--show] [--show-dir SHOW_DIR] [--wait-time WAIT_TIME]
               [--cfg-options CFG_OPTIONS [CFG_OPTIONS ...]] [--launcher {none,pytorch,slurm,mpi}] [--local_rank LOCAL_RANK]
               config checkpoint
test.py: error: unrecognized arguments: --show-score-thr 0.5
```

### Additional information

It seems that `--show-score-thr` is implemented in master branch, and it is mentioned in `test.md` in 3.x."
[Bug] faster-rcnn_r50-caffe_fpn_ms-2x and 3x don't work ,open-mmlab/mmdetection,2022-11-07 07:21:12,2,v-3.x,9254,1437886341,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

3.x branch https://github.com/open-mmlab/mmdetection/tree/3.x

### Environment

OrderedDict([('sys.platform', 'linux'),
             ('Python', '3.7.15 (default, Oct 12 2022, 19:14:55) [GCC 7.5.0]'),
             ('CUDA available', True),
             ('numpy_random_seed', 2147483648),
             ('GPU 0', 'Tesla T4'),
             ('CUDA_HOME', '/usr/local/cuda'),
             ('NVCC', 'Cuda compilation tools, release 11.2, V11.2.152'),
             ('GCC',
              'x86_64-linux-gnu-gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0'),
             ('PyTorch', '1.9.0+cu111'),
             ('PyTorch compiling details',
              'PyTorch built with:\n  - GCC 7.3\n  - C++ Version: 201402\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 11.1\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n  - CuDNN 8.0.5\n  - Magma 2.5.2\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n'),
             ('TorchVision', '0.10.0+cu111'),
             ('OpenCV', '4.6.0'),
             ('MMEngine', '0.3.0'),
             ('MMDetection', '2.25.3+5b0d5b4')])

### Reproduces the problem - code sample
（the other models work well）
```python
!mim download mmdet --config faster-rcnn_r50-caffe_fpn_ms-2x_coco --dest ./checkpoints
```
and then we see 
`faster_rcnn_r50_caffe_fpn_mstrain_2x_coco_bbox_mAP-0.397_20200504_231813-10b2de58.pth`  or `faster_rcnn_r50_caffe_fpn_mstrain_3x_coco_20210526_095054-1f77628b`



### Reproduces the problem - command or script

``` python
from mmdet.apis import init_detector, inference_detector
from mmdet.utils import register_all_modules
from mmdet.registry import VISUALIZERS
import mmcv
register_all_modules()
config_file = './checkpoints/faster-rcnn_r50-caffe_fpn_ms-2x_coco.py'
checkpoint_file = './checkpoints/faster_rcnn_r50_caffe_fpn_mstrain_2x_coco_bbox_mAP-0.397_20200504_231813-10b2de58.pth'

model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'
image = mmcv.imread( ""demo/demo.jpg"", channel_order='rgb')
result = inference_detector(model, image)

print(result)
```


### Reproduces the problem - error message

and i see:
```
local loads checkpoint from path: ./checkpoints/faster_rcnn_r50_caffe_fpn_mstrain_2x_coco_bbox_mAP-0.397_20200504_231813-10b2de58.pth
<DetDataSample(

    META INFORMATION
    img_path: None
    pad_shape: (800, 1216)
    scale_factor: (1.8734375, 1.873536299765808)
    img_id: 0
    ori_shape: (427, 640)
    batch_input_shape: (800, 1216)
    img_shape: (800, 1199)

    DATA FIELDS
    gt_instances: <InstanceData(
        
            META INFORMATION
        
            DATA FIELDS
            labels: tensor([], dtype=torch.int64)
            bboxes: tensor([], size=(0, 4))
        ) at 0x7f8e7cabc150>
    _ignored_instances: <InstanceData(
        
            META INFORMATION
        
            DATA FIELDS
            labels: tensor([], dtype=torch.int64)
            bboxes: tensor([], size=(0, 4))
        ) at 0x7f8e7cabce90>
    _pred_instances: <InstanceData(
        
            META INFORMATION
        
            DATA FIELDS
            scores: tensor([])
            labels: tensor([], dtype=torch.int64)
            bboxes: tensor([], size=(0, 4))
        ) at 0x7f8e7f14b190>
    _gt_instances: <InstanceData(
        
            META INFORMATION
        
            DATA FIELDS
            labels: tensor([], dtype=torch.int64)
            bboxes: tensor([], size=(0, 4))
        ) at 0x7f8e7cabc150>
```

All results are empty.

### Additional information

_No response_"
[Bug]  serve docker file does not build (current master branch),open-mmlab/mmdetection,2022-11-07 01:57:37,0,,9253,1437653106,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

N/A, this is for building the docker image 

### Reproduces the problem - code sample

In the `docker/serve` folder:

```bash
docker build . --tag mmdetection-serve:latest
```

### Reproduces the problem - command or script

In the `docker/serve` folder:

```bash
docker build . --tag mmdetection-serve:latest
```

### Reproduces the problem - error message

```
Sending build context to Docker daemon   5.12kB
Step 1/26 : ARG PYTORCH=""1.6.0""
Step 2/26 : ARG CUDA=""10.1""
Step 3/26 : ARG CUDNN=""7""
Step 4/26 : FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel
 ---> bb833e4d631f
Step 5/26 : ARG MMCV=""1.3.17""
 ---> Running in 06a2ddd98d65
Removing intermediate container 06a2ddd98d65
 ---> 435930d73ac0
Step 6/26 : ARG MMDET=""2.25.3""
 ---> Running in dd3a767ffa05
Removing intermediate container dd3a767ffa05
 ---> bd22e79492b0
Step 7/26 : ENV PYTHONUNBUFFERED TRUE
 ---> Running in e40ebc92a587
Removing intermediate container e40ebc92a587
 ---> ff545d20710e
Step 8/26 : RUN apt-get update &&     DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y     ca-certificates     g++     openjdk-11-jre-headless     ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6     && rm -rf /var/lib/apt/lists/*
 ---> Running in cac9edd9d559
Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]
Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease
Get:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]
Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]
Get:6 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]
Err:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC
Get:7 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]
Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [73.8 kB]
Get:9 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1554 kB]
Get:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1226 kB]
Get:11 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3040 kB]
Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]
Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]
Get:15 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]
Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]
Get:17 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]
Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3472 kB]
Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [37.1 kB]
Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1267 kB]
Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2332 kB]
Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [64.0 kB]
Get:23 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [20.6 kB]
Reading package lists...
W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC
E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.
The command '/bin/sh -c apt-get update &&     DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y     ca-certificates     g++     openjdk-11-jre-headless     ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6     && rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100
```

### Additional information

_No response_"
"[Bug] I created a new folder under the mmdet, which is called mmdet.distillation, and some error occurred.",open-mmlab/mmdetection,2022-11-06 13:26:11,2,,9251,1437404531,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I have modified the scripts/configs, or I'm working on my own tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

sys.platform: linux
Python: 3.9.13 (main, Oct 13 2022, 21:15:33) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0
OpenCV: 4.6.0
MMCV: 1.6.2
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.2+


### Reproduces the problem - code sample

   I created a new folder under the mmdet, which is called distillation, and parallel to the mmdet.dataset and mmdet.models（the first picture）. The files in mmdet.distillation is in this link, https://github.com/yzd-v/FGD, I followed the suggestions in this link to put some files into mmdetection, and when I ran the train.py file(modified in above link, from mmdet.distillation import build_distiller ), the following error occurred. I don't know how to change, Thank you for your advice.

![01](https://user-images.githubusercontent.com/90205209/200173057-888a57cc-fa4f-4c07-b23d-32c2f1085b96.png)


![QQ截图20221105212601](https://user-images.githubusercontent.com/90205209/200172866-857efcc5-f331-435e-88e4-59388f5c1a2e.png)


### Reproduces the problem - command or script

.

### Reproduces the problem - error message

.

### Additional information

The dataste is COCO.
I think maybe the new module is not recognized by the compiler.
"
"[Bug] Runing detection node after realsense node is runing, an error will occur.",open-mmlab/mmdetection,2022-11-05 11:13:04,1,,9247,1436975093,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I have modified the scripts/configs, or I'm working on my own tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

sys.platform: linux
Python: 3.8.10 (default, Jun  4 2021, 15:09:15) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 2060
CUDA_HOME: None
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.7.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.25.2+9d3e162
 
ROS2 foxy
Realsense-ros
opencv

How you installed PyTorch: conda
Other environment variables that may be related: 
PATH:
/home/bread/anaconda3/envs/pytorch-test/bin:/home/bread/ros2_ws/install/costmap_converter/bin:/home/bread/autodock_ros2_ws/install/apriltag_ros/bin:/home/bread/autodock_ros2_ws/install/apriltag/bin:/opt/ros/foxy/bin:/home/bread/anaconda3/condabin:/home/bread/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin

PYTHONPATH:
/home/bread/ros2_ws/install/teb_msgs/lib/python3.8/site-packages:/home/bread/ros2_ws/install/ros2_aruco_interfaces/lib/python3.8/site-packages:/home/bread/ros2_ws/install/ros2_aruco/lib/python3.8/site-packages:/home/bread/ros2_ws/install/robot_vision_ros2/lib/python3.8/site-packages:/home/bread/ros2_ws/install/robot_navigation_ros2/lib/python3.8/site-packages:/home/bread/ros2_ws/install/robot_description/lib/python3.8/site-packages:/home/bread/ros2_ws/install/realsense2_camera_msgs/lib/python3.8/site-packages:/home/bread/ros2_ws/install/costmap_converter_msgs/lib/python3.8/site-packages:/home/bread/ros2_ws/install/base_control_ros2/lib/python3.8/site-packages:/home/bread/autodock_ros2_ws/install/autodock_msgs/lib/python3.8/site-packages:/home/bread/autodock_ros2_ws/install/apriltag_msgs/lib/python3.8/site-packages:/home/bread/dev_ws/install/two_wheeled_robot/lib/python3.9/site-packages:/opt/ros/foxy/lib/python3.8/site-packages

### Reproduces the problem - code sample

#after i launch realsense camera node (ros2 launch realsense2_camera rs_launch.py)
#then i launch another python file whitch subscribe color stream topic for finding object bounding box, and the import down below #is in it. 
from mmdet.models import build_detector

import pyrealsense2 as rs
import numpy as np
import cv2
import mmcv
from mmcv.runner import load_checkpoint
from rcl_interfaces.msg import ParameterDescriptor
from geometry_msgs.msg import PoseStamped,Twist 
from mmdet.apis import inference_detector, show_result_pyplot
from mmdet.models import build_detector
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import message_filters
import cv2
from rclpy.node import Node
from cv_bridge import CvBridge, CvBridgeError
import rclpy
class FrameListener(Node):
    def __init__(self):
        super().__init__('get_image')

### Load model
        config = '/home/bread/mmdetection/configs/pallet/yolact_r50_1x8_pallet_20220711.py'
        # Setup a checkpoint file to load
        checkpoint = '/home/bread/mmdetection/work_dir/latest.pth'
        # Set the device to be used for evaluation
        self.device='cuda:0'

        # Load the config
        config = mmcv.Config.fromfile(config)
        # Set pretrained to be None since we do not need pretrained model here
        config.model.pretrained = None

        # Initialize the detector
        self.model = build_detector(config.model)

        # Load checkpoint
        checkpoint = load_checkpoint(self.model, checkpoint, map_location=self.device)

        # Set the classes of models for inference
        self.model.CLASSES = checkpoint['meta']['CLASSES']

        # We need to set the model's cfg for inference
        self.model.cfg = config

        # Convert the model to GPU
        self.model.to(self.device)
        # Convert the model into evaluation mode
        self.model.eval()

### Reproduces the problem - command or script

ros2 launch realsense2_camera rs_launch.py
python3 DLnodetest.py (for finding center point of bounding box)

### Reproduces the problem - error message

free(): invalid pointer
Aborted (core dumped)

### Additional information

I have a I python file for finding center point from detected bounding box without ros parameter，when i run this file alone, it works well, But when i run the detection node after the realsense node is running(ros2 launch realsense2_camera rs_launch.py), the error will occurd."
[Bug] multiple deletion of best checkpoint while using dist_train.sh,open-mmlab/mmdetection,2022-11-05 07:01:19,1,v-3.x,9246,1436877059,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I have modified the scripts/configs, or I'm working on my own tasks/models/datasets.

### Branch

3.x branch https://github.com/open-mmlab/mmdetection/tree/3.x

### Environment

System environment:
    sys.platform: linux
    Python: 3.8.13 (default, Oct 21 2022, 23:50:54) [GCC 11.2.0]
    CUDA available: True
    numpy_random_seed: 429034773
    GPU 0,1,2,3,4,5,6,7: NVIDIA A40
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.5, V11.5.119
    GCC: gcc (GCC) 8.5.0 20210514 (Red Hat 8.5.0-10)
    PyTorch: 1.10.0
    PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

    TorchVision: 0.11.0
    OpenCV: 4.6.0
    MMEngine: 0.3.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8

### Reproduces the problem - code sample

```python
default_hooks = dict(
    logger=dict(type='LoggerHook', interval=interval),
    checkpoint=dict(
        interval=interval,
        max_keep_ckpts=3,
        save_best=['coco/bbox_mAP'],
          # only keep latest 3 checkpoints
    ))
```

### Reproduces the problem - command or script

```
bash tools/dist_train.sh configs/rtmdet/rtmdet_x_8xb16_300e_panicle.py 8 --cfg-options load_from='p_weights/rtmdet_x_8xb32-300e_coco_20220715_230555-cc79b9ae.pth'
```

### Reproduces the problem - error message

```
Traceback (most recent call last):
  File ""tools/train.py"", line 120, in <module>
    main()
  File ""tools/train.py"", line 116, in main
    runner.train()
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/runner/runner.py"", line 1661, in train
    model = self.train_loop.run()  # type: ignore
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/runner/loops.py"", line 96, in run
    self.runner.val_loop.run()
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/runner/loops.py"", line 351, in run
    self.runner.call_hook('after_val_epoch', metrics=metrics)
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/runner/runner.py"", line 1723, in call_hook
    getattr(hook, fn_name)(self, **kwargs)
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/hooks/checkpoint_hook.py"", line 297, in after_val_epoch
    self._save_best_checkpoint(runner, metrics)
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/hooks/checkpoint_hook.py"", line 412, in _save_best_checkpoint
    self.file_client.remove(best_ckpt_path)
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/fileio/file_client.py"", line 346, in remove
    self.client.remove(filepath)
  File ""/data/home/rlchen/app/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmengine/fileio/backends/local_backend.py"", line 416, in remove
    raise FileNotFoundError(f'filepath {filepath} does not exist')
FileNotFoundError: filepath /data/home/rlchen/ProJect/mmdetection/work_dirs/rtmdet_x_8xb16_300e_panicle/best_coco/bbox_mAP_epoch_1.pth does not exist
11/05 14:56:10 - mmengine - INFO - The best checkpoint with 0.0020 coco/bbox_mAP at 7 epoch is saved to best_coco/bbox_mAP_epoch_7.pth.
```

### Additional information

_No response_"
[Bug] mask_rcnn got all loss=0.000 for custom COCO dataset with RLE instance segmentation encoding,open-mmlab/mmdetection,2022-11-02 20:32:53,1,,9211,1433753251,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

```bash
mmdet                   2.25.2
```

### Reproduces the problem - code sample

I build a custom COCO dataset with 1 class, and the instance segmentation is RLE encoding with ""iscrowd:1"":
```bash
""annotations"": [
        {
            ""category_id"": 1,
            ""image_id"": 900,
            ""iscrowd"": 1,
            ""segmentation"": {
                ""counts"": [
                    2043410,
                    6,
                    1,
                    11,
                    .......... (too many numbers, ignore with this line)
                    9,
                    974843
                ],
                ""size"": [
                    1544,
                    2064
                ]
            },
            ""bbox"": [
                1323.0,
                684.0,
                110.0,
                288.0
            ],
            ""area"": 23775.0,
            ""id"": 4611
        },
```

I also check the validation of my dataset with pycocotools, the visualization shows that there's no problem for my dataset:
![image](https://user-images.githubusercontent.com/38804949/199596664-963ab86c-5609-4194-9ce0-572d02b8725d.png)


### Reproduces the problem - command or script

```bash
(mmdet) kb@gpu01:/data-r10/kb/Projects/SynDataGen/mmdetection$ python3 tools/train.py configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_mps1texture_coco.py 
```

### Reproduces the problem - error message

```bash
/data-r10/kb/Projects/SynDataGen/mmdetection/mmdet/utils/setup_env.py:39: UserWarning: Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting OMP_NUM_THREADS environment variable for each process '
/data-r10/kb/Projects/SynDataGen/mmdetection/mmdet/utils/setup_env.py:49: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
2022-11-02 21:24:52,354 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA GeForce GTX 1080 Ti
CUDA_HOME: None
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.1+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.2+cu102
OpenCV: 4.6.0
MMCV: 1.6.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.25.2+9d3e162
------------------------------------------------------------

2022-11-02 21:24:53,575 - mmdet - INFO - Distributed training: False
2022-11-02 21:24:54,769 - mmdet - INFO - Config:
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet50_caffe')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=1,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=1,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = '/data-r10/kb/Projects/SynDataGen/datasets/mps1/'
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        poly2mask=False),
    dict(
        type='Resize',
        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),
                   (1333, 768), (1333, 800)],
        multiscale_mode='value',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file='/data-r10/kb/Projects/SynDataGen/datasets/mps1/train.json',
        img_prefix=
        '/data-r10/kb/Projects/SynDataGen/datasets/mps1/train_texture/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='LoadAnnotations',
                with_bbox=True,
                with_mask=True,
                poly2mask=False),
            dict(
                type='Resize',
                img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),
                           (1333, 768), (1333, 800)],
                multiscale_mode='value',
                keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='/data-r10/kb/Projects/SynDataGen/datasets/mps1/val.json',
        img_prefix=
        '/data-r10/kb/Projects/SynDataGen/datasets/mps1/val_texture/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[103.53, 116.28, 123.675],
                        std=[1.0, 1.0, 1.0],
                        to_rgb=False),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='/data-r10/kb/Projects/SynDataGen/datasets/mps1/json.json',
        img_prefix=
        '/data-r10/kb/Projects/SynDataGen/datasets/mps1/val_texture/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[103.53, 116.28, 123.675],
                        std=[1.0, 1.0, 1.0],
                        to_rgb=False),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=1,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
work_dir = './work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_mps1texture_coco'
auto_resume = False
gpu_ids = [0]

2022-11-02 21:24:54,769 - mmdet - INFO - Set random seed to 695735717, deterministic: False
2022-11-02 21:24:56,621 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'open-mmlab://detectron2/resnet50_caffe'}
2022-11-02 21:24:56,622 - mmcv - INFO - load model from: open-mmlab://detectron2/resnet50_caffe
2022-11-02 21:24:56,622 - mmcv - INFO - load checkpoint from openmmlab path: open-mmlab://detectron2/resnet50_caffe
2022-11-02 21:24:57,748 - mmcv - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: conv1.bias

2022-11-02 21:24:59,876 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2022-11-02 21:25:00,280 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2022-11-02 21:25:00,328 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
loading annotations into memory...
Done (t=6.19s)
creating index...
index created!
2022-11-02 21:25:11,138 - mmdet - INFO - Automatic scaling of learning rate (LR) has been disabled.
loading annotations into memory...
Done (t=1.61s)
creating index...
index created!
2022-11-02 21:25:12,764 - mmdet - INFO - Start running, host: kb@ar-gpu01, work_dir: /data-r10/kb/Projects/SynDataGen/mmdetection/work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_mps1texture_coco
2022-11-02 21:25:12,764 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2022-11-02 21:25:12,764 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2022-11-02 21:25:12,765 - mmdet - INFO - Checkpoints will be saved to /data-r10/kb/Projects/SynDataGen/mmdetection/work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_mps1texture_coco by HardDiskBackend.
2022-11-02 21:25:15,527 - mmdet - INFO - Epoch [1][1/450]	lr: 2.000e-05, eta: 3:56:26, time: 2.628, data_time: 2.292, memory: 2781, loss_rpn_cls: 0.6497, loss_rpn_bbox: 0.0000, loss_cls: 0.5660, acc: 75.5859, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 1.2157
2022-11-02 21:25:15,836 - mmdet - INFO - Epoch [1][2/450]	lr: 5.996e-05, eta: 2:10:26, time: 0.272, data_time: 0.080, memory: 2781, loss_rpn_cls: 0.6493, loss_rpn_bbox: 0.0000, loss_cls: 0.5279, acc: 80.9570, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 1.1772
2022-11-02 21:25:16,121 - mmdet - INFO - Epoch [1][3/450]	lr: 9.992e-05, eta: 1:35:51, time: 0.297, data_time: 0.116, memory: 2781, loss_rpn_cls: 0.6466, loss_rpn_bbox: 0.0000, loss_cls: 0.3724, acc: 96.4844, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 1.0189
2022-11-02 21:25:16,434 - mmdet - INFO - Epoch [1][4/450]	lr: 1.399e-04, eta: 1:18:19, time: 0.287, data_time: 0.103, memory: 2875, loss_rpn_cls: 0.6418, loss_rpn_bbox: 0.0000, loss_cls: 0.1563, acc: 99.9023, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.7981
2022-11-02 21:25:16,730 - mmdet - INFO - Epoch [1][5/450]	lr: 1.798e-04, eta: 1:08:20, time: 0.316, data_time: 0.130, memory: 2875, loss_rpn_cls: 0.6305, loss_rpn_bbox: 0.0000, loss_cls: 0.0428, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.6733
2022-11-02 21:25:17,000 - mmdet - INFO - Epoch [1][6/450]	lr: 2.198e-04, eta: 1:01:05, time: 0.277, data_time: 0.109, memory: 2875, loss_rpn_cls: 0.6223, loss_rpn_bbox: 0.0000, loss_cls: 0.0132, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.6354
2022-11-02 21:25:17,312 - mmdet - INFO - Epoch [1][7/450]	lr: 2.598e-04, eta: 0:56:10, time: 0.297, data_time: 0.101, memory: 2875, loss_rpn_cls: 0.5971, loss_rpn_bbox: 0.0000, loss_cls: 0.0055, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.6026
2022-11-02 21:25:17,598 - mmdet - INFO - Epoch [1][8/450]	lr: 2.997e-04, eta: 0:52:27, time: 0.295, data_time: 0.117, memory: 2875, loss_rpn_cls: 0.5806, loss_rpn_bbox: 0.0000, loss_cls: 0.0030, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.5836
2022-11-02 21:25:17,941 - mmdet - INFO - Epoch [1][9/450]	lr: 3.397e-04, eta: 0:49:45, time: 0.314, data_time: 0.108, memory: 3051, loss_rpn_cls: 0.5443, loss_rpn_bbox: 0.0000, loss_cls: 0.0018, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.5460
2022-11-02 21:25:18,271 - mmdet - INFO - Epoch [1][10/450]	lr: 3.796e-04, eta: 0:47:47, time: 0.337, data_time: 0.137, memory: 3051, loss_rpn_cls: 0.5179, loss_rpn_bbox: 0.0000, loss_cls: 0.0007, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.5186
2022-11-02 21:25:18,619 - mmdet - INFO - Epoch [1][11/450]	lr: 4.196e-04, eta: 0:46:13, time: 0.341, data_time: 0.133, memory: 3051, loss_rpn_cls: 0.4778, loss_rpn_bbox: 0.0000, loss_cls: 0.0004, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.4782
2022-11-02 21:25:18,961 - mmdet - INFO - Epoch [1][12/450]	lr: 4.596e-04, eta: 0:44:58, time: 0.348, data_time: 0.137, memory: 3051, loss_rpn_cls: 0.4473, loss_rpn_bbox: 0.0000, loss_cls: 0.0002, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.4475
2022-11-02 21:25:19,262 - mmdet - INFO - Epoch [1][13/450]	lr: 4.995e-04, eta: 0:43:40, time: 0.314, data_time: 0.132, memory: 3051, loss_rpn_cls: 0.3984, loss_rpn_bbox: 0.0000, loss_cls: 0.0001, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.3985
2022-11-02 21:25:19,600 - mmdet - INFO - Epoch [1][14/450]	lr: 5.395e-04, eta: 0:42:35, time: 0.319, data_time: 0.117, memory: 3051, loss_rpn_cls: 0.3340, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.3341
2022-11-02 21:25:19,899 - mmdet - INFO - Epoch [1][15/450]	lr: 5.794e-04, eta: 0:41:41, time: 0.327, data_time: 0.137, memory: 3051, loss_rpn_cls: 0.2874, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.2874
2022-11-02 21:25:20,180 - mmdet - INFO - Epoch [1][16/450]	lr: 6.194e-04, eta: 0:40:39, time: 0.282, data_time: 0.107, memory: 3051, loss_rpn_cls: 0.2382, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.2383
2022-11-02 21:25:20,486 - mmdet - INFO - Epoch [1][17/450]	lr: 6.594e-04, eta: 0:39:50, time: 0.297, data_time: 0.107, memory: 3051, loss_rpn_cls: 0.2009, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.2009
2022-11-02 21:25:20,818 - mmdet - INFO - Epoch [1][18/450]	lr: 6.993e-04, eta: 0:39:12, time: 0.319, data_time: 0.116, memory: 3051, loss_rpn_cls: 0.1716, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.1716
2022-11-02 21:25:21,151 - mmdet - INFO - Epoch [1][19/450]	lr: 7.393e-04, eta: 0:38:40, time: 0.326, data_time: 0.130, memory: 3051, loss_rpn_cls: 0.1310, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.1310
2022-11-02 21:25:21,418 - mmdet - INFO - Epoch [1][20/450]	lr: 7.792e-04, eta: 0:38:06, time: 0.309, data_time: 0.137, memory: 3051, loss_rpn_cls: 0.0997, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0997
2022-11-02 21:25:21,745 - mmdet - INFO - Epoch [1][21/450]	lr: 8.192e-04, eta: 0:37:32, time: 0.292, data_time: 0.094, memory: 3051, loss_rpn_cls: 0.0846, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0846
2022-11-02 21:25:22,047 - mmdet - INFO - Epoch [1][22/450]	lr: 8.592e-04, eta: 0:37:08, time: 0.324, data_time: 0.130, memory: 3051, loss_rpn_cls: 0.0786, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0786

 .............

2022-11-02 21:37:28,074 - mmdet - INFO - Epoch [1][109/450]	lr: 4.336e-03, eta: 0:30:03, time: 0.318, data_time: 0.131, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:28,426 - mmdet - INFO - Epoch [1][110/450]	lr: 4.376e-03, eta: 0:30:03, time: 0.341, data_time: 0.118, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:28,760 - mmdet - INFO - Epoch [1][111/450]	lr: 4.416e-03, eta: 0:30:03, time: 0.347, data_time: 0.140, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:29,066 - mmdet - INFO - Epoch [1][112/450]	lr: 4.456e-03, eta: 0:30:01, time: 0.314, data_time: 0.119, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:29,395 - mmdet - INFO - Epoch [1][113/450]	lr: 4.496e-03, eta: 0:29:59, time: 0.308, data_time: 0.109, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:29,687 - mmdet - INFO - Epoch [1][114/450]	lr: 4.535e-03, eta: 0:29:58, time: 0.314, data_time: 0.131, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:30,018 - mmdet - INFO - Epoch [1][115/450]	lr: 4.575e-03, eta: 0:29:56, time: 0.309, data_time: 0.109, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:30,303 - mmdet - INFO - Epoch [1][116/450]	lr: 4.615e-03, eta: 0:29:54, time: 0.313, data_time: 0.131, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:30,610 - mmdet - INFO - Epoch [1][117/450]	lr: 4.655e-03, eta: 0:29:52, time: 0.293, data_time: 0.102, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:30,875 - mmdet - INFO - Epoch [1][118/450]	lr: 4.695e-03, eta: 0:29:49, time: 0.286, data_time: 0.116, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:31,182 - mmdet - INFO - Epoch [1][119/450]	lr: 4.735e-03, eta: 0:29:47, time: 0.287, data_time: 0.095, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:31,471 - mmdet - INFO - Epoch [1][120/450]	lr: 4.775e-03, eta: 0:29:44, time: 0.297, data_time: 0.117, memory: 3056, loss_rpn_cls: 0.0000, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0000
2022-11-02 21:37:31,809 - mmdet - INFO - Epoch [1][121/450]	lr: 4.815e-03, eta: 0:29:43, time: 0.311, data_time: 0.108, memory: 3056, loss_rpn_cls: 0.0001, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0001
2022-11-02 21:37:32,088 - mmdet - INFO - Epoch [1][122/450]	lr: 4.855e-03, eta: 0:29:41, time: 0.311, data_time: 0.137, memory: 3056, loss_rpn_cls: 0.0001, loss_rpn_bbox: 0.0000, loss_cls: 0.0000, acc: 100.0000, loss_bbox: 0.0000, loss_mask: 0.0000, loss: 0.0001
```

### Additional information

1. Does mmdetection support rle for mask_rcnn?
2. Why is the loss all zero?"
"[Bug] facing ""Floating point exception"" when training on slurm cluster",open-mmlab/mmdetection,2022-11-02 13:54:37,2,,9210,1433189000,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I have modified the scripts/configs, or I'm working on my own tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

sys.platform: linux
Python: 3.7.7 (default, May  7 2020, 21:25:33) [GCC 7.3.0]
CUDA available: False
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.6.0
MMCV: 1.4.4
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
MMDetection: 2.20.0+1d24486

### Reproduces the problem - code sample

https://github.com/sivaguru-pat/mmdet-slurm

### Reproduces the problem - command or script

sudo bash new_slurm_train.sh

### Reproduces the problem - error message

srun: error: slurm-1: task 1: Floating point exception
srun: launch/slurm: _step_signal: Terminating StepId=18.0
slurmstepd-slurm-0: error: *** STEP 18.0 ON slurm-0 CANCELLED AT 2022-11-02T13:48:11 ***
srun: error: slurm-0: task 0: Exited with exit code 15
srun: error: slurm-2: task 2: Exited with exit code 15

### Additional information

slurm cluster setup is on Kubernetes, dataset used  is pothole"
[Bug] cpu 版的 mmdet.apis 接口是运行不了的会报错,open-mmlab/mmdetection,2022-11-02 07:28:57,2,,9206,1432639705,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

1. macos 系统+m1处理器
2. Python 3.8.13
3. mmcv-full: 1.6.0, mmdet: 2.25.0

### Reproduces the problem - code sample

```python
from mmdet.apis import init_detector, inference_detector
```

### Reproduces the problem - command or script

```shell
无
```

### Reproduces the problem - error message

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'mmdet.api'
>>> import mmdet.apis
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/apis/__init__.py"", line 2, in <module>
    from .inference import (async_inference_detector, inference_detector,
  File ""/opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet/apis/inference.py"", line 8, in <module>
    from mmcv.ops import RoIPool
  File ""/opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/ops/__init__.py"", line 2, in <module>
    from .active_rotated_filter import active_rotated_filter
  File ""/opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/ops/active_rotated_filter.py"", line 10, in <module>
    ext_module = ext_loader.load_ext(
  File ""/opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/ext_loader.py"", line 13, in load_ext
    ext = importlib.import_module('mmcv.' + name)
  File ""/opt/anaconda3/envs/openmmlab/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: dlopen(/opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/_ext.cpython-38-darwin.so, 0x0002): Library not loaded: @rpath/libtorch_cpu.dylib
  Referenced from: /opt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/_ext.cpython-38-darwin.so
  Reason: tried: '/opt/anaconda3/envs/openmmlab/lib/libtorch_cpu.dylib' (no such file), '/opt/anaconda3/envs/openmmlab/bin/../lib/libtorch_cpu.dylib' (no such file), '/opt/anaconda3/envs/openmmlab/lib/libtorch_cpu.dylib' (no such file), '/opt/anaconda3/envs/openmmlab/bin/../lib/libtorch_cpu.dylib' (no such file), '/usr/local/lib/libtorch_cpu.dylib' (no such file), '/usr/lib/libtorch_cpu.dylib' (no such file)
```

### Additional information

<img width=""1250"" alt=""image"" src=""https://user-images.githubusercontent.com/37138671/199425752-7b026e0f-cf87-42fc-987a-b59e05d7e524.png"">
"
[Bug] AttributeError: 'ConfigDict' object has no attribute 'log_level',open-mmlab/mmdetection,2022-11-01 14:04:16,35,,9204,1431487403,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [x] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I have modified the scripts/configs, or I'm working on my own tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

I have installed all of the required packages that are required to run mmDetection. I have also tried reinstalling packages, setting up new Conda environments and reinstalling mmDetection. I am currently working on training mmDetection with a custom dataset using the tutorial on the official mmLab website, however something has gone wrong. I try to run the programme using the command at the bottom of the tutorial however it does not work.

### Reproduces the problem - code sample

```
# The new config inherits a base config to highlight the necessary modification
base_ = 'configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_1x_coco.py'

# We also need to change the num_classes in head to match the dataset's annotation
# dict is a python dictionary object which is used to save or load models from PyTorch

model=dict(
    roi_head=dict(
        # defining the number of classes a bounding box can go around
        bbox_head=dict(num_classes=1),
            # 
        mask_head=dict(num_classes=1)))


    # Modify dataset related settings
dataset_type = 'COCODataset'
    #Defining the classes
classes = ('Pantograph')
    #grabbing the dataset so it can read the classes and train off the dataset
data=dict(
    train=dict(
        dataset=dict(
            img_prefix='balloon/train',
            classes=classes,
            ann_file='balloon/train/Pan2_COCO.json'),
    val=dict(
        img_prefix='balloon/val',
        classes=classes,
        ann_file='balloon/val/Pan2_COCO.json'),
    test=dict(
        img_prefix='balloon/val',
        classes=classes,
        ann_file='val/Pan2_COCO.json')))

# We can use the pre-trained Mask RCNN model to obtain higher performance
load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'
```

### Reproduces the problem - command or script

```
python tools/train.py configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_balloon.py
```


### Reproduces the problem - error message

```
Traceback (most recent call last):
  File ""tools/train.py"", line 244, in <module>
    main()
  File ""tools/train.py"", line 184, in main
    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)
  File ""/home/dtl-admin/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmcv/utils/config.py"", line 519, in __getattr__
    return getattr(self._cfg_dict, name)
  File ""/home/dtl-admin/miniconda3/envs/mmdet/lib/python3.8/site-packages/mmcv/utils/config.py"", line 50, in __getattr__
    raise ex
AttributeError: 'ConfigDict' object has no attribute 'log_level'
```

### Additional information

_No response_"
How to get polygon,open-mmlab/mmdetection,2022-11-01 09:02:41,3,,9203,1431100165,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### 💬 Describe the reimplementation questions


![1975_02_04_0007](https://user-images.githubusercontent.com/114475074/199195888-e4eaf19e-b4f4-4eee-a2e9-e2df1616abc6.png)
![seg](https://user-images.githubusercontent.com/114475074/199195857-c792d6c1-1895-4209-af31-85fe32360dd6.png)Attached is my original image and its annotated version besides it. After training I can only get boxes prediction but not polygons.
How can I get both articles that are in polygon or box shape.
I followed instruction in the following notebook to train my dataset
[https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_InstanceSeg_Tutorial.ipynb](url)

### Environment

PyTorch installed with pip

### Expected results

waiting for polygon outputs

### Additional information

Also using this link
[https://github.com/open-mmlab/mmdetection/issues/3599
](url)
I tried to make this change in config
`dict(type='LoadAnnotations', with_bbox=True, with_mask=True, poly2mask=True)`
but i got this error
KeyError: 'LoadAnnotations is not in the hook registry'
When I searched, I found that it was a bug."
"Problem with workflow = [('train', 1),('val',1)]",open-mmlab/mmdetection,2022-10-31 15:34:27,1,,9199,1430049807,"I'm wanting to see the validation loss, so looking at previous issues, I added the `workflow = [('train', 1),('val',1)]` to this, but I'm getting the following error: **AttributeError: 'ConfigDict' object has no attribute 'dataset'**
What could be causing this?
Thanks"
failure with dyhead swin onnx model to tensorrt engine with FP16 using mmdeploy,open-mmlab/mmdetection,2022-10-31 13:42:59,1,,9196,1429873609,"### Discussed in https://github.com/open-mmlab/mmdetection/discussions/9139

<div type='discussions-op-text'>

<sup>Originally posted by **vedrusss** October 25, 2022</sup>
Hi!
I'm trying to convert dyhead swin based model from onnx to tensorrt engine in fp16 mode using mmdeploy converter:
`python tools/onnx2tensorrt.py  configs/mmdet/detection/detection_tensorrt-fp16_static-1280x1920.py detection_tensorrt_static-1280x1920/fp16/end2end.onnx detection_tensorrt_static-1280x1920/fp16/end2end`

and running in following error:

-------------- The current device memory allocations dump as below --------------
[0]:34359738368 :HybridGlobWriter in reserveRegion: at optimizer/common/globWriter.cpp: 246 idx: 28 time: 0.000158613
[0x302000000]:11324620800 :HybridGlobWriter in reserveRegion: at optimizer/common/globWriter.cpp: 224 idx: 23 time: 0.00316042
[0x7f83c0000000]:29493600 :DeviceActivationSize in reserveNetworkTensorMemory: at optimizer/common/tactic/optimizer.cpp: 4603 idx: 8 time: 0.000331153
[10/24/2022-18:37:08] [TRT] [W] Requested amount of GPU memory (34359738368 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[10/24/2022-18:37:08] [TRT] [W] Skipping tactic 4 due to insuficient memory on requested size of 34359738368 detected for tactic 31.
Try decreasing the workspace size with IBuilderConfig::setMaxWorkspaceSize().
[10/24/2022-18:37:08] [TRT] [E] 2: [pointWiseBuilder.cpp::splitVectorizedHub::277] Error Code 2: Internal Error (Assertion !never(a.z >= 0) failed. )
Traceback (most recent call last):
  File ""tools/onnx2tensorrt.py"", line 73, in <module>
    main()
  File ""tools/onnx2tensorrt.py"", line 58, in main
    from_onnx(
  File ""/root/workspace/mmdeploy/mmdeploy/backend/tensorrt/utils.py"", line 225, in from_onnx
    assert engine is not None, 'Failed to create TensorRT engine'
AssertionError: Failed to create TensorRT engine

The actual **available GPU memory onto my device is 16Gb** and it does **support FP16**.
I'm **able to obtain the tensorrt** engine for the same model in **FP32** mode and cannot find out the reason why FP16 doesn't work.
Are there any nuances with using mmdeploy and mmdetection models for obtaining FP16 engines?
</div>"
[Bug] training over PASCAL does not work due to different metainfo,open-mmlab/mmdetection,2022-10-31 13:28:35,1,,9195,1429851240,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

3.x branch https://github.com/open-mmlab/mmdetection/tree/3.x

### Environment

`collect_env.py` does not work, see #9194 , but find here environment details

Installation procedure:

```
python -m venv env_iadet
source env_iadet/bin/activate
pip install --upgrade pip
pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu
pip install -U openmim
mim install mmengine
mim install ""mmcv>=2.0.0rc1""
git clone https://github.com/open-mmlab/mmdetection.git -b 3.x
cd mmdetection
pip install -v -e .
```

From `pip freeze`:
```
addict==2.4.0
certifi==2022.9.24
charset-normalizer==2.1.1
click==8.1.3
colorama==0.4.6
commonmark==0.9.1
contourpy==1.0.6
cycler==0.11.0
fonttools==4.38.0
idna==3.4
kiwisolver==1.4.4
Markdown==3.4.1
matplotlib==3.6.1
mmcv==2.0.0rc2
-e git+https://github.com/open-mmlab/mmdetection.git@3221ecf33885bcf185e1ea63ac72a861403d81b0#egg=mmdet
mmengine==0.2.0
model-index==0.1.11
numpy==1.23.4
opencv-python==4.6.0.66
openmim==0.3.2
ordered-set==4.1.0
packaging==21.3
pandas==1.5.1
Pillow==9.3.0
pycocotools==2.0.5
Pygments==2.13.0
pyparsing==3.0.9
python-dateutil==2.8.2
pytz==2022.5
PyYAML==6.0
requests==2.28.1
rich==12.6.0
six==1.16.0
tabulate==0.9.0
termcolor==2.1.0
terminaltables==3.1.10
torch==1.13.0+cpu
torchvision==0.14.0+cpu
typing_extensions==4.4.0
urllib3==1.26.12
yapf==0.32.0
```

### Reproduces the problem - code sample

Download PASCAL VOC using `torchvision`:
```python
import os
import torchvision

mmdetpath = "".""
torchvision.datasets.VOCDetection(os.path.join(mmdetpath, ""data""), year=""2012"", image_set=""trainval"", download=True)
torchvision.datasets.VOCDetection(os.path.join(mmdetpath, ""data""), year=""2007"", image_set=""trainval"", download=True)
```

### Reproduces the problem - command or script

After downloading the dataset, run:
```bash
python tools/train.py configs/pascal_voc/ssd300_voc0712.py
```

### Reproduces the problem - error message

```bash
Traceback (most recent call last):
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/tools/train.py"", line 120, in <module>
    main()
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/tools/train.py"", line 116, in main
    runner.train()
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/runner.py"", line 1629, in train
    self._train_loop = self.build_train_loop(
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/runner.py"", line 1444, in build_train_loop
    loop = LOOPS.build(
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/registry.py"", line 421, in build
    return self.build_func(cfg, *args, **kwargs, registry=self)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 135, in build_from_cfg
    raise type(e)(
ValueError: class `EpochBasedTrainLoop` in mmengine/runner/loops.py: class `RepeatDataset` in mmengine/dataset/dataset_wrapper.py: class `ConcatDataset` in mmengine/dataset/dataset_wrapper.py: The meta information of the 2-th dataset does not match meta information of the first dataset
```

<details>
<summary>Full output</summary>
```bash
10/31 14:17:01 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]
    CUDA available: False
    numpy_random_seed: 518614498
    GCC: gcc (Ubuntu 11.2.0-19ubuntu1) 11.2.0
    PyTorch: 1.13.0+cpu
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.0, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.14.0+cpu
    OpenCV: 4.6.0
    MMEngine: 0.2.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

10/31 14:17:02 - mmengine - INFO - Config:
input_size = 300
model = dict(
    type='SingleStageDetector',
    data_preprocessor=dict(
        type='DetDataPreprocessor',
        mean=[123.675, 116.28, 103.53],
        std=[1, 1, 1],
        bgr_to_rgb=True,
        pad_size_divisor=1),
    backbone=dict(
        type='SSDVGG',
        depth=16,
        with_last_pool=False,
        ceil_mode=True,
        out_indices=(3, 4),
        out_feature_indices=(22, 34),
        init_cfg=dict(
            type='Pretrained', checkpoint='open-mmlab://vgg16_caffe')),
    neck=dict(
        type='SSDNeck',
        in_channels=(512, 1024),
        out_channels=(512, 1024, 512, 256, 256, 256),
        level_strides=(2, 2, 1, 1),
        level_paddings=(1, 1, 0, 0),
        l2_norm_scale=20),
    bbox_head=dict(
        type='SSDHead',
        in_channels=(512, 1024, 512, 256, 256, 256),
        num_classes=20,
        anchor_generator=dict(
            type='SSDAnchorGenerator',
            scale_major=False,
            input_size=300,
            basesize_ratio_range=(0.2, 0.9),
            strides=[8, 16, 32, 64, 100, 300],
            ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[0.1, 0.1, 0.2, 0.2])),
    train_cfg=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.5,
            min_pos_iou=0.0,
            ignore_iof_thr=-1,
            gt_max_assign_all=False),
        sampler=dict(type='PseudoSampler'),
        smoothl1_beta=1.0,
        allowed_border=-1,
        pos_weight=-1,
        neg_pos_ratio=3,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        nms=dict(type='nms', iou_threshold=0.45),
        min_bbox_size=0,
        score_thr=0.02,
        max_per_img=200))
cudnn_benchmark = True
dataset_type = 'VOCDataset'
data_root = 'data/VOCdevkit/'
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Expand',
        mean=[123.675, 116.28, 103.53],
        to_rgb=True,
        ratio_range=(1, 4)),
    dict(
        type='MinIoURandomCrop',
        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
        min_crop_size=0.3),
    dict(type='Resize', scale=(300, 300), keep_ratio=False),
    dict(type='RandomFlip', prob=0.5),
    dict(
        type='PhotoMetricDistortion',
        brightness_delta=32,
        contrast_range=(0.5, 1.5),
        saturation_range=(0.5, 1.5),
        hue_delta=18),
    dict(type='PackDetInputs')
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', scale=(300, 300), keep_ratio=False),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                   'scale_factor'))
]
train_dataloader = dict(
    batch_size=8,
    num_workers=3,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=True),
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    dataset=dict(
        type='RepeatDataset',
        times=10,
        dataset=dict(
            type='ConcatDataset',
            datasets=[
                dict(
                    type='VOCDataset',
                    data_root='data/VOCdevkit/',
                    ann_file='VOC2007/ImageSets/Main/trainval.txt',
                    data_prefix=dict(sub_data_root='VOC2007/'),
                    filter_cfg=dict(filter_empty_gt=True, min_size=32),
                    pipeline=[
                        dict(type='LoadImageFromFile'),
                        dict(type='LoadAnnotations', with_bbox=True),
                        dict(
                            type='Expand',
                            mean=[123.675, 116.28, 103.53],
                            to_rgb=True,
                            ratio_range=(1, 4)),
                        dict(
                            type='MinIoURandomCrop',
                            min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
                            min_crop_size=0.3),
                        dict(
                            type='Resize', scale=(300, 300), keep_ratio=False),
                        dict(type='RandomFlip', prob=0.5),
                        dict(
                            type='PhotoMetricDistortion',
                            brightness_delta=32,
                            contrast_range=(0.5, 1.5),
                            saturation_range=(0.5, 1.5),
                            hue_delta=18),
                        dict(type='PackDetInputs')
                    ]),
                dict(
                    type='VOCDataset',
                    data_root='data/VOCdevkit/',
                    ann_file='VOC2012/ImageSets/Main/trainval.txt',
                    data_prefix=dict(sub_data_root='VOC2012/'),
                    filter_cfg=dict(filter_empty_gt=True, min_size=32),
                    pipeline=[
                        dict(type='LoadImageFromFile'),
                        dict(type='LoadAnnotations', with_bbox=True),
                        dict(
                            type='Expand',
                            mean=[123.675, 116.28, 103.53],
                            to_rgb=True,
                            ratio_range=(1, 4)),
                        dict(
                            type='MinIoURandomCrop',
                            min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
                            min_crop_size=0.3),
                        dict(
                            type='Resize', scale=(300, 300), keep_ratio=False),
                        dict(type='RandomFlip', prob=0.5),
                        dict(
                            type='PhotoMetricDistortion',
                            brightness_delta=32,
                            contrast_range=(0.5, 1.5),
                            saturation_range=(0.5, 1.5),
                            hue_delta=18),
                        dict(type='PackDetInputs')
                    ])
            ])))
val_dataloader = dict(
    batch_size=1,
    num_workers=2,
    persistent_workers=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='VOCDataset',
        data_root='data/VOCdevkit/',
        ann_file='VOC2007/ImageSets/Main/test.txt',
        data_prefix=dict(sub_data_root='VOC2007/'),
        test_mode=True,
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', scale=(300, 300), keep_ratio=False),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='PackDetInputs',
                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                           'scale_factor'))
        ]))
test_dataloader = dict(
    batch_size=1,
    num_workers=2,
    persistent_workers=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='VOCDataset',
        data_root='data/VOCdevkit/',
        ann_file='VOC2007/ImageSets/Main/test.txt',
        data_prefix=dict(sub_data_root='VOC2007/'),
        test_mode=True,
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', scale=(300, 300), keep_ratio=False),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='PackDetInputs',
                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                           'scale_factor'))
        ]))
val_evaluator = dict(type='VOCMetric', metric='mAP', eval_mode='11points')
test_evaluator = dict(type='VOCMetric', metric='mAP', eval_mode='11points')
train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=24, val_interval=1)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
param_scheduler = [
    dict(
        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),
    dict(
        type='MultiStepLR',
        begin=0,
        end=24,
        by_epoch=True,
        milestones=[16, 20],
        gamma=0.1)
]
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0005))
auto_scale_lr = dict(enable=False, base_batch_size=64)
default_scope = 'mmdet'
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=50),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', interval=1),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='DetVisualizationHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'))
vis_backends = [dict(type='LocalVisBackend')]
visualizer = dict(
    type='DetLocalVisualizer',
    vis_backends=[dict(type='LocalVisBackend')],
    name='visualizer')
log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)
log_level = 'INFO'
load_from = None
resume = False
custom_hooks = [
    dict(type='NumClassCheckHook'),
    dict(type='CheckInvalidLossHook', interval=50, priority='VERY_LOW')
]
launcher = 'none'
work_dir = './work_dirs/ssd300_voc0712'

Result has been saved to /home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/work_dirs/ssd300_voc0712/modules_statistic_results.json
10/31 14:17:03 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
Traceback (most recent call last):
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 121, in build_from_cfg
    obj = obj_cls(**args)  # type: ignore
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/dataset/dataset_wrapper.py"", line 52, in __init__
    raise ValueError(
ValueError: The meta information of the 2-th dataset does not match meta information of the first dataset

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 121, in build_from_cfg
    obj = obj_cls(**args)  # type: ignore
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/dataset/dataset_wrapper.py"", line 183, in __init__
    self.dataset = DATASETS.build(dataset)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/registry.py"", line 421, in build
    return self.build_func(cfg, *args, **kwargs, registry=self)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 135, in build_from_cfg
    raise type(e)(
ValueError: class `ConcatDataset` in mmengine/dataset/dataset_wrapper.py: The meta information of the 2-th dataset does not match meta information of the first dataset

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 121, in build_from_cfg
    obj = obj_cls(**args)  # type: ignore
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/loops.py"", line 43, in __init__
    super().__init__(runner, dataloader)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/base_loop.py"", line 26, in __init__
    self.dataloader = runner.build_dataloader(
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/runner.py"", line 1337, in build_dataloader
    dataset = DATASETS.build(dataset_cfg)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/registry.py"", line 421, in build
    return self.build_func(cfg, *args, **kwargs, registry=self)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 135, in build_from_cfg
    raise type(e)(
ValueError: class `RepeatDataset` in mmengine/dataset/dataset_wrapper.py: class `ConcatDataset` in mmengine/dataset/dataset_wrapper.py: The meta information of the 2-th dataset does not match meta information of the first dataset

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/tools/train.py"", line 120, in <module>
    main()
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/tools/train.py"", line 116, in main
    runner.train()
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/runner.py"", line 1629, in train
    self._train_loop = self.build_train_loop(
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/runner/runner.py"", line 1444, in build_train_loop
    loop = LOOPS.build(
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/registry.py"", line 421, in build
    return self.build_func(cfg, *args, **kwargs, registry=self)
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/registry/build_functions.py"", line 135, in build_from_cfg
    raise type(e)(
ValueError: class `EpochBasedTrainLoop` in mmengine/runner/loops.py: class `RepeatDataset` in mmengine/dataset/dataset_wrapper.py: class `ConcatDataset` in mmengine/dataset/dataset_wrapper.py: The meta information of the 2-th dataset does not match meta information of the first dataset
```


</details>

### Additional information

After some investigation, I think the problem lies in the following lines of `mmdet/datasets/voc.py`

https://github.com/open-mmlab/mmdetection/blob/3221ecf33885bcf185e1ea63ac72a861403d81b0/mmdet/datasets/voc.py#L26-L31 

which causes the metadata to be different and metainfo consistency check to fail (linked below)

https://github.com/open-mmlab/mmengine/blob/64ac14303fc786151beabba1de4ddfd7c2e5a408/mmengine/dataset/dataset_wrapper.py#L48-L54
"
[Bug] collect_env.py fails due to circular import,open-mmlab/mmdetection,2022-10-31 13:10:27,0,v-3.x,9194,1429823417,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

3.x branch https://github.com/open-mmlab/mmdetection/tree/3.x

### Environment

From `pip freeze`:
```
addict==2.4.0
certifi==2022.9.24
charset-normalizer==2.1.1
click==8.1.3
colorama==0.4.6
commonmark==0.9.1
contourpy==1.0.6
cycler==0.11.0
fonttools==4.38.0
idna==3.4
kiwisolver==1.4.4
Markdown==3.4.1
matplotlib==3.6.1
mmcv==2.0.0rc2
-e git+https://github.com/open-mmlab/mmdetection.git@3221ecf33885bcf185e1ea63ac72a861403d81b0#egg=mmdet
mmengine==0.2.0
model-index==0.1.11
numpy==1.23.4
opencv-python==4.6.0.66
openmim==0.3.2
ordered-set==4.1.0
packaging==21.3
pandas==1.5.1
Pillow==9.3.0
pycocotools==2.0.5
Pygments==2.13.0
pyparsing==3.0.9
python-dateutil==2.8.2
pytz==2022.5
PyYAML==6.0
requests==2.28.1
rich==12.6.0
six==1.16.0
tabulate==0.9.0
termcolor==2.1.0
terminaltables==3.1.10
torch==1.13.0+cpu
torchvision==0.14.0+cpu
typing_extensions==4.4.0
urllib3==1.26.12
yapf==0.32.0
```

### Reproduces the problem - code sample

```bash
python mmdet/utils/collect_env.py
```

### Reproduces the problem - command or script

```bash
python mmdet/utils/collect_env.py
```

### Reproduces the problem - error message

```
Traceback (most recent call last):
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/mmdet/utils/collect_env.py"", line 2, in <module>
    from mmengine.utils import get_git_hash
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/__init__.py"", line 3, in <module>
    from .config import *
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/config/__init__.py"", line 2, in <module>
    from .config import Config, ConfigDict, DictAction
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/env_iadet/lib/python3.10/site-packages/mmengine/config/config.py"", line 15, in <module>
    from typing import Any, Optional, Sequence, Tuple, Union
  File ""/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/mmdet/utils/typing.py"", line 3, in <module>
    from typing import List, Optional, Sequence, Tuple, Union
ImportError: cannot import name 'List' from partially initialized module 'typing' (most likely due to a circular import) (/home/franchesoni/mine/creations/phd/projects/current/iadet/mmdetection/mmdet/utils/typing.py)
```

### Additional information

The expected result is the information of the environment, not an error.

Although this might be my fault, I followed a pretty standard installation:

```
python -m venv env_iadet
source env_iadet/bin/activate
pip install --upgrade pip
pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu
pip install -U openmim
mim install mmengine
mim install ""mmcv>=2.0.0rc1""
git clone https://github.com/open-mmlab/mmdetection.git -b 3.x
cd mmdetection
pip install -v -e .
```"
mmdet写的CrossEntropyLoss与pytorch的CrossEntropyLoss有点不一样,open-mmlab/mmdetection,2022-10-31 03:30:35,1,,9193,1429194645,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### Task

I'm using the official example scripts/configs for the officially supported tasks/models/datasets.

### Branch

master branch https://github.com/open-mmlab/mmdetection

### Environment

xxx

### Reproduces the problem - code sample

xxx

### Reproduces the problem - command or script

xxxx

### Reproduces the problem - error message

`   
 # average loss over non-ignored elements
    # pytorch's official cross_entropy average loss over non-ignored elements
    # refer to https://github.com/pytorch/pytorch/blob/56b43f4fec1f76953f15a627694d4bba34588969/torch/nn/functional.py#L2660  # noqa
    if (avg_factor is None) and avg_non_ignore and reduction == 'mean':
        avg_factor = label.numel() - (label == ignore_index).sum().item()
`
mmdet里面的代码写交叉熵损失时，求平均的时候，（有class_weight参数）分母avg_factor简单的使用了样本数量减去忽略类别的样本数量，但是我看pytorch中的交叉熵最后求平均的公式，分母不是样本数量，而是各个样本的class_weight求和。
是故意和pytorch不一样的吗，还是说mmdet这种方式更好

### Additional information

xxx"
loading grayscale images,open-mmlab/mmdetection,2022-10-31 02:24:38,2,,9188,1429149015,"Hi,
My dataset consists of grayscale images in voc annotation style. If I want to train with mmdetection, where should I change in the original code to load grayscale images correctly?
Thanks!"
[Feature] omit setting gt labels to be zeros in RPN Head,open-mmlab/mmdetection,2022-10-30 11:53:22,1,v-3.x,9182,1428752267,"### What's the feature?

In RPN head, there are some legacy codes do the following:
```python
# set cat_id of gt_labels to 0 in RPN
            for data_sample in rpn_data_samples:
                data_sample.gt_instances.labels = \
                    torch.zeros_like(data_sample.gt_instances.labels)
```
However, we can keep the assignment progress of RPN the same as that in RCNN, and only re-map the learning targets of labels/logits to zero just before the loss calculation.
This unifies and simplifies the code and assignment.



### Any other context?

_No response_"
[Reimplementation] the function python tools/test.py with more iteration ,open-mmlab/mmdetection,2022-10-26 18:17:52,5,Doc#How-to,9148,1424508177,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### 💬 Describe the reimplementation questions

it is not a bug , just i want to know if there is a possible to run the function 
` python tools/test.py` 
with more iteration , instead of 1 iteration , on another words instead of test each image dependently, i want to go through group of images , for examples 50 images at ones to decrease the time. 

![test iteration](https://user-images.githubusercontent.com/10245810/198104584-f590b2ba-4b01-418e-bc2b-f7bd36073c82.jpg)

here in yellow , i have 5000 images in test , and the code goes by each image ,which take a while . 

how can i do so? is that could increase the speed of getting a results  of mAP for boundary boxes ?


### Environment

my environment is totally correct as i already trained my model.

### Expected results

_No response_

### Additional information

_No response_"
how to change the thickness of produced boundary boxes. ,open-mmlab/mmdetection,2022-10-26 13:43:36,1,,9147,1424050194,"### What's the feature?

Hello , 

Thanks for your effort . after training my model with my custom dataset, i use the function `python tools/test.py`   to produce a results of images that contain the boundary boxes as bellow. 

![images ](https://user-images.githubusercontent.com/10245810/198042230-3e8c9321-cc04-4d2e-b24f-e995a5add4e8.jpg)


some images look good , while another look not so clear. 
I would like to ask about the possibility to have such bbox more thick and with more obvious color. 

I try to find a way, I read the script `tools/test.py` but i  could not figure it out. I don't know where shall i search or update them or change them. 

any help will be highly apricated.  

### Any other context?

_No response_"
[Reimplementation] HTC's validation iteration number issue.,open-mmlab/mmdetection,2022-10-26 12:18:52,1,,9146,1423928788,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### 💬 Describe the reimplementation questions

I'm wondering about the log that you provided.
As I know, in COCO-instance val dataset, there exist 5k images.
However, the log that you provided for the model which used backbone as ResNet-50-FPN with lr 20e, the last line is as follow. 

{""mode"": ""val"", ""epoch"": 20, ""iter"": 7330, ""lr"": 0.0002, ""bbox_mAP"": 0.433, ""bbox_mAP_50"": 0.622, ""bbox_mAP_75"": 0.471, ""bbox_mAP_s"": 0.243, ""bbox_mAP_m"": 0.464, ""bbox_mAP_l"": 0.577, ""bbox_mAP_copypaste"": ""0.433 0.622 0.471 0.243 0.464 0.577"", ""segm_mAP"": 0.383, ""segm_mAP_50"": 0.593, ""segm_mAP_75"": 0.414, ""segm_mAP_s"": 0.199, ""segm_mAP_m"": 0.41, ""segm_mAP_l"": 0.53, ""segm_mAP_copypaste"": ""0.383 0.593 0.414 0.199 0.410 0.530""}

You can see that the number of iteration is 7330, and even the single GPU is used, it must be 5k, not 7330.

Is the dataset used for validation not coco-instance val?

### Environment

sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: RTX A6000
CUDA_HOME: /usr
NVCC: Cuda compilation tools, release 9.1, V9.1.8
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.8.1+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.1+cu111
OpenCV: 4.6.0
MMCV: 1.5.3
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.25.0+c88509c

### Expected results

_No response_

### Additional information

_No response_"
"[Reimplementation] YOLOX bbox pred using t,l,r,b format always produce loss_bbox: 5.0",open-mmlab/mmdetection,2022-10-25 09:50:49,10,,9138,1422184355,"### Prerequisite

- [X] I have searched [Issues](https://github.com/open-mmlab/mmdetection/issues) and [Discussions](https://github.com/open-mmlab/mmdetection/discussions) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version (master)](https://github.com/open-mmlab/mmdetection) or [latest version (3.x)](https://github.com/open-mmlab/mmdetection/tree/dev-3.x).

### 💬 Describe the reimplementation questions

I replace `_bbox_decode` function in YOLOX like this:
```
    def _bbox_decode(self, priors, bbox_preds):
        tl_x = priors[..., 0] - bbox_preds[..., 0]
        tl_y = priors[..., 1] - bbox_preds[..., 1]
        br_x = priors[..., 0] + bbox_preds[..., 2]
        br_y = priors[..., 1] + bbox_preds[..., 3]
        return torch.stack([tl_x, tl_y, br_x, br_y], dim=-1)
```
My config:
```
_base_ = [
    '../configs/yolox/yolox_s_8x8_300e_coco.py'
]

# dataset settings
dataset_type = 'CocoDataset'
data_root = 'path'

img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    _delete_=True,
    samples_per_gpu=8,
    workers_per_gpu=4,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_minitrain2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline)
)
evaluation = dict(interval=1, metric='bbox')

# optimizer
optimizer = dict(_delete_=True, type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(_delete_=True, grad_clip=None)
# learning policy
lr_config = dict(
    _delete_=True,
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
```
My result: 
```
2022-10-25 09:29:29,856 - mmdet - INFO - Epoch [1][50/1564]	lr: 1.978e-03, eta: 1:15:21, time: 0.242, data_time: 0.070, memory: 10625, loss_cls: 0.9370, loss_bbox: 5.0000, loss_obj: 14.5174, loss: 20.4544
2022-10-25 09:29:38,077 - mmdet - INFO - Epoch [1][100/1564]	lr: 3.976e-03, eta: 1:03:09, time: 0.164, data_time: 0.019, memory: 10625, loss_cls: 0.7869, loss_bbox: 5.0000, loss_obj: 7.7109, loss: 13.4978
2022-10-25 09:29:47,287 - mmdet - INFO - Epoch [1][150/1564]	lr: 5.974e-03, eta: 1:01:02, time: 0.184, data_time: 0.018, memory: 10625, loss_cls: 0.6463, loss_bbox: 5.0000, loss_obj: 6.0049, loss: 11.6513
2022-10-25 09:29:55,211 - mmdet - INFO - Epoch [1][200/1564]	lr: 7.972e-03, eta: 0:57:55, time: 0.158, data_time: 0.019, memory: 10625, loss_cls: 0.5865, loss_bbox: 5.0000, loss_obj: 5.3746, loss: 10.9611
2022-10-25 09:30:03,355 - mmdet - INFO - Epoch [1][250/1564]	lr: 9.970e-03, eta: 0:56:15, time: 0.163, data_time: 0.019, memory: 10625, loss_cls: 0.3879, loss_bbox: 5.0000, loss_obj: 5.0617, loss: 10.4496
2022-10-25 09:30:12,697 - mmdet - INFO - Epoch [1][300/1564]	lr: 1.197e-02, eta: 0:56:20, time: 0.187, data_time: 0.017, memory: 10692, loss_cls: 0.3199, loss_bbox: 5.0000, loss_obj: 5.3904, loss: 10.7103
```


### Environment

sys.platform: linux
Python: 3.9.12 (main, Apr  5 2022, 06:56:58) [GCC 7.5.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.8, V11.8.89
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.10.1
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.2
OpenCV: 4.6.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1+1b4891c

### Expected results

Normal YOLOX bbox pred, same config, different `bbox_decode` function:
```
2022-10-25 09:30:40,552 - mmdet - INFO - Epoch [1][50/1564]	lr: 1.978e-03, eta: 1:15:08, time: 0.241, data_time: 0.069, memory: 10627, loss_cls: 1.6528, loss_bbox: 4.7309, loss_obj: 12.7018, loss: 19.0855
2022-10-25 09:30:48,810 - mmdet - INFO - Epoch [1][100/1564]	lr: 3.976e-03, eta: 1:03:09, time: 0.165, data_time: 0.020, memory: 10627, loss_cls: 1.8888, loss_bbox: 4.4457, loss_obj: 5.9525, loss: 12.2871
2022-10-25 09:30:58,025 - mmdet - INFO - Epoch [1][150/1564]	lr: 5.974e-03, eta: 1:01:03, time: 0.184, data_time: 0.018, memory: 10627, loss_cls: 2.3753, loss_bbox: 4.0260, loss_obj: 5.8119, loss: 12.2131
2022-10-25 09:31:05,916 - mmdet - INFO - Epoch [1][200/1564]	lr: 7.972e-03, eta: 0:57:52, time: 0.158, data_time: 0.019, memory: 10627, loss_cls: 2.4342, loss_bbox: 3.9371, loss_obj: 5.5452, loss: 11.9165
```

### Additional information

_No response_"
make browse_dataset.py faster~,open-mmlab/mmdetection,2022-10-24 09:14:05,2,feature request#v-3.x#v-2.x,9126,1420504901,"### What is the problem this feature will solve?

make browse_dataset.py fast~

### What is the feature you are proposing to solve the problem?

make browse_dataset.py fast~

### What alternatives have you considered?

make browse_dataset.py fast~"
Mixed Precision Training,open-mmlab/mmdetection,2022-10-20 16:13:04,11,feature request,9092,1416902195,"### What is the problem this feature will solve?

Hello , 
Thanks for your effort in developing such a huge library. 

I would like to ask about using mixed precision training within some sort of models . 

for examples , the anchor free models such as FoveaBox , FCOS and FSAF model. 
is that possible ? does such model support the using of Mixed Precision Training (fp16)? 

would it be possible to add the following line in my config of such models:

# fp16 settings
fp16 = dict(loss_scale=512.)

or that could cause me an error in configuration. 

I need it in order to increase up the training time , is there any other solution to increase up the training time ? as it cause me more that 3 days. 

looking forward to your help, and ideas 

### What is the feature you are proposing to solve the problem?

FP16 , is it works within all types of models , such as anchor free model (FSAF,FCOS ) even more with centriapetalNet .. 

### What alternatives have you considered?

_No response_"
Loss value when training QueryInst,open-mmlab/mmdetection,2022-10-19 06:31:48,2,,9073,1414379521,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 💬 Describe the reimplementation questions

I have followed the tutorial on how to train QueryInst with custom data and modified config and other files accordingly to include my class names and number of classes, and have also organised my data into the coco format. But, when I try to train, I get this message displayed recursively a hundred times for every epoch, ""Ground Truth Not Found!"" - with the original repo from author.

### Environment

GPU A6000
mmcv:1.3.17
pytorch: 1.10.1 cuda 11.3

### Expected results

 I tried to train on your work so the loss values zero except loss_cls. Can you help me to solve that, I really appreciate it.

[stage0_loss_cls: 0.0025, stage0_pos_acc0, stage4_loss_iou: 0.0000, stage4_loss_mask: 0.0000, stage5_loss_cls: 0.0024, stage5_pos_acc: 0.0000, stage5_loss_bbox: 0.0000, stage5_loss_iou: 0.0000, stage5_loss_mask: 0.0000,0.0000, stage1_loss_mask: 0.0000, stage2_loss_cls: 0.0012, stage2_pos_acc: 0.0000, stage2_loss_bbox: 0.0000, stage2_loss_iou: 0.0000, stage2_loss_mask: 0.0000, stage3_loss_cls: 0.0012, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.0000, stage3_loss_iou: 0.0000, stage3_loss_mask: 0.0000, stage4_loss_cls: 0.0010, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.000](lr: 1.499e-05, eta: 17:44:38, time: 0.620, data_time: 0.004, memory: 10924, stage0_loss_cls: 125712.0807, stage0_pos_acc: 0.0000, stage0_loss_bbox: 0.0000, stage0_loss_iou: 0.0000, stage0_loss_mask: 0.0000, stage1_loss_cls: 90232.6852, stage1_pos_acc: 0.0000, stage1_loss_bbox: 0.0000, stage1_loss_iou: 0.0000, stage1_loss_mask: 0.0000, stage2_loss_cls: 46718.8418, stage2_pos_acc: 0.0000, stage2_loss_bbox: 0.0000, stage2_loss_iou: 0.0000, stage2_loss_mask: 0.0000, stage3_loss_cls: 32931.4105, stage3_pos_acc: 0.0000, stage3_loss_bbox: 0.0000, stage3_loss_iou: 0.0000, stage3_loss_mask: 0.0000, stage4_loss_cls: 41905.0223, stage4_pos_acc: 0.0000, stage4_loss_bbox: 0.0000, stage4_loss_iou: 0.0000, stage4_loss_mask: 0.0000, stage5_loss_cls: 47268.3891, stage5_pos_acc: 0.0000, stage5_loss_bbox: 0.0000, stage5_loss_iou: 0.0000, stage5_loss_mask: 0.0000, loss: 384768.4313, grad_norm: 1482184.3400)

### Additional information

_No response_"
[Feature Request] TensorboardVisBackend Images,open-mmlab/mmdetection,2022-10-18 18:32:22,2,feature request#v-3.x,9070,1413669535,"### What is the problem this feature will solve?

Visualizing outputs makes debugging training and improving performance much easier

### What is the feature you are proposing to solve the problem?

I'd love an option for the `TensorboardVisBackend` to display a couple batches of validation (and maybe also training) predictions each epoch. For example:

![image](https://user-images.githubusercontent.com/12224358/196514787-aa84f930-d9e8-4296-8ccb-6326e223b9d4.png)

Another possible feature would be to display these both before and after postprocessing is applied.

### What alternatives have you considered?

Did not see a feature or request for this already, but please let me know if I missed it! People could write custom hooks, but this feature seems useful to enough people that maybe it's a good idea to include directly."
Loading COCO dataset with empty GT masks when using default code.,open-mmlab/mmdetection,2022-10-18 07:42:40,5,,9064,1412729040,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

I'm trying to load COCO-train2017 using default code:
```python
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)

train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Rotate', level=3),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    samples_per_gpu=12,
    workers_per_gpu=12,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_train2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline))
```
However, when training the queryinst, in `forward_train` function, the gt masks of some input images are empty:

![image](https://user-images.githubusercontent.com/17793134/196366754-438e1409-fef4-434c-b7e6-2e472d1961ee.png)

The img_meta of this image is shown below :
```bash
'filename':'data/coco/train2017/000000373284.jpg'
'ori_filename':'000000373284.jpg'
'ori_shape':(428, 640, 3)
'img_shape':(800, 1196, 3)
'pad_shape':(800, 1216, 3)
'scale_factor':array([1.86875  , 1.8691589, 1.86875  , 1.8691589], dtype=float32)
'flip':True
'flip_direction':'horizontal'
'img_norm_cfg':{'mean': array([123.675, 116....e=float32), 'std': array([58.395, 57.12...e=float32), 'to_rgb': True}
len():9
```
What's more, I run visualization tools like fiftyone. It shows that `000000373284.jpg` does have labeled masks:
![image](https://user-images.githubusercontent.com/17793134/196367414-0be35e59-1e86-4966-a744-fcb670c876ec.png)

It will be appreciated if the problem can be fixed.

### Environment

(leo_pytorch) hao@ROCE-node35:/nas/self-define/hao/research_workspace$ mim list
```bash
/home/hao/anaconda3/envs/leo_pytorch/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
Package         Version    Source
--------------  ---------  ----------------------------------------------
mmcls           0.23.1     https://github.com/open-mmlab/mmclassification
mmcv-full       1.4.8      https://github.com/open-mmlab/mmcv
mmdet           2.25.0     https://github.com/open-mmlab/mmdetection
mmsegmentation  0.26.0     http://github.com/open-mmlab/mmsegmentation
mmselfsup       0.9.1      https://github.com/open-mmlab/mmselfsup
```

### Additional information

_No response_"
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 7 (pid: 2192862) of binary: ,open-mmlab/mmdetection,2022-10-15 14:10:02,2,,9032,1410178822,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

Hello~
       I use order `bash tools/dist_train.sh /home/tcexeexe/mmdetection/configs/yolox/yolox_l_8x8_300e_coco.py 8` to train the object365 dataset.
      When l use six 3090 graphics, every things is OK. But when l use seven or eight 3090 graphics, the error always occurred after 2~3 epoch,
![1665842802808](https://user-images.githubusercontent.com/28287196/195990843-f5cf9a32-8cbe-4f81-9a89-d60f2ff7616b.png)

Do anybody know how to locate the error?

### Environment

Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.6, V11.6.112
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.6
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.6, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF,

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.2
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.6
MMDetection: 2.25.2+9d3e162

### Additional information

_No response_"
我跑自己的数据集  然后输出各个类别  结果会少一个 数据集的category从1开始的 会不会有什么影响,open-mmlab/mmdetection,2022-10-15 04:43:46,4,,9031,1410043072,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

test输出类别少1

### Environment

11

### Additional information

_No response_"
custom model doesn't work,open-mmlab/mmdetection,2022-10-14 07:29:07,3,,9023,1408901367,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

I added 'mmdetection/configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco_mydata.py'
```
_base_ = [
    '../_base_/models/mask_rcnn_r50_fpn.py',
    '../_base_/datasets/coco_instance.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]

model = dict(
    roi_head=dict(
        bbox_head=dict(num_classes=2),
        mask_head=dict(num_classes=2)))

dataset_type = 'CocoDataset'
data_root = 'data/coco/'

data = dict(
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations.json',
        img_prefix=data_root),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations.json',
        img_prefix=data_root),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations.json',
        img_prefix=data_root))
evaluation = dict(metric=['bbox', 'segm'])

# optimizer
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
```

I modified 'mmdet/core/evaluation/class_names.py'
```
def coco_classes():
    return [
        '_background_', 'flash_light'
    ]
```

I modified 'mmdet/datasets/coco.py'
```
CLASSES = ('_background_', 'flash_light')

PALETTE = [(0, 0, 0), (0, 0, 128)]
```

I get a log file
```
2022-10-14 15:32:36,204 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2022-10-14 15:32:36,204 - mmdet - INFO - Checkpoints will be saved to /home/unstruct/combination/mmdetection/work_dirs/mask_rcnn_r50_fpn_1x_coco_mydata by HardDiskBackend.
2022-10-14 15:32:39,200 - mmdet - INFO - Saving checkpoint at 1 epochs
2022-10-14 15:32:40,275 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:32:40,308 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:40,308 - mmdet - INFO - Evaluating segm...
2022-10-14 15:32:40,356 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.001
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:40,356 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:32:40,356 - mmdet - INFO - Epoch(val) [1][11]	bbox_mAP: 0.0000, bbox_mAP_50: 0.0000, bbox_mAP_75: 0.0000, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0000, bbox_mAP_l: 0.0000, bbox_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000, segm_mAP: 0.0000, segm_mAP_50: 0.0010, segm_mAP_75: 0.0000, segm_mAP_s: 0.0000, segm_mAP_m: 0.0000, segm_mAP_l: 0.0000, segm_mAP_copypaste: 0.000 0.001 0.000 0.000 0.000 0.000
2022-10-14 15:32:43,315 - mmdet - INFO - Saving checkpoint at 2 epochs
2022-10-14 15:32:44,381 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:32:44,421 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:44,421 - mmdet - INFO - Evaluating segm...
2022-10-14 15:32:44,484 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:44,485 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:32:44,485 - mmdet - INFO - Epoch(val) [2][11]	bbox_mAP: 0.0000, bbox_mAP_50: 0.0000, bbox_mAP_75: 0.0000, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0000, bbox_mAP_l: 0.0000, bbox_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000, segm_mAP: 0.0000, segm_mAP_50: 0.0000, segm_mAP_75: 0.0000, segm_mAP_s: 0.0000, segm_mAP_m: 0.0000, segm_mAP_l: 0.0000, segm_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000
2022-10-14 15:32:47,480 - mmdet - INFO - Saving checkpoint at 3 epochs
2022-10-14 15:32:48,561 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:32:48,603 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.005
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:48,603 - mmdet - INFO - Evaluating segm...
2022-10-14 15:32:48,669 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:48,670 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:32:48,670 - mmdet - INFO - Epoch(val) [3][11]	bbox_mAP: 0.0000, bbox_mAP_50: 0.0000, bbox_mAP_75: 0.0000, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0000, bbox_mAP_l: 0.0000, bbox_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000, segm_mAP: 0.0000, segm_mAP_50: 0.0000, segm_mAP_75: 0.0000, segm_mAP_s: 0.0000, segm_mAP_m: 0.0000, segm_mAP_l: 0.0000, segm_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000
2022-10-14 15:32:51,667 - mmdet - INFO - Saving checkpoint at 4 epochs
2022-10-14 15:32:52,494 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:32:52,501 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:52,501 - mmdet - INFO - Evaluating segm...
2022-10-14 15:32:52,506 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000

2022-10-14 15:32:52,506 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:32:52,506 - mmdet - INFO - Epoch(val) [4][11]	bbox_mAP: 0.0000, bbox_mAP_50: 0.0000, bbox_mAP_75: 0.0000, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0000, bbox_mAP_l: 0.0000, bbox_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000, segm_mAP: 0.0000, segm_mAP_50: 0.0000, segm_mAP_75: 0.0000, segm_mAP_s: 0.0000, segm_mAP_m: 0.0000, segm_mAP_l: 0.0000, segm_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000
2022-10-14 15:32:55,488 - mmdet - INFO - Saving checkpoint at 5 epochs
2022-10-14 15:32:56,297 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:32:56,297 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:32:56,297 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:32:56,297 - mmdet - INFO - Epoch(val) [5][11]	
2022-10-14 15:32:59,288 - mmdet - INFO - Saving checkpoint at 6 epochs
2022-10-14 15:33:00,108 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:00,108 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:00,108 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:00,108 - mmdet - INFO - Epoch(val) [6][11]	
2022-10-14 15:33:03,106 - mmdet - INFO - Saving checkpoint at 7 epochs
2022-10-14 15:33:03,924 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:03,924 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:03,925 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:03,925 - mmdet - INFO - Epoch(val) [7][11]	
2022-10-14 15:33:06,938 - mmdet - INFO - Saving checkpoint at 8 epochs
2022-10-14 15:33:07,776 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:07,777 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:07,777 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:07,777 - mmdet - INFO - Epoch(val) [8][11]	
2022-10-14 15:33:10,800 - mmdet - INFO - Saving checkpoint at 9 epochs
2022-10-14 15:33:11,632 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:11,632 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:11,633 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:11,633 - mmdet - INFO - Epoch(val) [9][11]	
2022-10-14 15:33:14,696 - mmdet - INFO - Saving checkpoint at 10 epochs
2022-10-14 15:33:15,530 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:15,530 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:15,531 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:15,531 - mmdet - INFO - Epoch(val) [10][11]	
2022-10-14 15:33:18,518 - mmdet - INFO - Saving checkpoint at 11 epochs
2022-10-14 15:33:19,347 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:19,348 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:19,348 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:19,348 - mmdet - INFO - Epoch(val) [11][11]	
2022-10-14 15:33:22,329 - mmdet - INFO - Saving checkpoint at 12 epochs
2022-10-14 15:33:23,155 - mmdet - INFO - Evaluating bbox...
2022-10-14 15:33:23,155 - mmdet - ERROR - The testing results of the whole dataset is empty.
2022-10-14 15:33:23,155 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco_mydata.py
2022-10-14 15:33:23,155 - mmdet - INFO - Epoch(val) [12][11]
```
when I run demo/image_demo.py using custom checkpoint and custom config file, I got just original image(nothing changed) 

### Environment

sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.1, V11.1.105
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.1+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.2+cu111
OpenCV: 4.6.0
MMCV: 1.6.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.25.0+ca11860

### Additional information

_No response_"
Cannot use half-precision training when replacing cascade_rcnn's RPN with gflhead,open-mmlab/mmdetection,2022-10-14 03:14:32,2,,9020,1408693144,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

When I replaced cascade_rcnn's RPN with gflhead and used amp training, the following error was encountered:
![LTAW$E7CK27F$}_J HMK5{N](https://user-images.githubusercontent.com/90194592/195753297-37281858-938c-428a-bb58-4a656acc02fd.png)



### Environment

System environment:
    sys.platform: linux
    Python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) [GCC 9.4.0]
    CUDA available: True
    numpy_random_seed: 821101802
    GPU 0: Tesla P100-PCIE-16GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.0, V11.0.221
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
    PyTorch: 1.11.0
    PyTorch compiling details: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2022.1-Product Build 20220311 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.0, CUDNN_VERSION=8.0.5, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.12.0
    OpenCV: 4.5.4
    MMEngine: 0.1.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    Distributed launcher: none
    Distributed training: False
    GPU number: 1

### Additional information

The config file for training is shown below:
```python
model = dict(
    type='CascadeRCNN',
    data_preprocessor=dict(
        type='DetDataPreprocessor',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        bgr_to_rgb=True,
        pad_size_divisor=32),
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5,
        start_level=1,
        add_extra_convs='on_output'),
    rpn_head=dict(
        type='GFLHead',
        num_classes=1,
        in_channels=256,
        stacked_convs=4,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0],
            octave_base_scale=8,
            scales_per_octave=1,
            strides=[8, 16, 32, 64, 128]),
        loss_cls=dict(
            type='QualityFocalLoss',
            use_sigmoid=True,
            beta=2.0,
            loss_weight=1.0),
        loss_dfl=dict(type='DistributionFocalLoss', loss_weight=0.25),
        reg_max=16,
        loss_bbox=dict(type='GIoULoss', loss_weight=2.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[8, 16, 32, 64, 128]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=20,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=20,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=20,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(type='ATSSAssigner', topk=9),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0,
            score_thr=0.05),
        rcnn=[
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.5,
                    neg_iou_thr=0.5,
                    min_pos_iou=0.5,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.6,
                    neg_iou_thr=0.6,
                    min_pos_iou=0.6,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.7,
                    neg_iou_thr=0.7,
                    min_pos_iou=0.7,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False)
        ]),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=100,
            nms=dict(type='nms', iou_threshold=0.6),
            min_bbox_size=0,
            score_thr=0.05),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100)))
dataset_type = 'CocoDataset'
data_root = '/kaggle/working/mmdetection/data/coco/'
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadImageFromFile', file_client_args=dict(backend='disk')),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', scale=(1000, 600), keep_ratio=True),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PackDetInputs')
]
test_pipeline = [
    dict(type='LoadImageFromFile', file_client_args=dict(backend='disk')),
    dict(type='Resize', scale=(1000, 600), keep_ratio=True),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='PackDetInputs',
        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                   'scale_factor'))
]
train_dataloader = dict(
    batch_size=8,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=True),
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    dataset=dict(
        type='CocoDataset',
        data_root='/kaggle/working/mmdetection/data/coco/',
        ann_file=
        '/kaggle/working/mmdetection/data/coco/annotations/voc12_train.json',
        data_prefix=dict(img='images/'),
        filter_cfg=dict(filter_empty_gt=True, min_size=32),
        pipeline=[
            dict(
                type='LoadImageFromFile',
                file_client_args=dict(backend='disk')),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', scale=(1000, 600), keep_ratio=True),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PackDetInputs')
        ],
        metainfo=dict(
            CLASSES=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
                     'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
                     'horse', 'motorbike', 'person', 'pottedplant', 'sheep',
                     'sofa', 'train', 'tvmonitor'),
            PALETTE=[(106, 0, 228), (119, 11, 32), (165, 42, 42), (0, 0, 192),
                     (197, 226, 255),
                     (0, 60, 100), (0, 0, 142), (255, 77, 255), (153, 69, 1),
                     (120, 166, 157), (0, 182, 199), (0, 226, 252),
                     (182, 182, 255), (0, 0, 230), (220, 20, 60),
                     (163, 255, 0), (0, 82, 0), (3, 95, 161), (0, 80, 100),
                     (183, 130, 88)])))
val_dataloader = dict(
    batch_size=8,
    num_workers=2,
    persistent_workers=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='CocoDataset',
        data_root='/kaggle/working/mmdetection/data/coco/',
        ann_file=
        '/kaggle/working/mmdetection/data/coco/annotations/voc12_val.json',
        data_prefix=dict(img='images/'),
        test_mode=True,
        pipeline=[
            dict(
                type='LoadImageFromFile',
                file_client_args=dict(backend='disk')),
            dict(type='Resize', scale=(1000, 600), keep_ratio=True),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='PackDetInputs',
                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                           'scale_factor'))
        ],
        metainfo=dict(
            CLASSES=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
                     'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
                     'horse', 'motorbike', 'person', 'pottedplant', 'sheep',
                     'sofa', 'train', 'tvmonitor'),
            PALETTE=[(106, 0, 228), (119, 11, 32), (165, 42, 42), (0, 0, 192),
                     (197, 226, 255),
                     (0, 60, 100), (0, 0, 142), (255, 77, 255), (153, 69, 1),
                     (120, 166, 157), (0, 182, 199), (0, 226, 252),
                     (182, 182, 255), (0, 0, 230), (220, 20, 60),
                     (163, 255, 0), (0, 82, 0), (3, 95, 161), (0, 80, 100),
                     (183, 130, 88)])))
test_dataloader = dict(
    batch_size=8,
    num_workers=2,
    persistent_workers=True,
    drop_last=False,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='CocoDataset',
        data_root='/kaggle/working/mmdetection/data/coco/',
        ann_file=
        '/kaggle/working/mmdetection/data/coco/annotations/voc12_val.json',
        data_prefix=dict(img='images/'),
        test_mode=True,
        pipeline=[
            dict(
                type='LoadImageFromFile',
                file_client_args=dict(backend='disk')),
            dict(type='Resize', scale=(1000, 600), keep_ratio=True),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='PackDetInputs',
                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                           'scale_factor'))
        ],
        metainfo=dict(
            CLASSES=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
                     'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
                     'horse', 'motorbike', 'person', 'pottedplant', 'sheep',
                     'sofa', 'train', 'tvmonitor'),
            PALETTE=[(106, 0, 228), (119, 11, 32), (165, 42, 42), (0, 0, 192),
                     (197, 226, 255),
                     (0, 60, 100), (0, 0, 142), (255, 77, 255), (153, 69, 1),
                     (120, 166, 157), (0, 182, 199), (0, 226, 252),
                     (182, 182, 255), (0, 0, 230), (220, 20, 60),
                     (163, 255, 0), (0, 82, 0), (3, 95, 161), (0, 80, 100),
                     (183, 130, 88)])))
val_evaluator = dict(
    type='CocoMetric',
    ann_file='/kaggle/working/mmdetection/data/coco/annotations/voc12_val.json',
    metric='bbox',
    format_only=False,
    classwise=True)
test_evaluator = dict(
    type='CocoMetric',
    ann_file='/kaggle/working/mmdetection/data/coco/annotations/voc12_val.json',
    metric='bbox',
    format_only=False,
    classwise=True)
train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=12, val_interval=1)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
param_scheduler = [
    dict(
        type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),
    dict(
        type='MultiStepLR',
        begin=0,
        end=12,
        by_epoch=True,
        milestones=[8, 11],
        gamma=0.1)
]
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001))
auto_scale_lr = dict(enable=True, base_batch_size=16)
default_scope = 'mmdet'
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=50),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', interval=1),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='DetVisualizationHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'))
vis_backends = [dict(type='LocalVisBackend')]
visualizer = dict(
    type='DetLocalVisualizer',
    vis_backends=[dict(type='LocalVisBackend')],
    name='visualizer')
log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)
log_level = 'INFO'
load_from = None
resume = False
METAINFO = dict(
    CLASSES=('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',
             'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
             'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'),
    PALETTE=[(106, 0, 228), (119, 11, 32), (165, 42, 42), (0, 0, 192),
             (197, 226, 255), (0, 60, 100), (0, 0, 142), (255, 77, 255),
             (153, 69, 1), (120, 166, 157), (0, 182, 199), (0, 226, 252),
             (182, 182, 255), (0, 0, 230), (220, 20, 60), (163, 255, 0),
             (0, 82, 0), (3, 95, 161), (0, 80, 100), (183, 130, 88)])
launcher = 'none'
work_dir = './work_dirs/my_config'
```"
Model trained with custom backbone did not produce checkpoints,open-mmlab/mmdetection,2022-10-14 03:13:16,8,,9019,1408692278,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 💬 Describe the reimplementation questions

I am training a MaskRCNN segmentation model with the UNet backbone. However, there are no checkpoints generated at the end of the training run.

Here is the implementation of the backbone.

```py
# Copyright (c) OpenMMLab. All rights reserved.
import argparse
import os
import os.path as osp
import time
import warnings

import mmcv
import torch
from mmcv import Config, DictAction
from mmcv.cnn import fuse_conv_bn
from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
                         wrap_fp16_model)

from mmdet.apis import multi_gpu_test, single_gpu_test
from mmdet.datasets import (build_dataloader, build_dataset,
                            replace_ImageToTensor)
from mmdet.models import build_detector
from mmdet.utils import (build_ddp, build_dp, compat_cfg, get_device,
                         replace_cfg_vals, setup_multi_processes,
                         update_data_root)


def parse_args():
    parser = argparse.ArgumentParser(
        description='MMDet test (and eval) a model')
    parser.add_argument('config', help='test config file path')
    parser.add_argument('checkpoint', help='checkpoint file')
    parser.add_argument(
        '--work-dir',
        help='the directory to save the file containing evaluation metrics')
    parser.add_argument('--out', help='output result file in pickle format')
    parser.add_argument(
        '--fuse-conv-bn',
        action='store_true',
        help='Whether to fuse conv and bn, this will slightly increase'
        'the inference speed')
    parser.add_argument(
        '--gpu-ids',
        type=int,
        nargs='+',
        help='(Deprecated, please use --gpu-id) ids of gpus to use '
        '(only applicable to non-distributed training)')
    parser.add_argument(
        '--gpu-id',
        type=int,
        default=0,
        help='id of gpu to use '
        '(only applicable to non-distributed testing)')
    parser.add_argument(
        '--format-only',
        action='store_true',
        help='Format the output results without perform evaluation. It is'
        'useful when you want to format the result to a specific format and '
        'submit it to the test server')
    parser.add_argument(
        '--eval',
        type=str,
        nargs='+',
        help='evaluation metrics, which depends on the dataset, e.g., ""bbox"",'
        ' ""segm"", ""proposal"" for COCO, and ""mAP"", ""recall"" for PASCAL VOC')
    parser.add_argument('--show', action='store_true', help='show results')
    parser.add_argument(
        '--show-dir', help='directory where painted images will be saved')
    parser.add_argument(
        '--show-score-thr',
        type=float,
        default=0.3,
        help='score threshold (default: 0.3)')
    parser.add_argument(
        '--gpu-collect',
        action='store_true',
        help='whether to use gpu to collect results.')
    parser.add_argument(
        '--tmpdir',
        help='tmp directory used for collecting results from multiple '
        'workers, available when gpu-collect is not specified')
    parser.add_argument(
        '--cfg-options',
        nargs='+',
        action=DictAction,
        help='override some settings in the used config, the key-value pair '
        'in xxx=yyy format will be merged into config file. If the value to '
        'be overwritten is a list, it should be like key=""[a,b]"" or key=a,b '
        'It also allows nested list/tuple values, e.g. key=""[(a,b),(c,d)]"" '
        'Note that the quotation marks are necessary and that no white space '
        'is allowed.')
    parser.add_argument(
        '--options',
        nargs='+',
        action=DictAction,
        help='custom options for evaluation, the key-value pair in xxx=yyy '
        'format will be kwargs for dataset.evaluate() function (deprecate), '
        'change to --eval-options instead.')
    parser.add_argument(
        '--eval-options',
        nargs='+',
        action=DictAction,
        help='custom options for evaluation, the key-value pair in xxx=yyy '
        'format will be kwargs for dataset.evaluate() function')
    parser.add_argument(
        '--launcher',
        choices=['none', 'pytorch', 'slurm', 'mpi'],
        default='none',
        help='job launcher')
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args()
    if 'LOCAL_RANK' not in os.environ:
        os.environ['LOCAL_RANK'] = str(args.local_rank)

    if args.options and args.eval_options:
        raise ValueError(
            '--options and --eval-options cannot be both '
            'specified, --options is deprecated in favor of --eval-options')
    if args.options:
        warnings.warn('--options is deprecated in favor of --eval-options')
        args.eval_options = args.options
    return args


def main():
    args = parse_args()

    assert args.out or args.eval or args.format_only or args.show \
        or args.show_dir, \
        ('Please specify at least one operation (save/eval/format/show the '
         'results / save the results) with the argument ""--out"", ""--eval""'
         ', ""--format-only"", ""--show"" or ""--show-dir""')

    if args.eval and args.format_only:
        raise ValueError('--eval and --format_only cannot be both specified')

    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):
        raise ValueError('The output file must be a pkl file.')

    cfg = Config.fromfile(args.config)

    # replace the ${key} with the value of cfg.key
    cfg = replace_cfg_vals(cfg)

    # update data root according to MMDET_DATASETS
    update_data_root(cfg)

    if args.cfg_options is not None:
        cfg.merge_from_dict(args.cfg_options)

    cfg = compat_cfg(cfg)

    # set multi-process settings
    setup_multi_processes(cfg)

    # set cudnn_benchmark
    if cfg.get('cudnn_benchmark', False):
        torch.backends.cudnn.benchmark = True

    if 'pretrained' in cfg.model:
        cfg.model.pretrained = None
    elif 'init_cfg' in cfg.model.backbone:
        cfg.model.backbone.init_cfg = None

    if cfg.model.get('neck'):
        if isinstance(cfg.model.neck, list):
            for neck_cfg in cfg.model.neck:
                if neck_cfg.get('rfp_backbone'):
                    if neck_cfg.rfp_backbone.get('pretrained'):
                        neck_cfg.rfp_backbone.pretrained = None
        elif cfg.model.neck.get('rfp_backbone'):
            if cfg.model.neck.rfp_backbone.get('pretrained'):
                cfg.model.neck.rfp_backbone.pretrained = None

    if args.gpu_ids is not None:
        cfg.gpu_ids = args.gpu_ids[0:1]
        warnings.warn('`--gpu-ids` is deprecated, please use `--gpu-id`. '
                      'Because we only support single GPU mode in '
                      'non-distributed testing. Use the first GPU '
                      'in `gpu_ids` now.')
    else:
        cfg.gpu_ids = [args.gpu_id]
    cfg.device = get_device()
    # init distributed env first, since logger depends on the dist info.
    if args.launcher == 'none':
        distributed = False
    else:
        distributed = True
        init_dist(args.launcher, **cfg.dist_params)

    test_dataloader_default_args = dict(
        samples_per_gpu=1, workers_per_gpu=2, dist=distributed, shuffle=False)

    # in case the test dataset is concatenated
    if isinstance(cfg.data.test, dict):
        cfg.data.test.test_mode = True
        if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:
            # Replace 'ImageToTensor' to 'DefaultFormatBundle'
            cfg.data.test.pipeline = replace_ImageToTensor(
                cfg.data.test.pipeline)
    elif isinstance(cfg.data.test, list):
        for ds_cfg in cfg.data.test:
            ds_cfg.test_mode = True
        if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:
            for ds_cfg in cfg.data.test:
                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)

    test_loader_cfg = {
        **test_dataloader_default_args,
        **cfg.data.get('test_dataloader', {})
    }

    rank, _ = get_dist_info()
    # allows not to create
    if args.work_dir is not None and rank == 0:
        mmcv.mkdir_or_exist(osp.abspath(args.work_dir))
        timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
        json_file = osp.join(args.work_dir, f'eval_{timestamp}.json')

    # build the dataloader
    dataset = build_dataset(cfg.data.test)
    data_loader = build_dataloader(dataset, **test_loader_cfg)

    # build the model and load checkpoint
    cfg.model.train_cfg = None
    model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))
    fp16_cfg = cfg.get('fp16', None)
    if fp16_cfg is not None:
        wrap_fp16_model(model)
    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')
    if args.fuse_conv_bn:
        model = fuse_conv_bn(model)
    # old versions did not save class info in checkpoints, this walkaround is
    # for backward compatibility
    if 'CLASSES' in checkpoint.get('meta', {}):
        model.CLASSES = checkpoint['meta']['CLASSES']
    else:
        model.CLASSES = dataset.CLASSES

    if not distributed:
        model = build_dp(model, cfg.device, device_ids=cfg.gpu_ids)
        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir,
                                  args.show_score_thr)
    else:
        model = build_ddp(
            model,
            cfg.device,
            device_ids=[int(os.environ['LOCAL_RANK'])],
            broadcast_buffers=False)
        outputs = multi_gpu_test(
            model, data_loader, args.tmpdir, args.gpu_collect
            or cfg.evaluation.get('gpu_collect', False))

    with open(""output_binary.txt"", ""w"") as fp:
        fp.write(str(outputs))

    rank, _ = get_dist_info()
    if rank == 0:
        if args.out:
            print(f'\nwriting results to {args.out}')
            mmcv.dump(outputs, args.out)
        kwargs = {} if args.eval_options is None else args.eval_options
        if args.format_only:
            dataset.format_results(outputs, **kwargs)
        if args.eval:
            eval_kwargs = cfg.get('evaluation', {}).copy()
            # hard-code way to remove EvalHook args
            for key in [
                    'interval', 'tmpdir', 'start', 'gpu_collect', 'save_best',
                    'rule', 'dynamic_intervals'
            ]:
                eval_kwargs.pop(key, None)
            eval_kwargs.update(dict(metric=args.eval, **kwargs))
            metric = dataset.evaluate(outputs, **eval_kwargs)
            print(metric)
            metric_dict = dict(config=args.config, metric=metric)
            if args.work_dir is not None and rank == 0:
                mmcv.dump(metric_dict, json_file)


if __name__ == '__main__':
    main()
```

### Environment

sys.platform: win32
Python: 3.9.12 (main, Apr 4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 2080 Ti
CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.2
NVCC: Cuda compilation tools, release 10.2, V10.2.8
MSVC: Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30145 for x64
GCC: n/a
PyTorch: 1.10.1+cu102
PyTorch compiling details: PyTorch built with:

    C++ Version: 199711
    MSVC 192829337
    Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
    Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
    OpenMP 2019
    LAPACK is enabled (usually provided by MKL)
    CPU capability usage: AVX2
    CUDA Runtime 10.2
    NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
    CuDNN 7.6.5
    Magma 2.5.4
    Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=C:/w/b/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/w/b/windows/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON,

TorchVision: 0.11.2+cu102
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: MSVC 192930137
MMCV CUDA Compiler: 10.2
MMDetection: 2.25.1+

### Expected results

There should be a latest.pth checkpoint in the output directory.

![Screenshot from 2022-10-14 10-07-38](https://user-images.githubusercontent.com/9401015/195753240-b0ad0596-67e4-4adc-9be2-b36d398f058c.png)


### Additional information

1.) I modified the backbone\__init__.py file by adding the .UNet imports.
![Screenshot from 2022-10-14 10-09-45](https://user-images.githubusercontent.com/9401015/195753501-4f5faf20-356b-43b5-ae27-0ccf670edc2a.png)
2.) The original dataset is in the labelme format. However, I converted it into the MSCOCO format. I was able to train models with other backbones without any issues, so I doubt the problem lies with the dataset."
About the format of .json,open-mmlab/mmdetection,2022-10-13 16:58:54,1,feature request,9018,1408140854,"### What is the problem this feature will solve?

If i want to inference with the trained model and save the detect results to json file, can you help to do this, i have tried the command of '--format-only --options ""jsonfile_prefix=./res""', but in fact it cannot save the detect result of json format.

### What is the feature you are proposing to solve the problem?

Cannot work with the exited command.

### What alternatives have you considered?

_No response_"
Can panoptic FPN be export to ONNX？？,open-mmlab/mmdetection,2022-10-13 02:25:07,1,feature request,9011,1407035891,"### What is the problem this feature will solve?

Can panoptic FPN be export to ONNX？？

### What is the feature you are proposing to solve the problem?

Can panoptic FPN be export to ONNX？？

### What alternatives have you considered?

_No response_"
Huggingface Hub Integration,open-mmlab/mmdetection,2022-10-11 23:24:33,2,feature request,9002,1405345502,"### What is the problem this feature will solve?

Pushing models to huggingface hub after training and running inference with models stored in hub?

### What is the feature you are proposing to solve the problem?

I would like to work on an integration to push models to huggingface hub and run inference from MMdetection models stored in the hub. 

What do you guys think?

### What alternatives have you considered?

_No response_"
MMDET is not fully Multi-Backend supported,open-mmlab/mmdetection,2022-10-11 09:48:16,3,feature request,8998,1404345368,"### What is the problem this feature will solve?

MMDET中部分逻辑限制了运行设备仅能为CUDA或CPU，导致在其他设备上运行时存在问题（例如MLU）。

### What is the feature you are proposing to solve the problem?

需要修改MMDET中部分模块的运行逻辑，使其支持根据运行状态切换设备后端，且新增的模块和功能也需要包含设备后端的判断及支持。

### What alternatives have you considered?

在以下代码中，infinite_sampler使用了无device配置的sync_random_seed。在默认情况下，此接口会将Tensor配置为CUDA Tensor，并导致设备类型不是CUDA时（例如MLU）运行失败。


https://github.com/open-mmlab/mmdetection/blob/9d3e162459590eee4cfc891218dfbb5878378842/mmdet/datasets/samplers/infinite_sampler.py#L59

https://github.com/open-mmlab/mmdetection/blob/9d3e162459590eee4cfc891218dfbb5878378842/mmdet/core/utils/dist_utils.py#L157

在类似的其他接口中，某些接口（如下面给出的distributed_sampler）提供了自动设备选择的功能，可以识别出当前的运行设备。

https://github.com/open-mmlab/mmdetection/blob/9d3e162459590eee4cfc891218dfbb5878378842/mmdet/datasets/samplers/distributed_sampler.py#L29
"
请问如何在性能指标中添加precision,open-mmlab/mmdetection,2022-10-11 06:40:49,2,feature request,8994,1404086386,"### What is the problem this feature will solve?

是

### What is the feature you are proposing to solve the problem?

是

### What alternatives have you considered?

_No response_"
Collate (Collate.py) raises errors with LoadMultiChannelImagesFromFile,open-mmlab/mmdetection,2022-10-10 16:24:05,2,,8990,1403430052,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

The function collate from [collate.py](https://github.com/open-mmlab/mmcv/blob/master/mmcv/parallel/collate.py) creates three errors when called in the [inference detector](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/apis/inference.py#L136). The first one is caused by the returned data structure of the FileClient called two lines above. The second and third error exist when the given structure for the collate-call is seemingly adapted to fit the call (shown in code below).

### Environment

PyTorch is installed via pip.

Collect_env():

```
No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'

{'sys.platform': 'linux',
 'Python': '3.7.14 (default, Sep  8 2022, 00:06:44) [GCC 7.5.0]',
 'CUDA available': False,
 'GCC': 'x86_64-linux-gnu-gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0',
 'PyTorch': '1.9.0+cu111',
 'PyTorch compiling details': 'PyTorch built with:\n  - GCC 7.3\n  - C++ Version: 201402\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n',
 'TorchVision': '0.10.0+cu111',
 'OpenCV': '4.6.0',
 'MMCV': '1.6.2',
 'MMCV Compiler': 'GCC 7.3',
 'MMCV CUDA Compiler': '11.1'}
```

### Additional information

I use a custom model with custom dataset and adapted the config to MultiChannelImages to be able to give a pair of images per instance. The nested list below is therefore a batch-list with only one element/instance. The model config currently has 'cpu' saved as device because I'm testing the workflow/functions on Google Colab and cuda is not found there (see collect_env()).

```
instance_ =[['/path/to/file/filename1.png', '/path/to/file/filename2.png']]
result = inference_detector(model, instance_)
```

The problem occurs in this snippet of inference_detector():
```
datas = []
    for img in imgs:
        # prepare data
        if isinstance(img, np.ndarray):
            # directly add img
            data = dict(img=img)
        else:
            # add information into dict
            data = dict(img_info=dict(filename=img), img_prefix=None)
        # build the data pipeline
        data = test_pipeline(data)
        datas.append(data)

    data = collate(datas, samples_per_gpu=len(imgs))
```

_datas[0]_ has this structure:
```
{'img_info': 
    {'filename': ['/path/to/file/filename1.png', '/path/to/file/filename2.png']}, 
'img_prefix': None, 
'filename': ['/path/to/file/filename1.png', '/path/to/file/filename2.png'], 
'ori_filename': ['/path/to/file/filename1.png', '/path/to/file/filename2.png'], 
'img': array([[[[116, 152],
            [111,  53],
            [102,  44]], .... ]], dtype=uint8), 'img_shape': (240, 320, 3, 2), 'ori_shape': (240, 320, 3, 2), 'pad_shape': (240, 320, 3, 2), 'scale_factor': 1.0, 'img_norm_cfg': {'mean': array([0., 0., 0.], dtype=float32), 'std': array([1., 1., 1.], dtype=float32), 'to_rgb': False}}
```
            
collate throws this error (exported inference_detector into an own defined method to be able to check some values in between, therefore line numbers are not original line numbers of inference.py):
`RecursionError: maximum recursion depth exceeded while calling a Python object`

```
/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in collate(batch, samples_per_gpu)
     79         return {
     80             key: collate([d[key] for d in batch], samples_per_gpu)
---> 81             for key in batch[0]
     82         }
     83     else:

/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in <dictcomp>(.0)
     79         return {
     80             key: collate([d[key] for d in batch], samples_per_gpu)
---> 81             for key in batch[0]
     82         }
     83     else:

/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in collate(batch, samples_per_gpu)
     79         return {
     80             key: collate([d[key] for d in batch], samples_per_gpu)
---> 81             for key in batch[0]
     82         }
     83     else:

/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in <dictcomp>(.0)
     79         return {
     80             key: collate([d[key] for d in batch], samples_per_gpu)
---> 81             for key in batch[0]
     82         }
     83     else:

/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in collate(batch, samples_per_gpu)
     75     elif isinstance(batch[0], Sequence):
     76         transposed = zip(*batch)
---> 77         return [collate(samples, samples_per_gpu) for samples in transposed]
     78     elif isinstance(batch[0], Mapping):
     79         return {

/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in <listcomp>(.0)
     75     elif isinstance(batch[0], Sequence):
     76         transposed = zip(*batch)
---> 77         return [collate(samples, samples_per_gpu) for samples in transposed]
     78     elif isinstance(batch[0], Mapping):
     79         return {

... last 2 frames repeated, from the frame below ...

/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in collate(batch, samples_per_gpu)
     75     elif isinstance(batch[0], Sequence):
     76         transposed = zip(*batch)
---> 77         return [collate(samples, samples_per_gpu) for samples in transposed]
     78     elif isinstance(batch[0], Mapping):
     79         return {
```

When replacing _datas_ with _datas[0]['img']_, where the actual image data is saved as tensors, I get a TypeError:
```
/usr/local/lib/python3.7/dist-packages/mmcv/parallel/collate.py in collate(batch, samples_per_gpu)
     22 
     23     if not isinstance(batch, Sequence):
---> 24         raise TypeError(f'{batch.dtype} is not supported.')
     25 
     26     if isinstance(batch[0], DataContainer):

TypeError: uint8 is not supported.
```

This is raised because the return value of the _'img'_-variable is returned as a tuple(iirc) and is not a subclass of Sequence. By adapting it slightly in a customized inference_detector-class to make an array:
`datas[0]['img'] = [x for x in datas[0]['img']]`
Then collate is working but the next line is throwing an IndexError:

```
<ipython-input-18-efad1e7fb76c> in inference_detector(model, imgs)
     70     data = collate(datas, samples_per_gpu=len(imgs))
     71    # just get the actual data from DataContainer
---> 72     data['img_metas'] = [img_metas.data[0] for img_metas in data['img_metas']]
     73     data['img'] = [img.data[0] for img in data['img']]
     74     if next(model.parameters()).is_cuda:

IndexError: too many indices for tensor of dimension 4
```

_data_ is only the tensor data after the collate-call."
"I use DTR to train my dataset, but mAP has always been 0",open-mmlab/mmdetection,2022-10-10 06:54:33,1,,8987,1402682706,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 💬 Describe the reimplementation questions

mAP is always 0

### Environment

dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='AutoAugment',
        policies=[[{
            'type':
            'Resize',
            'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333),
                          (608, 1333), (640, 1333), (672, 1333), (704, 1333),
                          (736, 1333), (768, 1333), (800, 1333)],
            'multiscale_mode':
            'value',
            'keep_ratio':
            True
        }],
                  [{
                      'type': 'Resize',
                      'img_scale': [(400, 1333), (500, 1333), (600, 1333)],
                      'multiscale_mode': 'value',
                      'keep_ratio': True
                  }, {
                      'type': 'RandomCrop',
                      'crop_type': 'absolute_range',
                      'crop_size': (384, 600),
                      'allow_negative_crop': True
                  }, {
                      'type':
                      'Resize',
                      'img_scale': [(480, 1333), (512, 1333), (544, 1333),
                                    (576, 1333), (608, 1333), (640, 1333),
                                    (672, 1333), (704, 1333), (736, 1333),
                                    (768, 1333), (800, 1333)],
                      'multiscale_mode':
                      'value',
                      'override':
                      True,
                      'keep_ratio':
                      True
                  }]]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=1),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_train2017.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='AutoAugment',
                policies=[[{
                    'type':
                    'Resize',
                    'img_scale': [(480, 1333), (512, 1333), (544, 1333),
                                  (576, 1333), (608, 1333), (640, 1333),
                                  (672, 1333), (704, 1333), (736, 1333),
                                  (768, 1333), (800, 1333)],
                    'multiscale_mode':
                    'value',
                    'keep_ratio':
                    True
                }],
                          [{
                              'type': 'Resize',
                              'img_scale': [(400, 1333), (500, 1333),
                                            (600, 1333)],
                              'multiscale_mode': 'value',
                              'keep_ratio': True
                          }, {
                              'type': 'RandomCrop',
                              'crop_type': 'absolute_range',
                              'crop_size': (384, 600),
                              'allow_negative_crop': True
                          }, {
                              'type':
                              'Resize',
                              'img_scale': [(480, 1333), (512, 1333),
                                            (544, 1333), (576, 1333),
                                            (608, 1333), (640, 1333),
                                            (672, 1333), (704, 1333),
                                            (736, 1333), (768, 1333),
                                            (800, 1333)],
                              'multiscale_mode':
                              'value',
                              'override':
                              True,
                              'keep_ratio':
                              True
                          }]]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=1),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=1),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric='bbox')
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
model = dict(
    type='DETR',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(3, ),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    bbox_head=dict(
        type='DETRHead',
        num_classes=4,
        in_channels=2048,
        transformer=dict(
            type='Transformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1)
                    ],
                    feedforward_channels=2048,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DetrTransformerDecoder',
                return_intermediate=True,
                num_layers=6,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=dict(
                        type='MultiheadAttention',
                        embed_dims=256,
                        num_heads=8,
                        dropout=0.1),
                    feedforward_channels=2048,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        loss_cls=dict(
            type='CrossEntropyLoss',
            bg_cls_weight=0.1,
            use_sigmoid=False,
            loss_weight=1.0,
            class_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0)),
    train_cfg=dict(
        assigner=dict(
            type='HungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=1.0),
            reg_cost=dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),
            iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))),
    test_cfg=dict(max_per_img=100))
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    weight_decay=0.0001,
    paramwise_cfg=dict(
        custom_keys=dict(backbone=dict(lr_mult=0.1, decay_mult=1.0))))
optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))
lr_config = dict(policy='step', step=[100])
runner = dict(type='EpochBasedRunner', max_epochs=150)
work_dir = 'Aexp/detr'
auto_resume = False
gpu_ids = range(0, 1)


### Expected results

_No response_

### Additional information

_No response_"
"I managed to run the code on the server, but I couldn't find the dataset while debugging on pycharm",open-mmlab/mmdetection,2022-10-10 06:52:16,2,,8986,1402680683,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

FileNotFoundError: CocoDataset: [Errno 2] No such file or directory: 'data/coco/annotations/instances_train2017.json'

### Environment

dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='AutoAugment',
        policies=[[{
            'type':
            'Resize',
            'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333),
                          (608, 1333), (640, 1333), (672, 1333), (704, 1333),
                          (736, 1333), (768, 1333), (800, 1333)],
            'multiscale_mode':
            'value',
            'keep_ratio':
            True
        }],
                  [{
                      'type': 'Resize',
                      'img_scale': [(400, 1333), (500, 1333), (600, 1333)],
                      'multiscale_mode': 'value',
                      'keep_ratio': True
                  }, {
                      'type': 'RandomCrop',
                      'crop_type': 'absolute_range',
                      'crop_size': (384, 600),
                      'allow_negative_crop': True
                  }, {
                      'type':
                      'Resize',
                      'img_scale': [(480, 1333), (512, 1333), (544, 1333),
                                    (576, 1333), (608, 1333), (640, 1333),
                                    (672, 1333), (704, 1333), (736, 1333),
                                    (768, 1333), (800, 1333)],
                      'multiscale_mode':
                      'value',
                      'override':
                      True,
                      'keep_ratio':
                      True
                  }]]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=1),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_train2017.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='AutoAugment',
                policies=[[{
                    'type':
                    'Resize',
                    'img_scale': [(480, 1333), (512, 1333), (544, 1333),
                                  (576, 1333), (608, 1333), (640, 1333),
                                  (672, 1333), (704, 1333), (736, 1333),
                                  (768, 1333), (800, 1333)],
                    'multiscale_mode':
                    'value',
                    'keep_ratio':
                    True
                }],
                          [{
                              'type': 'Resize',
                              'img_scale': [(400, 1333), (500, 1333),
                                            (600, 1333)],
                              'multiscale_mode': 'value',
                              'keep_ratio': True
                          }, {
                              'type': 'RandomCrop',
                              'crop_type': 'absolute_range',
                              'crop_size': (384, 600),
                              'allow_negative_crop': True
                          }, {
                              'type':
                              'Resize',
                              'img_scale': [(480, 1333), (512, 1333),
                                            (544, 1333), (576, 1333),
                                            (608, 1333), (640, 1333),
                                            (672, 1333), (704, 1333),
                                            (736, 1333), (768, 1333),
                                            (800, 1333)],
                              'multiscale_mode':
                              'value',
                              'override':
                              True,
                              'keep_ratio':
                              True
                          }]]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=1),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=1),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=1),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric='bbox')
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
model = dict(
    type='DETR',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(3, ),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    bbox_head=dict(
        type='DETRHead',
        num_classes=4,
        in_channels=2048,
        transformer=dict(
            type='Transformer',
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=[
                        dict(
                            type='MultiheadAttention',
                            embed_dims=256,
                            num_heads=8,
                            dropout=0.1)
                    ],
                    feedforward_channels=2048,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
            decoder=dict(
                type='DetrTransformerDecoder',
                return_intermediate=True,
                num_layers=6,
                transformerlayers=dict(
                    type='DetrTransformerDecoderLayer',
                    attn_cfgs=dict(
                        type='MultiheadAttention',
                        embed_dims=256,
                        num_heads=8,
                        dropout=0.1),
                    feedforward_channels=2048,
                    ffn_dropout=0.1,
                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',
                                     'ffn', 'norm')))),
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        loss_cls=dict(
            type='CrossEntropyLoss',
            bg_cls_weight=0.1,
            use_sigmoid=False,
            loss_weight=1.0,
            class_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=5.0),
        loss_iou=dict(type='GIoULoss', loss_weight=2.0)),
    train_cfg=dict(
        assigner=dict(
            type='HungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=1.0),
            reg_cost=dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),
            iou_cost=dict(type='IoUCost', iou_mode='giou', weight=2.0))),
    test_cfg=dict(max_per_img=100))
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    weight_decay=0.0001,
    paramwise_cfg=dict(
        custom_keys=dict(backbone=dict(lr_mult=0.1, decay_mult=1.0))))
optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))
lr_config = dict(policy='step', step=[100])
runner = dict(type='EpochBasedRunner', max_epochs=150)
work_dir = 'Aexp/detr'
auto_resume = False
gpu_ids = range(0, 1)


### Additional information

_No response_"
support val and test dataset in `tools/analysis_tools/browse_dataset.py` (MMDet 2.x and MMDet 3.x),open-mmlab/mmdetection,2022-10-07 15:32:14,1,feature request#v-3.x#v-2.x,8960,1401361853,"### What is the problem this feature will solve?

现有 `tools/analysis_tools/browse_dataset.py`的脚本支持训练集，测试集和验证集还不支持~

The existing `tools/analysis_tools/browse_dataset.py` script supports the training set, the test and validation sets are not yet supported ~


### What is the feature you are proposing to solve the problem?

support val and test dataset in `tools/analysis_tools/browse_dataset.py`

### What alternatives have you considered?

_No response_"
TypeError: PCB_Dataset: __init__() got an unexpected keyword argument 'dataset',open-mmlab/mmdetection,2022-10-05 09:18:36,3,,8941,1397493837,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [x] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/environment/miniconda3/lib/python3.7/site-packages/mmcv/utils/registry.py in build_from_cfg(cfg, registry, default_args)
     68     try:
---> 69         return obj_cls(**args)
     70     except Exception as e:

~/work/mmdet_PCB/mmdetection/mmdet/datasets/PCB_Dataset.py in __init__(self, min_size, img_subdir, ann_subdir, **kwargs)
     49         self.ann_subdir = ann_subdir
---> 50         super(PCB_Dataset, self).__init__(**kwargs)
     51         self.cat2label = {cat: i for i, cat in enumerate(self.CLASSES)}

TypeError: __init__() got an unexpected keyword argument 'dataset'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_22530/4226514775.py in <module>
      4 
      5 # 构建数据集
----> 6 datasets = [build_dataset(cfg.data.train)]
      7 
      8 # 构建检测模型

~/work/mmdet_PCB/mmdetection/mmdet/datasets/builder.py in build_dataset(cfg, default_args)
     80         dataset = _concat_dataset(cfg, default_args)
     81     else:
---> 82         dataset = build_from_cfg(cfg, DATASETS, default_args)
     83 
     84     return dataset

/environment/miniconda3/lib/python3.7/site-packages/mmcv/utils/registry.py in build_from_cfg(cfg, registry, default_args)
     70     except Exception as e:
     71         # Normal TypeError does not print class name.
---> 72         raise type(e)(f'{obj_cls.__name__}: {e}')
     73 
     74 

TypeError: PCB_Dataset: __init__() got an unexpected keyword argument 'dataset'

### Environment
```
Config:
model = dict(
    type='CascadeRCNN',
    backbone=dict(
        type='mmcls.ResNeSt',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            '/home/featurize/work/mmclassification/checkpoint/resnest50_imagenet_converted-1ebf0afe.pth',
            prefix='backbone.')),
    neck=dict(
        type='PAFPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=80,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=[
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.5,
                    neg_iou_thr=0.5,
                    min_pos_iou=0.5,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.6,
                    neg_iou_thr=0.6,
                    min_pos_iou=0.6,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.7,
                    neg_iou_thr=0.7,
                    min_pos_iou=0.7,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False)
        ]),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100)))
dataset_type = 'PCB_Dataset'
data_root = '/home/featurize/data/PCB_DATASET/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='Mosaic', img_scale=(1333, 800), pad_val=114.0),
    dict(
        type='RandomAffine', scaling_ratio_range=(0.1, 2),
        border=(-667, -400)),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.68, 116.779, 103.939],
                std=[58.393, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
evaluation = dict(interval=1, metric='bbox')
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='MultiImageMixDataset',
        ann_file='/home/featurize/data/PCB_DATASET/ImageSets/Main/train.txt',
        img_prefix='/home/featurize/data/PCB_DATASET/JPEGImages/',
        pipeline=[
            dict(type='Mosaic', img_scale=(1333, 800), pad_val=114.0),
            dict(
                type='RandomAffine',
                scaling_ratio_range=(0.1, 2),
                border=(-667, -400)),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        dataset=dict(
            type='PCB_Dataset',
            ann_file=
            '/home/featurize/data/PCB_DATASET/ImageSets/Main/train.txt',
            img_prefix='/home/featurize/data/PCB_DATASET/JPEGImages/',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True)
            ],
            filter_empty_gt=False)),
    val=dict(
        type='PCB_Dataset',
        ann_file='/home/featurize/data/PCB_DATASET/ImageSets/Main/val.txt',
        img_prefix='/home/featurize/data/PCB_DATASET/JPEGImages/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='PCB_Dataset',
        ann_file='/home/featurize/data/PCB_DATASET/ImageSets/Main/val.txt',
        img_prefix='/home/featurize/data/PCB_DATASET/JPEGImages/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    persistent_workers=True)
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[26, 29])
runner = dict(type='EpochBasedRunner', max_epochs=30)
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type='TensorboardLoggerHook'),
        dict(type='WandbLoggerHook')
    ])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
custom_imports = dict(imports=['mmcls.models'], allow_failed_imports=False)
pretrained = '/home/featurize/work/mmclassification/checkpoint/resnest50_imagenet_converted-1ebf0afe.pth'
img_scale = (1333, 800)
```
### Additional information

加Masic增强，按照文档中做了，但是依旧出问题。数据集格式是PASCAL VOC"
Creating multi-modal (bi-modal) detectors / pipelines for them,open-mmlab/mmdetection,2022-10-04 22:21:04,22,,8939,1396908790,"I adapted Faster R-CNN to be bi-modal (colour images and an aligned second 3-channel 2D sensor data). For this I duplicated the ResNet50-backbone and use a splitted forward-function. Given input _x_ for the function is a list of two elements. x[0] is forwarded into the first ResNet50-backbone, x[1] is forwarded into the second branch. Afterwards is fusioned by an element-wise max-operation and then continued as regular Faster R-CNN. 

The model structure created looks as expected, loading an adapted checkpoint works as well, but running images through the network does not work as the pipeline is not made for two images per instance and interprets it as a batch. Adapting the first-level methods (e.g. inference_detector()) does not work as the problem is deeply rooted inside the base-code of the models. So I figured out that I have to adapt the training- and test-pipelines. 

But [the custom pipeline tutorial](https://mmdetection.readthedocs.io/en/v2.0.0/tutorials/data_pipeline.html) doesn't explain how  I can achieve that or at least I can't derive it from its content. During research in the issues here I came across issue #1861 where it was asked how to change backbone input from 3 to 6 channels. This seems to be a suitable solution for my problem to have [6, width, height]-shaped image tensors as input instead of [2, 3, width, height]. During the forward-function of my custom model I could then split it into 2 [3, 320, 240] tensors and use it as before and the batch-problem would not occur. 

But neither in this issue nor in #6977 nor #4182 I can find a description about what to do and I haven't found the solution for creating such a custom pipeline by myself. That would be a great help. I also don't know how the img_norm_cfg get its values and how I would need to adopt this for 6 channels:

`img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)`
    
The annotations-json includes only the colour images (and annotations only refer to colour images therefore), but they are working for both modals as they are aligned and a pixel in one modal refers to the same point in the real world as the pixel at the same position in the second modal image.

**Summary**:
The new custom-backbone for 2 three-channel 2D-data is registered in registry and the adapted model is built by only adapting the backbone-dict in the config. Checkpoints are adapted accordingly.
Data set is a custom data set with 2 same-sized images per instance.
The reason for it not working is most likely an un-adapted pipeline config which I don't know how to customize correctly."
Bug from [Fix]: Fix get train_pipeline method of val workflow (#8575),open-mmlab/mmdetection,2022-10-04 14:34:23,4,awaiting response,8935,1396390956,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

The above fix only works when one chooses to go with RepeatDataset. 
This bug is coming from the config file attached below. 

### Environment

sys.platform: linux
Python: 3.9.12 (main, Jun  1 2022, 11:38:51) [GCC 7.5.0]
CUDA available: True
GPU 0,1: NVIDIA TITAN RTX
CUDA_HOME: /usr/local/cuda-11.5
NVCC: Cuda compilation tools, release 11.5, V11.5.119
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.12.0
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.0
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1+f8bbba2

### Additional information

checkpoint_config = dict(interval=5)
log_config = dict(
    interval=5,
    hooks=[
        dict(type='TensorboardLoggerHook'),
        dict(
            type='MMDetWandbHook',
            init_kwargs=dict(project='Anchor_based_obj_det'),
            interval=2,
            log_checkpoint=False,
            num_eval_images=10)
    ])
custom_hooks = [
    dict(type='NumClassCheckHook'),
    dict(type='CheckInvalidLossHook', interval=5, priority='VERY_LOW')
]
dist_params = dict(backend='nccl')
log_level = 'INFO'
resume_from = None
workflow = [('train', 1), ('val', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=24)
model = dict(
    type='YOLOV3',
    backbone=dict(
        type='Darknet',
        depth=53,
        out_indices=(3, 4, 5),
        init_cfg=dict(type='Pretrained', checkpoint='open-mmlab://darknet53')),
    neck=dict(
        type='YOLOV3Neck',
        num_scales=3,
        in_channels=[1024, 512, 256],
        out_channels=[512, 256, 128]),
    bbox_head=dict(
        type='YOLOV3Head',
        num_classes=9,
        in_channels=[512, 256, 128],
        out_channels=[1024, 512, 256],
        anchor_generator=dict(
            type='YOLOAnchorGenerator',
            base_sizes=[[(116, 90), (156, 198), (373, 326)],
                        [(30, 61), (62, 45), (59, 119)],
                        [(10, 13), (16, 30), (33, 23)]],
            strides=[32, 16, 8]),
        bbox_coder=dict(type='YOLOBBoxCoder'),
        featmap_strides=[32, 16, 8],
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            loss_weight=1.0,
            reduction='sum'),
        loss_conf=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            loss_weight=1.0,
            reduction='sum'),
        loss_xy=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            loss_weight=2.0,
            reduction='sum'),
        loss_wh=dict(type='MSELoss', loss_weight=2.0, reduction='sum')),
    train_cfg=dict(
        assigner=dict(
            type='GridAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.5,
            min_pos_iou=0)),
    test_cfg=dict(
        nms_pre=1000,
        min_bbox_size=0,
        score_thr=0.05,
        conf_thr=0.005,
        nms=dict(type='nms', iou_threshold=0.45),
        max_per_img=100))
dataset_type = 'CocoDataset'
data_root = '/datasets/KITTI/2d_object/training/'
img_norm_cfg = dict(mean=[0, 0, 0], std=[255.0, 255.0, 255.0], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Expand', mean=[0, 0, 0], to_rgb=True, ratio_range=(1, 2)),
    dict(
        type='MinIoURandomCrop',
        min_ious=(0.4, 0.5, 0.6, 0.7, 0.8, 0.9),
        min_crop_size=0.3),
    dict(type='Resize', img_scale=(416, 416), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[0, 0, 0],
        std=[255.0, 255.0, 255.0],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(416, 416),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[0, 0, 0],
                std=[255.0, 255.0, 255.0],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=48,
    workers_per_gpu=32,
    train=dict(
        classes=('Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting',
                 'Cyclist', 'Tram', 'Misc', 'DontCare'),
        type='CocoDataset',
        ann_file=
        '/datasets/KITTI/2d_object/training/split/image_2_label_CoCo_format_train_split.json',
        img_prefix='/datasets/KITTI/2d_object/training/image_2/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='Expand', mean=[0, 0, 0], to_rgb=True,
                ratio_range=(1, 2)),
            dict(
                type='MinIoURandomCrop',
                min_ious=(0.4, 0.5, 0.6, 0.7, 0.8, 0.9),
                min_crop_size=0.3),
            dict(type='Resize', img_scale=(416, 416), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[0, 0, 0],
                std=[255.0, 255.0, 255.0],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        classes=('Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting',
                 'Cyclist', 'Tram', 'Misc', 'DontCare'),
        ann_file=
        '/datasets/KITTI/2d_object/training/split/image_2_label_CoCo_format_val_split.json',
        img_prefix='/datasets/KITTI/2d_object/training/image_2/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(416, 416),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[255.0, 255.0, 255.0],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        classes=('Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting',
                 'Cyclist', 'Tram', 'Misc', 'DontCare'),
        ann_file=
        '/datasets/KITTI/2d_object/training/split/image_2_label_CoCo_format_test_split.json',
        img_prefix='/datasets/KITTI/2d_object/training/image_2/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(416, 416),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[0, 0, 0],
                        std=[255.0, 255.0, 255.0],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
optimizer = dict(type='SGD', lr=0.0001, momentum=0.99, weight_decay=1e-03)
# optimizer = dict(type='Adam', lr=0.0001, weight_decay=1e-03)
optimizer_config = dict(grad_clip=dict(max_norm=5, norm_type=2))
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=50,
    warmup_ratio=0.005,
    step=[150, 290])
runner = dict(type='EpochBasedRunner', max_epochs=300)
evaluation = dict(interval=1, classwise=True, metric=['bbox'], bbox_extra_attr=True)
classes = ('Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist',
           'Tram', 'Misc', 'DontCare')
total_epochs = 300
load_from = '/mmdetection/configs/yolo/saved_checkpoints/yolo_kitti_loss_3/latest.pth'
work_dir = '/mmdetection/configs/yolo/saved_checkpoints/yolo_kitti_loss_4'
auto_resume = False"
None results while using async_inference_detector,open-mmlab/mmdetection,2022-09-30 17:55:53,1,,8922,1392773258,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

I have used the `async_inference_detector` code from [here](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/1_exist_data_model.md#asynchronous-interface---supported-for-python-37)

The final results turned out to be `None` instead of actual results.

But using `inference_detector` instead of `async_inference_detector`, I get the actual results.
Is this related to bug of `async_inference_detector` ?

Here is my code:
```python
import asyncio
import os
import shutil
import urllib

import mmcv
import torch

from mmdet.apis import (async_inference_detector, inference_detector,
                        init_detector)
from mmdet.utils.contextmanagers import concurrent
from mmdet.utils.profiling import profile_time


async def main():
    """"""Benchmark between async and synchronous inference interfaces.

    Sample runs for 20 demo images on K80 GPU, model - mask_rcnn_r50_fpn_1x:

    async       sync

    7981.79 ms  9660.82 ms
    8074.52 ms  9660.94 ms
    7976.44 ms  9406.83 ms

    Async variant takes about 0.83-0.85 of the time of the synchronous
    interface.
    """"""
    
    checkpoint_file = ""./deepCrystal/epoch_dc_orig.pth""
    config_file = ""./deepCrystal/deep_crystal_orig.py"" 
    device = 'cuda:0'
    model = init_detector(
        config_file, checkpoint=checkpoint_file, device=device)

    # queue is used for concurrent inference of multiple images
    streamqueue = asyncio.Queue()
    # queue size defines concurrency level
    streamqueue_size = 4

    for _ in range(streamqueue_size):
        streamqueue.put_nowait(torch.cuda.Stream(device=device))

    # test a single image and show the results
    img = mmcv.imread(os.path.join('../test.png'))

    # warmup
    await async_inference_detector(model, img)

    async def detect(img):
        async with concurrent(streamqueue):
            return await async_inference_detector(model, img)

    num_of_images = 20
    with profile_time('benchmark', 'async'):
        tasks = [
            asyncio.create_task(detect(img)) for _ in range(num_of_images)
        ]
        async_results = await asyncio.gather(*tasks)

    with torch.cuda.stream(torch.cuda.default_stream()):
        with profile_time('benchmark', 'sync'):
            sync_results = [
                inference_detector(model, img) for _ in range(num_of_images)
            ]
    print(async_results)  #It is None
 


if __name__ == '__main__':
    asyncio.run(main())

```

### Environment

```
TorchVision: 0.10.0+cu111
OpenCV: 4.6.0
MMCV: 1.5.3
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.14.0+
```

### Additional information

_No response_"
Adding the possibility of getting ratio-preserving resizes performed according to height and width (i.e. not according to shortest size and longest size),open-mmlab/mmdetection,2022-09-30 07:10:53,1,feature request,8921,1391954178,"### What is the problem this feature will solve?

Currently, if we add for instance
`dict(type='Resize', img_scale=[(375, 667)], keep_ratio=True)`
at some point in a pipeline defined in a config file, then a resize will be performed so that we get the biggest possible image that fills these 3 conditions:
- the ratio of the original image is preserved
- the longest size is less than 667
- the shortest size is less than 375

However for my use case, theses conditions are not ideal; I would rather need a feature that would allow the conditions to be as follow:
- the ratio of the original image is preserved
- the width is less than 375
- the height is less than 667

Let's give a fictitious example that would illustrate the type of situation in which such a feature would be useful.
Let's say we have a dataset of pictures of people that stand in line while looking at the photographer (like 
souvenir pictures or something):
- the number of people in the picture as well as the image resolution vary from image to image, 
- the head and feet are never cut, so the apparent size of each person (as seen by the model) will be quite correlated to the height of the image (it may also be slightly affected by the margin that the photographer chose to take, but that does not amount to much)
- If there are few people in the picture, the longest size of the image will likely be the height (e.g. case of a picture taken with a smartphone vertically), otherwise it will likely be the width (e.g. case of a picure taken with a smartphone horizontally)

In such a scenario, with the current implementation of `Resize`, there is no possibility of resizing the image so that the apparent size of a person (as seen by the model) remains more or less constant from image to image (and thus allowing the accuracy to get slightly better, especially in situations where TTA would be too costly to perform).

However, if we had a feature looking like above, we could simply set the width to a value arbitrarily great and set the height to a fixed value to get what we want.
For example, writing something like this in the config file where the corresponding pipeline is defined would do the job:
`dict(type='Resize', img_scale=[(9999, 667)], keep_ratio=True, resize_according_to_height_and_width=True)`

Of course the above example is a fictitious one, but my actual use case _actually_ happened to be very similar, which is why I decided to fill this feature request.

### What is the feature you are proposing to solve the problem?

What I'm thinking of is basically a feature that would, thanks to an additional argument in the `__init__` method of `Resize`, allow the user to specify if he wants the resize to be performed wether according to shortest size and longest size (current behavior), or according to height and width (requested additional behavior).

### What alternatives have you considered?

I actually ended up modifying the source code of the `Resize` class so that the `scale` parameter gets dynamically set a second time according to the size of the input image (`scale` is re-set in such a way that its width/height ratio becomes the same as the one of the input image, hence allowing to get rid of the problem).

Of course I guess this is not ideal for a lot of reasons, one of them being possibly that the `scale` parameter is also output in the image metadatas. 

Below is the code for reference.

Modified `__init__` method:
```
    def __init__(self,
                 img_scale=None,
                 multiscale_mode='range',
                 ratio_range=None,
                 keep_ratio=True,
                 bbox_clip_border=True,
                 backend='cv2',
                 override=False,
                 height_width_based_resize=False):
        if img_scale is None:
            self.img_scale = None
        else:
            if isinstance(img_scale, list):
                self.img_scale = img_scale
            else:
                self.img_scale = [img_scale]
            assert mmcv.is_list_of(self.img_scale, tuple)

        if ratio_range is not None:
            # mode 1: given a scale and a range of image ratio
            assert len(self.img_scale) == 1
        else:
            # mode 2: given multiple scales or a range of scales
            assert multiscale_mode in ['value', 'range']

        self.backend = backend
        self.multiscale_mode = multiscale_mode
        self.ratio_range = ratio_range
        self.keep_ratio = keep_ratio
        # TODO: refactor the override option in Resize
        self.override = override
        self.bbox_clip_border = bbox_clip_border
        self.height_width_based_resize = height_width_based_resize
```

Modified `_random_scale` method:
```
    def _random_scale(self, results):
        """"""Randomly sample an img_scale according to ``ratio_range`` and
        ``multiscale_mode``.

        If ``ratio_range`` is specified, a ratio will be sampled and be
        multiplied with ``img_scale``.
        If multiple scales are specified by ``img_scale``, a scale will be
        sampled according to ``multiscale_mode``.
        Otherwise, single scale will be used.

        Args:
            results (dict): Result dict from :obj:`dataset`.

        Returns:
            dict: Two new keys 'scale` and 'scale_idx` are added into \
                ``results``, which would be used by subsequent pipelines.
        """"""

        if self.ratio_range is not None:
            scale, scale_idx = self.random_sample_ratio(
                self.img_scale[0], self.ratio_range)
        elif len(self.img_scale) == 1:
            scale, scale_idx = self.img_scale[0], 0
        elif self.multiscale_mode == 'range':
            scale, scale_idx = self.random_sample(self.img_scale)
        elif self.multiscale_mode == 'value':
            scale, scale_idx = self.random_select(self.img_scale)
        else:
            raise NotImplementedError

        if self.height_width_based_resize:
            # resize based on height and width (i.e. not based on shortest size and longest size)
            height, width = results['img'].shape[:2]
            width_s, height_s = scale
            ratio_widths, ratio_heights = width_s / width, height_s / height
            if ratio_widths >= ratio_heights:
                scale = round(width * ratio_heights), height_s
            else:
                scale = width_s, round(height * ratio_widths)

        results['scale'] = scale
        results['scale_idx'] = scale_idx
```

Modified `__repr__` method:
```
    def __repr__(self):
        repr_str = self.__class__.__name__
        repr_str += f'(img_scale={self.img_scale}, '
        repr_str += f'multiscale_mode={self.multiscale_mode}, '
        repr_str += f'ratio_range={self.ratio_range}, '
        repr_str += f'keep_ratio={self.keep_ratio}, '
        repr_str += f'bbox_clip_border={self.bbox_clip_border}),'
        repr_str += f'height_width_based_resize={self.height_width_based_resize})'
        return repr_str
```"
implement a model of FSAF model in distributed GPU's has an error of 'utf-8' codec can't decode byte 0x80 ,open-mmlab/mmdetection,2022-09-29 09:59:19,10,,8911,1390596238,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 💬 Describe the reimplementation questions

I am not sure if it is a bug or error in implementation 

I run this code for distribution training for a model fsaf:
bash tools/dist_train.sh configs/machine/fsaf_r101_fpn_1x_coco.py 4 --work-dir train_results/fsaf_ciou_resnet101

I run it within a bash script with activate python environment. 

the error massage is as bellow:
![recording error1](https://user-images.githubusercontent.com/10245810/193000185-e65d454d-c0ed-49e3-81fa-0b7b361d2877.jpg)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte

the model is being able to run with one gpu outside the bash script , but with low speed , so i would like to use the distribution training on 4 gpu's to make it faster. 

### Environment

my environment after running 'python mmdet/utils/collect_env.py' is 
![collect env](https://user-images.githubusercontent.com/10245810/192999579-f1ede1b9-4c78-4fb1-b555-92df661026f8.jpg)


I created the environment using the python environment as the coda is not working with the server system i have. 
I installed pytorch using pip from the official site.  

### Expected results

there is no results until now, as the model is not running. 

### Additional information

i did some modification on my model which i am sure it is correct. as i understand each steps
i used my own dataset which has similar format as the one of coco dataset and json files are valid 
i don't have any idea why this error occurred. "
[Feature Request] Direct SageMaker support?,open-mmlab/mmdetection,2022-09-28 16:01:29,5,planned feature#feature request,8910,1389606793,"### What is the problem this feature will solve?

A lot of individuals and companies use SageMaker for model training and deployment, but often they are not experts in taking repositories like this one and understanding how to wrap them with SageMaker. So instead, they often default to examples they can find that are already integrated with SageMaker. However in the object detection space, these examples can often be much less capable than MMDetection.

### What is the feature you are proposing to solve the problem?

Creating a `tools/train_sagemaker.py` and an example for training.

### What alternatives have you considered?

Right now I have a `train_sagemaker.py` script that launches training by executing`subprocess.Popen` with a command that uses `torchrun` to launch `tools/train.py`. For example:

```python
    # Train script config
    launch_config = [""torchrun"",
                     ""--nnodes"", str(world['number_of_machines']), ""--node_rank"", str(world['machine_rank']),
                     ""--nproc_per_node"", str(world['number_of_processes']), ""--master_addr"", world['master_addr'],
                     ""--master_port"", world['master_port']]

    train_config = [os.path.join(os.environ[""MMDETECTION""], ""tools/train.py""),
                    config_file,
                    ""--launcher"", ""pytorch"",
                    ""--work-dir"", '/opt/ml/checkpoints']

    if not args.validate:
        train_config.append(""--no-validate"")

    # Concat Pytorch Distributed Launch config and MMdetection config
    joint_cmd = "" "".join(str(x) for x in launch_config+train_config)
    print(""Following command will be executed: \n"", joint_cmd)

    process = subprocess.Popen(joint_cmd,  stderr=subprocess.STDOUT, stdout=subprocess.PIPE, shell=True)

    while True:
        output = process.stdout.readline()

        if process.poll() is not None:
            break
        if output:
            print(output.decode(""utf-8"").strip())

    rc = process.poll()

    if process.returncode != 0:
        raise subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)
```

But maybe there's a better way to accomplish this and integrate it more directly?"
faster_rcnn导出onnx模型时使用--verfiy参数报错,open-mmlab/mmdetection,2022-09-28 06:37:16,1,,8898,1388811953,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

command:
`python tools/deployment/pytorch2onnx.py  configs/faster_rcnn/faster_rcnn_r50_fpn_2x_coco.py  work_dirs/faster_rcnn_r50_fpn_2x_coco/epoch_24.pth --verify`


load checkpoint from local path: work_dirs/faster_rcnn_r50_fpn_2x_coco/epoch_24.pth
load checkpoint from local path: work_dirs/faster_rcnn_r50_fpn_2x_coco/epoch_24.pth
/home/CaiYingJie/workshop/Swin-Transformer-Object-Detection-master/mmdet/models/dense_heads/rpn_head.py:124: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  nms_pre_tensor = torch.tensor(
/home/CaiYingJie/workshop/Swin-Transformer-Object-Detection-master/mmdet/models/dense_heads/rpn_head.py:129: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]
/home/CaiYingJie/workshop/Swin-Transformer-Object-Detection-master/mmdet/models/dense_heads/rpn_head.py:145: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if nms_pre_tensor > 0:
/home/CaiYingJie/workshop/Swin-Transformer-Object-Detection-master/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py:78: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert pred_bboxes.size(0) == bboxes.size(0)
/home/CaiYingJie/workshop/Swin-Transformer-Object-Detection-master/mmdet/core/bbox/coder/delta_xywh_bbox_coder.py:80: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert pred_bboxes.size(1) == bboxes.size(1)
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/torch/tensor.py:587: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
  warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/mmcv/ops/nms.py:298: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  offsets = idxs.to(boxes) * (max_coordinate + torch.tensor(1).to(boxes))
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/mmcv/ops/nms.py:306: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if boxes_for_nms.shape[0] < split_thr or torch.onnx.is_in_onnx_export():
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/mmcv/ops/nms.py:159: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert boxes.size(1) == 4
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/mmcv/ops/nms.py:160: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert boxes.size(0) == scores.size(0)
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/mmcv/ops/roi_align.py:80: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert rois.size(1) == 5, 'RoI must be (idx, x1, y1, x2, y2)!'
/home/CaiYingJie/workshop/Swin-Transformer-Object-Detection-master/mmdet/core/post_processing/bbox_nms.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if multi_bboxes.shape[1] > 4:
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:2620: UserWarning: Exporting aten::index operator of advanced indexing in opset 11 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.
  warnings.warn(""Exporting aten::index operator of advanced indexing in opset "" +
/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:661: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.
  warnings.warn(""This model contains a squeeze operation on dimension "" + str(squeeze_dim) + "". If the model is "" +
**Successfully exported ONNX model: tmp.onnx**
2022-09-28 14:25:22.000022872 [W:onnxruntime:, graph.cc:1220 Graph] Initializer 2416 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
onnxruntime output names: ['boxes', 'labels'],             output shapes: [(100, 5), (100,)]
Traceback (most recent call last):
  File ""tools/deployment/pytorch2onnx.py"", line 231, in <module>
    pytorch2onnx(
  File ""tools/deployment/pytorch2onnx.py"", line 147, in pytorch2onnx
    np.testing.assert_allclose(
  File ""/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 1527, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/home/CaiYingJie/anaconda3/envs/swin/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 763, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.001, atol=1e-05

(shapes (1, 5), (0, 5) mismatch)
 x: array([[5.362104e+02, 4.808753e+02, 5.736962e+02, 5.813888e+02,
        1.238688e-02]], dtype=float32)
 y: array([], shape=(0, 5), dtype=float32)

### Environment

Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
sys.platform: linux
Python: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) [GCC 10.3.0]
CUDA available: True
GPU 0,1,2,3: Tesla V100-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (GCC) 5.2.0
PyTorch: 1.8.1+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
 
TorchVision: 0.9.1+cu102
OpenCV: 4.6.0
MMCV: 1.4.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.11.0+

### Additional information

1，更改了输入图片的尺寸大小，其他配置文件没有改动
2，使用的是coco数据集"
Found bug (by code inspection),open-mmlab/mmdetection,2022-09-27 22:57:59,1,v-3.x,8892,1388484940,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

CascadeRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin)
BBoxTestMixin has method: simple_test_bboxes

It will likely crash on line 89:  bbox_results = self._bbox_forward(x, rois). 

CascadeRoIHead's implementation of this method shows this signature: def _bbox_forward(self, stage, x, rois):
(Note, the parameter 'stage' is missing from the BBoxTestMixin's call.

Note: the only other class that uses the above Mixin classes, is StandardRoIHead(BaseRoIHead, BBoxTestMixin, MaskTestMixin).  The implementation of _bbox_forward in this class agrees with the signature from BBoxTestMixin.

Therefore, I wonder is self._bbox_forward(x, rois) ever called?  (or perhaps not ever called when training CascadeRCNN?).  It should crash if it is called.



### Environment

Environment probably not relevant since I ""found"" the above bug candidate by code inspection.

### Additional information

_No response_"
how can i change detection model for semantic segmentation task such as mask_RCNN,open-mmlab/mmdetection,2022-09-27 13:20:30,1,,8891,1387755274,"### Model/Dataset description

i want to change a detection model for semantic segmentation to test my tasks, but i have no idea now.

### Open source status

- [X] The model implementation is available
- [X] The model weights are available.

### Provide useful links for the implementation

_No response_"
"concat 2 diff types dataset train ,val and test issue",open-mmlab/mmdetection,2022-09-27 08:17:26,1,good first issue,8890,1387326804,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

I got 2 dataset 
Dataset_A have 15 classes with  COCO format
Dataset_B have 20 classes with VOC format

I followed the format in https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html#concatenate-dataset note part to create a concated dataset and the config like this:

```
img_norm_cfg = dict(
          mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
      train_pipeline = [
          dict(type='LoadImageFromFile'),
          dict(type='LoadAnnotations', with_bbox=True),
          dict(type='Resize', img_scale=(1024, 1024), keep_ratio=True),
          dict(type='RandomFlip', flip_ratio=0.5),
          dict(type='Normalize', **img_norm_cfg),
          dict(type='Pad', size_divisor=32),
          dict(type='DefaultFormatBundle'),
          dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
      ]
      test_pipeline = [
          dict(type='LoadImageFromFile'),
          dict(
              type='MultiScaleFlipAug',
              img_scale=(1024, 1024),
              flip=False,
              transforms=[
                  dict(type='Resize', keep_ratio=True),
                  dict(type='RandomFlip'),
                  dict(type='Normalize', **img_norm_cfg),
                  dict(type='Pad', size_divisor=32),
                  dict(type='ImageToTensor', keys=['img']),
                  dict(type='Collect', keys=['img']),
              ])
      ]
      dota_train_dict=dict(
          type='RepeatDataset',
          times=1,
          dataset=dict(
              type='CocoDataset',
              ann_file='ann_path/train_ann.json',
              img_prefix=img_prefix/images/',
              pipeline=train_pipeline
          )
      )
      dota_val_dict=dict(
              type='CocoDataset',
              ann_file='ann_path/val_ann.json',
              img_prefix=img_prefix/images/',
              pipeline=test_pipeline    
      )
      dota_test_dict=dict(
              type='CocoDataset',
              ann_file='ann_path/val_ann.json',
              img_prefix=img_prefix/images/',
              pipeline=test_pipeline
           
      )
      mar20_train_dict=dict(
          type='RepeatDataset',
          times=3,
          dataset=dict(
              type='VOCDataset',
              ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
              img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
              pipeline=train_pipeline
          )
      )
      # mar20_val_dict=dict(
      #     dataset=dict(
      #         type='VOCDataset',
      #        ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
      #        img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
      #         pipeline=test_pipeline
      #     )    
      # )
      # mar20_test_dict=dict(
      #     dataset=dict(
      #         type='VOCDataset',
      #        ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
      #        img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
      #         pipeline=test_pipeline
      #     )    
      # )
      data = dict(
          samples_per_gpu=4,
          workers_per_gpu=2,
          train=[dota_train_dict,mar20_train_dict],
          val=dota_val_dict,
          test=dota_test_dict
          )
      evaluation = dict(interval=1, metric='bbox')

```
I try to train this concat in a normal faster rcnn r50 fpn network.
so i set the num_classes to **a+b** which is 35 here
![emobile_2022-09-27_15-16-00](https://user-images.githubusercontent.com/17230723/192459095-046f43f8-3fcb-439a-843b-e4c3d4f6875b.png)

then i change both  /mmdet/datasets/coco.py and /mmdet/datasets/voc.py  file  CLASSES part to 
![emobile_2022-09-27_15-19-07](https://user-images.githubusercontent.com/17230723/192459770-07e4052b-699e-4dca-a35c-d8af4482e685.png)
which also num = 35 

**CASE1:** when I started training, everything was normal ,but i will  got issue when this 1st training process and the 1st val process finished.

```
2022-09-27 15:04:44,246 - mmdet - INFO - Epoch [1][1850/1898]   lr: 1.000e-04, eta: 3:05:32, time: 0.528, data_time: 0.022, memory: 8168, loss_rpn_cls: 0.1822, loss_rpn_bbox: 0.0940, loss_cls: 0.3737, acc: 91.3897, loss_bbox: 0.2509, loss: 0.9008
2022-09-27 15:05:16,011 - mmdet - INFO - Saving checkpoint at 1 epochs
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10131/10130, 42.3 task/s, elapsed: 240s, ETA:     0s

Traceback (most recent call last):
  File ""/home/xxxxx/xxxxx/mmdetection/./tools/train.py"", line 185, in <module>
    main()
  File ""/home/xxxxx/xxxxx/mmdetection/./tools/train.py"", line 174, in main
    train_detector(
  File ""/home/xxxxx/xxxxx/mmdetection/mmdet/apis/train.py"", line 203, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/xxxxx/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/xxxxx/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 54, in train
    self.call_hook('after_train_epoch')
  File ""/home/xxxxx/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/runner/base_runner.py"", line 307, in call_hook
    getattr(hook, fn_name)(self)
  File ""/home/xxxxx/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/runner/hooks/evaluation.py"", line 267, in after_train_epoch
    self._do_evaluate(runner)
  File ""/home/xxxxx/xxxxx/mmdetection/mmdet/core/evaluation/eval_hooks.py"", line 123, in _do_evaluate
    key_score = self.evaluate(runner, results)
  File ""/home/xxxxx/anaconda3/envs/openmmlab/lib/python3.9/site-packages/mmcv/runner/hooks/evaluation.py"", line 361, in evaluate
    eval_res = self.dataloader.dataset.evaluate(
  File ""/home/xxxxx/xxxxx/mmdetection/mmdet/datasets/coco.py"", line 445, in evaluate
    result_files, tmp_dir = self.format_results(results, jsonfile_prefix)
  File ""/home/xxxxx/xxxxx/mmdetection/mmdet/datasets/coco.py"", line 390, in format_results
    result_files = self.results2json(results, jsonfile_prefix)
  File ""/home/xxxxx/xxxxx/mmdetection/mmdet/datasets/coco.py"", line 322, in results2json
    json_results = self._det2json(results)
  File ""/home/xxxxx/xxxxx/mmdetection/mmdet/datasets/coco.py"", line 259, in _det2json
    data['category_id'] = self.cat_ids[label]
IndexError: list index out of range

```

**list out of range here**

**CASE2:** so I change the faster rcnn r50 fpn num_classes to 15 which equal to dataset_a class num 
at the same time i change the /mmdet/dataset/coco.py CLASSES part to only include dataset_A classes  and  the /mmdet/dataset/voc.py CLASSES part to only include dataset_B classes

but i still got issue
 
```
Done (t=0.94s)
creating index...
Done (t=0.96s)
creating index...
index created!
index created!
2022-09-27 15:40:25,563 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:194: nll_loss_forward_no_reduce_cuda_kernel: block: [1,0,0], thread: [0,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:194: nll_loss_forward_no_reduce_cuda_kernel: block: [1,0,0], thread: [1,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/pytorch/aten/src/ATen/native/cuda/Loss.cu:194: nll_loss_forward_no_reduce_cuda_kernel: block: [1,0,0], thread: [2,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.
/home/shuzhilian/anaconda3/envs/openmmlab/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 14 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
```

**CASE3:** SO I TRY LIKE THIS WAY
and this num_classes still 35

```
train_class=('plane', 'baseball-diamond',""bridge"",""ground-track-field"", ""small-vehicle"", ""large-vehicle"",""ship"", ""tennis-court"", 
              ""basketball-court"",""storage-tank"",""soccer-ball-field"",""roundabout"",""harbor"",""swimming-pool"",""helicopter"",'A1', 'A2', 'A3', 'A4', 
               'A5', 'A6', 'A7',
               'A8', 'A9', 'A10', 'A11', 'A12', 'A13',
               'A14', 'A15', 'A16', 'A17', 'A18', 'A19',
               'A20')
dota_class=('plane', 'baseball-diamond',""bridge"",""ground-track-field"", ""small-vehicle"", ""large-vehicle"",""ship"", ""tennis-court"", 
              ""basketball-court"",""storage-tank"",""soccer-ball-field"",""roundabout"",""harbor"",""swimming-pool"",""helicopter"")
mar20_class=('A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7',
               'A8', 'A9', 'A10', 'A11', 'A12', 'A13',
               'A14', 'A15', 'A16', 'A17', 'A18', 'A19',
               'A20')
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1024, 1024), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 1024),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
dota_train_dict=dict(
          type='RepeatDataset',
          times=1,
          dataset=dict(
              type='CocoDataset',
              classes=train_class,
              ann_file='ann_path/train_ann.json',
              img_prefix=img_prefix/images/',
              pipeline=train_pipeline
          )
      )
      dota_val_dict=dict(
              type='CocoDataset',
               classes=dota_class,
              ann_file='ann_path/val_ann.json',
              img_prefix=img_prefix/images/',
              pipeline=test_pipeline    
      )
      dota_test_dict=dict(
              type='CocoDataset',
               classes=dota_class,
              ann_file='ann_path/val_ann.json',
              img_prefix=img_prefix/images/',
              pipeline=test_pipeline
           
      )
      mar20_train_dict=dict(
          type='RepeatDataset',
          times=3,
          dataset=dict(
              type='VOCDataset',
               classes=train_class,
              ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
              img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
              pipeline=train_pipeline
          )
      )
      # mar20_val_dict=dict(
      #     dataset=dict(
      #         type='VOCDataset',
      #        ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
      #        img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
      #         pipeline=test_pipeline
      #     )    
      # )
      # mar20_test_dict=dict(
      #     dataset=dict(
      #         type='VOCDataset',
      #        ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
      #        img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
      #         pipeline=test_pipeline
      #     )    
      # )
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=2,
    train=[dota_train_dict,mar20_train_dict],
    val=dota_val_dict,
    test=dota_test_dict
    )
evaluation = dict(interval=1, metric='bbox')
```
IT SHOWS THE SAME PROBLEM AT **CASE1**



I really wonder how to correctly concat two different types of datasets for normal training, val and testing processes
and another question is :  it seem that i can not val and test this two different types datasets together after each epoch finished use the config below：
```
       mar20_val_dict=dict(
           dataset=dict(
               type='VOCDataset',
              ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
             img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
              pipeline=test_pipeline
           )    
       )
       mar20_test_dict=dict(
           dataset=dict(
               type='VOCDataset',
              ann_file='xxxxxxxxxx/ImageSets/Main/train.txt',
              img_prefix='xxxxxxxxxxxxxxxx/JPEGImage/',
               pipeline=test_pipeline
           )    
       )
      data = dict(
          samples_per_gpu=4,
          workers_per_gpu=2,
          train=[dota_train_dict,mar20_train_dict],
          val=[dota_val_dict,mar20_val_dict]
          test=[dota_test_dict,mar20_test_dict]
          )
      evaluation = dict(interval=1, metric='bbox')
      evaluation = dict(interval=1, metric='mAP')
``` 



### Environment

fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
sys.platform: linux
Python: 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]
CUDA available: True
GPU 0,2,3,4,6,7: TITAN RTX
GPU 1,5: Tesla P40
CUDA_HOME: :/usr/local/cuda-10.2
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~19.10) 7.5.0
PyTorch: 1.10.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu102
OpenCV: 4.5.4
MMCV: 1.4.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.19.0+ 

### Additional information

_No response_"
Custom iou_thrs cause bbox_mAP_50 and bbox_mAP_75 equal to -1,open-mmlab/mmdetection,2022-09-23 03:17:04,2,,8865,1383244039,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

Using custom iou_thrs:
```
evaluation = dict(
    save_best='auto',
    interval=interval,
    dynamic_intervals=[(max_epochs - num_last_epochs, 1)],
    metric='bbox',
    iou_thrs=[0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75])
```

In coco.py, it says the iou_thrs could be a list.
```
def evaluate(self,
             results,
             metric='bbox',
             logger=None,
             jsonfile_prefix=None,
             classwise=False,
             proposal_nums=(100, 300, 1000),
             iou_thrs=None,
             metric_items=None):
    """"""Evaluation in COCO protocol.

    Args:
        results (list[list | tuple]): Testing results of the dataset.
        metric (str | list[str]): Metrics to be evaluated. Options are
            'bbox', 'segm', 'proposal', 'proposal_fast'.
        logger (logging.Logger | str | None): Logger used for printing
            related information during evaluation. Default: None.
        jsonfile_prefix (str | None): The prefix of json files. It includes
            the file path and the prefix of filename, e.g., ""a/b/prefix"".
            If not specified, a temp file will be created. Default: None.
        classwise (bool): Whether to evaluating the AP for each class.
        proposal_nums (Sequence[int]): Proposal number used for evaluating
            recalls, such as recall@100, recall@1000.
            Default: (100, 300, 1000).
        iou_thrs (Sequence[float], optional): IoU threshold used for
            evaluating recalls/mAPs. If set to a list, the average of all
            IoUs will also be computed. If not specified, [0.50, 0.55,
            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.
            Default: None.
        metric_items (list[str] | str, optional): Metric items that will
            be returned. If not specified, ``['AR@100', 'AR@300',
            'AR@1000', 'AR_s@1000', 'AR_m@1000', 'AR_l@1000' ]`` will be
            used when ``metric=='proposal'``, ``['mAP', 'mAP_50', 'mAP_75',
            'mAP_s', 'mAP_m', 'mAP_l']`` will be used when
            ``metric=='bbox' or metric=='segm'``.

    Returns:
        dict[str, float]: COCO style evaluation metric.
    """"""
```

In cocoeval.py: 
```
def _summarize( ap=1, iouThr=None, areaRng='all', maxDets=100 ):
    p = self.params
    iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'
    titleStr = 'Average Precision' if ap == 1 else 'Average Recall'
    typeStr = '(AP)' if ap==1 else '(AR)'
    iouStr = '{:0.2f}:{:0.2f}'.format(p.iouThrs[0], p.iouThrs[-1]) \
        if iouThr is None else '{:0.2f}'.format(iouThr)

    aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]
    mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]
    if ap == 1:
        # dimension of precision: [TxRxKxAxM]
        s = self.eval['precision']
        # IoU
        if iouThr is not None:
            t = np.where(iouThr == p.iouThrs)[0]
            s = s[t]
        s = s[:,:,:,aind,mind]
    else:
        # dimension of recall: [TxKxAxM]
        s = self.eval['recall']
        if iouThr is not None:
            t = np.where(iouThr == p.iouThrs)[0]
            s = s[t]
        s = s[:,:,aind,mind]
    if len(s[s>-1])==0:
        mean_s = -1
    else:
        mean_s = np.mean(s[s>-1])
    print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))
    return mean_
```

p.iouThrs is the custom iou_thrs whose type is list. iouThr could be 0.5 or 0.75. np.where does not work for list. np.where([0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75] == 0.5) will return []. iou_thrs should be a numpy array.

I modified this function in coco.py, it worked.
```
def evaluate_det_segm(self,
                      results,
                      result_files,
                      coco_gt,
                      metrics,
                      logger=None,
                      classwise=False,
                      proposal_nums=(100, 300, 1000),
                      iou_thrs=None,
                      metric_items=None):
    """"""Instance segmentation and object detection evaluation in COCO
    protocol.

    Args:
        results (list[list | tuple | dict]): Testing results of the
            dataset.
        result_files (dict[str, str]): a dict contains json file path.
        coco_gt (COCO): COCO API object with ground truth annotation.
        metric (str | list[str]): Metrics to be evaluated. Options are
            'bbox', 'segm', 'proposal', 'proposal_fast'.
        logger (logging.Logger | str | None): Logger used for printing
            related information during evaluation. Default: None.
        classwise (bool): Whether to evaluating the AP for each class.
        proposal_nums (Sequence[int]): Proposal number used for evaluating
            recalls, such as recall@100, recall@1000.
            Default: (100, 300, 1000).
        iou_thrs (Sequence[float], optional): IoU threshold used for
            evaluating recalls/mAPs. If set to a list, the average of all
            IoUs will also be computed. If not specified, [0.50, 0.55,
            0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95] will be used.
            Default: None.
        metric_items (list[str] | str, optional): Metric items that will
            be returned. If not specified, ``['AR@100', 'AR@300',
            'AR@1000', 'AR_s@1000', 'AR_m@1000', 'AR_l@1000' ]`` will be
            used when ``metric=='proposal'``, ``['mAP', 'mAP_50', 'mAP_75',
            'mAP_s', 'mAP_m', 'mAP_l']`` will be used when
            ``metric=='bbox' or metric=='segm'``.

    Returns:
        dict[str, float]: COCO style evaluation metric.
    """"""
    if iou_thrs is None:
        iou_thrs = np.linspace(
            .5, 0.95, int(np.round((0.95 - .5) / .05)) + 1, endpoint=True)
    else:
        iou_thrs = np.array(iou_thrs)
```

### Environment

Ubuntu.

### Additional information

_No response_"
How can I get the forward function of pretrain_RPN?,open-mmlab/mmdetection,2022-09-20 03:22:19,2,community discussion,8836,1378761801,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 💬 Describe the reimplementation questions

Currently I am trying to use pretrain_RPN from mmdet to build my own model. However, what I need is not only the output of RPN, I also need image feature from ResNet( in RPN). So, I decide to decomposite the RPN model. I found it contains 3 modules, which is Resnet , FPN and RPN_head, repectively. I create 3 modules and assign them as ResNet, FPN, RPN_head. The problems is the output is different when I using pretrian_RPN model in a different ways.
> when I use what I learned from [Docs](https://mmdetection.readthedocs.io/en/latest/1_exist_data_model.html), I can get 1000 proposal coordinates and reliability rate.
> when I make a image go through 3 modules in sequence, I only get 2 tensor, their channel dimension is 12 and 3.

So, here is my question. How can I get 1000 proposal coordinates by using the second way( make images go through 3 module in squence)? I doubt there is some effort in the source code which can transfer the 2 tensor to proposal coordinates, but I can't found it. Other than that, what do channel dimension 12 and 3 means? How can I calculate 12 and 3?

### Environment

Python 3.8.13
mmdet 2.25.2
torch                   1.12.1
torch-summary           1.4.5
torchvision             0.13.1

### Expected results

_No response_

### Additional information

_No response_"
"mask_rcnn_r50_fpn_2x_coco ,Training error , IndexError: list index out of range",open-mmlab/mmdetection,2022-09-19 14:46:02,1,,8834,1378078703,"```
(open-mmlab) PS E:\MM\mmdetection> python tools/train.py  configs\mask_rcnn\mask_rcnn_r50_fpn_2x_coco.py
D:\anaconda308\envs\open-mmlab\lib\site-packages\torchvision\io\image.py:13: UserWarning: Failed to load image Python extension:
  warn(f""Failed to load image Python extension: {e}"")
D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\utils\setup_env.py:39: UserWarning: Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting OMP_NUM_THREADS environment variable for each process '
D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\utils\setup_env.py:49: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
2022-09-19 22:43:01,043 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: win32
Python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]
CUDA available: True
GPU 0: GeForce RTX 3060
CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
NVCC: Cuda compilation tools, release 11.2, V11.2.152
MSVC: 用于 x64 的 Microsoft (R) C/C++ 优化编译器 19.29.30146 版
GCC: n/a
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - C++ Version: 199711
  - MSVC 192829337
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/cb/pytorch_1000000000000/work/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF,

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: MSVC 192930140
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1+f8bbba2
------------------------------------------------------------

2022-09-19 22:43:01,653 - mmdet - INFO - Distributed training: False
2022-09-19 22:43:02,315 - mmdet - INFO - Config:
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=1,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=1,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = 'I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/'
classes = ('Xia', )
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        classes=('Xia', ),
        ann_file=
        'I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/Annotation\Train.json',
        img_prefix='I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/Train/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='CocoDataset',
        classes=('Xia', ),
        ann_file=
        'I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/Annotation/Val.json',
        img_prefix='I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/Val/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        classes=('Xia', ),
        ann_file=
        'I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/Annotation/Test.json',
        img_prefix='I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/Test/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[16, 22])
runner = dict(type='EpochBasedRunner', max_epochs=24)
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 2)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
work_dir = './work_dirs\mask_rcnn_r50_fpn_2x_coco'
auto_resume = False
gpu_ids = [0]

2022-09-19 22:43:02,315 - mmdet - INFO - Set random seed to 2097934672, deterministic: False
2022-09-19 22:43:02,535 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}
2022-09-19 22:43:02,535 - mmcv - INFO - load model from: torchvision://resnet50
2022-09-19 22:43:02,535 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50
2022-09-19 22:43:02,613 - mmcv - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

2022-09-19 22:43:02,644 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2022-09-19 22:43:02,659 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2022-09-19 22:43:02,659 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
2022-09-19 22:43:02,961 - mmdet - INFO - Automatic scaling of learning rate (LR) has been disabled.
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
2022-09-19 22:43:02,976 - mmdet - INFO - Start running, host: Intsoft@DESKTOP-6A8UMDH, work_dir: E:\MM\mmdetection\work_dirs\mask_rcnn_r50_fpn_2x_coco
2022-09-19 22:43:02,976 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook
(NORMAL      ) CheckpointHook
(LOW         ) EvalHook
(VERY_LOW    ) TextLoggerHook
 --------------------
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook
(NORMAL      ) NumClassCheckHook
(LOW         ) IterTimerHook
(LOW         ) EvalHook
(VERY_LOW    ) TextLoggerHook
 --------------------
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook
(LOW         ) IterTimerHook
(LOW         ) EvalHook
 --------------------
after_train_iter:
(ABOVE_NORMAL) OptimizerHook
(NORMAL      ) CheckpointHook
(LOW         ) IterTimerHook
(LOW         ) EvalHook
(VERY_LOW    ) TextLoggerHook
 --------------------
after_train_epoch:
(NORMAL      ) CheckpointHook
(LOW         ) EvalHook
(VERY_LOW    ) TextLoggerHook
 --------------------
before_val_epoch:
(NORMAL      ) NumClassCheckHook
(LOW         ) IterTimerHook
(VERY_LOW    ) TextLoggerHook
 --------------------
before_val_iter:
(LOW         ) IterTimerHook
 --------------------
after_val_iter:
(LOW         ) IterTimerHook
 --------------------
after_val_epoch:
(VERY_LOW    ) TextLoggerHook
 --------------------
after_run:
(VERY_LOW    ) TextLoggerHook
 --------------------
2022-09-19 22:43:02,976 - mmdet - INFO - workflow: [('train', 2)], max: 24 epochs
2022-09-19 22:43:02,976 - mmdet - INFO - Checkpoints will be saved to E:\MM\mmdetection\work_dirs\mask_rcnn_r50_fpn_2x_coco by HardDiskBackend.
D:\anaconda308\envs\open-mmlab\lib\site-packages\torchvision\io\image.py:13: UserWarning: Failed to load image Python extension:
  warn(f""Failed to load image Python extension: {e}"")
D:\anaconda308\envs\open-mmlab\lib\site-packages\torchvision\io\image.py:13: UserWarning: Failed to load image Python extension:
  warn(f""Failed to load image Python extension: {e}"")
Traceback (most recent call last):
  File ""tools/train.py"", line 242, in <module>
    main()
  File ""tools/train.py"", line 238, in main
    meta=meta)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\apis\train.py"", line 244, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmcv\runner\epoch_based_runner.py"", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmcv\runner\epoch_based_runner.py"", line 49, in train
    for i, data_batch in enumerate(self.data_loader):
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\utils\data\dataloader.py"", line 681, in __next__
    data = self._next_data()
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\utils\data\dataloader.py"", line 1376, in _next_data
    return self._process_data(data)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\utils\data\dataloader.py"", line 1402, in _process_data
    data.reraise()
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\_utils.py"", line 461, in reraise
    raise exception
IndexError: Caught IndexError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\utils\data\_utils\worker.py"", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\custom.py"", line 218, in __getitem__
    data = self.prepare_train_img(idx)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\custom.py"", line 241, in prepare_train_img
    return self.pipeline(results)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\pipelines\compose.py"", line 41, in __call__
    data = t(data)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\pipelines\loading.py"", line 398, in __call__
    results = self._load_masks(results)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\pipelines\loading.py"", line 350, in _load_masks
    [self._poly2mask(mask, h, w) for mask in gt_masks], h, w)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\pipelines\loading.py"", line 350, in <listcomp>
    [self._poly2mask(mask, h, w) for mask in gt_masks], h, w)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\datasets\pipelines\loading.py"", line 306, in _poly2mask
    rles = maskUtils.frPyObjects(mask_ann, img_h, img_w)
  File ""pycocotools/_mask.pyx"", line 293, in pycocotools._mask.frPyObjects
IndexError: list index out of range


```


```

# dataset settings
dataset_type = 'CocoDataset'
data_root = 'I:/2022/XIA/01_FG/COCOtrain_OK_zq/MsCOCO_dataset/'

# 
classes = ('Xia',)
# 
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        classes=classes,
        # num_classes=1,
        ann_file=data_root + 'Annotation\Train.json',
        img_prefix=data_root + 'Train/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        classes=classes,
        # num_classes=1,
        ann_file=data_root + 'Annotation/Val.json',
        img_prefix=data_root + 'Val/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        classes=classes,
        # num_classes=1,
        ann_file=data_root + 'Annotation/Test.json',
        img_prefix=data_root + 'Test/',
        pipeline=test_pipeline))
evaluation = dict(metric=['bbox', 'segm'])

```"
Inference on M1 processor with device = MPS : RuntimeError: Placeholder storage has not been allocated on MPS device,open-mmlab/mmdetection,2022-09-19 12:41:56,7,,8832,1377897211,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

Hi, I am trying to run mmdetection inference with MPS acceleration on my macbook m1. It fail with this error :


```
    this_res = inference_detector(model, img)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/apis/inference.py"", line 151, in inference_detector
    results = model(return_loss=False, rescale=True, **data)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/Users/aya/git/mmcv/mmcv/runner/fp16_utils.py"", line 116, in new_func
    return old_func(*args, **kwargs)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/models/detectors/base.py"", line 174, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/models/detectors/base.py"", line 147, in forward_test
    return self.simple_test(imgs[0], img_metas[0], **kwargs)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/models/detectors/two_stage.py"", line 177, in simple_test
    x = self.extract_feat(img)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/models/detectors/two_stage.py"", line 67, in extract_feat
    x = self.backbone(img)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/models/backbones/detectors_resnet.py"", line 331, in forward
    outs = list(super(DetectoRS_ResNet, self).forward(x))
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/mmdet/models/backbones/resnet.py"", line 637, in forward
    x = self.norm1(x)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py"", line 171, in forward
    return F.batch_norm(
  File ""/Users/aya/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/functional.py"", line 2446, in batch_norm
    return torch.batch_norm(
RuntimeError: Placeholder storage has not been allocated on MPS device!

```



The same code with device='CPU' work fine , but is very slow (7 sec / image) and on another computer with an nvidia gpu and device='cuda:0' also works fine (0.5 sec / image). 

 
```
device = 'mps'
config = mmcv.Config.fromfile(configPath)
config.model.pretrained = None
model = build_detector(config.model)
checkpoint = load_checkpoint(model, checkpointPath, map_location=device)
model.CLASSES = checkpoint['meta']['CLASSES']
model.cfg = config
model.to(device)
model.eval()
...
        for img in dataset:
            this_res = inference_detector(model, img)
```

Any idea ? 

thanks


### Environment

Macbook Pro with M1 processor
pytorch-nightly = pytorch-1.13.0.dev20
MMCV = V1.6.1
MMDetection = V2.25.2

### Additional information

_No response_"
Running out of memory while training on CPU,open-mmlab/mmdetection,2022-09-19 11:25:28,1,,8831,1377803672,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

Hello,
I am trying to train an object detection model on CPU server and the memory that is taking up for the training process is going over more than 24GB (max limit set inside the docker container) and the training gets failed after 10 to 20 epochs. What is the reason for this issue to happen? Is there any way to capture the memory that it would be required for the whole training process, with respect to model and dataset, prior?

### Environment

2022-09-19 10:09:04,166 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.7 (default, May  7 2020, 21:25:33) [GCC 7.3.0]
CUDA available: False
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.6.0
MMCV: 1.4.4
MMCV Compiler: GCC 7.4
MMCV CUDA Compiler: 10.1
MMDetection: 2.20.0+
------------------------------------------------------------

### Additional information

_No response_"
Visualise and Fine-tuning model ATSS,open-mmlab/mmdetection,2022-09-19 06:27:10,1,feature request,8829,1377464287,"### What is the problem this feature will solve?

fine-tuning the custom dataset and visualise the model network

### What is the feature you are proposing to solve the problem?

I need to freeze everything except the head of the model, and since currently the pre-trained model only trained 12 epoch for coco dataset, I also need either pre-trained model with 300 epoch on coco or pascal voc or do it by myself :(

### What alternatives have you considered?

_No response_"
 fatal: not a git repository when saving checkpoints,open-mmlab/mmdetection,2022-09-16 14:57:49,1,,8819,1376070392,"### Prerequisite

- [X] I have searched [the existing and past issues](https://github.com/open-mmlab/mmdetection/issues) but cannot get the expected help.
- [X] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [X] The bug has not been fixed in the [latest version](https://github.com/open-mmlab/mmdetection).

### 🐞 Describe the bug

Whenever a checkpoint is saved, I get additional git messages:

```
09/16 14:47:35 - mmengine - INFO - Saving checkpoint at 9 epochs
fatal: not a git repository (or any parent up to mount point /opt/ml)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
```

I'm not sure why these messages appear or how to remove them.

### Environment

```
git clone https://github.com/open-mmlab/mmdetection.git -b 3.x
cd mmdetection
python mmdet/utils/collect_env.py
```
Appears to be bug in collect env:
```
root@3deae1c5c230:~/mmdetection# python -V
Python 3.8.13
root@3deae1c5c230:~/mmdetection# python mmdet/utils/collect_env.py
Traceback (most recent call last):
  File ""mmdet/utils/collect_env.py"", line 2, in <module>
    from mmengine.utils import get_git_hash
  File ""/opt/conda/lib/python3.8/site-packages/mmengine/__init__.py"", line 3, in <module>
    from .config import *
  File ""/opt/conda/lib/python3.8/site-packages/mmengine/config/__init__.py"", line 2, in <module>
    from .config import Config, ConfigDict, DictAction
  File ""/opt/conda/lib/python3.8/site-packages/mmengine/config/config.py"", line 15, in <module>
    from typing import Any, Optional, Sequence, Tuple, Union
  File ""/root/mmdetection/mmdet/utils/typing.py"", line 3, in <module>
    from typing import List, Optional, Sequence, Tuple, Union
ImportError: cannot import name 'List' from partially initialized module 'typing' (most likely due to a circular import) (/root/mmdetection/mmdet/utils/typing.py)
```

### Additional information

I changed the logger to:

```
log_config = dict(  # config to register logger hook
    interval=50,  # Interval to print the log
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type='TensorboardLoggerHook', log_dir='/opt/ml/checkpoints')
    ]
)
```

and set `work-dir` to `/opt/ml/checkpoints`"
COCO per Category evaluation problem,open-mmlab/mmdetection,2022-09-16 11:28:14,1,,8816,1375815561,"I am currently training a custom dataset and everything works fine (training, config setup ...), but when i evaluate the results per category only 3 of 5 classes are achieving an AP above zero... :

`Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.296
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.565
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.272
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.144
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.298
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.345
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.345
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.345
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.209
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.347

+----------+-------+-------------+-------+----------+-------+
| category | AP    | category    | AP    | category | AP    |
+----------+-------+-------------+-------+----------+-------+
| PO1      | 0.468 | MobilePhone | 0.421 | PO2      | 0.000 |
| Laptop   | 0.000 | Tablet      | 0.592 | None     | None  |
+----------+-------+-------------+-------+----------+-------+
OrderedDict([('bbox_mAP', 0.296), ('bbox_mAP_50', 0.565), ('bbox_mAP_75', 0.272), ('bbox_mAP_s', 0.0), ('bbox_mAP_m', 0.144), ('bbox_mAP_l', 0.298), ('bbox_mAP_copypaste', '0.296 0.565 0.272 0.000 0.144 0.298')])`

I double checked my dataset (basic COCO format) and couldnt find any issues (negative values or something) so i dont know why those two classes (PO2 and Laptop) get an AP of zero... It also stays the same for multiple Epochs and it seems like only the other three classes get trained properly.

Can anyone help w this issue?
"
Dist training failed due to FileNotFoundError,open-mmlab/mmdetection,2022-09-16 03:00:02,3,,8812,1375333205,"Hi, @BIGWangYuDong @ZwwWayne ,  Thanks for your help and I appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
I use two machines to train deformable-detr. The training is smooth but fail to eval, which means I have to re-train model after one single epoch.

**Reproduction**

1. What command or script did you run?

* For node 1
```bash
NNODES=2 NODE_RANK=0 PORT=23456 MASTER_ADDR=xxxx bash tools/dist_train.sh configs/coco/deformable_detr_r50_16x2_50e_coco.py 8
```

* For node 2
```bash
NNODES=2 NODE_RANK=1 PORT=23456 MASTER_ADDR=xxxx bash tools/dist_train.sh configs/coco/deformable_detr_r50_16x2_50e_coco.py 8
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
No.Yes.

3. What dataset did you use?
COCO2017

**Error traceback**
```bash
raceback (most recent call last):
  File ""tools/train.py"", line 242, in <module>
    main()
  File ""tools/train.py"", line 238, in main
    meta=meta)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmdet/apis/train.py"", line 244, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 58, in train
    self.call_hook('after_train_epoch')
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/runner/base_runner.py"", line 317, in call_hook
    getattr(hook, fn_name)(self)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py"", line 271, in after_train_epoch
    self._do_evaluate(runner)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmdet/core/evaluation/eval_hooks.py"", line 130, in _do_evaluate
    gpu_collect=self.gpu_collect)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmdet/apis/test.py"", line 132, in multi_gpu_test
    results = collect_results_cpu(results, len(dataset), tmpdir)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmdet/apis/test.py"", line 167, in collect_results_cpu
    part_list.append(mmcv.load(part_file))
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/fileio/io.py"", line 67, in load
    with BytesIO(file_client.get(file)) as f:
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/fileio/file_client.py"", line 1014, in get
    return self.client.get(filepath)
  File ""/home/tiger/.local/lib/python3.7/site-packages/mmcv/fileio/file_client.py"", line 535, in get
    with open(filepath, 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/tiger/code/mmdet/work_dirs/deformable_detr_r50_16x2_50e_coco/.eval_hook/part_8.pkl'
```

**Environment**
```
sys.platform: linux
Python: 3.7.3 (default, Jan 22 2021, 20:04:44) [GCC 8.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: A100-SXM-80GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: x86_64-linux-gnu-gcc (Debian 8.3.0-6) 8.3.0
PyTorch: 1.10.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON,

TorchVision: 0.11.1+cu113
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1+
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
Using ConcatDataset in validation set in new mmdet v3,open-mmlab/mmdetection,2022-09-13 15:55:53,4,v-3.x,8785,1371675262,"For the latest release mmdet v3, if `ConcatDataset` is used in validation dataset, how can `val_evaluator` be set in the config? 

I've tried not providing ann_file to `val_evaluator` as well as giving a list of `CocoMetric` dicts, each corresponding to the ann_file in `ConcatDataset`-- both does not work. 

Please advise. Thank you!  "
"Reimplemented an NMS under mmdetection/mmdet/core/post_processing, and did not use the operation of mmcv.ops.nms. When called in the configuration file, it showed that it was not found.",open-mmlab/mmdetection,2022-09-13 07:30:21,3,reimplementation,8781,1370990481,"Reimplemented an NMS under mmdetection/mmdet/core/post_processing, and did not use the operation of mmcv.ops.nms. When called in the configuration file, it showed that it was not found."
"如何实现 重叠滑窗预测,How to realize overlapping sliding window prediction,Add Sahi",open-mmlab/mmdetection,2022-09-13 06:40:21,3,feature request,8780,1370935432,"**Describe the feature**

**Motivation**

比如9000*9000
不能直接滑窗裁剪

需要重叠裁剪 有重复部分
原图预测 自动裁剪

也可以批量对文件夹图片进行裁剪

建议增加 Sahi 重叠滑窗预测功能

For example, 9000 * 9000

You cannot directly slide the window to cut

If overlapping clipping is required, there will be duplicate parts

Automatic clipping of original image prediction

can also crop folder pictures in batches

It is suggested to add Sahi overlapping sliding window prediction function



**Related resources**
`https://github.com/obss/sahi`

**Additional context**
Add any other context or screenshots about the feature request here.
If you would like to implement the feature and create a PR, please leave a comment here and that would be much appreciated.
"
The testing results of the whole dataset is empty. OrderedDict()   when running test.py for convnext,open-mmlab/mmdetection,2022-09-11 00:49:14,11,,8766,1368767027,"Dear Open-mmlab,

I had a problem while testing convnext on my GTX1660ti. I was running this in command
python tools/test.py configs/convnext/cascade_mask_rcnn_convnext-s_p4_w7_fpn_giou_4conv1f_fp16_ms-crop_3x_coco.py checkpoints/cascade_mask_rcnn_convnext-s_p4_w7_fpn_giou_4conv1f_fp16_ms-crop_3x_coco_20220510_201004-3d24f5a4.pth --eval bbox

The output is like:
load checkpoint from local path: checkpoints/cascade_mask_rcnn_convnext-s_p4_w7_fpn_giou_4conv1f_fp16_ms-crop_3x_coco_20220510_201004-3d24f5a4.pth
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 5000/5000, 1.0 task/s, elapsed: 4996s, ETA: 0s
Evaluating bbox...
Loading and preparing results...
The testing results of the whole dataset is empty.
OrderedDict()

I ran several other models and they were OK but convnext does not work.
Can somebody help me with this problem? Any information could help.

All the best
Neil
"
using jetson nano kit ,open-mmlab/mmdetection,2022-09-10 16:54:30,1,,8765,1368675108,"Hello , 

Can i use jetson nano kit for training models in your repository with my custom dataset? 

would the specification of jetson nano kit fine to train models design according to distillation learning strategies and incremental learning?


thnaks"
Introducing mmselfsup.core in mmdet leads to unexpected results.,open-mmlab/mmdetection,2022-09-10 00:16:07,8,reimplementation,8762,1368427630,"Hi @ZwwWayne @BIGWangYuDong @RangiLyu @jbwang1997 @hhaAndroid @chhluo , happy mid-Autumn Festival (🐶)~

This issue also open in [mmselfsup](https://github.com/open-mmlab/mmselfsup/issues/479)

Thanks for reporting the unexpected results and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. The unexpected results still exist in the latest version.

**Describe the Issue**
I tried to combine mmselfsup and mmdet to do something, however, when I added:
```
custom_imports = dict(imports=['mmselfsup.core'])
```

I found that DETR's training fails to converge, I also added this line into other methods like fcos/retinanet/mask-rcnn. In the case of fixed random seeds, their loss values and training results have huge changes. I'm clueless about this and what's causing the huge discrepancy, hoping to get some help from the community.

**Reproduction**

1. What command, code, or script did you run?

Add this line into any original mmdet config, you can find the loss and result changed a lot, even with fixed random seed.
```bash
custom_imports = dict(imports=['mmselfsup.core'])
```

2. Did you make any modifications on the code? Did you understand what you have modified?
Yes. Yes.

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!"
Model return results slightly differ each inference,open-mmlab/mmdetection,2022-09-09 03:17:59,2,,8759,1367228391,"Hello. I use yolact model to infer on one image and notice slightly different results each time I run inference. However, I can not find any random code in test_pipeline or test_cfg. How do I stabilize the results for every inference, thanks in advance
```
1st infer:
... array([[2.8686313e+02, 1.2799496e+03, 6.9883295e+02, 1.7178850e+03,
        9.8540848e-01]], dtype=float32)], [[], [], [], [], [array([[0, 0, 0, ..., 0, 0, 0]
2nd infer:
... array([[2.8685156e+02, 1.2799402e+03, 6.9881934e+02, 1.7178893e+03,
        9.8541760e-01]], dtype=float32)], [[], [], [], [], [array([[0, 0, 0, ..., 0, 0, 0],
```"
onnx tensorrt model infer results are different in some special shapes.,open-mmlab/mmdetection,2022-09-07 02:41:27,1,,8748,1364011957,"Hi,

i found two weird things in trt model inferencing process. Any one knows why?

1. ""dict(type='Pad', size_divisor=32)"", is not work in trt model, it can lead different results(eg. 10 pixels shifting in det box-x.) between onnx and trt. but when i used ""dict(type='Pad', size_divisor=128),"" the result is same with onnx result, the shifting is gone.

2. when i trained a model only 1 fg class num, i found 
""rcnn=dict(
            score_thr=0.3,"" ....)
is not working, i debuged and found some zeros det boxes, their scores is near to 0.5. 

eg.
tensor([[[3.2900e+02, 1.1648e+02, 7.1000e+02, 1.5277e+02, 9.9482e-01],
         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0489e-01],   ...    # this score is weird. 
"
pip install -v -e . Warning!!!,open-mmlab/mmdetection,2022-09-07 02:00:06,1,enhancement,8747,1363989026,"when  i install mmdetection,when i run pip install -v -e .

Warning coming:

warning: no files found matching 'mmdet\VERSION'
  warning: no files found matching 'mmdet\.mim\model-index.yml'
  warning: no files found matching 'mmdet\.mim\demo\*\*'
  warning: no files found matching '*.py' under directory 'mmdet\.mim\configs'
  warning: no files found matching '*.yml' under directory 'mmdet\.mim\configs'
  warning: no files found matching '*.sh' under directory 'mmdet\.mim\tools'
  warning: no files found matching '*.py' under directory 'mmdet\.mim\tools'


  Running setup.py develop for mmdet
    Running command python setup.py develop
    running develop
    running egg_info
    creating mmdet.egg-info
    writing mmdet.egg-info\PKG-INFO
    writing dependency_links to mmdet.egg-info\dependency_links.txt
    writing requirements to mmdet.egg-info\requires.txt
    writing top-level names to mmdet.egg-info\top_level.txt
    writing manifest file 'mmdet.egg-info\SOURCES.txt'
    reading manifest file 'mmdet.egg-info\SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching 'mmdet\VERSION'
    warning: no files found matching 'mmdet\.mim\demo\*\*'
"
Getting sample-wise loss,open-mmlab/mmdetection,2022-09-06 07:26:52,1,community help wanted,8742,1362828311,"Hi, I'd like to implement a hook that saves hard training samples, choosing them by their loss value. Training my model in batches returns either a cumulative loss (`mean`/`sum`) or, when using `reduction='none'` raw loss tensors for my 5 feature maps, flattened into `(n_batches*n_anchors,1)` shape. To be clear, i'm getting these from a forward pass calling `losses = runner.model(**runner.data_batch)`

My questions is how to unpack these flattened tensors, to get the sample-wise losses? (e.g. tensor of shape `(n_batches, n_anchors)` or `(n_batches, n_anchors_row, n_anchors_column)`, or similar).
"
'gbk' codec can't decode byte 0x99 in position 4742: illegal multibyte sequence,open-mmlab/mmdetection,2022-09-04 07:55:15,1,windows,8732,1361073385,"`python tools/train.py configs/yolox/yolox_s_8x8_300e_coco.py`
```

D:\anaconda308\envs\open-mmlab\lib\site-packages\torchvision\io\image.py:13: UserWarning: Failed to load image Python exte
  warn(f""Failed to load image Python extension: {e}"")
Traceback (most recent call last):
  File ""tools/train.py"", line 242, in <module>
    main()
  File ""tools/train.py"", line 114, in main
    cfg = replace_cfg_vals(cfg)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmdet\utils\replace_cfg_vals.py"", line 65, in replace_cfg_vals
    replace_value(ori_cfg._cfg_dict), filename=ori_cfg.filename)
  File ""D:\anaconda308\envs\open-mmlab\lib\site-packages\mmcv\utils\config.py"", line 405, in __init__
    text = f.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0x99 in position 4742: illegal multibyte sequence
```




```
_base_ = ['../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py']

img_scale = (640, 640)  # height, width

# model settings
model = dict(
    type='YOLOX',
    input_size=img_scale,
    random_size_range=(15, 25),
    random_size_interval=10,
    backbone=dict(type='CSPDarknet', deepen_factor=0.33, widen_factor=0.5),
    neck=dict(
        type='YOLOXPAFPN',
        in_channels=[128, 256, 512],
        out_channels=128,
        num_csp_blocks=1),
    bbox_head=dict(
        type='YOLOXHead', num_classes=80, in_channels=128, feat_channels=128),
    train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),
    # In order to align the source code, the threshold of the val phase is
    # 0.01, and the threshold of the test phase is 0.001.
    test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))

# dataset settings
data_root = 'E:/MM/coco/'
dataset_type = 'CocoDataset'
classes = ['CP']

train_pipeline = [
    dict(type='Mosaic', img_scale=img_scale, pad_val=114.0),
    dict(
        type='RandomAffine',
        scaling_ratio_range=(0.1, 2),
        border=(-img_scale[0] // 2, -img_scale[1] // 2)),
    dict(
        type='MixUp',
        img_scale=img_scale,
        ratio_range=(0.8, 1.6),
        pad_val=114.0),
    dict(type='YOLOXHSVRandomAug'),
    dict(type='RandomFlip', flip_ratio=0.5),
    # According to the official implementation, multi-scale
    # training is not considered here but in the
    # 'mmdet/models/detectors/yolox.py'.
    dict(type='Resize', img_scale=img_scale, keep_ratio=True),
    dict(
        type='Pad',
        pad_to_square=True,
        # If the image is three-channel, the pad value needs
        # to be set separately for each channel.
        pad_val=dict(img=(114.0, 114.0, 114.0))),
    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]

train_dataset = dict(
    type='MultiImageMixDataset',
    dataset=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_train2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True)
        ],
        filter_empty_gt=False,
    ),
    pipeline=train_pipeline)

test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=img_scale,
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Pad',
                pad_to_square=True,
                pad_val=dict(img=(114.0, 114.0, 114.0))),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img'])
        ])
]

data = dict(
    samples_per_gpu=8,
    workers_per_gpu=4,
    persistent_workers=True,
    train=train_dataset,
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline))

# optimizer
# default 8 gpu
optimizer = dict(
    type='SGD',
    lr=0.01,
    momentum=0.9,
    weight_decay=5e-4,
    nesterov=True,
    paramwise_cfg=dict(norm_decay_mult=0., bias_decay_mult=0.))
optimizer_config = dict(grad_clip=None)

max_epochs = 300
num_last_epochs = 15
resume_from = None
interval = 10

# learning policy
lr_config = dict(
    _delete_=True,
    policy='YOLOX',
    warmup='exp',
    by_epoch=False,
    warmup_by_epoch=True,
    warmup_ratio=1,
    warmup_iters=5,  # 5 epoch
    num_last_epochs=num_last_epochs,
    min_lr_ratio=0.05)

runner = dict(type='EpochBasedRunner', max_epochs=max_epochs)

custom_hooks = [
    dict(
        type='YOLOXModeSwitchHook',
        num_last_epochs=num_last_epochs,
        priority=48),
    dict(
        type='SyncNormHook',
        num_last_epochs=num_last_epochs,
        interval=interval,
        priority=48),
    dict(
        type='ExpMomentumEMAHook',
        resume_from=resume_from,
        momentum=0.0001,
        priority=49)
]
checkpoint_config = dict(interval=interval)
evaluation = dict(
    save_best='auto',
    # The evaluation interval is 'interval' when running epoch is
    # less than ‘max_epochs - num_last_epochs’.
    # The evaluation interval is 1 when running epoch is greater than
    # or equal to ‘max_epochs - num_last_epochs’.
    interval=interval,
    dynamic_intervals=[(max_epochs - num_last_epochs, 1)],
    metric='bbox')
log_config = dict(interval=50)

# NOTE: `auto_scale_lr` is for automatically scaling LR,
# USER SHOULD NOT CHANGE ITS VALUES.
# base_batch_size = (8 GPUs) x (8 samples per GPU)
auto_scale_lr = dict(base_batch_size=64)

```


`python tools/train.py configs/yolox/yolox_tiny_8x8_300e_coco.py`
Pass, no problem"
model.show_result  label modify,open-mmlab/mmdetection,2022-09-02 02:40:08,1,,8705,1359634402,"![微信截图_20220902103801](https://user-images.githubusercontent.com/6490927/188047320-c1d2b64d-ff27-4401-a91b-e316ee76fd59.png)

`model.show_result(img, result, score_thr=0.2,  show=True)
`
I don't know how to change it

Because the box is inside, it will block my goal

I need to move outside"
GPU inference doesn't work when importing open3d,open-mmlab/mmdetection,2022-09-01 16:58:08,1,,8702,1359144360,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
The GPU inference doesn't work when importing the open3d module. There is no error if the inference is done with CPU. 
There are no problems if the open3d module is not imported.

**Reproduction**

1. What command or script did you run?

```none
import open3d as o3d

from mmdet.apis import (async_inference_detector, inference_detector,  init_detector, show_result_pyplot)
    model = init_detector('/xxxxx.py',
                        'xxxxx.pth', device='cuda:0')
result = inference_detector(model, im)
```

2. Did you make any modifications on the code or config? Did you understand what you have modified? I did not modify anything
3. What dataset did you use?

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here. 
sys.platform: linux
Python: 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 2060
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: x86_64-linux-gnu-gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.10.2+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.3+cu113
OpenCV: 4.5.5
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1+9b01b45


4. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch: pip

**Error traceback**
If applicable, paste the error trackback here.

```none
A placeholder for trackback.

UserWarning: ""ImageToTensor"" pipeline is replaced by ""DefaultFormatBundle"" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.
  warnings.warn(
Fatal Python error: Segmentation fault

Current thread 0x00007f0e5acc6740 (most recent call first):
  File ""/home/appuser/.local/lib/python3.8/site-packages/torch/nn/functional.py"", line 1848 in linear
  File ""/home/appuser/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py"", line 103 in forward
  File ""/home/appuser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1102 in _call_impl
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/roi_heads/bbox_heads/convfc_bbox_head.py"", line 172 in forward
  File ""/home/appuser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1102 in _call_impl
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/roi_heads/standard_roi_head.py"", line 125 in _bbox_forward
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/roi_heads/test_mixins.py"", line 89 in simple_test_bboxes
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/roi_heads/standard_roi_head.py"", line 253 in simple_test
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/detectors/two_stage.py"", line 183 in simple_test
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/detectors/base.py"", line 147 in forward_test
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/models/detectors/base.py"", line 174 in forward
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py"", line 116 in new_func
  File ""/home/appuser/.local/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1102 in _call_impl
  File ""/home/appuser/.local/lib/python3.8/site-packages/mmdet/apis/inference.py"", line 151 in inference_detector
  File ""test_new_net.py"", line 215 in <module>
Segmentation fault (core dumped)

```
"
AssertionError: loss log variables are different across GPUs!,open-mmlab/mmdetection,2022-08-31 13:28:25,4,,8687,1357341224,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

**Reproduction**

1. What command or script did you run?

```none
tools/dist_train.sh configs/faster_rcnn/faster_rcnn_r101_fpn_2x_coco.py 2
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
```none
_base_ = './faster_rcnn_r50_fpn_2x_coco.py'
model = dict(
    backbone=dict(
        depth=101,
        init_cfg=dict(type='Pretrained',
                      checkpoint='torchvision://resnet101')))

log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type='MMDetWandbHook',
             init_kwargs={
                'project': 'mmdetection',
                'group': 'faster-rcnn-r101-fpn-2x-coco'
             },
             interval=50,
             #log_checkpoint=True,
             #log_checkpoint_metadata=True,
             num_eval_images=94)
        ])
```
3. What dataset did you use?
```none
Custom datasets
```
**Environment**
```none
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.1, V11.1.105
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.8.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0
OpenCV: 4.5.5
MMCV: 1.5.0
MMCV Compiler: GCC 7.5
MMCV CUDA Compiler: 11.1
MMDetection: 2.25.0+
```


**Error traceback**
If applicable, paste the error trackback here.

```none
AssertionError: loss log variables are different across GPUs!
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
TypeError: __init__() missing 2 required positional arguments: 'url' and 'message',open-mmlab/mmdetection,2022-08-31 08:56:53,1,,8680,1357026242,"
![1661936026733](https://user-images.githubusercontent.com/30501264/187639547-0b82d388-151d-4186-b990-0cad470eb3f5.png)


The above error is suddenly reported during the model training，Help!!
how to solve this proplem!
"
Problems with DeformableDetrTransformer,open-mmlab/mmdetection,2022-08-31 08:40:31,1,,8679,1357003854,"Why are the reference_points dimensions of the encoder and decoder in DeformableDetrTransformer different? The reference_points in the encoder are four-dimensional. The reference_points in the decoder are three-dimensional, but both the encoder and the decoder call Multi Scale Attention. The reference_points in the Multi Scale Attention are required to be four-dimensional, so the Multi Scale Attention of the decoder reports an error. Have you encountered this situation?
  
The error is as follows：
 sampling_locations = reference_points[:, :, None, :, None, :] \
IndexError: too many indices for tensor of dimension 3
：：：That is to say, the reference_points in the decoder are three-dimensional and cannot run the following code (reference_points should be four-dimensional)"
OpenCV failed to load the ONNX model,open-mmlab/mmdetection,2022-08-31 06:10:00,2,,8675,1356836218,"OpenCV(4.6.0) D:\a\opencv-python\opencv-python\opencv\modules\dnn\src\onnx\onnx_importer.cpp:1040: error: (-2:Unspecified error) in function 'cv::dnn::dnn4_v20220524::ONNXImporter::handleNode'
> Node [TopK@ai.onnx]:(onnx_node!TopK_210) parse error: OpenCV(4.6.0) d:\a\opencv-python\opencv-python\opencv\modules\dnn\src\layer_internals.hpp:110: error: (-2:Unspecified error) Can't create layer ""onnx_node!TopK_210"" of type ""TopK"" in function 'cv::dnn::dnn4_v20220524::detail::LayerData::getLayerInstance'
![image](https://user-images.githubusercontent.com/96858577/187606273-8b0fe285-14e4-4e74-b6ac-e7545c5d29e4.png)
"
Inference time of YOLACT is too slow,open-mmlab/mmdetection,2022-08-30 10:49:02,2,,8669,1355565472,"I test 40 images using mmdetection yolact_r50_1x8_coco model, the average latency time is 199.85ms. It is weird, since I got 18ms using the official YOLACT code. What's the difference between them？"
Errors in panoptic_gt_processing for maskformer series models,open-mmlab/mmdetection,2022-08-30 08:02:44,8,,8666,1355342371,"Thanks for your error report and we appreciate it a lot.

**Describe the bug**
I face data type matching problem when I simply run a script on coco panoptic using mask2former. The log is shown as follows.
```none
Traceback (most recent call last):
  File ""train.py"", line 242, in <module>
    main()
  File ""train.py"", line 238, in main
    meta=meta)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/apis/train.py"", line 244, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmcv/runner/iter_based_runner.py"", line 138, in run
    iter_runner(iter_loaders[i], **kwargs)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmcv/runner/iter_based_runner.py"", line 62, in train
    outputs = self.model.train_step(data_batch, self.optimizer, **kwargs)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py"", line 75, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/models/detectors/base.py"", line 248, in train_step
    losses = self(**data)
  File ""/mnt/lustre/share/polaris/env/miniconda3.7/envs/pt1.5s1/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __c
all__
    result = self.forward(*input, **kwargs)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py"", line 110, in new_func
    return old_func(*args, **kwargs)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/models/detectors/base.py"", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/models/detectors/maskformer.py"", line 109, in forward_tr
ain
    gt_bboxes_ignore)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/models/dense_heads/maskformer_head.py"", line 519, in for
ward_train
    gt_semantic_seg, img_metas)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/models/dense_heads/maskformer_head.py"", line 170, in pre
process_gt    num_stuff_list, img_metas)
  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/core/utils/misc.py"", line 30, in multi_apply
    return tuple(map(list, zip(*map_results)))  File ""/mnt/lustre/guohansheng/.local.pt1.5s1/lib/python3.7/site-packages/mmdet/models/utils/panoptic_gt_processing.py"", line 61, in pre
process_panoptic_gt
    labels = torch.cat([things_labels, stuff_labels], dim=0)
RuntimeError: Expected object of scalar type long int but got scalar type int for sequence element 1.
phoenix-srun: error: SH-IDC2-172-20-21-34: task 0: Exited with exit code 1
```
**Reproduction**

1. What command or script did you run?

```none
python train.py configs/mask2former/mask2former_r50_lsj_8x2_50e_coco-panoptic.py
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?
I just changed the file path of coco dataset and the dataset is from official [website](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip). I use pip to install the mmdet and didn't change the source code at all.
4. What dataset did you use?
coco panoptic

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
```none
sys.platform: linux
Python: 3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: GeForce GTX 1080
CUDA_HOME: /mnt/lustre/share/polaris/dep/cuda-9.0-cudnn7.6.5
NVCC: Cuda compilation tools, release 9.0, V9.0.17
GCC: gcc (GCC) 5.4.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:
  - GCC 5.4
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 912ce228837d1ce28e1a61806118835de03f5751)
  - OpenMP 201307 (a.k.a. OpenMP 4.0)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 9.0
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_70,code=compute_70
  - CuDNN 7.6.5
  - Magma 2.5.0
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -used-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-eused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-eused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-e
rror=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXC
EPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.6.0
OpenCV: 4.2.0
MMCV: 1.5.2
MMCV Compiler: GCC 5.4
MMCV CUDA Compiler: 9.0
MMDetection: 2.25.1+ca11860
```
3. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
   - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Config file**
Please also check my config files as follows.
```python
dataset_type = 'CocoPanopticDataset'
data_root = '/mnt/lustre/share/mmdet-projects/data/coco/'
pano_root = '/mnt/lustre/guohansheng/downloads/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile', to_float32=True),
    dict(
        type='LoadPanopticAnnotations',
        with_bbox=True,
        with_mask=True,
        with_seg=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Resize',
        img_scale=(1024, 1024),
        ratio_range=(0.1, 2.0),
        multiscale_mode='range',
        keep_ratio=True),
    dict(
        type='RandomCrop',
        crop_size=(1024, 1024),
        crop_type='absolute',
        recompute_bbox=True,
        allow_negative_crop=True),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(1024, 1024)),
    dict(type='DefaultFormatBundle', img_to_float=True),
    dict(
        type='Collect',
        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoPanopticDataset',
        ann_file=
        '/mnt/lustre/guohansheng/downloads/annotations/panoptic_train2017.json',
        img_prefix='/mnt/lustre/share/mmdet-projects/data/coco/train2017/',
        seg_prefix=
        '/mnt/lustre/guohansheng/downloads/annotations/panoptic_train2017/',
        pipeline=[
            dict(type='LoadImageFromFile', to_float32=True),
            dict(
                type='LoadPanopticAnnotations',
                with_bbox=True,
                with_mask=True,
                with_seg=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Resize',
                img_scale=(1024, 1024),
                ratio_range=(0.1, 2.0),
                multiscale_mode='range',
                keep_ratio=True),
            dict(
                type='RandomCrop',
                crop_size=(1024, 1024),
                crop_type='absolute',
                recompute_bbox=True,
                allow_negative_crop=True),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(1024, 1024)),
            dict(type='DefaultFormatBundle', img_to_float=True),
            dict(
                type='Collect',
                keys=[
                    'img', 'gt_bboxes', 'gt_labels', 'gt_masks',
                    'gt_semantic_seg'
                ])
        ]),
    val=dict(
        type='CocoPanopticDataset',
        ann_file=
        '/mnt/lustre/guohansheng/downloads/annotations/panoptic_val2017.json',
        img_prefix='/mnt/lustre/share/mmdet-projects/data/coco/val2017/',
        seg_prefix=
        '/mnt/lustre/guohansheng/downloads/annotations/panoptic_val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        ins_ann_file=
        '/mnt/lustre/share/mmdet-projects/data/coco/annotations/instances_val2017.json'
    ),
    test=dict(
        type='CocoPanopticDataset',
        ann_file=
        '/mnt/lustre/guohansheng/downloads/annotations/panoptic_val2017.json',
        img_prefix='/mnt/lustre/share/mmdet-projects/data/coco/val2017/',
        seg_prefix=
        '/mnt/lustre/guohansheng/downloads/annotations/panoptic_val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        ins_ann_file=
        '/mnt/lustre/share/mmdet-projects/data/coco/annotations/instances_val2017.json'
    ))
evaluation = dict(
    interval=5000,
    metric=['PQ', 'bbox', 'segm'],
    dynamic_intervals=[(365001, 368750)])
checkpoint_config = dict(
    interval=5000, by_epoch=False, save_last=True, max_keep_ckpts=3)
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook', by_epoch=False),
        dict(type='TensorboardLoggerHook', by_epoch=False)
    ])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 5000)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
num_things_classes = 80
num_stuff_classes = 53
num_classes = 133
model = dict(
    type='Mask2Former',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=-1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    panoptic_head=dict(
        type='Mask2FormerHead',
        in_channels=[256, 512, 1024, 2048],
        strides=[4, 8, 16, 32],
        feat_channels=256,
        out_channels=256,
        num_things_classes=80,
        num_stuff_classes=53,
        num_queries=100,
        num_transformer_feat_level=3,
        pixel_decoder=dict(
            type='MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                type='DetrTransformerEncoder',
                num_layers=6,
                transformerlayers=dict(
                    type='BaseTransformerLayer',
                    attn_cfgs=dict(
                        type='MultiScaleDeformableAttention',
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=False,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfgs=dict(
                        type='FFN',
                        embed_dims=256,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True)),
                    operation_order=('self_attn', 'norm', 'ffn', 'norm')),
                init_cfg=None),
            positional_encoding=dict(
                type='SinePositionalEncoding', num_feats=128, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(
            type='SinePositionalEncoding', num_feats=128, normalize=True),
        transformer_decoder=dict(
            type='DetrTransformerDecoder',
            return_intermediate=True,
            num_layers=9,
            transformerlayers=dict(
                type='DetrTransformerDecoderLayer',
                attn_cfgs=dict(
                    type='MultiheadAttention',
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=False),
                ffn_cfgs=dict(
                    embed_dims=256,
                    feedforward_channels=2048,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True),
                feedforward_channels=2048,
                operation_order=('cross_attn', 'norm', 'self_attn', 'norm',
                                 'ffn', 'norm')),
            init_cfg=None),
        loss_cls=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 0.1
            ]),
        loss_mask=dict(
            type='CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0)),
    panoptic_fusion_head=dict(
        type='MaskFormerFusionHead',
        num_things_classes=80,
        num_stuff_classes=53,
        loss_panoptic=None,
        init_cfg=None),
    train_cfg=dict(
        num_points=12544,
        oversample_ratio=3.0,
        importance_sample_ratio=0.75,
        assigner=dict(
            type='MaskHungarianAssigner',
            cls_cost=dict(type='ClassificationCost', weight=2.0),
            mask_cost=dict(
                type='CrossEntropyLossCost', weight=5.0, use_sigmoid=True),
            dice_cost=dict(
                type='DiceCost', weight=5.0, pred_act=True, eps=1.0)),
        sampler=dict(type='MaskPseudoSampler')),
    test_cfg=dict(
        panoptic_on=True,
        semantic_on=False,
        instance_on=True,
        max_per_image=100,
        iou_thr=0.8,
        filter_low_score=True),
    init_cfg=None)
image_size = (1024, 1024)
embed_multi = dict(lr_mult=1.0, decay_mult=0.0)
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    weight_decay=0.05,
    eps=1e-08,
    betas=(0.9, 0.999),
    paramwise_cfg=dict(
        custom_keys=dict(
            backbone=dict(lr_mult=0.1, decay_mult=1.0),
            query_embed=dict(lr_mult=1.0, decay_mult=0.0),
            query_feat=dict(lr_mult=1.0, decay_mult=0.0),
            level_embed=dict(lr_mult=1.0, decay_mult=0.0)),
        norm_decay_mult=0.0))
optimizer_config = dict(grad_clip=dict(max_norm=0.01, norm_type=2))
lr_config = dict(
    policy='step',
    gamma=0.1,
    by_epoch=False,
    step=[327778, 355092],
    warmup='linear',
    warmup_by_epoch=False,
    warmup_ratio=1.0,
    warmup_iters=10)
max_iters = 368750
runner = dict(type='IterBasedRunner', max_iters=368750)
interval = 5000
dynamic_intervals = [(365001, 368750)]
work_dir = './work_dirs/mask2former_r50_lsj_8x2_50e_coco-panoptic'
auto_resume = False
gpu_ids = [0]
```
"
K-Fold Cross Validation in MMDet,open-mmlab/mmdetection,2022-08-29 14:23:54,1,,8662,1354394283,"Can I implement K-Fold cross validation on my dataset using MMDetection toolbox, is this supported at current version of the toolbox? Are there anyways to implement it?"
Help with understanding grad_clip parameters - is it a feature or bug,open-mmlab/mmdetection,2022-08-29 10:30:51,1,,8659,1354066043,"**Describe the bug**
Gradient clipping seems to work in some experiments, and does not work in other runs with same parameters (unstable). 

All experiment parameters (configuration file, code version, dataset) kept the same, the clipping works in some runs, and in other runs the gradient can have large values (larger than the `max_norm` parameter). Runs are in the same environment, on the same worker.  We are not using `Deterministic` or `seed`.

1. What command or script did you run?

`tools/train.py` + customized config file

2. Did you make any modifications on the code or config? Did you understand what you have modified?

Config changes: 
-  Added the cyclic lr scheduler with linear warm up + cyclic momentum scheduler
- Custom anchors
- albumentations based training dataset augmentation pipeline
- 1 GPU, base_batch_size=64
- grad_clip=dict(max_norm=35, norm_type=2)
Code changes:
Custom ClearML logging hook on top of mmcv clearmllogger. Only has after_train_epoch callback implemented, but spikes occur at any iteration.

3. What dataset did you use?

Custom datasets in coco format

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
collect_env.py

```
sys.platform: linux
Python: 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla T4
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0
OpenCV: 4.6.0
MMCV: 1.5.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.1+d49d92a 
```


2. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
   - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

PATH 
`/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin`

LD_LIBRARY_PATH
`/usr/local/nvidia/lib:/usr/local/nvidia/lib64`

Docker file
```
ARG PYTORCH=""1.11.0""
ARG CUDA=""11.3""
ARG CUDNN=""8""

FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel

ENV TORCH_CUDA_ARCH_LIST=""6.0 6.1 7.0 8.0+PTX""
ENV TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""
ENV CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../""

RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys A4B469963BF863CC \
    && apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 libcurl4 mlocate nano \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* 

# Install MMCV
RUN pip install --no-cache-dir --upgrade pip wheel setuptools
RUN pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
RUN pip install --no-cache-dir mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html

# Install ClearML & Tensorboard
RUN pip install clearml future tensorboard

# Install FiftyOne
RUN pip install --index-url https://@pypi.fiftyone.ai -U fiftyone

# Install MMDetection and requirements
RUN conda clean --all
#RUN git clone https://github.com/open-mmlab/mmdetection.git /mmdetection
#WORKDIR /mmdetection
#ENV FORCE_CUDA=""1""
#RUN pip install --no-cache-dir -r requirements/build.txt
#RUN pip install --no-cache-dir -r requirements/albu.txt
#RUN pip install --no-cache-dir -r requirements/tests.txt
#RUN pip install --no-cache-dir -r requirements/runtime.txt

# RUN pip install --no-cache-dir -e .
```

**Error traceback**

No error, the training runs regularly, and no error traceback. But we have gradients larger than max_norm, causing spikes in loss functions as well. 
"
using dish_train.sh raise subprocess.CalledProcessError多卡训练报错，单卡训练没出现异常,open-mmlab/mmdetection,2022-08-29 06:24:14,1,,8657,1353769247,"runner = dict(type='EpochBasedRunner', max_epochs=24)
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=True, base_batch_size=16)
work_dir = './out/faster_rcnn_r101_fpn_2x_object365'
auto_resume = False
gpu_ids = range(0, 4)

2022-08-29 05:53:33,263 - mmdet - INFO - Set random seed to 0, deterministic: False
2022-08-29 05:53:34,031 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet101'}
2022-08-29 05:53:34,031 - mmcv - INFO - load model from: torchvision://resnet101
2022-08-29 05:53:34,031 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet101
2022-08-29 05:53:37,636 - mmcv - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

2022-08-29 05:53:37,677 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2022-08-29 05:53:37,697 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2022-08-29 05:53:37,702 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Killing subprocess 49397
Killing subprocess 49398
Killing subprocess 49399
Killing subprocess 49400
Traceback (most recent call last):
  File ""/sensebee2/dinghaojie/anaconda3/envs/openmmdetection/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/sensebee2/dinghaojie/anaconda3/envs/openmmdetection/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/sensebee2/dinghaojie/anaconda3/envs/openmmdetection/lib/python3.6/site-packages/torch/distributed/launch.py"", line 340, in <module>
    main()
  File ""/sensebee2/dinghaojie/anaconda3/envs/openmmdetection/lib/python3.6/site-packages/torch/distributed/launch.py"", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File ""/sensebee2/dinghaojie/anaconda3/envs/openmmdetection/lib/python3.6/site-packages/torch/distributed/launch.py"", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/sensebee2/dinghaojie/anaconda3/envs/openmmdetection/bin/python', '-u', './tools/train.py', '--local_rank=3', './train_dirs/faster_rcnn_r101_fpn_2x_object365/faster_rcnn_r101_fpn_2x_object365.py', '--seed', '0', '--launcher', 'pytorch']' died with <Signals.SIGKILL: 9>."
The reference points in the Deformable DETR,open-mmlab/mmdetection,2022-08-29 04:49:16,5,community discussion,8656,1353696582,"I can't understand the way of the reference point generation. In the `DeformableDetrTransformer` class, the `get_reference_points` is used to calculate the initial reference point in the encode stage, but the calculation seems strange:
(1) the reference points' coordinate is normalized with the valid H, W, and will this cause the padding pixels to have the normalized coordinate greater than 1?
(2) after the normalization operation, why there need to multiply these normalized coordinates with the valid ratios again?
<img width=""699"" alt=""image"" src=""https://user-images.githubusercontent.com/51013927/187123751-16c77d27-4559-4cb4-a49c-3d5882549d39.png"">
After get these reference points, in the Multi-Scale-Deformable-Attention, you calculate the sampling location which will be used in the deformable attention, do these locations' coordinate need to have the range of [0, 1]? If this is true, in the calculation, it seems a little strange, cause the calculation can't guarantee the range is [0, 1] for the sampling offset is unbounded.
![image](https://user-images.githubusercontent.com/51013927/187125124-737360a8-1cc9-461c-bd52-d614a4fc31fb.png)

Hope to get your reply. Thanks!"
use trained model to separate files,open-mmlab/mmdetection,2022-08-26 14:26:29,1,,8647,1352343549,"Hi, before we start, I know this question may be out of scope, but it's still something related to mmdetection, so come on, I'm getting to know the tool and I trained a model (autoassign) to recognize a certain type of animal (dog specifically) and I would like to use this model trained in a script to separate images into a folder with several images of the trained animal and garbage (everything that is not a dog), the problem is that I am at a loss when it comes to knowing when the animal was detected after making the inference, reading a little the argmax in the variable that received the result of the inference seemed to solve my problem since it always returns a fixed value when it recognizes the animal in the image, but it is not always, so I would like to know if there is another most reliable method of knowing when my model has recognized the animal in the image so I can separate it from the rest."
Precision and recall at different confidence score,open-mmlab/mmdetection,2022-08-24 13:50:59,3,,8629,1349484504,"Is there any way to calculate precision and recall at different confidence score?  

I have trained model with 5 classes with Coco format. However, in evaluation results shows the Average precision and average recall.

I want to check only precision and recall at different confidence score. Is there any way to do this?

Thanks. "
Missing output of masks when serving Mask R-CNN with pytorch serve,open-mmlab/mmdetection,2022-08-23 11:16:04,1,feature request,8621,1347780702,"**Describe the bug**

I followed [these instructions](https://mmdetection.readthedocs.io/en/latest/useful_tools.html#model-serving) to serve a Mask R-CNN model via pytorch serve. Inference on an example image works fine, however returns only bounding boxes, class labels and score. Segmentation masks are missing.

**Reproduction**

I looked into the [mmdet_handler.py](https://github.com/open-mmlab/mmdetection/blob/master/tools/deployment/mmdet_handler.py) and it seems to me that in the `postprocess` method the `segm_result` is not properly included in the returned output.

I tried to include `segm_result` into the output list, however got errors like
```
{
  ""code"": 503,
  ""type"": ""InternalServerException"",
  ""message"": ""number of batch response mismatched""
}
```
and
```
{
  ""code"": 503,
  ""type"": ""InternalServerException"",
  ""message"": ""Unsupported model output data type.""
}
``` 
I guess, some other parts of the code need to be altered. However, I am not familiar enough with the code base to make these changes.

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.

```
fatal: unsafe repository ('/mmdetection' is owned by someone else)
To add an exception for this directory, call:

	git config --global --add safe.directory /mmdetection
sys.platform: linux
Python: 3.7.7 (default, May  7 2020, 21:25:33) [GCC 7.3.0]
CUDA available: True
GPU 0: GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.243
GCC: gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.6.0
MMCV: 1.3.17
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
MMDetection: 2.25.0+
```"
Fix swin backbone absolute pos_embed,open-mmlab/mmdetection,2022-08-23 04:04:21,1,community help wanted#v-3.x,8616,1347296078,"This PR needs to be migrated to dev-3.x

_Originally posted by @ZwwWayne in https://github.com/open-mmlab/mmdetection/issues/8127#issuecomment-1223506509_"
Some problems that occurred while panoptic segmentation using mask2former (Cityscapes),open-mmlab/mmdetection,2022-08-22 11:59:48,2,,8610,1346286283,"Hello, thanks to you, I'm using the mmdetection platform well.

Learning seems to be progressing, but I'm asking because some problems have occurred.
First, I changed the Cityscapes dataset to COCO Panoptic format.

1. After learning, I ran Demo as a weight file, and the result came out like the result of Semantic Segmentation.
2. The PQ appears to be small.
3. It is included in the class such as road but cannot be inferred.

### Result of training at 60epochs
![test_city_epoch60](https://user-images.githubusercontent.com/75903850/185915958-a26c4032-a1ad-4d4e-890c-aa572498a637.png)

```
+--------+--------+--------+--------+------------+
|        | PQ     | SQ     | RQ     | categories |
+--------+--------+--------+--------+------------+
| All    | 22.164 | 45.263 | 26.866 | 19         |
| Things | 26.524 | 75.686 | 33.684 | 8          |
| Stuff  | 18.994 | 23.138 | 21.908 | 11         |
+--------+--------+--------+--------+------------+
```
Is there any problem you can predict? I've tried several workarounds, but they don't work.

I will upload my log file and annotation example file in the comments.
Thank you."
RuntimeError: Expected to mark a variable ready only once.,open-mmlab/mmdetection,2022-08-22 11:52:17,1,,8609,1346278191," In my implemented algorithms, I need to backward the classification loss and location loss respectively to obtain the respective gradients, in other word, I need to backward twice for one forward. When I run it,  a single GPU run is normal, multiple GPUs run will report the following error:

""RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the `forward` function. Please make sure model parameters are not shared across multiple concurrent forward-backward passes2) Reused parameters in multiple reentrant backward passes. For example, if you use multiple `checkpoint` functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases yet.3) Incorrect unused parameter detection. The return value of the `forward` function is inspected by the distributed data parallel wrapper to figure out if any of the module's parameters went unused. For unused parameters, DDP would not expect gradients from then. However, if an unused parameter becomes part of the autograd graph at a later point in time (e.g., in a reentrant backward when using `checkpoint`), the gradient will show up unexpectedly. If all parameters in the model participate in the backward pass, you can disable unused parameter detection by passing the keyword argument `find_unused_parameters=False` to `torch.nn.parallel.DistributedDataParallel`.""

And I have set ' find_unused_parameters=False', however, it still doesn't work.
Thank you very much."
How to a extract feature vector  from a pretrain model,open-mmlab/mmdetection,2022-08-22 10:55:27,1,,8608,1346207730,"Thanks for the awesome framework!! 

I am trying to extract feature vectors from a pretrained model.  I have attached the `model` and I have indicated three locations from where I would like extract the feature vectors (lines 330,365,378).

I am unsure of what or where the code I write should be to extract the vectors at these locations? 

Any help will be appreciated. 
[model.txt](https://github.com/open-mmlab/mmdetection/files/9393703/model.txt)

"
"As a result of training on a custom dataset, the AP is always 0.",open-mmlab/mmdetection,2022-08-22 05:57:30,6,,8604,1345857869,"I trained the maskrcnn-swin transformer on my custom dataset.

But the result is that ap is always 0, like this:

May I know what is the problem?
-----------------------------------------------------------------------------------------------------------------------------------------
``2022-08-22`` 01:51:54,806 - mmdet - INFO - workflow: [('train', 1), ('val', 1)], max: 30 epochs
2022-08-22 01:51:54,806 - mmdet - INFO - Checkpoints will be saved to /scratch/dohyeon/mmdetection/work_dirs/final by HardDiskBackend.
wandb: Currently logged in as: dohyeonyoon. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.1
wandb: Run data is saved locally in /scratch/dohyeon/mmdetection/wandb/run-20220822_015155-3ldd4oe0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dohyeon
wandb: ⭐️ View project at https://wandb.ai/dohyeonyoon/smartfarm
wandb: 🚀 View run at https://wandb.ai/dohyeonyoon/smartfarm/runs/3ldd4oe0
2022-08-22 01:52:15,480 - mmdet - INFO - Saving checkpoint at 1 epochs
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 4/4, 1.4 task/s, elapsed: 3s, ETA:     0s2022-08-22 01:52:22,441 - mmdet - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.83s).
Accumulating evaluation results...
DONE (t=0.01s).
2022-08-22 01:52:26,287 - mmdet - INFO -
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000

2022-08-22 01:52:26,287 - mmdet - INFO - Evaluating segm...
/scratch/dohyeon/anaconda3/envs/smartfarm/lib/python3.8/site-packages/mmdet/datasets/coco.py:472: UserWarning: The key ""bbox"" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.
  warnings.warn(
Loading and preparing results...
DONE (t=0.06s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=3.90s).
Accumulating evaluation results...
/scratch/dohyeon/anaconda3/envs/smartfarm/lib/python3.8/site-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)
DONE (t=0.07s).
2022-08-22 01:52:30,419 - mmdet - INFO -
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = -1.000

2022-08-22 01:52:30,421 - mmdet - INFO - Exp name: final.py
2022-08-22 01:52:30,421 - mmdet - INFO - Epoch(val) [1][4]      bbox_mAP: 0.0000, bbox_mAP_50: 0.0000, bbox_mAP_75: 0.0000, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0000, bbox_mAP_l: -1.0000, bbox_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 -1.000, segm_mAP: 0.0000, segm_mAP_50: 0.0000, segm_mAP_75: 0.0000, segm_mAP_s: 0.0000, segm_mAP_m: 0.0000, segm_mAP_l: -1.0000, segm_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 -1.000
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.75s).
Accumulating evaluation results...
DONE (t=0.01s).
/scratch/dohyeon/anaconda3/envs/smartfarm/lib/python3.8/site-packages/mmdet/datasets/coco.py:472: UserWarning: The key ""bbox"" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.
  warnings.warn(
Loading and preparing results...
DONE (t=0.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=3.88s).
Accumulating evaluation results...
/scratch/dohyeon/anaconda3/envs/smartfarm/lib/python3.8/site-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)
DONE (t=0.01s).
2022-08-22 01:52:46,585 - mmdet - INFO - Exp name: final.py
2022-08-22 01:52:46,585 - mmdet - INFO - Epoch(val) [1][2]      loss_rpn_cls: 0.6864, loss_rpn_bbox: 0.2324, loss_cls: 0.6164, acc: 70.3613, loss_bbox: 0.0304, loss_mask: 4.0536, loss: 5.6193
``"
Panoptic dataloader for Cityscapes to train maskformer or mask2former,open-mmlab/mmdetection,2022-08-20 11:06:31,1,feature request,8601,1345136776,Does mmdetection have a panoptic cityscapes dataloader to train mask2former for panoptic segmentation?
"How can I only use mask and images to train Instance segmentation, such as Mask2former?",open-mmlab/mmdetection,2022-08-19 14:37:39,4,,8599,1344532291," In a cell segmentation competition, I only get mask and image.But this is a typical instance segmentation task.I can train in mmsegmentation , but I want to use Mask2former.
I would appreciate it ，if you could give me an example.
About my dataset is very common in semantic segmentation.Background 0 ,Interior 1 ,Boundary 2.I can easy train in mmsegentation"
MMDetWandbHook not logging images ,open-mmlab/mmdetection,2022-08-17 16:41:30,4,,8585,1342028189,"Using v2.25 and latest wandb with the following code. images do not show in wandb dashboard

```
log_config = dict(
    interval=10,
    hooks=[
        # dict(type='TensorboardLoggerHook'),
        dict(type='TextLoggerHook'),
        dict(type='MMDetWandbHook',
            init_kwargs={
                'project': PROJECT,
                'entity': ENTITY,
                'name': EXP_NAME,
                'config': {
                    'lr': 0.02 / GPUS, 'batch_size': BATCH_SIZE
                },
                'tags': WANDB_TAGS
            },
            interval=10,
            log_checkpoint=False,
            log_checkpoint_metadata=True,
            num_eval_images=10)
    ])
```"
Can pretrained models from torchvision hub be used with mmdetection?,open-mmlab/mmdetection,2022-08-17 10:10:55,3,feature request,8583,1341523341,"Suppose a Faster R-CNN model or any other pretrained model is imported from torchvision hub (e.g., https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn), can it be loaded into mmdetection directly for performing inference or ONNX/TensorRT conversion? I know mmdetect provides pretrained models, but asking just in case. If not, why? Is there any architecture difference between the pretrained models here and pretrained models from torchvision hub?"
Any plan support ViTDet?,open-mmlab/mmdetection,2022-08-17 08:27:20,2,enhancement#feature request,8577,1341376906,"**Describe the feature**
support ViTDet project.

**Motivation**

There is a recent paper https://arxiv.org/abs/2203.16527 which is very helpful for Object Detection

**Related resources**
If there is an official code release or third-party implementations, please also provide the information here, which would be very helpful.

**Additional context**
Add any other context or screenshots about the feature request here.
If you would like to implement the feature and create a PR, please leave a comment here and that would be much appreciated.
"
typo in msdeformattn_pixel_decoder.py,open-mmlab/mmdetection,2022-08-16 19:20:40,2,enhancement#typo,8574,1340780687,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

In [mmdetection/blob/master/mmdet/models/plugins/msdeformattn_pixel_decoder.py#](https://github.com/open-mmlab/mmdetection/blob/3b72b12fe9b14de906d1363982b9fba05e7d47c1/mmdet/models/plugins/msdeformattn_pixel_decoder.py#L228) , valid_ratios seems to be mistyped as valid_radios, effectively making this argument useless?

**Bug fix**
Fix typo.
"
PoseConv3D pseudo heatmap volume,open-mmlab/mmdetection,2022-08-12 20:50:00,1,,8558,1337695960,"Thanks for your promising work (PoseConv3D). i am wandering  about the reason of stacking pseudo heatmap of joints along temporal dimension instead single image containing all joints for example if we have video clip of length 30 frame, the stacking heatmaps will produce matrix of 17 (num_joints) X 30 X 56 (height) X 56 (width) but putting all joints in single frame produce a matrix of 30X56X56.  "
Tracing Forward Function,open-mmlab/mmdetection,2022-08-10 23:57:01,1,,8538,1335324017,"I am looking to optimize the data pipeline of the Mask RCNN model with a Swin backbone (this one to be specific: https://github.com/open-mmlab/mmdetection/blob/master/configs/swin/mask_rcnn_swin-s-p4-w7_fpn_fp16_ms-crop-3x_coco.py) to apply the transformations on GPU using pytorch.  To do this I need to adjust the forward function of model but am having trouble tracing it through the model.  I am not focusing much on tracing the actual pipeline since I can just adjust the results of the optimized version to match as needed.  But will need to adjust the model itself.

So far I've been able to trace the forward function from:

Here (https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/detectors/mask_rcnn.py) -> some stuff in btw that I haven't been able to trace -> here(https://github.com/open-mmlab/mmdetection/blob/3b72b12fe9b14de906d1363982b9fba05e7d47c1/mmdet/models/detectors/two_stage.py#L173) -> some stuff in btw that I haven't been able to trace -> here (https://github.com/open-mmlab/mmdetection/blob/3b72b12fe9b14de906d1363982b9fba05e7d47c1/mmdet/models/detectors/base.py#L112).

The order in above might be incorrect.  If anyone is able to offer guidance on how input is flowing through the forward pass of the model it would be greatly appreciated!  As I am pretty stuck due to the massive amount of inheritance going on."
"Caffe -> ONNX ValueError: cannot reshape array of size 0 into shape (96,3,11,11)",open-mmlab/mmdetection,2022-08-10 22:21:20,1,,8537,1335262981,"Looking to convert legacy caffe models to onnx. 

We keep getting a zero for input, even when we --inputShape 227, 227, 3.

Please see our log below: 

mmconvert -sf caffe in deploy.prototxt -iw snapshot_iter_58295.caffemodel -df onnx -om test8.onnx --inputShape 227,227,3

------------------------------------------------------------
    WARNING: PyCaffe not found!
    Falling back to a pure protocol buffer implementation.
    * Conversions will be drastically slower.
    * This backend is UNTESTED!
------------------------------------------------------------

Traceback (most recent call last):
  File ""/home/host/.local/bin/mmconvert"", line 8, in <module>
    sys.exit(_main())
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/_script/convert.py"", line 102, in _main
    ret = convertToIR._convert(ir_args)
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/_script/convertToIR.py"", line 16, in _convert
    transformer = CaffeTransformer(args.network, args.weights, ""tensorflow"", inputshape[0], phase = args.caffePhase)
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 315, in __init__
    def_path, self.data_injector = self.gen_prototxt_from_caffemodel(data_path, self.input_shape)
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 351, in gen_prototxt_from_caffemodel
    data_injector = DataInjector(None, data_path)
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 31, in __init__
    self.load_using_pb()
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 44, in load_using_pb
    self.params = [pair(layer) for layer in layers if layer.blobs]
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 44, in <listcomp>
    self.params = [pair(layer) for layer in layers if layer.blobs]
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 42, in <lambda>
    pair = lambda layer: (layer.name, self.normalize_pb_data(layer))
  File ""/home/host/.local/lib/python3.8/site-packages/mmdnn/conversion/caffe/transformer.py"", line 59, in normalize_pb_data
    data = np.array(blob.data, dtype=np.float32).reshape(c_o, c_i, h, w)
ValueError: cannot reshape array of size 0 into shape (96,3,11,11)

Here is the deploy.prototxt: 
input: ""data""
input_shape {
  dim: 1
  dim: 3
  dim: 227
  dim: 227
}
layer {
  name: ""conv1""
  type: ""Convolution""
  bottom: ""data""
  top: ""conv1""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: ""gaussian""
      std: 0.00999999977648
    }
    bias_filler {
      type: ""constant""
      value: 0.0
    }
  }
}
layer {
  name: ""relu1""
  type: ""ReLU""
  bottom: ""conv1""
  top: ""conv1""
}
layer {
  name: ""norm1""
  type: ""LRN""
  bottom: ""conv1""
  top: ""norm1""
  lrn_param {
    local_size: 5
    alpha: 9.99999974738e-05
    beta: 0.75
  }
}
layer {
  name: ""pool1""
  type: ""Pooling""
  bottom: ""norm1""
  top: ""pool1""
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: ""conv2""
  type: ""Convolution""
  bottom: ""pool1""
  top: ""conv2""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: ""gaussian""
      std: 0.00999999977648
    }
    bias_filler {
      type: ""constant""
      value: 0.10000000149
    }
  }
}
layer {
  name: ""relu2""
  type: ""ReLU""
  bottom: ""conv2""
  top: ""conv2""
}
layer {
  name: ""norm2""
  type: ""LRN""
  bottom: ""conv2""
  top: ""norm2""
  lrn_param {
    local_size: 5
    alpha: 9.99999974738e-05
    beta: 0.75
  }
}
layer {
  name: ""pool2""
  type: ""Pooling""
  bottom: ""norm2""
  top: ""pool2""
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: ""conv3""
  type: ""Convolution""
  bottom: ""pool2""
  top: ""conv3""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: ""gaussian""
      std: 0.00999999977648
    }
    bias_filler {
      type: ""constant""
      value: 0.0
    }
  }
}
layer {
  name: ""relu3""
  type: ""ReLU""
  bottom: ""conv3""
  top: ""conv3""
}
layer {
  name: ""conv4""
  type: ""Convolution""
  bottom: ""conv3""
  top: ""conv4""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: ""gaussian""
      std: 0.00999999977648
    }
    bias_filler {
      type: ""constant""
      value: 0.10000000149
    }
  }
}
layer {
  name: ""relu4""
  type: ""ReLU""
  bottom: ""conv4""
  top: ""conv4""
}
layer {
  name: ""conv5""
  type: ""Convolution""
  bottom: ""conv4""
  top: ""conv5""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: ""gaussian""
      std: 0.00999999977648
    }
    bias_filler {
      type: ""constant""
      value: 0.10000000149
    }
  }
}
layer {
  name: ""relu5""
  type: ""ReLU""
  bottom: ""conv5""
  top: ""conv5""
}
layer {
  name: ""pool5""
  type: ""Pooling""
  bottom: ""conv5""
  top: ""pool5""
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: ""fc6""
  type: ""InnerProduct""
  bottom: ""pool5""
  top: ""fc6""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: ""gaussian""
      std: 0.00499999988824
    }
    bias_filler {
      type: ""constant""
      value: 0.10000000149
    }
  }
}
layer {
  name: ""relu6""
  type: ""ReLU""
  bottom: ""fc6""
  top: ""fc6""
}
layer {
  name: ""drop6""
  type: ""Dropout""
  bottom: ""fc6""
  top: ""fc6""
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: ""fc7""
  type: ""InnerProduct""
  bottom: ""fc6""
  top: ""fc7""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: ""gaussian""
      std: 0.00499999988824
    }
    bias_filler {
      type: ""constant""
      value: 0.10000000149
    }
  }
}
layer {
  name: ""relu7""
  type: ""ReLU""
  bottom: ""fc7""
  top: ""fc7""
}
layer {
  name: ""drop7""
  type: ""Dropout""
  bottom: ""fc7""
  top: ""fc7""
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: ""fc8""
  type: ""InnerProduct""
  bottom: ""fc7""
  top: ""fc8""
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 0.0
  }
  inner_product_param {
    num_output: 77
    weight_filler {
      type: ""gaussian""
      std: 0.00999999977648
    }
    bias_filler {
      type: ""constant""
      value: 0.0
    }
  }
}
layer {
  name: ""softmax""
  type: ""Softmax""
  bottom: ""fc8""
  top: ""softmax""
}





"
FoveaBox : wrong initialization of bbox_targets ,open-mmlab/mmdetection,2022-08-10 20:11:47,1,,8536,1335148938,"**Bug**

In mmdet/models/dense_heads/fovea_head.py, the bbox_targets is not initialized correctly.
Indeed, Tensor.new() is an equivalent of Tensor.new_empty() and will create the Tensor with uninitialized memory.


**Bug fix**

It is currently :
```
labels = gt_labels_raw.new_zeros(featmap_size) + self.num_classes
bbox_targets = gt_bboxes_raw.new(featmap_size[0], featmap_size[1], 4) + 1
```

But should be :
```
labels = gt_labels_raw.new_zeros(featmap_size) + self.num_classes
bbox_targets = gt_bboxes_raw.new_zeros(featmap_size[0], featmap_size[1],  4) + 1
```

Note : using torch.full_like() syntax would be slightly faster and more comprehensible in my opinion
"
How to compute F1 score for each class? ,open-mmlab/mmdetection,2022-08-10 16:05:43,3,,8534,1334853901,Can we compute F1 score for each class? I am using 6 classes and I want to compute F1 score for each class. Can anyone please let me know how can we calculate this.
Find a bug in load_masks,open-mmlab/mmdetection,2022-08-10 03:08:35,12,,8531,1333999666,"**Issue:** When training an instance segmentation model with setting poly2mask=False，the mask map is compared to poly2mask=True, it drops by 5-6 points.

**Reason:** When load_masks (polygons format) is not forced to convert to **np.float64**, in crop_and_resize, multiple operations are performed to take integers, resulting in a large deviation of mask_target."
IndexError: list index out of range,open-mmlab/mmdetection,2022-08-10 00:30:38,6,,8529,1333918191,"I have successfully trained my mask-rcnn model in the custom dataset(in coco format). However I am facing following error while I was trying to test the model. I have gone through every possible thread to find the solution I have changed the classes in all possible files where class needs to get changed. 
 
 File ""/home/shreyachauhan/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/datasets/coco.py"", line 641, in evaluate
    result_files, tmp_dir = self.format_results(results, jsonfile_prefix)
  File ""/home/shreyachauhan/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/datasets/coco.py"", line 383, in format_results
    result_files = self.results2json(results, jsonfile_prefix)
  File ""/home/shreyachauhan/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/datasets/coco.py"", line 320, in results2json
    json_results = self._segm2json(results)
  File ""/home/shreyachauhan/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/datasets/coco.py"", line 271, in _segm2json
    data['category_id'] = self.cat_ids[label]
IndexError: list index out of range

If anyone knows the solution to it please help me. "
Simple copy paste giving errors during inference,open-mmlab/mmdetection,2022-08-09 16:50:34,1,,8528,1333527339,"Inference is not working for Simple Copy Paste config below. 
https://github.com/open-mmlab/mmdetection/tree/master/configs/simple_copy_paste

I have reproduced on a Google Colab below. It is working for other models like detr, mask-rcnn
https://colab.research.google.com/drive/1C3pUiaOnjBRZal98997uXPjpV03c-v0-#scrollTo=0k5_iiBUaviK

@Czm369 @sudz123 @ZwwWayne @Johnson-Wang @BIGWangYuDong @torchmyheart "
Any plan to add Vit and MVit,open-mmlab/mmdetection,2022-08-09 06:18:49,2,enhancement#feature request,8523,1332768901,"**Describe the feature**
Add backbones below:
1.Multiscale Vision Transformer (MViT)
2.Vision Transformer
**Motivation**

There is a recent paper https://arxiv.org/abs/2203.16527  which is very helpful for Object Detection
**Related resources**
If there is an official code release or third-party implementations, please also provide the information here, which would be very helpful.

**Additional context**
Add any other context or screenshots about the feature request here.
If you would like to implement the feature and create a PR, please leave a comment here and that would be much appreciated.
"
docker mmvc hard coded pytorch and cuda version,open-mmlab/mmdetection,2022-08-08 22:35:20,1,,8520,1332477482,"You might want to consider making [this line](https://github.com/open-mmlab/mmdetection/blob/master/docker/Dockerfile#L21) dependent on the [args](https://github.com/open-mmlab/mmdetection/blob/master/docker/Dockerfile#L1). I had errors when using an image built with `ARG PYTORCH=""1.9.0""` and `ARG CUDA=""11.1""` while keeping the mmvc install line as is. When changed to `RUN pip install --no-cache-dir mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html`, it worked great."
AP=0,open-mmlab/mmdetection,2022-08-07 10:45:38,1,,8514,1330979185,"When I try to train my dataset through mask RCNN, I find that my AP appear - 1 and 0 during training. I want to know what the reason is. I don't know how to solve this problem。Here are the AP for my training process
Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=1000 ] = -1.000
Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=1000 ] = -1.000
Average Precision (AP) @[ IoU=0.50:0.50 | area= small | maxDets=1000 ] = 0.000
Average Precision (AP) @[ IoU=0.50:0.50 | area=medium | maxDets=1000 ] = -1.000
Average Precision (AP) @[ IoU=0.50:0.50 | area= large | maxDets=1000 ] = 0.663
Average Recall (AR) @[ IoU=0.50:0.50 | area= all | maxDets=100 ] = 0.794
Average Recall (AR) @[ IoU=0.50:0.50 | area= all | maxDets=300 ] = 0.794
Average Recall (AR) @[ IoU=0.50:0.50 | area= all | maxDets=1000 ] = 0.794
Average Recall (AR) @[ IoU=0.50:0.50 | area= small | maxDets=1000 ] = 0.000
Average Recall (AR) @[ IoU=0.50:0.50 | area=medium | maxDets=1000 ] = -1.000
Average Recall (AR) @[ IoU=0.50:0.50 | area= large | maxDets=1000 ] = 0.816"
How to get the instance segmentation of mask2fomrer after training model on coco?,open-mmlab/mmdetection,2022-08-03 16:59:09,3,,8488,1327540578,"Hi, I use coco pre-trained model (swin-tiny from model zoo) to train coco for instance segmentation.
But when I use the following command: 
python test.py --show-dir 
I get the segmentation result. 

However, if I directly use pre-trained model from model zoo to test, I get the instance segmentation result.

So, what should I do to get instance segmentation after fine-tuned? Thanks"
"when i use mask rcnn to detection,result is false",open-mmlab/mmdetection,2022-08-03 13:28:24,2,,8486,1327219776,"config_file='../../mmdetection/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'
#config_file='../../mmdetection/configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco.py'
#config_file='../../mmdetection/configs/yolo/yolov3_d53_mstrain-416_273e_coco.py'
checkpoint_file='../mmdetection/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'
#checkpoint_file='../mmdetection/checkpoints/faster_rcnn_r50_caffe_fpn_mstrain_3x_coco_20210526_095054-1f77628b.pth'
model=init_detector(config_file,checkpoint_file,device='cuda:0')
#in_folder='../mmdetection/rumor_images/'
in_folder='../mmdetection/nonrumor_images/'
#out_folder='../mmdetection/fenge_rumor_images/'
#out_folder='../mmdetection/newsplit_rumor_images/'
out_folder='../mmdetection/fengenonrumor_images/'

"
Low mAP on coco with mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x,open-mmlab/mmdetection,2022-08-02 08:53:57,5,bug,8471,1325507556,"**Describe the bug**
When running tools/test.py on mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x on coco, the performance (mAP) is much lower than reported here: https://github.com/open-mmlab/mmdetection/tree/master/configs/mask_rcnn .  
For other models, I was able to reproduce the reported results. Is it possible there is something wrong with the config or checkpoint of this network?

-tools/test.py output: bbox: 40.4%, segm: 35.9%
-mmdetection github page: bbox: 44.3%, segm: 39.5%

**Reproduction**

1. What command or script did you run?

```
python tools/test.py configs/mask_rcnn/mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x_coco.py checkpoints/mask_rcnn/mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x_coco/mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x_coco_20210607_161042-8bd2c639.pth --eval bbox segm
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
No modifications
3. What dataset did you use?
Coco2017 validation set

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.

sys.platform: linux
Python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]
CUDA available: True
GPU 0: Tesla T4
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.8.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.0+cu111
OpenCV: 4.6.0
MMCV: 1.5.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.25.0+56e42e7

2. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
   With pip
"
About MMDetWandbHook,open-mmlab/mmdetection,2022-07-29 08:31:51,8,bug,8455,1321928744,"when i use WandbLoggerHook, it's ok! Replace by MMDetWandbHook, the error likes this.
Traceback (most recent call last):
  File ""E:/mm250/tools/trains/train.py"", line 237, in <module>
    main()
  File ""E:/mm250/tools/trains/train.py"", line 226, in main
    train_detector(
  File ""E:\mm250\mmdet\apis\train.py"", line 244, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""E:\ProgramData\Anaconda3\envs\mmdet\lib\site-packages\mmcv\runner\epoch_based_runner.py"", line 111, in run
    self.call_hook('before_run')
  File ""E:\ProgramData\Anaconda3\envs\mmdet\lib\site-packages\mmcv\runner\base_runner.py"", line 309, in call_hook
    getattr(hook, fn_name)(self)
  File ""E:\ProgramData\Anaconda3\envs\mmdet\lib\site-packages\mmcv\runner\dist_utils.py"", line 135, in wrapper
    return func(*args, **kwargs)
  File ""E:\mm250\mmdet\core\hook\wandblogger_hook.py"", line 201, in before_run
    self._add_ground_truth(runner)
  File ""E:\mm250\mmdet\core\hook\wandblogger_hook.py"", line 380, in _add_ground_truth
    wandb_masks = self._get_wandb_masks(
  File ""E:\mm250\mmdet\core\hook\wandblogger_hook.py"", line 536, in _get_wandb_masks
    mask = polygon_to_bitmap(mask, height, width)
  File ""E:\mm250\mmdet\core\mask\structures.py"", line 1069, in polygon_to_bitmap
    rles = maskUtils.frPyObjects(polygons, height, width)
  File ""pycocotools/_mask.pyx"", line 293, in pycocotools._mask.frPyObjects
IndexError: list index out of range
wandb: Waiting for W&B process to finish... (failed 1). Press Ctrl-C to abort syncing."
How to calculate average precision of each class,open-mmlab/mmdetection,2022-07-28 12:45:14,2,,8453,1320878219,"Hi, 

After completing each epoch I am getting the following results, but it shows the average precision of all classes. I want to check the average precision of each class. Can anyone please guide me on how to do this?

Thanks.

I was using 5 classes and got the following results.

 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.139
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.408
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.048
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.110
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.128
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.341
"
Running inference on subset of Coco,open-mmlab/mmdetection,2022-07-27 17:59:43,3,,8448,1319901073,"I'm trying to run inference on 100 images selected from the COCO validation set,  instead of the whole thing. I want to do example 6 in 
https://mmdetection.readthedocs.io/en/latest/1_exist_data_model.html

but only for 100 images in the 'person' category. I have the 100 images in its own directory and an annotation .json for those 100. How do I point the inference to that directory to produce instance segmentation results and output into an .json? "
how to buid a custom scheduler,open-mmlab/mmdetection,2022-07-27 12:48:30,5,,8444,1319516499,"
I plan to use Ranger, an optimizer. After successfully registering optimizer, I find that the only available strategies are step, cosine, etc. I want to import the official training strategy. How should I import it。
Because I found that the effect of using only Ranger and step strategies is not as good as adamw。
I tryed the hook，but appeared can not imprt the hoos from the mmcv.core.runner
"
pytorch2onnx has  unsupport type: numpy.ndarray,open-mmlab/mmdetection,2022-07-27 06:11:56,2,,8438,1319069398,"when run pytorch2onnx, show that ""Only tuples, Lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: numpy.ndarray"". what should i do?"
Implementing problems in Rotated YOLOX,open-mmlab/mmdetection,2022-07-25 14:03:17,1,reimplementation,8427,1316889093,"I am implementing Rotated YOLOX for MMRotate in https://github.com/open-mmlab/mmrotate/pull/409, SimOTA Assigner has CUDA Error while training.

Compared with mmdet, only `get_in_gt_and_in_center_info` and `bbox_overlaps` is different to support rotated detection. After set `CUDA_LAUNCH_BLOCKING=1`, the error log shows that error may cause by binary_cross_entropy. It's werid because there is no error when training with fp16. Is there any suggestion to debug that?

Error info:
```
./aten/src/ATen/native/cuda/Loss.cu:115: operator(): block: [58,0,0], thread: [63,0,0] Assertion `input_val >= zero && input_val <= one` failed.
Traceback (most recent call last):
  File ""/miniconda3/lib/python3.9/site-packages/mmdet/core/bbox/assigners/sim_ota_assigner.py"", line 67, in assign
    assign_result = self._assign(pred_scores, priors, decoded_bboxes,
  File ""/workspace/mmrotate/mmrotate/core/bbox/assigners/r_sim_ota_assinger.py"", line 85, in _assign
    F.binary_cross_entropy(
  File ""/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py"", line 3065, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
RuntimeError: CUDA error: device-side assert triggered
```"
Request for best checkpoint from training,open-mmlab/mmdetection,2022-07-25 13:48:10,4,,8426,1316869210,"Hi there,

I'm wondering if there's a way to modify checkpoint hook or make config that let the model updat/store the best pth instead of the lastest?

Thanks so much!!"
"When I was training MaskrCNN, I was killed after training 1 epoch, why? The output is as follows",open-mmlab/mmdetection,2022-07-25 13:05:32,2,,8425,1316810948,"





tools/train.py:155: UserWarning: `--gpus` is deprecated because we only support single GPU mode in non-distributed training. Use `gpus=1` now.
  warnings.warn('`--gpus` is deprecated because we only support '
2022-07-25 19:54:09,961 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) [GCC 10.3.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda-11.4
NVCC: Cuda compilation tools, release 11.4, V11.4.152
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0
OpenCV: 4.6.0
MMCV: 1.6.0
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.0+56e42e7
------------------------------------------------------------

2022-07-25 19:54:10,121 - mmdet - INFO - Distributed training: False
2022-07-25 19:54:10,239 - mmdet - INFO - Config:
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=2,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=2,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=0,
    train=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_train2017.json',
        img_prefix='data/coco/train2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='data/coco/annotations/instances_val2017.json',
        img_prefix='data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.0002, momentum=0.9, weight_decay=1e-05)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
work_dir = './work_dirs/mask_rcnn_r50_fpn_1x_coco'
auto_resume = False
gpu_ids = range(0, 1)

2022-07-25 19:54:10,239 - mmdet - INFO - Set random seed to 801622512, deterministic: False
2022-07-25 19:54:10,433 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}
2022-07-25 19:54:10,433 - mmcv - INFO - load model from: torchvision://resnet50
2022-07-25 19:54:10,433 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50
2022-07-25 19:54:10,489 - mmcv - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

2022-07-25 19:54:10,504 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2022-07-25 19:54:10,518 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2022-07-25 19:54:10,520 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
loading annotations into memory...
Done (t=0.26s)
creating index...
index created!
2022-07-25 19:54:12,601 - mmdet - INFO - Automatic scaling of learning rate (LR) has been disabled.
loading annotations into memory...
Done (t=0.02s)
creating index...
index created!
2022-07-25 19:54:12,625 - mmdet - INFO - Start running, host: ai-team1@aiteam1, work_dir: /home/ai-team1/lxs/mmdetection/work_dirs/mask_rcnn_r50_fpn_1x_coco
2022-07-25 19:54:12,625 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-07-25 19:54:12,625 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2022-07-25 19:54:12,625 - mmdet - INFO - Checkpoints will be saved to /home/ai-team1/lxs/mmdetection/work_dirs/mask_rcnn_r50_fpn_1x_coco by HardDiskBackend.
2022-07-25 20:02:44,753 - mmdet - INFO - Saving checkpoint at 1 epochs
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 22/22, 0.8 task/s, elapsed: 26s, ETA:     0s2022-07-25 20:03:11,605 - mmdet - INFO - Evaluating bbox...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=5.91s).
Accumulating evaluation results...
DONE (t=0.02s).
2022-07-25 20:03:17,536 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.001

2022-07-25 20:03:17,536 - mmdet - INFO - Evaluating segm...
/home/ai-team1/lxs/mmdetection/mmdet/datasets/coco.py:470: UserWarning: The key ""bbox"" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.
  warnings.warn(
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=6.09s).
Accumulating evaluation results...
/home/ai-team1/anaconda3/envs/openmmlab/lib/python3.8/site-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)
DONE (t=0.05s).
2022-07-25 20:03:23,724 - mmdet - INFO - 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.001

2022-07-25 20:03:23,726 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_1x_coco.py
2022-07-25 20:03:23,726 - mmdet - INFO - Epoch(val) [1][22]	bbox_mAP: 0.0000, bbox_mAP_50: 0.0000, bbox_mAP_75: 0.0000, bbox_mAP_s: 0.0000, bbox_mAP_m: 0.0000, bbox_mAP_l: 0.0000, bbox_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000, segm_mAP: 0.0000, segm_mAP_50: 0.0000, segm_mAP_75: 0.0000, segm_mAP_s: 0.0000, segm_mAP_m: 0.0000, segm_mAP_l: 0.0000, segm_mAP_copypaste: 0.000 0.000 0.000 0.000 0.000 0.000
Has killed
"
How to perform semantic segmentation with semantic FPN?,open-mmlab/mmdetection,2022-07-25 02:49:02,1,How-to,8418,1316131505,"Hi, @chhluo @BIGWangYuDong @Czm369 

By adding semantic_head, we can introduce semantic segmentation tasks at training time, however there seems to be no tutorial and config that let us output semantic segmentation results at test time as well?

Update:
I found an https://github.com/open-mmlab/mmdetection/pull/4969, but it did not get pulled."
"try to use convnext_s as backbone for yolact,but there is some error when start to train yolact.  ",open-mmlab/mmdetection,2022-07-20 09:23:52,1,,8392,1310733361,"**configs:**

custom_imports = dict(imports=['mmcls.models'], allow_failed_imports=False)
checkpoint_file = 'https://download.openmmlab.com/mmclassification/v0/convnext/downstream/convnext-small_3rdparty_32xb128-noema_in1k_20220301-303e75e3.pth'  # noqa

img_size = 550
model = dict(
    type='YOLACT',
    backbone=dict(
        type='mmcls.ConvNeXt',
        arch='small',
        out_indices=[0, 1, 2, 3],
        drop_path_rate=0.6,
        layer_scale_init_value=1.0,
        gap_before_final_norm=False,
        init_cfg=dict(
            type='Pretrained', checkpoint=checkpoint_file,
            prefix='backbone.')),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 384, 768],
        out_channels=256,
        start_level=1,
        add_extra_convs='on_input',
        num_outs=5,
        upsample_cfg=dict(mode='bilinear')),


**error:**

index created!
index created!
Traceback (most recent call last):
  File ""./tools/train.py"", line 242, in <module>
    main()
  File ""./tools/train.py"", line 231, in main
    train_detector(
  File ""/home/mmdetection/mmdet/apis/train.py"", line 244, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/opt/conda/envs/openmmlab/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py"", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/opt/conda/envs/openmmlab/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py"", line 53, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File ""/opt/conda/envs/openmmlab/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py"", line 31, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File ""/opt/conda/envs/openmmlab/lib/python3.8/site-packages/mmcv/parallel/distributed.py"", line 46, in train_step
    and self.reducer._rebuild_buckets()):
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 3: 340 341
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error



"
Multi-gpu training get stacked at the first epoch,open-mmlab/mmdetection,2022-07-19 02:58:46,2,,8379,1308909757,"I customize EpochBasedRunner to input different pictures under different control conditions. This is my change:
```python
StitchEpochBasedRunner.py
def train(self, data_loader, **kwargs):
        self.model.train()
        self.mode = 'train'
        self.data_loader_regular = data_loader[0]
        data_loader_regular_iter = iter(self.data_loader_regular)
        self.data_loader_unregular = data_loader[1]
        data_loader_unregular_iter = iter(self.data_loader_unregular)
        self.data_loader = self.data_loader_regular
        self._max_iters = self._max_epochs * len(self.data_loader)
        self.call_hook('before_train_epoch')
        time.sleep(2)  # Prevent possible deadlock during epoch transition
        # under dist training, len(data_batch) = samplers_per_gpu * num_gpus
        if not hasattr(self, ""ratio_small""):
            self.ratio_small = 0.0
        for i in range(len(self.data_loader)):
            if self.ratio_small < self.stitch_thresh:
                # use stitch input
                data_batch = next(data_loader_unregular_iter)
            else:
                data_batch = next(data_loader_regular_iter)
            self._inner_iter = i
            self.call_hook('before_train_iter')
            self.run_iter(data_batch, train_mode=True, **kwargs)
            self.call_hook('after_train_iter')
            self._iter += 1

        self.call_hook('after_train_epoch')
        self._epoch += 1
```
As you can see, I define two seperate dataloaders to input different images. I can train normally on single gpu. But when I train on multi-gpu, it get stacked at the first epoch. When one epoch ends, the dataloader, the two dataloaders will not necessarily be iterated over. I don't know whether it is a cause of my problem.
```python
2022-07-19 10:34:59,355 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) StitchDistSamplerSeedHook          
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) StitchDistSamplerSeedHook          
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-07-19 10:34:59,356 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2022-07-19 10:34:59,356 - mmdet - INFO - Checkpoints will be saved to 
2022-07-19 10:39:34,734 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.
2022-07-19 10:39:40,640 - mmdet - INFO - Epoch [1][10/173]      lr: 9.991e-05, eta: 0:33:32, time: 0.974, data_time: 0.366, memory: 3743, loss_cls_pos: 1.2167, loss_loc: 1.0404, loss_cls_neg: 0.2963, loss: 2.5535
2022-07-19 10:39:46,590 - mmdet - INFO - Epoch [1][20/173]      lr: 1.998e-04, eta: 0:26:54, time: 0.596, data_time: 0.057, memory: 3765, loss_cls_pos: 1.4346, loss_loc: 1.0611, loss_cls_neg: 0.0309, loss: 2.5265
2022-07-19 10:39:52,411 - mmdet - INFO - Epoch [1][30/173]      lr: 2.997e-04, eta: 0:24:28, time: 0.583, data_time: 0.059, memory: 3765, loss_cls_pos: 1.2396, loss_loc: 1.0163, loss_cls_neg: 0.1322, loss: 2.3881
2022-07-19 10:39:58,263 - mmdet - INFO - Epoch [1][40/173]      lr: 3.996e-04, eta: 0:23:15, time: 0.589, data_time: 0.049, memory: 3765, loss_cls_pos: 1.2351, loss_loc: 0.9365, loss_cls_neg: 0.0640, loss: 2.2355
2022-07-19 10:40:03,726 - mmdet - INFO - Epoch [1][50/173]      lr: 4.995e-04, eta: 0:22:12, time: 0.546, data_time: 0.059, memory: 3795, loss_cls_pos: 1.2171, loss_loc: 0.9125, loss_cls_neg: 0.0981, loss: 2.2277
2022-07-19 10:40:09,842 - mmdet - INFO - Epoch [1][60/173]      lr: 5.994e-04, eta: 0:21:50, time: 0.611, data_time: 0.047, memory: 3795, loss_cls_pos: 1.1973, loss_loc: 0.8528, loss_cls_neg: 0.0869, loss: 2.1370
2022-07-19 10:40:15,588 - mmdet - INFO - Epoch [1][70/173]      lr: 6.993e-04, eta: 0:21:22, time: 0.575, data_time: 0.044, memory: 3795, loss_cls_pos: 1.1586, loss_loc: 0.7953, loss_cls_neg: 0.0840, loss: 2.0379
2022-07-19 10:40:20,915 - mmdet - INFO - Epoch [1][80/173]      lr: 7.992e-04, eta: 0:20:49, time: 0.532, data_time: 0.054, memory: 3795, loss_cls_pos: 1.1260, loss_loc: 0.8029, loss_cls_neg: 0.0772, loss: 2.0061
2022-07-19 10:40:26,728 - mmdet - INFO - Epoch [1][90/173]      lr: 8.991e-04, eta: 0:20:32, time: 0.580, data_time: 0.044, memory: 3872, loss_cls_pos: 1.1049, loss_loc: 0.8055, loss_cls_neg: 0.0878, loss: 1.9982
2022-07-19 10:40:32,350 - mmdet - INFO - Epoch [1][100/173]     lr: 9.990e-04, eta: 0:20:15, time: 0.563, data_time: 0.044, memory: 3872, loss_cls_pos: 1.0540, loss_loc: 0.7553, loss_cls_neg: 0.0880, loss: 1.8973
2022-07-19 10:40:38,558 - mmdet - INFO - Epoch [1][110/173]     lr: 1.099e-03, eta: 0:20:09, time: 0.616, data_time: 0.051, memory: 3872, loss_cls_pos: 1.0888, loss_loc: 0.7567, loss_cls_neg: 0.0703, loss: 1.9158
2022-07-19 10:40:43,759 - mmdet - INFO - Epoch [1][120/173]     lr: 1.199e-03, eta: 0:19:46, time: 0.511, data_time: 0.045, memory: 3872, loss_cls_pos: 1.0190, loss_loc: 0.7143, loss_cls_neg: 0.0795, loss: 1.8128
2022-07-19 10:40:50,363 - mmdet - INFO - Epoch [1][130/173]     lr: 1.299e-03, eta: 0:19:50, time: 0.674, data_time: 0.064, memory: 3872, loss_cls_pos: 0.9927, loss_loc: 0.6856, loss_cls_neg: 0.0855, loss: 1.7638
2022-07-19 10:40:55,980 - mmdet - INFO - Epoch [1][140/173]     lr: 1.399e-03, eta: 0:19:35, time: 0.548, data_time: 0.035, memory: 3872, loss_cls_pos: 0.9711, loss_loc: 0.6598, loss_cls_neg: 0.0725, loss: 1.7033
2022-07-19 10:41:02,187 - mmdet - INFO - Epoch [1][150/173]     lr: 1.499e-03, eta: 0:19:31, time: 0.626, data_time: 0.074, memory: 3872, loss_cls_pos: 0.9494, loss_loc: 0.6867, loss_cls_neg: 0.0766, loss: 1.7127
2022-07-19 10:41:07,718 - mmdet - INFO - Epoch [1][160/173]     lr: 1.598e-03, eta: 0:19:20, time: 0.563, data_time: 0.057, memory: 3872, loss_cls_pos: 0.9103, loss_loc: 0.6144, loss_cls_neg: 0.0717, loss: 1.5965
2022-07-19 10:41:13,126 - mmdet - INFO - Epoch [1][170/173]     lr: 1.698e-03, eta: 0:19:06, time: 0.541, data_time: 0.060, memory: 3872, loss_cls_pos: 0.9272, loss_loc: 0.6205, loss_cls_neg: 0.0771, loss: 1.6248
```
I try to find where it get stucked. So I set ipdb.set_trace at CheckpointHook:
```python
 def after_train_epoch(self, runner):
        if not self.by_epoch:
            return

        # save checkpoint for following cases:
        # 1. every ``self.interval`` epochs
        # 2. reach the last epoch of training
        if self.every_n_epochs(
                runner, self.interval) or (self.save_last
                                           and self.is_last_epoch(runner)):
            runner.logger.info(
                f'Saving checkpoint at {runner.epoch + 1} epochs')
            if self.sync_buffer:
                allreduce_params(runner.model.buffers())
            self._save_checkpoint(runner)

    @master_only
    def _save_checkpoint(self, runner):
        """"""Save the current checkpoint and delete unwanted checkpoint.""""""
        runner.save_checkpoint(
            self.out_dir, save_optimizer=self.save_optimizer, **self.args)
        if runner.meta is not None:
            if self.by_epoch:
                cur_ckpt_filename = self.args.get(
                    'filename_tmpl', 'epoch_{}.pth').format(runner.epoch + 1)
            else:
                cur_ckpt_filename = self.args.get(
                    'filename_tmpl', 'iter_{}.pth').format(runner.iter + 1)
            runner.meta.setdefault('hook_msgs', dict())
            runner.meta['hook_msgs']['last_ckpt'] = self.file_client.join_path(
                self.out_dir, cur_ckpt_filename)
        
        #  stop here
        ipdb.set_trace()
        # remove other checkpoints
        if self.max_keep_ckpts > 0:
            if self.by_epoch:
                name = 'epoch_{}.pth'
                current_ckpt = runner.epoch + 1
            else:
                name = 'iter_{}.pth'
                current_ckpt = runner.iter + 1
            redundant_ckpts = range(
                current_ckpt - self.max_keep_ckpts * self.interval, 0,
                -self.interval)
            filename_tmpl = self.args.get('filename_tmpl', name)
            for _step in redundant_ckpts:
                ckpt_path = self.file_client.join_path(
                    self.out_dir, filename_tmpl.format(_step))
                if self.file_client.isfile(ckpt_path):
                    self.file_client.remove(ckpt_path)
                else:
                    break
```
Surprisingly, the first epoch can be saved successfully (it was not possible before). However, it get stucked again after this. Any advice to this problem?"
FCOS + Cityscapes -> insufficient accuracy,open-mmlab/mmdetection,2022-07-13 16:33:14,8,reimplementation,8364,1303681194,"###  Question:
I use the Cityscapes dataset for testing target detection. Using the same training strategy (1 gpu) and dataset, the models are FasterRCNN, FCOS.
The FasterRCNN replication results are consistent with the official map, which is 42.
However, when using FCOS training, you can only get about 30.
The results of the two training models on COCO are approximately the same, but their performance on Cityscapes is quite different.
###  Seek help:
For Cityscapes datasets, can the authorities provide training results for more models?
Because COCO is relatively large, Cityscapes can provide faster reference in the early stages."
Heatmap question?,open-mmlab/mmdetection,2022-07-13 08:33:15,3,,8360,1303095514,Hello! Could you please tell me how to get the heatmap
How to show only precision and recall at different confidence threshold and IoU threshold,open-mmlab/mmdetection,2022-07-13 00:24:24,3,,8356,1302751056,"After every checkpoint, an evaluation on the test set is successfully completed and shows the following results. But how can we show only precision and recall (not AP and AR)? 

Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.583
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.416
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.373
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.593
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.593
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.593
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.605 

"
Mask2Former training with V100 instead of A100 GPUs,open-mmlab/mmdetection,2022-07-12 21:09:35,7,,8354,1302625753,"Hi,

Mask2Former Training Resources says `8x A100 GPUs`:

```
Collections:
  - Name: Mask2Former
    Metadata:
      Training Data: COCO
      Training Techniques:
        - AdamW
        - Weight Decay
      Training Resources: 8x A100 GPUs
```

What changes (e.g. smaller batch size per GPU) are needed if I use V100 instead of A100 GPUs?

Note: V100 has only 16GB GPU ram but A100 has 40GB or 80GB ram."
Converting Mask R-CNN results to COCO Json,open-mmlab/mmdetection,2022-07-11 14:19:48,1,,8346,1300782428,"**Hi, I am trying to convert detection results of Mask R-CNN to COCO Json using ""results2json"" function, however I receive  IndexError. Please find my config as well as the error below. Any ideas what the problem is? Thank you.**

Config:
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ResNeXt',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(
            type='Pretrained', checkpoint='open-mmlab://resnext101_32x4d'),
        groups=32,
        base_width=4),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=3,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=3,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = '/content/sample_data/material_data'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file=
        '/content/sample_data/material_data/train/_annotations.coco.json',
        img_prefix='/content/sample_data/material_data/train/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ],
        classes=('Less, 'Thick', 'Thin')),
    val=dict(
        type='CocoDataset',
        ann_file=
        '/content/sample_data/material_data/valid/_annotations.coco.json',
        img_prefix='/content/sample_data/material_data/valid/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Less', 'Thick', 'Thin')),
    test=dict(
        type='CocoDataset',
        ann_file=
        '/content/sample_data/material_data/test/_annotations.coco.json',
        img_prefix='/content/sample_data/material_data/test/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Less', 'Thick', 'Thin')))
evaluation = dict(metric=['bbox', 'segm'], interval=1)
optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup=None,
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=5)
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'checkpoints/mask_rcnn_r101_fpn_2x_coco_bbox_mAP-0.408__segm_mAP-0.366_20200505_071027-14b391c7.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
auto_scale_lr = dict(enable=False, base_batch_size=16)
classes = ('Less', 'Thick', 'Thin')
work_dir = './tutorial_exps2'
seed = 0
gpu_ids = range(0, 1)
device = 'cuda'


```
`# Run segmentation detection for bunch of images

from mmdet.apis import init_detector, inference_detector, show_result_pyplot
from google.colab.patches import cv2_imshow
from skimage.io import imread_collection

model.cfg = cfg

# path to the images
col_dir = '/content/sample_data/material_data/test/*.jpg'
#creating a collection with the available images
col = imread_collection(col_dir)
results_test=[] # save the results in this file (it has the same data as results.pkl when you load it)
for i in range(len(col)):
    name=col.files[i]
    image=mmcv.imread(name)
    result = inference_detector(model, image)
    results_test.append(result)`
```
```
#### convert  the results to json format
dataset=build_dataset(cfg.data.test)
results_test_json=dataset.results2json(results_test, '/content/sample_data/ann_testtest')

loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
[<ipython-input-49-4289b8ba4b86>](https://localhost:8080/#) in <module>()
      1 #### convert  the results to json format
      2 dataset=build_dataset(cfg.data.test)
----> 3 results_test_json=dataset.results2json(results_test, '/content/sample_data/ann_testtest')

1 frames
[/content/mmdetection/mmdet/datasets/coco.py](https://localhost:8080/#) in _segm2json(self, results)
    286                     data['score'] = float(mask_score[i])
    287                     data['category_id'] = self.cat_ids[label]
--> 288                     if isinstance(segms[i]['counts'], bytes):
    289                         segms[i]['counts'] = segms[i]['counts'].decode()
    290                     data['segmentation'] = segms[i]

IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

```"
 How do I change the name displayed on the masked object?,open-mmlab/mmdetection,2022-07-11 08:24:24,1,,8345,1300374981,"hello
I inference with coins using mask-rcnn.

here is my code


    `img_name = path_dir + '/' + file_list[i]
    img_arr= cv2.imread(img_name, cv2.IMREAD_COLOR)
    img_arr_rgb = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)
    fig= plt.figure(figsize=(12, 12))
    plt.imshow(img_arr_rgb)
    
            
    results = inference_detector(model, img_arr)
    
    ##save result
    model.show_result(img_arr, results, score_thr=0.8, title=  bbox_color=(0,0,255),thickness=0.5,font_size=7, out_file= f'{save_dir1}{file_list[i]}')
`
 
What I am curious about is that
I want to change the name of the bounding box
I want to assign random variables instead of class name and confidence score.

How can I do this?

Do I have to change mmdetection/mmdet/apis/inference.py/inference_detector function? or show_result_pyplot function?

I would be very grateful if you could give me example code.

thank you
![image](https://user-images.githubusercontent.com/66056440/178220999-35a781fd-ad80-4000-9c8d-2d1eea46467d.png)
"
Code location for calculating mask IOU,open-mmlab/mmdetection,2022-07-11 03:03:15,1,,8344,1300140088,"Where does your code calculate mask IOU. Because I want to know how you transform the annotation data in the coco dataset and the result obtained by mmdetection into the same type of data.
"
How to supervise two head branches using one loss function?,open-mmlab/mmdetection,2022-07-11 02:53:03,1,reimplementation,8343,1300135542,"I added an iou prediction branch to the head of the detector, and then I calculated the loss of the output of the bbox and the iou prediction branch using one loss function. However, the following error occurs during training:

RuntimeError: CUDA error: device-side assert triggered
terminate called after throwing an instance of 'c10::Error'
what(): CUDA error: device-side assert triggered

How should I train two branches with one loss function? Thanks"
YoloV7？,open-mmlab/mmdetection,2022-07-09 02:51:43,1,feature request,8333,1299553641,"https://github.com/WongKinYiu/yolov7
https://arxiv.org/abs/2207.02696
Will you support it in the future?"
Where is the tool for writing detection results to a json files,open-mmlab/mmdetection,2022-07-08 02:24:40,1,,8328,1298394732,"Preciously, using /mmdet/core/evaluation/coco_utils.py can write detection results to a json file as coco format.But now I can find the file and can't use the function.Can anyone help me."
Dockerfile build failed,open-mmlab/mmdetection,2022-07-08 00:36:30,3,,8327,1298314228,"### Bug and how to reproduce
First get the Dockerfile
```
wget https://raw.githubusercontent.com/open-mmlab/mmdetection/master/docker/Dockerfile
```
Then build
```
docker built -t mmdetection .
```
It would error 
```
Sending build context to Docker daemon   2.56kB
Step 1/16 : ARG PYTORCH=""1.6.0""
Step 2/16 : ARG CUDA=""10.1""
Step 3/16 : ARG CUDNN=""7""
Step 4/16 : FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel
1.6.0-cuda10.1-cudnn7-devel: Pulling from pytorch/pytorch
Digest: sha256:ccebb46f954b1d32a4700aaeae0e24bd68653f92c6f276a608bf592b660b63d7
Status: Downloaded newer image for pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel
 ---> bb833e4d631f
Step 5/16 : ENV TORCH_CUDA_ARCH_LIST=""6.0 6.1 7.0+PTX""
 ---> Using cache
 ---> f281e77f7b35
Step 6/16 : ENV TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""
 ---> Using cache
 ---> a9d14377a7fa
Step 7/16 : ENV CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../""
 ---> Using cache
 ---> bf1aafb55cd1
Step 8/16 : RUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6     && apt-get clean     && rm -rf /var/lib/apt/lists/*
 ---> Running in 76ba14c38bbb
Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]
Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease
Get:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]
Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]
Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]
Err:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC
Get:6 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]
Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [73.8 kB]
Get:8 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.8 kB]
Get:9 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1521 kB]
Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
Get:11 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2871 kB]
Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1031 kB]
Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]
Get:14 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]
Get:15 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]
Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]
Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]
Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2298 kB]
Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3302 kB]
Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [29.8 kB]
Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1075 kB]
Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [12.9 kB]
Get:23 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [12.2 kB]
Reading package lists...
W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC
E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.
The command '/bin/sh -c apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6     && apt-get clean     && rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100
```

### Environment
* system: centos7

### Fix
working on a fix now
"
Getting Top K bounding box predictions,open-mmlab/mmdetection,2022-07-07 21:40:04,1,,8326,1298132910,"Hi,

I'm running ```inference_detector(model, image)``` which returns the top scoring bounding box.

How can I get the 'top k' bounding boxes and their scores?
"
"Augmented test raises exception, shape '[-1, 4]' is invalid for input of size 485",open-mmlab/mmdetection,2022-07-07 13:06:31,0,,8324,1297409138,"**Checklist**

- [x] 1. I have searched related issues but cannot get the expected help.
- [x] 2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
- [x] 3. The bug has not been fixed in the latest version.

**Describe the bug**
Simple demo run with Yolo model succeeds with simple_test but raises error with augmented test. 

**Reproduction**

If I simply run the demo script as below, it succeeds without error:.

```bash
python image_demo.py demo.jpg ../configs/yolo/yolov3_d53_fp16_mstrain-608_273e_coco.py yolov3_d53_fp16_mstrain-608_273e_coco_20210517_213542-4bc34944.pth --out-file out.jpg
```

But if I add multiple scales to the test pipeline, as in `configs/yolo/yolov3_d53_mstrain-608_273e_coco.py`, it throws the error with traceback in below.

```python
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        # img_scale=(608, 608),
        img_scale=[(608, 608), (1216,1216)],
        flip=False,
        transforms=[
```


**Environment**

Tested in clean conda environment, with mmcv-full=1.5.3 installed with `mim` and mmdetection newly cloned at `56e42e72` and installed in develop mode.

```
Python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]
CUDA available: True
GPU 0: Tesla T4
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 11.0, V11.0.221
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.11.0
TorchVision: 0.12.0
OpenCV: 4.6.0
MMCV: 1.5.3
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.25.0+56e42e7
```

**Error traceback**

```none
Traceback (most recent call last):
  File ""image_demo.py"", line 68, in <module>
    main(args)
  File ""image_demo.py"", line 36, in main
    result = inference_detector(model, args.img)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/apis/inference.py"", line 151, in inference_detector
    results = model(return_loss=False, rescale=True, **data)
  File ""/home/ubuntu/anaconda3/envs/mmdet_dev/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/mmdet_dev/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py"", line 116, in new_func
    return old_func(*args, **kwargs)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/models/detectors/base.py"", line 174, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/models/detectors/base.py"", line 154, in forward_test
    return self.aug_test(imgs, img_metas, **kwargs)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/models/detectors/single_stage.py"", line 134, in aug_test
    feats, img_metas, rescale=rescale)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/models/dense_heads/yolo_head.py"", line 510, in aug_test
    return self.aug_test_bboxes(feats, img_metas, rescale=rescale)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/models/dense_heads/dense_test_mixins.py"", line 91, in aug_test_bboxes
    aug_bboxes, aug_scores, img_metas)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/models/dense_heads/dense_test_mixins.py"", line 199, in merge_aug_bboxes
    flip_direction)
  File ""/home/ubuntu/workspace/dev/mmdetection/mmdet/core/bbox/transforms.py"", line 71, in bbox_mapping_back
    new_bboxes = new_bboxes.view(-1, 4) / new_bboxes.new_tensor(scale_factor)
RuntimeError: shape '[-1, 4]' is invalid for input of size 485
```

"
AssertionError: TensorRT plugin should be compiled. </pre>,open-mmlab/mmdetection,2022-07-07 09:43:33,0,,8322,1297110726,"
Hi, When I use ""python tools/deployment/onnx2tensorrt.py""  to convert the onnx file to trt, it appears ""TensorRT plugin should be compiled."""
Pipeline can't work if the negative sample image don't have box,open-mmlab/mmdetection,2022-07-07 07:42:10,0,,8321,1296969775,"**Describe the bug**
I trained my custom data with model yolox, and find the error in trainpipeline. 

**Reproduction**

1. What command or script did you run?
I run my custom config file.
```none
python ../root/mmdetection/tools/train.py ../root/mmdetection/configs/yolox/myyoloxdict.py
```
2. Did you make any modifications on the code or config? Did you understand what you have modified?
I add some print function in the `transform.py` and tried to unsequeeze the array in the `transform.py`

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
```
fatal: not a git repository (or any parent up to mount point /)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla V100S-PCIE-32GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.1, V10.1.24
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.7.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.8.1
OpenCV: 4.6.0
MMCV: 1.5.2
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.0
MMDetection: 2.25.0+
```
2. You may add addition that may be helpful for locating the problem, such as
I add some print function, and print the `mosaic_transform`
```
def _mosaic_transform(self, results):
        """"""Mosaic transform function.

        Args:
            results (dict): Result dict.

        Returns:
            dict: Updated result dict.
        """"""

        assert 'mix_results' in results
        mosaic_labels = []
        mosaic_bboxes = []
        if len(results['img'].shape) == 3:
            mosaic_img = np.full(
                (int(self.img_scale[0] * 2), int(self.img_scale[1] * 2), 3),
                self.pad_val,
                dtype=results['img'].dtype)
        else:
            mosaic_img = np.full(
                (int(self.img_scale[0] * 2), int(self.img_scale[1] * 2)),
                self.pad_val,
                dtype=results['img'].dtype)

        # mosaic center x, y
        center_x = int(
            random.uniform(*self.center_ratio_range) * self.img_scale[1])
        center_y = int(
            random.uniform(*self.center_ratio_range) * self.img_scale[0])
        center_position = (center_x, center_y)

        loc_strs = ('top_left', 'top_right', 'bottom_left', 'bottom_right')
        for i, loc in enumerate(loc_strs):
            if loc == 'top_left':
                results_patch = copy.deepcopy(results)
            else:
                results_patch = copy.deepcopy(results['mix_results'][i - 1])

            img_i = results_patch['img']
            h_i, w_i = img_i.shape[:2]
            # keep_ratio resize
            scale_ratio_i = min(self.img_scale[0] / h_i,
                                self.img_scale[1] / w_i)
            img_i = mmcv.imresize(
                img_i, (int(w_i * scale_ratio_i), int(h_i * scale_ratio_i)))

            # compute the combine parameters
            paste_coord, crop_coord = self._mosaic_combine(
                loc, center_position, img_i.shape[:2][::-1])
            x1_p, y1_p, x2_p, y2_p = paste_coord
            x1_c, y1_c, x2_c, y2_c = crop_coord

            # crop and paste image
            mosaic_img[y1_p:y2_p, x1_p:x2_p] = img_i[y1_c:y2_c, x1_c:x2_c]

            # adjust coordinate
            gt_bboxes_i = results_patch['gt_bboxes']
            gt_labels_i = results_patch['gt_labels']

            if gt_bboxes_i.shape[0] > 0:
                padw = x1_p - x1_c
                padh = y1_p - y1_c
                gt_bboxes_i[:, 0::2] = \
                    scale_ratio_i * gt_bboxes_i[:, 0::2] + padw
                gt_bboxes_i[:, 1::2] = \
                    scale_ratio_i * gt_bboxes_i[:, 1::2] + padh

            mosaic_bboxes.append(gt_bboxes_i)
            mosaic_labels.append(gt_labels_i)
            print('mosaic',mosaic_bboxes)
            for i in mosaic_bboxes:
                print('i',i)
                if len(i) == 0:
                    i = np.expand_dims(i,0)

        if len(mosaic_labels) > 0:
            mosaic_bboxes = np.concatenate(mosaic_bboxes, 0)
            mosaic_labels = np.concatenate(mosaic_labels, 0)

            if self.bbox_clip_border:
                mosaic_bboxes[:, 0::2] = np.clip(mosaic_bboxes[:, 0::2], 0,
                                                 2 * self.img_scale[1])
                mosaic_bboxes[:, 1::2] = np.clip(mosaic_bboxes[:, 1::2], 0,
                                                 2 * self.img_scale[0])

            if not self.skip_filter:
                mosaic_bboxes, mosaic_labels = \
                    self._filter_box_candidates(mosaic_bboxes, mosaic_labels)
```

and it print like this followed:

```
mosaic [array([], dtype=float32), array([], dtype=float32), array([[ 84.125   , 433.0127  ,  84.02344 , 433.02148 ],
       [ 84.140625, 433.26172 ,  84.03516 , 433.02344 ],
       [ 84.06836 , 433.4287  ,  84.01953 , 433.0254  ],
       [ 84.12598 , 433.4707  ,  84.01758 , 433.02344 ],
       [ 84.12012 , 433.51855 ,  84.021484, 433.01758 ],
       [ 84.08594 , 433.34375 ,  84.01953 , 433.01562 ],
       [ 84.09766 , 433.36426 ,  84.01953 , 433.0254  ],
       [ 84.04199 , 433.38086 ,  84.02539 , 433.01953 ],
       [ 84.15332 , 433.2793  ,  84.02539 , 433.01953 ],
       [ 84.17676 , 433.29395 ,  84.021484, 433.02148 ],
       [ 84.24219 , 433.28418 ,  84.01953 , 433.02148 ],
       [ 84.271484, 433.11914 ,  84.046875, 433.04297 ],
       [ 84.14746 , 433.58398 ,  84.03711 , 433.03125 ],
       [ 84.56348 , 433.48047 ,  84.0332  , 433.03906 ]], dtype=float32), array([[596.3721 , 433.1211 , 596.0293 , 433.01953]], dtype=float32)]
i []
i []
i [[ 84.125    433.0127    84.02344  433.02148 ]
 [ 84.140625 433.26172   84.03516  433.02344 ]
 [ 84.06836  433.4287    84.01953  433.0254  ]
 [ 84.12598  433.4707    84.01758  433.02344 ]
 [ 84.12012  433.51855   84.021484 433.01758 ]
 [ 84.08594  433.34375   84.01953  433.01562 ]
 [ 84.09766  433.36426   84.01953  433.0254  ]
 [ 84.04199  433.38086   84.02539  433.01953 ]
 [ 84.15332  433.2793    84.02539  433.01953 ]
 [ 84.17676  433.29395   84.021484 433.02148 ]
 [ 84.24219  433.28418   84.01953  433.02148 ]
 [ 84.271484 433.11914   84.046875 433.04297 ]
 [ 84.14746  433.58398   84.03711  433.03125 ]
 [ 84.56348  433.48047   84.0332   433.03906 ]]
```
and I guess the empty array is the point of the error, could you give me some advice to let the empty array have two-dimension?

**Error traceback**
```
Original Traceback (most recent call last):
  File ""/root/.local/conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 198, in _worker_loop
    data = fetcher.fetch(index)
  File ""/root/.local/conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/root/.local/conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/root/mmdetection/mmdet/datasets/dataset_wrappers.py"", line 431, in __getitem__
    updated_results = transform(copy.deepcopy(results))
  File ""/root/mmdetection/mmdet/datasets/pipelines/transforms.py"", line 2042, in __call__
    results = self._mosaic_transform(results)
  File ""/root/mmdetection/mmdet/datasets/pipelines/transforms.py"", line 2134, in _mosaic_transform
    mosaic_bboxes = np.concatenate(mosaic_bboxes, 0)
  File ""<__array_function__ internals>"", line 6, in concatenate
ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 2 has 2 dimension(s)
```

```none
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
Unexpected keyword argument,open-mmlab/mmdetection,2022-07-07 07:06:00,0,,8320,1296932820,"Hi, I'm trying to add an extra _initial_loss_cls_ argument to _model= dict(....)_ but the model reports an error:  ___init__() got an unexpected keyword argument 'initial_loss_cls'_.
How do I handle this? Thank you!"
How to make the hyper-parameters of the model gradually increase with the iteration of training?,open-mmlab/mmdetection,2022-07-07 00:23:33,0,,8318,1296626610,"Hello, How to make the hyper-parameters gradually increase when we train the model in mmdetection? I mean, how can I get the epoch or iteration information during the training process. For example, when I implement the Gridmask, I want to change the mask ratio gradually. Or I want to change the match cost in the sample assignment process, how could I do it?"
Issue while installing MMCV and MMDET,open-mmlab/mmdetection,2022-07-06 19:49:15,1,,8316,1296311163,"Thanks for your error report and we appreciate it a lot.

**Describe the bug**
Not able to set up the environment for mmdetection.  I installed all the prerequisite libraries.
`pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html`
`pip install tqdm`
`pip install torchpack`
`pip install mmcv-full==1.4 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html`
`pip install mmdet==2.22.0`

**Reproduction**

1. What command or script did you run?

```
Running setup.py after installing all the above packages, results in an error.
```

3. Did you make any modifications on the code or config? Did you understand what you have modified?
No

5. What dataset did you use?
I am using nuscenes but before that, I am not able to build the environment to run create_Data.py

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
```
Collecting environment information...
PyTorch version: 1.9.0+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0
Libc version: glibc-2.9

Python version: 3.6.10 |Anaconda, Inc.| (default, Mar 23 2020, 23:13:11)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-debian-buster-sid
Is CUDA available: True
CUDA runtime version: 11.0.167
GPU models and configuration: GPU 0: NVIDIA A100-PCIE-40GB
Nvidia driver version: 510.73.08
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip] crumpets-torch==3.0.0a5+cuda110
[pip] msgpack-numpy==0.4.8
[pip] numpy==1.18.1
[pip] pytorch-ignite==0.4.1
[pip] pytorch-transformers==1.1.0
[pip] torch==1.9.0+cu111
[pip] torch-geometric==1.6.0
[pip] torchaudio==0.9.0
[pip] torchpack==0.3.1
[pip] torchtext==0.6.0
[pip] torchvision==0.10.0+cu111
[conda] crumpets-torch            3.0.0a5+cuda110          pypi_0    pypi
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] msgpack-numpy             0.4.8                    pypi_0    pypi
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.18.1           py36h94c655d_0  
[conda] numpy-base                1.18.1           py36h2f8d375_1  
[conda] pytorch-ignite            0.4.1                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0+cu111              pypi_0    pypi
[conda] torch-geometric           1.6.0                    pypi_0    pypi
[conda] torchaudio                0.9.0                    pypi_0    pypi
[conda] torchpack                 0.3.1                    pypi_0    pypi
[conda] torchtext                 0.6.0                    pypi_0    pypi
[conda] torchvision               0.10.0+cu111             pypi_0    pypi
```
3. You may add addition that may be helpful for locating the problem, such as
 Already mentioned above.

**Error traceback**
If applicable, paste the error trackback here.

```
*********************************calling setup.py*********************************
running develop
running egg_info
creating mmdet3d.egg-info
writing mmdet3d.egg-info/PKG-INFO
writing dependency_links to mmdet3d.egg-info/dependency_links.txt
writing top-level names to mmdet3d.egg-info/top_level.txt
writing manifest file 'mmdet3d.egg-info/SOURCES.txt'
reading manifest file 'mmdet3d.egg-info/SOURCES.txt'
writing manifest file 'mmdet3d.egg-info/SOURCES.txt'
running build_ext
building 'mmdet3d.ops.spconv.sparse_conv_ext' extension
creating /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build
creating /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6
creating /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d
creating /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops
creating /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv
creating /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src
Emitting ninja build file /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/7] /usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/reordering_cuda.cu -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/reordering_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -w -std=c++14 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/reordering_cuda.o 
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/reordering_cuda.cu -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/reordering_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -w -std=c++14 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
nvcc fatal   : Unsupported gpu architecture 'compute_86'
[2/7] /usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/maxpool_cuda.cu -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/maxpool_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -w -std=c++14 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/maxpool_cuda.o 
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/maxpool_cuda.cu -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/maxpool_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -w -std=c++14 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
nvcc fatal   : Unsupported gpu architecture 'compute_86'
[3/7] /usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/indice_cuda.cu -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/indice_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -w -std=c++14 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
FAILED: /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/indice_cuda.o 
/usr/local/cuda/bin/nvcc  -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/indice_cuda.cu -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/indice_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''""'""'-fPIC'""'""'' -w -std=c++14 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
nvcc fatal   : Unsupported gpu architecture 'compute_86'
[4/7] c++ -MMD -MF /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/reordering.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/reordering.cc -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/reordering.o -w -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
[5/7] c++ -MMD -MF /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/maxpool.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/maxpool.cc -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/maxpool.o -w -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
[6/7] c++ -MMD -MF /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/indice.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/indice.cc -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/indice.o -w -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
[7/7] c++ -MMD -MF /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/all.o.d -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -DWITH_CUDA -I/netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/include -I/opt/conda/lib/python3.6/site-packages/torch/include -I/opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/lib/python3.6/site-packages/torch/include/TH -I/opt/conda/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/include/python3.6m -c -c /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/mmdet3d/ops/spconv/src/all.cc -o /netscratch/mishra/Thesis_Fusion/BEVFusion/BEVFusion_Baseline/bevfusion/build/temp.linux-x86_64-3.6/mmdet3d/ops/spconv/src/all.o -w -std=c++14 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=""_gcc""' '-DPYBIND11_STDLIB=""_libstdcpp""' '-DPYBIND11_BUILD_ABI=""_cxxabi1011""' -DTORCH_EXTENSION_NAME=sparse_conv_ext -D_GLIBCXX_USE_CXX11_ABI=0
cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
ninja: build stopped: subcommand failed.
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 1672, in _run_ninja_build
    env=env)
  File ""/opt/conda/lib/python3.6/subprocess.py"", line 438, in run
    output=stdout, stderr=stderr)
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""setup.py"", line 161, in <module>
    zip_safe=False,
  File ""/opt/conda/lib/python3.6/site-packages/setuptools/__init__.py"", line 161, in setup
    return distutils.core.setup(**attrs)
  File ""/opt/conda/lib/python3.6/distutils/core.py"", line 148, in setup
    dist.run_commands()
  File ""/opt/conda/lib/python3.6/distutils/dist.py"", line 955, in run_commands
    self.run_command(cmd)
  File ""/opt/conda/lib/python3.6/distutils/dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""/opt/conda/lib/python3.6/site-packages/setuptools/command/develop.py"", line 38, in run
    self.install_for_development()
  File ""/opt/conda/lib/python3.6/site-packages/setuptools/command/develop.py"", line 140, in install_for_development
    self.run_command('build_ext')
  File ""/opt/conda/lib/python3.6/distutils/cmd.py"", line 313, in run_command
    self.distribution.run_command(command)
  File ""/opt/conda/lib/python3.6/distutils/dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""/opt/conda/lib/python3.6/site-packages/setuptools/command/build_ext.py"", line 87, in run
    _build_ext.run(self)
  File ""/opt/conda/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 186, in run
    _build_ext.build_ext.run(self)
  File ""/opt/conda/lib/python3.6/distutils/command/build_ext.py"", line 339, in run
    self.build_extensions()
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 709, in build_extensions
    build_ext.build_extensions(self)
  File ""/opt/conda/lib/python3.6/site-packages/Cython/Distutils/old_build_ext.py"", line 195, in build_extensions
    _build_ext.build_ext.build_extensions(self)
  File ""/opt/conda/lib/python3.6/distutils/command/build_ext.py"", line 448, in build_extensions
    self._build_extensions_serial()
  File ""/opt/conda/lib/python3.6/distutils/command/build_ext.py"", line 473, in _build_extensions_serial
    self.build_extension(ext)
  File ""/opt/conda/lib/python3.6/site-packages/setuptools/command/build_ext.py"", line 208, in build_extension
    _build_ext.build_extension(self, ext)
  File ""/opt/conda/lib/python3.6/distutils/command/build_ext.py"", line 533, in build_extension
    depends=ext.depends)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 539, in unix_wrap_ninja_compile
    with_cuda=with_cuda)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 1360, in _write_ninja_file_and_compile_objects
    error_prefix='Error compiling objects for extension')
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/cpp_extension.py"", line 1682, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error compiling objects for extension
```

"
TypeError: 'int' object is not subscriptable,open-mmlab/mmdetection,2022-07-06 14:14:25,0,,8315,1295906954,"I ran get_flops.py to test SCNet, but the result shows ""TypeError: 'int' object is not subscriptable"". How should I debug?Thank you~

**Error traceback**
Traceback (most recent call last):
  File ""tools/analysis_tools/get_flops.py"", line 97, in <module>
    main()
  File ""tools/analysis_tools/get_flops.py"", line 82, in main
    flops, params = get_model_complexity_info(model, input_shape)
  File ""/home/scsc01/anaconda3/envs/lxx220/lib/python3.7/site-packages/mmcv/cnn/utils/flops_counter.py"", line 105, in get_model_complexity_info
    _ = flops_model(batch)
  File ""/home/scsc01/anaconda3/envs/lxx220/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1071, in _call_impl
    result = forward_call(*input, **kwargs)
  File ""/home/scsc01/lxx220/mmdet/models/detectors/two_stage.py"", line 86, in forward_dummy
    roi_outs = self.roi_head.forward_dummy(x, proposals)
  File ""/home/scsc01/lxx220/mmdet/models/roi_heads/cascade_roi_head.py"", line 126, in forward_dummy
    mask_results = self._mask_forward(i, x, mask_rois)
  File ""/home/scsc01/lxx220/mmdet/models/roi_heads/scnet_roi_head.py"", line 133, in _mask_forward
    x[:self.mask_roi_extractor.num_inputs], rois)
TypeError: 'int' object is not subscriptable"
How to use ClassAwareSampler?,open-mmlab/mmdetection,2022-07-06 13:39:01,0,,8314,1295858762,"When use ClassAwareSampler, 'dataset must have `get_cat2imgs` function'. Can you tell me where to add `get_cat2imgs`?"
How to supervise two head branches using one loss function?,open-mmlab/mmdetection,2022-07-06 12:05:13,0,reimplementation,8313,1295737981,"I added an iou prediction branch to the head of the detector, and then I calculated the loss of the output of the bbox and the iou prediction branch using one loss function. However, the following error occurs during training:

RuntimeError: CUDA error: device-side assert triggered
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered

How should I train two branches with one loss function? Thanks"
YOLOv3: iou_calculation and label assign,open-mmlab/mmdetection,2022-07-06 05:24:26,0,,8312,1295163080,"1. iou_calculation:
    官方版本中, 在计算 anchor与gt_bbox 的iou大小时, 只考虑两者的w/h是否匹配(即: 视两者的中心点在同一位置);
    mmdet中, 会将中心点的差异也参与到iou的计算中;
2. label assign
    因为上述iou计算方式的不同, 会导致后续label assign(正负样本分配)存在一定给的差异;

望得到回复, 谢谢!
"
Can anyone explain how to implement CASCADE MASK R C NNN on custom dataset?,open-mmlab/mmdetection,2022-07-06 00:00:46,3,,8311,1294913444,
One suggestion for MinIoURandomCrop,open-mmlab/mmdetection,2022-07-05 12:24:33,0,,8309,1294228856,"https://github.com/open-mmlab/mmdetection/blob/56e42e72cdf516bebb676e586f408b98f854d84c/mmdet/datasets/pipelines/transforms.py#L1209
In lines 1209 to 1214 of MinIoURandomCrop, if  getting new_w and new_h every time and then judge whether their ratio meets the requirements, in extreme cases, all of them miss target and finally the function will jump out. This will waste a lot of time. 

There is an implementation：First of all, we randomly sample from the range we want, and then calculate new_h and new_w, so that one less continue is written. Here is the code sample：
```
h2w = random.uniform(0.5, 2)
if h2w>1:
    new_h = random.uniform(self.min_crop_size * h, h)
    new_w = new_h / h2w
else:
    new_w = random.uniform(self.min_crop_size * w, w)
    new_h = new_w * h2w
```
"
Which backend can be selected when datasets are stored in Minio,open-mmlab/mmdetection,2022-07-05 09:00:10,0,,8308,1294002833,Which backend can be selected when datasets are stored in Minio？
RuntimeError: Distributed package doesn't have NCCL built in,open-mmlab/mmdetection,2022-07-05 08:32:07,5,,8307,1293970862,"
**Describe the bug**
Benchmarking script breaks on Jetson Xavier NX & Jetson TX2 with error message `RuntimeError: Distributed package doesn't have NCCL built in`.

**Reproduction**
After clean install of mmdetection following the best practices guide:

```none
python3 -m torch.distributed.launch --nproc_per_node=1 tools/analysis_tools/benchmark.py ""$CFG"" ""$WEIGHTS"" --launcher pytorch
```

**Environment**
Docker image built from `docker run --rm -ti --runtime nvidia nvcr.io/nvidia/l4t-ml:r32.6.1-py3`, manually installing everythin else on top. Also seen to happen outside docker environment. The Docker machine yielded the following env:

<details>
  <summary> Click to expand </summary>

```
root@nvidia-desktop:/mmdetection# python3 mmdet/utils/collect_env.py
sys.platform: linux
Python: 3.6.9 (default, Jan 26 2021, 15:33:00) [GCC 8.4.0]
CUDA available: True
GPU 0: Xavier
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.300
GCC: aarch64-linux-gnu-gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.9.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.5
  - C++ Version: 201402
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: NO AVX
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_53,code=sm_53;-gencode;arch=compute_62,code=sm_62;-gencode;arch=compute_72,code=sm_72
  - CuDNN 8.2.1
    - Built with CuDNN 8.0
  - Build settings: BLAS_INFO=open, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=8.0.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -DMISSING_ARM_VST1 -DMISSING_ARM_VLD1 -Wno-stringop-overflow, FORCE_FALLBACK_CUDA_MPI=1, LAPACK_INFO=open, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=OFF, USE_MPI=ON, USE_NCCL=0, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.0a0+300a8a4
OpenCV: 4.5.0
MMCV: 1.5.3
MMCV Compiler: GCC 7.5
MMCV CUDA Compiler: 10.2
MMDetection: 2.25.0+56e42e7
```
</details>


**Error traceback**
<details>
  <summary> Click to expand </summary>

```
/usr/local/lib/python3.6/dist-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension:
  warn(f""Failed to load image Python extension: {e}"")
Traceback (most recent call last):
  File ""tools/analysis_tools/benchmark.py"", line 195, in <module>
    main()
  File ""tools/analysis_tools/benchmark.py"", line 187, in main
    init_dist(args.launcher, **cfg.dist_params)
  File ""/home/catec/.local/lib/python3.6/site-packages/mmcv/runner/dist_utils.py"", line 41, in init_dist
    _init_dist_pytorch(backend, **kwargs)
  File ""/home/catec/.local/lib/python3.6/site-packages/mmcv/runner/dist_utils.py"", line 64, in _init_dist_pytorch
    dist.init_process_group(backend=backend, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py"", line 510, in init_process_group
    timeout=timeout))
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py"", line 597, in _new_process_group_helper
    raise RuntimeError(""Distributed package doesn't have NCCL ""
RuntimeError: Distributed package doesn't have NCCL built in
Killing subprocess 21275
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 340, in <module>
    main()
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'tools/analysis_tools/benchmark.py', '--local_rank=0', '../jetson_nano/retinanet_swin-t-p4-w7_fpn_1x_coco_AIRPLANE/retinanet_swin-t-p4-w7_fpn_1x_coco_AIRPLANE.py', '../jetson_nano/retinanet_swin-t-p4-w7_fpn_1x_coco_AIRPLANE/epoch_100.pth', '--launcher', 'pytorch']' returned non-zero exit status 1.
```

</details>


**Bug fix**
Adding the (+) line in the following file overcomes the issue so that the benchmarking can continue:
```
$ sudo vim /usr/local/lib/python3.6/dist-packages/torch/distributed/distributed_c10d.py
  
  prefix_store = PrefixStore(group_name, store)
+ backend = ""gloo""
  if backend == Backend.GLOO:
      pg = ProcessGroupGloo(
          prefix_store,
          rank,
          world_size,
          timeout=timeout)
      _pg_map[pg] = (Backend.GLOO, store)
      _pg_names[pg] = group_name
  elif backend == Backend.NCCL:
      if not is_nccl_available():
          raise RuntimeError(""Distributed package doesn't have NCCL ""
                             ""built in"")

```
This fix was taken from [here](https://www.zhaoyabo.com/?p=8251) and reposted for visibility.


"
Visualization of FasterRCNN graph from Tensorboard,open-mmlab/mmdetection,2022-07-04 15:37:56,3,,8304,1293323352,"Hi, 

First of all, thanks for dev of mmdetection, this seems to be a great tool to focus on design of advanced architecture for detection task family. 

## Bug description

I am trying to visualize [FASTER RCNN](https://github.com/open-mmlab/mmdetection/tree/master/configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py) model **graph** with Tensorboard. 
I am not able to properly run the line `write.add_graph` to send the model to tensorboard.

## Reproduction

```python
from torch.utils.tensorboard import SummaryWriter
from mmdet.models import build_detector
from mmcv import Config
from tests.test_models.test_forward import _demo_mm_inputs

cfg_model = ""configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py""
cfg = Config.fromfile(cfg_model)

model = cfg.model
model['pretrained'] = None

writer = SummaryWriter(""tensorboard_runs/faster_rcnn"")

detector = build_detector(model)
input_shape = (1, 3, 256, 256)

# Test forward train with a non-empty truth batch
mm_inputs = _demo_mm_inputs(input_shape, num_items=[10])
imgs = mm_inputs.pop('imgs')
img_metas = mm_inputs.pop('img_metas')
gt_bboxes = mm_inputs['gt_bboxes']
gt_labels = mm_inputs['gt_labels']
gt_masks = mm_inputs['gt_masks']
losses = detector.eval().forward(
    imgs,
    img_metas,
    gt_bboxes=gt_bboxes,
    gt_labels=gt_labels,
    gt_masks=gt_masks,
    return_loss=True,
    )

writer.add_graph(detector.eval(),(imgs, img_metas,gt_bboxes,gt_labels,gt_masks))
writer.close()
```

the code is strongly based on issue #5140. 

## Environment

```
sys.platform: linux
Python: 3.8.8 (default, Feb 24 2021, 21:46:12) [GCC 7.3.0]
CUDA available: True
GPU 0: NVIDIA RTX A4000 Laptop GPU
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.1.TC455_06.29190527_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.8.1
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.9.1
OpenCV: 4.6.0
MMCV: 1.3.17
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.25.0+56e42e7
```

Installed from Dockerfile which is strongly based on the [repo Dockerfile](https://github.com/open-mmlab/mmdetection/blob/master/docker/Dockerfile). 
I have done minor changes, here the list :

- Change PYTORCH/CUDA version in image and mmcv-full install to match a compatible version for my GPU.
- Add new public key of nvidia apt-get repo.
- Tensorboard installation
- Add `script.py` which is code written above.

```Dockerfile
ARG PYTORCH=""1.8.1""
ARG CUDA=""11.1""
ARG CUDNN=""8""

FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel

ENV TORCH_CUDA_ARCH_LIST=""6.0 6.1 7.0+PTX""
ENV TORCH_NVCC_FLAGS=""-Xfatbin -compress-all""
ENV CMAKE_PREFIX_PATH=""$(dirname $(which conda))/../""

RUN rm /etc/apt/sources.list.d/cuda.list \
  && rm /etc/apt/sources.list.d/nvidia-ml.list \
  && apt-key del 7fa2af80 \
  && apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/3bf863cc.pub \
  && apt-get update && apt-get install -y ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

# Install MMCV
RUN pip install --no-cache-dir --upgrade pip wheel setuptools \
  && pip install --no-cache-dir mmcv-full==1.3.17 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html

# Install MMDetection
RUN conda clean --all \
  && git clone https://github.com/open-mmlab/mmdetection.git /mmdetection
WORKDIR /mmdetection
ENV FORCE_CUDA=""1""
RUN pip install --no-cache-dir -r requirements/build.txt \
  && pip install --no-cache-dir -e .

# Install tensorboard
RUN pip install --no-cache-dir tensorboard

# Add model
COPY ./script.py .
```

## Error trackback

Basically the same output as this [comment](https://github.com/open-mmlab/mmdetection/issues/5140#issuecomment-839423892). 

```
Error occurs, No graph saved
Traceback (most recent call last):
  File ""/mmdetection/script.py"", line 33, in <module>
    writer.add_graph(detector.eval(),(imgs, img_metas,gt_bboxes,gt_labels,gt_masks))
  File ""/opt/conda/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py"", line 723, in add_graph
    self._get_file_writer().add_graph(graph(model, input_to_model, verbose))
  File ""/opt/conda/lib/python3.8/site-packages/torch/utils/tensorboard/_pytorch_graph.py"", line 292, in graph
    raise e
  File ""/opt/conda/lib/python3.8/site-packages/torch/utils/tensorboard/_pytorch_graph.py"", line 286, in graph
    trace = torch.jit.trace(model, args)
  File ""/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py"", line 733, in trace
    return trace_module(
  File ""/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py"", line 934, in trace_module
    module._c._create_method_from_trace(
RuntimeError: Tracer cannot infer type of (tensor([[[[0.5488, 0.7152, 0.6028,  ..., 0.7487, 0.9037, 0.0834],
          [0.5522, 0.5845, 0.9619,  ..., 0.4461, 0.1046, 0.3485],
          [0.7401, 0.6805, 0.6224,  ..., 0.9450, 0.9919, 0.3767],
          ...,
          [0.5347, 0.1355, 0.3433,  ..., 0.9448, 0.5501, 0.4152],
          [0.6550, 0.8695, 0.7427,  ..., 0.7006, 0.1240, 0.1202],
          [0.9689, 0.2844, 0.2874,  ..., 0.6264, 0.0966, 0.8441]],

         [[0.1356, 0.8424, 0.9262,  ..., 0.9498, 0.7877, 0.2874],
          [0.8619, 0.4222, 0.5368,  ..., 0.5023, 0.0962, 0.6300],
          [0.8680, 0.9831, 0.6594,  ..., 0.0459, 0.6224, 0.2136],
          ...,
          [0.4898, 0.7893, 0.4492,  ..., 0.6242, 0.3400, 0.5197],
          [0.0792, 0.1700, 0.2699,  ..., 0.7916, 0.3890, 0.6352],
          [0.6384, 0.0778, 0.4447,  ..., 0.3807, 0.9440, 0.1556]],

         [[0.4450, 0.7703, 0.7811,  ..., 0.3255, 0.2211, 0.4891],
          [0.7809, 0.8071, 0.4325,  ..., 0.4420, 0.6906, 0.9553],
          [0.1343, 0.5162, 0.6405,  ..., 0.7125, 0.6389, 0.4962],
          ...,
          [0.5266, 0.0853, 0.4650,  ..., 0.7655, 0.7202, 0.9694],
          [0.1602, 0.1703, 0.0182,  ..., 0.4301, 0.1518, 0.9542],
          [0.1611, 0.1013, 0.6607,  ..., 0.0081, 0.9399, 0.8611]]]],
       requires_grad=True), [{'img_shape': (256, 256, 3), 'ori_shape': (256, 256, 3), 'pad_shape': (256, 256, 3), 'filename': '<demo>.png', 'scale_factor': array([1.1, 1.2, 1.1, 1.2]), 'flip': False, 'flip_direction': None}], [tensor([[  0.0000,   0.0000,  62.4313, 140.5942],
        [ 91.9011, 133.2158,  98.5872, 170.8792],
        [  0.0000, 161.8527,  92.9992, 207.3777],
        [  0.0000, 129.3509, 131.0759, 211.6468],
        [ 17.0086, 173.0515,  75.2071, 249.1510],
        [  0.0000,   0.0000, 132.8615,  88.4271],
        [  0.0000, 154.4528,  94.0470, 156.2830],
        [  0.0000,   0.0000, 117.2081, 208.8974],
        [  0.0000,  73.4916,  82.0480, 256.0000],
        [219.8390,  73.8144, 256.0000, 127.1684]])], [tensor([1, 7, 2, 5, 5, 1, 6, 4, 9, 4])], [BitmapMasks(num_masks=10, height=256, width=256)])
:Could not infer type of list element: Dictionary inputs to traced functions must have consistent type. Found Tuple[int, int, int] and str
```

## Bug fix

I read the following issue #6722, #5140, #5018, #5076, #1006 but I do not think it fixes the issue. Or I really missed something. 
These issues are solved with an upgrade of `mmcv` or `torch` version. My version of torch is `1.8.1` and my version of mmcv is `1.3.17` which are higher or equal to the _updated_ version in the issues.
I got it that for visualizing metrics I need to use hooks for getting scalars from runner (from what I understood). And for graph, the only thing I can see is to export to ONXX from the [mmdet doc](https://mmdetection.readthedocs.io/en/latest/useful_tools.html#visualize-models) but in my case it did not work. Plus, I aim to work on more complex model which use a mix of Deformable Attention layer and language model so I am not sure that ONXX would be usable for visualization of such complex layers. 

So I am starting to think there is no way to visualize model from MMDET in tensorboard :( I would love to keep using mmdet lib without dropping native torch visualization.

Let me know if you want more details or tell me if I missed something, please. 

Regards,
Mathias. 
"
AvoidCUDAOOM usage,open-mmlab/mmdetection,2022-07-04 10:09:07,3,Doc,8302,1292940091,"Hi Team,

I am training a faster RCNN model and tried to use the AvoidCUDAOOM  in the following way

```
    from mmdet.utils import AvoidOOM
    from mmdet.apis import train_detector
    from mmdet.models import build_detector
    from mmdet.datasets import build_dataset
    AvoidCUDAOOM = AvoidOOM()
    cfg, _ = load_config(data_folder)
    datasets = [build_dataset(cfg.data.train)]
    model = build_detector(cfg.model, train_cfg=cfg.get(""train_cfg""), test_cfg=cfg.get(""train_cfg""))
    AvoidCUDAOOM.retry_if_cuda_oom(train_detector)(model, datasets, cfg, validate=True)
```

The code working fine without the OOM. But when I Use the OOM, it's throwing the following error. Can you please let me know if I am doing something wrong?

```
Traceback (most recent call last):
  File ""basicHighResolution.py"", line 150, in <module>
    output = AvoidCUDAOOM.retry_if_cuda_oom(train_detector)(model, datasets, cfg, validate=True)
  File ""/mmdetection/mmdet/utils/memory.py"", line 160, in wrapped
    raise ValueError('There is no tensor in the inputs, cannot get dtype and device')

```"
Mask RCNN swin transformer to TensorRT KeyError: 'onnx::NonMaxSuppression_12575',open-mmlab/mmdetection,2022-07-04 05:42:52,2,,8299,1292654812,"I was trying to convert Swin-T Mask R-CNN but getting following error:

```
Traceback (most recent call last):
  File ""/home/sort/mmdetection/tools/deployment/onnx2tensorrt.py"", line 247, in <module>
    onnx2tensorrt(
  File ""/home/sort/mmdetection/tools/deployment/onnx2tensorrt.py"", line 40, in onnx2tensorrt
    trt_engine = onnx2trt(
  File ""/home/sort/ved/mmcv/mmcv/tensorrt/tensorrt_utils.py"", line 71, in onnx2trt
    onnx_model = preprocess_onnx(onnx_model)
  File ""/home/sort/ved/mmcv/mmcv/tensorrt/preprocess.py"", line 102, in preprocess_onnx
    mark_nodes_to_remove(node_inputs[2])
  File ""/home/sort/ved/mmcv/mmcv/tensorrt/preprocess.py"", line 59, in mark_nodes_to_remove
    node = node_dict[name]
KeyError: 'onnx::NonMaxSuppression_12575'
``` 

Here, are the steps I followed:
1) mmcv compile with TRT and ONNX:
```
export TENSORRT_DIR=/home/sort/ved/TensorRT-7.2.3.4
export ONNXRUNTIME_DIR=/home/sort/ved/onnxruntime-linux-x64-gpu-1.11.1
export LD_LIBRARY_PATH=/home/sort/ved/onnxruntime-linux-x64-gpu-1.11.1/lib:/home/sort/ved/TensorRT-7.2.3.4/lib:/usr/local/cuda/lib64


git clone https://github.com/open-mmlab/mmcv.git

cd mmcv ## to MMCV root directory
MMCV_WITH_OPS=1 MMCV_WITH_ORT=1 MMCV_WITH_TRT=1 pip install -e .
```
2) To ONNX:
```
python /home/sort/mmdetection/tools/deployment/pytorch2onnx.py \
    /home/sort/ved/sort/onion_l2/mask_rcnn_swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py \
    /home/sort/ved/sort/onion_l2/mask_rcnn_swin/epoch_5.pth \
    --output-file mask_rcnn_swin_e5.onnx \
    --input-img /home/sort/ved/sort/onion_l2/data/val/images/1000.jpg \
    --test-img /home/sort/ved/sort/onion_l2/data/val/images/0.jpg \
    --shape 750 750 \
    --show \
    --dynamic-export 
```

3) To TRT:
```
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/home/sort/ved/TensorRT-7.2.3.4/lib:/home/sort/ved/TensorRT-8.4.1.5/lib

python -m pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com
polygraphy surgeon sanitize --fold-constants mask_rcnn_swin_e5.onnx -o mask_rcnn_swin_e5_folded.onnx


python /home/sort/mmdetection/tools/deployment/onnx2tensorrt.py \
    /home/sort/ved/sort/onion_l2/mask_rcnn_swin/mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py \
    mask_rcnn_swin_e5_folded.onnx \
    --trt-file mask_rcnn_swin_e5.trt \
    --input-img /home/sort/ved/sort/onion_l2/data/val/images/0.jpg \
    --shape 750 750 \
    --show
```

I'm using both `TensorRT-7.2.3.4` and `TensorRT-8.4.1.5`, otherwise I was getting errors like:
```
ImportError: libnvinfer.so.8: cannot open shared object file: No such file or directory
```"
Add tools to help visualize effective receptive field,open-mmlab/mmdetection,2022-07-03 14:14:42,1,,8296,1292321043,"**Describe the feature**
Visualize effective receptive field (ERF) of a given layer in network.  
![image](https://user-images.githubusercontent.com/69593462/177043623-b3ac23e6-bc8d-4851-9359-5b5d51c859af.png)  
Image taken from [Segformer paper](https://arxiv.org/pdf/2105.15203.pdf)

**Motivation**
I want to know why my model fails

**Related resources**
I found a receptive visualization implemented in Pytorch: https://github.com/shelfwise/receptivefield  
but I don't know if the implementation is correct.  
Segformer ERF visualization implemented based on [ERF paper](https://arxiv.org/pdf/2105.15203.pdf) but they didn't provide the code

"
UnicodeDecodeError: 'gbk' codec can't decode byte 0xaa in position 34: illegal multibyte sequence,open-mmlab/mmdetection,2022-07-02 07:46:01,2,Doc,8291,1291995216,"""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python39_64\python.exe"" D:/zhjrProjects/source/OpenMMLab/MMDetection/tools/train.py ../configs/balloon/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_balloon.py
Traceback (most recent call last):
  File ""D:\zhjrProjects\source\OpenMMLab\MMDetection\tools\train.py"", line 242, in <module>
    main()
  File ""D:\zhjrProjects\source\OpenMMLab\MMDetection\tools\train.py"", line 114, in main
    cfg = replace_cfg_vals(cfg)
  File ""D:\zhjrProjects\source\OpenMMLab\MMDetection\mmdet\utils\replace_cfg_vals.py"", line 64, in replace_cfg_vals
    updated_cfg = Config(
  File ""C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python39_64\lib\site-packages\mmcv\utils\config.py"", line 405, in __init__
    text = f.read()
UnicodeDecodeError: 'gbk' codec can't decode byte 0xaa in position 34: illegal multibyte sequence

Process finished with exit code 1"
"Trained model weights for ""Strong Baselines""",open-mmlab/mmdetection,2022-07-01 12:00:27,1,bug#Doc#v-2.x,8288,1291315795,"Hi, will like to check where can we find the trained model weights for [strong baselines](https://github.com/open-mmlab/mmdetection/tree/master/configs/strong_baselines)? The links to all the ""model"" does not point to the weights download url."
Train Mask2Former with Swin-L backbone for instance segmentation on one GPU,open-mmlab/mmdetection,2022-06-30 13:10:00,3,feature request,8285,1290116765,"I am having trouble training Mask2Former with Swin-L for instance segmentation. The loss does not improve for quite some while and stays at around 50.

I am training on one 3090 GPU using 

`python tools/train.py configs/mask2former/mask2former_swin-l-p4-w12-384-in21k_lsj_16x1_100e_coco.py `

with the config file `mask2former_swin-l-p4-w12-384-in21k_lsj_16x1_100e_coco.py` as


```
_base_ = ['./mask2former_swin-b-p4-w12-384_lsj_8x2_50e_coco.py']
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth'  # noqa

model = dict(
    backbone=dict(
        embed_dims=192,
        num_heads=[6, 12, 24, 48],
        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),
    panoptic_head=dict(num_queries=200, in_channels=[192, 384, 768, 1536]))

data = dict(samples_per_gpu=1, workers_per_gpu=1)

lr_config = dict(step=[655556, 710184])

max_iters = 737500
runner = dict(type='IterBasedRunner', max_iters=max_iters)

# Before 735001th iteration, we do evaluation every 5000 iterations.
# After 735000th iteration, we do evaluation every 737500 iterations,
# which means that we do evaluation at the end of training.'
interval = 5000
dynamic_intervals = [(max_iters // interval * interval + 1, max_iters)]
evaluation = dict(
    interval=interval,
    dynamic_intervals=dynamic_intervals,
    metric=['bbox', 'segm'])
```

and the config file `mask2former_swin-b-p4-w12-384_lsj_8x2_50e_coco.py`


```
_base_ = ['./mask2former_swin-t-p4-w7-224_lsj_8x2_50e_coco.py']
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384.pth'  # noqa

depths = [2, 2, 18, 2]
model = dict(
    backbone=dict(
        pretrain_img_size=384,
        embed_dims=128,
        depths=depths,
        num_heads=[4, 8, 16, 32],
        window_size=12,
        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),
    panoptic_head=dict(in_channels=[128, 256, 512, 1024]))

# set all layers in backbone to lr_mult=0.1
# set all norm layers, position_embeding,
# query_embeding, level_embeding to decay_multi=0.0
backbone_norm_multi = dict(lr_mult=0.1, decay_mult=0.0)
backbone_embed_multi = dict(lr_mult=0.1, decay_mult=0.0)
embed_multi = dict(lr_mult=1.0, decay_mult=0.0)
custom_keys = {
    'backbone': dict(lr_mult=0.1, decay_mult=1.0),
    'backbone.patch_embed.norm': backbone_norm_multi,
    'backbone.norm': backbone_norm_multi,
    'absolute_pos_embed': backbone_embed_multi,
    'relative_position_bias_table': backbone_embed_multi,
    'query_embed': embed_multi,
    'query_feat': embed_multi,
    'level_embed': embed_multi
}
custom_keys.update({
    f'backbone.stages.{stage_id}.blocks.{block_id}.norm': backbone_norm_multi
    for stage_id, num_blocks in enumerate(depths)
    for block_id in range(num_blocks)
})
custom_keys.update({
    f'backbone.stages.{stage_id}.downsample.norm': backbone_norm_multi
    for stage_id in range(len(depths) - 1)
})
# optimizer
optimizer = dict(
    paramwise_cfg=dict(custom_keys=custom_keys, norm_decay_mult=0.0))
```



Do I need to change some configuration in order to get it work?"
Link error in docs,open-mmlab/mmdetection,2022-06-27 06:35:53,1,,8267,1285319236,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
```
Some models require additional [COCO-stuff](http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip) datasets, such as HTC, DetectoRS and SCNet, you can download and unzip then move to the coco folder. The directory should be like this.

mmdetection
├── data
│   ├── coco
│   │   ├── annotations
│   │   ├── train2017
│   │   ├── val2017
│   │   ├── test2017
│   │   ├── stuffthingmaps
Panoptic segmentation models like PanopticFPN require additional [COCO Panoptic](http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip) datasets, you can download and unzip then move to the coco annotation folder. The directory should be like this.
```
The `href` value in the above text should be started with `https`, or the downloading will be failed since the Chrome has stopped the downloading process.
```
Mixed Content: The site at 'https://github.com/' was loaded over a secure connection, but the file at 'https://calvin-vision.net/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip' was redirected through an insecure connection. This file should be served over HTTPS. This download has been blocked. See https://blog.chromium.org/2020/02/protecting-users-from-insecure.html for more details.
```

**Reproduction**

1. What command or script did you run?

```none
A placeholder for the command.
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
3. What dataset did you use?

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
2. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
   - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**
If applicable, paste the error trackback here.

```none
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!

```"
"train with multi gpus, always blocking...",open-mmlab/mmdetection,2022-06-25 10:23:15,3,,8265,1284539768,"Train with 1 node, 8 gpus is ok. But train with 2 nodes, 16 gpus is not ok.

1. train with 2 nodes(a & b), each node has 8 gpus.
2. run with the following shell scripts：./tools/dist_train.sh configs/goods/faster_rcnn_r50_fpn_2x_coco.py 8

in dist_train.sh:
node a is :
#!/usr/bin/env bash

CONFIG=$1
GPUS=$2
NNODES=${NNODES:-2}
NODE_RANK=${NODE_RANK:-0}
PORT=${PORT:-29511}
MASTER_ADDR=${MASTER_ADDR:-""10.193.21.228""}

PYTHONPATH=""$(dirname $0)/.."":$PYTHONPATH \
python -m torch.distributed.launch \
    --nnodes=$NNODES \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --nproc_per_node=$GPUS \
    --master_port=$PORT \
    $(dirname ""$0"")/train.py \
    $CONFIG \
    --seed 0 \
    --launcher pytorch ${@:3}

node b is:

#!/usr/bin/env bash

CONFIG=$1
GPUS=$2
NNODES=${NNODES:-2}
NODE_RANK=${NODE_RANK:-1}
PORT=${PORT:-29511}
MASTER_ADDR=${MASTER_ADDR:-""10.193.21.228""}

PYTHONPATH=""$(dirname $0)/.."":$PYTHONPATH \
python -m torch.distributed.launch \
    --nnodes=$NNODES \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --nproc_per_node=$GPUS \
    --master_port=$PORT \
    $(dirname ""$0"")/train.py \
    $CONFIG \
    --seed 0 \
    --launcher pytorch ${@:3}

But, the programming will block and print ：
/mnt/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
*****************************************

Please help me. 

"
Multi GPU training (ConvNeXT) using Weights and Biases Hook makes the training freeze after 50 steps,open-mmlab/mmdetection,2022-06-24 09:21:34,4,,8259,1283495792,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
When I try to train a ConvNeXt Model for Instance Segmentation using 4 GPUs and the new Weights and Biases Hook, the training freezes after the first 50 steps. When I try to train the model using the same config while utilizing only one GPU, the training behaves as expected.

**Reproduction**

1. What command or script did you run?

```none
bash tools/dist_train.sh config/<rest_of_config_path> 4
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
didn't make many adjustments, but i understood what I did. This is the whole config:
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='mmcls.ConvNeXt',
        arch='tiny',
        out_indices=[0, 1, 2, 3],
        drop_path_rate=0.5,
        layer_scale_init_value=1.0,
        gap_before_final_norm=False,
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            'https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_3rdparty_32xb128-noema_in1k_20220222-2908964a.pth',
            prefix='backbone.')),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 384, 768],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=38,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=38,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1,
                gpu_assign_thr=1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1,
                gpu_assign_thr=1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = 'data/dataset/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Resize', img_scale=(1200, 800), keep_ratio=True),
    dict(
        type='Albu',
        transforms=[dict(type='RandomRotate90', p=0.5)],
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=['gt_labels'],
            min_visibility=0.0,
            filter_lost_elements=True),
        keymap=dict(img='image', gt_bboxes='bboxes', gt_masks='masks'),
        update_pad_shape=False,
        skip_img_without_anno=True),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1200, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
classes = (""normally, 38 classes are written here, cut out just for this issue"")
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=1,
    train=dict(
        type='CocoDataset',
        img_prefix='data/dataset/Images/train/',
        classes=(""normally, 38 classes are written here, cut out just for this issue""),
        ann_file='data/dataset/Annotations/train/coco_annotations.json',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='Resize', img_scale=(1200, 800), keep_ratio=True),
            dict(
                type='Albu',
                transforms=[dict(type='RandomRotate90', p=0.5)],
                bbox_params=dict(
                    type='BboxParams',
                    format='pascal_voc',
                    label_fields=['gt_labels'],
                    min_visibility=0.0,
                    filter_lost_elements=True),
                keymap=dict(img='image', gt_bboxes='bboxes', gt_masks='masks'),
                update_pad_shape=False,
                skip_img_without_anno=True),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='CocoDataset',
        img_prefix='data/dataset/Images/valid/',
        classes=(""normally, 38 classes are written here, cut out just for this issue""),
        ann_file='data/dataset/Annotations/valid/coco_annotations.json',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1200, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        img_prefix='data/dataset/Images/test/',
        classes=(""normally, 38 classes are written here, cut out just for this issue""),
        ann_file='data/dataset/Annotations/test/coco_annotations.json',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1200, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
optimizer = dict(
    constructor='LearningRateDecayOptimizerConstructor',
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    paramwise_cfg=dict(decay_rate=0.95, decay_type='layer_wise', num_layers=6))
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[27, 33])
runner = dict(type='EpochBasedRunner', max_epochs=36)
checkpoint_config = dict(interval=6)
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(
            type='MMDetWandbHook',
            init_kwargs=dict(project='project'),
            interval=50,
            log_checkpoint=True,
            log_checkpoint_metadata=True,
            num_eval_images=100)
    ])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
custom_imports = dict(imports=['mmcls.models'], allow_failed_imports=False)
pretrained = 'https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_3rdparty_32xb128-noema_in1k_20220222-2908964a.pth'
num_classes = 38
fp16 = dict(loss_scale=dict(init_scale=512))
work_dir = './work_dirs/mask_rcnn_convnext-t_p4_w7_fpn_fp16_ms-crop_3x_coco'
auto_resume = False
gpu_ids = range(0, 4)

3. What dataset did you use?
 Custom Dataset in COCO annotation format
**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3: Tesla V100-SXM2-16GB
CUDA_HOME: /usr/local/cuda-11.3
NVCC: Build cuda_11.3.r11.3/compiler.29920130_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter-Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1,USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF,

TorchVision: 0.12.0
OpenCV: 4.5.5
MMCV: 1.4.8
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.0+e7d339d

2. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
        - with pip
   - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**


```none
no Error.. the training just does not continue
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
train error,open-mmlab/mmdetection,2022-06-22 14:07:33,1,reimplementation,8238,1280206872,"Traceback (most recent call last):
  File ""E:\Anaconda\envs\whmmdet\lib\site-packages\mmcv\utils\registry.py"", line 52, in build_from_cfg
    return obj_cls(**args)
  File ""H:\WH\MM\mmdetection-master\mmdet\datasets\custom.py"", line 121, in __init__
    valid_inds = self._filter_imgs()
  File ""H:\WH\MM\mmdetection-master\mmdet\datasets\coco.py"", line 139, in _filter_imgs
    if min(img_info['width'], img_info['height']) >= min_size:    #min_size
TypeError: '>=' not supported between instances of 'str' and 'int'"
How to get and evaluate the semantic segmentation result of HTC ?,open-mmlab/mmdetection,2022-06-22 06:21:58,3,,8233,1279611468,"I want to evaluate the semantic segmentation (not instance segmentation) result of HTC by mIoU on COCO.
How can I do that ?"
analyze_results.py doesn't accept format of test.py's results.pkl,open-mmlab/mmdetection,2022-06-21 12:09:44,3,bug,8231,1278354672,"hello, i try to figure out what am i doing wrong. I would like to analyse some mask rcnn model predictions created by 'test.py' via 'analyse_results.py' script but i'm getting an error.

**call of test.py**:
`!python tools/test.py  configs/container/r50_x4split_fold1.py  work_dirs/r50_x4split_fold1/latest.pth  --out work_dirs/r50_x4split_fold1/results.pkl --eval bbox segm`
output:
>/content/mmdetection/mmdet/utils/setup_env.py:39: UserWarning: Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting OMP_NUM_THREADS environment variable for each process '
/content/mmdetection/mmdet/utils/setup_env.py:49: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
loading annotations into memory...
Done (t=1.58s)
creating index...
index created!
load checkpoint from local path: work_dirs/r50_x4split_fold1/latest.pth
[>>] 100/100, 1.8 task/s, elapsed: 56s, ETA:     0s
writing results to work_dirs/r50_x4split_fold1/results.pkl
>Evaluating bbox...
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.30s).
Accumulating evaluation results...
DONE (t=0.03s).
 >Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.198
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.348
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.198
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.287
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.265
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.437
>Evaluating segm...
/content/mmdetection/mmdet/datasets/coco.py:474: UserWarning: The key ""bbox"" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.
  UserWarning)
Loading and preparing results...
DONE (t=0.03s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=0.41s).
Accumulating evaluation results...
/usr/local/lib/python3.7/dist-packages/pycocotools/cocoeval.py:378: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)
DONE (t=0.03s).
 >Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.191
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.316
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.190
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.201
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.273
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.312
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.312
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.312
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.420
>OrderedDict([('bbox_mAP', 0.198), ('bbox_mAP_50', 0.348), ('bbox_mAP_75', 0.198), ('bbox_mAP_s', 0.053), ('bbox_mAP_m', 0.197), ('bbox_mAP_l', 0.287), ('bbox_mAP_copypaste', '0.198 0.348 0.198 0.053 0.197 0.287'), ('segm_mAP', 0.191), ('segm_mAP_50', 0.316), ('segm_mAP_75', 0.19), ('segm_mAP_s', 0.032), ('segm_mAP_m', 0.201), ('segm_mAP_l', 0.273), ('segm_mAP_copypaste', '0.191 0.316 0.190 0.032 0.201 0.273')])

**call of analyze_results.py**:
`!python tools/analysis_tools/analyze_results.py /content/mmdetection/configs/container/r50_x4split_fold1.py work_dirs/r50_x4split_fold1/results.pkl  /content/mmdetection/work_dirs/r50_x4split_fold1`
output:
>loading annotations into memory...
Done (t=0.01s)
creating index...
index created!
Traceback (most recent call last):
  File ""tools/analysis_tools/analyze_results.py"", line 365, in <module>
    main()
  File ""tools/analysis_tools/analyze_results.py"", line 361, in main
    dataset, outputs, topk=args.topk, show_dir=args.show_dir)
  File ""tools/analysis_tools/analyze_results.py"", line 171, in evaluate_and_show
    raise 'The format of result is not supported yet. ' \
TypeError: exceptions must derive from BaseException
 

**further information:**
i'm running mmdetection in google colab on a custom dataset in coco format.
The config used for training is as follows:
![image](https://user-images.githubusercontent.com/70146359/174794009-018dcaf4-f0ef-4dad-87e9-060bba8e88d5.png)
![image](https://user-images.githubusercontent.com/70146359/174794090-44545ba2-295a-4d0f-b58b-8992441fe531.png)
![image](https://user-images.githubusercontent.com/70146359/174794156-8270b1eb-919f-441e-96e2-b5cc08d06d37.png)
![image](https://user-images.githubusercontent.com/70146359/174794301-765d3a0a-1047-4bc8-ba0d-47dbfbf9e2bb.png)
![image](https://user-images.githubusercontent.com/70146359/174794358-bced3f8f-58d3-4833-93e1-2a5afd833487.png)

Scripts like 'analyze_logs.py' or 'eval_metric.py' work fine. The 'confusion_matrix.py' is also working but gives me a strange output (maybe its because i only have one class to detect?). Shouldn't there be any axis labeling?
![image](https://user-images.githubusercontent.com/70146359/174792918-bbfbde3a-2ba5-4c0e-9ecb-6da5d477b2a6.png)

I would be very grateful for any tips. I need to inspect/visualize the model predictions."
"Following best practice, installation leads to dependency conflict",open-mmlab/mmdetection,2022-06-20 11:51:27,4,,8224,1276809648,"Hello, 

when following the get_started.md the following dependency conflict occures

running
```
pip install -U openmim
```
I get 
```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.
```
because the installation automatically uninstalls, click==8.1.2
Trying to go back to  click==8.1.2 leads to
```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
openmim 0.1.6 requires Click==7.1.2, but you have click 8.1.2 which is incompatible.
```
This leads to failure of the installation.

Whether or not one sticks with 7.1.2 or 8.1.2, the further installation creates errors (more so when going with 8.1.2)

<details>
  <summary>Output when running everything with click 7.1.2</summary>
  
  ```(base) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ conda create --name openmmlab python=3.8 -y
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 4.12.0
  latest version: 4.13.0

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/malte/anaconda3/envs/openmmlab

  added / updated specs:
    - python=3.8


The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.4.26-h06a4308_0
  certifi            pkgs/main/linux-64::certifi-2022.5.18.1-py38h06a4308_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1
  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2
  openssl            pkgs/main/linux-64::openssl-1.1.1o-h7f8727e_0
  pip                pkgs/main/linux-64::pip-21.2.4-py38h06a4308_0
  python             pkgs/main/linux-64::python-3.8.13-h12debd9_0
  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1
  setuptools         pkgs/main/linux-64::setuptools-61.2.0-py38h06a4308_0
  sqlite             pkgs/main/linux-64::sqlite-3.38.3-hc218d9a_0
  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0
  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0
  xz                 pkgs/main/linux-64::xz-5.2.5-h7f8727e_1
  zlib               pkgs/main/linux-64::zlib-1.2.12-h7f8727e_2


Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate openmmlab
#
# To deactivate an active environment, use
#
#     $ conda deactivate

(base) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ conda activate openmmlab
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ conda install pytorch torchvision -c pytorch
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 4.12.0
  latest version: 4.13.0

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/malte/anaconda3/envs/openmmlab

  added / updated specs:
    - pytorch
    - torchvision


The following NEW packages will be INSTALLED:

  blas               pkgs/main/linux-64::blas-1.0-mkl
  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py38h27cfd23_1003
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0
  cffi               pkgs/main/linux-64::cffi-1.15.0-py38hd667e15_1
  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0
  cryptography       pkgs/main/linux-64::cryptography-37.0.1-py38h9ce1e76_0
  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2
  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0
  freetype           pkgs/main/linux-64::freetype-2.11.0-h70c0345_0
  giflib             pkgs/main/linux-64::giflib-5.2.1-h7b6447c_0
  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3
  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0
  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0
  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561
  jpeg               pkgs/main/linux-64::jpeg-9e-h7f8727e_0
  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0
  lcms2              pkgs/main/linux-64::lcms2-2.12-h3be6417_0
  libiconv           pkgs/main/linux-64::libiconv-1.16-h7f8727e_2
  libidn2            pkgs/main/linux-64::libidn2-2.3.2-h7f8727e_0
  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0
  libtasn1           pkgs/main/linux-64::libtasn1-4.16.0-h27cfd23_0
  libtiff            pkgs/main/linux-64::libtiff-4.2.0-h2818925_1
  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0
  libuv              pkgs/main/linux-64::libuv-1.40.0-h7b6447c_0
  libwebp            pkgs/main/linux-64::libwebp-1.2.2-h55f646e_0
  libwebp-base       pkgs/main/linux-64::libwebp-base-1.2.2-h7f8727e_0
  lz4-c              pkgs/main/linux-64::lz4-c-1.9.3-h295c915_1
  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640
  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py38h7f8727e_0
  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.1-py38hd3c417c_0
  mkl_random         pkgs/main/linux-64::mkl_random-1.2.2-py38h51133e4_0
  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1
  numpy              pkgs/main/linux-64::numpy-1.22.3-py38he7a7128_0
  numpy-base         pkgs/main/linux-64::numpy-base-1.22.3-py38hf524024_0
  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0
  pillow             pkgs/main/linux-64::pillow-9.0.1-py38h22f2fdc_0
  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0
  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-pyhd3eb1b0_0
  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38h06a4308_0
  pytorch            pytorch/linux-64::pytorch-1.11.0-py3.8_cuda11.3_cudnn8.2.0_0
  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda
  requests           pkgs/main/noarch::requests-2.27.1-pyhd3eb1b0_0
  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1
  torchvision        pytorch/linux-64::torchvision-0.12.0-py38_cu113
  typing_extensions  pkgs/main/noarch::typing_extensions-4.1.1-pyh06a4308_0
  urllib3            pkgs/main/linux-64::urllib3-1.26.9-py38h06a4308_0
  zstd               pkgs/main/linux-64::zstd-1.5.2-ha4553b6_0


Proceed ([y]/n)? y

Preparing transaction: done
Verifying transaction: done
Executing transaction: / By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html

done
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ pip install -U openmim
Collecting openmim
  Using cached openmim-0.1.6-py2.py3-none-any.whl
Requirement already satisfied: colorama in /home/malte/.local/lib/python3.8/site-packages (from openmim) (0.4.4)
Collecting Click==7.1.2
  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)
Requirement already satisfied: requests in /home/malte/.local/lib/python3.8/site-packages (from openmim) (2.27.1)
Requirement already satisfied: pandas in /home/malte/.local/lib/python3.8/site-packages (from openmim) (1.4.2)
Collecting tabulate
  Using cached tabulate-0.8.9-py3-none-any.whl (25 kB)
Collecting model-index
  Using cached model_index-0.1.11-py3-none-any.whl (34 kB)
Requirement already satisfied: pyyaml in /home/malte/.local/lib/python3.8/site-packages (from model-index->openmim) (6.0)
Requirement already satisfied: markdown in /home/malte/.local/lib/python3.8/site-packages (from model-index->openmim) (3.3.6)
Collecting ordered-set
  Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)
Requirement already satisfied: importlib-metadata>=4.4 in /home/malte/.local/lib/python3.8/site-packages (from markdown->model-index->openmim) (4.11.3)
Requirement already satisfied: zipp>=0.5 in /home/malte/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (3.8.0)
Requirement already satisfied: numpy>=1.18.5 in /home/malte/.local/lib/python3.8/site-packages (from pandas->openmim) (1.22.3)
Requirement already satisfied: pytz>=2020.1 in /home/malte/.local/lib/python3.8/site-packages (from pandas->openmim) (2022.1)
Requirement already satisfied: python-dateutil>=2.8.1 in /home/malte/.local/lib/python3.8/site-packages (from pandas->openmim) (2.8.2)
Requirement already satisfied: six>=1.5 in /home/malte/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->openmim) (1.16.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (1.26.9)
Requirement already satisfied: certifi>=2017.4.17 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (2021.10.8)
Requirement already satisfied: charset-normalizer~=2.0.0 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (2.0.12)
Requirement already satisfied: idna<4,>=2.5 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (3.3)
Installing collected packages: ordered-set, Click, tabulate, model-index, openmim
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.
Successfully installed Click-7.1.2 model-index-0.1.11 openmim-0.1.6 ordered-set-4.1.0 tabulate-0.8.9
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ mim install mmcv-full
/home/malte/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
installing mmcv-full from wheel.
Looking in links: https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html
Collecting mmcv-full==1.5.3
  Using cached https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/mmcv_full-1.5.3-cp38-cp38-manylinux1_x86_64.whl (38.0 MB)
Requirement already satisfied: packaging in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (21.3)
Requirement already satisfied: Pillow in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (9.1.0)
Collecting addict
  Using cached addict-2.4.0-py3-none-any.whl (3.8 kB)
Requirement already satisfied: numpy in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (1.22.3)
Requirement already satisfied: pyyaml in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (6.0)
Collecting yapf
  Using cached yapf-0.32.0-py2.py3-none-any.whl (190 kB)
Requirement already satisfied: opencv-python>=3 in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (4.5.5.64)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/malte/.local/lib/python3.8/site-packages (from packaging->mmcv-full==1.5.3) (3.0.8)
Installing collected packages: yapf, addict, mmcv-full
Successfully installed addict-2.4.0 mmcv-full-1.5.3 yapf-0.32.0
Successfully installed mmcv-full.
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ pip install -v -e .
Using pip 21.2.4 from /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/pip (python 3.8)
Obtaining file:///home/malte/AI-Plantrecognition/mmdetection
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info
    writing /tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching 'mmdet/VERSION'
    warning: no files found matching 'mmdet/.mim/demo/*/*'
    adding license file 'LICENSE'
    writing manifest file '/tmp/pip-pip-egg-info-5jkru6wb/mmdet.egg-info/SOURCES.txt'
Requirement already satisfied: matplotlib in /home/malte/.local/lib/python3.8/site-packages (from mmdet==2.25.0) (3.5.1)
Requirement already satisfied: numpy in /home/malte/.local/lib/python3.8/site-packages (from mmdet==2.25.0) (1.22.3)
Collecting pycocotools
  Using cached pycocotools-2.0.4-cp38-cp38-linux_x86_64.whl
Requirement already satisfied: six in /home/malte/.local/lib/python3.8/site-packages (from mmdet==2.25.0) (1.16.0)
Collecting terminaltables
  Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: cycler>=0.10 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (4.32.0)
Requirement already satisfied: pillow>=6.2.0 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (9.1.0)
Requirement already satisfied: python-dateutil>=2.7 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (2.8.2)
Requirement already satisfied: packaging>=20.0 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (21.3)
Requirement already satisfied: pyparsing>=2.2.1 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (3.0.8)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (1.4.2)
Installing collected packages: terminaltables, pycocotools, mmdet
  Running setup.py develop for mmdet
    Running command /home/malte/anaconda3/envs/openmmlab/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/home/malte/AI-Plantrecognition/mmdetection/setup.py'""'""'; __file__='""'""'/home/malte/AI-Plantrecognition/mmdetection/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' develop --no-deps
    running develop
    running egg_info
    writing mmdet.egg-info/PKG-INFO
    writing dependency_links to mmdet.egg-info/dependency_links.txt
    writing requirements to mmdet.egg-info/requires.txt
    writing top-level names to mmdet.egg-info/top_level.txt
    reading manifest file 'mmdet.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    /home/malte/.local/lib/python3.8/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.
      warnings.warn(
    /home/malte/.local/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
      warnings.warn(
    warning: no files found matching 'mmdet/VERSION'
    warning: no files found matching 'mmdet/.mim/demo/*/*'
    adding license file 'LICENSE'
    writing manifest file 'mmdet.egg-info/SOURCES.txt'
    running build_ext
    Creating /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet.egg-link (link to .)
    Adding mmdet 2.25.0 to easy-install.pth file

    Installed /home/malte/AI-Plantrecognition/mmdetection
    /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/utils/cpp_extension.py:387: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.
      warnings.warn(msg.format('we could not find ninja.'))
Successfully installed mmdet pycocotools-2.0.4 terminaltables-3.1.10
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .
/home/malte/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
Traceback (most recent call last):
  File ""/home/malte/anaconda3/envs/openmmlab/bin/mim"", line 8, in <module>
    sys.exit(cli())
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/download.py"", line 44, in cli
    download(package, configs, dest_root)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/download.py"", line 75, in download
    model_info = get_model_info(
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/search.py"", line 170, in get_model_info
    dataframe = convert2df(metadata)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/search.py"", line 396, in convert2df
    for key, value in name2collection[collection_name].items():
KeyError: 'Cascade Mask R-CNN'
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ python demo/image_demo.py demo/demo.jpg yolov3_mobilenetv2_320_300e_coco.py yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth --device cpu --out-file result.jpg
Traceback (most recent call last):
  File ""demo/image_demo.py"", line 68, in <module>
    main(args)
  File ""demo/image_demo.py"", line 34, in main
    model = init_detector(args.config, args.checkpoint, device=args.device)
  File ""/home/malte/AI-Plantrecognition/mmdetection/mmdet/apis/inference.py"", line 33, in init_detector
    config = mmcv.Config.fromfile(config)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/config.py"", line 340, in fromfile
    cfg_dict, cfg_text = Config._file2dict(filename,
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/config.py"", line 183, in _file2dict
    check_file_exist(filename)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/path.py"", line 23, in check_file_exist
    raise FileNotFoundError(msg_tmpl.format(filename))
FileNotFoundError: file ""/home/malte/AI-Plantrecognition/mmdetection/yolov3_mobilenetv2_320_300e_coco.py"" does not exist
  ```
</details>

<details>
  <summary>It is a similar problem when using click 8.1.2</summary>
  
  ```(base) malte@AI-Malte:~/AI-Plantrecognition$ conda create --name openmmlab python=3.8 -y
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 4.12.0
  latest version: 4.13.0

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/malte/anaconda3/envs/openmmlab

  added / updated specs:
    - python=3.8


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    _openmp_mutex-5.1          |            1_gnu          21 KB
    ca-certificates-2022.4.26  |       h06a4308_0         124 KB
    certifi-2022.5.18.1        |   py38h06a4308_0         147 KB
    ld_impl_linux-64-2.38      |       h1181459_1         654 KB
    libgcc-ng-11.2.0           |       h1234567_1         5.3 MB
    libgomp-11.2.0             |       h1234567_1         474 KB
    libstdcxx-ng-11.2.0        |       h1234567_1         4.7 MB
    openssl-1.1.1o             |       h7f8727e_0         2.5 MB
    sqlite-3.38.3              |       hc218d9a_0         1.0 MB
    tk-8.6.12                  |       h1ccaba5_0         3.0 MB
    xz-5.2.5                   |       h7f8727e_1         339 KB
    ------------------------------------------------------------
                                           Total:        18.3 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.4.26-h06a4308_0
  certifi            pkgs/main/linux-64::certifi-2022.5.18.1-py38h06a4308_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1
  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  ncurses            pkgs/main/linux-64::ncurses-6.3-h7f8727e_2
  openssl            pkgs/main/linux-64::openssl-1.1.1o-h7f8727e_0
  pip                pkgs/main/linux-64::pip-21.2.4-py38h06a4308_0
  python             pkgs/main/linux-64::python-3.8.13-h12debd9_0
  readline           pkgs/main/linux-64::readline-8.1.2-h7f8727e_1
  setuptools         pkgs/main/linux-64::setuptools-61.2.0-py38h06a4308_0
  sqlite             pkgs/main/linux-64::sqlite-3.38.3-hc218d9a_0
  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0
  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0
  xz                 pkgs/main/linux-64::xz-5.2.5-h7f8727e_1
  zlib               pkgs/main/linux-64::zlib-1.2.12-h7f8727e_2



Downloading and Extracting Packages
_openmp_mutex-5.1    | 21 KB     | ############################################################################# | 100% 
xz-5.2.5             | 339 KB    | ############################################################################# | 100% 
libgomp-11.2.0       | 474 KB    | ############################################################################# | 100% 
openssl-1.1.1o       | 2.5 MB    | ############################################################################# | 100% 
ca-certificates-2022 | 124 KB    | ############################################################################# | 100% 
libgcc-ng-11.2.0     | 5.3 MB    | ############################################################################# | 100% 
libstdcxx-ng-11.2.0  | 4.7 MB    | ############################################################################# | 100% 
tk-8.6.12            | 3.0 MB    | ############################################################################# | 100% 
sqlite-3.38.3        | 1.0 MB    | ############################################################################# | 100% 
ld_impl_linux-64-2.3 | 654 KB    | ############################################################################# | 100% 
certifi-2022.5.18.1  | 147 KB    | ############################################################################# | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate openmmlab
#
# To deactivate an active environment, use
#
#     $ conda deactivate

(base) malte@AI-Malte:~/AI-Plantrecognition$ conda activate openmmlab
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ conda install pytorch torchvision -c pytorch
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 4.12.0
  latest version: 4.13.0

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/malte/anaconda3/envs/openmmlab

  added / updated specs:
    - pytorch
    - torchvision


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    cryptography-37.0.1        |   py38h9ce1e76_0         1.3 MB
    gmp-6.2.1                  |       h295c915_3         544 KB
    libtiff-4.2.0              |       h2818925_1         452 KB
    numpy-1.22.3               |   py38he7a7128_0          10 KB
    numpy-base-1.22.3          |   py38hf524024_0         5.4 MB
    pyopenssl-22.0.0           |     pyhd3eb1b0_0          50 KB
    zstd-1.5.2                 |       ha4553b6_0         488 KB
    ------------------------------------------------------------
                                           Total:         8.2 MB

The following NEW packages will be INSTALLED:

  blas               pkgs/main/linux-64::blas-1.0-mkl
  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py38h27cfd23_1003
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0
  cffi               pkgs/main/linux-64::cffi-1.15.0-py38hd667e15_1
  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0
  cryptography       pkgs/main/linux-64::cryptography-37.0.1-py38h9ce1e76_0
  cudatoolkit        pkgs/main/linux-64::cudatoolkit-11.3.1-h2bc3f7f_2
  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0
  freetype           pkgs/main/linux-64::freetype-2.11.0-h70c0345_0
  giflib             pkgs/main/linux-64::giflib-5.2.1-h7b6447c_0
  gmp                pkgs/main/linux-64::gmp-6.2.1-h295c915_3
  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0
  idna               pkgs/main/noarch::idna-3.3-pyhd3eb1b0_0
  intel-openmp       pkgs/main/linux-64::intel-openmp-2021.4.0-h06a4308_3561
  jpeg               pkgs/main/linux-64::jpeg-9e-h7f8727e_0
  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0
  lcms2              pkgs/main/linux-64::lcms2-2.12-h3be6417_0
  libiconv           pkgs/main/linux-64::libiconv-1.16-h7f8727e_2
  libidn2            pkgs/main/linux-64::libidn2-2.3.2-h7f8727e_0
  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0
  libtasn1           pkgs/main/linux-64::libtasn1-4.16.0-h27cfd23_0
  libtiff            pkgs/main/linux-64::libtiff-4.2.0-h2818925_1
  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0
  libuv              pkgs/main/linux-64::libuv-1.40.0-h7b6447c_0
  libwebp            pkgs/main/linux-64::libwebp-1.2.2-h55f646e_0
  libwebp-base       pkgs/main/linux-64::libwebp-base-1.2.2-h7f8727e_0
  lz4-c              pkgs/main/linux-64::lz4-c-1.9.3-h295c915_1
  mkl                pkgs/main/linux-64::mkl-2021.4.0-h06a4308_640
  mkl-service        pkgs/main/linux-64::mkl-service-2.4.0-py38h7f8727e_0
  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.1-py38hd3c417c_0
  mkl_random         pkgs/main/linux-64::mkl_random-1.2.2-py38h51133e4_0
  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1
  numpy              pkgs/main/linux-64::numpy-1.22.3-py38he7a7128_0
  numpy-base         pkgs/main/linux-64::numpy-base-1.22.3-py38hf524024_0
  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0
  pillow             pkgs/main/linux-64::pillow-9.0.1-py38h22f2fdc_0
  pycparser          pkgs/main/noarch::pycparser-2.21-pyhd3eb1b0_0
  pyopenssl          pkgs/main/noarch::pyopenssl-22.0.0-pyhd3eb1b0_0
  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38h06a4308_0
  pytorch            pytorch/linux-64::pytorch-1.11.0-py3.8_cuda11.3_cudnn8.2.0_0
  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda
  requests           pkgs/main/noarch::requests-2.27.1-pyhd3eb1b0_0
  six                pkgs/main/noarch::six-1.16.0-pyhd3eb1b0_1
  torchvision        pytorch/linux-64::torchvision-0.12.0-py38_cu113
  typing_extensions  pkgs/main/noarch::typing_extensions-4.1.1-pyh06a4308_0
  urllib3            pkgs/main/linux-64::urllib3-1.26.9-py38h06a4308_0
  zstd               pkgs/main/linux-64::zstd-1.5.2-ha4553b6_0


Proceed ([y]/n)? y


Downloading and Extracting Packages
gmp-6.2.1            | 544 KB    | ############################################################################# | 100% 
pyopenssl-22.0.0     | 50 KB     | ############################################################################# | 100% 
numpy-1.22.3         | 10 KB     | ############################################################################# | 100% 
cryptography-37.0.1  | 1.3 MB    | ############################################################################# | 100% 
zstd-1.5.2           | 488 KB    | ############################################################################# | 100% 
libtiff-4.2.0        | 452 KB    | ############################################################################# | 100% 
numpy-base-1.22.3    | 5.4 MB    | ############################################################################# | 100% 
Preparing transaction: done
Verifying transaction: done
Executing transaction: \ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html

done
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ pip install -U openmim
Collecting openmim
  Downloading openmim-0.1.6.tar.gz (37 kB)
Collecting Click==7.1.2
  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)
     |████████████████████████████████| 82 kB 308 kB/s 
Requirement already satisfied: colorama in /home/malte/.local/lib/python3.8/site-packages (from openmim) (0.4.4)
Requirement already satisfied: requests in /home/malte/.local/lib/python3.8/site-packages (from openmim) (2.27.1)
Collecting model-index
  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)
Requirement already satisfied: pandas in /home/malte/.local/lib/python3.8/site-packages (from openmim) (1.4.2)
Collecting tabulate
  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)
Collecting ordered-set
  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)
Requirement already satisfied: markdown in /home/malte/.local/lib/python3.8/site-packages (from model-index->openmim) (3.3.6)
Requirement already satisfied: pyyaml in /home/malte/.local/lib/python3.8/site-packages (from model-index->openmim) (6.0)
Requirement already satisfied: importlib-metadata>=4.4 in /home/malte/.local/lib/python3.8/site-packages (from markdown->model-index->openmim) (4.11.3)
Requirement already satisfied: zipp>=0.5 in /home/malte/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (3.8.0)
Requirement already satisfied: pytz>=2020.1 in /home/malte/.local/lib/python3.8/site-packages (from pandas->openmim) (2022.1)
Requirement already satisfied: python-dateutil>=2.8.1 in /home/malte/.local/lib/python3.8/site-packages (from pandas->openmim) (2.8.2)
Requirement already satisfied: numpy>=1.18.5 in /home/malte/.local/lib/python3.8/site-packages (from pandas->openmim) (1.22.3)
Requirement already satisfied: six>=1.5 in /home/malte/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->openmim) (1.16.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (1.26.9)
Requirement already satisfied: idna<4,>=2.5 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (2021.10.8)
Requirement already satisfied: charset-normalizer~=2.0.0 in /home/malte/.local/lib/python3.8/site-packages (from requests->openmim) (2.0.12)
Building wheels for collected packages: openmim
  Building wheel for openmim (setup.py) ... done
  Created wheel for openmim: filename=openmim-0.1.6-py2.py3-none-any.whl size=43913 sha256=cf31256e953319c766e7051b7cd1903e977dc7b6c4499d396875831e00f2c6ce
  Stored in directory: /home/malte/.cache/pip/wheels/5f/cb/7c/1654fbb58d3c96061064451a70e1d9ef0dc2714157b7d0b67f
Successfully built openmim
Installing collected packages: ordered-set, Click, tabulate, model-index, openmim
  Attempting uninstall: Click
    Found existing installation: click 8.1.2
    Uninstalling click-8.1.2:
      Successfully uninstalled click-8.1.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.
Successfully installed Click-7.1.2 model-index-0.1.11 openmim-0.1.6 ordered-set-4.1.0 tabulate-0.8.9
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ pip update click 8.1.2
ERROR: unknown command ""update""
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ pip install click=8.1.2
ERROR: Invalid requirement: 'click=8.1.2'
Hint: = is not a valid operator. Did you mean == ?
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ pip install click==8.1.2
Collecting click==8.1.2
  Using cached click-8.1.2-py3-none-any.whl (96 kB)
Installing collected packages: click
  Attempting uninstall: click
    Found existing installation: click 7.1.2
    Uninstalling click-7.1.2:
      Successfully uninstalled click-7.1.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
openmim 0.1.6 requires Click==7.1.2, but you have click 8.1.2 which is incompatible.
Successfully installed click-8.1.2
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ pip install click==7.1.2
Collecting click==7.1.2
  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)
Installing collected packages: click
  Attempting uninstall: click
    Found existing installation: click 8.1.2
    Uninstalling click-8.1.2:
      Successfully uninstalled click-8.1.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.
Successfully installed click-7.1.2
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ ^C
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ ^C
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ ^C
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ mim install mmcv-full
/home/malte/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
installing mmcv-full from wheel.
Looking in links: https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html
Collecting mmcv-full==1.5.3
  Downloading https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/mmcv_full-1.5.3-cp38-cp38-manylinux1_x86_64.whl (38.0 MB)
     |████████████████████████████████| 38.0 MB 6.9 MB/s 
Requirement already satisfied: pyyaml in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (6.0)
Collecting addict
  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)
Requirement already satisfied: packaging in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (21.3)
Requirement already satisfied: Pillow in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (9.1.0)
Requirement already satisfied: numpy in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (1.22.3)
Collecting yapf
  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)
     |████████████████████████████████| 190 kB 1.3 MB/s 
Requirement already satisfied: opencv-python>=3 in /home/malte/.local/lib/python3.8/site-packages (from mmcv-full==1.5.3) (4.5.5.64)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/malte/.local/lib/python3.8/site-packages (from packaging->mmcv-full==1.5.3) (3.0.8)
Installing collected packages: yapf, addict, mmcv-full
Successfully installed addict-2.4.0 mmcv-full-1.5.3 yapf-0.32.0
Successfully installed mmcv-full.
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ git clone https://github.com/open-mmlab/mmdetection.git
Cloning into 'mmdetection'...
remote: Enumerating objects: 24959, done.
remote: Total 24959 (delta 0), reused 0 (delta 0), pack-reused 24959
Receiving objects: 100% (24959/24959), 37.76 MiB | 1.91 MiB/s, done.
Resolving deltas: 100% (17492/17492), done.
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition$ cd mmdetection/
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ pip install -v -e .
Using pip 21.2.4 from /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/pip (python 3.8)
Obtaining file:///home/malte/AI-Plantrecognition/mmdetection
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info
    writing /tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching 'mmdet/VERSION'
    warning: no files found matching 'mmdet/.mim/model-index.yml'
    warning: no files found matching 'mmdet/.mim/demo/*/*'
    warning: no files found matching '*.py' under directory 'mmdet/.mim/configs'
    warning: no files found matching '*.yml' under directory 'mmdet/.mim/configs'
    warning: no files found matching '*.sh' under directory 'mmdet/.mim/tools'
    warning: no files found matching '*.py' under directory 'mmdet/.mim/tools'
    adding license file 'LICENSE'
    writing manifest file '/tmp/pip-pip-egg-info-7_s6pdqk/mmdet.egg-info/SOURCES.txt'
Requirement already satisfied: matplotlib in /home/malte/.local/lib/python3.8/site-packages (from mmdet==2.25.0) (3.5.1)
Requirement already satisfied: numpy in /home/malte/.local/lib/python3.8/site-packages (from mmdet==2.25.0) (1.22.3)
Collecting pycocotools
  Downloading pycocotools-2.0.4.tar.gz (106 kB)
     |████████████████████████████████| 106 kB 1.3 MB/s 
  Running command /home/malte/anaconda3/envs/openmmlab/bin/python /tmp/pip-standalone-pip-s5ergp7m/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-hl782qow/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'cython>=0.27.3' oldest-supported-numpy 'setuptools>=43.0.0' wheel
  Collecting cython>=0.27.3
    Downloading Cython-0.29.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)
  Collecting oldest-supported-numpy
    Downloading oldest_supported_numpy-2022.5.28-py3-none-any.whl (3.9 kB)
  Collecting setuptools>=43.0.0
    Downloading setuptools-62.6.0-py3-none-any.whl (1.2 MB)
  Collecting wheel
    Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)
  Collecting numpy==1.17.3
    Downloading numpy-1.17.3-cp38-cp38-manylinux1_x86_64.whl (20.5 MB)
  Installing collected packages: numpy, wheel, setuptools, oldest-supported-numpy, cython
  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
  twisted 22.4.0 requires constantly>=15.1, which is not installed.
  tifffile 2022.4.8 requires numpy>=1.19.2, but you have numpy 1.17.3 which is incompatible.
  tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.17.3 which is incompatible.
  pandas 1.4.2 requires numpy>=1.18.5; platform_machine != ""aarch64"" and platform_machine != ""arm64"" and python_version < ""3.10"", but you have numpy 1.17.3 which is incompatible.
  Successfully installed cython-0.29.30 numpy-1.17.3 oldest-supported-numpy-2022.5.28 setuptools-62.6.0 wheel-0.37.1
  Installing build dependencies ... done
  Running command /home/malte/anaconda3/envs/openmmlab/bin/python /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py get_requires_for_build_wheel /tmp/tmp3u1ebcvz
  running egg_info
  writing pycocotools.egg-info/PKG-INFO
  writing dependency_links to pycocotools.egg-info/dependency_links.txt
  writing requirements to pycocotools.egg-info/requires.txt
  writing top-level names to pycocotools.egg-info/top_level.txt
  reading manifest file 'pycocotools.egg-info/SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  writing manifest file 'pycocotools.egg-info/SOURCES.txt'
  Getting requirements to build wheel ... done
    Running command /home/malte/anaconda3/envs/openmmlab/bin/python /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmppe9kjcep
    running dist_info
    creating /tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info
    writing /tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/requires.txt
    writing top-level names to /tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/top_level.txt
    writing manifest file '/tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    writing manifest file '/tmp/pip-modern-metadata-txgz05s1/pycocotools.egg-info/SOURCES.txt'
    creating '/tmp/pip-modern-metadata-txgz05s1/pycocotools-2.0.4.dist-info'
    Preparing wheel metadata ... done
Requirement already satisfied: six in /home/malte/.local/lib/python3.8/site-packages (from mmdet==2.25.0) (1.16.0)
Collecting terminaltables
  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: pyparsing>=2.2.1 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (3.0.8)
Requirement already satisfied: cycler>=0.10 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (0.11.0)
Requirement already satisfied: pillow>=6.2.0 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (9.1.0)
Requirement already satisfied: python-dateutil>=2.7 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (2.8.2)
Requirement already satisfied: packaging>=20.0 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (21.3)
Requirement already satisfied: kiwisolver>=1.0.1 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (1.4.2)
Requirement already satisfied: fonttools>=4.22.0 in /home/malte/.local/lib/python3.8/site-packages (from matplotlib->mmdet==2.25.0) (4.32.0)
Building wheels for collected packages: pycocotools
  Running command /home/malte/anaconda3/envs/openmmlab/bin/python /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmpoutjyofs
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-cpython-38
  creating build/lib.linux-x86_64-cpython-38/pycocotools
  copying pycocotools/coco.py -> build/lib.linux-x86_64-cpython-38/pycocotools
  copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-cpython-38/pycocotools
  copying pycocotools/mask.py -> build/lib.linux-x86_64-cpython-38/pycocotools
  copying pycocotools/__init__.py -> build/lib.linux-x86_64-cpython-38/pycocotools
  running build_ext
  skipping 'pycocotools/_mask.c' Cython extension (up-to-date)
  building 'pycocotools._mask' extension
  creating build/temp.linux-x86_64-cpython-38
  creating build/temp.linux-x86_64-cpython-38/common
  creating build/temp.linux-x86_64-cpython-38/pycocotools
  gcc -pthread -B /home/malte/anaconda3/envs/openmmlab/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/tmp/pip-build-env-hl782qow/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/home/malte/anaconda3/envs/openmmlab/include/python3.8 -c ./common/maskApi.c -o build/temp.linux-x86_64-cpython-38/./common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99
  ./common/maskApi.c: In function ‘rleDecode’:
  ./common/maskApi.c:46:7: warning: this ‘for’ clause does not guard... [-Wmisleading-indentation]
     46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}
        |       ^~~
  ./common/maskApi.c:46:49: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘for’
     46 |       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}
        |                                                 ^
  ./common/maskApi.c: In function ‘rleToBbox’:
  ./common/maskApi.c:135:32: warning: unused variable ‘xp’ [-Wunused-variable]
    135 |     uint h, w, xs, ys, xe, ye, xp, cc; siz j, m;
        |                                ^~
  ./common/maskApi.c: In function ‘rleFrPoly’:
  ./common/maskApi.c:181:3: warning: this ‘for’ clause does not guard... [-Wmisleading-indentation]
    181 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];
        |   ^~~
  ./common/maskApi.c:181:54: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘for’
    181 |   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];
        |                                                      ^
  ./common/maskApi.c:182:3: warning: this ‘for’ clause does not guard... [-Wmisleading-indentation]
    182 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];
        |   ^~~
  ./common/maskApi.c:182:54: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘for’
    182 |   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];
        |                                                      ^
  ./common/maskApi.c: In function ‘rleToString’:
  ./common/maskApi.c:227:7: warning: this ‘if’ clause does not guard... [-Wmisleading-indentation]
    227 |       if(more) c |= 0x20; c+=48; s[p++]=c;
        |       ^~
  ./common/maskApi.c:227:27: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘if’
    227 |       if(more) c |= 0x20; c+=48; s[p++]=c;
        |                           ^
  ./common/maskApi.c: In function ‘rleFrString’:
  ./common/maskApi.c:235:3: warning: this ‘while’ clause does not guard... [-Wmisleading-indentation]
    235 |   while( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;
        |   ^~~~~
  ./common/maskApi.c:235:22: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘while’
    235 |   while( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;
        |                      ^~~~
  ./common/maskApi.c:243:5: warning: this ‘if’ clause does not guard... [-Wmisleading-indentation]
    243 |     if(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;
        |     ^~
  ./common/maskApi.c:243:34: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘if’
    243 |     if(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;
        |                                  ^~~~
  gcc -pthread -B /home/malte/anaconda3/envs/openmmlab/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/tmp/pip-build-env-hl782qow/overlay/lib/python3.8/site-packages/numpy/core/include -I./common -I/home/malte/anaconda3/envs/openmmlab/include/python3.8 -c pycocotools/_mask.c -o build/temp.linux-x86_64-cpython-38/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99
  pycocotools/_mask.c: In function ‘__pyx_pf_11pycocotools_5_mask_12iou’:
  pycocotools/_mask.c:5952:31: warning: comparison of integer expressions of different signedness: ‘Py_ssize_t’ {aka ‘long int’} and ‘siz’ {aka ‘long unsigned int’} [-Wsign-compare]
   5952 |     if (unlikely(!((__pyx_t_8 == __pyx_v_n) != 0))) {
        |                               ^~
  pycocotools/_mask.c:807:43: note: in definition of macro ‘unlikely’
    807 |   #define unlikely(x) __builtin_expect(!!(x), 0)
        |                                           ^
  gcc -pthread -shared -B /home/malte/anaconda3/envs/openmmlab/compiler_compat -L/home/malte/anaconda3/envs/openmmlab/lib -Wl,-rpath=/home/malte/anaconda3/envs/openmmlab/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-cpython-38/./common/maskApi.o build/temp.linux-x86_64-cpython-38/pycocotools/_mask.o -o build/lib.linux-x86_64-cpython-38/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so
  installing to build/bdist.linux-x86_64/wheel
  running install
  running install_lib
  creating build/bdist.linux-x86_64
  creating build/bdist.linux-x86_64/wheel
  creating build/bdist.linux-x86_64/wheel/pycocotools
  copying build/lib.linux-x86_64-cpython-38/pycocotools/coco.py -> build/bdist.linux-x86_64/wheel/pycocotools
  copying build/lib.linux-x86_64-cpython-38/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/wheel/pycocotools
  copying build/lib.linux-x86_64-cpython-38/pycocotools/mask.py -> build/bdist.linux-x86_64/wheel/pycocotools
  copying build/lib.linux-x86_64-cpython-38/pycocotools/__init__.py -> build/bdist.linux-x86_64/wheel/pycocotools
  copying build/lib.linux-x86_64-cpython-38/pycocotools/_mask.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/pycocotools
  running install_egg_info
  running egg_info
  writing pycocotools.egg-info/PKG-INFO
  writing dependency_links to pycocotools.egg-info/dependency_links.txt
  writing requirements to pycocotools.egg-info/requires.txt
  writing top-level names to pycocotools.egg-info/top_level.txt
  reading manifest file 'pycocotools.egg-info/SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  writing manifest file 'pycocotools.egg-info/SOURCES.txt'
  Copying pycocotools.egg-info to build/bdist.linux-x86_64/wheel/pycocotools-2.0.4-py3.8.egg-info
  running install_scripts
  creating build/bdist.linux-x86_64/wheel/pycocotools-2.0.4.dist-info/WHEEL
  creating '/tmp/pip-wheel-pu61z7h6/tmp4ls0s95_/pycocotools-2.0.4-cp38-cp38-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
  adding 'pycocotools/__init__.py'
  adding 'pycocotools/_mask.cpython-38-x86_64-linux-gnu.so'
  adding 'pycocotools/coco.py'
  adding 'pycocotools/cocoeval.py'
  adding 'pycocotools/mask.py'
  adding 'pycocotools-2.0.4.dist-info/METADATA'
  adding 'pycocotools-2.0.4.dist-info/WHEEL'
  adding 'pycocotools-2.0.4.dist-info/top_level.txt'
  adding 'pycocotools-2.0.4.dist-info/RECORD'
  removing build/bdist.linux-x86_64/wheel
  Building wheel for pycocotools (PEP 517) ... done
  Created wheel for pycocotools: filename=pycocotools-2.0.4-cp38-cp38-linux_x86_64.whl size=422723 sha256=9ad49ebb3548f13cb1eebd868eb08d0fc8b3c055150f4de6bd4d3f993431c09b
  Stored in directory: /home/malte/.cache/pip/wheels/dd/e2/43/3e93cd653b3346b3d702bb0509bc611189f95d60407bff1484
Successfully built pycocotools
Installing collected packages: terminaltables, pycocotools, mmdet
  Running setup.py develop for mmdet
    Running command /home/malte/anaconda3/envs/openmmlab/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/home/malte/AI-Plantrecognition/mmdetection/setup.py'""'""'; __file__='""'""'/home/malte/AI-Plantrecognition/mmdetection/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' develop --no-deps
    running develop
    running egg_info
    creating mmdet.egg-info
    writing mmdet.egg-info/PKG-INFO
    writing dependency_links to mmdet.egg-info/dependency_links.txt
    writing requirements to mmdet.egg-info/requires.txt
    writing top-level names to mmdet.egg-info/top_level.txt
    writing manifest file 'mmdet.egg-info/SOURCES.txt'
    reading manifest file 'mmdet.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    /home/malte/.local/lib/python3.8/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.
      warnings.warn(
    /home/malte/.local/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
      warnings.warn(
    warning: no files found matching 'mmdet/VERSION'
    warning: no files found matching 'mmdet/.mim/demo/*/*'
    adding license file 'LICENSE'
    writing manifest file 'mmdet.egg-info/SOURCES.txt'
    running build_ext
    Creating /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmdet.egg-link (link to .)
    Adding mmdet 2.25.0 to easy-install.pth file

    Installed /home/malte/AI-Plantrecognition/mmdetection
    /home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/torch/utils/cpp_extension.py:387: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.
      warnings.warn(msg.format('we could not find ninja.'))
Successfully installed mmdet pycocotools-2.0.4 terminaltables-3.1.10
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .
/home/malte/.local/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")
Traceback (most recent call last):
  File ""/home/malte/anaconda3/envs/openmmlab/bin/mim"", line 8, in <module>
    sys.exit(cli())
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/download.py"", line 44, in cli
    download(package, configs, dest_root)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/download.py"", line 75, in download
    model_info = get_model_info(
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/search.py"", line 170, in get_model_info
    dataframe = convert2df(metadata)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mim/commands/search.py"", line 396, in convert2df
    for key, value in name2collection[collection_name].items():
KeyError: 'Cascade Mask R-CNN'
(openmmlab) malte@AI-Malte:~/AI-Plantrecognition/mmdetection$ python demo/image_demo.py demo/demo.jpg yolov3_mobilenetv2_320_300e_coco.py yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth --device cpu --out-file result.jpg
Traceback (most recent call last):
  File ""demo/image_demo.py"", line 68, in <module>
    main(args)
  File ""demo/image_demo.py"", line 34, in main
    model = init_detector(args.config, args.checkpoint, device=args.device)
  File ""/home/malte/AI-Plantrecognition/mmdetection/mmdet/apis/inference.py"", line 33, in init_detector
    config = mmcv.Config.fromfile(config)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/config.py"", line 340, in fromfile
    cfg_dict, cfg_text = Config._file2dict(filename,
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/config.py"", line 183, in _file2dict
    check_file_exist(filename)
  File ""/home/malte/anaconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/utils/path.py"", line 23, in check_file_exist
    raise FileNotFoundError(msg_tmpl.format(filename))
FileNotFoundError: file ""/home/malte/AI-Plantrecognition/mmdetection/yolov3_mobilenetv2_320_300e_coco.py"" does not exist
  ```
</details>

  
For any and all input, I'd be grateful.  
Cheers"
[HowTo] AR in validationset evaluation for TensorBoard ,open-mmlab/mmdetection,2022-06-20 11:35:16,5,,8223,1276790802,"Hi,

i really like mmdetection, it really makes live easier to train a variety of detectors and coco datasets.

What i am searching for a while is a way on how to get the AR metrics loggable in validation Eval Hooks like the Tensorboard or MlFlow using COCO datasets. The AP is already logged perfectly there. What do i need to configure at which spot to get the metrics also there? 

It looks like there should be a way to activate this , as one can set the Metrics to be sent out from the coco eval in the specific file: https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/coco.py#L574

If i also understood the codebase well enough, it is also possible to send arguments for validationset evaulation through the train.py api using the `evaluation`-dict:
https://github.com/open-mmlab/mmdetection/blob/master/mmdet/apis/train.py#L231

But i dont yet understand what i need to set in the specific training configuration to get those metrics finally in my tensorboard or other hooks. Can someone tell if this is possible or how to do it?

Thank you very much!"
"When i train the detr_r50_8*2_150e_coco.py on my own coco dataset, but the ap=0?",open-mmlab/mmdetection,2022-06-20 09:55:00,6,,8222,1276669185,"When i train the detr_r50_8*2_150e_coco.py on my own coco dataset, but the ap=0?
![bf2722a000595ab3fc884dc2dab3040](https://user-images.githubusercontent.com/86214036/174576698-5c79cb4e-4108-43a7-bf14-2f9e6935a7ac.png)
"
Swin Transformer作为主干网络的模型计算量问题,open-mmlab/mmdetection,2022-06-19 12:13:02,1,,8219,1276042533,为什么Swin Transformer模型相比较官方模型计算量小了很多？怎样修改的？
How to use mosaic in cascade mask rcnn?,open-mmlab/mmdetection,2022-06-19 02:40:11,2,community help wanted#planned feature,8218,1275935597,"I add 'mosaic' and 'randomaffine'
![image](https://user-images.githubusercontent.com/49092533/174463531-37ba585b-26e7-4018-bfd4-2fc86c7051d9.png)
and 'MultiImageMixDataset'.
![image](https://user-images.githubusercontent.com/49092533/174463557-e3e6fd0d-95dc-4a7f-bcf1-c8ebe2af61d8.png)
And then 
![image](https://user-images.githubusercontent.com/49092533/174463577-df8b4b27-8821-4474-ae23-166edba0dfa8.png)
How to solve it?"
Data Registration,open-mmlab/mmdetection,2022-06-17 13:18:34,4,reimplementation,8211,1275022132,"Dear MMDetection Team,
First I really appreciate your work of bringing everything at one place and helping the community.

My question might be very simple, but is it possible to explain the process of adding Custom Data a little more??
Specially how can it be registered coz I face xxxDataset not in registry and I haven't found anything well descriptive.
will be great help."
Partially freeze SWIN backbone,open-mmlab/mmdetection,2022-06-17 08:27:46,2,,8208,1274734562,"Hi, can I check how we can partially freeze a SWIN backbone? I've tried adding `frozen_stages=3` to the backbone in the config, but met with the following error: 

```None
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicate
s that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors
in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
24 25 26 27 28 29 30 31 32 325 326
```

Is there something else that needs to be set? Thank you!"
Adjusting the output format of a model (mask2former),open-mmlab/mmdetection,2022-06-15 20:47:23,3,,8199,1272718929,"I am currently about 90% of the way there in converting mask2former into torchscript.  The issue I am running into is how to adjust the output of the model as it outputs a list of numpy arrays that are of course incompatible with torchscript.  I so far have been unable to find the code relating to the model output in the mm detection code base.  My assumption is that since it is a pytorch model, it must be outputting a tensor and the conversion into numpy arrays is happening after the fact during a post-processing step.  If someone could point me to the spot where that it is happening it would be greatly appreciated!"
Error while trying to train the model for a different data set,open-mmlab/mmdetection,2022-06-15 12:33:10,2,,8197,1272166863,"I am trying to follow the tutorial and have written the following code:

import os.path as osp
import mmcv

import numpy as np 

import pdb

# The new config inherits a base config to highlight the necessary modification
_base_ = '/home/anam_zahra/QuantEx_Projects/mmdet/mmdetection/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py'

pdb.set_trace()


dataset_type = 'CocoDataset'
pdb.set_trace()
classes =  (""person"",""kitchenware"",""screen"",""animal"",""toy"",""book"",""object"")
pdb.set_trace()
# explicitly over-write all the `num_classes` field from default 80 to 5.

data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        classes=classes,
        img_prefix='/Processed_Data/Sample_DataSet/images/train',
        ann_file='/Processed_Data/Sample_DataSet/labels/train.json'),
    val=dict(
        type=dataset_type,
        classes=classes,
        img_prefix='/Processed_Data/Sample_DataSet/images/valid',
        ann_file='/Processed_Data/Sample_DataSet/labels/valid.json'),
    test=dict(
        type=dataset_type,
        classes=classes,
        img_prefix='/Processed_Data/Sample_DataSet/images/test',
        ann_file='/Processed_Data/Sample_DataSet/labels/test.json'))

pdb.set_trace()
model = dict(
    roi_head=dict(
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                # explicitly over-write all the `num_classes` field from default 80 to 7.
                num_classes=7),
            dict(
                type='Shared2FCBBoxHead',
                # explicitly over-write all the `num_classes` field from default 80 to 7.
                num_classes=7),
            dict(
                type='Shared2FCBBoxHead',
                # explicitly over-write all the `num_classes` field from default 80 to 7.
                num_classes=7)],
    # explicitly over-write all the `num_classes` field from default 80 to 7.
    mask_head=dict(num_classes=7)))
# We can use the pre-trained Mask RCNN model to obtain higher performance
pdb.set_trace()

load_from = '/mmdet/mmdetection/checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_2x_coco_bbox.pth'

And I get the following error: 
mmdet/lib/python3.6/site-packages/mmdet/datasets/samplers/group_sampler.py"", line 36, in __iter__
    indices = np.concatenate(indices)
  File ""<__array_function__ internals>"", line 6, in concatenate
ValueError: need at least one array to concatenate



Further report is as follows:2022-06-15 14:27:55,554 - mmdet - INFO - workflow: [('train', 1)], max: 8 epochs
2022-06-15 14:27:55,554 - mmdet - INFO - Checkpoints will be saved to /home/anam_zahra/QuantEx_Projects/mmdet/mmdetection/work_dirs/retrain_demo by HardDiskBackend.
Traceback (most recent call last):
  File ""tools/train.py"", line 242, in <module>
    main()
  File ""tools/train.py"", line 238, in main
    meta=meta)
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/mmdet/apis/train.py"", line 244, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py"", line 130, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/mmcv/runner/epoch_based_runner.py"", line 47, in train
    for i, data_batch in enumerate(self.data_loader):
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 359, in __iter__
    return self._get_iterator()
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 305, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 944, in __init__
    self._reset(loader, first_iter=True)
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 975, in _reset
    self._try_put_index()
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 1209, in _try_put_index
    index = self._next_index()
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 512, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/torch/utils/data/sampler.py"", line 229, in __iter__
    for idx in self.sampler:
  File ""/home/anam_zahra/QuantEx_Projects/mmdet/lib/python3.6/site-packages/mmdet/datasets/samplers/group_sampler.py"", line 36, in __iter__
    indices = np.concatenate(indices)
  File ""<__array_function__ internals>"", line 6, in concatenate
ValueError: need at least one array to concatenate


I was wondering if you can help please


"
Saving MMDet's installation in Google Drive ,open-mmlab/mmdetection,2022-06-14 12:31:10,1,community help wanted,8184,1270733261,"[Here ](https://colab.research.google.com/github/dudifrid/Swin-Transformer-Object-Detection/blob/master/demo/MMDet_Tutorial.ipynb)you can see a basic tutorial. It contains installations of a variety of libraries and of the repository from the gate, and then uses the installations for training and inference.
I've been trying for a few days to change the tutorial's notebook so that I do not have to reinstall everything there every time I want to train the model, but everything will already be installed in my Google Drive (as I use Google Collab and not my local machine, as it lacks a GPU).

Here's what I tried: [Here ](https://colab.research.google.com/drive/1o__owG1CtWHz55og89-1ZfR-xtSHP2vF?usp=sharing)the installations are done and saved to Drive. And [here ](https://colab.research.google.com/drive/1w_Hr4uJl0aGQbHLpwDu19qs0HgkiB4mP?usp=sharing)they are supposed to use it.
I would be happy for your help, because it does not work for me:
-The installations often do not work for me and I get an error that the mmdet or mmcv module does not exist (non-deterministically! There is something random here, but in most runs this is the case, although in some of the runs it does manage to find and to correctly use these modules!)
-And even when the installations do work for me, the use of it (the second file above) does not work for me (it says that the module terminaltables does not exist).

I would greatly appreciate your help!"
什么时候支持GHOSTnet模块,open-mmlab/mmdetection,2022-06-14 09:10:14,1,feature request,8183,1270496095,您好，请问一下有支持ghostnet模块的计划吗
Generalized Attention with config 0100 and 0001 ,open-mmlab/mmdetection,2022-06-13 16:10:20,2,,8179,1269662097,"Hello all , 

I would like to thank you first for such effort. second i would like to ask regards the Generalized Attention which is one of spatial attention mechanism as i noticed. 

I try to use it with different models rather than faster rcnn , it works perfectly when i did the config to be 1111 or 0010 (similar to what you already have in this repository ) but when i try another config like 0100 it does not working at all . 
it gave me such error massage : 

`RuntimeError: The size of tensor a (84) must match the size of tensor b (48) at non-singleton dimension 2
`

Actually , I did not get on with this error ,and did not understand the reason of it . 

can you please help ? 
"
Plot Confusion Matrix for each Individual Image in Test Dataset,open-mmlab/mmdetection,2022-06-13 15:56:48,1,,8178,1269646526,"Hi, is it possible to plot the confusion matrix (or calculate it) for each **individual image** in test/valid dataset and see the result? "
Initialize custom layer / module,open-mmlab/mmdetection,2022-06-13 15:00:24,2,,8177,1269575013,"Hello! mmlab!

I'm wondering how does this model initialize new layer?
I added my own module or layer. And I don't set specifin initialization onto my layer.

I want to know how this model initialize layer which is not included in model basically. (So it should be missing keys ~~ blah blah~~)

thanks"
Exported Mask-RCNN model shows worse accuracy with larger image sizes,open-mmlab/mmdetection,2022-06-13 08:11:23,4,,8175,1269068750,"Hi there, I have posted [this question](https://github.com/open-mmlab/mmdeploy/issues/552) on MMDeploy already and they told me to post it here. 

I am using an exported Mask-RCNN Model (ResNet-50, COCO pretrained) for inference on a Jetson AGX Xavier using TensorRT (I exported it using MMDeploy). It generally works fine. However I am realizing that the accuracy of the predictions on the images decays a lot with larger image sizes such as 3MP (1544x2064). 

I have recognized that the model performs best when the image dimensions are within a range of [800, 1344]. This is also written on [this page from ONNX on Mask-RCNN](https://github.com/onnx/models/tree/main/vision/object_detection_segmentation/mask-rcnn)

I was therefore asking myself why the accuracy of the predictions gets worse if I input higher resolution images. 

The [answer to my issue](https://github.com/open-mmlab/mmdeploy/issues/552#issuecomment-1147449791) suggests that this might be due to the anchor size and scales that might not cover the objects in my HR images. 
Can you confirm that this might be the problem? 
I am not an expert regarding anchor sizes... are these parameters that can be trained or how can I adjust them?

Thanks a lot!"
Some question about the analysis speed of model with fp16 ?,open-mmlab/mmdetection,2022-06-12 06:42:31,2,,8172,1268479537,"I have tested the loaded speed and analysis speed of model with fp16 and without fp16, the result sees below. I am confused the speed of model with fp16 is slower.
![1b896edcbdc02daab71f6601ac14011](https://user-images.githubusercontent.com/68552295/173220846-6fb3dbb5-89b6-45bd-b394-e6316edf172b.png)
"
Discobox evaluation failed with custom dataset - implementation based on mmdet,open-mmlab/mmdetection,2022-06-11 20:25:47,1,reimplementation,8170,1268366205,"Type of reimplementation - 
3) Reimplement a custom model but all the components are implemented in MMDetection - [DiscoBox](https://github.com/NVlabs/DiscoBox)

**Checklist**

1. I have searched related issues but cannot get the expected help - **Done**
2. The issue has not been fixed in the latest version. - **Not Sure**

**Describe the issue**

A clear and concise description of what the problem you meet and what have you done.

**Reproduction**

1. What command or script did you run?

```
bash tools/dist_train.sh configs/discobox/custom_discobox_solov2_r50_fpn_3x.py 2
```

2. What config dir you run?

```
fp16 = dict(loss_scale=512.)
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
model = dict(
    type='DiscoBoxSOLOv2',
    pretrained='torchvision://resnet50',
    train_cfg=dict(),
    test_cfg = dict(
        nms_pre=500,
        score_thr=0.1,
        mask_thr=0.4,
        update_thr=0.05,
        kernel='gaussian',  # gaussian/linear
        sigma=2.0,
        max_per_img=100),
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3), # C2, C3, C4, C5
        frozen_stages=1,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        start_level=0,
        num_outs=5),
    bbox_head=dict(
        type='DiscoBoxSOLOv2Head',
        num_classes=6,
        in_channels=256,
        stacked_convs=4,
        seg_feat_channels=512,
        strides=[8, 8, 16, 32, 32],
        scale_ranges=((1, 96), (48, 192), (96, 384), (192, 768), (384, 2048)),
        sigma=0.2,
        num_grids=[40, 36, 24, 16, 12],
        ins_out_channels=256,
        loss_ins=dict(
            type='DiceLoss',
            use_sigmoid=True,
            loss_weight=1.0),
        loss_ts=dict(
            type='DiceLoss',
            momentum=0.999,
            use_ind_teacher=True,
            loss_weight=1.0,
            kernel=3,
            max_iter=10,
            alpha0=2.0,
            theta0=0.5,
            theta1=30.0,
            theta2=20.0,
            base=0.10,
            crf_height=28,
            crf_width=28,
        ),
        loss_cate=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.0),
        loss_corr=dict(
            type='InfoNCE',
            loss_weight=1.0,
            corr_exp=1.0,
            corr_eps=0.05,
            gaussian_filter_size=3,
            low_score=0.3,
            corr_num_iter=10,
            corr_num_smooth_iter=1,
            save_corr_img=False,
            dist_kernel=9,
            obj_bank=dict(
                img_norm_cfg=img_norm_cfg,
                len_object_queues=100,
                fg_iou_thresh=0.7,
                bg_iou_thresh=0.7,
                ratio_range=[0.9, 1.2],
                appear_thresh=0.7,
                min_retrieval_objs=2,
                max_retrieval_objs=5,
                feat_height=7,
                feat_width=7,
                mask_height=28,
                mask_width=28,
                img_height=200,
                img_width=200,
                min_size=32,
                num_gpu_bank=20,
            )
        )
    ),
    mask_feat_head=dict(
            type='DiscoBoxMaskFeatHead',
            in_channels=256,
            out_channels=128,
            start_level=0,
            end_level=3,
            num_classes=256,
            norm_cfg=dict(type='GN', num_groups=32, requires_grad=True)),
    )

# dataset settings
dataset_type = 'CocoDataset'
data_root = 'data/'
classes = ('a','b','c','d','e','f',)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='GenerateBoxMask'),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=0,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/train.json',
        img_prefix=data_root + 'train/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/val.json',
        img_prefix=data_root + 'val/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/test.json',
        img_prefix=data_root + 'test/',
        pipeline=test_pipeline))
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=2000,
    warmup_ratio=0.01,
    step=[8, 9])
checkpoint_config = dict(interval=1)
# yapf:disable
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook')
    ])
# yapf:enable
# runtime settings
runner = dict(type='EpochBasedRunner', max_epochs=20)
evaluation = dict(interval=1, metric=['bbox', 'segm'])
device_ids = range(8)
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = './work_dirs/exp1'
load_from = None
resume_from = None
workflow = [('train', 1)]
```

3. Did you make any modifications on the code or config? Did you understand what you have modified?
**Modified the custom config for custom Coco-style datasets with 6 classes**

4. What dataset did you use?
**Custom coco style dataset with bbox and segmentation annotations**

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.

```
sys.platform: linux
Python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) [GCC 9.4.0]
CUDA available: True
GPU 0,1: Tesla P100-PCIE-16GB
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.3.r11.3/compiler.29920130_0
GCC: gcc (Debian 8.3.0-6) 8.3.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.6.0
MMCV: 1.3.17
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.1
MMDetection: 2.25.0+5bdb4ee
```

2. You may add addition that may be helpful for locating the problem, such as
   1. How you installed PyTorch \[e.g., pip, conda, source\] - **conda**

**Results**

If applicable, paste the related results here, e.g., what you expect and what you get.
Expected behaviour - run training and evaluation
Error during evaluation
```
  [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>                     ] 166/279, 1.3 task/s, elapsed: 124s, ETA:    85sTraceback (most recent call last):
  File ""/opt/conda/envs/open-mmlab/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/envs/open-mmlab/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/envs/open-mmlab/lib/python3.7/site-packages/torch/distributed/launch.py"", line 261, in <module>
    main()
  File ""/opt/conda/envs/open-mmlab/lib/python3.7/site-packages/torch/distributed/launch.py"", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/open-mmlab/bin/python', '-u', 'tools/test.py', '--local_rank=0', 'configs/discobox/custom_solov2_r50_fpn_3x.py', 'work_dirs/roboflow_data/epoch_1.pth', '--launcher', 'pytorch', '--eval', 'bbox', 'segm']' died with <Signals.SIGKILL: 9>.
```

Any help will be appreciated!
"
adding features,open-mmlab/mmdetection,2022-06-09 21:01:01,1,reimplementation,8163,1266653177,"I would like to try to add some attributes between bounding boxes myself, such as lines and calculations (like the existing probabilities). Which files can I do that? What are the general algorithms or procedures can be done? Many Thanks."
build_dp 非常慢,open-mmlab/mmdetection,2022-06-09 09:57:29,2,,8158,1265888310,"hi，在util_distribution.py文件中，build_dp中MMDataParallel非常慢，在rtx3060要等待30分钟才能往下走，单个gpu，为什么这样？
"
Get the confusion matrix,open-mmlab/mmdetection,2022-06-08 16:21:35,3,,8153,1265002114,"Hi. I was wondering how to get the confusion matrix during the training and saving the results (without using test.py). Here is my code to train DetectoRS on a custom dataset:

````
# Applying DetectoRS (Cascade + ResNet-50)

# Read the data 
with zipfile.ZipFile('/content/sample_data/Modified_Classes Dataset_ObjectDetection.v2i.coco.zip', 'r') as zip_ref:
# with zipfile.ZipFile('/content/sample_data/Modified_Classes.v3i.coco-segmentation.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/sample_data/data/')

from mmcv import Config
from mmdet.apis import set_random_seed

# Make the checkpoints
!mkdir checkpoints
!wget -c https://download.openmmlab.com/mmdetection/v2.0/detectors/detectors_cascade_rcnn_r50_1x_coco/detectors_cascade_rcnn_r50_1x_coco-32a10ba0.pth \
      -O checkpoints/detectors_cascade_rcnn_r50_1x_coco-32a10ba0.pth
config_path='/content/mmdetection/configs/detectors/detectors_cascade_rcnn_r50_1x_coco.py'
load_path='/content/mmdetection/checkpoints/detectors_cascade_rcnn_r50_1x_coco-32a10ba0.pth'


cfg = Config.fromfile(config_path)  
cfg.load_from = load_path

cfg.dataset_type = 'CocoDataset'
cfg.classes = ('Less_Thick','Thick','Thin')
cfg.data_root = '/content/sample_data/data'

cfg.model.roi_head.bbox_head[0]['num_classes']=3
cfg.model.roi_head.bbox_head[1]['num_classes']=3
cfg.model.roi_head.bbox_head[2]['num_classes']=3

cfg.data.test.classes = ('Less_Thick_BN','Thick_BN','Thin_BN')
cfg.data.test.ann_file = '/content/sample_data/data/test/_annotations.coco.json'
cfg.data.test.img_prefix = '/content/sample_data/data/test/'


cfg.data.train.ann_file = '/content/sample_data/data/train/_annotations.coco.json'
cfg.data.train.img_prefix = '/content/sample_data/data/train/'
cfg.data.train.classes = ('Less_Thick','Thick','Thin')


cfg.data.val.ann_file = '/content/sample_data/data/valid/_annotations.coco.json'
cfg.data.val.img_prefix = '/content/sample_data/data/valid/'
cfg.data.val.classes = ('Less_Thick','Thick','Thin')

cfg.work_dir = './tutorial_exps3'

# The original learning rate (LR) is set for 8-GPU training.
# You divide it by 8 since you only use one GPU with Kaggle.
cfg.optimizer.lr = 0.02 / 8
cfg.lr_config.warmup = None
cfg.log_config.interval = 10

# We can set the evaluation interval to reduce the evaluation times
cfg.evaluation.interval = 1
# We can set the checkpoint saving interval to reduce the storage cost
cfg.checkpoint_config.interval = 1
cfg.runner.max_epochs = 5

# Set seed thus the results are more reproducible
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.gpu_ids = range(1)

cfg.device='cuda' # added myself since working on cpu

# We can also use tensorboard to log the training process
cfg.log_config.hooks = [
    dict(type='TextLoggerHook'),
    dict(type='TensorboardLoggerHook')]

# We can initialize the logger for training and have a look
# at the final config used for training
print(f'Config:\n{cfg.pretty_text}')


# Train our model (DetectoRS)
import mmcv
import os.path as osp

import os
from mmdet.datasets import build_dataset
from mmdet.models import build_detector
from mmdet.apis import train_detector

# Build dataset
datasets = [build_dataset(cfg.data.train)]

# Build the detector
model = build_detector(cfg.model)

# Add an attribute for visualization convenience
model.CLASSES = datasets[0].CLASSES

# Create work_dir
mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))
train_detector(model, datasets, cfg, distributed=False, validate=True)
```"
Minimum IoF overlap in RandomCrop Transform,open-mmlab/mmdetection,2022-06-07 14:27:01,1,feature request,8149,1263396071,"**Usecase: support fine control over foreground/background training samples with RandomCrop**

When allow_negative_crop == False current behavior returns None when the random crop location does not contain bbox foreground.  This can result in many images being skipped when foreground is sparse and image sizes are large relative to the crop size e.g. when training models for sliding window inference.  Skipping images adds unintended sampling bias towards images with more foreground coverage.  It is also time consuming to load the next image when one is skipped.  

It isn't currently possible to use RandomCrop to allow_negative_crop by a probability to enable the model to learn from a balanced sampling of background.    

MinIoURandomCrop gives fine control over sampling foreground/background by setting min_ious to replicate a probability of foreground/background.  MinIoURandomCrop does not support different crop types like: 'relative_range', 'relative', 'absolute', 'absolute_range'.  MinIoURandomCrop uses IoU a box center point check rather than IoF.  IoF has the benefit of putting the overlap calculation in terms of the foreground e.g. an image with a minimum .5 IoF threshold will contain an instance with at least half its area within the crop. 

 If the maintainers have an interest in supporting this functionality in RandomCrop I have an implementation for a PR.   

[The relevant code is here](https://github.com/PeterVennerstrom/mmdetection/blob/59eb0f147401b84d960e46c98d77617eec4a56dc/mmdet/datasets/pipelines/transforms.py#L814-L869).  
- allow_negative_crop takes a bool or float as argument.
- When allow_negative_crop is False (either defined as False in the argument or set to False by probability in the code) margin intervals likely to contain instance(s) with the desired min_overlap (IoF) are calculated and randomly sampled from. 
- The resulting crop window is compared by IoF with the bboxes in a loop
- If no foreground is found the method returns None as before
"
How to test a model on a custom dataset?,open-mmlab/mmdetection,2022-06-07 06:25:14,2,community help wanted,8146,1262783377,"Thanks for the amazing repo!! 

I have fine-tuned a model using my custom `train` and `Val` datasets. I have the config file as well as the weights file of the model.
I would like to test the model on unseen images in my `test` dataset.  Is it possible to use /tools/test.py to test a model on a custom dataset? If so how should I go about doing that?

Many thanks,
Dave"
deadlock using Wandb,open-mmlab/mmdetection,2022-06-07 04:00:02,15,bug,8145,1262688755,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
Hello mmdet developers,

We found the training loop can be dead lock in some places if we use multiGPU training and enable wandb tracking. Single GPU works perfectly fine. I only tested with YOLOX. Please see the command below.


**Reproduction**

1. What command or script did you run?


```
./tools/dist_train.sh ./configs/yolox/yolox_s_8x8_300e_coco.py 2
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
No
3. What dataset did you use?
MSCOCO

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0,1: Quadro GV100
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.3.r11.3/compiler.29745058_0
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.10.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.0
OpenCV: 4.5.5
MMCV: 1.4.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.25.0+ca11860

2. You may add addition that may be helpful for locating the problem, such as
   - How you installed PyTorch \[e.g., pip, conda, source\]
   - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)
   We used the provided docker.

**Error traceback**
If applicable, paste the error trackback here.

```none
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
cannot import name 'SENet' from 'mmdet.models',open-mmlab/mmdetection,2022-06-06 10:00:11,1,reimplementation,8143,1261624240,"我已经在mmdet/models/backbones中建立了senet50.py,里面也对SENet进行了register
并且在mmdet/models/__init__.py中导入了SENet类
不过运行from mmdet.models import SENet出现上面这个错误,请问怎样解决？
![image](https://user-images.githubusercontent.com/43646121/172139672-7ed463ce-cd50-4e04-9125-43dc04894b46.png)
"
How can I get the proposals' level0 feature without ROI_Align or ROI_pooling?,open-mmlab/mmdetection,2022-06-06 07:57:55,3,,8141,1261488472,"I want to get the original feature map of a proposal, I found the ROI_head code and mmcv/ops/roi_align.py, but I can't tell where is the original feature map of a proposal. Could you tell me where it is?"
"Why i had  set mmdet/datasets/coco.py iou_thrs=[0.50], but it occur map=-1?",open-mmlab/mmdetection,2022-06-06 02:05:31,3,,8139,1261251993,"Hello!
I had set (def evaluate_det_segm、def evaluate)：iou_thrs=[0.50] like this
![0cdc3015b33ffc8d00b3b1f35c9b631](https://user-images.githubusercontent.com/86214036/172093668-2b4cb8ad-6f06-4007-9672-ed0330229ea2.jpg)

but it occer the map = -1?
![Inked2022-06-06 11-48-09屏幕截图_LI](https://user-images.githubusercontent.com/86214036/172093653-fb278768-2678-4350-a551-930e41f32133.jpg)

"
"How to set different optimizers (e.g., SGD and AdamW) for various modules (e.g., backbone, neck, and head)?",open-mmlab/mmdetection,2022-06-04 14:26:39,1,How-to,8130,1260774812,"Hi, 
as described in my topic, I want to set up different optimizers for different modules. How can I achieve this effectively?
I would be really appreciative if you could help me. Thank you very much! "
DCN_v2 implementation question,open-mmlab/mmdetection,2022-06-03 13:17:57,1,,8126,1259906048,"Hi,
I was going through the DCN code [here](https://github.com/open-mmlab/mmdetection/blob/3c021b1167095767c7be32d3007b57ed71753f80/mmdet/ops/dcn/src/cuda/deform_conv_cuda_kernel.cu).
I've two questions in this:
1) In the [line](https://github.com/open-mmlab/mmdetection/blob/3c021b1167095767c7be32d3007b57ed71753f80/mmdet/ops/dcn/src/cuda/deform_conv_cuda_kernel.cu#L594), it is written that
```
const int h_in = h_col * stride_h - pad_h;
const int w_in = w_col * stride_w - pad_w;
``` 
I guess this comes from the formula relating input to the output after convolution `[h_col,w_col] = [floor((h_in + 2*pad_h - kernel_h)/stride_h + 1), floor((w_in + 2*pad_w - kernel_w)/stride_w + 1)]`, assuming `(h_in + pad_h)` is divisible by `stride_h` and dilation=1. So, my question is, why is the dilation was considered as 1 ?

2) In the [line](https://github.com/open-mmlab/mmdetection/blob/3c021b1167095767c7be32d3007b57ed71753f80/mmdet/ops/dcn/src/cuda/deform_conv_cuda_kernel.cu#L615), it is written that
```
const scalar_t h_im = h_in + i * dilation_h + offset_h;
const scalar_t w_im = w_in + j * dilation_w + offset_w;
``` 
Why was dilation considered here? What does `h_im` and `w_im` represent here and where does this equation come from ?"
Question about Data parrel cuda out of memory problems.,open-mmlab/mmdetection,2022-06-03 05:46:19,1,,8124,1259453351,"Hello, Thanks for your great work.

I have a question when I use **pretrained**.
As you may know, when I use load stat_dict, One of the multi gpu should load the state dict.
In that case, There must be inbalace of using GPU.

For example, I have 5 1080ti gpus.
And If I load state dict, The first gpu should load to rest of the 4 gpus.
The first gpu must be overloaded and sometimes it happens CUDA out of memory problem.

I want to seperate those loading memory to all gpus not only for one gpus.

What should I do? or Is there any solution??

Thanks.
"
Potential bugs when mmdetection runs on PyTorch < 1.8,open-mmlab/mmdetection,2022-06-01 12:27:00,5,bug,8110,1255867761,"Thanks for your error report and we appreciate it a lot.

**Describe the bug**

There is a small [bug](https://github.com/pytorch/pytorch/issues/5059) of  PyTorch (version < 1.8).

**In short, when we use `DataLoadr` with `num_workers != 0`, these forked children process share the same `numpy` random seeds.**

A toy example, run with PyTorch 1.6.

```
import numpy as np
from torch.utils.data import Dataset, DataLoader

class RandomDataset(Dataset):
    def __getitem__(self, index):
        return np.random.randint(0, 1000, 3)

    def __len__(self):
        return 8

dataset = RandomDataset()
dataloader = DataLoader(dataset, batch_size=2, num_workers=2)
for batch in dataloader:
    print(batch)
```

Output:

```
tensor([[437, 650, 998],  # process 0
        [108,  34, 197]])
tensor([[437, 650, 998],  # process 1
        [108,  34, 197]])
tensor([[153, 629, 103],  # process 0
        [695, 102, 728]])
tensor([[153, 629, 103],  # process 1
        [695, 102, 728]])
```

Currently, [mmdetection sets different `numpy` seeds for different workers according to the `worker_id`](https://github.com/open-mmlab/mmdetection/blob/240d7a31c745578aa8c4df54c3074ce78b690c34/mmdet/datasets/builder.py#L209). However, the worker processes are killed at the end of each epoch, and all worker resources are lost. At the next epoch, mmdetection will set the same seeds as the previous epoch.

Example:

```
import numpy as np
from torch.utils.data import Dataset, DataLoader

class RandomDataset(Dataset):
    def __getitem__(self, index):
        return np.random.randint(0, 1000, 3)

    def __len__(self):
        return 8

def worker_init_fn(worker_id):
    np.random.seed(worker_id)

dataset = RandomDataset()
dataloader = DataLoader(dataset, batch_size=2, num_workers=2, worker_init_fn=worker_init_fn)

print(""the first epoch is ok"")
for batch in dataloader:
    print(batch)

print(""the second epoch is the same as the first epoch"")
for batch in dataloader:
    print(batch)
```

Output:

```
the first epoch is ok

tensor([[684, 559, 629],
        [192, 835, 763]])
tensor([[ 37, 235, 908],
        [ 72, 767, 905]])
tensor([[707, 359,   9],
        [723, 277, 754]])
tensor([[715, 645, 847],
        [960, 144, 129]])

the second epoch is the same as the first epoch

tensor([[684, 559, 629],
        [192, 835, 763]])
tensor([[ 37, 235, 908],
        [ 72, 767, 905]])
tensor([[707, 359,   9],
        [723, 277, 754]])
tensor([[715, 645, 847],
        [960, 144, 129]])
```

Maybe we can use the solution of [detectron2](https://github.com/facebookresearch/detectron2/blob/523c4023bc79719d2fc3fe0181311f88ad35a271/detectron2/data/build.py#L554) or [pytorch-image-models](https://github.com/rwightman/pytorch-image-models/blob/e4360e6125bb0bb4279785810c8eb33b40af3ebd/timm/data/loader.py#L149). They use a seed that changed dynamically with epoch, i.e., `torch.initial_seed()`, `torch.utils.data.get_worker_info().seed`.

Relevent issue: https://github.com/pytorch/pytorch/issues/5059

**Environment**

PyTorch 1.6.0
"
Inconsistent behavior of the test script in mmdet vs mmdeploy,open-mmlab/mmdetection,2022-05-30 13:08:24,2,,8092,1252718483,"Just a discussion.

A mmdeploy user reported `Inconsistent behavior of the test script in mmdetection vs mmdeploy`, check this issue: https://github.com/open-mmlab/mmdeploy/issues/513

I think mmdeploy meets the original needs of model trainer: prioritize user input.  

What do you think about it ?"
rpn head中将gts作为proposals时，num_valid_anchors是不是也应该加上num_gts?,open-mmlab/mmdetection,2022-05-30 06:14:39,1,,8086,1252244200,
"## **I checked and this is predictive, but why can't it generate a thermal map?**",open-mmlab/mmdetection,2022-05-30 04:39:59,3,,8085,1252180708,"## **I checked and this is predictive, but why can't it generate a thermal map?**
load checkpoint from local path: /content/gdrive/MyDrive/mmdetection/bighead-mask_rcnn/mask_rcnn_r101_fpn_mstrain-poly_3x_coco_20210524_200244-5675c317.pth
/content/gdrive/MyDrive/mmdetection1/mmdet/datasets/utils.py:70: UserWarning: ""ImageToTensor"" pipeline is replaced by ""DefaultFormatBundle"" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.
  'data pipeline in your config file.', UserWarning)
Traceback (most recent call last):
  File ""demo/vis_cam.py"", line 238, in <module>
    main()
  File ""demo/vis_cam.py"", line 219, in main
    eigen_smooth=args.eigen_smooth)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/utils/det_cam_visualizer.py"", line 327, in __call__
    return self.cam(img, targets, aug_smooth, eigen_smooth)[0, :]
  File ""/usr/local/lib/python3.7/dist-packages/pytorch_grad_cam/base_cam.py"", line 185, in __call__
    targets, eigen_smooth)
  File ""/usr/local/lib/python3.7/dist-packages/pytorch_grad_cam/base_cam.py"", line 74, in forward
    outputs = self.activations_and_grads(input_tensor)
  File ""/usr/local/lib/python3.7/dist-packages/pytorch_grad_cam/activations_and_gradients.py"", line 42, in __call__
    return self.model(x)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/utils/det_cam_visualizer.py"", line 176, in __call__
    loss = self.detector(return_loss=True, **self.input_data)
  File ""/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/mmcv/runner/fp16_utils.py"", line 110, in new_func
    return old_func(*args, **kwargs)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/models/detectors/base.py"", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/models/detectors/two_stage.py"", line 150, in forward_train
    **kwargs)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/models/roi_heads/standard_roi_head.py"", line 113, in forward_train
    gt_masks, img_metas)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/models/roi_heads/standard_roi_head.py"", line 173, in _mask_forward_train
    self.train_cfg)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/models/roi_heads/mask_heads/fcn_mask_head.py"", line 144, in get_targets
    gt_masks, rcnn_train_cfg)
  File ""/content/gdrive/MyDrive/mmdetection1/mmdet/core/mask/mask_target.py"", line 60, in mask_target
    pos_assigned_gt_inds_list, gt_masks_list, cfg_list)
TypeError: 'NoneType' object is not iterable

_Originally posted by @BigHeadShang in https://github.com/open-mmlab/mmdetection/issues/7987#issuecomment-1140676877_"
RuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same,open-mmlab/mmdetection,2022-05-29 08:24:57,1,bug,8081,1251824155,"
lad/lad_r101_paa_r50_fpn_coco_1x.py

```shell
    train_detector(
  File ""/project/mmlab_template/mmdetection/mmdet/apis/train.py"", line 208, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 50, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 29, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/mmcv/parallel/data_parallel.py"", line 75, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/project/mmlab_template/mmdetection/mmdet/models/detectors/base.py"", line 248, in train_step
    losses = self(**data)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 139, in new_func
    output = old_func(*new_args, **new_kwargs)
  File ""/project/mmlab_template/mmdetection/mmdet/models/detectors/base.py"", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/project/mmlab_template/mmdetection/mmdet/models/detectors/lad.py"", line 80, in forward_train
    x_teacher = self.extract_teacher_feat(img)
  File ""/project/mmlab_template/mmdetection/mmdet/models/detectors/lad.py"", line 49, in extract_teacher_feat
    x = self.teacher_model.backbone(img)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/project/mmlab_template/mmdetection/mmdet/models/backbones/resnet.py"", line 636, in forward
    x = self.conv1(x)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/torch/nn/modules/conv.py"", line 443, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File ""/home/elaine/miniconda3/envs/cvmart/lib/python3.9/site-packages/torch/nn/modules/conv.py"", line 439, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.FloatTensor) should be the same
```"
Panoptic FPN support for Cityscapes,open-mmlab/mmdetection,2022-05-28 23:38:54,1,community help wanted#feature request,8078,1251745856,"Hi, does Panoptic FPN supports training on CItyscapes dataset?"
Deformable DETR——Error docstring in `DeformableDetrTransformer.get_reference_points()`,open-mmlab/mmdetection,2022-05-27 09:37:29,2,community discussion,8066,1250555851,"I think [the docstring here](https://github.com/open-mmlab/mmdetection/blob/240d7a31c745578aa8c4df54c3074ce78b690c34/mmdet/models/utils/transformer.py#L832) is wrong. The reference points from `get_reference_points` should be used in encoder.

In DeformableDetrTransformer, the relative code is copied here: 
```
       reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,     # here is the reference point used in encoder
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)
```

In default one stage mode, the reference points used in decoder are gain from `self.reference_points()` which is defined in `__init__`,  `self.reference_points = nn.Linear(self.embed_dims, 2)`.

```
        query_pos, query = torch.split(query_embed, c, dim=1)
        query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
        query = query.unsqueeze(0).expand(bs, -1, -1)
        reference_points = self.reference_points(query_pos).sigmoid()
        init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=reg_branches,
            **kwargs)

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out,\
                inter_references_out, enc_outputs_class,\
                enc_outputs_coord_unact
        return inter_states, init_reference_out, \
            inter_references_out, None, None

```


"
Running Torchserve failure,open-mmlab/mmdetection,2022-05-27 09:04:06,1,community help wanted,8065,1250523979,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.
```
I followed the guide building a  mmdet Torchserve, however there seemed to be something wrong.
```

**Reproduction**

1. What command or script did you run?

First, I changed the dockerfile, replacing like apt/pip sources, in order to speed up the build procedure.
```none
ARG PYTORCH=""1.6.0""
ARG CUDA=""10.1""
ARG CUDNN=""7""
FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel

ARG MMCV=""1.3.17""
ARG MMDET=""2.23.0""

ENV PYTHONUNBUFFERED TRUE

RUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list
RUN apt-get clean
RUN rm /etc/apt/sources.list.d/cuda.list
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y \
    ca-certificates \
    g++ \
    openjdk-11-jre-headless \
    # MMDet Requirements
    ffmpeg libsm6 libxext6 git ninja-build libglib2.0-0 libsm6 libxrender-dev libxext6 \
    && rm -rf /var/lib/apt/lists/*

ENV PATH=""/opt/conda/bin:$PATH""
RUN export FORCE_CUDA=1

# TORCHSEVER
RUN pip install torchserve torch-model-archiver nvgpu -i https://pypi.tuna.tsinghua.edu.cn/simple/

# MMLAB
ARG PYTORCH
ARG CUDA
RUN [""/bin/bash"", ""-c"", ""pip install mmcv-full==${MMCV} -f https://download.openmmlab.com/mmcv/dist/cu${CUDA//./}/torch${PYTORCH}/index.html -i https://pypi.tuna.tsinghua.edu.cn/simple/""]
RUN pip install mmdet==${MMDET} -i https://pypi.tuna.tsinghua.edu.cn/simple/

RUN useradd -m model-server \
    && mkdir -p /home/model-server/tmp

COPY entrypoint.sh /usr/local/bin/entrypoint.sh

RUN chmod +x /usr/local/bin/entrypoint.sh \
    && chown -R model-server /home/model-server

COPY config.properties /home/model-server/config.properties
RUN mkdir /home/model-server/model-store && chown -R model-server /home/model-server/model-store

EXPOSE 8080 8081 8082

## 清理安装文件缓存
#RUN apt-get clean \
#&& conda clean -y --all \
#&& rm -rf /tmp/* /var/tmp/* \
#&& rm -rf ~/.cache/pip/* \
#&& rm -rf /var/lib/apt/lists/*


USER model-server
WORKDIR /home/model-server
ENV TEMP=/home/model-server/tmp
ENTRYPOINT [""/usr/local/bin/entrypoint.sh""]
CMD [""serve""]
```

Then, after the docker image generated, I built a model using the pretrained model of YOLOX-S

Then I ran the docker container with:

```
docker run --rm -p 8080:8080 -p 8081:8081 -p 8082:8082 --mount type=bind,source=/home/server/Pycharmprojects/model_store,target=/home/model-server/model-store mmdet-serve:latest

```
I didn't use the """"""--gpus"""""" because the nvidia-docker is another version.

When doing inference I found an error :
```
{
  ""code"": 404,
  ""type"": ""ResourceNotFoundException"",
  ""message"": ""Requested resource is not found, please refer to API document.""
}
```

I found that ""nvgpu"" is not installed and I fixed it.

However when I restart the torchserve in the container, It still showed the same problem.
"
the min_overlap(0.3) is different from CenterNet Code(0.7),open-mmlab/mmdetection,2022-05-24 08:55:35,2,community discussion,8036,1246220947,"https://github.com/open-mmlab/mmdetection/blob/4bd4ace53a626659070204236da5b8e14ae9cd08/mmdet/models/dense_heads/centernet_head.py#L229
Hi, I do not understand why change the min_overlap value, Could you tell me why?"
TypeError: TwoStageDetectionAUGFPN: __init__() got an unexpected keyword argument 'roi_head',open-mmlab/mmdetection,2022-05-23 03:14:38,3,,8028,1244516284,"Thanks for your error report and we appreciate it a lot.

**Describe the bug**
```none
model = dict(
    type='TwoStageDetectorAUGFPN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='HighFPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    bbox_roi_extractor=dict(
        type='StandardRoIHead',
        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),
        out_channels=256,
        featmap_strides=[4, 8, 16, 32]),
    bbox_head=dict(
        type='Shared2FCBBoxHead',
        num_fcs=2,
        in_channels=256,
        fc_out_channels=1024,
        roi_feat_size=7,
        num_classes=81,
        target_means=[0.0, 0.0, 0.0, 0.0],
        target_stds=[0.1, 0.1, 0.2, 0.2],
        reg_class_agnostic=False),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            use_consistent_supervision=True,
            alpha=0.25,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_across_levels=False,
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            use_consistent_supervision=True)),
    roi_head=dict(bbox_head=dict(num_classes=8)))
dataset_type = 'CocoDataset'
data_root = 'datas/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CocoDataset',
        ann_file='datas/train.json',
        img_prefix='datas/images/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        classes=('Crack', 'Manhole', 'Net', 'Pothole', 'Patch-Crack',
                 'Patch-Net', 'Patch-Pothole', 'other')),
    val=dict(
        type='CocoDataset',
        ann_file='datas/train.json',
        img_prefix='datas/images/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Crack', 'Manhole', 'Net', 'Pothole', 'Patch-Crack',
                 'Patch-Net', 'Patch-Pothole', 'other')),
    test=dict(
        type='CocoDataset',
        ann_file='datas/train.json',
        img_prefix='datas/images/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('Crack', 'Manhole', 'Net', 'Pothole', 'Patch-Crack',
                 'Patch-Net', 'Patch-Pothole', 'other')))
evaluation = dict(interval=1, metric='bbox')
optimizer = dict(type='SGD', lr=0.025, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=100)
checkpoint_config = dict(interval=4)
log_config = dict(interval=10, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
work_dir = './work_dirs/load_aug_50'
gpu_ids = range(0, 1)


**Traceback (most recent call last):
  File ""/home/z/miniconda3/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 51, in build_from_cfg
    return obj_cls(**args)
TypeError: __init__() got an unexpected keyword argument 'roi_head'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tools/train.py"", line 188, in <module>
    main()
  File ""tools/train.py"", line 161, in main
    test_cfg=cfg.get('test_cfg'))
  File ""/home/z/miniconda3/lib/python3.7/site-packages/mmdet-2.14.0-py3.7.egg/mmdet/models/builder.py"", line 58, in build_detector
    cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))
  File ""/home/z/miniconda3/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 210, in build
    return self.build_func(*args, **kwargs, registry=self)
  File ""/home/z/miniconda3/lib/python3.7/site-packages/mmcv/cnn/builder.py"", line 26, in build_model_from_cfg
    return build_from_cfg(cfg, registry, default_args)
  File ""/home/z/miniconda3/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 54, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
TypeError: TwoStageDetectorAUGFPN: __init__() got an unexpected keyword argument 'roi_head'**
```

**Reproduction**

1. What command or script did you run?

```none
python tools/train.py configs/faster_rcnn/load_aug_50.py --gpus 1
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?


Using some of the models like 'HighFPN', 'two_stage' in  https://github.com/Gus-Guo/AugFPN 
'https://github.com/Gus-Guo/AugFPN' is also in MMdetection format


3. What dataset did you use?

A public data set in coco format

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.

Python: 3.7.0 (default, Oct  9 2018, 10:31:47) [GCC 7.3.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.8.0 
TorchVision: 0.9.0
OpenCV: 4.5.5
MMCV: 1.3.9
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMDetection: 2.14.0+



**Error traceback**
If applicable, paste the error trackback here.

```none
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
About segm.json for mask-rcnn,open-mmlab/mmdetection,2022-05-22 07:13:52,3,,8026,1244168331,"When I output the test result in json file, the result in results.segm.json will looks like it:

{""image_id"": 0, ""bbox"": [57.16876220703125, 217.72210693359375, 689.4974975585938, 208.8153076171875], ""score"": 0.9798983335494995, ""category_id"": 1, ""segmentation"": {""size"": [434, 914], ""counts"": ""Vmh0R1]<5L4K5L3M2O101N100O2O000O101O000O101O001N10001N2O0O2O001N101O00000O2O00000O2O000O2O000O2N101N1O2O0O1O2O000O101N1000001N100000001O00001O0O101O0HPMnEQ3Q:TMjEl2U:901N10001N1000001N100000010O0001hLoEn2P:QMRFn2o9oLTFP3l9oLWFP3i9mLZFT3R:0014K5K;E2M2O2M6KO0100O0010O0001O001UNbDa1^;\\NdDd1\\;YNgDg1c;O1O001O001O1O1O1O001O10O01O001O001O000001O0001O01O000000001O0001O00000001O000001O0000000000O1000000000000000000O100000000000000000000O100000000000000O100000000000000O100000000000000O10000000000001O00000000000000000000000000000000000000001O01O0001O001O00001O001O0000001O000000001O00000000000000001O0000000000001O0000000000001O0000000000000000000000000000000000000000000000000000000000O10000000000000O100000000000000000000000000000O100000000000000O10000000000O10000000000O1000000O10000O1O100O1O1O1O1O1N2O1O1O1O1N2O1O100O1O1O100O100O100N2O1O1N2O1O1O100O100O010O100O100O10000O100O1000O10O1000O10O1000O1000O10O10000000O010000000O1000O1000O100000O01000O0100O010OO2N1O2N2O0100O01000O1000O0100000O01000O010O100O00100O10O01000O0100000O10O10O100O00100O001N1000N3M2I8O010O10O010000O0100000O010000O010M3K401O0000001O000010O01O010O010O010O10O10O010O00010O01O0O1N3MGlMYEU2g:lMXES2h:PNUEP2l:QNSEo1l:SNREm1o:;00O010E_MeEa2Z:`MeEa2[:`MdE_2]:aMcE_2\\:cMcE]2]:dMbE\\2]:eMbE[2_:fM`EZ2_:hM`EX2_:jM_EV2b:=O000N2M3O10O1001O01O01O10O010O10O1000O010O10O01O001N1O2M2J6O2N2N10100O0010000O10O11N10000O100O100O1O1O1N2O1O1O100O100O10000O10000O10000O100O100O1O1O1O100O10000O100000000000O01000001O000000000O101O000O101M2N2fNPDP1Z<O1O10001N100O10000O1O2N1O1N3K4G:M3N1N3N2Mh\\W2""}}

So what is the mean of segmentation"" part? If I want to get something like this, what should I do? 
""segmentation"":[
[
1485.8382858000696,
497.0831564962864,
2538.697008928284,
431.28022196888924,
2696.6283068284392,
2681.7719365358353,
1643.7775838784873,
2747.574352309108,
1485.8382858000696,
497.0831564962864
]
]
By the way, can I calculate the measure（Number of pixels occupied by the result） by this result?"
Bug in MultiScaleDeformableAttention?,open-mmlab/mmdetection,2022-05-21 01:11:53,1,,8023,1243817888,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
In DefromableDETRDecoder, reference points of each query are calculated through sigmoid function, which means the range of them is [0,1]. However, in class MultiScaleDeformableAttention, the offsets are divided by size of each feature maps and the range of them is uncontrollable. The final range of reference points is not in [0,1], I do think it's weird and will be harmful for training.
"
"After adding albu, an error occurs: AttributeError: 'list' object has no attribute 'rescale'",open-mmlab/mmdetection,2022-05-19 13:04:24,3,,8020,1241743153,"I choosed albumentations to do data aug when I use QuestInst to tackle instance segmentation task , here is my corresponding config setting:

```python
    dict(
        type='Albu',
        transforms=[
            dict(
                type='RandomBrightnessContrast',
                brightness_limit=[-0.6, 0.6],
                contrast_limit=[-0.4, 0.4],
                p=0.7),
            dict(
                type='HueSaturationValue',
                hue_shift_limit=20,
                sat_shift_limit=30,
                val_shift_limit=20,
                p=0.2
            ),
            dict(
                type='GaussianBlur',
                blur_limit=(3, 7),
                sigma_limit=0,
                p=0.7
            )
        ],
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=[""gt_labels""],
        )),
```

I think I got everything right, but an error occurs:
```bash
Original Traceback (most recent call last):
  File ""/home/chiebot-cv/anaconda3/envs/mmdet/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File ""/home/chiebot-cv/anaconda3/envs/mmdet/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/chiebot-cv/anaconda3/envs/mmdet/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/custom.py"", line 218, in __getitem__
    data = self.prepare_train_img(idx)
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/custom.py"", line 241, in prepare_train_img
    return self.pipeline(results)
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/pipelines/compose.py"", line 41, in __call__
    data = t(data)
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/pipelines/auto_augment.py"", line 106, in __call__
    return transform(results)
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/pipelines/compose.py"", line 41, in __call__
    data = t(data)
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/pipelines/transforms.py"", line 313, in __call__
    self._resize_masks(results)
  File ""/home/chiebot-cv/project/mmdet222/Orignal_mmdet/mmdet/datasets/pipelines/transforms.py"", line 259, in _resize_masks
    results[key] = results[key].rescale(results['scale'])
AttributeError: 'list' object has no attribute 'rescale'
```

I think this is because in:
```python
    def _resize_masks(self, results):
        """"""Resize masks with ``results['scale']``""""""
        for key in results.get('mask_fields', []):
            if results[key] is None:
                continue
            if self.keep_ratio:
                results[key] = results[key].rescale(results['scale'])
            else:
                results[key] = results[key].resize(results['img_shape'][:2])

``` 
`results['gt_masks']` is a list with an ndarray in it. So it can't call rescale()

What should I do?

btw: If I don't use albu, there would be no error.  "
Problem installing and running mmcv-full on nvidia cuda docker,open-mmlab/mmdetection,2022-05-19 08:21:38,4,installation/env,8018,1241413009,"Hi, all,
I am trying to install and run mmdetection as a component in my code, and as such- trying to install it in my Dockerfile.
I am using the nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04 image as base, with pytorch 1.6.0.
It seems to install mmcv correctly during the docker build stage, but when trying to run my code, I recieve the following error:
""No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'""
and my code exits.
When I tried to specify --gpus or --runtime in my docker run command, I recieved this error: 
""RuntimeError: nms_impl: implementation for device cuda:0 not found""

This is the relevant  section from my Dockerfile:

FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu18.04

RUN apt-get update
RUN apt-get install -y \
    build-essential \
    cmake \
    git \
    libgoogle-glog-dev  \
    libgflags-dev\
    libatlas-base-dev \
    libeigen3-dev \
    libopencv-dev \
    libssl-dev \
    openssl \
    python3.6 \
    python3-opencv \
    python3-pip \
    python3-pyproj \
    curl \
    libsuitesparse-dev \
    gdal-bin \
    libgdal-dev \
    wget \
    unzip \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

RUN pip3 install torch==1.6.0 torchvision==0.7.0
RUN pip3 install mmcv-full==1.5.0 -f https://download.openmmlab.com/mmcv/dist/cu100/torch1.6.0/index.html \
        -e git+https://github.com/open-mmlab/mmdetection.git@...#egg=mmdet"
add CBAM attention mechanism block within Resnet backbone ,open-mmlab/mmdetection,2022-05-18 14:30:02,2,,8013,1240057146,"Hello , I would like to thank you all for such a great work. 
I am using mmdetection now since a round 1 year. and i like this environment and platform. 

I would like to as about adding the general attention mechanism block (CBAM ) within my algorithm or backbone to improve the detection.  

I used the spatial attention mechanism represented by empirical_attention within my config , and i  use the instruction `plugins`
 
I am still wounding how can I add the CBAM in my model, can you please help me in that? 
i already have some resnet_CBAM model ready but in tensorflow. keras , what shall i do to add such block easily within mmdetection either backbone or neck structure. 

looking forward to your answer. 

"
"bbox_targets of samples with label=0 is [0. ,0. ,0. , 0.]",open-mmlab/mmdetection,2022-05-17 09:02:06,4,,8000,1238315851,"Hi, I'm trying to add centerness into FSAF module and meet a problem, which is very confusing.
Why there are so many samples with label=0 have [0. ,0. ,0. , 0.] as their bbox_targets.Postive samples' label range is [0,19] in PASCOVOC right? If you know please teach me, thank you very much!


Here is my code and log:
I add  codes below in /models/dense_heads/anchor_head.py's loss_single function:      
 for i ,j in zip(labels,bbox_targets):
            if i ==0.:
                print ('oritgiom label',i ,'targetsss',j)
and got:

origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')
origion label tensor(0, device='cuda:0') targetsss tensor([0., 0., 0., 0.], device='cuda:0')


samples with label=0 means areoplane right? why there bbox targets are zero? please help me !thank you so much !!"
Bboxes export as json from video / frames / images. ,open-mmlab/mmdetection,2022-05-16 13:31:35,2,,7998,1237167616,"Dear Friends. 

I have seen this tool very user friendly. 
I want to ask that is it possible to export the bbox in json with 4 points for each detected object? and the colors of the detected object?

If you can guide me. I have seen the results from inference demo routine and it has 5d array and I cannot understand what it represents. 

your kind help is highly appreciated. "
[Question] Error when converting yolact to ONNX,open-mmlab/mmdetection,2022-05-16 09:24:14,2,,7995,1236870481,"Hello. May I ask if converting yolact to ONNX/TRT is currently available? I tried to convert using tools/deployment/pytorch2onnx.py but it seems to be errors. 
Please help me, thanks a lot in advance.
```
python tools/deployment/pytorch2onnx.py configs/yolact/yolact_r50_1x8_coco.py /home/gggggg/projects/yolact_weights/r50_1x8_coco/2104.pth --output-file test.onnx --input-img /home/gggggg/projects/test.jpg
mmdetection/tools/deployment/pytorch2onnx.py:299: UserWarning: Arguments like `--mean`, `--std`, `--dataset` would be         parsed directly from config file and are deprecated and         will be removed in future releases.
  warnings.warn('Arguments like `--mean`, `--std`, `--dataset` would be \
/home/gggggg/miniconda3/lib/python3.9/site-packages/mmcv/onnx/symbolic.py:481: UserWarning: DeprecationWarning: This function will be deprecated in future. Welcome to use the unified model deployment toolbox MMDeploy: https://github.com/open-mmlab/mmdeploy
  warnings.warn(msg)
load checkpoint from local path: /home/gggggg/projects/yolact_weights/r50_1x8_coco/2104.pth
[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.
/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py:415: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
  nms_pre_tensor = torch.tensor(
/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py:436: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert cls_score.size()[-2:] == bbox_pred.size()[-2:]
/home/gggggg/mmdetection/./mmdet/core/export/onnx_helper.py:63: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if k <= 0 or size <= 0:
/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py:457: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if nms_pre > 0:
Traceback (most recent call last):
  File ""/home/gggggg/mmdetection/tools/deployment/pytorch2onnx.py"", line 335, in <module>
    pytorch2onnx(
  File ""/home/gggggg/mmdetection/tools/deployment/pytorch2onnx.py"", line 91, in pytorch2onnx
    torch.onnx.export(
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/__init__.py"", line 305, in export
    return utils.export(model, args, f, export_params, verbose, training,
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 118, in export
    _export(model, args, f, export_params, verbose, training, input_names, output_names,
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 719, in _export
    _model_to_graph(model, args, verbose, input_names,
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 499, in _model_to_graph
    graph, params, torch_out, module = _create_jit_graph(model, args)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 440, in _create_jit_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/onnx/utils.py"", line 391, in _trace_and_get_graph_from_model
    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py"", line 1166, in _get_trace_graph
    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py"", line 127, in forward
    graph, out = torch._C._create_graph_by_tracing(
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/jit/_trace.py"", line 118, in wrapper
    outs.append(self.inner(*trace_inputs))
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1098, in _slow_forward
    result = self.forward(*input, **kwargs)
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 109, in new_func
    return old_func(*args, **kwargs)
  File ""/home/gggggg/mmdetection/./mmdet/models/detectors/base.py"", line 169, in forward
    return self.onnx_export(img[0], img_metas[0])
  File ""/home/gggggg/mmdetection/./mmdet/models/detectors/single_stage.py"", line 168, in onnx_export
    det_bboxes, det_labels = self.bbox_head.onnx_export(
  File ""/home/gggggg/miniconda3/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 197, in new_func
    return old_func(*args, **kwargs)
  File ""/home/gggggg/mmdetection/./mmdet/models/dense_heads/base_dense_head.py"", line 460, in onnx_export
    nms_pre_score = (nms_pre_score * score_factors[..., None])
RuntimeError: The size of tensor a (45600) must match the size of tensor b (1459200) at non-singleton dimension 1
```"
Multi-GPU training hangs,open-mmlab/mmdetection,2022-05-13 02:19:16,5,,7976,1234660894,"Command run:
`bash ./tools/dist_train.sh configs/carafe/mask_rcnn_r50_fpn_carafe_1x_coco.py 8`

No error, but hangs after output `2022-05-12 22:06:13,674 - mmcv - INFO - Reducer buckets have been rebuilt in this iteration.` and stays around ~90% utilization (continuing to fluctuate) until killed.

PyTorch version: 1.7.1
CUDA version: 11.6

Haven't been able to track down this exact problem in previous issues, but it seems like getting stuck while training in general is a known issue?"
mmfewshot question,open-mmlab/mmdetection,2022-05-12 13:00:53,1,,7975,1233950913,"i don't understand how to divide data sets now.....
i looked at the training method of 'attention RPN' on coco(attention-rpn_r50_c4_4xb2_coco_official-base-training.py and attention-rpn_r50_c4_4xb2_coco_official-10shot-fine-tuning.py),  and made statistics on the base set and few-shot set used by them. I found that the samples in the few-shot set have used a large number of samples for training in base training. I don't know what this is!
in base train
![image](https://user-images.githubusercontent.com/44466856/168076492-6b1694fa-dcb4-42ff-a5c0-041f166d2470.png)
in few-shot train
![image](https://user-images.githubusercontent.com/44466856/168076629-7696feb8-587f-42bc-a5b5-595263cbe63a.png)

can you help me? Hope to get your reply, thanks!"
how can use def areas from .../mask/structures.py ? ,open-mmlab/mmdetection,2022-05-12 07:32:00,2,,7969,1233566953,"I fount this issues(https://github.com/open-mmlab/mmdetection/issues/3418), so tried to calculate areas of mask. but always error had been occured.

my code is this

        `masks = np.where(result[1][0][0])

        maskss = masks[0:1]
        x_list=[]
        for i in maskss:
            x_list.append(maskss[0][i])
        
        masksss = masks[1:2]
        y_list=[]
        for i in maskss:
            y_list.append(masksss[0][i])

        areaaa = np.column_stack((x_list,y_list))

        area = PolygonMasks(areaaa, len(areaaa), 2).areas`

How can i get width, height of mask? It's very hard to gain the values... 

Can anyone helpme? Thank you!


"
I change the code but it is not changed. why this situations is occur?? ,open-mmlab/mmdetection,2022-05-10 13:54:40,3,community discussion,7957,1231207139,"I put the print code in code. 

and I change font_size code also but never changed.

What do I do??? What I have to change???? 

It never changed I don't know how to do it is so stressful 

please help me

Thank you:)

Under is my code 

from mmdet.apis import init_detector, inference_detector, show_result_pyplot
import mmcv

config_file = '/home/sangjoon/realmmdetection/mmdetection/configs/swin/cascade_rcnn_swin_base_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_coco.py'
checkpoint_file = '/media/sensor-lab/sensor-lab/sangjoon/20220504_mmdet_swint/white2/best_bbox_mAP_epoch_160.pth'

model = init_detector(config_file, checkpoint_file, device='cuda:0')


img = '/home/sangjoon/realmmdetection/mmdetection/test_result_0504/aedes53_white.jpg'  # or img = mmcv.imread(img), which will only load it once
result = inference_detector(model, img)
show_result_pyplot(model, img, result,score_thr=0.5, out_file='result9.jpg')"
mmdet_handler.py  and fuse_conv_bn,open-mmlab/mmdetection,2022-05-09 21:11:29,1,,7949,1230256028,It seems that the `mmdet_handler.py` script used with TorchServe export does not include the use of `mmcv.cnn.fuse_conv_bn`. Is there any reason not to add this and also wrap the model in it? I just want to make sure I'm not missing a reason why it might have been intentionally left out.
"About batch, iter and augmentation",open-mmlab/mmdetection,2022-05-09 15:35:07,4,,7948,1229908589,"When I check my training log I found that SSD300 and Faster RCNN used the same dataset but got different iter, I want to know what determines iter ? 
And I notice that when I use augmentaion in training, iter still the same as before. How can I konw the augmentation is worked?

Sorry for asking such a rudimentary question, looking forward to your reply"
How to change the activation function?,open-mmlab/mmdetection,2022-05-09 14:56:41,2,,7947,1229857206,"For example, I want to change the activation function Swish of EfficientNet to Mish, but the official does not seem to provide the Mish activation function?Should I write Mish function and register it? Or modify the code of the Swish function to the Mish function? But I can't seem to find where the code for the Swish function is, please help  me, thanks a lot!"
solo model output result is empty,open-mmlab/mmdetection,2022-05-09 10:58:22,5,,7946,1229532276,"Hello~
我在训练solo模型后测试效果，返回结果为空。
利用`python tools/train.py configs/solo/solo_r50_fpn_1x_coco.py`训练模型，其中训练数据为coco-val集
走完12轮测试效果，测试脚本demo.py代码如下：
`from argparse import ArgumentParser
import cv2

from mmdet.apis import (async_inference_detector, inference_detector,
                        init_detector)

def parse_args():
    parser = ArgumentParser()
    parser.add_argument('img', help='Image file')
    parser.add_argument('out', help='Image file')
    parser.add_argument('config', help='Config file')
    parser.add_argument('checkpoint', help='Checkpoint file')
    parser.add_argument(
        '--device', default='cuda:0', help='Device used for inference')
    parser.add_argument(
        '--score-thr', type=float, default=0.65, help='bbox score threshold')
    args = parser.parse_args()
    return args

def show_result_pyplot_own(model,
                       img,
                       result,
                       out_file, 
                       score_thr):
    if hasattr(model, 'module'):
        model = model.module
    res_img = model.show_result(
        img,
        result,
        score_thr=score_thr,
        show=False
        )
    cv2.imwrite(out_file, res_img)
    
def main(args):
    # build the model from a config file and a checkpoint file
    model = init_detector(args.config, args.checkpoint, device=args.device)
    # test a single image
    result = inference_detector(model, args.img)
    # show the results
    # print(""res:"",result)
    show_result_pyplot_own(model, args.img, result, args.out, score_thr=args.score_thr)

if __name__ == '__main__':
    args = parse_args()
    main(args)
`
执行
`python demo.py 000000007173.jpg 000000007173-res.jpg configs/solo/solo_r50_fpn_1x_coco.py work_dirs/solo_r50_fpn_1x_coco/epoch_11.pth`
得到的图片效果为空，和原图一样，如下所示：
![2345截图20220509183901](https://user-images.githubusercontent.com/25747262/167393972-62350959-3de7-459d-9759-09fe9d955a20.png)
训练日志如下：
[20220509_115210.log](https://github.com/open-mmlab/mmdetection/files/8651000/20220509_115210.log)
请问是什么原因呢？是因为训练数据量比较小导致没有收敛，迭代次数不够或者是学习率参数设置的问题？
"
Are there any tutorials to ensemble models (specially for instance segmentation)?,open-mmlab/mmdetection,2022-05-09 09:07:01,2,,7944,1229406729,
Inference on Video and saving results as video(mp4),open-mmlab/mmdetection,2022-05-09 07:25:36,2,,7942,1229296661,"Is there a way to save the results from inference on a video? 

I would like to have the prediction results as a .mp4 for easy visualization. 

I have tried using the implementation shown below: 

![image](https://user-images.githubusercontent.com/64862726/167360648-47d2022b-3f87-4d9d-a97a-5a9b6049fc16.png)
"
"what is the exact meaning of ""level"" and ""prob"" in transforms?",open-mmlab/mmdetection,2022-05-09 07:13:52,1,,7941,1229284225,"I have a questions about title.

In transforms, there are ""level"" and ""prob"".

but it is just said ""shoud be in range [0,max_level], probability for performing color transformation.

So I don't know what is means exactly. please help me :) "
"How to modify coco eval to set my own APs,APm,APl",open-mmlab/mmdetection,2022-05-09 07:08:59,1,,7940,1229279473,"I'm working on remote sensing imagery,so about small/medium/large area's setting maybe is different from coco datasets. I wanna know where or how to modify coco metric paras(like small area is 0~32, medium,:32~96...) to fit my own datasets."
checkpoint file of YOLOX-m model trained on COCO dataset,open-mmlab/mmdetection,2022-05-08 08:56:54,1,planned feature,7934,1228832009,"Many thanks for having provided ""Results and Models"" of YOLOX-s, YOLOX-l, YOLOX-x on [this page](https://github.com/open-mmlab/mmdetection/blob/master/configs/yolox/README.md), is there a link to download .pth file coresponding to the config ""yolox_m_8x8_300e_coco.py""? "
新的模型库,open-mmlab/mmdetection,2022-05-07 11:22:10,2,community help wanted#feature request,7930,1228603359,请问mmdetection是否会添加PyramidTNT模型库?
How to upload media at wandb,open-mmlab/mmdetection,2022-05-06 13:28:36,2,,7919,1227872246,"I want to upload media(image,bounding box) to wandb.
Is there a way to upload media?"
[Feature Request] PyTorch Profiler support,open-mmlab/mmdetection,2022-05-06 13:06:00,2,feature request,7918,1227844402,"**Describe the feature**

Built-in support for PyTorch Profiler with TensorBoard:

![image](https://user-images.githubusercontent.com/12224358/167136929-cf7fc79c-46e3-4985-b78a-41ecea4880bc.png)

**Motivation**
This profiler seems like the best way currently to check for performance issues. It would be great if it was built-in via a cfg toggle with sensible default settings.
"
如何利用dota数据集的测试集？,open-mmlab/mmdetection,2022-05-02 15:48:04,1,,7899,1223046650,博主，您好，DOTA数据集中的测试集里面只有图片，没有标注文件.txt，这样的test能用于mmdetection的test.py测试吗？我看见论文里面好多都有dota数据集测试的结果，他们用的是测试集吗？还是验证集？为什么dota数据集的测试集没有标注文件官网还提供了生成测试集的json文件？生产的测试集的json文件真的有用吗？
能否实现不同尺度下推理与标注差异的可视化,open-mmlab/mmdetection,2022-05-02 12:45:02,2,,7898,1222843137,Can we achieve visualization of inference and labeling differences at different scales?
Point Sampling for Point Rend,open-mmlab/mmdetection,2022-05-02 11:51:30,1,,7896,1222793113,"Hello,

I want to sample uncertain points with a bias as mentioned in the paper of pointrend. Sharing relevant section from the paper as below:

![bias](https://user-images.githubusercontent.com/85254524/166228920-748bfee4-4bab-4d00-a6dd-653864177e84.png)

I have been looking for parameters to implement mildly biased point sampling but couldn't find which parameters I should change. Could you please help me to identify relevant paramters to increase bias instead of implementing a regular grid?

Thanks
"
conn't use dist_test.sh,open-mmlab/mmdetection,2022-05-02 08:25:39,2,,7895,1222621723,"I get SyntaxError: invalid syntax when I use the dist_test.sh command:
(swin_det) amax@admin:~/LJW/swin_transformer/Swin-Transformer-Object-Detection$ python ./tools/dist_test.sh configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_3x_coco.py ./work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_3x_coco/latest.pth 2 --format-only --options “jsonfile_prefix=./results”
  File ""./tools/dist_test.sh"", line 3
    CONFIG=$1
           ^
SyntaxError: invalid syntax
"
"How to add the evaluation index of map_s，map_m,map_l to voc dataset?",open-mmlab/mmdetection,2022-05-02 07:29:53,1,,7893,1222576543,"How to add the evaluation index of map_s，map_m,map_l to voc dataset?"
mask rcnn swin b,open-mmlab/mmdetection,2022-05-02 06:59:46,1,,7892,1222551190,mask rcnn现在的backbone只有swin-t和swin-s，还没有swin-b的，会打算release么
Add the crop-like setting to the test or val dateset pipeline,open-mmlab/mmdetection,2022-04-29 09:46:31,2,,7875,1220379293,"Add the crop-like setting to the test or val datasets pipeline

**Motivation**
Due to some reason, the images I used to build datasets involve interference on the broad edge of the image. I can crop the parts I wanted, but I don't want to save the parts image because of information loss,  otherwise training or test them directly.
But, I found when add crop operation to test_pipeline, the AP calculated to 0.0 always. And https://github.com/open-mmlab/mmdetection/issues/5340 issued same problem.

So, will you take care of it?"
why doesn't the yolox configs have Normalize？,open-mmlab/mmdetection,2022-04-29 08:19:59,3,,7873,1220200697,i cant't find the Normalize in yolox_s_8x8_300e_coco.py? does yolox need it?
analyze_logs.py: IndexError: list index out of range,open-mmlab/mmdetection,2022-04-28 19:40:39,1,,7870,1219192300,"```
plot curve of work_dirs/yolox_s_8x8_300e_coco/20220427_152722.log.json, metric is loss_cls
Traceback (most recent call last):
  File ""D:\yoloxtest\tools\analysis_tools\analyze_logs.py"", line 203, in <module>
    main()
  File ""D:\yoloxtest\tools\analysis_tools\analyze_logs.py"", line 199, in main
    eval(args.task)(log_dicts, args)
  File ""D:\yoloxtest\tools\analysis_tools\analyze_logs.py"", line 57, in plot_curve
    if metric not in log_dict[epochs[int(args.start_epoch) - 1]]:
IndexError: list index out of range
```

The json:
```
{""env_info"": ""sys.platform: linux\nPython: 3.7.10 (default, Jun  4 2021, 14:48:32) [GCC 7.5.0]\nCUDA available: True\nGPU 0: NVIDIA GeForce RTX 3080 Ti\nCUDA_HOME: /usr/local/cuda\nNVCC: Cuda compilation tools, release 11.1, V11.1.105\nGCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\nPyTorch: 1.9.0+cu111\nPyTorch compiling details: PyTorch built with:\n  - GCC 7.3\n  - C++ Version: 201402\n  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 11.1\n  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n  - CuDNN 8.0.5\n  - Magma 2.5.2\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n\nTorchVision: 0.10.0+cu111\nOpenCV: 4.5.2\nMMCV: 1.5.0\nMMCV Compiler: GCC 7.3\nMMCV CUDA Compiler: 11.1\nMMDetection: 2.24.0+1376e77"", ""config"": ""optimizer = dict(\n    type='SGD',\n    lr=0.00125,\n    momentum=0.1125,\n    weight_decay=0.0005,\n    nesterov=True,\n    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))\noptimizer_config = dict(grad_clip=None)\nlr_config = dict(\n    policy='YOLOX',\n    warmup='exp',\n    by_epoch=False,\n    warmup_by_epoch=True,\n    warmup_ratio=1,\n    warmup_iters=5,\n    num_last_epochs=15,\n    min_lr_ratio=0.05)\nrunner = dict(type='EpochBasedRunner', max_epochs=20)\ncheckpoint_config = dict(interval=2)\nlog_config = dict(interval=2, hooks=[dict(type='TextLoggerHook')])\ncustom_hooks = [\n    dict(type='YOLOXModeSwitchHook', num_last_epochs=15, priority=48),\n    dict(type='SyncNormHook', num_last_epochs=15, interval=2, priority=48),\n    dict(\n        type='ExpMomentumEMAHook',\n        resume_from=None,\n        momentum=0.0001,\n        priority=49)\n]\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nload_from = 'https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_s_8x8_300e_coco/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\nresume_from = None\nworkflow = [('train', 1)]\nopencv_num_threads = 0\nmp_start_method = 'fork'\nauto_scale_lr = dict(enable=False, base_batch_size=16)\nimg_scale = (640, 640)\nmy_num_classes = 29\nmodel = dict(\n    type='YOLOX',\n    input_size=(640, 640),\n    random_size_range=(15, 25),\n    random_size_interval=10,\n    backbone=dict(type='CSPDarknet', deepen_factor=0.33, widen_factor=0.5),\n    neck=dict(\n        type='YOLOXPAFPN',\n        in_channels=[128, 256, 512],\n        out_channels=128,\n        num_csp_blocks=1),\n    bbox_head=dict(\n        type='YOLOXHead', num_classes=29, in_channels=128, feat_channels=128),\n    train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),\n    test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))\ndata_root = 'data/coco/'\ndataset_type = 'CocoDataset'\ntrain_pipeline = [\n    dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),\n    dict(\n        type='RandomAffine', scaling_ratio_range=(0.1, 2),\n        border=(-320, -320)),\n    dict(\n        type='MixUp',\n        img_scale=(640, 640),\n        ratio_range=(0.8, 1.6),\n        pad_val=114.0),\n    dict(type='YOLOXHSVRandomAug'),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Resize', img_scale=(640, 640), keep_ratio=True),\n    dict(\n        type='Pad',\n        pad_to_square=True,\n        pad_val=dict(img=(114.0, 114.0, 114.0))),\n    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n]\ntrain_dataset = dict(\n    type='MultiImageMixDataset',\n    dataset=dict(\n        type='CocoDataset',\n        ann_file='data/coco/annotations/instances_train2017.json',\n        img_prefix='data/coco/train2017/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(type='LoadAnnotations', with_bbox=True)\n        ],\n        filter_empty_gt=False),\n    pipeline=[\n        dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),\n        dict(\n            type='RandomAffine',\n            scaling_ratio_range=(0.1, 2),\n            border=(-320, -320)),\n        dict(\n            type='MixUp',\n            img_scale=(640, 640),\n            ratio_range=(0.8, 1.6),\n            pad_val=114.0),\n        dict(type='YOLOXHSVRandomAug'),\n        dict(type='RandomFlip', flip_ratio=0.5),\n        dict(type='Resize', img_scale=(640, 640), keep_ratio=True),\n        dict(\n            type='Pad',\n            pad_to_square=True,\n            pad_val=dict(img=(114.0, 114.0, 114.0))),\n        dict(\n            type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),\n        dict(type='DefaultFormatBundle'),\n        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n    ])\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug',\n        img_scale=(640, 640),\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'),\n            dict(\n                type='Pad',\n                pad_to_square=True,\n                pad_val=dict(img=(114.0, 114.0, 114.0))),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img'])\n        ])\n]\ndata = dict(\n    samples_per_gpu=8,\n    workers_per_gpu=4,\n    persistent_workers=True,\n    train=dict(\n        type='MultiImageMixDataset',\n        dataset=dict(\n            type='CocoDataset',\n            ann_file='data/coco/annotations/instances_train2017.json',\n            img_prefix='data/coco/train2017/',\n            pipeline=[\n                dict(type='LoadImageFromFile'),\n                dict(type='LoadAnnotations', with_bbox=True)\n            ],\n            filter_empty_gt=False),\n        pipeline=[\n            dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),\n            dict(\n                type='RandomAffine',\n                scaling_ratio_range=(0.1, 2),\n                border=(-320, -320)),\n            dict(\n                type='MixUp',\n                img_scale=(640, 640),\n                ratio_range=(0.8, 1.6),\n                pad_val=114.0),\n            dict(type='YOLOXHSVRandomAug'),\n            dict(type='RandomFlip', flip_ratio=0.5),\n            dict(type='Resize', img_scale=(640, 640), keep_ratio=True),\n            dict(\n                type='Pad',\n                pad_to_square=True,\n                pad_val=dict(img=(114.0, 114.0, 114.0))),\n            dict(\n                type='FilterAnnotations',\n                min_gt_bbox_wh=(1, 1),\n                keep_empty=False),\n            dict(type='DefaultFormatBundle'),\n            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n        ]),\n    val=dict(\n        type='CocoDataset',\n        ann_file='data/coco/annotations/instances_val2017.json',\n        img_prefix='data/coco/val2017/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(640, 640),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Pad',\n                        pad_to_square=True,\n                        pad_val=dict(img=(114.0, 114.0, 114.0))),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]),\n    test=dict(\n        type='CocoDataset',\n        ann_file='data/coco/annotations/instances_val2017.json',\n        img_prefix='data/coco/val2017/',\n        pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(640, 640),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip'),\n                    dict(\n                        type='Pad',\n                        pad_to_square=True,\n                        pad_val=dict(img=(114.0, 114.0, 114.0))),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]))\nmax_epochs = 20\nnum_last_epochs = 15\ninterval = 2\nevaluation = dict(\n    save_best='auto', interval=2, dynamic_intervals=[(5, 1)], metric='bbox')\nwork_dir = './work_dirs/yolox_s_8x8_300e_coco'\nauto_resume = False\ngpu_ids = [0]\n"", ""seed"": 863651103, ""exp_name"": ""yolox_s_8x8_300e_coco.py"", ""hook_msgs"": {}}

```
"
Nonlocal block question?,open-mmlab/mmdetection,2022-04-28 09:11:53,1,Doc#How-to,7860,1218434702,"In this part, it show me that the resnet can use the nonlocal_block, could you please tell me how to use it? Could you please give me a demo?
https://github.com/open-mmlab/mmdetection/blob/1376e77e6ecbaad609f6003725158de24ed42e84/mmdet/models/backbones/resnet.py
![image](https://user-images.githubusercontent.com/86214036/165719075-d88cb5f5-57ca-4171-b161-ad9ec356549e.png)
"
Support CSwin,open-mmlab/mmdetection,2022-04-28 08:25:27,1,feature request,7858,1218379273,"**Motivation**
Support CSwin in the models

**Related resources**
https://github.com/microsoft/CSWin-Transformer
"
Solo question?,open-mmlab/mmdetection,2022-04-26 07:09:27,1,enhancement#feature request,7831,1215502829,"![solo](https://user-images.githubusercontent.com/86214036/165241937-68c1a190-0110-437d-b149-3f39fe22a885.jpg)
When I run the script: python tools/analysis_tools/get_flops.py, but it occur the error ""forward_dummy"" is not implemented in SOLO? 
How to solve it? "
"Any plans to add MobileNetV3, ShuffleNetV2？",open-mmlab/mmdetection,2022-04-25 16:07:57,1,,7828,1214736027,"**Motivation**
Perform light-weight backbones for real-time object detection
"
error during training my custom dataset on centriapetalNet/tensors must be long,open-mmlab/mmdetection,2022-04-25 15:15:17,2,,7827,1214666213,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help. yes 
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help. yes
3. The bug has not been fixed in the latest version. yes

**Describe the bug**
A clear and concise description of what the bug is.
 I got such an error massage 


![error in centriapetal training model](https://user-images.githubusercontent.com/10245810/165118822-e90515b1-f049-483b-bac0-2dcf4548cdee.jpg)


**Reproduction**

1. What command or script did you run?
 I run the following command in a cluster. 
```none
python tools/train.py configs/const/centripetalnet_hourglass104_mstest_16x6_210e_coco.py --work-dir Con_train_results/centripetalnet

```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
 yes , as i used to do , changing the config according to my new dataset and its classes , adding fp16 =dynamic 
4. What dataset did you use?
my own custom dataset in COCO format

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
here is my environment , i upgrade the mmdet to be 2.23 

![new config to train centriapetalnet](https://user-images.githubusercontent.com/10245810/165118243-faf20b48-47ab-4183-abe1-77efa379123c.jpg)


3. You may add addition that may be helpful for locating the problem, such as
    - How you installed PyTorch [e.g., pip, conda, source]
    pip using sh file  
    - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**
If applicable, paste the error trackback here.
labled above in yellow
```none
index error : tensors used as indices must be long, byte or bool tensors
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
difference confidence_threshold and score_threshold,open-mmlab/mmdetection,2022-04-25 13:54:08,2,,7826,1214550773,"Hi i don't know the difference confidence_threshold and score_threshold.
In Yolo v3 you also use confidence_threshold for nms.
I tried to read to source code post_processing/bbox_nms, but there is no confidence_threshold anywhere.
And i am also wondering how you use soft nms, because in post_processing/bbox_nms only greedy nms is supported, maybe you can refer the right file.

Thanks a lot!"
AssertionError:MixuporMosaic,open-mmlab/mmdetection,2022-04-25 12:28:03,7,,7825,1214434545,"I set up the data enhancement method of Mixup and Mosaic according to the official documentation and yolox.config, but it failed, and the following error occurred.
Traceback (most recent call last):
  File ""/home/cdzk/anaconda3/envs/fu-mmlab/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 52, in build_from_cfg
    return obj_cls(**args)
  File ""/home/cdzk/PYC/code/fewmmdetection/mmdet/datasets/pipelines/transforms.py"", line 2006, in __init__
    assert isinstance(img_scale, tuple)
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tools/train.py"", line 221, in <module>
    main()
  File ""tools/train.py"", line 197, in main
    datasets = [build_dataset(cfg.data.train)]
  File ""/home/cdzk/PYC/code/fewmmdetection/mmdet/datasets/builder.py"", line 78, in build_dataset
    dataset = MultiImageMixDataset(**cp_cfg)
  File ""/home/cdzk/PYC/code/fewmmdetection/mmdet/datasets/dataset_wrappers.py"", line 380, in __init__
    transform = build_from_cfg(transform, PIPELINES)
  File ""/home/cdzk/anaconda3/envs/fu-mmlab/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 55, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
AssertionError: Mosaic: 
"
Labelling Training Set - Bounding Boxes,open-mmlab/mmdetection,2022-04-25 05:33:09,2,,7823,1213994301,"Hi,

I have a question about training set labelling. Because I'm trying to detect an objects mask, I have been labelling my images as below:

![Screenshot from 2022-04-25 08-29-55](https://user-images.githubusercontent.com/85254524/165026256-fdf98d5e-cf88-45aa-8fe3-d321f5946bd2.png)

I only selected mask boundaries for mask detection and didn't select any bounding boxes. Do I need to select bounding boxes as well for more proper results? Moreover when I don't select bounding boxes, how the algorithm compares its results with the test set since there is no bounding box in the set?


"
Error when pretrained on Cascade mask RCNN SwinTransformer,open-mmlab/mmdetection,2022-04-25 04:12:32,1,,7822,1213943339,"I use the pretrained weight from ""https://github.com/SwinTransformer/storage/releases/download/v1.0.2/cascade_mask_rcnn_swin_base_patch4_window7.pth"". When I try to pretrain on it (I set 'convert_weights=True'), following error appear:
```python
unexpected key in source state_dict: backbone.layers.0.blocks.0.norm1.weight, backbone.layers.0.blocks.0.norm1.bias, backbone.layers.0.blocks.0.attn.relative_position_bias_table, backbone.layers.0.blocks.0.attn.relative_position_index, backbone.layers.0.blocks.0.attn.qkv.weight, backbone.layers.0.blocks.0.attn.qkv.bias, backbone.layers.0.blocks.0.attn.proj.weight, backbone.layers.0.blocks.0.attn.proj.bias, backbone.layers.0.blocks.0.norm2.weight, backbone.layers.0.blocks.0.norm2.bias, backbone.layers.0.blocks.0.mlp.fc1.weight, backbone.layers.0.blocks.0.mlp.fc1.bias, backbone.layers.0.blocks.0.mlp.fc2.weight, backbone.layers.0.blocks.0.mlp.fc2.bias, backbone.layers.0.blocks.1.norm1.weight, backbone.layers.0.blocks.1.norm1.bias, backbone.layers.0.blocks.1.attn.relative_position_bias_table, backbone.layers.0.blocks.1.attn.relative_position_index, backbone.layers.0.blocks.1.attn.qkv.weight, backbone.layers.0.blocks.1.attn.qkv.bias, backbone.layers.0.blocks.1.attn.proj.weight, backbone.layers.0.blocks.1.attn.proj.bias, backbone.layers.0.blocks.1.norm2.weight, backbone.layers.0.blocks.1.norm2.bias, backbone.layers.0.blocks.1.mlp.fc1.weight, backbone.layers.0.blocks.1.mlp.fc1.bias, backbone.layers.0.blocks.1.mlp.fc2.weight, backbone.layers.0.blocks.1.mlp.fc2.bias, backbone.layers.0.downsample.reduction.weight, backbone.layers.0.downsample.norm.weight, backbone.layers.0.downsample.norm.bias, backbone.layers.1.blocks.0.norm1.weight, backbone.layers.1.blocks.0.norm1.bias, backbone.layers.1.blocks.0.attn.relative_position_bias_table, backbone.layers.1.blocks.0.attn.relative_position_index, backbone.layers.1.blocks.0.attn.qkv.weight, backbone.layers.1.blocks.0.attn.qkv.bias, backbone.layers.1.blocks.0.attn.proj.weight, backbone.layers.1.blocks.0.attn.proj.bias, backbone.layers.1.blocks.0.norm2.weight, backbone.layers.1.blocks.0.norm2.bias, backbone.layers.1.blocks.0.mlp.fc1.weight, backbone.layers.1.blocks.0.mlp.fc1.bias, backbone.layers.1.blocks.0.mlp.fc2.weight, backbone.layers.1.blocks.0.mlp.fc2.bias, backbone.layers.1.blocks.1.norm1.weight, backbone.layers.1.blocks.1.norm1.bias, backbone.layers.1.blocks.1.attn.relative_position_bias_table, backbone.layers.1.blocks.1.attn.relative_position_index, backbone.layers.1.blocks.1.attn.qkv.weight, backbone.layers.1.blocks.1.attn.qkv.bias, backbone.layers.1.blocks.1.attn.proj.weight, backbone.layers.1.blocks.1.attn.proj.bias, backbone.layers.1.blocks.1.norm2.weight, backbone.layers.1.blocks.1.norm2.bias, backbone.layers.1.blocks.1.mlp.fc1.weight, backbone.layers.1.blocks.1.mlp.fc1.bias, backbone.layers.1.blocks.1.mlp.fc2.weight, backbone.layers.1.blocks.1.mlp.fc2.bias, backbone.layers.1.downsample.reduction.weight, backbone.layers.1.downsample.norm.weight, backbone.layers.1.downsample.norm.bias, backbone.layers.2.blocks.0.norm1.weight, backbone.layers.2.blocks.0.norm1.bias, backbone.layers.2.blocks.0.attn.relative_position_bias_table, backbone.layers.2.blocks.0.attn.relative_position_index, backbone.layers.2.blocks.0.attn.qkv.weight, backbone.layers.2.blocks.0.attn.qkv.bias, backbone.layers.2.blocks.0.attn.proj.weight, backbone.layers.2.blocks.0.attn.proj.bias, backbone.layers.2.blocks.0.norm2.weight, backbone.layers.2.blocks.0.norm2.bias, backbone.layers.2.blocks.0.mlp.fc1.weight, backbone.layers.2.blocks.0.mlp.fc1.bias, backbone.layers.2.blocks.0.mlp.fc2.weight, backbone.layers.2.blocks.0.mlp.fc2.bias, backbone.layers.2.blocks.1.norm1.weight, backbone.layers.2.blocks.1.norm1.bias, backbone.layers.2.blocks.1.attn.relative_position_bias_table, backbone.layers.2.blocks.1.attn.relative_position_index, backbone.layers.2.blocks.1.attn.qkv.weight, backbone.layers.2.blocks.1.attn.qkv.bias, backbone.layers.2.blocks.1.attn.proj.weight, backbone.layers.2.blocks.1.attn.proj.bias, backbone.layers.2.blocks.1.norm2.weight, backbone.layers.2.blocks.1.norm2.bias, backbone.layers.2.blocks.1.mlp.fc1.weight, backbone.layers.2.blocks.1.mlp.fc1.bias, backbone.layers.2.blocks.1.mlp.fc2.weight, backbone.layers.2.blocks.1.mlp.fc2.bias, backbone.layers.2.blocks.2.norm1.weight, backbone.layers.2.blocks.2.norm1.bias, backbone.layers.2.blocks.2.attn.relative_position_bias_table, backbone.layers.2.blocks.2.attn.relative_position_index, backbone.layers.2.blocks.2.attn.qkv.weight, backbone.layers.2.blocks.2.attn.qkv.bias, backbone.layers.2.blocks.2.attn.proj.weight, backbone.layers.2.blocks.2.attn.proj.bias, backbone.layers.2.blocks.2.norm2.weight, backbone.layers.2.blocks.2.norm2.bias, backbone.layers.2.blocks.2.mlp.fc1.weight, backbone.layers.2.blocks.2.mlp.fc1.bias, backbone.layers.2.blocks.2.mlp.fc2.weight, backbone.layers.2.blocks.2.mlp.fc2.bias, backbone.layers.2.blocks.3.norm1.weight, backbone.layers.2.blocks.3.norm1.bias, backbone.layers.2.blocks.3.attn.relative_position_bias_table, backbone.layers.2.blocks.3.attn.relative_position_index, backbone.layers.2.blocks.3.attn.qkv.weight, backbone.layers.2.blocks.3.attn.qkv.bias, backbone.layers.2.blocks.3.attn.proj.weight, backbone.layers.2.blocks.3.attn.proj.bias, backbone.layers.2.blocks.3.norm2.weight, backbone.layers.2.blocks.3.norm2.bias, backbone.layers.2.blocks.3.mlp.fc1.weight, backbone.layers.2.blocks.3.mlp.fc1.bias, backbone.layers.2.blocks.3.mlp.fc2.weight, backbone.layers.2.blocks.3.mlp.fc2.bias, backbone.layers.2.blocks.4.norm1.weight, backbone.layers.2.blocks.4.norm1.bias, backbone.layers.2.blocks.4.attn.relative_position_bias_table, backbone.layers.2.blocks.4.attn.relative_position_index, backbone.layers.2.blocks.4.attn.qkv.weight, backbone.layers.2.blocks.4.attn.qkv.bias, backbone.layers.2.blocks.4.attn.proj.weight, backbone.layers.2.blocks.4.attn.proj.bias, backbone.layers.2.blocks.4.norm2.weight, backbone.layers.2.blocks.4.norm2.bias, backbone.layers.2.blocks.4.mlp.fc1.weight, backbone.layers.2.blocks.4.mlp.fc1.bias, backbone.layers.2.blocks.4.mlp.fc2.weight, backbone.layers.2.blocks.4.mlp.fc2.bias, backbone.layers.2.blocks.5.norm1.weight, backbone.layers.2.blocks.5.norm1.bias, backbone.layers.2.blocks.5.attn.relative_position_bias_table, backbone.layers.2.blocks.5.attn.relative_position_index, backbone.layers.2.blocks.5.attn.qkv.weight, backbone.layers.2.blocks.5.attn.qkv.bias, backbone.layers.2.blocks.5.attn.proj.weight, backbone.layers.2.blocks.5.attn.proj.bias, backbone.layers.2.blocks.5.norm2.weight, backbone.layers.2.blocks.5.norm2.bias, backbone.layers.2.blocks.5.mlp.fc1.weight, backbone.layers.2.blocks.5.mlp.fc1.bias, backbone.layers.2.blocks.5.mlp.fc2.weight, backbone.layers.2.blocks.5.mlp.fc2.bias, backbone.layers.2.blocks.6.norm1.weight, backbone.layers.2.blocks.6.norm1.bias, backbone.layers.2.blocks.6.attn.relative_position_bias_table, backbone.layers.2.blocks.6.attn.relative_position_index, backbone.layers.2.blocks.6.attn.qkv.weight, backbone.layers.2.blocks.6.attn.qkv.bias, backbone.layers.2.blocks.6.attn.proj.weight, backbone.layers.2.blocks.6.attn.proj.bias, backbone.layers.2.blocks.6.norm2.weight, backbone.layers.2.blocks.6.norm2.bias, backbone.layers.2.blocks.6.mlp.fc1.weight, backbone.layers.2.blocks.6.mlp.fc1.bias, backbone.layers.2.blocks.6.mlp.fc2.weight, backbone.layers.2.blocks.6.mlp.fc2.bias, backbone.layers.2.blocks.7.norm1.weight, backbone.layers.2.blocks.7.norm1.bias, backbone.layers.2.blocks.7.attn.relative_position_bias_table, backbone.layers.2.blocks.7.attn.relative_position_index, backbone.layers.2.blocks.7.attn.qkv.weight, backbone.layers.2.blocks.7.attn.qkv.bias, backbone.layers.2.blocks.7.attn.proj.weight, backbone.layers.2.blocks.7.attn.proj.bias, backbone.layers.2.blocks.7.norm2.weight, backbone.layers.2.blocks.7.norm2.bias, backbone.layers.2.blocks.7.mlp.fc1.weight, backbone.layers.2.blocks.7.mlp.fc1.bias, backbone.layers.2.blocks.7.mlp.fc2.weight, backbone.layers.2.blocks.7.mlp.fc2.bias, backbone.layers.2.blocks.8.norm1.weight, backbone.layers.2.blocks.8.norm1.bias, backbone.layers.2.blocks.8.attn.relative_position_bias_table, backbone.layers.2.blocks.8.attn.relative_position_index, backbone.layers.2.blocks.8.attn.qkv.weight, backbone.layers.2.blocks.8.attn.qkv.bias, backbone.layers.2.blocks.8.attn.proj.weight, backbone.layers.2.blocks.8.attn.proj.bias, backbone.layers.2.blocks.8.norm2.weight, backbone.layers.2.blocks.8.norm2.bias, backbone.layers.2.blocks.8.mlp.fc1.weight, backbone.layers.2.blocks.8.mlp.fc1.bias, backbone.layers.2.blocks.8.mlp.fc2.weight, backbone.layers.2.blocks.8.mlp.fc2.bias, backbone.layers.2.blocks.9.norm1.weight, backbone.layers.2.blocks.9.norm1.bias, backbone.layers.2.blocks.9.attn.relative_position_bias_table, backbone.layers.2.blocks.9.attn.relative_position_index, backbone.layers.2.blocks.9.attn.qkv.weight, backbone.layers.2.blocks.9.attn.qkv.bias, backbone.layers.2.blocks.9.attn.proj.weight, backbone.layers.2.blocks.9.attn.proj.bias, backbone.layers.2.blocks.9.norm2.weight, backbone.layers.2.blocks.9.norm2.bias, backbone.layers.2.blocks.9.mlp.fc1.weight, backbone.layers.2.blocks.9.mlp.fc1.bias, backbone.layers.2.blocks.9.mlp.fc2.weight, backbone.layers.2.blocks.9.mlp.fc2.bias, backbone.layers.2.blocks.10.norm1.weight, backbone.layers.2.blocks.10.norm1.bias, backbone.layers.2.blocks.10.attn.relative_position_bias_table, backbone.layers.2.blocks.10.attn.relative_position_index, backbone.layers.2.blocks.10.attn.qkv.weight, backbone.layers.2.blocks.10.attn.qkv.bias, backbone.layers.2.blocks.10.attn.proj.weight, backbone.layers.2.blocks.10.attn.proj.bias, backbone.layers.2.blocks.10.norm2.weight, backbone.layers.2.blocks.10.norm2.bias, backbone.layers.2.blocks.10.mlp.fc1.weight, backbone.layers.2.blocks.10.mlp.fc1.bias, backbone.layers.2.blocks.10.mlp.fc2.weight, backbone.layers.2.blocks.10.mlp.fc2.bias, backbone.layers.2.blocks.11.norm1.weight, backbone.layers.2.blocks.11.norm1.bias, backbone.layers.2.blocks.11.attn.relative_position_bias_table, backbone.layers.2.blocks.11.attn.relative_position_index, backbone.layers.2.blocks.11.attn.qkv.weight, backbone.layers.2.blocks.11.attn.qkv.bias, backbone.layers.2.blocks.11.attn.proj.weight, backbone.layers.2.blocks.11.attn.proj.bias, backbone.layers.2.blocks.11.norm2.weight, backbone.layers.2.blocks.11.norm2.bias, backbone.layers.2.blocks.11.mlp.fc1.weight, backbone.layers.2.blocks.11.mlp.fc1.bias, backbone.layers.2.blocks.11.mlp.fc2.weight, backbone.layers.2.blocks.11.mlp.fc2.bias, backbone.layers.2.blocks.12.norm1.weight, backbone.layers.2.blocks.12.norm1.bias, backbone.layers.2.blocks.12.attn.relative_position_bias_table, backbone.layers.2.blocks.12.attn.relative_position_index, backbone.layers.2.blocks.12.attn.qkv.weight, backbone.layers.2.blocks.12.attn.qkv.bias, backbone.layers.2.blocks.12.attn.proj.weight, backbone.layers.2.blocks.12.attn.proj.bias, backbone.layers.2.blocks.12.norm2.weight, backbone.layers.2.blocks.12.norm2.bias, backbone.layers.2.blocks.12.mlp.fc1.weight, backbone.layers.2.blocks.12.mlp.fc1.bias, backbone.layers.2.blocks.12.mlp.fc2.weight, backbone.layers.2.blocks.12.mlp.fc2.bias, backbone.layers.2.blocks.13.norm1.weight, backbone.layers.2.blocks.13.norm1.bias, backbone.layers.2.blocks.13.attn.relative_position_bias_table, backbone.layers.2.blocks.13.attn.relative_position_index, backbone.layers.2.blocks.13.attn.qkv.weight, backbone.layers.2.blocks.13.attn.qkv.bias, backbone.layers.2.blocks.13.attn.proj.weight, backbone.layers.2.blocks.13.attn.proj.bias, backbone.layers.2.blocks.13.norm2.weight, backbone.layers.2.blocks.13.norm2.bias, backbone.layers.2.blocks.13.mlp.fc1.weight, backbone.layers.2.blocks.13.mlp.fc1.bias, backbone.layers.2.blocks.13.mlp.fc2.weight, backbone.layers.2.blocks.13.mlp.fc2.bias, backbone.layers.2.blocks.14.norm1.weight, backbone.layers.2.blocks.14.norm1.bias, backbone.layers.2.blocks.14.attn.relative_position_bias_table, backbone.layers.2.blocks.14.attn.relative_position_index, backbone.layers.2.blocks.14.attn.qkv.weight, backbone.layers.2.blocks.14.attn.qkv.bias, backbone.layers.2.blocks.14.attn.proj.weight, backbone.layers.2.blocks.14.attn.proj.bias, backbone.layers.2.blocks.14.norm2.weight, backbone.layers.2.blocks.14.norm2.bias, backbone.layers.2.blocks.14.mlp.fc1.weight, backbone.layers.2.blocks.14.mlp.fc1.bias, backbone.layers.2.blocks.14.mlp.fc2.weight, backbone.layers.2.blocks.14.mlp.fc2.bias, backbone.layers.2.blocks.15.norm1.weight, backbone.layers.2.blocks.15.norm1.bias, backbone.layers.2.blocks.15.attn.relative_position_bias_table, backbone.layers.2.blocks.15.attn.relative_position_index, backbone.layers.2.blocks.15.attn.qkv.weight, backbone.layers.2.blocks.15.attn.qkv.bias, backbone.layers.2.blocks.15.attn.proj.weight, backbone.layers.2.blocks.15.attn.proj.bias, backbone.layers.2.blocks.15.norm2.weight, backbone.layers.2.blocks.15.norm2.bias, backbone.layers.2.blocks.15.mlp.fc1.weight, backbone.layers.2.blocks.15.mlp.fc1.bias, backbone.layers.2.blocks.15.mlp.fc2.weight, backbone.layers.2.blocks.15.mlp.fc2.bias, backbone.layers.2.blocks.16.norm1.weight, backbone.layers.2.blocks.16.norm1.bias, backbone.layers.2.blocks.16.attn.relative_position_bias_table, backbone.layers.2.blocks.16.attn.relative_position_index, backbone.layers.2.blocks.16.attn.qkv.weight, backbone.layers.2.blocks.16.attn.qkv.bias, backbone.layers.2.blocks.16.attn.proj.weight, backbone.layers.2.blocks.16.attn.proj.bias, backbone.layers.2.blocks.16.norm2.weight, backbone.layers.2.blocks.16.norm2.bias, backbone.layers.2.blocks.16.mlp.fc1.weight, backbone.layers.2.blocks.16.mlp.fc1.bias, backbone.layers.2.blocks.16.mlp.fc2.weight, backbone.layers.2.blocks.16.mlp.fc2.bias, backbone.layers.2.blocks.17.norm1.weight, backbone.layers.2.blocks.17.norm1.bias, backbone.layers.2.blocks.17.attn.relative_position_bias_table, backbone.layers.2.blocks.17.attn.relative_position_index, backbone.layers.2.blocks.17.attn.qkv.weight, backbone.layers.2.blocks.17.attn.qkv.bias, backbone.layers.2.blocks.17.attn.proj.weight, backbone.layers.2.blocks.17.attn.proj.bias, backbone.layers.2.blocks.17.norm2.weight, backbone.layers.2.blocks.17.norm2.bias, backbone.layers.2.blocks.17.mlp.fc1.weight, backbone.layers.2.blocks.17.mlp.fc1.bias, backbone.layers.2.blocks.17.mlp.fc2.weight, backbone.layers.2.blocks.17.mlp.fc2.bias, backbone.layers.2.downsample.reduction.weight, backbone.layers.2.downsample.norm.weight, backbone.layers.2.downsample.norm.bias, backbone.layers.3.blocks.0.norm1.weight, backbone.layers.3.blocks.0.norm1.bias, backbone.layers.3.blocks.0.attn.relative_position_bias_table, backbone.layers.3.blocks.0.attn.relative_position_index, backbone.layers.3.blocks.0.attn.qkv.weight, backbone.layers.3.blocks.0.attn.qkv.bias, backbone.layers.3.blocks.0.attn.proj.weight, backbone.layers.3.blocks.0.attn.proj.bias, backbone.layers.3.blocks.0.norm2.weight, backbone.layers.3.blocks.0.norm2.bias, backbone.layers.3.blocks.0.mlp.fc1.weight, backbone.layers.3.blocks.0.mlp.fc1.bias, backbone.layers.3.blocks.0.mlp.fc2.weight, backbone.layers.3.blocks.0.mlp.fc2.bias, backbone.layers.3.blocks.1.norm1.weight, backbone.layers.3.blocks.1.norm1.bias, backbone.layers.3.blocks.1.attn.relative_position_bias_table, backbone.layers.3.blocks.1.attn.relative_position_index, backbone.layers.3.blocks.1.attn.qkv.weight, backbone.layers.3.blocks.1.attn.qkv.bias, backbone.layers.3.blocks.1.attn.proj.weight, backbone.layers.3.blocks.1.attn.proj.bias, backbone.layers.3.blocks.1.norm2.weight, backbone.layers.3.blocks.1.norm2.bias, backbone.layers.3.blocks.1.mlp.fc1.weight, backbone.layers.3.blocks.1.mlp.fc1.bias, backbone.layers.3.blocks.1.mlp.fc2.weight, backbone.layers.3.blocks.1.mlp.fc2.bias, backbone.patch_embed.proj.weight, backbone.patch_embed.proj.bias, roi_head.mask_head.0.convs.0.conv.weight, roi_head.mask_head.0.convs.0.conv.bias, roi_head.mask_head.0.convs.1.conv.weight, roi_head.mask_head.0.convs.1.conv.bias, roi_head.mask_head.0.convs.2.conv.weight, roi_head.mask_head.0.convs.2.conv.bias, roi_head.mask_head.0.convs.3.conv.weight, roi_head.mask_head.0.convs.3.conv.bias, roi_head.mask_head.0.upsample.weight, roi_head.mask_head.0.upsample.bias, roi_head.mask_head.0.conv_logits.weight, roi_head.mask_head.0.conv_logits.bias, roi_head.mask_head.1.convs.0.conv.weight, roi_head.mask_head.1.convs.0.conv.bias, roi_head.mask_head.1.convs.1.conv.weight, roi_head.mask_head.1.convs.1.conv.bias, roi_head.mask_head.1.convs.2.conv.weight, roi_head.mask_head.1.convs.2.conv.bias, roi_head.mask_head.1.convs.3.conv.weight, roi_head.mask_head.1.convs.3.conv.bias, roi_head.mask_head.1.upsample.weight, roi_head.mask_head.1.upsample.bias, roi_head.mask_head.1.conv_logits.weight, roi_head.mask_head.1.conv_logits.bias, roi_head.mask_head.2.convs.0.conv.weight, roi_head.mask_head.2.convs.0.conv.bias, roi_head.mask_head.2.convs.1.conv.weight, roi_head.mask_head.2.convs.1.conv.bias, roi_head.mask_head.2.convs.2.conv.weight, roi_head.mask_head.2.convs.2.conv.bias, roi_head.mask_head.2.convs.3.conv.weight, roi_head.mask_head.2.convs.3.conv.bias, roi_head.mask_head.2.upsample.weight, roi_head.mask_head.2.upsample.bias, roi_head.mask_head.2.conv_logits.weight, roi_head.mask_head.2.conv_logits.bias

missing keys in source state_dict: backbone.patch_embed.projection.weight, backbone.patch_embed.projection.bias, backbone.stages.0.blocks.0.norm1.weight, backbone.stages.0.blocks.0.norm1.bias, backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.0.attn.w_msa.qkv.weight, backbone.stages.0.blocks.0.attn.w_msa.qkv.bias, backbone.stages.0.blocks.0.attn.w_msa.proj.weight, backbone.stages.0.blocks.0.attn.w_msa.proj.bias, backbone.stages.0.blocks.0.norm2.weight, backbone.stages.0.blocks.0.norm2.bias, backbone.stages.0.blocks.0.ffn.layers.0.0.weight, backbone.stages.0.blocks.0.ffn.layers.0.0.bias, backbone.stages.0.blocks.0.ffn.layers.1.weight, backbone.stages.0.blocks.0.ffn.layers.1.bias, backbone.stages.0.blocks.1.norm1.weight, backbone.stages.0.blocks.1.norm1.bias, backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.qkv.weight, backbone.stages.0.blocks.1.attn.w_msa.qkv.bias, backbone.stages.0.blocks.1.attn.w_msa.proj.weight, backbone.stages.0.blocks.1.attn.w_msa.proj.bias, backbone.stages.0.blocks.1.norm2.weight, backbone.stages.0.blocks.1.norm2.bias, backbone.stages.0.blocks.1.ffn.layers.0.0.weight, backbone.stages.0.blocks.1.ffn.layers.0.0.bias, backbone.stages.0.blocks.1.ffn.layers.1.weight, backbone.stages.0.blocks.1.ffn.layers.1.bias, backbone.stages.0.downsample.norm.weight, backbone.stages.0.downsample.norm.bias, backbone.stages.0.downsample.reduction.weight, backbone.stages.1.blocks.0.norm1.weight, backbone.stages.1.blocks.0.norm1.bias, backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.qkv.weight, backbone.stages.1.blocks.0.attn.w_msa.qkv.bias, backbone.stages.1.blocks.0.attn.w_msa.proj.weight, backbone.stages.1.blocks.0.attn.w_msa.proj.bias, backbone.stages.1.blocks.0.norm2.weight, backbone.stages.1.blocks.0.norm2.bias, backbone.stages.1.blocks.0.ffn.layers.0.0.weight, backbone.stages.1.blocks.0.ffn.layers.0.0.bias, backbone.stages.1.blocks.0.ffn.layers.1.weight, backbone.stages.1.blocks.0.ffn.layers.1.bias, backbone.stages.1.blocks.1.norm1.weight, backbone.stages.1.blocks.1.norm1.bias, backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.qkv.weight, backbone.stages.1.blocks.1.attn.w_msa.qkv.bias, backbone.stages.1.blocks.1.attn.w_msa.proj.weight, backbone.stages.1.blocks.1.attn.w_msa.proj.bias, backbone.stages.1.blocks.1.norm2.weight, backbone.stages.1.blocks.1.norm2.bias, backbone.stages.1.blocks.1.ffn.layers.0.0.weight, backbone.stages.1.blocks.1.ffn.layers.0.0.bias, backbone.stages.1.blocks.1.ffn.layers.1.weight, backbone.stages.1.blocks.1.ffn.layers.1.bias, backbone.stages.1.downsample.norm.weight, backbone.stages.1.downsample.norm.bias, backbone.stages.1.downsample.reduction.weight, backbone.stages.2.blocks.0.norm1.weight, backbone.stages.2.blocks.0.norm1.bias, backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.qkv.weight, backbone.stages.2.blocks.0.attn.w_msa.qkv.bias, backbone.stages.2.blocks.0.attn.w_msa.proj.weight, backbone.stages.2.blocks.0.attn.w_msa.proj.bias, backbone.stages.2.blocks.0.norm2.weight, backbone.stages.2.blocks.0.norm2.bias, backbone.stages.2.blocks.0.ffn.layers.0.0.weight, backbone.stages.2.blocks.0.ffn.layers.0.0.bias, backbone.stages.2.blocks.0.ffn.layers.1.weight, backbone.stages.2.blocks.0.ffn.layers.1.bias, backbone.stages.2.blocks.1.norm1.weight, backbone.stages.2.blocks.1.norm1.bias, backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.qkv.weight, backbone.stages.2.blocks.1.attn.w_msa.qkv.bias, backbone.stages.2.blocks.1.attn.w_msa.proj.weight, backbone.stages.2.blocks.1.attn.w_msa.proj.bias, backbone.stages.2.blocks.1.norm2.weight, backbone.stages.2.blocks.1.norm2.bias, backbone.stages.2.blocks.1.ffn.layers.0.0.weight, backbone.stages.2.blocks.1.ffn.layers.0.0.bias, backbone.stages.2.blocks.1.ffn.layers.1.weight, backbone.stages.2.blocks.1.ffn.layers.1.bias, backbone.stages.2.blocks.2.norm1.weight, backbone.stages.2.blocks.2.norm1.bias, backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.qkv.weight, backbone.stages.2.blocks.2.attn.w_msa.qkv.bias, backbone.stages.2.blocks.2.attn.w_msa.proj.weight, backbone.stages.2.blocks.2.attn.w_msa.proj.bias, backbone.stages.2.blocks.2.norm2.weight, backbone.stages.2.blocks.2.norm2.bias, backbone.stages.2.blocks.2.ffn.layers.0.0.weight, backbone.stages.2.blocks.2.ffn.layers.0.0.bias, backbone.stages.2.blocks.2.ffn.layers.1.weight, backbone.stages.2.blocks.2.ffn.layers.1.bias, backbone.stages.2.blocks.3.norm1.weight, backbone.stages.2.blocks.3.norm1.bias, backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.qkv.weight, backbone.stages.2.blocks.3.attn.w_msa.qkv.bias, backbone.stages.2.blocks.3.attn.w_msa.proj.weight, backbone.stages.2.blocks.3.attn.w_msa.proj.bias, backbone.stages.2.blocks.3.norm2.weight, backbone.stages.2.blocks.3.norm2.bias, backbone.stages.2.blocks.3.ffn.layers.0.0.weight, backbone.stages.2.blocks.3.ffn.layers.0.0.bias, backbone.stages.2.blocks.3.ffn.layers.1.weight, backbone.stages.2.blocks.3.ffn.layers.1.bias, backbone.stages.2.blocks.4.norm1.weight, backbone.stages.2.blocks.4.norm1.bias, backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.qkv.weight, backbone.stages.2.blocks.4.attn.w_msa.qkv.bias, backbone.stages.2.blocks.4.attn.w_msa.proj.weight, backbone.stages.2.blocks.4.attn.w_msa.proj.bias, backbone.stages.2.blocks.4.norm2.weight, backbone.stages.2.blocks.4.norm2.bias, backbone.stages.2.blocks.4.ffn.layers.0.0.weight, backbone.stages.2.blocks.4.ffn.layers.0.0.bias, backbone.stages.2.blocks.4.ffn.layers.1.weight, backbone.stages.2.blocks.4.ffn.layers.1.bias, backbone.stages.2.blocks.5.norm1.weight, backbone.stages.2.blocks.5.norm1.bias, backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.qkv.weight, backbone.stages.2.blocks.5.attn.w_msa.qkv.bias, backbone.stages.2.blocks.5.attn.w_msa.proj.weight, backbone.stages.2.blocks.5.attn.w_msa.proj.bias, backbone.stages.2.blocks.5.norm2.weight, backbone.stages.2.blocks.5.norm2.bias, backbone.stages.2.blocks.5.ffn.layers.0.0.weight, backbone.stages.2.blocks.5.ffn.layers.0.0.bias, backbone.stages.2.blocks.5.ffn.layers.1.weight, backbone.stages.2.blocks.5.ffn.layers.1.bias, backbone.stages.2.blocks.6.norm1.weight, backbone.stages.2.blocks.6.norm1.bias, backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.6.attn.w_msa.relative_position_index, backbone.stages.2.blocks.6.attn.w_msa.qkv.weight, backbone.stages.2.blocks.6.attn.w_msa.qkv.bias, backbone.stages.2.blocks.6.attn.w_msa.proj.weight, backbone.stages.2.blocks.6.attn.w_msa.proj.bias, backbone.stages.2.blocks.6.norm2.weight, backbone.stages.2.blocks.6.norm2.bias, backbone.stages.2.blocks.6.ffn.layers.0.0.weight, backbone.stages.2.blocks.6.ffn.layers.0.0.bias, backbone.stages.2.blocks.6.ffn.layers.1.weight, backbone.stages.2.blocks.6.ffn.layers.1.bias, backbone.stages.2.blocks.7.norm1.weight, backbone.stages.2.blocks.7.norm1.bias, backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.7.attn.w_msa.relative_position_index, backbone.stages.2.blocks.7.attn.w_msa.qkv.weight, backbone.stages.2.blocks.7.attn.w_msa.qkv.bias, backbone.stages.2.blocks.7.attn.w_msa.proj.weight, backbone.stages.2.blocks.7.attn.w_msa.proj.bias, backbone.stages.2.blocks.7.norm2.weight, backbone.stages.2.blocks.7.norm2.bias, backbone.stages.2.blocks.7.ffn.layers.0.0.weight, backbone.stages.2.blocks.7.ffn.layers.0.0.bias, backbone.stages.2.blocks.7.ffn.layers.1.weight, backbone.stages.2.blocks.7.ffn.layers.1.bias, backbone.stages.2.blocks.8.norm1.weight, backbone.stages.2.blocks.8.norm1.bias, backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.8.attn.w_msa.relative_position_index, backbone.stages.2.blocks.8.attn.w_msa.qkv.weight, backbone.stages.2.blocks.8.attn.w_msa.qkv.bias, backbone.stages.2.blocks.8.attn.w_msa.proj.weight, backbone.stages.2.blocks.8.attn.w_msa.proj.bias, backbone.stages.2.blocks.8.norm2.weight, backbone.stages.2.blocks.8.norm2.bias, backbone.stages.2.blocks.8.ffn.layers.0.0.weight, backbone.stages.2.blocks.8.ffn.layers.0.0.bias, backbone.stages.2.blocks.8.ffn.layers.1.weight, backbone.stages.2.blocks.8.ffn.layers.1.bias, backbone.stages.2.blocks.9.norm1.weight, backbone.stages.2.blocks.9.norm1.bias, backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.9.attn.w_msa.relative_position_index, backbone.stages.2.blocks.9.attn.w_msa.qkv.weight, backbone.stages.2.blocks.9.attn.w_msa.qkv.bias, backbone.stages.2.blocks.9.attn.w_msa.proj.weight, backbone.stages.2.blocks.9.attn.w_msa.proj.bias, backbone.stages.2.blocks.9.norm2.weight, backbone.stages.2.blocks.9.norm2.bias, backbone.stages.2.blocks.9.ffn.layers.0.0.weight, backbone.stages.2.blocks.9.ffn.layers.0.0.bias, backbone.stages.2.blocks.9.ffn.layers.1.weight, backbone.stages.2.blocks.9.ffn.layers.1.bias, backbone.stages.2.blocks.10.norm1.weight, backbone.stages.2.blocks.10.norm1.bias, backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.10.attn.w_msa.relative_position_index, backbone.stages.2.blocks.10.attn.w_msa.qkv.weight, backbone.stages.2.blocks.10.attn.w_msa.qkv.bias, backbone.stages.2.blocks.10.attn.w_msa.proj.weight, backbone.stages.2.blocks.10.attn.w_msa.proj.bias, backbone.stages.2.blocks.10.norm2.weight, backbone.stages.2.blocks.10.norm2.bias, backbone.stages.2.blocks.10.ffn.layers.0.0.weight, backbone.stages.2.blocks.10.ffn.layers.0.0.bias, backbone.stages.2.blocks.10.ffn.layers.1.weight, backbone.stages.2.blocks.10.ffn.layers.1.bias, backbone.stages.2.blocks.11.norm1.weight, backbone.stages.2.blocks.11.norm1.bias, backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.11.attn.w_msa.relative_position_index, backbone.stages.2.blocks.11.attn.w_msa.qkv.weight, backbone.stages.2.blocks.11.attn.w_msa.qkv.bias, backbone.stages.2.blocks.11.attn.w_msa.proj.weight, backbone.stages.2.blocks.11.attn.w_msa.proj.bias, backbone.stages.2.blocks.11.norm2.weight, backbone.stages.2.blocks.11.norm2.bias, backbone.stages.2.blocks.11.ffn.layers.0.0.weight, backbone.stages.2.blocks.11.ffn.layers.0.0.bias, backbone.stages.2.blocks.11.ffn.layers.1.weight, backbone.stages.2.blocks.11.ffn.layers.1.bias, backbone.stages.2.blocks.12.norm1.weight, backbone.stages.2.blocks.12.norm1.bias, backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.12.attn.w_msa.relative_position_index, backbone.stages.2.blocks.12.attn.w_msa.qkv.weight, backbone.stages.2.blocks.12.attn.w_msa.qkv.bias, backbone.stages.2.blocks.12.attn.w_msa.proj.weight, backbone.stages.2.blocks.12.attn.w_msa.proj.bias, backbone.stages.2.blocks.12.norm2.weight, backbone.stages.2.blocks.12.norm2.bias, backbone.stages.2.blocks.12.ffn.layers.0.0.weight, backbone.stages.2.blocks.12.ffn.layers.0.0.bias, backbone.stages.2.blocks.12.ffn.layers.1.weight, backbone.stages.2.blocks.12.ffn.layers.1.bias, backbone.stages.2.blocks.13.norm1.weight, backbone.stages.2.blocks.13.norm1.bias, backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.13.attn.w_msa.relative_position_index, backbone.stages.2.blocks.13.attn.w_msa.qkv.weight, backbone.stages.2.blocks.13.attn.w_msa.qkv.bias, backbone.stages.2.blocks.13.attn.w_msa.proj.weight, backbone.stages.2.blocks.13.attn.w_msa.proj.bias, backbone.stages.2.blocks.13.norm2.weight, backbone.stages.2.blocks.13.norm2.bias, backbone.stages.2.blocks.13.ffn.layers.0.0.weight, backbone.stages.2.blocks.13.ffn.layers.0.0.bias, backbone.stages.2.blocks.13.ffn.layers.1.weight, backbone.stages.2.blocks.13.ffn.layers.1.bias, backbone.stages.2.blocks.14.norm1.weight, backbone.stages.2.blocks.14.norm1.bias, backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.14.attn.w_msa.relative_position_index, backbone.stages.2.blocks.14.attn.w_msa.qkv.weight, backbone.stages.2.blocks.14.attn.w_msa.qkv.bias, backbone.stages.2.blocks.14.attn.w_msa.proj.weight, backbone.stages.2.blocks.14.attn.w_msa.proj.bias, backbone.stages.2.blocks.14.norm2.weight, backbone.stages.2.blocks.14.norm2.bias, backbone.stages.2.blocks.14.ffn.layers.0.0.weight, backbone.stages.2.blocks.14.ffn.layers.0.0.bias, backbone.stages.2.blocks.14.ffn.layers.1.weight, backbone.stages.2.blocks.14.ffn.layers.1.bias, backbone.stages.2.blocks.15.norm1.weight, backbone.stages.2.blocks.15.norm1.bias, backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.15.attn.w_msa.relative_position_index, backbone.stages.2.blocks.15.attn.w_msa.qkv.weight, backbone.stages.2.blocks.15.attn.w_msa.qkv.bias, backbone.stages.2.blocks.15.attn.w_msa.proj.weight, backbone.stages.2.blocks.15.attn.w_msa.proj.bias, backbone.stages.2.blocks.15.norm2.weight, backbone.stages.2.blocks.15.norm2.bias, backbone.stages.2.blocks.15.ffn.layers.0.0.weight, backbone.stages.2.blocks.15.ffn.layers.0.0.bias, backbone.stages.2.blocks.15.ffn.layers.1.weight, backbone.stages.2.blocks.15.ffn.layers.1.bias, backbone.stages.2.blocks.16.norm1.weight, backbone.stages.2.blocks.16.norm1.bias, backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.16.attn.w_msa.relative_position_index, backbone.stages.2.blocks.16.attn.w_msa.qkv.weight, backbone.stages.2.blocks.16.attn.w_msa.qkv.bias, backbone.stages.2.blocks.16.attn.w_msa.proj.weight, backbone.stages.2.blocks.16.attn.w_msa.proj.bias, backbone.stages.2.blocks.16.norm2.weight, backbone.stages.2.blocks.16.norm2.bias, backbone.stages.2.blocks.16.ffn.layers.0.0.weight, backbone.stages.2.blocks.16.ffn.layers.0.0.bias, backbone.stages.2.blocks.16.ffn.layers.1.weight, backbone.stages.2.blocks.16.ffn.layers.1.bias, backbone.stages.2.blocks.17.norm1.weight, backbone.stages.2.blocks.17.norm1.bias, backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table, backbone.stages.2.blocks.17.attn.w_msa.relative_position_index, backbone.stages.2.blocks.17.attn.w_msa.qkv.weight, backbone.stages.2.blocks.17.attn.w_msa.qkv.bias, backbone.stages.2.blocks.17.attn.w_msa.proj.weight, backbone.stages.2.blocks.17.attn.w_msa.proj.bias, backbone.stages.2.blocks.17.norm2.weight, backbone.stages.2.blocks.17.norm2.bias, backbone.stages.2.blocks.17.ffn.layers.0.0.weight, backbone.stages.2.blocks.17.ffn.layers.0.0.bias, backbone.stages.2.blocks.17.ffn.layers.1.weight, backbone.stages.2.blocks.17.ffn.layers.1.bias, backbone.stages.2.downsample.norm.weight, backbone.stages.2.downsample.norm.bias, backbone.stages.2.downsample.reduction.weight, backbone.stages.3.blocks.0.norm1.weight, backbone.stages.3.blocks.0.norm1.bias, backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.qkv.weight, backbone.stages.3.blocks.0.attn.w_msa.qkv.bias, backbone.stages.3.blocks.0.attn.w_msa.proj.weight, backbone.stages.3.blocks.0.attn.w_msa.proj.bias, backbone.stages.3.blocks.0.norm2.weight, backbone.stages.3.blocks.0.norm2.bias, backbone.stages.3.blocks.0.ffn.layers.0.0.weight, backbone.stages.3.blocks.0.ffn.layers.0.0.bias, backbone.stages.3.blocks.0.ffn.layers.1.weight, backbone.stages.3.blocks.0.ffn.layers.1.bias, backbone.stages.3.blocks.1.norm1.weight, backbone.stages.3.blocks.1.norm1.bias, backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.qkv.weight, backbone.stages.3.blocks.1.attn.w_msa.qkv.bias, backbone.stages.3.blocks.1.attn.w_msa.proj.weight, backbone.stages.3.blocks.1.attn.w_msa.proj.bias, backbone.stages.3.blocks.1.norm2.weight, backbone.stages.3.blocks.1.norm2.bias, backbone.stages.3.blocks.1.ffn.layers.0.0.weight, backbone.stages.3.blocks.1.ffn.layers.0.0.bias, backbone.stages.3.blocks.1.ffn.layers.1.weight, backbone.stages.3.blocks.1.ffn.layers.1.bias
```
I try to fix it by change the code to ' new_ckpt['backbone.' + new_k] = new_v"" as mentioned in https://github.com/open-mmlab/mmdetection/pull/6019#issuecomment-911622490. However, the error still appear. And the method in https://github.com/open-mmlab/mmdetection/pull/6019https://github.com/open-mmlab/mmdetection/pull/6019  doesn't work either. My config file is as following:
```python
# model settings
pretrained = 'checkpoints/swin_base_patch4_window7_224.pth'
model = dict(
    type='CascadeRCNN',
    backbone=dict(
        type='SwinTransformer',
        embed_dims=128,
        depths=[2, 2, 18, 2],
        num_heads=[4, 8, 16, 32],
        window_size=7,
        mlp_ratio=4,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.,
        attn_drop_rate=0.,
        drop_path_rate=0.3,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        with_cp=False,
        convert_weights=True,
        init_cfg=dict(type='Pretrained', checkpoint=pretrained)),
load_from = 'checkpoints/cascade_mask_rcnn_swin_base_patch4_window7.pth'
```
And my mmdet version is 2.23.0. How to fix it ?"
Output prediction file,open-mmlab/mmdetection,2022-04-25 03:35:37,3,,7820,1213920502,"mmdetection现在的output file文件格式只有一种pkl了是么？之前可以直接输出json，现在不行了。

and生成的pkl文件，只是单纯的预测值，没有coco格式的其他信息。转化为json格式后，不是输出一个coco格式的annotation预测文件。这个问题要怎么解决？"
Question of using my own dataset ,open-mmlab/mmdetection,2022-04-25 02:44:29,2,,7819,1213883041,"Hi, I using mmDetection on my own UAVDT dataset, but I found the obejects on the right of all images can not be detected. I wonder if I  should change the image size in anywhere to fix this problem? The images in UAVDT is 1024 * 540."
Can I use HTC directly for target detection?,open-mmlab/mmdetection,2022-04-25 02:15:09,3,,7818,1213864621,"In my parameter Settings, I annotated all the MASK part of HTC, at this time I saw that the program can be trained, and the RPN part used HTC module. Is this using the HTC module?"
How can I do not use the pretraining model?,open-mmlab/mmdetection,2022-04-24 13:35:08,3,Doc#How-to,7817,1213656923,"I use resnet50_fpn, But I do not want to use pretraining model, so I let **model = dict( backbone=dict( init_cfg=None))**. But I found that the default parameters, importing the pretraining model or setting  init_cfg to none all have the same loss, which is around 1.0. 

I found that the loss in the official readme finally converged to around 0.5, so it's not that I canceled the pretraining model, but that the pretraining model was not loaded at all?"
Severe memory leaks when num_workers != 0,open-mmlab/mmdetection,2022-04-21 17:15:14,18,community discussion,7786,1211327726,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**

**Any model trained on a single or multiple gpus with train num_workers > 0 will cause a severe cpu memory leak (no gpu memory leaks detected). The memory consumption follows a sawtooth pattern, gradually increasing until there is a memory error. I have now tested this with several datasets and models, mmcv_full == 1.4.8, mmdet == 2.23.0. Based on my investigation of the bug, it is unclear if this is due to pytorch or mmdetection code. Does your team have a protocol for detecting memory leaks in your code? It may be useful to identify the issue.**

1. What command or script did you run?

**Tested with several default config models with default and custom datasets.**

2. Did you make any modifications on the code or config? Did you understand what you have modified?

**No changes made.**

4. What dataset did you use?

**Environment**

**sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Quadro RTX 8000
CUDA_HOME: /usr
NVCC: Cuda compilation tools, release 10.0, V10.0.130
GCC: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF,

TorchVision: 0.12.0
OpenCV: 4.5.5
MMCV: 1.4.8
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.23.0+e97e900**


**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
faster rcnn 不同loss的配置文件,open-mmlab/mmdetection,2022-04-21 16:07:28,2,bug#Doc,7783,1211250412,"configs下的faster rcnn的readme.md中的Different regression loss的四个loss对应的config文件是一模一样的？？？是不是链接错了？？
and最新版本的mmdetection多了其他的loss，例如CIoULoss，还没有更新到readme文件中"
"""TypeError: argument of type 'NoneType' is not iterable"" when use mixup and alub augmentation together",open-mmlab/mmdetection,2022-04-18 12:51:34,3,,7746,1207019384,"When I try to use mixup and alub augmentation together, I get following error:
```python
Original Traceback (most recent call last):
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 202, in _worker_loop
    data = fetcher.fetch(index)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/sist/tqzouustc/code/mmdetection/mmdet/datasets/dataset_wrappers.py"", line 369, in __getitem__
    if 'mix_results' in results:
TypeError: argument of type 'NoneType' is not iterable
```
My config is as below:
```python
# model settings
model = dict(
    type='CascadeRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        # anchor_size = (scales*base_sizes(默认为strides))^2
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[4],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),
    roi_head=dict(
        # 提交结果时改成HazyCascadeRoIHeadS
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=1,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=1,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=1,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=[
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.5,
                    neg_iou_thr=0.5,
                    min_pos_iou=0.5,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.6,
                    neg_iou_thr=0.6,
                    min_pos_iou=0.6,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.7,
                    neg_iou_thr=0.7,
                    min_pos_iou=0.7,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False)
        ]),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.01,
            nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.01),
            max_per_img=100)))


# dataset settings
dataset_type = 'CocoDataset'
data_root = '/home/sist/tqzouustc/dataset/Object detection in haze/coco/'
classes = ('vehicle',)
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
hrc_train_transforms = [
    dict(
        type=""OneOf"",
        transforms=[
            dict(type=""HueSaturationValue"", hue_shift_limit=10, sat_shift_limit=35, val_shift_limit=25),
            dict(type=""RandomGamma""),
            dict(type=""CLAHE""),
        ],
        p=0.5)
]

albu_train_transforms = [
    dict(
        type='RandomBrightnessContrast',
        brightness_limit=[0.1, 0.3],
        contrast_limit=[0.1, 0.3],
        p=0.2),
    dict(
        type='ShiftScaleRotate',
        shift_limit=0.0625,
        scale_limit=0.0,
        rotate_limit=180,
        interpolation=1,
        p=0.5),
    dict(type='ChannelShuffle', p=0.1),
    dict(type='JpegCompression', quality_lower=85, quality_upper=95, p=0.2),
    dict(
        type='OneOf',
        transforms=[
            dict(type='Blur', blur_limit=3, p=1.0),
            dict(type='MedianBlur', blur_limit=3, p=1.0)
        ],
        p=0.1),
    dict(type='Cutout', num_holes=8, max_h_size=64, max_w_size=64, fill_value=img_norm_cfg[""mean""][::-1], p=0.5),
]

# problem code
**train_pipeline = [
    dict(
        type='MixUp',
        img_scale=(1845, 750),
        ratio_range=(0.8, 1.6),
        pad_val=114.0),
    dict(
        type='Resize',
        # 256-512 multi-scale
        img_scale=[(1845, 400), (1845, 1000)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(
        type='Albu',
        transforms=albu_train_transforms,
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=['gt_labels'],
            min_visibility=0.0,
            filter_lost_elements=True),
        keymap={
            'img': 'image',
            'gt_bboxes': 'bboxes'
        },
        update_pad_shape=False,
        skip_img_without_anno=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])#这里定义了dataloader每个batch含有的信息
]**
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1845, 750),
        flip=True,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=4,
    train=dict(
        type='MultiImageMixDataset',
        dataset=dict(
            type=dataset_type,
            classes=classes,
            ann_file=data_root + 'annotations/instances_train2017_hazy_cropped.json',
            img_prefix=data_root + 'images/train2017',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True),
                dict(
                    type='Albu',
                    transforms=hrc_train_transforms,
                    bbox_params=dict(
                        type='BboxParams',
                        format='pascal_voc',
                        label_fields=['gt_labels'],
                        min_visibility=0.0,
                        filter_lost_elements=True),
                    keymap={
                        'img': 'image',
                        'gt_bboxes': 'bboxes'
                    },
                    update_pad_shape=False,
                    skip_img_without_anno=True),
            ],
            filter_empty_gt=False,
        ),
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        samples_per_gpu=1,
        classes=classes,
        ann_file=data_root + 'annotations/instances_val2017_dry_run.json',
        img_prefix=data_root + 'images/val_dry_run',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        classes=classes,
        samples_per_gpu=1,
        ann_file=data_root + 'annotations/instances_val2017_dry_run.json',
        img_prefix=data_root + 'images/val_dry_run',
        pipeline=test_pipeline))
evaluation = dict(interval=1, metric='bbox')

# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[27, 33])
runner = dict(type='EpochBasedRunner', max_epochs=36)

checkpoint_config = dict(interval=1)
# yapf:disable
log_config = dict(
    interval=2,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook')
    ])
# yapf:enable
custom_hooks = [dict(type='NumClassCheckHook')]

dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'checkpoints/cascade_mask_rcnn_r50_fpn_20.pth'
resume_from = None
workflow = [('train', 1)]


```
Is this a bug?"
Where can i see validation loss ,open-mmlab/mmdetection,2022-04-18 05:56:40,3,,7744,1206695828,"hello,

Where can I see the calculation code to calculate the validation loss??
Or I want to take the validation loss and use it as a criterion for early stopping, but is there a way?"
RuntimeError: CUDA error: no kernel image is available for execution on the device,open-mmlab/mmdetection,2022-04-18 02:57:32,6,,7742,1206599154,"**Describe the bug**

```
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
```


**Reproduction**

```
python demo/image_demo.py demo/demo.jpg configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
```


**Environment**

` python mmdet/utils/collect_env.py`

```
sys.platform: linux
Python: 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: NVIDIA RTX A5000
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.5.r11.5/compiler.30672275_0
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.11.0+cu115
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.5
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.5, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF,

TorchVision: 0.12.0+cu115
OpenCV: 4.5.5
MMCV: 1.4.7
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.5
MMDetection: 2.22.0+70f6d9c

```

**Error traceback**
```
    result = inference_detector(model, args.img)
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/apis/inference.py"", line 150, in inference_detector
    results = model(return_loss=False, rescale=True, **data)
  File ""/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py"", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 109, in new_func
    return old_func(*args, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/detectors/base.py"", line 174, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/detectors/base.py"", line 147, in forward_test
    return self.simple_test(imgs[0], img_metas[0], **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/detectors/two_stage.py"", line 179, in simple_test
    proposal_list = self.rpn_head.simple_test_rpn(x, img_metas)
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/dense_heads/dense_test_mixins.py"", line 130, in simple_test_rpn
    proposal_list = self.get_bboxes(*rpn_outs, img_metas=img_metas)
  File ""/opt/conda/lib/python3.9/site-packages/mmcv/runner/fp16_utils.py"", line 197, in new_func
    return old_func(*args, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/dense_heads/base_dense_head.py"", line 102, in get_bboxes
    results = self._get_bboxes_single(cls_score_list, bbox_pred_list,
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/dense_heads/rpn_head.py"", line 185, in _get_bboxes_single
    return self._bbox_post_process(mlvl_scores, mlvl_bbox_preds,
  File ""/opt/conda/lib/python3.9/site-packages/mmdet/models/dense_heads/rpn_head.py"", line 231, in _bbox_post_process
    dets, _ = batched_nms(proposals, scores, ids, cfg.nms)
  File ""/opt/conda/lib/python3.9/site-packages/mmcv/ops/nms.py"", line 326, in batched_nms
    dets, keep = nms_op(boxes_for_nms, scores, **nms_cfg_)
  File ""/opt/conda/lib/python3.9/site-packages/mmcv/utils/misc.py"", line 340, in new_func
    output = old_func(*args, **kwargs)
  File ""/opt/conda/lib/python3.9/site-packages/mmcv/ops/nms.py"", line 172, in nms
    inds = NMSop.apply(boxes, scores, iou_threshold, offset,
  File ""/opt/conda/lib/python3.9/site-packages/mmcv/ops/nms.py"", line 26, in forward
    inds = ext_module.nms(
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
```

"
mAP==-1,open-mmlab/mmdetection,2022-04-17 05:45:08,1,,7739,1206288041,"What's wrong with me, the mAP ==-1, and  have run 5 epochs total 12 epochs
![image](https://user-images.githubusercontent.com/43838913/163702240-cccd7269-5ca4-47fa-9dd1-08fc78e384b6.png)


```python
num_classes = 50
# model settings
model = dict(
    type='CascadeRCNN',
    backbone=dict(
        type='SwinTransformer',
        embed_dims=128,
        depths=[2, 2, 18, 2],
        num_heads=[4, 8, 16, 32],
        window_size=7,
        mlp_ratio=4,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.3,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        with_cp=False,
        convert_weights=True,
        init_cfg=dict(
            type='Pretrained',
            checkpoint='swin_base_patch4_window7_224.pth')
    ),
    neck=dict(
        type='FPN',
        in_channels=[128, 256, 512, 1024],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.25, 0.5, 1.0, 2.0, 4.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            # gc_context=True,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=num_classes,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0,
                    smoothing=0.001),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=num_classes,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0,
                    smoothing=0.001),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=num_classes,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0,
                    smoothing=0.001),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=[
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.5,
                    neg_iou_thr=0.5,
                    min_pos_iou=0.5,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.6,
                    neg_iou_thr=0.6,
                    min_pos_iou=0.6,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.7,
                    neg_iou_thr=0.7,
                    min_pos_iou=0.7,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False)
        ]),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.0001,
            nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.0001),
            max_per_img=300)
    )
)

train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Resize',
        img_scale=[(800, 800), (1500, 1500)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]

test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=[(800, 800), (1000, 1000), (1200, 1200), (1400, 1400)],
        flip=True,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]

dataset_type = 'CocoDataset'

data = dict(
    samples_per_gpu=5,
    workers_per_gpu=4,
    train=dict(
        type=dataset_type,
        ann_file='data/train/annotations/instances_train2017.json',
        img_prefix='data/train/images',
        pipeline=train_pipeline
    ),
    val=dict(
        type=dataset_type,
        ann_file='data/val/annotations/instances_val2017.json',
        img_prefix='data/val/images',
        pipeline=test_pipeline
    ),
)

evaluation = dict(interval=1, metric='bbox', start=1, classwise=True)    # epoch
optimizer_config = dict(grad_clip=None)
optimizer = dict(type='AdamW', lr=1e-3, betas=(0.9, 0.999), weight_decay=0.05,
                 paramwise_cfg=dict(custom_keys={'absolute_pos_embed': dict(decay_mult=0.),
                                                 'relative_position_bias_table': dict(decay_mult=0.),
                                                 'norm': dict(decay_mult=0.)}))

lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=250,
    warmup_ratio=0.001,
    step=[8, 11])

runner = dict(type='EpochBasedRunner', max_epochs=12)   # EpochBasedRunner、IterBasedRunner 
checkpoint_config = dict(interval=1)
log_config = dict(
    interval=10,     # iter
    hooks=[
        dict(type='TextLoggerHook'),
        #  dict(type='TensorboardLoggerHook')
    ])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
fp16 = dict(loss_scale=512.)    # fp16

```"
Documentation resource on Anchor Generation Faster Rcnn parameters,open-mmlab/mmdetection,2022-04-15 14:27:03,1,,7734,1205620866,"Hi, 

Where can i find resources on understanding the anchor generator arguments and the output in faster rcnn config. The original paper talks simply about sizes and ratios. Here there is scale, strides, ratios, base_size and I am finding it very difficult to understand how its generating anchors. I have spent some 8-10 hours reading the documentation and playing around with the code below, still didn't get it.

```
from mmdet.core import AnchorGenerator
self = AnchorGenerator([1],   #stride
                       [1.0, 0.5], #ratio
                       [8])  #scales
all_anchors = self.grid_anchors([(256, 256)], device='cpu')
print(all_anchors) 
```

Please point me to the right resource to understand how the anchors are calculated in mmdetection and what these parameters are. "
Instance Segmentation result on test image,open-mmlab/mmdetection,2022-04-15 07:06:40,3,,7730,1205328804,"I have trained SCNet model on my custom dataset. 

This code gives good result.
```
import mmcv
img = mmcv.imread('test.jpg')

model.cfg = cfg
result = inference_detector(model, img)
show_result_pyplot(model, img, result)
```

But I want the .pth to be used later. So I decided to load the saved .pth file which was stored after training was completed and used it for inference. 

This is my code to build model based on cfg
```
from mmcv import Config
from mmcv.runner import load_checkpoint

from mmdet.apis import inference_detector, show_result_pyplot
from mmdet.models import build_detector
from mmdet.apis import set_random_seed

cf = Config.fromfile('./configs/scnet/scnet_r50_fpn_20e_coco.py')
# Modify dataset type and path
cf.dataset_type = 'COCODataset'

cf.data.test.ann_file = '../coco_final_test.json'
cf.data.test.img_prefix = '../'
cf.data.test.classes = ('particle',)

cf.data.train.ann_file = '../coco_final_train.json'
cf.data.train.img_prefix = '../'
cf.data.train.classes = ('particle',)


cf.data.val.ann_file = '../coco_final_test.json'
cf.data.val.img_prefix = '../'
cf.data.val.classes = ('particle',)


# modify num classes of the model in box head and mask head
for x in cf.model.roi_head.bbox_head:
    x.num_classes=1
# for x in cfg.model.roi_head.mask_head:
#     x.num_classes=1
# cfg.model.roi_head.bbox_head.num_classes = 1
cf.model.roi_head.mask_head.num_classes = 1

cf.load_from = 'tutorial_exps/epoch_3.pth'


# Set seed thus the results are more reproducible
cf.seed = 0
set_random_seed(0, deterministic=False)
cf.gpu_ids = range(1)

# We can also use tensorboard to log the training process
cf.log_config.hooks = [
    dict(type='TextLoggerHook'),
    dict(type='TensorboardLoggerHook')]
# Build the detector
model1 = build_detector(cf.model)
```

You can see here that I have used this part to load the saved .pth file:
```
cf.load_from = 'tutorial_exps/epoch_3.pth'
```

For inference I have used this code:
```
import mmcv
img = mmcv.imread('test.jpg')

model1.cfg = cf
result = inference_detector(model1, img)
show_result_pyplot(model1, img, result, )
```


Sadly the result was very bad and was not near to that result which was generated by the topmost code. Am I missing something here?"
Model Addition of SOLOv2 and CBNetv2,open-mmlab/mmdetection,2022-04-14 06:39:22,2,,7719,1204101671,"It would be great if mmdetection will supports model like 
- [SOLOv2](https://github.com/WXinlong/SOLO) and 
- [CBNetV2](https://github.com/VDIGPKU/CBNetV2)

Since we already have SOLO, addition of SOLOv2 will not be very difficult and both of the above models are using earlier version of mmdetection
"
# Support Conditional DETR on MMDetection,open-mmlab/mmdetection,2022-04-13 13:35:03,0,Dev-RD,7715,1203312367,"## Steps
- First have a discuss with your teams about the development plan. We can divide the full algorithms into multiple modules and add each modules step by step in different PR.
- In each PR, add a sub-modules of Conditional DETR and add unit tests to test your code
- Benchmark the performance when all modules have all added
- Prepare and release the models


## Resources
https://github.com/Atten4Vis/ConditionalDETR

## Reviewers
@chhluo"
# Support exporting K-Net to ONNX in MMDploy After MMDetection official KNet.,open-mmlab/mmdetection,2022-04-13 13:34:21,0,Dev-RD,7714,1203311510,"## Steps
- Find out what operations in K-Net are not able to export
- Use the rewrite function in MMDeploy to rewrite the K-Net model and make in convertible.
- Align accuracy between the ONNX model and PyTorch model.
- Add unit tests.
- Release this features.

## Resources
https://github.com/ZwwWayne/K-Net/

https://github.com/open-mmlab/mmdeploy

## Reviewers
@chhluo"
# Support Panoptic SegFormer on COCO instance segmentation and panoptic segmentation,open-mmlab/mmdetection,2022-04-13 13:33:10,0,Dev-RD,7713,1203309931,"## Steps
- First have a discuss with your teams about the development plan. We can divide the full algorithms into multiple modules and add each modules step by step in different PR.
- In each PR, add a sub-modules of Panoptic SegFormer and add unit tests to test your code
- Benchmark the performance when all modules have all added
- Prepare and release the models

## Resources
https://github.com/zhiqi-li/Panoptic-SegFormer

## Reviewer
@chhluo"
Video_demo not finishing,open-mmlab/mmdetection,2022-04-12 13:08:05,1,bug,7705,1201825804,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
Video inference does not complete, it stops midway.


**Reproduction**

1. What command or script did you run?
`$ python demo/video_demo.py test_video.mp4 configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco/latest.pth --out mot.mp4`


Output after command:
```(open-mmlab) jz7rt@jz7rt-ub:~/Development/Python/anaconda/open-mmlab/mmdetection
$ python demo/video_demo.py test_video.mp4 
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py 
work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco/latest.pth 
--out mot.mp4 
configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py 
work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco/latest.pth --out mot.mp4

load checkpoint from local path: work_dirs/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco/latest.pth

[                                                  ] 0/1101, elapsed: 0s, ETA
:/home/jz7rt/Development/Python/anaconda/open-mmlab/mmdetection/mmdet/datasets/utils.py:70: 
UserWarning: ""ImageToTensor"" pipeline is replaced by ""DefaultFormatBundle"" for batch inference.
 It is recommended to manually replace it in the test data pipeline in your config file.
  'data pipeline in your config file.', UserWarning)
[>>>>>>>>>>>>>>>>>>>>>>>>>>>>                      ] 636/1101, 3.7 task/s, elapsed: 174s, ETA:   127sKilled
```
"
"When i run one class_num instances segment by queryinst,I get poor effect.",open-mmlab/mmdetection,2022-04-11 14:17:05,1,,7694,1200043555,"My config is that.
Do I need to modify more configuration to train one class of instance segmentation?

```
# 这个新的配置文件继承自一个原始配置文件，只需要突出必要的修改部分即可
_base_ = '../configs/queryinst/queryinst_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco.py'

# _delete_=True
# 我们需要对头中的类别数量进行修改来匹配数据集的标注
num_stages = 6

model = dict(    
    roi_head=dict(
#         type='SparseRoIHead',
#         num_stages=num_stages,
#         stage_loss_weights=[1] * num_stages,
#         proposal_feature_channel=256,
#         bbox_roi_extractor=dict(
#             type='SingleRoIExtractor',
#             roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),
#             out_channels=256,
#             featmap_strides=[4, 8, 16, 32]),
#         mask_roi_extractor=dict(
#             type='SingleRoIExtractor',
#             roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=2),
#             out_channels=256,
#             featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='DIIHead',
                num_classes=1,
                num_ffn_fcs=2,
                num_heads=8,
                num_cls_fcs=1,
                num_reg_fcs=3,
                feedforward_channels=2048,
                in_channels=256,
                dropout=0.0,
                ffn_act_cfg=dict(type='ReLU', inplace=True),
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=7,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                loss_bbox=dict(type='L1Loss', loss_weight=5.0),
                loss_iou=dict(type='GIoULoss', loss_weight=2.0),
                loss_cls=dict(
                    type='FocalLoss',
                    use_sigmoid=True,
                    gamma=2.0,
                    alpha=0.25,
                    loss_weight=2.0),
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    clip_border=False,
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.5, 0.5, 1., 1.])) for _ in range(num_stages)
        ],
        mask_head=[
            dict(
                type='DynamicMaskHead',
                dynamic_conv_cfg=dict(
                    type='DynamicConv',
                    in_channels=256,
                    feat_channels=64,
                    out_channels=256,
                    input_feat_shape=14,
                    with_proj=False,
                    act_cfg=dict(type='ReLU', inplace=True),
                    norm_cfg=dict(type='LN')),
                num_convs=4,
                num_classes=1,
                roi_feat_size=14,
                in_channels=256,
                conv_kernel_size=3,
                conv_out_channels=256,
                class_agnostic=False,
                norm_cfg=dict(type='BN'),
                upsample_cfg=dict(type='deconv', scale_factor=2),
                loss_mask=dict(
                    type='DiceLoss',
                    loss_weight=8.0,
                    use_sigmoid=True,
                    activate=False,
                    eps=1e-5)) for _ in range(num_stages)
        ]))


runner = runner = dict(type='EpochBasedRunner', max_epochs=20)

# 修改数据集相关设置
dataset_type = 'CocoDataset'
classes = ('building',)


data = dict(
    samples_per_gpu=1,
    workers_per_gpu=1,
    train=dict(
        img_prefix='/data/qinhaobo/proxyrecon_dataset/proxyrecon/suit_for_qingdao_coco',
        classes=classes,
        ann_file='/data/qinhaobo/proxyrecon_dataset/proxyrecon/suit_for_qingdao_coco/annotations.json'),
    val=dict(
        img_prefix='/data/qinhaobo/proxyrecon_dataset/proxyrecon/all_instance_val_coco',
        classes=classes,
        ann_file='/data/qinhaobo/proxyrecon_dataset/proxyrecon/all_instance_val_coco/annotations.json'),
    test=dict(
        img_prefix='/data/qinhaobo/proxyrecon_dataset/proxyrecon/all_instance_val_coco',
        classes=classes,
        ann_file='/data/qinhaobo/proxyrecon_dataset/proxyrecon/all_instance_val_coco/annotations.json'))

# 我们可以使用预训练的 Mask R-CNN 来获取更好的性能
load_from = 'checkpoints/queryinst/queryinst_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco_20210904_101802-85cffbd8.pth'
```"
guassianBlur,open-mmlab/mmdetection,2022-04-09 16:13:45,2,,7684,1198701934,"Hi,

would like to check how can i incorporate GaussianBlur in the data augmentation?  i am using mmsegmentation_tutorial.py to check for some data augmentation technique.  when i add  
dict(type='GaussianBlur',kernel_size=(5,9),sigma=(0.1,5)),  to the train_pipeline and data.train i encountered this error:
KeyError: ""StanfordBackgroundDataset: 'GaussianBlur is not in the pipeline registry'""

kindly advise."
Fail to allocate bitmap,open-mmlab/mmdetection,2022-04-09 12:02:30,0,,7681,1198555144,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

Fail to allocate bitmap

**Reproduction**

1. What command or script did you run?
python demo/video_demo.py configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth --file demo/ligang/1.mp4  --out demo/ligang/result.mp4

```none
A placeholder for the command.
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?  
NO.                                                                                
3. What dataset did you use?
COCO2014
**Environment**
addict	2.4.0	
albumentations	1.1.0	
appdirs	1.4.4	
asynctest	0.13.0	
atomicwrites	1.4.0	
attrs	21.4.0	
blas	2.113	
blas-devel	3.9.0	
ca-certificates	2021.10.8	
certifi	2021.10.8	
charset-normalizer	2.0.12	
cityscapesscripts	2.2.0	
click	8.1.1	
codecov	2.1.12	
colorama	0.4.4	
coloredlogs	15.0.1	
coverage	6.3.2	
cudatoolkit	11.1.1	
cycler	0.11.0	
cython	0.29.28	
flake8	4.0.1	
flatbuffers	2.0	
fonttools	4.31.2	
freetype	2.10.4	
humanfriendly	10.0	
idna	3.3	
imagecorruptions	1.1.2	
imageio	2.16.1	
importlib-resources	5.6.0	
iniconfig	1.1.1	
intel-openmp	2022.0.0	
interrogate	1.5.0	
isort	4.3.21	
jaraco-classes	3.2.1	
jaraco-collections	3.5.1	
jaraco-context	4.1.1	
jaraco-functools	3.5.0	
jaraco-structures	2.1.0	
jaraco-text	3.7.0	
jaraco-ui	2.3.0	
jaraco-windows	5.7.0	
joblib	1.1.0	
jpeg	9b	
kiwisolver	1.4.2	
kwarray	0.6.0	
libblas	3.9.0	
libcblas	3.9.0	
liblapack	3.9.0	
liblapacke	3.9.0	
libpng	1.6.37	
libtiff	4.2.0	
libuv	1.43.0	
libwebp	1.2.2	
libwebp-base	1.2.2	
libzlib	1.2.11	
lmdb	1.3.0	
lz4-c	1.9.3	
m2w64-gcc-libgfortran	5.3.0	
m2w64-gcc-libs	5.3.0	
m2w64-gcc-libs-core	5.3.0	
m2w64-gmp	6.1.0	
m2w64-libwinpthread-git	5.0.0.4634.697f757	
matplotlib	3.5.1	
mccabe	0.6.1	
mkl	2022.0.0	
mkl-devel	2022.0.0	
mkl-include	2022.0.0	
mmcv-full	1.4.8	
mmdet	2.23.0	
mmlvis	10.5.3	
mmpycocotools	12.0.3	
more-itertools	8.12.0	
msys2-conda-epoch	20160418	
networkx	2.7.1	
ninja	1.10.2.3	
numpy	1.22.3	
onnx	1.7.0	
onnxoptimizer	0.2.6	
onnxruntime	1.11.0	
opencv-contrib-python	4.5.5.64	
opencv-python	4.5.5.64	
opencv-python-headless	4.5.5.64	
openssl	1.1.1n	
ordered-set	4.1.0	
packaging	21.3	
path	16.4.0	
pillow	9.0.1	
pip	21.2.2	
pluggy	1.0.0	
protobuf	3.19.4	
py	1.11.0	
pycocotools	2.0.4	
pycodestyle	2.8.0	
pyflakes	2.4.0	
pyparsing	3.0.7	
pyquaternion	0.9.9	
pyreadline3	3.4.1	
pytest	7.1.1	
pytest-runner	6.0.0	
python	3.8.13	
python-dateutil	2.8.2	
python_abi	3.8	
pytorch	1.8.0	
pyturbojpeg	1.6.6	
pywavelets	1.3.0	
pyyaml	6.0	
qudida	0.0.4	
regex	2022.3.15	
requests	2.27.1	
scikit-image	0.19.2	
scikit-learn	1.0.2	
scipy	1.8.0	
setuptools	58.0.4	
six	1.16.0	
sqlite	3.38.0	
tabulate	0.8.9	
tbb	2021.5.0	
terminaltables	3.1.10	
threadpoolctl	3.1.0	
tifffile	2022.3.25	
tk	8.6.12	
toml	0.10.2	
tomli	2.0.1	
torchvision	0.9.0	
tqdm	4.63.1	
typing	3.7.4.3	
typing_extensions	4.1.1	
ubelt	1.0.1	
urllib3	1.26.9	
vc	14.2	
vs2015_runtime	14.27.29016	
wheel	0.37.1	
wincertstore	0.2	
xdoctest	1.0.0	
xz	5.2.5	
yapf	0.32.0	
zipp	3.7.0	
zlib	1.2.11	
1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
sys.platform: win32
Python: 3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1
NVCC: Not Available
GCC: n/a
PyTorch: 1.8.0
PyTorch compiling details: PyTorch built with:
  - C++ Version: 199711
  - MSVC 192829337
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 2019
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_7
5,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL
 -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_XNNPACK, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, US
E_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON,

TorchVision: 0.9.0
OpenCV: 4.5.5
MMCV: 1.4.8
MMCV Compiler: MSVC 192930141
MMCV CUDA Compiler: 11.1
MMDetection: 2.23.0+

2. You may add addition that may be helpful for locating the problem, such as
    - How you installed PyTorch [e.g., pip, conda, source]

I configure mmdetection on win10 and mmcv-full compiles on VS2019 for myself.

    - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)
C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\MSVC\14.29.30133\bin\Hostx86\x64
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin

**Error traceback**
If applicable, paste the error trackback here.

```none
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
TensorRT deploy error,open-mmlab/mmdetection,2022-04-07 12:37:40,5,deployment,7657,1195986967,"**Environment**

sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0: Tesla T4
CUDA_HOME: /usr/local/cuda-10.2
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (GCC) 5.4.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.5 Product Build 20190808 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.6.0
OpenCV: 4.5.5
MMCV: 1.3.13
MMCV Compiler: GCC 5.4
MMCV CUDA Compiler: 10.2
MMDetection: 2.11.0+b7077d5
onnx                   1.7.0
onnx-graphsurgeon      0.2.6
onnx-simplifier        0.3.7
onnxoptimizer          0.2.6
onnxruntime            1.8.1

PyTorch by pip

**Error traceback**
[TensorRT] VERBOSE: ModelImporter.cpp:125: Equal_370 [Equal] inputs: [706 -> (-1, -1)], [707 -> ()], 
Traceback (most recent call last):
  File ""tools/deployment/onnx2tensorrt_ori.py"", line 255, in <module>
    verbose=args.verbose)
  File ""tools/deployment/onnx2tensorrt_ori.py"", line 46, in onnx2tensorrt
    max_workspace_size=max_workspace_size)
  File ""/usr/local/app/sumeru_coa/ft_local/OCR_Universal/mmcv/mmcv/tensorrt/tensorrt_utils.py"", line 63, in onnx2trt
    raise RuntimeError(f'parse onnx failed:\n{error_msgs}')
RuntimeError: parse onnx failed:
In node -1 (elementwiseHelper): UNSUPPORTED_NODE: Assertion failed: elementwiseCheck(inputs, binary_op)

It seem like ""elementwiseCheck"" not supported by tensorrt. Any method can avoid this problem?
"
"Transfer Learning bewteen models with different dataset, partially same in classes. ",open-mmlab/mmdetection,2022-04-07 10:53:24,3,community help wanted#community discussion,7655,1195871803,"Hello , 
I would like to ask regard transfer learning between models with same structure , but with different dataset. 

for more clarifications , I have two datasets , one is huger than the another (by the term of the number of images as well as classes) 
the first dataset has three classes [A,B,C]  ,while the another dataset has [A,B,D]
I trained well my model in detecting the classes in the first huge dataset , and now i want to do transfer learning to the another dataset which has one different class (D). 
I already trained the same model on this class (D) ,with another middle size dataset. 
and instead of repeating the training again ,I only want to make the same model being able to detect the four classes [A,B,C,D] by doing transfer learning. 

so you can see that i have the same model trained once on detecting class D , and then to detect the classes [A,B,C]. and i want a way to combine  that in one general model and use it for detecting the four classes. 

<img width=""265"" alt=""transfer_learning"" src=""https://user-images.githubusercontent.com/10245810/162183692-58021e89-42bb-4f19-a28c-13fa7716c468.png"">


taking into consideration , that my 2nd (smaller)  dataset is originally part form the middle size dataset that used for training only on class D. 
Is there any way to do so? "
How to use command python tools/train.py configs/xxxx.py,open-mmlab/mmdetection,2022-03-31 07:46:00,5,,7597,1187655890,"Dear mmdetection users, I am used to use mmdetection v1.0.0 and now I am struggling to use mmdetection v2.x.x. In the old version, I can use the command python tools/train.py configs/xxxx.py to train my model. In newer version, I find the implementation codes have become more complicated. Can I use the old command to train a model? Thank you very much."
Looking forward exporting Mask2Former to ONNX,open-mmlab/mmdetection,2022-03-31 06:36:21,2,ONNX#deployment,7594,1187587580,"**Describe the feature**

if mask2former can be exported to  onnx，that will be absolutely wonderfully !!! 



"
IndexError: CocoDataset: list index out of range,open-mmlab/mmdetection,2022-03-28 13:01:24,2,,7556,1183406275,"when i try to train my coco dataset with fast_rcnn,  and error ,  I also change  the num-classes.  and i use yolo to train successfully.
Traceback (most recent call last):
  File ""tools\train.py"", line 209, in <module>
    main()
  File ""tools\train.py"", line 185, in main
    datasets = [build_dataset(cfg.data.train)]
  File ""C:\Users\mukai\anaconda3\envs\open-mmlab\lib\site-packages\mmdet\datasets\builder.py"", line 81, in build_dataset    dataset = build_from_cfg(cfg, DATASETS, default_args)
  File ""C:\Users\mukai\anaconda3\envs\open-mmlab\lib\site-packages\mmcv\utils\registry.py"", line 55, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
IndexError: CocoDataset: list index out of range"
[Feature Request] TorchData example,open-mmlab/mmdetection,2022-03-27 14:14:54,1,feature request,7546,1182543825,"**Describe the feature**

Hi, I know that TorchData is very new, but it would be awesome to see an example of how to create a custom iterable-style datapipe with TorchData for use with this library.

**Motivation**
Iterable-style datasets can be faster and are ideal when all annotations do not fit into memory. TorchData is also going to gain significant traction, so it'd be nice to get ahead of that now.

**Related resources**
https://pytorch.org/data/beta/index.html

**Additional context**
Example:

```python
from torchvision.prototype.datasets.utils import DatasetConfig
from torchvision.prototype import datasets as new_datasets
from torch.utils.data import DataLoader

ds = new_datasets._api.find('coco')
config = DatasetConfig(split='val', year='2017', annotations='instances')
ds = ds.load('./root', config=config)

dataloader = DataLoader(dataset=datapipe, batch_size=1, shuffle=True)

batch = next(iter(dataloader))
print(batch.keys())
```"
[Feature Request] Lightning support,open-mmlab/mmdetection,2022-03-25 11:47:37,4,community discussion#feature request,7534,1180699154,"**Describe the feature**

I'd love it if there was a `train_lightning.py` script for training with pytorch lightning in the tools folder.

**Motivation**

Lightning makes it easier to swap models, datamodules, and callbacks across a range of model types. I'd like to use mmdetection over detectron2, but [detectron2 supports training with lightning](https://github.com/facebookresearch/detectron2/blob/main/tools/lightning_train_net.py). I think this is possible with mmdetection and icevision, but it could be easier to just build support directly.  

**Related resources**
https://github.com/sathishkumark27/mmdetection-icevision"
QueryInst with FP16  RuntimeError: transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered,open-mmlab/mmdetection,2022-03-24 18:39:42,5,fp16,7528,1179903956,"
1. What command or script did you run?

```
python tools/train.py configs/queryinst/queryinst_r50_fpn_1x_coco.py 
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
   Yes, added `fp16 = dict(loss_scale=512.)`
4. What dataset did you use? COCO

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
```
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1: GeForce RTX 2080 SUPER
CUDA_HOME: /usr/local/cuda-10.2
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.11.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.5.2 (Git Hash a9302535553c73243c632ad3c4c80beec3d19a1e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.11.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.12.0
OpenCV: 4.5.5
MMCV: 1.4.7
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.22.0+6b87ac2
```



"
auto_resume support ceph checkpoint,open-mmlab/mmdetection,2022-03-24 12:24:32,1,enhancement,7526,1179444500,"**Describe the feature**

**Motivation**
The auto_resume only supports local checkpoint. It is more convenient to support ceph checkpoint, to incorporate with the writing checkpoint to ceph  function.
"
Will open some lane det model ?,open-mmlab/mmdetection,2022-03-24 10:01:46,1,feature request,7524,1179278183,
Will add Keypoint-RCNN like detectron2 ? Thanks,open-mmlab/mmdetection,2022-03-24 02:54:07,1,community help wanted#feature request,7518,1178876103,
"pls help me, i can't train the model on custom dataset.",open-mmlab/mmdetection,2022-03-21 19:35:02,2,,7488,1175852321,"Hello, I tried to change almost all the configuration parameters, but nothing helps. The training always stops at 200 iterations, I've spent over 100 hours trying to figure out why. 1400 photos in the train and 450 for validation.
I adapted my custom dataset
I adapted the structure of my dataset to the Kitty_tiny dataset, as shown in the DEMO in mmdetection, in order to train the model exactly as shown in the DEMO
MODEL - SSD300
The training always stops at 200 iterations, i don't know why((((((



tmp/ipykernel_3063/3895221999.py:58: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  labels=np.array(gt_labels, dtype=np.long),
/tmp/ipykernel_3063/3895221999.py:61: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  labels_ignore=np.array(gt_labels_ignore, dtype=np.long))
/home/ivan/Рабочий стол/mmdetection/mmdet/datasets/custom.py:179: UserWarning: CustomDataset does not support filtering empty gt images.
  warnings.warn(
2022-03-21 21:46:05,961 - mmdet - INFO - load checkpoint from local path: checkpoints/test.pth
2022-03-21 21:46:07,965 - mmdet - WARNING - The model and loaded state dict do not match exactly

size mismatch for bbox_head.cls_convs.0.0.weight: copying a param with shape torch.Size([324, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 512, 3, 3]).
size mismatch for bbox_head.cls_convs.0.0.bias: copying a param with shape torch.Size([324]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for bbox_head.cls_convs.1.0.weight: copying a param with shape torch.Size([486, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 1024, 3, 3]).
size mismatch for bbox_head.cls_convs.1.0.bias: copying a param with shape torch.Size([486]) from checkpoint, the shape in current model is torch.Size([96]).
size mismatch for bbox_head.cls_convs.2.0.weight: copying a param with shape torch.Size([486, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 512, 3, 3]).
size mismatch for bbox_head.cls_convs.2.0.bias: copying a param with shape torch.Size([486]) from checkpoint, the shape in current model is torch.Size([96]).
size mismatch for bbox_head.cls_convs.3.0.weight: copying a param with shape torch.Size([486, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 256, 3, 3]).
size mismatch for bbox_head.cls_convs.3.0.bias: copying a param with shape torch.Size([486]) from checkpoint, the shape in current model is torch.Size([96]).
size mismatch for bbox_head.cls_convs.4.0.weight: copying a param with shape torch.Size([324, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 3, 3]).
size mismatch for bbox_head.cls_convs.4.0.bias: copying a param with shape torch.Size([324]) from checkpoint, the shape in current model is torch.Size([64]).
size mismatch for bbox_head.cls_convs.5.0.weight: copying a param with shape torch.Size([324, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 3, 3]).
size mismatch for bbox_head.cls_convs.5.0.bias: copying a param with shape torch.Size([324]) from checkpoint, the shape in current model is torch.Size([64]).
2022-03-21 21:46:07,966 - mmdet - INFO - Start running, host: ivan@ivan-GL73-8RC, work_dir: /home/ivan/Рабочий стол/mmdetection/tutorial_exps
2022-03-21 21:46:07,966 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) CheckInvalidLossHook               
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-03-21 21:46:07,967 - mmdet - INFO - workflow: [('train', 1)], max: 24 epochs
2022-03-21 21:46:07,967 - mmdet - INFO - Checkpoints will be saved to /home/ivan/Рабочий стол/mmdetection/tutorial_exps by HardDiskBackend.
/home/ivan/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
2022-03-21 21:46:36,019 - mmdet - INFO - Epoch [1][10/881]	lr: 2.500e-03, eta: 15:59:09, time: 2.723, data_time: 0.817, memory: 1782, loss_cls: 9.9198, loss_bbox: 5.8576, loss: 15.7773
2022-03-21 21:46:53,797 - mmdet - INFO - Epoch [1][20/881]	lr: 2.500e-03, eta: 13:15:12, time: 1.794, data_time: 1.053, memory: 1782, loss_cls: 6.9702, loss_bbox: 7.9114, loss: 14.8815
2022-03-21 21:47:13,140 - mmdet - INFO - Epoch [1][30/881]	lr: 2.500e-03, eta: 12:37:01, time: 1.936, data_time: 1.203, memory: 1782, loss_cls: 6.8268, loss_bbox: 7.8771, loss: 14.7039
2022-03-21 21:47:43,220 - mmdet - INFO - Epoch [1][40/881]	lr: 2.500e-03, eta: 13:43:44, time: 2.914, data_time: 2.131, memory: 1782, loss_cls: 5.4283, loss_bbox: 7.3805, loss: 12.8088
2022-03-21 21:48:02,945 - mmdet - INFO - Epoch [1][50/881]	lr: 2.500e-03, eta: 13:24:00, time: 2.067, data_time: 1.381, memory: 1782, loss_cls: 5.2074, loss_bbox: 7.2640, loss: 12.4714
2022-03-21 21:48:19,138 - mmdet - INFO - Epoch [1][60/881]	lr: 2.500e-03, eta: 12:44:30, time: 1.619, data_time: 0.859, memory: 1782, loss_cls: 5.2744, loss_bbox: 7.4937, loss: 12.7681
2022-03-21 21:48:36,040 - mmdet - INFO - Epoch [1][70/881]	lr: 2.500e-03, eta: 12:19:47, time: 1.690, data_time: 1.008, memory: 1782, loss_cls: 5.6761, loss_bbox: 7.8002, loss: 13.4763
2022-03-21 21:48:55,586 - mmdet - INFO - Epoch [1][80/881]	lr: 2.500e-03, eta: 12:12:47, time: 1.955, data_time: 1.255, memory: 1782, loss_cls: 5.1228, loss_bbox: 8.7396, loss: 13.8623
2022-03-21 21:49:16,196 - mmdet - INFO - Epoch [1][90/881]	lr: 2.500e-03, eta: 12:11:24, time: 2.060, data_time: 1.383, memory: 1782, loss_cls: 5.6127, loss_bbox: 6.9650, loss: 12.5777

2022-03-21 21:49:29,004 - mmdet - INFO - Epoch [1][100/881]	lr: 2.500e-03, eta: 11:42:51, time: 1.281, data_time: 0.631, memory: 1782, loss_cls: 5.2070, loss_bbox: 6.3978, loss: 11.6048
2022-03-21 21:49:45,604 - mmdet - INFO - Epoch [1][110/881]	lr: 2.500e-03, eta: 11:31:33, time: 1.660, data_time: 0.957, memory: 1782, loss_cls: 5.5467, loss_bbox: 6.4412, loss: 11.9879
2022-03-21 21:49:58,321 - mmdet - INFO - Epoch [1][120/881]	lr: 2.500e-03, eta: 11:10:45, time: 1.272, data_time: 0.534, memory: 1782, loss_cls: 5.1328, loss_bbox: 5.7493, loss: 10.8820
2022-03-21 21:50:16,054 - mmdet - INFO - Epoch [1][130/881]	lr: 2.500e-03, eta: 11:06:39, time: 1.774, data_time: 1.048, memory: 1782, loss_cls: 4.5745, loss_bbox: 5.6167, loss: 10.1911
2022-03-21 21:50:31,525 - mmdet - INFO - Epoch [1][140/881]	lr: 2.500e-03, eta: 10:57:24, time: 1.547, data_time: 0.810, memory: 1782, loss_cls: 4.9017, loss_bbox: 6.4228, loss: 11.3245
2022-03-21 21:50:45,390 - mmdet - INFO - Epoch [1][150/881]	lr: 2.500e-03, eta: 10:45:37, time: 1.386, data_time: 0.842, memory: 1782, loss_cls: 4.7573, loss_bbox: 5.2906, loss: 10.0479
2022-03-21 21:51:03,105 - mmdet - INFO - Epoch [1][160/881]	lr: 2.500e-03, eta: 10:43:42, time: 1.771, data_time: 1.122, memory: 1782, loss_cls: 4.5290, loss_bbox: 5.5199, loss: 10.0489
2022-03-21 21:51:23,782 - mmdet - INFO - Epoch [1][170/881]	lr: 2.500e-03, eta: 10:47:59, time: 2.064, data_time: 1.287, memory: 1782, loss_cls: 4.8839, loss_bbox: 5.0466, loss: 9.9305
2022-03-21 21:51:40,147 - mmdet - INFO - Epoch [1][180/881]	lr: 2.500e-03, eta: 10:43:26, time: 1.636, data_time: 1.056, memory: 1782, loss_cls: 4.4631, loss_bbox: 5.1512, loss: 9.6143
2022-03-21 21:51:57,418 - mmdet - INFO - Epoch [1][190/881]	lr: 2.500e-03, eta: 10:41:08, time: 1.733, data_time: 1.045, memory: 1782, loss_cls: 4.4632, loss_bbox: 5.4469, loss: 9.9100
2022-03-21 21:52:14,014 - mmdet - INFO - Epoch [1][200/881]	lr: 2.500e-03, eta: 10:37:46, time: 1.661, data_time: 0.895, memory: 1782, loss_cls: nan, loss_bbox: nan, loss: nan
2022-03-21 21:52:14,738 - mmdet - INFO - loss become infinite or NaN!



AssertionError                            Traceback (most recent call last)
/tmp/ipykernel_3063/79739429.py in <module>
     23 # Create work_diri
     24 mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))
---> 25 train_detector(model, datasets, cfg, distributed=False, validate=True)
     26 

~/Рабочий стол/mmdetection/mmdet/apis/train.py in train_detector(model, dataset, cfg, distributed, validate, timestamp, meta)
    206     elif cfg.load_from:
    207         runner.load_checkpoint(cfg.load_from)
--> 208     runner.run(data_loaders, cfg.workflow)

~/anaconda3/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py in run(self, data_loaders, workflow, max_epochs, **kwargs)
    125                     if mode == 'train' and self.epoch >= self._max_epochs:
    126                         break
--> 127                     epoch_runner(data_loaders[i], **kwargs)
    128 
    129         time.sleep(1)  # wait for some hooks like loggers to finish

~/anaconda3/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py in train(self, data_loader, **kwargs)
     49             self.call_hook('before_train_iter')
     50             self.run_iter(data_batch, train_mode=True, **kwargs)
---> 51             self.call_hook('after_train_iter')
     52             self._iter += 1
     53 

~/anaconda3/lib/python3.9/site-packages/mmcv/runner/base_runner.py in call_hook(self, fn_name)
    307         """"""
    308         for hook in self._hooks:
--> 309             getattr(hook, fn_name)(self)
    310 
    311     def get_hook_info(self):

~/Рабочий стол/mmdetection/mmdet/core/hook/checkloss_hook.py in after_train_iter(self, runner)
     21     def after_train_iter(self, runner):
     22         if self.every_n_iters(runner, self.interval):
---> 23             assert torch.isfinite(runner.outputs['loss']), \
     24                 runner.logger.info('loss become infinite or NaN!')




Config:
input_size = 300
model = dict(
    type='SingleStageDetector',
    backbone=dict(
        type='SSDVGG',
        depth=16,
        with_last_pool=False,
        ceil_mode=True,
        out_indices=(3, 4),
        out_feature_indices=(22, 34),
        init_cfg=dict(
            type='Pretrained', checkpoint='open-mmlab://vgg16_caffe')),
    neck=dict(
        type='SSDNeck',
        in_channels=(512, 1024),
        out_channels=(512, 1024, 512, 256, 256, 256),
        level_strides=(2, 2, 1, 1),
        level_paddings=(1, 1, 0, 0),
        l2_norm_scale=20),
    bbox_head=dict(
        type='SSDHead',
        in_channels=(512, 1024, 512, 256, 256, 256),
        num_classes=15,
        anchor_generator=dict(
            type='SSDAnchorGenerator',
            scale_major=False,
            input_size=300,
            basesize_ratio_range=(0.15, 0.9),
            strides=[8, 16, 32, 64, 100, 300],
            ratios=[[2], [2, 3], [2, 3], [2, 3], [2], [2]]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[0.1, 0.1, 0.2, 0.2])),
    train_cfg=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.5,
            neg_iou_thr=0.5,
            min_pos_iou=0.0,
            ignore_iof_thr=-1,
            gt_max_assign_all=False),
        smoothl1_beta=1.0,
        allowed_border=-1,
        pos_weight=-1,
        neg_pos_ratio=3,
        debug=False),
    test_cfg=dict(
        nms_pre=1000,
        nms=dict(type='nms', iou_threshold=0.45),
        min_bbox_size=0,
        score_thr=0.02,
        max_per_img=200))
cudnn_benchmark = True
dataset_type = 'DOTA'
data_root = 'AdaptedDataset/'
img_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Expand',
        mean=[123.675, 116.28, 103.53],
        to_rgb=True,
        ratio_range=(1, 4)),
    dict(
        type='MinIoURandomCrop',
        min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
        min_crop_size=0.3),
    dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='PhotoMetricDistortion',
        brightness_delta=32,
        contrast_range=(0.5, 1.5),
        saturation_range=(0.5, 1.5),
        hue_delta=18),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[1, 1, 1],
        to_rgb=True),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(300, 300),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=False),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[1, 1, 1],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=3,
    train=dict(
        type='RepeatDataset',
        times=5,
        dataset=dict(
            type='DOTA',
            ann_file='train.txt',
            img_prefix='training/image_2',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True),
                dict(
                    type='Expand',
                    mean=[123.675, 116.28, 103.53],
                    to_rgb=True,
                    ratio_range=(1, 4)),
                dict(
                    type='MinIoURandomCrop',
                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9),
                    min_crop_size=0.3),
                dict(type='Resize', img_scale=(300, 300), keep_ratio=False),
                dict(type='RandomFlip', flip_ratio=0.5),
                dict(
                    type='PhotoMetricDistortion',
                    brightness_delta=32,
                    contrast_range=(0.5, 1.5),
                    saturation_range=(0.5, 1.5),
                    hue_delta=18),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[1, 1, 1],
                    to_rgb=True),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
            ],
            data_root='AdaptedDataset/')),
    val=dict(
        type='DOTA',
        ann_file='val.txt',
        img_prefix='training/image_2',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(300, 300),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='AdaptedDataset/'),
    test=dict(
        type='DOTA',
        ann_file='train.txt',
        img_prefix='training/image_2',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(300, 300),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=False),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[1, 1, 1],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='AdaptedDataset/'))
evaluation = dict(interval=1, metric='mAP')
optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0005)
optimizer_config = dict()
lr_config = dict(
    policy='step',
    warmup=None,
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[16, 22])
runner = dict(type='EpochBasedRunner', max_epochs=24)
checkpoint_config = dict(interval=1)
log_config = dict(interval=10, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [
    dict(type='NumClassCheckHook'),
    dict(type='CheckInvalidLossHook', interval=50, priority='VERY_LOW')
]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'checkpoints/test.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
work_dir = 'tutorial_exps'
seed = 0
gpu_ids = range(0, 1)









annotations:


import copy
import os.path as osp

import mmcv
import numpy as np

from mmdet.datasets.builder import DATASETS
from mmdet.datasets.custom import CustomDataset

@DATASETS.register_module()
class DOTA(CustomDataset):

    CLASSES = ('ship','small-vehicle','large-vehicle','plane','harbor','storage-tank','tennis-court','bridge',
               'swimming-pool','helicopter','basketball-court','baseball-diamond','roundabout','soccer-ball-field',
               'ground-track-field')

    def load_annotations(self, ann_file):
        cat2label = {k: i for i, k in enumerate(self.CLASSES)}
        # load image list from file
        image_list = mmcv.list_from_file(self.ann_file)
    
        data_infos = []
        # convert annotations to middle format
        for image_id in image_list:
            filename = f'{self.img_prefix}/{image_id}.png'
            image = mmcv.imread(filename)
            height, width = image.shape[:2]
    
            data_info = dict(filename=f'{image_id}.png', width=width, height=height)
    
            # load annotations
            label_prefix = self.img_prefix.replace('image_2', 'label_2')
            lines = mmcv.list_from_file(osp.join(label_prefix, f'{image_id}.txt'))
    
            content = [line.strip().split(' ') for line in lines]
            bbox_names = [x[8] for x in content]
            bboxes = [[float(info) for info in (x[0:2] + x[4:6])] for x in content]
#[:n] + l[n:]
            gt_bboxes = []
            gt_labels = []
            gt_bboxes_ignore = []
            gt_labels_ignore = []
    
            # filter 'DontCare'
            for bbox_name, bbox in zip(bbox_names, bboxes):
                if bbox_name in cat2label:
                    gt_labels.append(cat2label[bbox_name])
                    gt_bboxes.append(bbox)
                else:
                    gt_labels_ignore.append(-1)
                    gt_bboxes_ignore.append(bbox)

            data_anno = dict(
                bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),
                labels=np.array(gt_labels, dtype=np.long),
                bboxes_ignore=np.array(gt_bboxes_ignore,
                                       dtype=np.float32).reshape(-1, 4),
                labels_ignore=np.array(gt_labels_ignore, dtype=np.long))

            data_info.update(ann=data_anno)
            data_infos.append(data_info)

        return data_infos


"
"about resume training, loss is large as initial",open-mmlab/mmdetection,2022-03-21 16:47:46,2,,7487,1175657469,"I use 'resume_from = xxx.pth ' to continue training, and find the loss is as larger as initial, mAP also decrease. How to solve it ?  Many thanks!"
About analysis_logs,open-mmlab/mmdetection,2022-03-21 16:45:22,2,,7486,1175654333,"Hello. I want to use analysis_logs.py to analyse the json files. However, my training stopped twice and it generates 3 json files. Are there any methods to analyse the overall results based on the 3 json files?"
About Evaluation.metric,open-mmlab/mmdetection,2022-03-20 01:29:09,2,Doc,7465,1174412462,"What are the indicators or results in metric?
and different models support different metric?
-------------------------------------------------------
metric中的指标有哪些？
不同模型支持的metric是否不同？"
How can i get the .pkl file to use on DetVisGUI。,open-mmlab/mmdetection,2022-03-18 11:51:53,1,,7455,1173503425,i follow the demo of instance segmentation colab to train solo on cocodataset.But there is no file i can use on DetVisGUI.
Swin V2,open-mmlab/mmdetection,2022-03-18 07:19:39,2,enhancement#feature request,7453,1173261228,How can i use swin-transformer-v2 as a backbone?
Very poor performance on custom dataset with partly overlapping instances,open-mmlab/mmdetection,2022-03-17 10:27:07,4,,7445,1172195106,"Hi,

I trained different default model configurations (mask_rcnn, cascade) with different backbones (swin, r50, x101)  for a custom COCO formatted dataset. The dataset contains only one class with up to 100 instances per image, but most of the instances have a very small overlapping region with one or more other instances.

The results are very strange for all tested models because only a fraction of the instances got recognized but the accuracy on the few detected instances is actually very good.

Using Detectron2 I got much better results on the very same dataset, but I would like to use mmdetection.

Do you have any suggestions what could be the reason for this behavior?

Thank you for your time,
Mathias


"
Slow Training Speed On LVIS_V1 DataSet.,open-mmlab/mmdetection,2022-03-17 06:41:02,2,,7439,1171991730,"In the beginning, all times (**_time_** and **_data_time_**) looks OK. 
While, at a certain point of Epoch 1, the training speed gradually slow down ( especially the _**data_time**_). 
I'm sure that there were no other operations during training, and i have reproduced this issue many times.

I have trained Mask RCNN on COCO Dataset, but did not reproduce this Issue. It seems that this Issue only exists in Lvis V1 DataSet.

**Detailed LOG**

2022-03-17 11:56:00,123 - mmdet - INFO - Epoch [1][250/7674]    lr: 9.970e-03, eta: 10:43:36, time: 0.441, data_time: 0.089, memory: 5023, loss_rpn_cls: 0.2425, loss_rpn_bbox: 
2022-03-17 11:56:21,294 - mmdet - INFO - Epoch [1][300/7674]    lr: 1.197e-02, eta: 10:44:01, time: 0.424, data_time: 0.059, memory: 5210, loss_rpn_cls: 0.2138, loss_rpn_bbox: 
2022-03-17 11:56:42,652 - mmdet - INFO - Epoch [1][350/7674]    lr: 1.397e-02, eta: 10:45:00, time: 0.427, data_time: 0.048, memory: 5210, loss_rpn_cls: 0.1998, loss_rpn_bbox: 
2022-03-17 11:57:05,258 - mmdet - INFO - Epoch [1][400/7674]    lr: 1.596e-02, eta: 10:50:31, time: 0.453, data_time: 0.047, memory: 6086, loss_rpn_cls: 0.1857, loss_rpn_bbox: 
2022-03-17 11:57:26,114 - mmdet - INFO - Epoch [1][450/7674]    lr: 1.796e-02, eta: 10:48:38, time: 0.417, data_time: 0.060, memory: 6086, loss_rpn_cls: 0.1690, loss_rpn_bbox: 
2022-03-17 11:57:46,272 - mmdet - INFO - Epoch [1][500/7674]    lr: 1.996e-02, eta: 10:45:03, time: 0.404, data_time: 0.061, memory: 6086, loss_rpn_cls: 0.1654, loss_rpn_bbox: 
2022-03-17 11:58:08,190 - mmdet - INFO - Epoch [1][550/7674]    lr: 2.000e-02, eta: 10:46:53, time: 0.438, data_time: 0.069, memory: 6086, loss_rpn_cls: 0.1696, loss_rpn_bbox: 
2022-03-17 11:58:27,782 - mmdet - INFO - Epoch [1][600/7674]    lr: 2.000e-02, eta: 10:42:24, time: 0.392, data_time: 0.046, memory: 6086, loss_rpn_cls: 0.1519, loss_rpn_bbox: 
2022-03-17 11:58:47,586 - mmdet - INFO - Epoch [1][650/7674]    lr: 2.000e-02, eta: 10:39:03, time: 0.396, data_time: 0.057, memory: 6086, loss_rpn_cls: 0.1531, loss_rpn_bbox: 
2022-03-17 11:59:08,557 - mmdet - INFO - Epoch [1][700/7674]    lr: 2.000e-02, eta: 10:38:44, time: 0.420, data_time: 0.068, memory: 6086, loss_rpn_cls: 0.1578, loss_rpn_bbox: 
2022-03-17 11:59:29,379 - mmdet - INFO - Epoch [1][750/7674]    lr: 2.000e-02, eta: 10:38:07, time: 0.417, data_time: 0.043, memory: 6086, loss_rpn_cls: 0.1591, loss_rpn_bbox: 
2022-03-17 11:59:55,387 - mmdet - INFO - Epoch [1][800/7674]    lr: 2.000e-02, eta: 10:47:21, time: 0.520, data_time: 0.056, memory: 8637, loss_rpn_cls: 0.1415, loss_rpn_bbox: 
2022-03-17 12:00:16,554 - mmdet - INFO - Epoch [1][850/7674]    lr: 2.000e-02, eta: 10:46:48, time: 0.423, data_time: 0.051, memory: 8637, loss_rpn_cls: 0.1531, loss_rpn_bbox: 
2022-03-17 12:00:37,603 - mmdet - INFO - Epoch [1][900/7674]    lr: 2.000e-02, eta: 10:46:05, time: 0.421, data_time: 0.057, memory: 8637, loss_rpn_cls: 0.1493, loss_rpn_bbox: 
2022-03-17 12:00:58,241 - mmdet - INFO - Epoch [1][950/7674]    lr: 2.000e-02, eta: 10:44:46, time: 0.413, data_time: 0.070, memory: 8637, loss_rpn_cls: 0.1443, loss_rpn_bbox: 
2022-03-17 12:01:18,652 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_sample1e-3_mstrain_1x_lvis_v1.py
2022-03-17 12:01:18,652 - mmdet - INFO - Epoch [1][1000/7674]   lr: 2.000e-02, eta: 10:43:09, time: 0.408, data_time: 0.058, memory: 8637, loss_rpn_cls: 0.1381, loss_rpn_bbox: 
2022-03-17 12:01:41,131 - mmdet - INFO - Epoch [1][1050/7674]   lr: 2.000e-02, eta: 10:44:41, time: 0.450, data_time: 0.067, memory: 8637, loss_rpn_cls: 0.1477, loss_rpn_bbox: 
2022-03-17 12:02:01,694 - mmdet - INFO - Epoch [1][1100/7674]   lr: 2.000e-02, eta: 10:43:23, time: 0.411, data_time: 0.046, memory: 8637, loss_rpn_cls: 0.1335, loss_rpn_bbox: 
2022-03-17 12:02:24,039 - mmdet - INFO - Epoch [1][1150/7674]   lr: 2.000e-02, eta: 10:44:31, time: 0.447, data_time: 0.068, memory: 8637, loss_rpn_cls: 0.1466, loss_rpn_bbox: 
2022-03-17 12:02:48,095 - mmdet - INFO - Epoch [1][1200/7674]   lr: 2.000e-02, eta: 10:47:41, time: 0.481, data_time: 0.057, memory: 8637, loss_rpn_cls: 0.1462, loss_rpn_bbox: 
2022-03-17 12:03:24,253 - mmdet - INFO - Epoch [1][1250/7674]   lr: 2.000e-02, eta: 11:05:14, time: 0.723, data_time: 0.243, memory: 8637, loss_rpn_cls: 0.1370, loss_rpn_bbox: 
2022-03-17 12:04:55,400 - mmdet - INFO - Epoch [1][1300/7674]   lr: 2.000e-02, eta: 12:25:23, time: 1.823, data_time: 0.187, memory: 8637, loss_rpn_cls: 0.1379, loss_rpn_bbox: 
2022-03-17 12:07:13,825 - mmdet - INFO - Epoch [1][1350/7674]   lr: 2.000e-02, eta: 14:32:28, time: 2.769, data_time: 1.395, memory: 8637, loss_rpn_cls: 0.1413, loss_rpn_bbox: 
2022-03-17 12:09:28,457 - mmdet - INFO - Epoch [1][1400/7674]   lr: 2.000e-02, eta: 16:26:11, time: 2.692, data_time: 0.123, memory: 8637, loss_rpn_cls: 0.1338, loss_rpn_bbox: 
2022-03-17 12:11:49,853 - mmdet - INFO - Epoch [1][1450/7674]   lr: 2.000e-02, eta: 18:18:57, time: 2.828, data_time: 0.357, memory: 8637, loss_rpn_cls: 0.1293, loss_rpn_bbox: 
2022-03-17 12:15:16,950 - mmdet - INFO - Epoch [1][1500/7674]   lr: 2.000e-02, eta: 21:10:11, time: 4.142, data_time: 0.528, memory: 8637, loss_rpn_cls: 0.1395, loss_rpn_bbox: 
2022-03-17 12:19:06,483 - mmdet - INFO - Epoch [1][1550/7674]   lr: 2.000e-02, eta: 1 day, 0:11:59, time: 4.591, data_time: 0.913, memory: 8637, loss_rpn_cls: 0.1275, loss_rpn_
2022-03-17 12:21:45,274 - mmdet - INFO - Epoch [1][1600/7674]   lr: 2.000e-02, eta: 1 day, 1:55:31, time: 3.176, data_time: 0.538, memory: 8637, loss_rpn_cls: 0.1253, loss_rpn_
2022-03-17 12:25:31,306 - mmdet - INFO - Epoch [1][1650/7674]   lr: 2.000e-02, eta: 1 day, 4:34:02, time: 4.521, data_time: 0.666, memory: 8637, loss_rpn_cls: 0.1294, loss_rpn_
2022-03-17 12:28:27,537 - mmdet - INFO - Epoch [1][1700/7674]   lr: 2.000e-02, eta: 1 day, 6:18:52, time: 3.524, data_time: 0.737, memory: 8637, loss_rpn_cls: 0.1299, loss_rpn_
2022-03-17 12:32:25,119 - mmdet - INFO - Epoch [1][1750/7674]   lr: 2.000e-02, eta: 1 day, 8:50:19, time: 4.751, data_time: 0.457, memory: 8637, loss_rpn_cls: 0.1342, loss_rpn_
2022-03-17 12:34:52,432 - mmdet - INFO - Epoch [1][1800/7674]   lr: 2.000e-02, eta: 1 day, 9:57:42, time: 2.947, data_time: 1.005, memory: 8637, loss_rpn_cls: 0.1268, loss_rpn_
2022-03-17 12:38:59,716 - mmdet - INFO - Epoch [1][1850/7674]   lr: 2.000e-02, eta: 1 day, 12:22:33, time: 4.946, data_time: 1.230, memory: 8637, loss_rpn_cls: 0.1276, loss_rpn
2022-03-17 12:42:29,645 - mmdet - INFO - Epoch [1][1900/7674]   lr: 2.000e-02, eta: 1 day, 14:10:01, time: 4.198, data_time: 0.453, memory: 8637, loss_rpn_cls: 0.1307, loss_rpn
2022-03-17 12:46:41,078 - mmdet - INFO - Epoch [1][1950/7674]   lr: 2.000e-02, eta: 1 day, 16:23:46, time: 5.029, data_time: 0.680, memory: 8637, loss_rpn_cls: 0.1182, loss_rpn
2022-03-17 12:49:55,960 - mmdet - INFO - Exp name: mask_rcnn_r50_fpn_sample1e-3_mstrain_1x_lvis_v1.py




**Reproduction**

1. Command :
```none
sh tools/dist_train.sh configs/lvis/mask_rcnn_r50_fpn_sample1e-3_mstrain_1x_lvis_v1.py 8
```
2. Dataset:
    LVIS_V1

**Environment**
sys.platform: linux
Python: 3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-16GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (GCC) 5.2.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.5 Product Build 20190808 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.3.0
MMCV: 1.4.6
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.22.0+6b87ac2







"
Support K-Net,open-mmlab/mmdetection,2022-03-16 02:58:31,0,Dev-RD,7416,1170478945,"This issue describes the development requirement of a feature we wish to collaborate with the community.
In this issue, we welcome the community developers to participate in supporting K-Net on COCO instance segmentation and panoptic segmentation.

## Steps

- First have a discuss with the mmdet team about the development plan. We can divide the full algorithms into multiple modules and add each module step by step in different PR.
- In each PR, add a sub-modules of K-Net and add unit tests to test your code
- Benchmark the performance when all modules have been added
- Prepare and release the models

## Resources

Official code release at https://github.com/ZwwWayne/K-Net/

## Reviewers
@ZwwWayne"
请问ResNeSt超参数cardinality在哪设置？,open-mmlab/mmdetection,2022-03-15 13:29:18,0,,7411,1169677329,请问ResNeSt超参数cardinality在哪设置？在resnest.py中， class SplitAttentionConv2d、class Bottleneck、class ResNeSt中均有超参数 group 和 radix 那么论文中超参数cardinality 和radix应如何设置，三个class中哪一个group参数对应的是cardinality呢？谢谢
Support instance segmentation on PASCAL VOC dataset. This will serve as quick examples for users to know and try the codebase.,open-mmlab/mmdetection,2022-03-15 10:01:12,0,Dev-RD,7406,1169441193,"This issue describes the development requirement of a feature we wish to collaborate with the community. In this issue, we welcome the community developers to participate in the development of the instance segmentation task on PASCAL VOC dataset.

## Steps

- Support loading instance segmentation annotations of PASCAL VOC
- Add unit tests to test the newly added codes
- Add configs with Mask R-CNN with benchmarked performance
- Prepare models based on the config
- Update README.md and metafile.yml

## Resources
Mask RCNN configs at: 
https://github.com/open-mmlab/mmdetection/tree/master/configs/mask_rcnn
VOCDataset: 
https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/voc.py

## Reviewers
@jbwang1997"
Support Conditional Convolutions for Instance Segmentation,open-mmlab/mmdetection,2022-03-15 09:59:18,0,Dev-RD,7405,1169438279,"This issue describes the development requirement of a feature we wish to collaborate with the community. In this issue, we welcome the community developers to participate in the development of the CondInst algorithom.

## Steps
- First have a discuss with your teams about the development plan. We can divide the full algorithms into multiple modules and add each module step by step in different PR.
- In each PR, add a sub-modules of CondInst and add unit tests to test your code
- Benchmark the performance when all modules have been added
- Prepare and release the models

## Resources
Paper release at: https://arxiv.org/abs/2003.05664
Official code release at:
GitHub - https://github.com/aim-uofa/AdelaiDet/blob/master/configs/CondInst/README.md

## Reviewers
@jbwang1997"
Support Dynamic Area Threshold in Datasets,open-mmlab/mmdetection,2022-03-15 09:56:59,0,Dev-RD,7404,1169434558,"This issue describes the development requirement of a feature we wish to collaborate with the community. In this issue, we welcome the community developers to participate in the development of the dynamic area threshold.

## Description

For now, mmdetection filters instances whose areas are less than 32. It's better to change an integer threshold for annotations filtering to a float ratio threshold based on annotation_area to image_area ratio.

## Steps

- Implement the feature
- Add unit tests to test your code

## Resources

- Related Issue: https://github.com/open-mmlab/mmdetection/issues/6826
- Code of filtering function: https://github.com/open-mmlab/mmdetection/blob/83eea4bec684708171f0b6433636df0873e50dac/mmdet/datasets/coco.py#L99

## Reviewers
@jbwang1997"
Stack of general questions,open-mmlab/mmdetection,2022-03-14 16:59:21,2,,7395,1168657459,"1. What is the ""accuracy"" provided during MaskRCNN training process?
2. Could I change the metric during training process?
3. Can I make some custom features in training process? Like pin_memory in data loaders or use more than one validation sample? "
How to use a trained model for validation to get metrics such as mAP,open-mmlab/mmdetection,2022-03-14 09:45:49,2,,7390,1168125107,"I only saw methods to get the predictions but not to get the metrics after the training is done.
Is there a way to do that?"
open-mmlab organization on Hugging Face,open-mmlab/mmdetection,2022-03-13 01:02:36,3,,7382,1167443826,"Hi, is there any interest in adding a open-mmlab organization to Huggingface to host models/datasets and spaces(web demos)

some example organizations

Keras: https://huggingface.co/keras-io
Facebook: https://huggingface.co/facebook
Microsoft: https://huggingface.co/microsoft
Pytorch: https://huggingface.co/pytorch

We can help with setting up the organization and adding models/datasets/spaces

a new organization can be added by going to https://huggingface.co/organizations/new, thanks"
"I think the matching of ""analysis result"" is not correct",open-mmlab/mmdetection,2022-03-11 10:05:28,9,enhancement#community discussion,7373,1166228433,"I am using your code
https://github.com/open-mmlab/mmdetection/blob/master/tools/analysis_tools/analyze_results.py
to evaluate the performance of single samples. However, 
![image](https://user-images.githubusercontent.com/39186017/157845861-c95287a7-8c6b-4588-a898-f31bb89bb77d.png)
Here's the images in ""good"" folder, whose mAP are all 1.0"
What is the difference of the bbox_map of coco and map of voc? ,open-mmlab/mmdetection,2022-03-10 02:59:48,1,Doc,7361,1164678349,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

**Reproduction**

1. What command or script did you run?

```none
A placeholder for the command.
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
3. What dataset did you use?

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
2. You may add addition that may be helpful for locating the problem, such as
    - How you installed PyTorch [e.g., pip, conda, source]
    - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

**Error traceback**
If applicable, paste the error trackback here.

```none
A placeholder for trackback.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
Number of Dets is too high,open-mmlab/mmdetection,2022-03-09 13:10:43,3,Doc#How-to,7356,1163921548,"Any suggestions how to fix this where the number of dets is too high?

```
2022-03-09 13:07:03,081 - mmdet - INFO - 
+--------+-----+-------+--------+-------+
| class  | gts | dets  | recall | ap    |
+--------+-----+-------+--------+-------+
| cavity | 490 | 25502 | 0.706  | 0.298 |
| pa     | 172 | 8075  | 0.797  | 0.393 |
+--------+-----+-------+--------+-------+
| mAP    |     |       |        | 0.345 |
+--------+-----+-------+--------+-------+
2022-03-09 13:07:03,087 - mmdet - INFO - Epoch(val) [10][351]	AP50: 0.3450, mAP: 0.3452
2022-03-09 13:07:41,323 - mmdet - INFO - Epoch [11][50/104]	lr: 2.500e-04, eta: 1:47:18, time: 0.763, data_time: 0.099, memory: 6895, loss_rpn_cls: 0.0304, loss_rpn_bbox: 0.0125, loss_cls: 0.0987, acc: 96.7466, loss_bbox: 0.1122, loss: 0.2539
2022-03-09 13:08:14,541 - mmdet - INFO - Epoch [11][100/104]	lr: 2.500e-04, eta: 1:46:32, time: 0.664, data_time: 0.027, memory: 6895, loss_rpn_cls: 0.0311, loss_rpn_bbox: 0.0132, loss_cls: 0.1054, acc: 96.5127, loss_bbox: 0.1198, loss: 0.2694
```

My config is the following and I am using a custom dataset
```
Config:
model = dict(
    type='FasterRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=2,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.001,
            nms=dict(type='nms', iou_threshold=0.4),
            max_per_img=100)))
dataset_type = 'BIPADataset'
data_root = 'dataset/bipa'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Albu',
        transforms=[
            dict(
                type='ShiftScaleRotate',
                shift_limit=0.0625,
                scale_limit=0.2,
                rotate_limit=45,
                interpolation=1,
                p=0.25),
            dict(
                type='RandomBrightnessContrast',
                brightness_limit=[-0.15, 0.15],
                contrast_limit=[-0.15, 0.15],
                p=0.7),
            dict(
                type='ImageCompression',
                quality_lower=85,
                quality_upper=95,
                p=0.2),
            dict(
                type='OneOf',
                transforms=[
                    dict(type='Blur', blur_limit=3, p=1.0),
                    dict(type='MedianBlur', blur_limit=3, p=1.0),
                    dict(type='GaussianBlur', blur_limit=3, p=1.0)
                ],
                p=0.25),
            dict(
                type='OneOf',
                transforms=[
                    dict(type='Sharpen', p=1.0),
                    dict(type='Emboss', p=1.0)
                ],
                p=0.25),
            dict(
                type='GaussNoise',
                var_limit=[20.0, 80.0],
                per_channel=False,
                p=0.3),
            dict(type='HorizontalFlip', p=0.5),
            dict(type='RandomRotate90', p=0.3),
            dict(type='Transpose', p=0.2)
        ],
        keymap=dict(img='image', gt_bboxes='bboxes'),
        update_pad_shape=False,
        skip_img_without_anno=True,
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=['gt_labels'],
            filter_lost_elements=True,
            min_visibility=0.1,
            min_area=1)),
    dict(type='Resize', img_scale=(512, 512), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.0),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(512, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=16,
    workers_per_gpu=8,
    train=dict(
        type='BIPADataset',
        ann_file='annotations/train.json',
        img_prefix='train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='Albu',
                transforms=[
                    dict(
                        type='ShiftScaleRotate',
                        shift_limit=0.0625,
                        scale_limit=0.2,
                        rotate_limit=45,
                        interpolation=1,
                        p=0.25),
                    dict(
                        type='RandomBrightnessContrast',
                        brightness_limit=[-0.15, 0.15],
                        contrast_limit=[-0.15, 0.15],
                        p=0.7),
                    dict(
                        type='ImageCompression',
                        quality_lower=85,
                        quality_upper=95,
                        p=0.2),
                    dict(
                        type='OneOf',
                        transforms=[
                            dict(type='Blur', blur_limit=3, p=1.0),
                            dict(type='MedianBlur', blur_limit=3, p=1.0),
                            dict(type='GaussianBlur', blur_limit=3, p=1.0)
                        ],
                        p=0.25),
                    dict(
                        type='OneOf',
                        transforms=[
                            dict(type='Sharpen', p=1.0),
                            dict(type='Emboss', p=1.0)
                        ],
                        p=0.25),
                    dict(
                        type='GaussNoise',
                        var_limit=[20.0, 80.0],
                        per_channel=False,
                        p=0.3),
                    dict(type='HorizontalFlip', p=0.5),
                    dict(type='RandomRotate90', p=0.3),
                    dict(type='Transpose', p=0.2)
                ],
                keymap=dict(img='image', gt_bboxes='bboxes'),
                update_pad_shape=False,
                skip_img_without_anno=True,
                bbox_params=dict(
                    type='BboxParams',
                    format='pascal_voc',
                    label_fields=['gt_labels'],
                    filter_lost_elements=True,
                    min_visibility=0.1,
                    min_area=1)),
            dict(type='Resize', img_scale=(512, 512), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.0),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        data_root='dataset/bipa',
        classes=('cavity', 'pa')),
    val=dict(
        type='BIPADataset',
        ann_file='annotations/val.json',
        img_prefix='val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='dataset/bipa',
        classes=('cavity', 'pa')),
    test=dict(
        type='BIPADataset',
        ann_file='annotations/val.json',
        img_prefix='val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(512, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='dataset/bipa',
        classes=('cavity', 'pa')))
evaluation = dict(interval=1, metric='mAP')
checkpoint_config = dict(interval=1000)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup=None,
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=100)
albu_train_transforms = [
    dict(
        type='ShiftScaleRotate',
        shift_limit=0.0625,
        scale_limit=0.2,
        rotate_limit=45,
        interpolation=1,
        p=0.25),
    dict(
        type='RandomBrightnessContrast',
        brightness_limit=[-0.15, 0.15],
        contrast_limit=[-0.15, 0.15],
        p=0.7),
    dict(type='ImageCompression', quality_lower=85, quality_upper=95, p=0.2),
    dict(
        type='OneOf',
        transforms=[
            dict(type='Blur', blur_limit=3, p=1.0),
            dict(type='MedianBlur', blur_limit=3, p=1.0),
            dict(type='GaussianBlur', blur_limit=3, p=1.0)
        ],
        p=0.25),
    dict(
        type='OneOf',
        transforms=[dict(type='Sharpen', p=1.0),
                    dict(type='Emboss', p=1.0)],
        p=0.25),
    dict(type='GaussNoise', var_limit=[20.0, 80.0], per_channel=False, p=0.3),
    dict(type='HorizontalFlip', p=0.5),
    dict(type='RandomRotate90', p=0.3),
    dict(type='Transpose', p=0.2)
]
classes = ('cavity', 'pa')
CLASSES = ('cavity', 'pa')
work_dir = './logs_bipa_frcnn'
seed = 0
gpu_ids = [0]
```
"
unable to register customdataset,open-mmlab/mmdetection,2022-03-09 05:49:07,3,Doc,7352,1163518452,"hi, I get the following error even after I have updated my `__init__.py` file and provided my `customdataset.py` file. Here is the error. 
 
```
Traceback (most recent call last):
  File ""tools/train-det.py"", line 196, in <module>
    main()
  File ""tools/train-det.py"", line 173, in main
    datasets = [build_dataset(cfg.data.train)]
  File ""/home/tushar/anaconda3/envs/perception/lib/python3.7/site-packages/mmdet/datasets/builder.py"", line 81, in build_dataset
    dataset = build_from_cfg(cfg, DATASETS, default_args)
  File ""/home/tushar/anaconda3/envs/perception/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 45, in build_from_cfg
    f'{obj_type} is not in the {registry.name} registry')
KeyError: 'IRAVENType is not in the dataset registry'
```
My current versions of all the packages are
```
(perception) tushar@tushar-XPS-8940:~/Storage/local_data/perceptionRPM$ pip list | grep ""mmcv\|mmdet\|^torch""
mmcv-full                     1.4.6
mmdet                         2.22.0
torch                         1.7.0
torchvision                   0.8.1
```"
Please add classwise results to the logger output json file,open-mmlab/mmdetection,2022-03-03 21:17:52,2,enhancement#feature request,7316,1158906962,"**Describe the feature**

Please log class-wise results in the logger output json file

**Motivation**

It is very inconvenient to register the values from the print outputs alone; it is currently not possible for me to log trendlines that are class specific, and if the program terminates when im not looking, I cant record the values manually. I will try to see if this is easy to implement on my side as well, but would like to see an mmdetection official implementation of it. 

This problem causes a significant waste of compute resources, because I would have to rerun eval on each checkpoint to check for trends. I would rather just parse the 'val' key in the json file at each val run... :)

**Related resources**

N/A

**Additional context**

N/A
"
loss is nan when training on voc dataset,open-mmlab/mmdetection,2022-03-01 06:30:08,3,How-to,7291,1154928631,"------------------------------------------------------------
envinfo is first 
```
sys.platform: linux
Python: 3.7.11 (default, Jul 27 2021, 14:32:16) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 2080 Ti
CUDA_HOME: /home/zhangy/lib/cuda-10.2
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.1
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.2
OpenCV: 4.5.4
MMCV: 1.4.1
MMCV Compiler: GCC 7.5
MMCV CUDA Compiler: 10.2
MMDetection: 2.19.1+5705378
```
------------------------------------------------------------

and, the dataset is 3 voc dataset from pascal voc offical webpage, and unziped in the directory `mmdetection/data/VOCdevkit`
they are
`VOCtrainval_06-Nov-2007.tar VOCtrainval_11-May-2012.tar VOCtest_06-Nov-2007.tar`

did i download the right dataset?

------------------------------------------------------------

after that, i start the train script
```shell
$ python tools/train.py configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py
```
and immediately, nan is there TT

here are some logs

```
2022-03-01 14:18:41,012 - mmdet - INFO - Epoch [1][50/24827]	lr: 1.000e-02, eta: 10:51:47, time: 0.394, data_time: 0.219, memory: 2573, loss_rpn_cls: 0.1486, loss_rpn_bbox: 0.0233, loss_cls: 0.3324, acc: 94.9375, loss_bbox: 0.1212, loss: 0.6254
2022-03-01 14:18:49,694 - mmdet - INFO - Epoch [1][100/24827]	lr: 1.000e-02, eta: 7:49:18, time: 0.174, data_time: 0.009, memory: 2573, loss_rpn_cls: 0.0910, loss_rpn_bbox: 0.0245, loss_cls: 0.2680, acc: 96.0449, loss_bbox: 0.1557, loss: 0.5392
2022-03-01 14:18:58,733 - mmdet - INFO - Epoch [1][150/24827]	lr: 1.000e-02, eta: 6:52:16, time: 0.181, data_time: 0.010, memory: 2573, loss_rpn_cls: 0.0751, loss_rpn_bbox: 0.0197, loss_cls: 0.2632, acc: 95.8047, loss_bbox: 0.1653, loss: 0.5234
2022-03-01 14:19:07,865 - mmdet - INFO - Epoch [1][200/24827]	lr: 1.000e-02, eta: 6:24:28, time: 0.183, data_time: 0.004, memory: 2573, loss_rpn_cls: 0.0637, loss_rpn_bbox: 0.0247, loss_cls: 0.2977, acc: 95.1953, loss_bbox: 0.1914, loss: 0.5776
2022-03-01 14:19:16,357 - mmdet - INFO - Epoch [1][250/24827]	lr: 1.000e-02, eta: 6:03:29, time: 0.170, data_time: 0.006, memory: 2573, loss_rpn_cls: nan, loss_rpn_bbox: nan, loss_cls: nan, acc: 36.8305, loss_bbox: nan, loss: nan
2022-03-01 14:19:24,163 - mmdet - INFO - Epoch [1][300/24827]	lr: 1.000e-02, eta: 5:45:43, time: 0.156, data_time: 0.004, memory: 2573, loss_rpn_cls: nan, loss_rpn_bbox: nan, loss_cls: nan, acc: 1.5000, loss_bbox: nan, loss: nan
2022-03-01 14:19:31,934 - mmdet - INFO - Epoch [1][350/24827]	lr: 1.000e-02, eta: 5:32:47, time: 0.155, data_time: 0.009, memory: 2573, loss_rpn_cls: nan, loss_rpn_bbox: nan, loss_cls: nan, acc: 10.2667, loss_bbox: nan, loss: nan
2022-03-01 14:19:40,107 - mmdet - INFO - Epoch [1][400/24827]	lr: 1.000e-02, eta: 5:24:42, time: 0.163, data_time: 0.014, memory: 2573, loss_rpn_cls: nan, loss_rpn_bbox: nan, loss_cls: nan, acc: 2.1667, loss_bbox: nan, loss: nan
```

------------------------------------------------------------

i tried to adjust lr(0.01, 0.001, 0.0001) and didnt fix it, how should i do ?
"
Confusion about padding when training COCO,open-mmlab/mmdetection,2022-02-28 15:24:14,3,community discussion#Doc,7286,1154223538,"Hello. I am training COCO, and I have some confusions when preparing COCO dataset.
1. What's the differences between ""img_shape"" and ""pad_shape""?
2. Different images are resized into different target sizes when fixing the short size. However to form them into a batch? Using padding? And where are the corresponding codes?"
YOLOX: num_last_epochs might not be the exact number what it's supposed to be,open-mmlab/mmdetection,2022-02-28 13:42:00,1,,7284,1154108408,"Hello. I might have found a small issue, please look into it :) 

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
While tuning the parameter ``` num_last_epochs``` in ``` yolox_s_8x8_300e_coco.py```, I found that ``` num_last_epochs ``` is not the exact number what it meant to be? 

Here is the simple example. 
Let's say 
```python 
max_epochs=20
num_last_epochs=5
```
Then ``` yolox_mode_switch_hook.py``` comes in before training epoch as a hook to turn off augmentation 
at the 14th epoch because the line number 35
```python
if (epoch + 1) == runner.max_epochs - self.num_last_epochs:
```
is satisfied. 
Then during 6 epochs (from 14th to 19th epoch) instead of 5 epochs (which is num_last_epochs), training proceeds without augmentation.
Shouldn't line 35 be like below to ensure num_last_epochs is what it meant to be?
```python
if epoch == runner.max_epochs - self.num_last_epochs:
```
Thank you!

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
How to use tow box-losses at the same time in roi_head of faster-rcnn model,open-mmlab/mmdetection,2022-02-28 09:02:03,2,enhancement#How-to,7283,1153826578,"I want to use two box losses (smoothL1loss and iou loss) at the same time in the roi_head module of the faster rcnn model, how should I modify the code "
"mmdetection/mmdet/datasets, PALETTE",open-mmlab/mmdetection,2022-02-28 08:48:25,1,Doc#How-to,7282,1153813740,"`PALETTE = []`
In the old version, there is no palette。 If i use my own coco-format datasets, I need to change the palette?"
'NoneType' object has no attribute 'sample_all',open-mmlab/mmdetection,2022-02-28 06:48:37,1,,7280,1153719526,"Traceback (most recent call last):
  File ""tools/train.py"", line 245, in <module>
    main()
  File ""tools/train.py"", line 241, in main
    meta=meta)
  File ""/data/.conda/envs/mmdetection3d/mmdet3d/apis/train.py"", line 71, in train_model
    meta=meta)
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/apis/train.py"", line 208, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 47, in train
    for i, data_batch in enumerate(self.data_loader):
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__
    data = self._next_data()
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1203, in _next_data
    return self._process_data(data)
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data
    data.reraise()
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/_utils.py"", line 434, in reraise
    raise exception
AttributeError: Caught AttributeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/datasets/dataset_wrappers.py"", line 178, in __getitem__
    return self.dataset[idx % self._ori_len]
  File ""/data/.conda/envs/mmdetection3d/mmdet3d/datasets/custom_3d.py"", line 361, in __getitem__
    data = self.prepare_train_data(idx)
  File ""/data/.conda/envs/mmdetection3d/mmdet3d/datasets/custom_3d.py"", line 155, in prepare_train_data
    example = self.pipeline(input_dict)
  File ""/home/aidrive1/miniconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet/datasets/pipelines/compose.py"", line 41, in __call__
    data = t(data)
  File ""/data/.conda/envs/mmdetection3d/mmdet3d/datasets/pipelines/transforms_3d.py"", line 329, in __call__
    sampled_dict = self.db_sampler.sample_all(
AttributeError: 'NoneType' object has no attribute 'sample_all'"
ImportError: cannot import name 'setup_multi_processes' from 'mmdet.utils',open-mmlab/mmdetection,2022-02-27 13:04:30,1,,7272,1153243898,"When running following line, I get:
`from mmdet.utils import collect_env, get_root_logger, setup_multi_processes`
> ImportError: cannot import name 'setup_multi_processes' from 'mmdet.utils' (C:\Users\jkranzen\Miniconda3\envs\mmlab\lib\site-packages\mmdet\utils\__init__.py)

**Environment**
I followed this thread to install everything on Windows: [#windows 10 mmdet](https://github.com/open-mmlab/mmcv/issues/1623)
Python 3.9.7
CUDA 11.3 (Nvidea Geforce MX130)
TorchVision: 0.11.1
MMCV: 1.4.2
MMDetection: 2.20.0

If additional info is needed, please ask. "
yolact  inference multi images in batch ,open-mmlab/mmdetection,2022-02-24 23:27:02,1,Doc#How-to,7251,1149872600,"Hello,First I want to thank you for the great work 
Did you support multi image inference in yolact model or how can i infer multiple images in the same batch thanks
"
运行AutoAugment类中example示例代码报错,open-mmlab/mmdetection,2022-02-24 12:45:02,1,community help wanted,7248,1149244515,"背景：想通过[代码链接](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/pipelines/auto_augment.py)链接文档中的example学习以下mmdet中的auto_augment


![image](https://user-images.githubusercontent.com/25839884/155526695-589b78fa-1f07-42a6-9c89-f2ec5f51c579.png)


测试python程序如下

```python

import mmdet.datasets.pipelines.auto_augment

from mmdet.datasets.pipelines import AutoAugment

import numpy as np

replace = (104, 116, 124)
policies = [
    [
        dict(type='Sharpness', prob=0.0, level=8),
        dict(
            type='Shear',
            prob=0.4,
            level=0,
            replace=replace,
            axis='x')
    ],
    [
        dict(
            type='Rotate',
            prob=0.6,
            level=10,
            replace=replace),
        dict(type='Color', prob=1.0, level=6)
    ]
]
augmentation = AutoAugment(policies)
img = np.ones(100, 100, 3)
gt_bboxes = np.ones(10, 4)
results = dict(img=img, gt_bboxes=gt_bboxes)
results = augmentation(results)
```

问题描述如下：

1.  Sharpness在mmdet中好像未实现（KeyError: 'Sharpness is not in the pipeline registry'）
2.  如下图片的replace=replace是什么意思呢，也报错了(TypeError: Shear: __init__() got an unexpected keyword argument 'replace')


![image](https://user-images.githubusercontent.com/25839884/155526804-9a7a1e1e-5fa8-4a3a-99d1-ce8d36ccc449.png)


环境信息

ubuntu 20.04
mmdet 2.21.0"
How to get AR for every class?,open-mmlab/mmdetection,2022-02-24 01:33:57,3,Doc#How-to,7236,1148761315,"As the title mentions, how can we get AR for every class?"
image sizes in pretrained models for SwinTransformer vs dataset image size,open-mmlab/mmdetection,2022-02-23 15:39:50,3,,7233,1148253856,"Hi all

I'm trying to retrain an object detector with a SwinTransformer backbone using a pretrained pth.

Everything is working well but if I'm understanding of the mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py config correctly:
- the backbone is instantiated with the default value for pretrained_img_size=224
- the backbone loads from a pretrained pth with the same image size (quite logical)
- the network is then applied on the COCO dataset, with a train_pipeline which involves resizing images with sizes between 480 and 800 (which is consistant with the original paper)

Though the whole process seems consistent with the original paper, I don't really understand how pretraining on images that are so much smaller makes a good pretrained model (though I imagine it is still better than no pretraining at all).
Wouldn't the patches bear very different descriptors between size 224 images and size 600 images? Wouldn't it be much more helpful to have a pretrained model with image size much closer to the target size ?
Or did I miss something that makes it not a problem ?

Thanks
"
Stochastic depth on backbone,open-mmlab/mmdetection,2022-02-20 19:57:35,1,,7207,1145118472,"Hi, I'm wondering if anyone could share some thoughts on whether it is recommended to train the detection/instance segmentation model **with stochastic depth on the backbone**? E.g. A (224x224, 28M parameters, 4.2G FLOPs) transformer backbone + cascade rcnn, should the backbone part contain stochastic depth when training the detection pipeline?

Thanks a lot!"
摄像头demo的运行速度（fps）,open-mmlab/mmdetection,2022-02-18 04:11:29,11,community discussion,7196,1142320057,"大家好，我在测试集上运行实例分割（采用mask-rcnn）时检测得到的fps为15
但是在我运行webcam_demo.py时（稍加修改，下面会介绍）会不断弹出窗口展示结果。大致判断在摄像头上展示结果为2fps.运行的非常的慢。有什么方法能够加速在摄像头上运行的速度吗？


修改部分为使用gazebo内的kinect摄像头。大致操作为接收摄像头的图像信息。然后转换为bgr8的矩阵。最后直接使用model的相关函数对bgr8的矩阵进行检测处理。
同时也在zed摄像头上工作。zed摄像头工作时采用多线程。对彩色图像的bgr8矩阵传输给webcam_demo的main函数然后进行检测处理。

在这两个摄像头上运行的都十分的慢。一秒只展示两帧图像。这是为什么？能够加速吗？"
Error while running mmdetection/tools/analysis_tools/analyze_results.py,open-mmlab/mmdetection,2022-02-17 16:18:34,2,,7192,1141553515,"  File ""/usr/local/lib/python3.7/dist-packages/mmcv/utils/misc.py"", line 73, in import_modules_from_strings
    imported_tmp = import_module(imp)
  File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'path'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/content/mmdetection/tools/analysis_tools/analyze_results.py"", line 199, in <module>
    main()
  File ""/content/mmdetection/tools/analysis_tools/analyze_results.py"", line 182, in main
    cfg = Config.fromfile(args.config)
  File ""/usr/local/lib/python3.7/dist-packages/mmcv/utils/config.py"", line 334, in fromfile
    import_modules_from_strings(**cfg_dict['custom_imports'])
  File ""/usr/local/lib/python3.7/dist-packages/mmcv/utils/misc.py"", line 80, in import_modules_from_strings
    raise ImportError
ImportError


**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
Grad_norm is Nan when resume training,open-mmlab/mmdetection,2022-02-17 13:41:49,1,bug,7189,1141354473,"TorchVision: 0.10.1+cu111
OpenCV: 4.5.3
MMCV: 1.3.9
MMCV Compiler: GCC 7.5
MMCV CUDA Compiler: 11.1
MMDetection: 2.12.0+/2.14.0+
TORCH_VERSION=1.9.1

Train SparseRCNN_R50_8stages
Train on single GPU 3080ti. Each GPU has 4 images. Cumulate gradients for 4 iterations to backpropagate.
Nothing else has been changed compared to the ""sparse_rcnn_r50_fpn_1x_coco.py""
When I resumed training, I found that grad_norm become NaN but the original training (training at one time) will not have any NaN value.

If the checkpoint file is needed, I am very happy to upload it."
error in ms_deformable_col2im_cuda: an illegal memory access was encountered,open-mmlab/mmdetection,2022-02-17 10:31:52,2,,7186,1141155166,"**Describe the bug**
I'm getting the following error when trying to run deformable_detr

**Reproduction**

1. What command or script did you run?
I tried to train the config file below 

https://github.com/open-mmlab/mmdetection/blob/7a9bc498d5cc972171ec4f7332afcd70bb50e60e/configs/deformable_detr/deformable_detr_r50_16x2_50e_coco.py


2. Did you make any modifications on the code or config? Did you understand what you have modified?
No I did not modify
3. What dataset did you use?

**Environment**
sys.platform: linux
Python: 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
CUDA available: True
GPU 0: TITAN RTX
CUDA_HOME: /usr/local/cuda-11.0
NVCC: Build cuda_11.0_bu.TC445_37.28845127_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.7.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON,

TorchVision: 0.8.0
OpenCV: 4.5.5
MMCV: 1.4.4
MMCV Compiler: GCC 7.5
MMCV CUDA Compiler: 11.0
MMDetection: 2.20.0+

torch is installed by pip 

**Error traceback**

```
error in ms_deformable_col2im_cuda: an illegal memory access was encountered
Traceback (most recent call last):
  File ""tools/train.py"", line 200, in <module>
    main()
  File ""tools/train.py"", line 188, in main
    train_detector(
  File ""/cta/users/mehmet/CenterNetMMCV/ssod/apis/train.py"", line 206, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/cta/users/mehmet/CenterNetMMCV/thirdparty/mmcv/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/cta/users/mehmet/CenterNetMMCV/thirdparty/mmcv/mmcv/runner/epoch_based_runner.py"", line 51, in train
    self.call_hook('after_train_iter')
  File ""/cta/users/mehmet/CenterNetMMCV/thirdparty/mmcv/mmcv/runner/base_runner.py"", line 309, in call_hook
    getattr(hook, fn_name)(self)
  File ""/cta/users/mehmet/CenterNetMMCV/thirdparty/mmcv/mmcv/runner/hooks/optimizer.py"", line 56, in after_train_iter
    runner.outputs['loss'].backward()
  File ""/cta/users/mehmet/.conda/envs/centernetmmcv/lib/python3.8/site-packages/torch/tensor.py"", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/cta/users/mehmet/.conda/envs/centernetmmcv/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 130, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA error: an illegal memory access was encountered.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
Runtime issues with mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py,open-mmlab/mmdetection,2022-02-17 05:21:04,1,,7182,1140890669,"I used the mask_rcnn_swin-t-p4-w7_fpn_fp16_ms-crop-3x_coco.py

and i get this error: RuntimeError: Expected object of scalar type Float but got scalar type Half for argument #2 'mat2' in call to _th_bmm


my system configurations are:

2022-02-17 12:53:24,806 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.9 (default, Apr  3 2021, 01:02:10) [GCC 5.4.0 20160609]
CUDA available: True
GPU 0: TITAN Xp
GPU 1: GeForce GTX 1080 Ti
CUDA_HOME: /usr/local/cuda-9.2
NVCC: Cuda compilation tools, release 9.2, V9.2.148
GCC: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
PyTorch: 1.5.1+cu92
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2019.0.5 Product Build 20190808 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 9.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.6.1+cu92
OpenCV: 4.5.5
MMCV: 1.4.0
MMCV Compiler: GCC 5.4
MMCV CUDA Compiler: 9.2
MMDetection: 2.21.0+ff9bc39
.4.0
MMCV CompÁë"
[Feature Request] Add LoadImageFromNdarray  alias for LoadImageFromWebcam,open-mmlab/mmdetection,2022-02-16 04:41:10,5,community discussion#feature request,7171,1139528090,"**Describe the feature**

**Motivation**
LoadImageFromWebcam is not easy to understand how it works.
Actually it transform ndarray to default mmdetection data format.

So I think it is good idea to add alias as LoadImageFromNdarray.
implementation will be like just below
```
# end of the mmdet.datasets.pipelines.py
LoadImageFromNdarray = LoadImageFromWebcam
```

If it seems good idea, I will create the PR."
Wandb Sweep with mmdetection,open-mmlab/mmdetection,2022-02-15 08:15:14,1,Doc#How-to,7165,1138334813,"Hi everyone,

Does anyone know that how to use Wandb Sweep with mmdetection? Is there any tutorial or examples?

Thanks :)"
yolact Out of memory during training,open-mmlab/mmdetection,2022-02-13 06:41:37,6,Doc#How-to,7148,1135329543,"I am using yolact for training, when an epoch ends, it will be **OOM** in the Val phase.

My environment is:

- Python  3.7.11
- CUDA  11.3
- CUDNN  8200
- numpy  1.21.2
- pycocotools  2.0.4 
- pytorch   1.10.1

The GPU is 3090 (24G Vram)

I just modified the `num_classes` in file `yolact_r50_1x8_coco.py`
```
data = dict(
    samples_per_gpu=8, 
    workers_per_gpu=4,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/train2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline))
```
then I started training `python tools/train.py configs/yolact/yolact_r50_1x8_coco.py` by using my dataset. 

My dataset：
- https://resources.mpi-inf.mpg.de/d2/orekondy/redactions/, The format of annotations has been changed to CoCo.

But when an epoch ends, it will be out of memory in the Val phase.

```
2022-02-13 14:38:01,355 - mmdet - INFO - Saving checkpoint at 1 epochs
[                                                  ] 26/1611, 0.6 task/s, elapsed: 43s, ETA:  2636sTraceback (most recent call last):
  File ""tools/train.py"", line 195, in <module>
    main()
  File ""tools/train.py"", line 191, in main
    meta=meta)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/apis/train.py"", line 209, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 54, in train
    self.call_hook('after_train_epoch')
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmcv/runner/base_runner.py"", line 309, in call_hook
    getattr(hook, fn_name)(self)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmcv/runner/hooks/evaluation.py"", line 267, in after_train_epoch
    self._do_evaluate(runner)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/core/evaluation/eval_hooks.py"", line 56, in _do_evaluate
    results = single_gpu_test(runner.model, self.dataloader, show=False)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/apis/test.py"", line 28, in single_gpu_test
    result = model(return_loss=False, rescale=True, **data)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmcv/parallel/data_parallel.py"", line 50, in forward
    return super().forward(*inputs, **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py"", line 98, in new_func
    return old_func(*args, **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/models/detectors/base.py"", line 174, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/models/detectors/base.py"", line 147, in forward_test
    return self.simple_test(imgs[0], img_metas[0], **kwargs)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/models/detectors/yolact.py"", line 113, in simple_test
    rescale=rescale)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/models/dense_heads/yolact_head.py"", line 999, in simple_test
    img_metas[i], rescale)
  File ""/home/zjj/anaconda3/envs/image/lib/python3.7/site-packages/mmdet/models/dense_heads/yolact_head.py"", line 869, in get_seg_masks
    align_corners=False).squeeze(0) > 0.5
RuntimeError: CUDA out of memory. Tried to allocate 4.69 GiB (GPU 0; 23.70 GiB total capacity; 19.18 GiB already allocated; 276.56 MiB free; 21.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```
After I exchange the test set and verification set, the error report disappears
```
data = dict(
    samples_per_gpu=8, 
    workers_per_gpu=4,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/train2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline))
```
Is there a way I can use to solve this issue? Thanks"
Questions on implementation of YOLOF,open-mmlab/mmdetection,2022-02-08 01:55:42,10,enhancement#reimplementation,7126,1126699792,"Dear Team,
In README of YOLOF's config, you mentioned ""sometimes there are large loss fluctuations and NAN"". I am wondering if this also happens in the original implementation([the detectron2 version](https://github.com/chensnathan/YOLOF)), and if you guys have found the reason that caused this issue?"
"Calculate mAP for different scenario ( easy, moderate, hard )",open-mmlab/mmdetection,2022-02-04 17:53:43,3,,7119,1124465962,"<img width=""230"" alt=""easymodhard"" src=""https://user-images.githubusercontent.com/9031472/152578251-9a2cf7a8-d032-43f4-96c7-6a8f13829db9.png"">
How do I display the mAP table for different conditions such as easy, moderate and hard. Currently the mAP is calculated and displayed once for each catagory but I want to display the  mAP result based on condition such as easy moderate and hard. I am using KITTI dataset and testing results using Faster RCNN."
'loss log variables are different across GPUs!' HowTo prevent?,open-mmlab/mmdetection,2022-02-04 10:09:20,8,,7116,1124023925,"Hi,

i am using mmdet v2.20. 
I have a custom  coco dataset with about 2k images with GroundTruth labels. 
I train a fasterrcnn and if i only use images with GT Labels everything learns smoothly. If i add about 0.5k additional images without GT Labels i get at random epochs the error 'loss log variables are different across GPUs!' and the training aborts. The message was built in to prevent GPU hanging via https://github.com/open-mmlab/mmsegmentation/pull/1035.

Can you give me a hint how to prevent this? 
* Does it have something to do that sometimes a GPU doesnt produce a loss because there are only images without GT Labels?
* Could a unshuffled dataset which always guarantees that a batch got always a image with GT labels and without?

Here is my config:
```

interval = 2
checkpoint_config = dict(interval=2)
evaluation = dict(interval=2, metric='bbox', iou_thrs=[0.1, 0.5, 0.9])
log_config = dict(
    interval=20,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
dataset_type = 'CocoDataset'
classes = ('boat', )
img_norm_cfg = dict(
    mean=[91.622, 120.867, 166.518], std=[51.634, 39.997, 42.519], to_rgb=True)
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1640, 1232),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[91.622, 120.867, 166.518],
                std=[51.634, 39.997, 42.519],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1640, 1232), keep_ratio=True),
    dict(
        type='RandomFlip',
        flip_ratio=0.5,
        direction=['horizontal', 'vertical', 'diagonal']),
    dict(
        type='Normalize',
        mean=[91.622, 120.867, 166.518],
        std=[51.634, 39.997, 42.519],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]

valdataset = dict(
    type='CocoDataset',
    data_root='/data',
    classes=('boat', ),
    pipeline=[
        dict(type='LoadImageFromFile'),
        dict(
            type='MultiScaleFlipAug',
            img_scale=(1640, 1232),
            flip=False,
            transforms=[
                dict(type='Resize', keep_ratio=True),
                dict(type='RandomFlip'),
                dict(
                    type='Normalize',
                    mean=[91.622, 120.867, 166.518],
                    std=[51.634, 39.997, 42.519],
                    to_rgb=True),
                dict(type='Pad', size_divisor=32),
                dict(type='ImageToTensor', keys=['img']),
                dict(type='Collect', keys=['img'])
            ])
    ],
    ann_file=[
        '.json'
    ],
    filter_empty_gt=False)
testdataset = dict(
    type='CocoDataset',
    data_root='/data',
    classes=('boat', ),
    pipeline=[
        dict(type='LoadImageFromFile'),
        dict(
            type='MultiScaleFlipAug',
            img_scale=(1640, 1232),
            flip=False,
            transforms=[
                dict(type='Resize', keep_ratio=True),
                dict(type='RandomFlip'),
                dict(
                    type='Normalize',
                    mean=[91.622, 120.867, 166.518],
                    std=[51.634, 39.997, 42.519],
                    to_rgb=True),
                dict(type='Pad', size_divisor=32),
                dict(type='ImageToTensor', keys=['img']),
                dict(type='Collect', keys=['img'])
            ])
    ],
    ann_file=[
        '.....json'
    ],
    filter_empty_gt=False)
traindataset = dict(
    type='CocoDataset',
    data_root='/data',
    classes=('boat', ),
    pipeline=[
        dict(type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(type='Resize', img_scale=(1640, 1232), keep_ratio=True),
        dict(
            type='RandomFlip',
            flip_ratio=0.5,
            direction=['horizontal', 'vertical', 'diagonal']),
        dict(
            type='Normalize',
            mean=[91.622, 120.867, 166.518],
            std=[51.634, 39.997, 42.519],
            to_rgb=True),
        dict(type='Pad', size_divisor=32),
        dict(type='DefaultFormatBundle'),
        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
    ],
    ann_file=[
        '....json'
    ],
    filter_empty_gt=False)

data = dict(
    samples_per_gpu=2,
    workers_per_gpu=4,
    shuffle=True,
    train=dict(
        type='CocoDataset',
        data_root='/data',
        classes=('boat', ),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1640, 1232), keep_ratio=True),
            dict(
                type='RandomFlip',
                flip_ratio=0.5,
                direction=['horizontal', 'vertical', 'diagonal']),
            dict(
                type='Normalize',
                mean=[91.622, 120.867, 166.518],
                std=[51.634, 39.997, 42.519],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        ann_file=[
            '...json'
        ],
        filter_empty_gt=False),
    test=dict(
        type='CocoDataset',
        data_root='/data',
        classes=('boat', ),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1640, 1232),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[91.622, 120.867, 166.518],
                        std=[51.634, 39.997, 42.519],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        ann_file=
        '.json',
        filter_empty_gt=False),
    val=dict(
        type='CocoDataset',
        data_root='/data',
        classes=('boat', ),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1640, 1232),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[91.622, 120.867, 166.518],
                        std=[51.634, 39.997, 42.519],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        ann_file=[
            '.json'
        ],
        filter_empty_gt=False))
model = dict(
    type='FasterRCNN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[1.5, 3.0, 6.0],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=1,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.01,
            nms=dict(type='nms', iou_threshold=0.7),
            max_per_img=100)))
optimizer = dict(
    type='SGD',
    lr=0.015,
    momentum=0.9,
    weight_decay=0.0005,
    nesterov=True,
    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[35, 45])
runner = dict(type='EpochBasedRunner', max_epochs=50)
work_dir = './work_dirs/train'
auto_resume = False
gpu_ids = range(0, 4)
```
"
TypeError: forward() missing 1 required positional argument: 'img_metas',open-mmlab/mmdetection,2022-02-04 08:36:22,11,,7115,1123943389,"Sorry for this basic doubt. I am trying to worl with the SWIN transformer backbone and need to do a forward pass for the model in order to achieve quantization with the help of the [Tensor-RT library ](https://github.com/NVIDIA/Torch-TensorRT).I am trying to do so using: `model('img_path')`.
However, I get the following error : `TypeError: forward() missing 1 required positional argument: 'img_metas' `

I am not really sure what exactly does the method exactly expects as img_metas. If I am wrong in trying to do a forward pass with this, Is there a better or a different way to do this?"
"Error Occurs when running train.py with the ""image"" field of json file (val2017.json) set empty",open-mmlab/mmdetection,2022-01-30 08:03:59,4,enhancement,7105,1118454059,"**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**

+ It's not a critical bug but more like an improvement. 
+ The bug occurs when the ""image"" field of the json file named val2017.json is empty. So when validating after each train epoch, the index subscript will be out of range. 
+ It's advisable to assert this potential error when an empty dataloader is set or before it's used. Or the alternative plan is not to skip validating if the dataloader is empty.

**Reproduction**

1. The json file(modified from val2017.json) that I used. 

```json
    ""licenses"": [
        ...
    ],
    ""images"": [],
    ""annotations"": [
    {
        ...
    }
```

2. Did you make any modifications on the code or config? Did you understand what you have modified?

- I didn't modified the code and the config. I just extract a smaller dataset from the original coco dataset, including modifying the json file and extracting the pictures corresponding to the json file.


3. What dataset did you use?

+ COCO dataset which is modifed by me as said above

**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.

```
sys.platform: linux
Python: 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA GeForce GTX 1660 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
PyTorch: 1.9.0+cu102
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.10.0
OpenCV: 4.5.5
MMCV: 1.4.3
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.20.0+ff9bc39

```



**Error traceback**

```python
Traceback (most recent call last):
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/tools/train.py"", line 196, in <module>
    main()
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/tools/train.py"", line 185, in main
    train_detector(
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/mmdet/apis/train.py"", line 209, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/PJLAB/chenxinyi/anaconda3/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 128, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/PJLAB/chenxinyi/anaconda3/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py"", line 55, in train
    self.call_hook('after_train_epoch')
  File ""/home/PJLAB/chenxinyi/anaconda3/lib/python3.9/site-packages/mmcv/runner/base_runner.py"", line 309, in call_hook
    getattr(hook, fn_name)(self)
  File ""/home/PJLAB/chenxinyi/anaconda3/lib/python3.9/site-packages/mmcv/runner/hooks/evaluation.py"", line 267, in after_train_epoch
    self._do_evaluate(runner)
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/mmdet/core/evaluation/eval_hooks.py"", line 59, in _do_evaluate
    key_score = self.evaluate(runner, results)
  File ""/home/PJLAB/chenxinyi/anaconda3/lib/python3.9/site-packages/mmcv/runner/hooks/evaluation.py"", line 361, in evaluate
    eval_res = self.dataloader.dataset.evaluate(
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/mmdet/datasets/coco.py"", line 416, in evaluate
    result_files, tmp_dir = self.format_results(results, jsonfile_prefix)
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/mmdet/datasets/coco.py"", line 361, in format_results
    result_files = self.results2json(results, jsonfile_prefix)
  File ""/home/PJLAB/chenxinyi/XinyiWorkspace/mmdetection/mmdet/datasets/coco.py"", line 292, in results2json
    if isinstance(results[0], list):
IndexError: list index out of range
```

**Bug fix**

- The fixed code is from mmdetection/core/evaluation/eval_hooks.py

```python
    def _do_evaluate(self, runner):
        """"""perform evaluation and save ckpt.""""""
        if not self._should_evaluate(runner):
            return

        from mmdet.apis import single_gpu_test
        assert len(self.dataloader) != 0, ""empty dataloader, which may be due to the \""image\"" field of json file""
        results = single_gpu_test(runner.model, self.dataloader, show=False)

        runner.log_buffer.output['eval_iter_num'] = len(self.dataloader)
        key_score = self.evaluate(runner, results)
        if self.save_best:
            self._save_ckpt(runner, key_score)
```

- I added one assert sentence to notify the user that the dataloader is empty which could lead to an error later, since the model can not fetch any images from it. It can help the user to better find the potential error in advance.

- I put it here because the later validation operation is all based on the image data in the dataloader, and they all require it to contain some data in default, otherwise it will lead to error, which may not be easy for the user to understand how it happens quickly.

"
Questions related to training,open-mmlab/mmdetection,2022-01-29 07:52:41,3,Doc,7100,1118108090,"I have two questions related to instance segmentation model training

- Does mmdetection iscrowd tag while training?
- Is training faster when annotation is polygon or when it is RLE?"
Visualize detection outputs during training,open-mmlab/mmdetection,2022-01-28 15:02:51,2,,7093,1117505525,"Hi, how would I go about implementing visualization of detection during training?
In ideal world, that would be a new hook that would run after each X epochs, take some images from validation set and save to file it's visualization.
I'm finding it impossible to implement for few reasons:
- I don't have access to data loaders from hooks
- I don't have access to test pipelines from hooks

Things that I considered:
- using mmdet.apis.inference_detector: this doesn't work, because it takes as argument some weird structure that is result of init_detector (for example config is added to it), so using it without running init_detector is hard.  However I can't run init_detector because that would require to load weights from checkpoint, which would make me save the checkpoint first - generally quite annoying workflow.
- using runner.model from the hook directly, however there's no way to access data loader so it's not possible to format images correctly.

I'm quite stuck with this. Perfect solution would be to:
1. Access train/val dataloader from hook
2. Run inference of runner.model
3. Run runner.model.show_results to save the result

Is there a recommended way or even some workaround to do it?

Please note that I'm aware that I can do it after the training via accessing model checkpoint, however it'd be nice to have it also as training hook.

Thank you!"
analyze_results.py ,open-mmlab/mmdetection,2022-01-26 16:43:24,2,bug#usage,7079,1115233312,"AttributeError: 'ConcatDataset' object has no attribute 'prepare_train_img'

**Reproduction**

_base_='../swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py'
dataset_type='CocoDataset'

prefix='./data/taco/'

classes=('Can', 'Carton', 'Cup', 'GlassBottle',
         'Other', 'Paper','PlasticBottle', 'PlasticContainer', 'Wrapper')

test_prefix='data/test105'

model = dict(
    roi_head=dict(
        bbox_head=dict(num_classes=9),
        mask_head=dict(num_classes=9)))
data=dict(
    train=dict(
        type=dataset_type,
        classes=classes,
        ann_file=[
            'data/week17/fixed_annotations.json'
        ],
        img_prefix=prefix
    ),

    val=dict(
        type=dataset_type,
        classes=classes,
        ann_file=[
            'data/week17/fixed_annotations.json'
        ],
        img_prefix=prefix
    ),

    test=dict(
        type=dataset_type,
        classes=classes,
        ann_file=[
                    ""data/test105/fixed_annotations.json""
                  ],
        img_prefix=test_prefix
    )
)





**Environment**

TorchVision: 0.2.1
OpenCV: 4.5.3
MMCV: 1.3.15
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.4
MMDetection: 2.17.0+78e3ec8



"
dataset Problem. Is my dataset wrong？,open-mmlab/mmdetection,2022-01-24 17:10:28,9,,7072,1112918690,"run:python tools/misc/browse_dataset.py configs/a_thz_mini/yolox_s_8x8_300e_thzmini.py 
error :    return self.dataset[idx % self._ori_len]
ZeroDivisionError: integer division or modulo by zero

run:python tools/train.py configs/a_thz_mini/yolox_s_8x8_300e_thzmini.py
error:    indices = np.concatenate(indices)
  File ""<__array_function__ internals>"", line 6, in concatenate
ValueError: need at least one array to concatenate


配置文件中的dataset改为官方VOC2007的数据集就可以正常加载，改为自己的数据集就是不行。
数据集已经制作成VOC2007格式的了，可是就是加载不了，真不知道我的数据集问题到底错哪了，请帮忙下载看一下这个数据集的问题到底在哪里？

只有一个类别



https://drive.google.com/file/d/1aTjYT1mMuijC6QErAxWsAMk0YGGjfyn9/view?usp=sharing
please help me QAQ

"
pycocotools installed from mim can not run on M1 Pro MacBook ,open-mmlab/mmdetection,2022-01-23 15:30:08,1,,7068,1111902691,"```
/dl/lib/python3.9/site-packages/pycocotools/mask.py"", line 3, in <module>
    import pycocotools._mask as _mask
ImportError: dlopen(
dl/lib/python3.9/site-packages/pycocotools/_mask.cpython-39-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))
```

I got some import error which caused by ARCH not right. How to use pycocotools on M1 pro?"
any support for ffcv,open-mmlab/mmdetection,2022-01-21 08:04:32,1,enhancement#community discussion,7056,1110169821,"recently a team release ""ffcv"",
https://github.com/libffcv/ffcv
will you support this?"
How to get IoU in validation metrics?,open-mmlab/mmdetection,2022-01-19 23:04:24,4,,7042,1108646186,"Hi,

I'm running an instance segmentation model (`queryinst`), and the validation logs contain the following:
```
{""mode"": ""val"", ""epoch"": 1, ""iter"": 100, ""lr"": 0.0, 
""bbox_mAP"": 0.979, ""bbox_mAP_50"": 0.983, ""bbox_mAP_75"": 0.983, ""bbox_mAP_s"": -1.0, 
""bbox_mAP_m"": -1.0, ""bbox_mAP_l"": 0.979, ""bbox_mAP_copypaste"": ""0.979 0.983 0.983 -1.000 -1.000 0.979"", 
""segm_mAP"": 0.921, ""segm_mAP_50"": 0.983, ""segm_mAP_75"": 0.983, ""segm_mAP_s"": -1.0, 
""segm_mAP_m"": -1.0, ""segm_mAP_l"": 0.921, ""segm_mAP_copypaste"": ""0.921 0.983 0.983 -1.000 -1.000 0.921""}
```

How can I include additional metrics, like segm/bbox IoU in this?
Thanks

**Edit:**
I've just noticed on larger validation sets that there are 2 separate entries for `val` in the log file, e.g. after epoch 8 I get this:
```
{""mode"": ""train"", ""epoch"": 8, ""iter"": 7150, ""lr"": 5e-05, ""memory"": 9352, ""data_time"": 0.00686, ""stage0_loss_cls"": 0.0, ""stage0_pos_acc"": 100.0, ""stage0_loss_bbox"": 0.08268, ""stage0_loss_iou"": 0.0569, ""stage0_loss_mask"": 0.03742, ""stage1_loss_cls"": 0.0, ""stage1_pos_acc"": 100.0, ""stage1_loss_bbox"": 0.03378, ""stage1_loss_iou"": 0.02396, ""stage1_loss_mask"": 0.0348, ""stage2_loss_cls"": 0.0, ""stage2_pos_acc"": 100.0, ""stage2_loss_bbox"": 0.02977, ""stage2_loss_iou"": 0.02085, ""stage2_loss_mask"": 0.03426, ""stage3_loss_cls"": 0.0, ""stage3_pos_acc"": 100.0, ""stage3_loss_bbox"": 0.02862, ""stage3_loss_iou"": 0.02, ""stage3_loss_mask"": 0.03339, ""stage4_loss_cls"": 0.0, ""stage4_pos_acc"": 100.0, ""stage4_loss_bbox"": 0.02888, ""stage4_loss_iou"": 0.02012, ""stage4_loss_mask"": 0.03425, ""stage5_loss_cls"": 0.0, ""stage5_pos_acc"": 100.0, ""stage5_loss_bbox"": 0.02893, ""stage5_loss_iou"": 0.02018, ""stage5_loss_mask"": 0.03425, ""loss"": 0.60305, ""grad_norm"": 30.55146, ""time"": 0.44809}
{""mode"": ""val"", ""epoch"": 8, ""iter"": 4772, ""lr"": 5e-05, ""bbox_mAP"": 0.989, ""bbox_mAP_50"": 0.99, ""bbox_mAP_75"": 0.99, ""bbox_mAP_s"": -1.0, ""bbox_mAP_m"": -1.0, ""bbox_mAP_l"": 0.989, ""bbox_mAP_copypaste"": ""0.989 0.990 0.990 -1.000 -1.000 0.989"", ""segm_mAP"": 0.99, ""segm_mAP_50"": 0.99, ""segm_mAP_75"": 0.99, ""segm_mAP_s"": -1.0, ""segm_mAP_m"": -1.0, ""segm_mAP_l"": 0.99, ""segm_mAP_copypaste"": ""0.990 0.990 0.990 -1.000 -1.000 0.990""}
{""mode"": ""val"", ""epoch"": 8, ""iter"": 2386, ""lr"": 5e-05, ""memory"": 9352, ""data_time"": 0.00707, ""stage0_loss_cls"": 0.0, ""stage0_pos_acc"": 100.0, ""stage0_loss_bbox"": 0.08284, ""stage0_loss_iou"": 0.05611, ""stage0_loss_mask"": 0.0345, ""stage1_loss_cls"": 0.0, ""stage1_pos_acc"": 100.0, ""stage1_loss_bbox"": 0.03609, ""stage1_loss_iou"": 0.02452, ""stage1_loss_mask"": 0.0358, ""stage2_loss_cls"": 0.0, ""stage2_pos_acc"": 100.0, ""stage2_loss_bbox"": 0.03275, ""stage2_loss_iou"": 0.02257, ""stage2_loss_mask"": 0.03543, ""stage3_loss_cls"": 0.0, ""stage3_pos_acc"": 100.0, ""stage3_loss_bbox"": 0.03332, ""stage3_loss_iou"": 0.0228, ""stage3_loss_mask"": 0.03543, ""stage4_loss_cls"": 0.0, ""stage4_pos_acc"": 100.0, ""stage4_loss_bbox"": 0.03324, ""stage4_loss_iou"": 0.02263, ""stage4_loss_mask"": 0.03591, ""stage5_loss_cls"": 0.0, ""stage5_pos_acc"": 100.0, ""stage5_loss_bbox"": 0.03384, ""stage5_loss_iou"": 0.02314, ""stage5_loss_mask"": 0.03572, ""loss"": 0.63663, ""time"": 0.18482}
```

My val set has 4772 elements, so why is the second val entry there in the middle of the validation iterations?"
Inference error: AttributeError: module 'pycocotools' has no attribute '__version__'`,open-mmlab/mmdetection,2022-01-19 08:31:41,1,,7037,1107803094,"`AttributeError              Traceback (most recent call last)
<ipython-input-3-dbd9ef735fb1> in <module>
      5 import torch
      6 from mmcv import Config
----> 7 from mmdet.apis import inference_detector, init_detector, show_result_pyplot
      8 import os
      9 import glob

~/mmdetection/mmdet/apis/__init__.py in <module>
      2                         init_detector, show_result_pyplot)
      3 from .test import multi_gpu_test, single_gpu_test
----> 4 from .train import get_root_logger, set_random_seed, train_detector
      5 
      6 __all__ = [

~/mmdetection/mmdet/apis/train.py in <module>
      9 
     10 from mmdet.core import DistEvalHook, EvalHook
---> 11 from mmdet.datasets import (build_dataloader, build_dataset,
     12                             replace_ImageToTensor)
     13 from mmdet.utils import get_root_logger

~/mmdetection/mmdet/datasets/__init__.py in <module>
      1 from .builder import DATASETS, PIPELINES, build_dataloader, build_dataset
----> 2 from .cityscapes import CityscapesDataset
      3 from .coco import CocoDataset
      4 from .custom import CustomDataset
      5 from .dataset_wrappers import (ClassBalancedDataset, ConcatDataset,

~/mmdetection/mmdet/datasets/cityscapes.py in <module>
     14 
     15 from .builder import DATASETS
---> 16 from .coco import CocoDataset
     17 
     18 

~/mmdetection/mmdet/datasets/coco.py in <module>
     19     import pycocotools
     20     if not hasattr(pycocotools, '__sphinx_mock__'):  # for doc generation
---> 21         assert pycocotools.__version__ >= '12.0.2'
     22 except AssertionError:
     23     raise AssertionError('Incompatible version of pycocotools is installed. '

AttributeError: module 'pycocotools' has no attribute '__version__'`"
RuntimeError: DataLoader worker (pid 41956) is killed by signal: Hangup. ,open-mmlab/mmdetection,2022-01-19 01:42:04,0,,7031,1107543327,"When I train my model(based on the Faster R-CNN) with COCO, it raises error after different number of iterations and sometimes the error doesn't appear at all. The error is below:
```python
Traceback (most recent call last):
  File ""tools/train.py"", line 192, in <module>
  File ""tools/train.py"", line 188, in main
    meta=meta)
  File ""/home/sist/tqzouustc/code/mmdetection/mmdet/apis/train.py"", line 174, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/semaphore_tracker.py:55: UserWarning: semaphore_tracker: process died unexpectedly, relaunching.  Some semaphores might leak.
  warnings.warn('semaphore_tracker: process died unexpectedly, '
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 47, in train
    for i, data_batch in enumerate(self.data_loader):
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 517, in __next__
    data = self._next_data()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1182, in _next_data
    idx, data = self._get_data()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1148, in _get_data
    success, data = self._try_get_data()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/queues.py"", line 113, in get
    return _ForkingPickler.loads(res)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/multiprocessing/reductions.py"", line 282, in rebuild_storage_fd
    fd = df.detach()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/resource_sharer.py"", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/resource_sharer.py"", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 498, in Client
Traceback (most recent call last):
  File ""tools/train.py"", line 192, in <module>
    main()
  File ""tools/train.py"", line 188, in main
    meta=meta)
  File ""/home/sist/tqzouustc/code/mmdetection/mmdet/apis/train.py"", line 174, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 51, in train
    self.call_hook('after_train_iter')
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_runner.py"", line 307, in call_hook
    answer_challenge(c, authkey)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 742, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/semaphore_tracker.py:55: UserWarning: semaphore_tracker: process died unexpectedly, relaunching.  Some semaphores might leak.
  warnings.warn('semaphore_tracker: process died unexpectedly, '
    getattr(hook, fn_name)(self)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py"", line 37, in after_train_iter
    grad_norm = self.clip_grads(runner.model.parameters())
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py"", line 31, in clip_grads
    return clip_grad.clip_grad_norm_(params, **self.grad_clip)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py"", line 38, in clip_grad_norm_
    if clip_coef < 1:
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 41956) is killed by signal: Hangup. 
```
And the error is not the same every time. Sometimes it's like below:
```python
Traceback (most recent call last):
  File ""tools/train.py"", line 192, in <module>
  File ""tools/train.py"", line 188, in main
    meta=meta)
  File ""/home/sist/tqzouustc/code/mmdetection/mmdet/apis/train.py"", line 174, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
Traceback (most recent call last):
  File ""tools/train.py"", line 192, in <module>
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 47, in train
    for i, data_batch in enumerate(self.data_loader):
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 517, in __next__
    main()
  File ""tools/train.py"", line 188, in main
    meta=meta)
  File ""/home/sist/tqzouustc/code/mmdetection/mmdet/apis/train.py"", line 174, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py"", line 51, in train
    self.call_hook('after_train_iter')
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/base_runner.py"", line 307, in call_hook
    data = self._next_data()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1182, in _next_data
    idx, data = self._get_data()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1148, in _get_data
    success, data = self._try_get_data()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 986, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/queues.py"", line 113, in get
    getattr(hook, fn_name)(self)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py"", line 37, in after_train_iter
    grad_norm = self.clip_grads(runner.model.parameters())
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py"", line 31, in clip_grads
    return clip_grad.clip_grad_norm_(params, **self.grad_clip)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py"", line 36, in clip_grad_norm_
    return _ForkingPickler.loads(res)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/multiprocessing/reductions.py"", line 282, in rebuild_storage_fd
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py"", line 36, in <listcomp>
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py"", line 1376, in norm
    fd = df.detach()
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/resource_sharer.py"", line 57, in detach
    return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 5598) is killed by signal: Hangup. 
    with _resource_sharer.get_connection(self._id) as conn:
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/resource_sharer.py"", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 498, in Client
    answer_challenge(c, authkey)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 742, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""/home/sist/tqzouustc/.conda/envs/openmmlab/lib/python3.7/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
```
It looks like that the error is related to clip_grad. I use clip_grad in my config.py.
```python
# 1、使用refined proposal feature + graph reason enhanced feature重新cls和reg
# 2、使用80*4的reg结果，如果最高概率为背景类，取原proposal location为最终location，即偏移为0
# 3、构建图的edge时考虑feature similarity
# 4、将邻接矩阵ZZ^T(top32)归一化
# 5、高斯核的输出特征concate而不是加和到一起
# 6、设置self.root_weight=True，即加上自身特征
# 7、聚合特征时，将所有邻居节点特征取平均，加和loss会变成NAN。
# 8、高斯核数目由25改为16
# model settings
model = dict(
    type='SGRN',
    backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[.0, .0, .0, .0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    # 都从P5取feature
    # 自定义的extract_level: 使用多尺度feature map情况时，
    # 且只想在一个feature上做roi pooling，extract_level决定从哪个feature做roi pooling，顺序是浅层到深层，0,1,2...
    roi_head=dict(
        type='SGRNRoIHead1',
        num_stages=2,
        stage_loss_weights=[1, 0.5],
        t=32,
        extract_level=3,
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),

        bbox_head=[
            dict(
            type='BBoxHead',
            with_avg_pool=False,
            in_channels=1024,
            roi_feat_size=1,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),

            dict(
            type='BBoxHead',
            with_avg_pool=False,
            in_channels=1024 + 256,
            roi_feat_size=1,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0., 0., 0., 0.],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0))
            ]),
    # model training and testing settings
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        # nms过程和retinaNet不一样, 每个scale level单独做nms，然后所有结果合并
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=[
            dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSamplerFixnum',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
            dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSamplerFixnum',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
            ]),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100)
        # soft-nms is also supported for rcnn testing
        # e.g., nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05)
    ))

# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
# max_norm: 梯度的最大范数；norm_type: 范数类型，二范数。
# 如果梯度的二范数之和（记为sum_norm）大于max_norm，则将所有梯度乘以max_norm/sum_norm，即将梯度的二范数之和变成max_norm
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[6, 10, 14])#在第16和22个epoch将lr调整为原来的1/10
runner = dict(type='EpochBasedRunner', max_epochs=24)

checkpoint_config = dict(interval=1)
# yapf:disable
log_config = dict(
    interval=50,
    hooks=[
        dict(type='TextLoggerHook'),
        # dict(type='TensorboardLoggerHook')
    ])
# yapf:enable
custom_hooks = [dict(type='NumClassCheckHook')]

dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]

# data settings
# dataset settings
dataset_type = 'CocoDataset'
data_root = '/gpfs/home/sist/tqzouustc/dataset/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),#gt_bboxes和gt_labels在这里定义
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])#这里定义了dataloader每个batch含有的信息
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_train2017.json',
        img_prefix=data_root + 'train2017/',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/instances_val2017.json',
        img_prefix=data_root + 'val2017/',
        pipeline=test_pipeline))
evaluation = dict(interval=1, metric='bbox')

load_from = 'work_dirs/baseline_faster_rcnn_r101_fpn_2x_raw_coco/epoch_12_sgrn_pretrain.pth'
```
So what is the real cause of the error? I'm really confused. Thanks!"
A problem about kill process ,open-mmlab/mmdetection,2022-01-17 12:22:53,0,,7018,1105790911,"When I was getting the COCO bbox error results per category from bbox json file(run the coco_error_analysis.py),my computer became very laggy and would kill the pycharm process.How can I solve this problem.The operating system is Ubuntu20.04,the memory of the computer is 16G.Is it because I don't have enough memory？"
运行结果添加深度信息,open-mmlab/mmdetection,2022-01-16 14:09:23,0,,7013,1105062174,"当前使用mmdetection中的实例分割功能。
正在使用RGB-D摄像头。首先将获得的RGB图像传输给mmdetection图像检测工具获得结果。
以mask_rcnn为例，如何为获得的实例分割结果添加深度信息呢？"
"No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-10.2'",open-mmlab/mmdetection,2022-01-15 07:17:10,3,,7010,1104588455,"After I shut down my computer before incompletely killing all model training processes, this problem occurred.
I try to reinstall the cuda driver and the anaconda environment is same as [get_started.md](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/get_started.md), but this problem still occurres?
How can I solve?
Moreover, when I import torch
torch.cuda.is_available=False
----------------------------------------------------------------------------------------
我在一次训练模型进程未关闭完就把电脑关闭了，然后尝试重新训练时出现了这个问题，我尝试过了重装cuda驱动，且anaconda下的虚拟环境也是按照教程中的来搭建的，请问为何会出现这个问题?
而且当我导入pytorch时会出现以下问题：
torch.cuda.is_available=False"
"GN to BN in auto_assign head, mAP for coco drops 3%．",open-mmlab/mmdetection,2022-01-12 05:26:14,5,,6991,1099894923,"GN to BN in auto_assign head, mAP for coco drops 3%．Do you encounter this problem?"
尝试输出cascade_rcnn到pickle时出现【'ConfigDict' object has no attribute 'eval'】,open-mmlab/mmdetection,2022-01-11 03:23:27,3,,6984,1098607746,这个模型可以添加eval关键字吗？如果可以我需要怎么更改配置文件呢？
Saving the model with state dict BEFORE training,open-mmlab/mmdetection,2022-01-06 14:28:39,0,,6963,1095372734,"Hi, I am trying to save a model before training, because I want to check the change of gradients in certain layers. Sadly, I cant manage to save it in the same way as it is done by the hooks apparently: No matter if I save with torch.save(model. path) or runner._save_checkpoint(), the size of the model (in MB) differs vastly. Is there a way to save the whole model with weights before training?

Also, I always get ""missing keys in source state_dict"" and ""size mismatch for bbox_head.cls_branches.0.weight"" when loading a model if I try to load_from and also change the backbone. Am I making mistakes in defining the model? This is my config:

```
_base_ = '/home/fuerste/mmdetection/configs/deformable_detr/deformable_detr_r50_16x2_50e_coco.py'

model = dict(
    backbone=dict(
        type='ResNet',
        depth=101,
        num_stages=4,
        out_indices=(1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(type='Pretrained',
                      # checkpoint=""https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth"")),
                      checkpoint='torchvision://resnet101')),
    neck=dict(
        type='ChannelMapper',
        in_channels=[512, 1024, 2048],
        kernel_size=1,
        out_channels=256,
        act_cfg=None,
        norm_cfg=dict(type='GN', num_groups=32),
        num_outs=4),
    bbox_head=dict(
        type='DeformableDETRHead',
        num_classes=17))


dataset_type = 'COCODataset'
classes = (
    ('bobcat', 'opossum', 'empty', 'coyote', 'raccoon', 'bird',
     'dog', 'cat', 'squirrel', 'rabbit', 'skunk', 'rodent',
     'badger', 'deer', 'car', 'fox', 'animal')
)

img_scale = (1024, 600)
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=img_scale, keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=img_scale,
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]

load_from = '/home/fuerste/mmdetection/checkpoints/deformable_detr_r50_16x2_50e_coco_20210419_220030-a12b9512.pth'

samples_per_gpu = 2
workers_per_gpu = 1

data = dict(
    samples_per_gpu=samples_per_gpu,
    workers_per_gpu=workers_per_gpu,
    train=dict(
        img_prefix='/home/datasets/camera_traps/Caltech-Camera-Traps/CCT20-benchmark/eccv_18_all_images_sm',
        classes=classes,
        filter_empty_gt=False,
        ann_file='/home/fuerste/thesis_root/data/cct20/annotations/hybrid/_5_train_annotations.json',
        pipeline=train_pipeline),
    val=dict(
        img_prefix='/home/datasets/camera_traps/Caltech-Camera-Traps/CCT20-benchmark/eccv_18_all_images_sm',
        classes=classes,
        filter_empty_gt=False,
        separate_eval=True,
        ann_file=['/home/fuerste/thesis_root/data/cct20/annotations/hybrid/_5_cis_val_annotations.json',
                  '/home/fuerste/thesis_root/data/cct20/annotations/hybrid/_5_trans_val_annotations.json'],
        pipeline=test_pipeline),
    test=dict(
        img_prefix='/home/datasets/camera_traps/Caltech-Camera-Traps/CCT20-benchmark/eccv_18_all_images_sm',
        classes=classes,
        filter_empty_gt=False,
        separate_eval=True,
        ann_file=['/home/fuerste/thesis_root/data/cct20/annotations/hybrid/_5_cis_test_annotations.json',
                  '/home/fuerste/thesis_root/data/cct20/annotations/hybrid/_5_trans_test_annotations.json'],
        pipeline=test_pipeline))

log_config = dict(
    interval=10,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(
            type='WandbLoggerHook',
            init_kwargs=dict(
                project='hybrid testing',
                config={},
                tags=[""cct20"", ""hybrid naive""]
            ))])


workflow = [('train', 1), ('val', 1)]
runner = dict(type='EpochBasedRunner', max_epochs=1)
optimizer = dict(
    type='AdamW',
    lr=2e-5,
    weight_decay=0.0001,
    paramwise_cfg=dict(
        custom_keys={
            'backbone': dict(lr_mult=0.5),
            'sampling_offsets': dict(lr_mult=0.1),
            'reference_points': dict(lr_mult=0.1)
        }))

optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))

lr_config = dict(
    policy='step',
    step=[25],
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001)

```"
segmentation results,open-mmlab/mmdetection,2022-01-05 00:17:45,1,,6949,1093858411,"I want to get the x and y coordinates of segmentation results. Not like 'True' or 'False'.
Is it possible ? 

Thank you in advance :D"
"IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed",open-mmlab/mmdetection,2022-01-04 10:06:02,0,,6948,1093194502,"when i use:
python tools/test.py ./tutorial_exps/customformat_kitti_val.py /u01/zourui/mmdetection/tutorial_exps/latest.pth --eval mAP
load checkpoint from local path: /u01/zourui/mmdetection/tutorial_exps/latest.pth
have this problem:
The model and loaded state dict do not match exactly

unexpected key in source state_dict: roi_head.bbox_head.fc_cls.weight, roi_head.bbox_head.fc_cls.bias, roi_head.bbox_head.fc_reg.weight, roi_head.bbox_head.fc_reg.bias, roi_head.bbox_head.shared_fcs.0.weight, roi_head.bbox_head.shared_fcs.0.bias, roi_head.bbox_head.shared_fcs.1.weight, roi_head.bbox_head.shared_fcs.1.bias

[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 5823/5823, 20.0 task/s, elapsed: 292s, ETA:     0s
---------------iou_thr: 0.5---------------
multiprocessing.pool.RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 47, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/core/evaluation/mean_ap.py"", line 232, in tpfp_default
    det_bboxes, gt_bboxes, use_legacy_coordinate=use_legacy_coordinate)
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/core/evaluation/bbox_overlaps.py"", line 48, in bbox_overlaps
    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + extra_length) * (
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""tools/test.py"", line 234, in <module>
    main()
  File ""tools/test.py"", line 226, in main
    metric = dataset.evaluate(outputs, **eval_kwargs)
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/datasets/voc.py"", line 85, in evaluate
    use_legacy_coordinate=True)
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/core/evaluation/mean_ap.py"", line 379, in eval_map
    [use_legacy_coordinate for _ in range(num_imgs)]))
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 276, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 657, in get
    raise self._value
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed

please help me,thanks.

./tutorial_exps/customformat_kitti_val.py,this is:
model = dict(
    type='RPN',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=False),
        norm_eval=True,
        style='caffe',
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet50_caffe')),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0)))
dataset_type = 'VOCDataset'
data_root = '/u01/zourui/mmdataset/VOC/VOC2012/'
img_norm_cfg = dict(
    mean=[103.53, 116.28, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_label=False),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[103.53, 116.28, 123.675],
        std=[1.0, 1.0, 1.0],
        to_rgb=False),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='RepeatDataset',
        times=3,
        dataset=dict(
            type='VOCDataset',
            ann_file=[
                '/u01/zourui/mmdataset/VOC/VOC2012/./ImageSets/Main/train.txt',
                '/u01/zourui/mmdataset/VOC/VOC2012/./ImageSets/Main/train.txt'
            ],
            img_prefix=[
                '/u01/zourui/mmdataset/VOC/VOC2012/',
                '/u01/zourui/mmdataset/VOC/VOC2012/'
            ],
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True),
                dict(type='Resize', img_scale=(1000, 600), keep_ratio=True),
                dict(type='RandomFlip', flip_ratio=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size_divisor=32),
                dict(type='DefaultFormatBundle'),
                dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
            ]),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_label=False),
            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[103.53, 116.28, 123.675],
                std=[1.0, 1.0, 1.0],
                to_rgb=False),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes'])
        ],
        classes=[
            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',
            'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
            'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
        ]),
    val=dict(
        type='VOCDataset',
        ann_file=
        '/u01/zourui/mmdataset/VOC/VOC2012/./ImageSets/Main/train_val.txt',
        img_prefix='/u01/zourui/mmdataset/VOC/VOC2012/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[103.53, 116.28, 123.675],
                        std=[1.0, 1.0, 1.0],
                        to_rgb=False),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=[
            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',
            'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
            'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
        ]),
    test=dict(
        type='VOCDataset',
        ann_file='/u01/zourui/mmdataset/VOC/VOC2012/./ImageSets/Main/val.txt',
        img_prefix='/u01/zourui/mmdataset/VOC/VOC2012/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[103.53, 116.28, 123.675],
                        std=[1.0, 1.0, 1.0],
                        to_rgb=False),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=[
            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',
            'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
            'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
        ]))
evaluation = dict(interval=1, metric='proposal_fast')
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 11])
runner = dict(type='EpochBasedRunner', max_epochs=24)
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
classes = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',
    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]
thanks!


"
啥时候有yolor,open-mmlab/mmdetection,2022-01-04 03:10:52,0,community help wanted#feature request,6945,1092971313,啥时候会有yolor
how can i use the se_layer in mmdet/models/utils/se_layer after the backbond like resnet,open-mmlab/mmdetection,2022-01-03 04:34:51,1,Doc,6940,1092219865,
how to  convert yolact to onnx?,open-mmlab/mmdetection,2021-12-31 07:20:50,1,community help wanted#deployment,6935,1091454016,
PASCAL VOC segmentation support ,open-mmlab/mmdetection,2021-12-30 04:53:50,1,enhancement#feature request,6928,1090876898,"**Describe the feature**
Semantic segmentation on PASCAL VOC datatset
 
**Motivation**
PASCAL VOC is a great way to check model's sanity in a short amount of the time.
A lot of segmentation models have been using PASCAL VOC as a default evaluation protocol.
https://paperswithcode.com/sota/semantic-segmentation-on-pascal-voc-2012

**Related resources**
https://github.com/AlessandroSaviolo/Instance-Segmentation-using-Detectron2 (this is instance-segmentation btw)

**Additional context**
I know that  you guys have turned down the same feature request before, but I do believe it would help a lot of researchers and students who are not easy to train segmentation models with enough gpu resources and time.
"
inference_detector,open-mmlab/mmdetection,2021-12-29 08:06:20,2,,6922,1090341119,"'inference_detector' shows me the results that [xmin, ymin, xmax, ymax, class confidence score].
But can I get the x and y coordinates of 4 vertices of the box like [[x1 , y1], [x2 , y2], [x3 , y3], [x4 , y4]]?

Thank you, in advance:D"
mmdetection custom dataset training error,open-mmlab/mmdetection,2021-12-28 15:27:09,3,,6914,1089969419,"Hi all !
I am trying to train mmdetection with my custom dataset : 
here is my config file : 
```

# The new config inherits a base config to highlight the necessary modification
_base_ = 'mask_rcnn_x101_64x4d_fpn_mstrain-poly_3x_coco.py'

# We also need to change the num_classes in head to match the dataset's annotation
model = dict(
    roi_head=dict(
        bbox_head=dict(num_classes=1),
        mask_head=dict(num_classes=1)))

# Modify dataset related settings
dataset_type = 'COCODataset'
classes = ('foliole_0',)
data = dict(
    train=dict(
        img_prefix='dataset/train/',
        classes=classes,
        ann_file='dataset/train/via_region_data_coco.json'),
    val=dict(
        img_prefix='dataset/val/',
        classes=classes,
        ann_file='dataset/val/via_region_data_coco.json'),
    test=dict(
        img_prefix='dataset/val/',
        classes=classes,
        ann_file='dataset/val/via_region_data_coco.json'))

# We can use the pre-trained Mask RCNN model to obtain higher performance
#load_from = #'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'

```

and the error : 

```
(openmmlab) C:\Users\MASTER\Dropbox\Documents Sylvain\travail avec Boubacar\mmdetection-master>python tools/train.py configs\mask_rcnn\custom.py
'gcc' n’est pas reconnu en tant que commande interne
ou externe, un programme exécutable ou un fichier de commandes.
fatal: not a git repository (or any of the parent directories): .git
2021-12-28 16:20:16,525 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: win32
Python: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1
NVCC: Build cuda_11.1.relgpu_drvr455TC455_06.29190527_0
GCC: n/a
PyTorch: 1.8.0
PyTorch compiling details: PyTorch built with:
  - C++ Version: 199711
  - MSVC 192829337
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191125 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 2019
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -DNDEBUG -DUSE_FBGEMM -DUSE_XNNPACK, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON,

TorchVision: 0.9.0
OpenCV: 4.5.4
MMCV: 1.4.2
MMCV Compiler: MSVC 192930133
MMCV CUDA Compiler: 11.1
MMDetection: 2.19.1+
------------------------------------------------------------

2021-12-28 16:20:16,714 - mmdet - INFO - Distributed training: False
2021-12-28 16:20:16,902 - mmdet - INFO - Config:
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
dataset_type = 'COCODataset'
data_root = 'data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=True,
        poly2mask=False),
    dict(
        type='Resize',
        img_scale=[(1333, 640), (1333, 800)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='RepeatDataset',
        times=3,
        dataset=dict(
            type='CocoDataset',
            ann_file='data/coco/annotations/instances_train2017.json',
            img_prefix='data/coco/train2017/',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(
                    type='LoadAnnotations',
                    with_bbox=True,
                    with_mask=True,
                    poly2mask=False),
                dict(
                    type='Resize',
                    img_scale=[(1333, 640), (1333, 800)],
                    multiscale_mode='range',
                    keep_ratio=True),
                dict(type='RandomFlip', flip_ratio=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size_divisor=32),
                dict(type='DefaultFormatBundle'),
                dict(
                    type='Collect',
                    keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
            ]),
        img_prefix='dataset/train/',
        classes=('foliole_0', ),
        ann_file='dataset/train/via_region_data_coco.json'),
    val=dict(
        type='CocoDataset',
        ann_file='dataset/val/via_region_data_coco.json',
        img_prefix='dataset/val/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('foliole_0', )),
    test=dict(
        type='CocoDataset',
        ann_file='dataset/val/via_region_data_coco.json',
        img_prefix='dataset/val/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        classes=('foliole_0', )))
evaluation = dict(interval=1, metric=['bbox', 'segm'])
optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[9, 11])
runner = dict(type='EpochBasedRunner', max_epochs=12)
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ResNeXt',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch',
        init_cfg=dict(
            type='Pretrained', checkpoint='open-mmlab://resnext101_64x4d'),
        groups=64,
        base_width=4),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=1,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=1,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
classes = ('foliole_0', )
work_dir = './work_dirs\custom'
gpu_ids = range(0, 1)

2021-12-28 16:20:16,904 - mmdet - INFO - Set random seed to 335105629, deterministic: False
2021-12-28 16:20:17,458 - mmdet - INFO - initialize ResNeXt with init_cfg {'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnext101_64x4d'}
2021-12-28 16:20:17,458 - mmcv - INFO - load model from: open-mmlab://resnext101_64x4d
2021-12-28 16:20:17,459 - mmcv - INFO - load checkpoint from openmmlab path: open-mmlab://resnext101_64x4d
2021-12-28 16:20:17,636 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2021-12-28 16:20:17,648 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2021-12-28 16:20:17,651 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
loading annotations into memory...
Traceback (most recent call last):
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmcv\utils\registry.py"", line 52, in build_from_cfg
    return obj_cls(**args)
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmdet\datasets\custom.py"", line 92, in __init__
    self.data_infos = self.load_annotations(local_path)
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmdet\datasets\coco.py"", line 50, in load_annotations
    self.coco = COCO(ann_file)
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmdet\datasets\api_wrappers\coco_api.py"", line 23, in __init__
    super().__init__(annotation_file=annotation_file)
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\pycocotools\coco.py"", line 81, in __init__
    with open(annotation_file, 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'data/coco/annotations/instances_train2017.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tools/train.py"", line 185, in <module>
    main()
  File ""tools/train.py"", line 161, in main
    datasets = [build_dataset(cfg.data.train)]
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmdet\datasets\builder.py"", line 69, in build_dataset
    build_dataset(cfg['dataset'], default_args), cfg['times'])
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmdet\datasets\builder.py"", line 81, in build_dataset
    dataset = build_from_cfg(cfg, DATASETS, default_args)
  File ""C:\Users\MASTER\.conda\envs\openmmlab\lib\site-packages\mmcv\utils\registry.py"", line 55, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
FileNotFoundError: CocoDataset: [Errno 2] No such file or directory: 'data/coco/annotations/instances_train2017.json'

(openmmlab) C:\Users\MASTER\Dropbox\Documents Sylvain\travail avec Boubacar\mmdetection-master>pause
Appuyez sur une touche pour continuer...
```

thank you for helping me !
Kind regards !"
"loss_cls: 0.0000, loss_bbox: 0.0000 when training YOLOX on my own dataset",open-mmlab/mmdetection,2021-12-28 13:05:42,16,,6911,1089883305,"我用YOLOX训练自己的数据集的时候一直显示loss_cls: 0.0000, loss_bbox: 0.0000，然后测试的时候会有ERROR The testing results of the whole dataset is empty 
请问可能是什么原因？"
shuffling in Concatenate datasets,open-mmlab/mmdetection,2021-12-28 11:53:10,0,,6910,1089840222,"according to documentation, there are three ways to concatenate datasets. 
my question is how the shuffling works in each scenario. I have two coco-like datasets and I like to train them sequentially so the model first trains on the first dataset and then the second one. Is it possible? which way should I use?"
TypeError: 'NoneType' object is not iterable for mask_target.py,open-mmlab/mmdetection,2021-12-28 06:58:14,6,,6907,1089657328,"Thanks for your error report and we appreciate it a lot.

**Checklist**

1. I have searched related issues but cannot get the expected help.
2. I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. The bug has not been fixed in the latest version.

**Describe the bug**
A clear and concise description of what the bug is.

**Reproduction**

1. What command or script did you run?

```
I am using ICEVISION to call MMDET to invoke SCNet on pennfudan dataset under ICEDATA. I am getting the error below when running on google colab:

<ipython-input-26-d81c6bd29d71> in <module>()
----> 1 learn.lr_find()

22 frames

/content/mmdetection/mmdet/core/mask/mask_target.py in mask_target(pos_proposals_list, pos_assigned_gt_inds_list, gt_masks_list, cfg)
     58     cfg_list = [cfg for _ in range(len(pos_proposals_list))]
     59     mask_targets = map(mask_target_single, pos_proposals_list,
---> 60                        pos_assigned_gt_inds_list, gt_masks_list, cfg_list)
     61     mask_targets = list(mask_targets)
     62     if len(mask_targets) > 0:

TypeError: 'NoneType' object is not iterable

I have followed the issue #3767 https://github.com/open-mmlab/mmdetection/issues/3767 (CLOSED) to ensure that 'with_seg' is set to FALSE in htc_r50_fpn_1x_coco.py and the semantic_head and semantic_roi_extractor is set to None.

When running the lr_find using fastai library, I am getting the above error.

[https://colab.research.google.com/drive/1si9YBEt6NTqnLrc0nFrkb7owr7lxvkQ0?usp=sharing](url) The google colab file has the requisite steps to get to the issue.



```

2. Did you make any modifications on the code or config? Did you understand what you have modified?
Followed the issue closed #3767 to make the required changes and validate it using model.cfg command

3. What dataset did you use?
pennfudan dataset under ICEDATA
**Environment**

1. Please run `python mmdet/utils/collect_env.py` to collect necessary environment information and paste it here.
2. You may add addition that may be helpful for locating the problem, such as
    - How you installed PyTorch [e.g., pip, conda, source]
    - Other environment variables that may be related (such as `$PATH`, `$LD_LIBRARY_PATH`, `$PYTHONPATH`, etc.)

```none
<ipython-input-26-d81c6bd29d71> in <module>()
----> 1 learn.lr_find()

22 frames

/content/mmdetection/mmdet/core/mask/mask_target.py in mask_target(pos_proposals_list, pos_assigned_gt_inds_list, gt_masks_list, cfg)
     58     cfg_list = [cfg for _ in range(len(pos_proposals_list))]
     59     mask_targets = map(mask_target_single, pos_proposals_list,
---> 60                        pos_assigned_gt_inds_list, gt_masks_list, cfg_list)
     61     mask_targets = list(mask_targets)
     62     if len(mask_targets) > 0:

TypeError: 'NoneType' object is not iterable

```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
如何在框架下实现半监督训练？,open-mmlab/mmdetection,2021-12-27 12:59:38,2,,6904,1089226575,如题，是否需要额外的包？
Questions on handling images of different sizes with DataContainer,open-mmlab/mmdetection,2021-12-27 09:34:41,1,,6901,1089107603,"Hi! I was wondering how images of different sizes are stacked together in a batch. I had a feeling that the content is copied to the top-left corner of a padded image tensor, and noticed that `DataContainer` is designed to deal with this. However `DataContainer` looks like a bare skeleton that doesn't have many operations defined, and I don't know where to find the collate function after `Collect` in the pipeline. How does the collate function work? And could you point me to the right place to check the implementation?

Thank you!"
Image normalization on my own dataset. ,open-mmlab/mmdetection,2021-12-27 09:24:00,3,,6900,1089100380,"Hello, 

I have a doubt about image normalization. 
I have my own dataset in COCO format for instance segmentation. 

I tried DCN. https://github.com/open-mmlab/mmdetection/blob/master/configs/dcn/cascade_mask_rcnn_r101_fpn_dconv_c3-c5_1x_coco.py

So the config comes with its own img_norm_cfg.
But my dataset has following values:
```
img_norm_cfg = dict(
    mean=[128, 128, 128], std=[11.578, 11.578, 11.578], to_rgb=True)
```

So when I am fine-tuning on my own dataset do I need to use the default img_norm_config or the values calculated from my own dataset? 

Will it have an impact on the results.? "
How to ensure the correctness,open-mmlab/mmdetection,2021-12-27 07:50:05,3,,6898,1089041802,"Hi, I'm very curious that how do you ensure the correctness of all the algorithms in MMDetection when update the framework? I've seen the `test` fold in the root python, but can they cover all the border situations and all the modules?"
"Will ""Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details"" be implemented for promoting research on the long-tail datasets?",open-mmlab/mmdetection,2021-12-27 02:39:58,1,feature request,6895,1088917381,"**Describe the feature**
Implement new long-tail dataset evaluation metrics ""APpool"" and ""APfix"" in paper ""Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details"".

**Motivation**
The new evaluation metrics ""APpool"" and ""APfix"" are proposed in this paper for evaluating the performance of different methods  on the long-tail datasets like LVIS. We think implement these metrics will effectively promote the study of the long-tail problem.

**Related resources**
You can look at this repo of large-vocab-devil based on Detectron2, which is not supported in mmdetection.
https://github.com/achalddave/large-vocab-devil

**Additional context**
It really worths to be implemented. After that different models can be evaluated more comprehensively on long-tail datasets such as LVIS.
"
Parameter scope for PhotoMetricDistortion,open-mmlab/mmdetection,2021-12-27 02:32:16,1,,6894,1088915391,"```python
import torchvision.transforms as T
T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
```

其中的brightness和hue在mmdet里面是整数范围：

```python
dict(
        type='PhotoMetricDistortion',
        brightness_delta=32,
        contrast_range=(0.5, 1.5),
        saturation_range=(0.5, 1.5),
        hue_delta=18),
```

如果要对应上述参数，应该设置为多少呢？"
Issue with 'inference_detector' in MMDetection,open-mmlab/mmdetection,2021-12-26 19:33:53,6,,6891,1088831702,"I am trying to work with the Mask RCNN with SWIN Transformer as the backbone and have tried some changes to the model (using quantization/pruning, etc) . All of these work fine and I can see the required changes in my model and now I wanted to run an inference with the same on a single image.

I initially load/initialize the model using the below code : 

```
!wget -c https://download.openmmlab.com/mmdetection/v2.0/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth \
   -O checkpoints/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth

from mmdet.apis import init_detector, inference_detector

config='/content/mmdetection/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py'
checkpoint = 'checkpoints/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth'
model = init_detector(config,checkpoint)
```


The code that I am trying to run the inference with is :  

```
config_file = '/content/mmdetection/configs/swin/mask_rcnn_swin-t-p4-w7_fpn_1x_coco.py'
checkpoint_file = '/content/mmdetection/checkpoints/model_complete.pth'

model = init_detector(config_file, checkpoint_file, device='cpu')


img = '/content/mmdetection/Datasets/cocodataset/val2017/val2017/000000100274.jpg'  # or img = mmcv.imread(img), which will only load it once
result = inference_detector(model, img)
model.show_result(img, result)
model.show_result(img, result, out_file='result.jpg')
```



The above code runs just fine when I try to use the checkpoint file 'mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth' that  I used to initialize the model in the first part of the code snippet. However, when I try to use the models that I have created/changed it does not seem to work. It throws an error saying : 
`
""RuntimeError: No state_dict found in checkpoint file /content/mmdetection/checkpoints/model_complete.pth""
`
I have tried to work with state_dicts as well but then it throws warnings that suggest that it cannot find the classes for this. 

On comparing the checkpoint file that gets downloaded in the initial part and the way my state_dict or the complete model gets saved using Pytorch are totally different. Is there a specific way that I should be saving my model or is there a workaround for the same? Any help with this would be great."
How to install a previous version of mmdet?,open-mmlab/mmdetection,2021-12-26 03:51:43,0,,6889,1088693419,"I would like to use a previous version of mmdet. Specifically:
```
- mmcv==0.6.2
- mmdet==2.2.1 
```

I have read the documents in https://mmdetection.readthedocs.io/en/latest/get_started.html#installation. However, I failed to find an instruction to install mmdet in a previous version after installing mcv==0.6.2.

I have tried to use ```   pip install mmdet ``` ,  ```   mim install mmdet ``` or clone the repository and then install it. But in all of these ways, the latest version of mmdet is installed.

Could you please offer an operation to guide me on how to install this? Thanks for your time!
"
export torchscript,open-mmlab/mmdetection,2021-12-25 07:20:33,0,,6888,1088560851,Will you consider supporting torchscript deployment?
Memory leak during testing,open-mmlab/mmdetection,2021-12-25 05:27:39,5,,6887,1088549750,"I used swin-transformer objection detection, test and selected the parameters --eval bbox --show --show-dir xxx to generate my prediction images. The test set is about 3,000. My computer's memory is 48g,the operating system is ubuntu 18.04.
When predicting the first picture, the memory usage of the process is about 9.2%. By the time it was more than two thousand, it had filled up all the memory and the computer crashed.

I found that when I commented out this code in mmdet.models.detectors.base.py, the memory leak problem disappeared

img = imshow_det_bboxes(
   img,
   bboxes,
   labels,
   segms,
   class_names=self.CLASSES,
   score_thr=score_thr,
   bbox_color=bbox_color,
   text_color=text_color,
   mask_color=mask_color,
   thickness=thickness,
   font_size=font_size,
   win_name=win_name,
   show=show,
   wait_time=wait_time,
   out_file=out_file)

It seems that the problem lies in the imshow_det_bboxes function of mmdet.core.visualization.image.py
Has anyone else encountered this problem?
"
Possible Bug:multiclass_nms,open-mmlab/mmdetection,2021-12-24 15:43:03,0,,6886,1088432992,"This is not returning indices as it suppose to if we pass return index=True. This is a provision in multiclass_nms but default to False, now its causing issue in running the evaluation for DSC ,which needs.
```
   if cfg is None:
            return bboxes, scores
        else:
            det_bboxes, det_labels = multiclass_nms(bboxes, scores,
                                                    cfg.score_thr, cfg.nms,
                                                    cfg.max_per_img)

            return det_bboxes, det_labels
```"
Is eval_recalls(proposal_nums = 1) same as COCO ARmax=1?? ,open-mmlab/mmdetection,2021-12-24 12:37:34,0,,6884,1088361230,Would eval_recalls (in mmdetection/mmdet/core/evaluation/recall.py) return same results as COCO ARmax = 1 if I use it with proposal_nums = 1 ??
YOLOv3 with bigger res.,open-mmlab/mmdetection,2021-12-24 10:10:20,0,,6882,1088291611,"I want to use [YOLO Lightweight models]
(https://github.com/open-mmlab/mmdetection/tree/master/configs/yolo) with a **bigger resolution (1280)** (my application is very resolution depedent.)

- Can I reuse the mentionned config, and just change the imput resolution of the image in the pipeline ? Or should I also modify the anchors size and maybe other things ? 

- Can I reuse the the pretrained backbone, or even eventually the full pretrained model ? 
"
function eval_map in file mean_ap.py is very slow,open-mmlab/mmdetection,2021-12-24 08:47:01,0,,6879,1088246867,It takes about 1s to calculate mAP for a single image under a single iou_thresh.
The class is changed and displayed on the image saved as a result.,open-mmlab/mmdetection,2021-12-24 05:44:09,0,,6875,1088165232,"I trained using the mask rcnn model and printed out the test results.
the training began after modifying the class to ""CLASSES = ('total', 'fork', 'body')"" at mmdetection/mmdet/datasets/coco.py.
However, when I looked at the output image, total and body switched and were output on the box.
Why is that? How can I solve this?"
about pre_process,open-mmlab/mmdetection,2021-12-23 12:19:38,2,,6870,1087666748,我想把数据预处理部分的nor注释掉，但是报错了，这是为什么啊，按理说，数据预处理的方法是可以自定义的啊，根据错误我也没有彻底解决，官方能指点下吗
Show detected results is only support non distributed mode,open-mmlab/mmdetection,2021-12-23 07:11:07,0,,6866,1087441383,"The visualization tool only supports non distributed configuration, which is very slow. Can it be accelerated？

https://github.com/open-mmlab/mmdetection/blob/master/tools/test.py#L199


可视化工具仅仅支持非分布式配置，速度很慢，能加速吗

"
mmlab设计的第三方包decord(视频快速解码)存在内存泄露,open-mmlab/mmdetection,2021-12-23 03:24:26,0,,6859,1087340871,"详见 seek_accurate, memory usage · Issue #157 · dmlc/decord
https://github.com/dmlc/decord/issues/157
"
Does mm detection allows to detect persons class only by using default weight files?,open-mmlab/mmdetection,2021-12-22 19:35:43,0,,6857,1087129224,
Wrong links in `configs/faster_rcnn/README.md`,open-mmlab/mmdetection,2021-12-22 13:09:51,0,,6856,1086807484,"**Describe the bug**
I found these links of config in [this table](https://github.com/open-mmlab/mmdetection/tree/master/configs/faster_rcnn#different-regression-loss) are same. It might be something wrong. Please fix them and thoroughly check other links in this file?
"
"IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed",open-mmlab/mmdetection,2021-12-22 11:37:30,0,,6855,1086736535,"when I use: python tools/test.py ./tutorial_exps/customformat_kitti_val.py ./uploads/saved_dict/e876d0e63b94c62131244e46c4b90809.pth --show-dir ./result  --eval mAP
have this problem:
load checkpoint from local path: ./uploads/saved_dict/e876d0e63b94c62131244e46c4b90809.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: roi_head.bbox_head.fc_cls.weight, roi_head.bbox_head.fc_cls.bias, roi_head.bbox_head.fc_reg.weight, roi_head.bbox_head.fc_reg.bias, roi_head.bbox_head.shared_fcs.0.weight, roi_head.bbox_head.shared_fcs.0.bias, roi_head.bbox_head.shared_fcs.1.weight, roi_head.bbox_head.shared_fcs.1.bias

[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 25/25, 10.2 task/s, elapsed: 2s, ETA:     0s
---------------iou_thr: 0.5---------------
multiprocessing.pool.RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 47, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/core/evaluation/mean_ap.py"", line 232, in tpfp_default
    det_bboxes, gt_bboxes, use_legacy_coordinate=use_legacy_coordinate)
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/core/evaluation/bbox_overlaps.py"", line 46, in bbox_overlaps
    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + extra_length) * (
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""tools/test.py"", line 236, in <module>
    main()
  File ""tools/test.py"", line 228, in main
    metric = dataset.evaluate(outputs, **eval_kwargs)
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/datasets/custom.py"", line 315, in evaluate
    logger=logger)
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmdet-2.19.0-py3.7.egg/mmdet/core/evaluation/mean_ap.py"", line 379, in eval_map
    [use_legacy_coordinate for _ in range(num_imgs)]))
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 276, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File ""/u01/zourui/anaconda3/envs/open-mmlab/lib/python3.7/multiprocessing/pool.py"", line 657, in get
    raise self._value
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed

don't know why,when I use --eval recall,it can normal output,but --eval mAP is can't.
Who knows thank you in advance."
Plan to support AI-TODs and VisDrone2019??,open-mmlab/mmdetection,2021-12-22 06:41:47,1,enhancement,6853,1086504457,"Is there any plan to support [AI-TODS](https://drive.google.com/file/d/1IiTp7gilwDCGr8QR_H9Covz8aVK7LXiI/view) or [VisDrone2019](http://aiskyeye.com/) dataset? 

If so, I think I could create a pull request. 

But in case it is not included in the MMDet roadmap, can you explain the policy to choose for the supported dataset?

Thanks!"
I want to see val loss on tensorboard but face with error TypeError: loss() missing 1 required positional argument: 'img_metas',open-mmlab/mmdetection,2021-12-21 12:19:14,4,bug,6851,1085774069,"In config file , workflow = [('train', 1)] , I change it to workflow = [('train', 1),('val',1)],hope to see val loss on tensorboard , but after one train epoch , val epoch begin , get error:TypeError: loss() missing 1 required positional argument: 'img_metas'.


Detail:
2021-12-21 12:11:29,428 - mmdet - INFO - Evaluating bbox...
Loading and preparing results...
2021-12-21 12:11:29,430 - mmdet - ERROR - The testing results of the whole dataset is empty.
2021-12-21 12:11:29,448 - mmdet - INFO - Exp name: small_val_loss.py
2021-12-21 12:11:29,449 - mmdet - INFO - Epoch(val) [1][1534]	
/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/utils/registry.py:252: UserWarning: The old API of register_module(module, force=False) is deprecated and will be removed, please use the new API register_module(name=None, force=False, module=None) instead.
  'The old API of register_module(module, force=False) '
/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/utils/registry.py:252: UserWarning: The old API of register_module(module, force=False) is deprecated and will be removed, please use the new API register_module(name=None, force=False, module=None) instead.
  'The old API of register_module(module, force=False) '
/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/utils/registry.py:252: UserWarning: The old API of register_module(module, force=False) is deprecated and will be removed, please use the new API register_module(name=None, force=False, module=None) instead.
  'The old API of register_module(module, force=False) '
/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/utils/registry.py:252: UserWarning: The old API of register_module(module, force=False) is deprecated and will be removed, please use the new API register_module(name=None, force=False, module=None) instead.
  'The old API of register_module(module, force=False) '
Traceback (most recent call last):
  File ""./tools/train.py"", line 189, in <module>
    main()
  File ""./tools/train.py"", line 185, in main
    meta=meta)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/apis/train.py"", line 177, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/opt/conda/envs/mmdetection/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/epoch_based_runner.py"", line 67, in val
    self.run_iter(data_batch, train_mode=False)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/epoch_based_runner.py"", line 32, in run_iter
    outputs = self.model.val_step(data_batch, self.optimizer, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/parallel/distributed.py"", line 94, in val_step
    output = self.module.val_step(*inputs[0], **kwargs[0])
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/detectors/base.py"", line 253, in val_step
    losses = self(**data)
  File ""/opt/conda/envs/mmdetection/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/fp16_utils.py"", line 98, in new_func
    return old_func(*args, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/detectors/base.py"", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/detectors/single_stage.py"", line 84, in forward_train
    gt_labels, gt_bboxes_ignore)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/dense_heads/base_dense_head.py"", line 326, in forward_train
    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/fp16_utils.py"", line 186, in new_func
    return old_func(*args, **kwargs)
TypeError: loss() missing 1 required positional argument: 'img_metas'
Traceback (most recent call last):
  File ""./tools/train.py"", line 189, in <module>
    main()
  File ""./tools/train.py"", line 185, in main
    meta=meta)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/apis/train.py"", line 177, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/epoch_based_runner.py"", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/opt/conda/envs/mmdetection/lib/python3.7/site-packages/torch/autograd/grad_mode.py"", line 26, in decorate_context
    return func(*args, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/epoch_based_runner.py"", line 67, in val
    self.run_iter(data_batch, train_mode=False)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/epoch_based_runner.py"", line 32, in run_iter
    outputs = self.model.val_step(data_batch, self.optimizer, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/parallel/distributed.py"", line 94, in val_step
    output = self.module.val_step(*inputs[0], **kwargs[0])
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/detectors/base.py"", line 253, in val_step
    losses = self(**data)
  File ""/opt/conda/envs/mmdetection/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/fp16_utils.py"", line 98, in new_func
    return old_func(*args, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/detectors/base.py"", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/detectors/single_stage.py"", line 84, in forward_train
    gt_labels, gt_bboxes_ignore)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmdetection/mmdet/models/dense_heads/base_dense_head.py"", line 326, in forward_train
    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
  File ""/data/E/personal/liang/Traffic_Sign_Detection/mmcv/mmcv/runner/fp16_utils.py"", line 186, in new_func
    return old_func(*args, **kwargs)
TypeError: loss() missing 1 required positional argument: 'img_metas'
Traceback (most recent call last):
  File ""/opt/conda/envs/mmdetection/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/envs/mmdetection/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/envs/mmdetection/lib/python3.7/site-packages/torch/distributed/launch.py"", line 260, in <module>
    main()
  File ""/opt/conda/envs/mmdetection/lib/python3.7/site-packages/torch/distributed/launch.py"", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/mmdetection/bin/python', '-u', './tools/train.py', '--local_rank=1', 'work_dirs/from_resnet/vfnet_xiangguan/vfnet_1x/small_val_loss.py', '--launcher', 'pytorch']' returned non-zero exit status 1.

"
Can use SeesawLoss for one-stage dectectors?,open-mmlab/mmdetection,2021-12-21 08:18:11,0,community help wanted,6848,1085557553,"Hi, dear authors!
Sorry to bother you!
Can SeesawLoss be used for one-stage dectectors? Like ATSS?
If so, how do I use it?
Thank you!"
"Test the validation set at the 1250th iteration, instead of the 7330th.",open-mmlab/mmdetection,2021-12-21 08:02:14,1,,6847,1085545037,"I am training the model with 4 GPUs, 4 images on each GPU, so the “iter” of the model when tested on the validation set should be 7330. But why my “iter” is 1250?
"
有没有办法直接使用内存中的数据而不是先读取图像文件呢？,open-mmlab/mmdetection,2021-12-21 02:14:15,1,,6844,1085365694,我的图片非常大，需要先切成几千个小块再预测。硬盘的 IO 非常慢，以至于模型在现实中不可用。有没有办法直接对内存里的图片进行预测呢？
建议一种比较优雅的框架写法，建议重写mmdet,open-mmlab/mmdetection,2021-12-20 13:46:22,1,,6842,1084811247,"灵感来自于allennlp框架。https://github.com/allenai/allennlp
--------------------------------


    from typing import Dict
    import torch
    import torch.nn as nn
    from common import Registrable
    
    
    class Backbone(torch.nn.Module, Registrable):
        def forward(self, **kwargs) -> Dict[str, torch.Tensor]:
            raise NotImplementedError
    
    
    @Backbone.register(""resnet"", constructor=""constructor"")
    class ResNet(Backbone):
        def __init__(self, input_channel, n_class) -> None:
            super().__init__()
            self.input_channel = input_channel
            self.n_class = n_class
            pass
    
        def forward(self, x):
            pass
    
        @classmethod
        def constructor(cls, input_channel, n_class):
            return cls(input_channel, n_class)
        
        @classmethod
        def from_file(config):
            # yaml构造config
            return config.cls(config.input_channel, config.n_class)
    
    
    class SampleModule(nn.Module):
        def __init__(self, backbone: Backbone) -> None:
            super().__init__()
            self.backbone = backbone
    
        def forward(self, x):
            x = self.backbone(x)
            return x
    
    backbone = ResNet(3, 80)
    backbone = ResNet.from_file('123.yaml')
    net = SampleModule(backbone)"
能否重写一版不套娃的mmdet框架，方便转模型和研究,open-mmlab/mmdetection,2021-12-20 13:25:41,1,,6840,1084790994,"能否重写一版不套娃的mmdet框架，方便转模型和研究.
模型基类都是nn.Module，子模型用self.成员保存，方便研究模型和转换tensorrt"
能否提供可以转tensorrt的cascade rcnn模型?,open-mmlab/mmdetection,2021-12-20 13:24:30,1,,6839,1084789812,"能否提供可以转tensorrt的cascade rcnn模型?建议重写roi head ,rpn head部分。这两部分转换不了tensorrt"
AttributeError: 'ECAlayer' object has no attribute 'init_weights',open-mmlab/mmdetection,2021-12-20 08:16:14,1,,6836,1084488039,"I am trying to add some modules into other people's model, which doesn't exist in the mmdet model right now, however, when  I ran the new model after adding attention module, the system always remind me this error.
AttributeError: 'ECAlayer' object has no attribute 'init_weights'
I find some similar issues in guthub, and the solution is to update mmcv, but since the model I used is based on an old version(mmcv 0.5.9), if I update it to the latest one, then the whole net seems can't run any longer. So What else can I do to debug it, or updating is the only way?

> Traceback (most recent call last):
  File ""../tools/train.py"", line 161, in <module>
    main()
  File ""../tools/train.py"", line 133, in main
    model = build_detector(
  File ""/home/bnu208/r3det-on-mmdetection-master/mmdet/models/builder.py"", line 48, in build_detector
    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))
  File ""/home/bnu208/r3det-on-mmdetection-master/mmdet/models/builder.py"", line 20, in build
    return build_from_cfg(cfg, registry, default_args)
  File ""/home/bnu208/anaconda3/lib/python3.8/site-packages/mmcv/utils/registry.py"", line 168, in build_from_cfg
    return obj_cls(**args)
  File ""/home/bnu208/r3det-on-mmdetection-master/mmdet/models/detectors/r3det.py"", line 49, in __init__
    self.init_weights(pretrained=pretrained)
  File ""/home/bnu208/r3det-on-mmdetection-master/mmdet/models/detectors/r3det.py"", line 57, in init_weights
    m.init_weights()
  File ""/home/bnu208/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1177, in __getattr__
    raise AttributeError(""'{}' object has no attribute '{}'"".format(
AttributeError: 'ECAlayer' object has no attribute 'init_weights'
"
Please remove pycocotools-windows on requirements since this lib doen't actively maintained caused Python3.9 users unable install on wndows!,open-mmlab/mmdetection,2021-12-20 06:51:14,5,,6835,1084424854,Please remove pycocotools-windows on requirements since this lib doen't actively maintained caused Python3.9 users unable install on windows!
edit mmdet saved checkpoint pth file's meta data,open-mmlab/mmdetection,2021-12-19 11:57:14,1,enhancement#upstream,6829,1084080632,"Hi, I have trained a checkpoint file, I currently changed the custom_imports paths, but checkpoints saved one doesn't changed.

I want load the checkpoint back, and met the problem:

1). How to edit pth meta, and assign new custom_imports paths;
2). Why load checkpoint read meta, but override config paths. (in this case, I changed config file and old pth can not be loaded raise import error).. **this doesn make any sense, since pth should only concern about weights, not import paths. Even PyTorch vanilla pth, will not include import paths in their weights names.**"
Filter annotations in dataset by an (annotation_area  / image_area) ratio threshold ,open-mmlab/mmdetection,2021-12-18 12:02:48,0,enhancement,6826,1083818122,"**Describe the feature**
Change an integer threshold for annotations filtering to a float ratio threshold based on annotation_area  to image_area ratio.

**Motivation**
Natural datasets may provide annotations of objects too hard to recognize. MMDet contains different datasets wrappers, e.g. MS COCO, where filtering removes annotation which have width or height less than an integer threshold, e.g. 32 like here ([code link](https://github.com/open-mmlab/mmdetection/blob/83eea4bec684708171f0b6433636df0873e50dac/mmdet/datasets/coco.py#L99)). 
Taking into account wide range of possible images sizes, this threshold seems to be rigid meanwhile the filtering is not a big deal in some tasks. 

"
"Suggest for changing ""diagonal flip""",open-mmlab/mmdetection,2021-12-18 06:46:11,1,bug,6824,1083763652,"The implementation of diagonal flip seems not an actual diagonal flip. To my understanding it is horizontal + vertical flip (H+V flip). See below.

https://github.com/open-mmlab/mmdetection/blob/83eea4bec684708171f0b6433636df0873e50dac/mmdet/datasets/pipelines/transforms.py#L411-L417

The difference between H+V flip and diagonal flip is shown in figure below. I suggest to change the ``flip_direction`` from ``diagonal`` to ``hor_ver`` or ``both`` for consistency between implementation and intuition. 

![266357577_1226950841124669_6673878699863538688_n](https://user-images.githubusercontent.com/20570053/146632141-3cbfe910-fd5b-45f8-91ba-e9989c3b44e7.jpg)

"
how to set gpu memory limit in mmdetection,open-mmlab/mmdetection,2021-12-17 07:33:54,1,,6818,1082970085,"hi,in tensorflow ,we can use config.gpu_options to set the gpu memory limit, so how can I set the limit in mmdetection.For example,there are two person use one gpu together,i cannot let my worker use the100% gpu memory.Thanks for your reply!!"
The training speed with InfiniBand ,open-mmlab/mmdetection,2021-12-17 07:23:53,1,community help wanted,6817,1082963417,"I updated our device from ethernet to InfiniBand, and expected there would be a huge boost in training speed. Unfortunately, it did not happen. 

Have you tested your training with InfiniBand? How the training speed is? 

Are there any extra steps for using InfiniBand? In my case, I just test the connection speed with ""ping"", and used it the same way as using ethernet."
Mobilenet v3,open-mmlab/mmdetection,2021-12-14 16:48:17,2,enhancement,6788,1079989745,"I think it is a good idea to add mobilenet v3
It is better than mobilenet v2 and more efficient than most backbones present here.
Are you planning on adding it soon ?

"
Is there a way to weigh losses by class?,open-mmlab/mmdetection,2021-12-11 11:19:50,3,,6759,1077508254,"I am trying to weigh the contribution of predictions according to their class. For example, a detection of class a should not have as big a impact on learning as a detection of class b.

I looked into the losses and the weighted_loss decorator, but there is no class information in there and I did not find anything else. Is there a way to weigh the losses according to class in configs?"
strange behaviour in log analysis graphs ,open-mmlab/mmdetection,2021-12-10 15:14:38,0,,6757,1076950621,"Hello, 

first of all i wanted to say tank you to all the team for this amazing framework. 

I'm training for object detection a mask rcnn detector with swin t transformers backbone on my custom dataset, which i converted into coco style dataset.
 
The training proceeds with no issue but i'm having trouble understanding why when i try to plot the accuracy and loss graphs (using the LOG ANALYSIS tool)  the curve seems to 'go back' on previous iterations(see the graphs below).
I think the curve 'goes back' after training one epoch due to the fact that it trained for 6 epochs and the curve has this strange behaviour for 6 times

any suggestions on why the logged metrics exibit this behaviour?
 
![Screenshot from 2021-12-10 16-07-29](https://user-images.githubusercontent.com/27857152/145595757-c2a6b103-a9a9-4415-87a9-b5049eb61d55.png)
![Screenshot from 2021-12-10 16-07-41](https://user-images.githubusercontent.com/27857152/145595781-fdb4ac71-fdf9-4c32-9f02-135fae0a2b37.png)


my full configuration is this:

```
my_classes = ('passaggio', 'paratia_aperta', 'paratia_semiaperta',
              'paratia_chiusa', 'canale_vuoto', 'canale_con_acqua',
              'occlusione')
n_classes = 7
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='SwinTransformer',
        embed_dims=96,
        depths=[2, 2, 6, 2],
        num_heads=[3, 6, 12, 24],
        window_size=7,
        mlp_ratio=4,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.2,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        with_cp=False,
        convert_weights=True,
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'
        )),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 384, 768],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared2FCBBoxHead',
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=7,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=7,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = 'data/salt/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(
        type='Resize',
        keep_ratio=True,
        img_scale=[(800, 600), (720, 540), (640, 480), (600, 450), (480, 360)],
        multiscale_mode='value'),
    dict(type='RandomFlip', flip_ratio=0.5, direction='horizontal'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=[(1024, 768), (800, 600), (720, 540), (640, 480)],
        flip=True,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5, direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=1,
    train=dict(
        type='CocoDataset',
        ann_file='data/saltocchio/annotations/instance_train.json',
        img_prefix='data/saltocchio/train/',
        classes=('passaggio', 'paratia_aperta', 'paratia_semiaperta',
                 'paratia_chiusa', 'canale_vuoto', 'canale_con_acqua',
                 'occlusione'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
            dict(
                type='Resize',
                keep_ratio=True,
                img_scale=[(800, 600), (720, 540), (640, 480), (600, 450),
                           (480, 360)],
                multiscale_mode='value'),
            dict(type='RandomFlip', flip_ratio=0.5, direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(
                type='Collect',
                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='data/saltocchio/annotations/instance_val.json',
        img_prefix='data/saltocchio/val/',
        classes=('passaggio', 'paratia_aperta', 'paratia_semiaperta',
                 'paratia_chiusa', 'canale_vuoto', 'canale_con_acqua',
                 'occlusione'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=[(1024, 768), (800, 600), (720, 540), (640, 480)],
                flip=True,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(
                        type='RandomFlip',
                        flip_ratio=0.5,
                        direction='horizontal'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='data/saltocchio/annotations/instance_test.json',
        img_prefix='data/saltocchio/test/',
        classes=('passaggio', 'paratia_aperta', 'paratia_semiaperta',
                 'paratia_chiusa', 'canale_vuoto', 'canale_con_acqua',
                 'occlusione'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=[(1024, 768), (800, 600), (720, 540), (640, 480)],
                flip=True,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(
    interval=1, metric=['segm'], save_best='auto', classwise=True)
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            relative_position_bias_table=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=1000,
    warmup_ratio=0.001,
    step=[27, 33])
runner = dict(type='EpochBasedRunner', max_epochs=36)
checkpoint_config = dict(interval=3)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1), ('val', 1)]
pretrained = 'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth'
fp16 = dict(loss_scale=dict(init_scale=512))
work_dir = 'data/results/saltocchio/split_2/test_2'
gpu_ids = range(0, 1)
```


and my environment is the following:

sys.platform: linux
Python: 3.8.10 (default, Sep 28 2021, 16:10:42) [GCC 9.3.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.3.r11.3/compiler.29920130_0
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.10.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu113
OpenCV: 4.5.4
MMCV: 1.4.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.19.0+f08548b"
Test-time Loss,open-mmlab/mmdetection,2021-12-09 17:26:47,4,,6749,1075822299,"Hi,

I'm working on a project involving adversarial attacks on object detectors, and using MMDetection models. In order to implement the methods I am using, I need the loss of the predicted bboxes (and potentially other loss functions). Could you possibly explain how I could output a pre-trained model's loss, given an image at test-time?

Thank you."
Patch Merging in swin ,open-mmlab/mmdetection,2021-12-09 15:46:59,1,bug,6747,1075722335,"Patch_Merging in mmdet is same with original one theoretically, but different from the original one in numerical computing when use the same weights. The difference resulted from the different order of element in tensor. The different order of element in ops (like sum/mean) may result in some numerical problems(like ""big number eat small number""), I think. This difference will be enlarge by layernorm and linear in Patch merging module.If we load the released pretrained weight, use the mmdet Patch merging module will cause difference.

**Describe the bug**
run following code will show the difference.

**env**
```
sys.platform: linux
Python: 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce RTX 3090
CUDA_HOME: /usr/local/cuda-11.4
NVCC: Build cuda_11.4.r11.4/compiler.30300941_0
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.10.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm$70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprec$ted -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBU$_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unk$own-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-str$ct-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-c$lor=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl,
PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, US$_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON,

TorchVision: 0.11.1+cu113
OpenCV: 4.5.3
MMCV: 1.3.17
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.19.0+a17089d
```

**Reproduction**

```none
import torch.nn as nn 
import torch
import torch.nn.functional as F
from mmcv.utils import to_2tuple
import math
from mmcv.cnn import (build_activation_layer,
                      build_norm_layer, xavier_init)
from typing import Sequence


# copy from https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/utils/ckpt_convert.py#L89
def correct_unfold_reduction_order(x):
    out_channel, in_channel = x.shape
    x = x.reshape(out_channel, 4, in_channel // 4)
    x = x[:, [0, 2, 1, 3], :].transpose(1,
                                        2).reshape(out_channel, in_channel)
    return x

def correct_unfold_norm_order(x):
    in_channel = x.shape[0]
    x = x.reshape(4, in_channel // 4)
    x = x[[0, 2, 1, 3], :].transpose(0, 1).reshape(in_channel)
    return x

# copy from https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/utils/transformer.py#L62
class AdaptivePadding(nn.Module):
    """"""Applies padding to input (if needed) so that input can get fully covered
    by filter you specified. It support two modes ""same"" and ""corner"". The
    ""same"" mode is same with ""SAME"" padding mode in TensorFlow, pad zero around
    input. The ""corner""  mode would pad zero to bottom right.

    Args:
        kernel_size (int | tuple): Size of the kernel:
        stride (int | tuple): Stride of the filter. Default: 1:
        dilation (int | tuple): Spacing between kernel elements.
            Default: 1
        padding (str): Support ""same"" and ""corner"", ""corner"" mode
            would pad zero to bottom right, and ""same"" mode would
            pad zero around input. Default: ""corner"".
    Example:
        >>> kernel_size = 16
        >>> stride = 16
        >>> dilation = 1
        >>> input = torch.rand(1, 1, 15, 17)
        >>> adap_pad = AdaptivePadding(
        >>>     kernel_size=kernel_size,
        >>>     stride=stride,
        >>>     dilation=dilation,
        >>>     padding=""corner"")
        >>> out = adap_pad(input)
        >>> assert (out.shape[2], out.shape[3]) == (16, 32)
        >>> input = torch.rand(1, 1, 16, 17)
        >>> out = adap_pad(input)
        >>> assert (out.shape[2], out.shape[3]) == (16, 32)
    """"""

    def __init__(self, kernel_size=1, stride=1, dilation=1, padding='corner'):

        super(AdaptivePadding, self).__init__()

        assert padding in ('same', 'corner')

        kernel_size = to_2tuple(kernel_size)
        stride = to_2tuple(stride)
        padding = to_2tuple(padding)
        dilation = to_2tuple(dilation)

        self.padding = padding
        self.kernel_size = kernel_size
        self.stride = stride
        self.dilation = dilation

    def get_pad_shape(self, input_shape):
        input_h, input_w = input_shape
        kernel_h, kernel_w = self.kernel_size
        stride_h, stride_w = self.stride
        output_h = math.ceil(input_h / stride_h)
        output_w = math.ceil(input_w / stride_w)
        pad_h = max((output_h - 1) * stride_h +
                    (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)
        pad_w = max((output_w - 1) * stride_w +
                    (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)
        return pad_h, pad_w

    def forward(self, x):
        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])
        if pad_h > 0 or pad_w > 0:
            if self.padding == 'corner':
                x = F.pad(x, [0, pad_w, 0, pad_h])
            elif self.padding == 'same':
                x = F.pad(x, [
                    pad_w // 2, pad_w - pad_w // 2, pad_h // 2,
                    pad_h - pad_h // 2
                ])
        return x


# mmdetection version Patch Merging
# https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/utils/transformer.py#L260
class MPatchMerging(nn.Module):
    """"""Merge patch feature map.
    This layer groups feature map by kernel_size, and applies norm and linear
    layers to the grouped feature map. Our implementation uses `nn.Unfold` to
    merge patch, which is about 25% faster than original implementation.
    Instead, we need to modify pretrained models for compatibility.
    Args:
        in_channels (int): The num of input channels.
            to gets fully covered by filter and stride you specified..
            Default: True.
        out_channels (int): The num of output channels.
        kernel_size (int | tuple, optional): the kernel size in the unfold
            layer. Defaults to 2.
        stride (int | tuple, optional): the stride of the sliding blocks in the
            unfold layer. Default: None. (Would be set as `kernel_size`)
        padding (int | tuple | string ): The padding length of
            embedding conv. When it is a string, it means the mode
            of adaptive padding, support ""same"" and ""corner"" now.
            Default: ""corner"".
        dilation (int | tuple, optional): dilation parameter in the unfold
            layer. Default: 1.
        bias (bool, optional): Whether to add bias in linear layer or not.
            Defaults: False.
        norm_cfg (dict, optional): Config dict for normalization layer.
            Default: dict(type='LN').
        init_cfg (dict, optional): The extra config for initialization.
            Default: None.
    """"""

    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=2,
                 stride=None,
                 padding='corner',
                 dilation=1,
                 bias=False,
                 norm_cfg=dict(type='LN'),
                 init_cfg=None):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        if stride:
            stride = stride
        else:
            stride = kernel_size

        kernel_size = to_2tuple(kernel_size)
        stride = to_2tuple(stride)
        dilation = to_2tuple(dilation)

        if isinstance(padding, str):
            self.adap_padding = AdaptivePadding(
                kernel_size=kernel_size,
                stride=stride,
                dilation=dilation,
                padding=padding)
            # disable the padding of unfold
            padding = 0
        else:
            self.adap_padding = None

        padding = to_2tuple(padding)
        self.sampler = nn.Unfold(
            kernel_size=kernel_size,
            dilation=dilation,
            padding=padding,
            stride=stride)

        sample_dim = kernel_size[0] * kernel_size[1] * in_channels

        if norm_cfg is not None:
            self.norm = build_norm_layer(norm_cfg, sample_dim)[1]
        else:
            self.norm = None

        self.reduction = nn.Linear(sample_dim, out_channels, bias=bias)

    def init_weight(self, w1, w2, b2):
        self.reduction.load_state_dict({
            ""weight"": correct_unfold_reduction_order(w1)
        })

        self.norm.load_state_dict({
            ""weight"": correct_unfold_norm_order(w2),
            ""bias"": correct_unfold_norm_order(b2)
        })


    def forward(self, x, input_size):
        """"""
        Args:
            x (Tensor): Has shape (B, H*W, C_in).
            input_size (tuple[int]): The spatial shape of x, arrange as (H, W).
                Default: None.
        Returns:
            tuple: Contains merged results and its spatial shape.
                - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)
                - out_size (tuple[int]): Spatial shape of x, arrange as
                    (Merged_H, Merged_W).
        """"""
        B, L, C = x.shape
        assert isinstance(input_size, Sequence), f'Expect ' \
                                                 f'input_size is ' \
                                                 f'`Sequence` ' \
                                                 f'but get {input_size}'

        H, W = input_size
        assert L == H * W, 'input feature has wrong size'

        x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W
        # Use nn.Unfold to merge patch. About 25% faster than original method,
        # but need to modify pretrained model for compatibility

        if self.adap_padding:
            x = self.adap_padding(x)
            H, W = x.shape[-2:]

        x = self.sampler(x)
        # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)

        out_h = (H + 2 * self.sampler.padding[0] - self.sampler.dilation[0] *
                 (self.sampler.kernel_size[0] - 1) -
                 1) // self.sampler.stride[0] + 1
        out_w = (W + 2 * self.sampler.padding[1] - self.sampler.dilation[1] *
                 (self.sampler.kernel_size[1] - 1) -
                 1) // self.sampler.stride[1] + 1

        output_size = (out_h, out_w)
        x = x.transpose(1, 2)  # B, H/2*W/2, 4*C
        x = self.norm(x) if self.norm else x
        x = self.reduction(x)
        return x, output_size


# original version in swin
class PatchMerging(nn.Module):
    """"""Patch Merging Layer
    Args:
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """"""

    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)

    def init_weight(self, w1, w2, b2):
        self.reduction.load_state_dict({
            ""weight"": w1
        })

        self.norm.load_state_dict({
            ""weight"": w2,
            ""bias"": b2
        })

    def forward(self, x, H, W):
        """"""Forward function.
        Args:
            x: Input feature, tensor size (B, H*W, C).
            H, W: Spatial resolution of the input feature.
        """"""
        B, L, C = x.shape
        assert L == H * W, ""input feature has wrong size""

        x = x.view(B, H, W, C)
        # padding
        pad_input = (H % 2 == 1) or (W % 2 == 1)
        if pad_input:
            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)

        x = self.reduction(x)
        return x


H = W = 160
B = 1
C = 96
x = torch.rand((B, C, H, W)).reshape(B, C, H*W).transpose(-1,-2) * 2

out1 = []
out2 = []

H1 = H 
W1 = W
H2 = H
W2 = W
x1 = x
x2 = x

# swin中有3个patch merging
for i in range(3):
    if i > 0:
        C = C * 2
    linear_weight = torch.rand((2*C, 4*C))
    norm_weight = torch.rand((4*C,))
    norm_bias = torch.rand((4*C,))

    det_p = PatchMerging(C)
    det_p.init_weight(linear_weight, norm_weight, norm_bias)

    mmdet_p = MPatchMerging(C, 2*C, 2, bias=False)
    mmdet_p.init_weight(linear_weight, norm_weight, norm_bias)

    with torch.no_grad():
        y1 = det_p(x1, H1, W1)
        H1 = H1 // 2
        W1 = W1 // 2
        out1.append(y1)
        x1 = y1

    with torch.no_grad():
        y2 = mmdet_p(x2, (H2, W2))
        H2, W2 = y2[1]
        y2 = y2[0]
        out2.append(y2)
        x2 = y2

for y1, y2 in zip(out1, out2):
    print(y1[0,0,0].cpu().numpy(), y2[0,0,0].cpu().numpy()) 
    print((y1 - y2).abs())
    print((y1 - y2).abs().sum()) # The cumulative error is large.
    print()

```
"
I want to know why the box and mask are not appear.,open-mmlab/mmdetection,2021-12-09 07:30:03,1,,6739,1075242620,"I already train mask r-cnn model, and i tested some image.
i want to get some information about mask, so i pick up mask information from result -> from inference_detector()
Some images have mask information with the inference_detector code, but when I run model.show_result and see the saved image, there is no mask. May I know why?"
能否增加数据集划分功能？,open-mmlab/mmdetection,2021-12-06 09:12:07,1,enhancement,6702,1071905380,"**Describe the feature**
希望框架可以提供数据集划分功能以达到K折交叉验证的目的

**Motivation**
目前框架下似乎只能自己切分数据集来实现训练、验证与测试
重复自己划分数据显得很费力

**Related resources**


**Additional context**
抱歉占用了您的时间"
"How can I save the checkpoint by iteration, not epoch?",open-mmlab/mmdetection,2021-11-30 02:48:22,3,bug,6635,1066722587,"Hi, I'm just wondering how I can save the checkpoint by iteration?
I already change my checkpoint config like, 
""checkpoint_config = dict(interval=1)"" -> ""checkpoint_config = dict(interval=1000, by_epoch=False)""
but it still saves by 1 epoch. Is there any solution for that??
And how about validation by iteration not by epoch? Is there any solution for that also?"
Add a notice about thread unsafety of 'mmdet.apis.init_detector',open-mmlab/mmdetection,2021-11-26 16:16:36,0,,6601,1064669002,"**Describe the feature**

Add a notice in the document and tutorial, warning users that `mmdet.apis.init_dectector` is not thread-safe.

**Motivation**

Hello, I am a newbie to `mmdetection`. Thanks a lot for such a wonderful framework.

Recently I encountered a problem when trying to call `mmdet.apis.init_detector` in multiple threads simultaneously. Displayed below is the error log:

```python
Traceback (most recent call last):
  File ""src/cmd/worker/worker.py"", line 31, in AddModel
    self.model_dict[request.source] = init_detector(
  File ""/usr/local/lib/python3.8/site-packages/mmdet/apis/inference.py"", line 32, in init_detector
    config = mmcv.Config.fromfile(config)
  File ""/usr/local/lib/python3.8/dist-packages/mmcv/utils/config.py"", line 331, in fromfile
    cfg_dict, cfg_text = Config._file2dict(filename,
  File ""/usr/local/lib/python3.8/dist-packages/mmcv/utils/config.py"", line 248, in _file2dict
    _cfg_dict, _cfg_text = Config._file2dict(osp.join(cfg_dir, f))
  File ""/usr/local/lib/python3.8/dist-packages/mmcv/utils/config.py"", line 206, in _file2dict
    mod = import_module(temp_module_name)
  File ""/usr/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 973, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'tmp6wm66ta3'
```

After some inspection into this error, I found the root cause is the code block below in `mmcv.Config._file2dict` when trying to load the configuration file into memory:

```python
temp_module_name = osp.splitext(temp_config_name)[0]
sys.path.insert(0, temp_config_dir)
Config._validate_py_syntax(filename)
mod = import_module(temp_module_name)
sys.path.pop(0)
```

Here `sys.path` is simply a Python list without any thread-safe guarantee. So if this block is called concurrently, the import path inserted by one thread might be poped by another, which leads to a `ModuleNotFoundError`.

What I am working on is an online system where a single inference worker handles multiple incoming streams concurrently, and these streams enter and exit dynamically. So I think this scenario is quite common and worth considering.

A simple walkaround is to load the configuration file with `mmcv.Config.fromfile` with mutex, and call `init_detector` using a `Config` object instead of a path `string` to the configuration file. This actually demands users handle thread safety by themselves. I am not sure whether it is a good idea. Or is it better to have it directly fixed inside `mmcv`?

In any case, I personally think it is necessary to mention thread unsafety of 'mmdet.apis.init_detector' (or `mmcv.Config.fromfile`, more essentially) in the manual or tutorial, so others will not spend quite some time on this issue again : )

Thanks a lot for your kind assistance.

**Related resources**

Not applicable.

**Additional context**

Not applicable."
PIoU Loss Implementation in MMdetection,open-mmlab/mmdetection,2021-11-26 10:23:13,1,enhancement#community help wanted,6598,1064311536,Is there someone who implements the [PIoU Loss](https://arxiv.org/abs/2007.09584) in MMDetection? 
Enable again setting random seed and Deterministic=True from the config file,open-mmlab/mmdetection,2021-11-20 10:29:23,4,enhancement,6553,1059114174,"**Describe the feature**

**Motivation**
Currently, it's not possible to set the random seed and the option Deterministic=True from the config file, but only from args (see https://github.com/open-mmlab/mmdetection/issues/6322). This is extremely inconvenient: in order to reproduce the training run at a later time, one has to store the CLI launch command _together_ with the config file! It would easier to reproduce experiments if we could set these parameters in the config file.

**Related resources**
Code [here](https://github.com/open-mmlab/mmdetection/blob/a7a16afbf2a4bdb4d023094da73d325cb864838b/tools/train.py#L146-L152) shows that in the current implementation, a seed specified in the config file will be overwritten by `args.seed`, which is set to `None` if no seed is passed as an argument. This makes effectively impossible to set the seed through the config file."
SGD优化器的定义在哪个文件里呀？如何把它改成Adam呢？,open-mmlab/mmdetection,2021-11-20 06:39:37,4,,6551,1059076901,"optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
这个SGD的定义在哪个文件里呀？想换成Adam怎么换呀"
Multi-gpu training gets stuck,open-mmlab/mmdetection,2021-11-17 17:40:23,23,community help wanted#community discussion,6534,1056412073,"**Checklist**

1. [x] I have searched related issues but cannot get the expected help.
2. [x] I have read the [FAQ documentation](https://mmdetection.readthedocs.io/en/latest/faq.html) but cannot get the expected help.
3. [x] The bug has not been fixed in the latest version.

**Describe the bug**

Single GPU training works fine, single node multi-GPU doesn't.
Relevant: #3823, #2193, #1979, #4535, maybe #3973
Rolling back `intel-openmp` doesn't help, and it uses only default configs.

I.e. running this is ok:
```
python tools/train.py configs/cityscapes/faster_rcnn_r50_fpn_1x_cityscapes.py
```
Outputs:
```
...
2021-11-17 17:33:20,582 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:33:20,583 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:33:20,584 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:33:20,586 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:33:20,588 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2021-11-17 17:33:20,589 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:33:20,598 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
2021-11-17 17:33:20,615 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}                                                                                       
2021-11-17 17:33:20,621 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg
'}}, {'type': 'Xavier', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]                                                                                                           
loading annotations into memory...
Done (t=0.44s)                                                                                                                                                                                                     
creating index...                                                                                        
index created!                                                                                                                                                                                                     
loading annotations into memory...                
Done (t=0.06s)                                                                                                                                                                                                     
creating index...        
index created!                                                                                                                                                                                                     
2021-11-17 17:33:23,497 - mmdet - INFO - load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
2021-11-17 17:33:23,597 - mmdet - WARNING - The model and loaded state dict do not match exactly                                                                                                                   
                                                                                                         
size mismatch for roi_head.bbox_head.fc_cls.weight: copying a param with shape torch.Size([81, 1024]) from checkpoint, the shape in current model is torch.Size([9, 1024]).                                        
size mismatch for roi_head.bbox_head.fc_cls.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([9]).                                                      
size mismatch for roi_head.bbox_head.fc_reg.weight: copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape in current model is torch.Size([32, 1024]).   
size mismatch for roi_head.bbox_head.fc_reg.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([32]).                                                    
2021-11-17 17:33:23,600 - mmdet - INFO - Start running, host: vince@wombat, work_dir: /home/vince/workspace/mmdetection/work_dirs/faster_rcnn_r50_fpn_1x_cityscapes
2021-11-17 17:33:23,600 - mmdet - INFO - Hooks will be executed in the following order:                                                                                                                            
before_run:                                                                                              
(VERY_HIGH   ) StepLrUpdaterHook                                                                         
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                          
...     
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2021-11-17 17:33:23,600 - mmdet - INFO - workflow: [('train', 1)], max: 8 epochs
2021-11-17 17:33:23,600 - mmdet - INFO - Checkpoints will be saved to /home/vince/workspace/mmdetection/work_dirs/faster_rcnn_r50_fpn_1x_cityscapes by HardDiskBackend.
2021-11-17 17:33:54,886 - mmdet - INFO - Epoch [1][100/23720]   lr: 1.988e-03, eta: 16:25:12, time: 0.312, data_time: 0.025, memory: 4024, loss_rpn_cls: 0.0439, loss_rpn_bbox: 0.0921, loss_cls: 0.7297, acc: 80.1914, loss_bbox: 0.3776, loss: 1.2433
2021-11-17 17:34:23,521 - mmdet - INFO - Epoch [1][200/23720]   lr: 3.986e-03, eta: 15:44:40, time: 0.286, data_time: 0.004, memory: 4073, loss_rpn_cls: 0.0450, loss_rpn_bbox: 0.1027, loss_cls: 0.3770, acc: 86.8848, loss_bbox: 0.2467, loss: 0.7713
```

Running this is not ok:
```
./tools/dist_train.sh configs/cityscapes/faster_rcnn_r50_fpn_1x_cityscapes.py 2
```
Outputs:
```
...
2021-11-17 17:31:12,229 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,230 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,231 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,232 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,233 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,233 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,234 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,236 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,237 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,238 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,239 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2021-11-17 17:31:12,241 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,242 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2021-11-17 17:31:12,246 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,250 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}
2021-11-17 17:31:12,253 - mmdet - INFO - initialize Bottleneck with init_cfg {'type': 'Constant', 'val': 0, 'override': {'name': 'norm3'}}                                                                         
2021-11-17 17:31:12,270 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}                                                                             
2021-11-17 17:31:12,287 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}                                                                                       
2021-11-17 17:31:12,290 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg
'}}, {'type': 'Xavier', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]                                                                                                           
loading annotations into memory...                                                                                                                                                                                 
Done (t=0.42s)                                                                                                                                                                                                     
creating index...                                                                                                                                                                                                  
index created!                                                                                                                                                                                                     
loading annotations into memory...                                                                                                                                                                                 
Done (t=0.06s)                                                                                                                                                                                                     
creating index...                                                                                                                                                                                                  
index created!                                                                                                                                                                                                     
2021-11-17 17:31:13,772 - mmdet - INFO - load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
```
and then just waits.




**Environment**

```
sys.platform: linux
Python: 3.8.12 (default, Nov 17 2021, 08:17:37) [GCC 9.3.0]
CUDA available: True
GPU 0,1: NVIDIA GeForce GTX 1080 Ti
CUDA_HOME: /usr/local/cuda
NVCC: Build cuda_11.5.r11.5/compiler.30411180_0
GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
PyTorch: 1.10.0+cu113
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.1+cu113
OpenCV: 4.5.4-dev
MMCV: 1.3.17
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.3
MMDetection: 2.18.1+c76ab0e
```
"
YOLOR Support,open-mmlab/mmdetection,2021-11-17 14:09:34,1,community help wanted,6531,1056176033,"Hello,

Does mmdetection support [YOLOR](https://github.com/WongKinYiu/yolor) ? Is it possible to add it?

Thanks in advance for your time."
"Torchserve error:  Load model failed: modelname, error: Worker died",open-mmlab/mmdetection,2021-11-13 09:02:50,0,deployment,6497,1052634675,"When I try to start torchserve, I encounter the following problems：

Removing orphan pid file.
2021-11-13 08:20:26,125 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2021-11-13 08:20:26,308 [INFO ] main org.pytorch.serve.ModelServer -
Torchserve version: 0.4.2
TS Home: /opt/conda/lib/python3.7/site-packages
Current directory: /root
Temp directory: /tmp
Number of GPUs: 4
Number of CPUs: 40
Max heap size: 30688 M
Python executable: /opt/conda/bin/python
Config file: /home/model-server/config.properties
Inference address: http://0.0.0.0:8080
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /home/model-server/model-store
Initial Models: all
Log dir: /root/logs
Metrics dir: /root/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 4
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Metrics report format: prometheus
Enable metrics API: true
Workflow Store: /home/model-server/model-store
Model config: N/A
2021-11-13 08:20:26,319 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2021-11-13 08:20:26,358 [DEBUG] main org.pytorch.serve.ModelServer - Loading models from model store: dota.mar
2021-11-13 08:20:31,010 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model dota
2021-11-13 08:20:31,010 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model dota
2021-11-13 08:20:31,011 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model dota loaded.
2021-11-13 08:20:31,011 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: dota, count: 4
2021-11-13 08:20:31,029 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2021-11-13 08:20:31,159 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080
2021-11-13 08:20:31,160 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2021-11-13 08:20:31,161 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2021-11-13 08:20:31,162 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2021-11-13 08:20:31,163 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
Model server started.
2021-11-13 08:20:31,378 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2021-11-13 08:20:31,438 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:42.9|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,439 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:1027.9766960144043|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,440 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:712.0338859558105|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,440 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:40.9|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,440 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:240422.453125|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,441 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:14321.85546875|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,441 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:6.8|#Level:Host|#hostname:67bfea95f1e6,timestamp:1636791631
2021-11-13 08:20:31,824 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2021-11-13 08:20:31,825 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - [PID]3414
2021-11-13 08:20:31,825 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Torch worker started.
2021-11-13 08:20:31,826 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Python runtime: 3.7.7
2021-11-13 08:20:31,826 [DEBUG] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-dota_1.0 State change null -> WORKER_STARTED
2021-11-13 08:20:31,835 [INFO ] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2021-11-13 08:20:31,845 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2021-11-13 08:20:31,846 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001
2021-11-13 08:20:31,846 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - [PID]3415
2021-11-13 08:20:31,846 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Torch worker started.
2021-11-13 08:20:31,847 [DEBUG] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-dota_1.0 State change null -> WORKER_STARTED
2021-11-13 08:20:31,847 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Python runtime: 3.7.7
2021-11-13 08:20:31,847 [INFO ] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001
2021-11-13 08:20:31,851 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.
2021-11-13 08:20:31,876 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - model_name: dota, batchSize: 1
2021-11-13 08:20:31,877 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - model_name: dota, batchSize: 1
2021-11-13 08:20:31,893 [INFO ] W-9003-dota_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003
2021-11-13 08:20:31,894 [INFO ] W-9003-dota_1.0-stdout MODEL_LOG - [PID]3417
2021-11-13 08:20:31,894 [INFO ] W-9003-dota_1.0-stdout MODEL_LOG - Torch worker started.
2021-11-13 08:20:31,894 [DEBUG] W-9003-dota_1.0 org.pytorch.serve.wlm.WorkerThread - W-9003-dota_1.0 State change null -> WORKER_STARTED
2021-11-13 08:20:31,894 [INFO ] W-9003-dota_1.0-stdout MODEL_LOG - Python runtime: 3.7.7
2021-11-13 08:20:31,894 [INFO ] W-9003-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003
2021-11-13 08:20:31,897 [INFO ] W-9003-dota_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.
2021-11-13 08:20:31,905 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002
2021-11-13 08:20:31,906 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - [PID]3416
2021-11-13 08:20:31,906 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Torch worker started.
2021-11-13 08:20:31,906 [DEBUG] W-9002-dota_1.0 org.pytorch.serve.wlm.WorkerThread - W-9002-dota_1.0 State change null -> WORKER_STARTED
2021-11-13 08:20:31,906 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Python runtime: 3.7.7
2021-11-13 08:20:31,906 [INFO ] W-9002-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002
2021-11-13 08:20:31,913 [INFO ] W-9003-dota_1.0-stdout MODEL_LOG - model_name: dota, batchSize: 1
2021-11-13 08:20:31,913 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.
2021-11-13 08:20:31,927 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - model_name: dota, batchSize: 1
2021-11-13 08:20:41,493 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Use load_from_local loader
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Backend worker process died.
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 183, in <module>
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     worker.run_server()
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 155, in run_server
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2021-11-13 08:20:41,494 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 117, in handle_connection
2021-11-13 08:20:41,495 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2021-11-13 08:20:41,495 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 90, in load_model
2021-11-13 08:20:41,495 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-11-13 08:20:41,495 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_loader.py"", line 110, in load
2021-11-13 08:20:41,495 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2021-11-13 08:20:41,495 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/tmp/models/bc96955a5ff24189b148a4bcd674664b/mmdet_handler.py"", line 28, in initialize
2021-11-13 08:20:41,496 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     self.model = init_detector(self.config_file, checkpoint, self.device)
2021-11-13 08:20:41,496 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/mmdet/apis/inference.py"", line 52, in init_detector
2021-11-13 08:20:41,496 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     model.to(device)
2021-11-13 08:20:41,496 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 607, in to
2021-11-13 08:20:41,496 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     return self._apply(convert)
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 354, in _apply
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     module._apply(fn)
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 354, in _apply
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     module._apply(fn)
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 376, in _apply
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     param_applied = fn(param)
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 605, in convert
2021-11-13 08:20:41,497 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG -     return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
2021-11-13 08:20:41,498 [INFO ] W-9000-dota_1.0-stdout MODEL_LOG - RuntimeError: CUDA error: invalid device ordinal
2021-11-13 08:20:41,504 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2021-11-13 08:20:41,504 [DEBUG] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-11-13 08:20:41,505 [DEBUG] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
        at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
        at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2021-11-13 08:20:41,506 [WARN ] W-9000-dota_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: dota, error: Worker died.
2021-11-13 08:20:41,506 [DEBUG] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-dota_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-11-13 08:20:41,507 [WARN ] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-dota_1.0-stderr
2021-11-13 08:20:41,507 [WARN ] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-dota_1.0-stdout
2021-11-13 08:20:41,508 [INFO ] W-9000-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2021-11-13 08:20:41,636 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Use load_from_local loader
2021-11-13 08:20:41,636 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Backend worker process died.
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 183, in <module>
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     worker.run_server()
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 155, in run_server
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 117, in handle_connection
2021-11-13 08:20:41,637 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 90, in load_model
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_loader.py"", line 110, in load
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/tmp/models/bc96955a5ff24189b148a4bcd674664b/mmdet_handler.py"", line 28, in initialize
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     self.model = init_detector(self.config_file, checkpoint, self.device)
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/mmdet/apis/inference.py"", line 52, in init_detector
2021-11-13 08:20:41,638 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     model.to(device)
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 607, in to
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     return self._apply(convert)
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 354, in _apply
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     module._apply(fn)
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 354, in _apply
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     module._apply(fn)
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 376, in _apply
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     param_applied = fn(param)
2021-11-13 08:20:41,639 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 605, in convert
2021-11-13 08:20:41,640 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG -     return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
2021-11-13 08:20:41,640 [INFO ] W-9001-dota_1.0-stdout MODEL_LOG - RuntimeError: CUDA error: invalid device ordinal
2021-11-13 08:20:41,648 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED
2021-11-13 08:20:41,648 [DEBUG] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-11-13 08:20:41,648 [DEBUG] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
        at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
        at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2021-11-13 08:20:41,648 [WARN ] W-9001-dota_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: dota, error: Worker died.
2021-11-13 08:20:41,649 [DEBUG] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerThread - W-9001-dota_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2021-11-13 08:20:41,649 [WARN ] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-dota_1.0-stderr
2021-11-13 08:20:41,649 [WARN ] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-dota_1.0-stdout
2021-11-13 08:20:41,649 [INFO ] W-9001-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.
2021-11-13 08:20:41,661 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Use load_from_local loader
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Backend worker process died.
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 183, in <module>
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     worker.run_server()
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 155, in run_server
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 117, in handle_connection
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_service_worker.py"", line 90, in load_model
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     service = model_loader.load(model_name, model_dir, handler, gpu, batch_size, envelope)
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/ts/model_loader.py"", line 110, in load
2021-11-13 08:20:41,662 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/tmp/models/bc96955a5ff24189b148a4bcd674664b/mmdet_handler.py"", line 28, in initialize
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     self.model = init_detector(self.config_file, checkpoint, self.device)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/mmdet/apis/inference.py"", line 52, in init_detector
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     model.to(device)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 607, in to
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     return self._apply(convert)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 354, in _apply
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     module._apply(fn)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 354, in _apply
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     module._apply(fn)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 376, in _apply
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     param_applied = fn(param)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -   File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 605, in convert
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG -     return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
2021-11-13 08:20:41,663 [INFO ] W-9002-dota_1.0-stdout MODEL_LOG - RuntimeError: CUDA error: invalid device ordinal
2021-11-13 08:20:41,672 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED
2021-11-13 08:20:41,672 [DEBUG] W-9002-dota_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2021-11-13 08:20:41,672 [DEBUG] W-9002-dota_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException
        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
        at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133)
        at java.base/java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432)
        at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:188)
        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.base/java.lang.Thread.run(Thread.java:829)
2021-11-13 08:20:41,673 [WARN ] W-9002-dota_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: dota, error: Worker died.

When I  test the deployment, the massage are as follow:

root@67bfea95f1e6:~# curl http://127.0.0.1:8080/preditions/dota -T /workspace/P0000__1__0___0.png
{
  ""code"": 404,
  ""type"": ""ResourceNotFoundException"",
  ""message"": ""Requested resource is not found, please refer to API document.""
}


How to solve the problem?
"
"norm_cfg = dict(type='BN', requires_grad=False) Can we close the 'BN' in training by ignoring setting norm_cfg?",open-mmlab/mmdetection,2021-11-12 19:33:42,1,,6494,1052351573,"I don't want to train with BN because my GPU resourse can not take large batch size in training.
Is there any way to close the 'BN' from config files?"
TypeError: CocoDataset: string indices must be integers,open-mmlab/mmdetection,2021-11-09 08:04:15,3,,6468,1048301100,"unexpected key in source state_dict: fc.weight, fc.bias

E:\mmdetection\week2_mmdet\Week2_mmdet\mmdetection-2.11.0\mmdet\datasets\custom.py:154: UserWarning: CustomDataset does not support filtering empty gt images.
  warnings.warn(
Traceback (most recent call last):
  File ""e:\mmdetection\week2_mmdet\week2_mmdet\mmdetection-2.11.0\mmcv-1.3.1\mmcv\utils\registry.py"", line 51, in build_from_cfg
    return obj_cls(**args)
  File ""E:\mmdetection\week2_mmdet\Week2_mmdet\mmdetection-2.11.0\mmdet\datasets\custom.py"", line 96, in __init__
    valid_inds = self._filter_imgs()
  File ""E:\mmdetection\week2_mmdet\Week2_mmdet\mmdetection-2.11.0\mmdet\datasets\custom.py"", line 158, in _filter_imgs
    if min(img_info['width'], img_info['height']) >= min_size:
TypeError: string indices must be integers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/mmdetection/week2_mmdet/Week2_mmdet/mmdetection-2.11.0/tools/train.py"", line 187, in <module>
    main()
  File ""E:/mmdetection/week2_mmdet/Week2_mmdet/mmdetection-2.11.0/tools/train.py"", line 163, in main
    datasets = [build_dataset(cfg.data.train)]
  File ""E:\mmdetection\week2_mmdet\Week2_mmdet\mmdetection-2.11.0\mmdet\datasets\builder.py"", line 71, in build_dataset
    dataset = build_from_cfg(cfg, DATASETS, default_args)
  File ""e:\mmdetection\week2_mmdet\week2_mmdet\mmdetection-2.11.0\mmcv-1.3.1\mmcv\utils\registry.py"", line 54, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
TypeError: CocoDataset: string indices must be integers"
How to initialize the backbone which is mixed with custom module? ,open-mmlab/mmdetection,2021-11-07 06:43:32,7,,6453,1046674477,"Hi, dear authors!
Thank you for your amazing job!
I have a small question about init.
I mixed the ResNet 18 with my custom module, which looks like as follow:
![屏幕截图 2021-11-07 144148](https://user-images.githubusercontent.com/48090608/140635189-41c4828c-bc96-4156-ba69-159e9d0c2ea9.jpg)
I want to initialize all ResNet layers with ResNet 18 pre-trained file, and initialize all custom layers with:
```
custom_init_cfg=[dict(type='Kaiming', layer=['Conv2d']),
                 dict(type='Constant', val=1, layer=['_BatchNorm', 'GroupNorm'])])
```
or can describe like this:
```
# for all resnet layers
resnet_init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet18')
 
# for all custom layers
custom_init_cfg=[dict(type='Kaiming', layer=['Conv2d']),
                 dict(type='Constant', val=1, layer=['_BatchNorm', 'GroupNorm'])])
```
How do I initialize this ""mix backbone"" in this way?
Thank you!"
Is it possible to log Tensorboard metrics as a function of epochs rather than iterations?,open-mmlab/mmdetection,2021-11-05 14:48:31,3,,6448,1045929287,"I'm using Tensorboard to visualize my training logs. However, since MMDetection logs metrics as a function of iterations, it becomes really awkward to compare results from different models on the same dataset and with the same number of epochs:

<img width=""1304"" alt=""Untitled"" src=""https://user-images.githubusercontent.com/8233615/140529214-994610ca-b471-4e43-9347-6267f557f904.png"">

The three validation curves seem to be of different length, but the number of epochs is the same. Is it possible to save metrics as a function of number of epochs, instead of number of iterations, in the Tensorboard logs?"
How can I easily get the feature when I test a model I have saved?,open-mmlab/mmdetection,2021-11-04 07:15:11,7,community discussion,6439,1044425532,The inference module only saves the results of an image. How can I get the feature after backbone or after FPN? Is there an easy way?
About SeesawLoss questions in use of YOLOX,open-mmlab/mmdetection,2021-11-01 09:02:24,8,community help wanted,6424,1040934630,"I am going to use SeesawLoss  in YOLOX head but i get an AssertionError:
assert cls_score.size(-1) == self.num_classes + 2
why the cls_score (torch.Tensor): The prediction with shape (N, C + 2)


This is my configs of bbox_head:
_base_ = './yolox_s_8x8_300e_coco.py'
bbox_head=dict(num_classes=12,in_channels=96, feat_channels=96,strides=[4,8, 16, 32],
                   loss_cls=dict(
                type='SeesawLoss',
                p=0.8,
                q=2.0,
                num_classes=12,
                loss_weight=1.0))
"
weight here will lead to zero loss in the dice loss,open-mmlab/mmdetection,2021-10-31 11:48:37,2,bug#enhancement,6416,1040448256,"https://github.com/open-mmlab/mmdetection/blob/db256a14bb7018ad36eed104ea0ce178a0d4050c/mmdet/models/losses/utils.py#L43
In the `mmdet.models.roi_heads.mask_heads.fcn_mask_head.py#L173`, the weight was initialized to zeros using `torch.zeros_like(labels)`. 

It will lead to zero loss when call `loss = weight_reduce_loss(loss, weight, reduction, avg_factor).` in the `mmdet.models.losses.dice_loss.py#L44`

I suggest that add 
```python
    if weight.sum() == 0:
        weight = torch.ones_like(weight)
    else:
        weight = weight/weight.sum()
```
before call `mmdet.models.losses.dice_loss.py#L44` "
"ERROR: Command errored out with exit status 1，import io, os, sys, setuptools, tokenize; sys.argv[0]",open-mmlab/mmdetection,2021-10-30 09:40:25,2,,6411,1040101253,"安装的时候 报错
Error during installation


```
pip install -v -e .

Installing collected packages: six, python-dateutil, kiwisolver, cycler, matplotlib, terminaltables, pycocotools-windows, mmdet
  Running setup.py develop for mmdet
    Running command 'E:\anaconda3\envs\mmdetection\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'F:\\mmdetection\\setup.py'""'""'; __file__='""'""'F:\\mmdetection\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' develop --no-deps
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""F:\mmdetection\setup.py"", line 182, in <module>
        add_mim_extension()
      File ""F:\mmdetection\setup.py"", line 169, in add_mim_extension
        os.symlink(src_relpath, tar_path)
    OSError: symbolic link privilege not held
ERROR: Command errored out with exit status 1: 'E:\anaconda3\envs\mmdetection\python.exe' -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'F:\\mmdetection\\setup.py'""'""'; __file__='""'""'F:\\mmdetection\\setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' develop --no-deps Check the logs for full command output.


python setup.py develop

Traceback (most recent call last):
  File ""setup.py"", line 182, in <module>
    add_mim_extension()
  File ""setup.py"", line 169, in add_mim_extension
    os.symlink(src_relpath, tar_path)
OSError: symbolic link privilege not held

```
```

------------------- ------------ --------
win11

addict              2.4.0
certifi             2021.10.8
cycler              0.11.0
Cython              0.29.24
kiwisolver          1.3.2
matplotlib          3.4.3
mmcv                1.3.16       f:\mmcv
mmcv-full           1.3.16       f:\mmcv
numpy               1.21.3
opencv-python       4.5.4.58
packaging           21.2
Pillow              8.4.0
pip                 21.2.4
pycocotools-windows 2.0.0.2
pyparsing           2.4.7
python-dateutil     2.8.2
PyYAML              6.0
regex               2021.10.23
setuptools          58.0.4
six                 1.16.0
terminaltables      3.1.0
torch               1.10.0+cu113
torchaudio          0.10.0+cu113
torchvision         0.11.1+cu113
typing-extensions   3.10.0.2
wheel               0.37.0
wincertstore        0.2
yapf                0.31.0
```"
Customized backbone model,open-mmlab/mmdetection,2021-10-24 06:56:09,2,,6355,1034341052,"Greetings, 

I want to use a customized backbone model. Now, I had already viewed the instructions mentioned in your GitHub repo about it. I had a couple of queries 

1) If I want to add such a customized backbone model, would I need to then clone your github repo and build it or can I do pip installation as well?
2) In the config, is it necessary to mention parameters such as depth, style etc other than 'type' or just this is also sufficient ?"
Custom dataset COCO-format annotations for empty images,open-mmlab/mmdetection,2021-10-22 15:31:47,0,,6351,1033708205,"I am trying to train a model on a custom dataset and am currently in the process of changing my annotations. The dataset provides a .json, but there is no segmentation information in it. 
- Can I just omit the 'segmentation'-key in dict['annotations'] or do I have to have an empty list in that place?
- There are also images without detections, which I also want to use in training. Should those not have an entry in dict['annotations'] at all or should they have one with 'area' = 0.0, 'bbox' = [], 'iscrowd' = 0?

The /customize_dataset.md says the annotations should be in the following format:
```json
""annotations"": [
    {
        ""segmentation"": [[192.81,
            247.09,
            ...
            219.03,
            249.06]],  # if you have mask labels
        ""area"": 1035.749,
        ""iscrowd"": 0,
        ""image_id"": 1268,
        ""bbox"": [192.81, 224.8, 74.73, 33.43],
        ""category_id"": 16,
        ""id"": 42986
    },
    ...
],
```"
wobbly edges of predicted masks with custom implementation in mmdetection,open-mmlab/mmdetection,2021-10-20 08:32:20,0,community discussion,6327,1031128102,"Hello all,

My issue is not directly related to mmdetection but I'm using mmdetection with custom implementation of refinemaskhead (https://github.com/zhanggang001/RefineMask) for precise segmentation. I'm getting wobbly edges of the predicted masks as shown in figure. Can someone help how to get more precise mask with any post processing computer vision/AI/ or manual techniques? Thank you in advance for your help!
![mask](https://user-images.githubusercontent.com/31433038/138053218-996e6b33-bc57-4c9a-a41a-b57c64aebfa1.png)
"
CUDA error: an illegal memory access was encountered,open-mmlab/mmdetection,2021-10-18 10:45:22,4,,6307,1028948103,"![image](https://user-images.githubusercontent.com/77336413/137716488-57ee0c72-cee0-491e-9f37-4c214e1753f5.png)

I would like to ask what caused this? I can't find a solution。
Then there is another problem. After training 1epoch, the verification is broken now. How can I make the trained epoch verify and get the map。"
how to save proposals in faster R-cnn models?,open-mmlab/mmdetection,2021-10-16 07:44:48,0,,6295,1027983659,as title
Easier way to edit cfg file // Pycharm auto-suggestion,open-mmlab/mmdetection,2021-10-15 15:10:40,1,enhancement#community discussion,6292,1027540636,"My current workflow is the following :
Browse the MMdet codebase, notice the interresting args.

Then I edit my cfg file manually to set these arguements, but avoiding **dict nesting level error (on hirarchy too high / too low in a dict)** or even typos error is bit tedious.

Is there an easier workflow, where I could benefit from my IDE (Pycharm) **auto-suggestion, auto-linting, auto-check** etc... ?  
Maybe there is a way to define the cfg in normally executed code and then auto generate the cfg file ?  

Thanks for this amazing framework :) "
Get Labels of the predicted images as a .txt file,open-mmlab/mmdetection,2021-10-08 16:18:07,0,,6247,1021270591,"I want to know how to get the labels of the predicted images as txt file , I am building an OCR so it would help me to get the labels as txt file so that I can merge them and get txt file as a result , as I don't want  to use any open source libraries like Pytesseract or easyocr."
When will gfocal loss+use_sigmoid=False be implemented?,open-mmlab/mmdetection,2021-10-06 03:29:35,4,enhancement,6235,1017350610,"**Describe the feature**
In gfocal loss, the use_sigmoind parameter must be set to True in latest versions. Or it will raise not implemented error.

**Motivation**
In CVPR2021, GFocal loss v2 is published and it uses a new GFocalHead but an old GFocal loss with use_sigmoid=False.

**Related resources**
You can look at this repo of GFocalV2 based on mmdetection 2.6.0, which is not supported in 2.16.0&2.17.0.
https://github.com/implus/GFocalV2

**Additional context**
It really worths to be implemented. After that you can simply add the config file in repo I mentioned above to support GFocalV2.
"
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpmo5dhu0b/tmpysztf6e9.py',open-mmlab/mmdetection,2021-10-02 12:51:27,3,,6221,1014041017,"Traceback (most recent call last):
  File ""tools/train.py"", line 161, in <module>
    main()
  File ""tools/train.py"", line 68, in main
    cfg = Config.fromfile(args.config)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/utils/config.py"", line 165, in fromfile
    cfg_dict, cfg_text = Config._file2dict(filename)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/utils/config.py"", line 125, in _file2dict
    _cfg_dict, _cfg_text = Config._file2dict(osp.join(cfg_dir, f))
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/utils/config.py"", line 95, in _file2dict
    mod = import_module(temp_module_name)
  File ""/opt/conda/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 724, in exec_module
  File ""<frozen importlib._bootstrap_external>"", line 860, in get_code
  File ""<frozen importlib._bootstrap_external>"", line 791, in source_to_code
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/tmp/tmpmo5dhu0b/tmpysztf6e9.py"", line 3
    model = dict(`
                 ^
SyntaxError: invalid syntax
Exception ignored in: <function _TemporaryFileCloser.__del__ at 0x7f78e349d4d0>
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/tempfile.py"", line 448, in __del__
    self.close()
  File ""/opt/conda/lib/python3.7/tempfile.py"", line 444, in close
    unlink(self.name)
FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpmo5dhu0b/tmpysztf6e9.py'

在mmdetection训练faster-RNN 和ssd的时候出现了问题，但是一开始训练ssd的时候是可以训练的，过几天之后出的这个问题"
Apex AMP erroring out on checkpointing after epoch and pre-trained models can't be loaded for fine-tuning/warm restart,open-mmlab/mmdetection,2021-10-01 21:35:45,1,,6219,1013782320,"Hi, I have created a WandB report that is attached to the issue that outlines the exact model and loss logs of the model. 
[WB_report.pdf](https://github.com/open-mmlab/mmdetection/files/7270223/WB_report.pdf)

The model I am using is [Swin transfomer](https://github.com/microsoft/Swin-Transformer) with Cascade RCNN. From what I deduce, it seems that one of my checkpoints is not being loaded.

1. It seems that the `backbone`s checkpoint is not being loaded - but the detector can be easily loaded.

2. Additionally, I have a dataset of about 5K images - divided into 3 classes. The Batch Size = 8; It may be overfitting, maybe not. I can't diagnose the problems 😢 

3.  I might also be interpreting the model checkpoints incorrectly; I think that the checkpoints [here](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection) should be for the whole model?

## Some errors:-
Interestingly enough,
```
2021-10-01 22:08:13,462 - mmdet - INFO - Set random seed to 42, deterministic: True
2021-10-01 22:08:14,210 - mmdet - INFO - load model from: /kaggle/working/checkpoints/cascade_mask_rcnn_swin_small_patch4_window7.pth
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 52, in build_from_cfg
    return obj_cls(**args)
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmdet/models/detectors/cascade_rcnn.py"", line 25, in __init__
    pretrained=pretrained)
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmdet/models/detectors/two_stage.py"", line 48, in __init__
    self.init_weights(pretrained=pretrained)
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmdet/models/detectors/two_stage.py"", line 68, in init_weights
    self.backbone.init_weights(pretrained=pretrained)
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmdet/models/backbones/swin_transformer.py"", line 594, in init_weights
    load_checkpoint(self, pretrained, strict=False, logger=logger)
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmcv_custom/checkpoint.py"", line 340, in load_checkpoint
    table_current = model.state_dict()[table_key]
KeyError: 'backbone.layers.0.blocks.0.attn.relative_position_bias_table'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tools/train.py"", line 187, in <module>
    main()
  File ""tools/train.py"", line 161, in main
    test_cfg=cfg.get('test_cfg'))
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmdet/models/builder.py"", line 77, in build_detector
    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))
  File ""/kaggle/working/Swin-Transformer-Object-Detection/mmdet/models/builder.py"", line 34, in build
    return build_from_cfg(cfg, registry, default_args)
  File ""/opt/conda/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 55, in build_from_cfg
    raise type(e)(f'{obj_cls.__name__}: {e}')
KeyError: ""CascadeRCNN: 'backbone.layers.0.blocks.0.attn.relative_position_bias_table'""
```
it seems that even if I set `load_from` and `model/pre_trained`  the same checkpoint path, it errors out. Perhaps I am trying the wrong combinations?

<h3>Config File:-</h3>

```py
%%writefile ""/kaggle/working/Swin-Transformer-Object-Detection/configs/cascade_rcnn/cascade_mask_rcnn_swin_small_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_coco.py""
model = dict(
    type='CascadeRCNN',
    pretrained=""/kaggle/working/checkpoints/swin_base_patch4_window7_224_22k.pth"",
    backbone=dict(
        type='SwinTransformer',
        embed_dim=96,
        depths=[2, 2, 6, 2],          # [2, 2, 18, 2] --> [2,2,6,2] = 86M
        num_heads=[3, 6, 12, 24],         #reduce to [3,6,9,12]
        window_size=7,
        mlp_ratio=4.0,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.2,
        ape=False,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        use_checkpoint=True),
    neck=dict(
        type='FPN',
        in_channels=[96, 192, 384, 768],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='ConvFCBBoxHead',
                num_shared_convs=4,
                num_shared_fcs=1,
                in_channels=256,
                conv_out_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=3,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=False,
                reg_decoded_bbox=True,
                norm_cfg=dict(type='BN', requires_grad=True),
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='GIoULoss', loss_weight=10.0)),
            dict(
                type='ConvFCBBoxHead',
                num_shared_convs=4,
                num_shared_fcs=1,
                in_channels=256,
                conv_out_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=3,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=False,
                reg_decoded_bbox=True,
                norm_cfg=dict(type='BN', requires_grad=True),
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='GIoULoss', loss_weight=10.0)),
            dict(
                type='ConvFCBBoxHead',
                num_shared_convs=4,
                num_shared_fcs=1,
                in_channels=256,
                conv_out_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=3,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=False,
                reg_decoded_bbox=True,
                norm_cfg=dict(type='BN', requires_grad=True),
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='GIoULoss', loss_weight=10.0))
        ]),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=0,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_across_levels=False,
            nms_pre=2000,
            nms_post=2000,
            max_per_img=2000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=[
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.5,
                    neg_iou_thr=0.5,
                    min_pos_iou=0.5,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.6,
                    neg_iou_thr=0.6,
                    min_pos_iou=0.6,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False),
            dict(
                assigner=dict(
                    type='MaxIoUAssigner',
                    pos_iou_thr=0.7,
                    neg_iou_thr=0.7,
                    min_pos_iou=0.7,
                    match_low_quality=False,
                    ignore_iof_thr=-1),
                sampler=dict(
                    type='RandomSampler',
                    num=512,
                    pos_fraction=0.25,
                    neg_pos_ub=-1,
                    add_gt_as_proposals=True),
                pos_weight=-1,
                debug=False)
        ]),
    )
dataset_type = 'CocoDataset'
data_root = '/kaggle/working/mmdetection/data/coco'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='AutoAugment',
        policies=[[{
            'type':
            'Resize',
            'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333),
                          (608, 1333), (640, 1333), (672, 1333), (704, 1333),
                          (736, 1333), (768, 1333), (800, 1333)],
            'multiscale_mode':
            'value',
            'keep_ratio':
            True
        }],
                  [{
                      'type': 'Resize',
                      'img_scale': [(400, 1333), (500, 1333), (600, 1333)],
                      'multiscale_mode': 'value',
                      'keep_ratio': True
                  }, {
                      'type': 'RandomCrop',
                      'crop_type': 'absolute_range',
                      'crop_size': (384, 600),
                      'allow_negative_crop': True
                  }, {
                      'type':
                      'Resize',
                      'img_scale': [(480, 1333), (512, 1333), (544, 1333),
                                    (576, 1333), (608, 1333), (640, 1333),
                                    (672, 1333), (704, 1333), (736, 1333),
                                    (768, 1333), (800, 1333)],
                      'multiscale_mode':
                      'value',
                      'override':
                      True,
                      'keep_ratio':
                      True
                  }]]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,        # Batch_size ==> 4/8
    workers_per_gpu=4,
    train=dict(
        type='CocoDataset',
        classes=('fruit_woodiness', 'fruit_brownspot', 'fruit_healthy'),
        ann_file='/kaggle/working/mmdetection/data/coco/annotations/train.json',
        img_prefix='./ID',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='AutoAugment',
                policies=[[{
                    'type':
                    'Resize',
                    'img_scale': [(480, 1333), (512, 1333), (544, 1333),
                                  (576, 1333), (608, 1333), (640, 1333),
                                  (672, 1333), (704, 1333), (736, 1333),
                                  (768, 1333), (800, 1333)],
                    'multiscale_mode':
                    'value',
                    'keep_ratio':
                    True
                }],
                          [{
                              'type': 'Resize',
                              'img_scale': [(400, 1333), (500, 1333),
                                            (600, 1333)],
                              'multiscale_mode': 'value',
                              'keep_ratio': True
                          }, {
                              'type': 'RandomCrop',
                              'crop_type': 'absolute_range',
                              'crop_size': (384, 600),
                              'allow_negative_crop': True
                          }, {
                              'type':
                              'Resize',
                              'img_scale': [(480, 1333), (512, 1333),
                                            (544, 1333), (576, 1333),
                                            (608, 1333), (640, 1333),
                                            (672, 1333), (704, 1333),
                                            (736, 1333), (768, 1333),
                                            (800, 1333)],
                              'multiscale_mode':
                              'value',
                              'override':
                              True,
                              'keep_ratio':
                              True
                          }]]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ],
        data_root='train'),
    val=dict(
        type='CocoDataset',
        classes=('fruit_woodiness', 'fruit_brownspot', 'fruit_healthy'),
        ann_file='/kaggle/working/mmdetection/data/coco/annotations/val.json',
        img_prefix='', #./ID
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        data_root='val'),
    test=dict(
        type='CocoDataset',
        ann_file='/kaggle/working/mmdetection/data/coco/annotations/val.json',
        img_prefix='', #ID
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1333, 800),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(metric=['mAP', 'bbox'], classwise=True, interval=1)
optimizer = dict(
    type='AdamW',
    lr=0.0025,
    betas=(0.9, 0.999),
    weight_decay=0.05,
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            relative_position_bias_table=dict(decay_mult=0.0),
            norm=dict(decay_mult=0.0))))
optimizer_config = dict(
    grad_clip=None,
    type='DistOptimizerHook',
    update_interval=1,
    coalesce=True,
    bucket_size_mb=-1,
    use_fp16=True)
lr_config = dict(
    policy='step',
    warmup='exponential',
    warmup_iters=500,
    warmup_ratio=0.333,
    step=[27, 33])
runner = dict(type='EpochBasedRunnerAmp', max_epochs=10)
checkpoint_config = dict(interval=2)
log_config = dict(interval=25, with_step=True, by_epoch=True, hooks=[dict(type='TextLoggerHook'), dict(type='TensorboardLoggerHook'), dict(type='WandbLoggerHook', init_kwargs=dict(project='makere-passion', name='neel'))])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = ""/kaggle/working/checkpoints/cascade_mask_rcnn_swin_small_patch4_window7.pth""
resume_from = None
workflow = [('train', 1)]
fp16 = None
work_dir = './results'
seed = 42
gpu_ids = 0
```
"
[Object detection & Keypoint regression] MMDetection VS MMPose VS Detectron2 ,open-mmlab/mmdetection,2021-09-30 11:15:26,1,community discussion,6202,1012047483,"The project I'm working on involve object detection and single keypoint detection (onto the object).
I have difficulties picking the best framework:
- **MMDetection** model zoo seems very vast with SOTA, but does not offer keypoint detection it seems.
    - Maybe I could write my own keypoint head and add it ? 
- **MMPose** seems to does keypoint regression, but only for human, and the outputed BoundingBox (important for me) might not be accurate since the main goal is only pose detection 
- **Detectron2** seems easy to use and does both, but the model zoo seems small.
Anyone has some tipps on which framework to choose ?"
potential bug of swin init_weights ,open-mmlab/mmdetection,2021-09-28 10:14:31,1,,6186,1009556168,"Hi,

Thanks for your work on mmdet.

It seems that swin init_weights function may have unexpected behavior when the checkpoint is with old version.

https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/backbones/swin.py#L697

If in the checkpoint, the states are with backbone. prefix, the swin_converter will not do the right thing. At the same time, the swin_converter will add ""backbone."" prefix at last, which seems redundant.
"
How to modify the codes and find the configs after installed with mim?,open-mmlab/mmdetection,2021-09-26 02:39:41,2,,6165,1007254246,"After installed with mim I can import packages, but how can I modify the codes and find the configs mentioned in the document? I can not find the mmdet in my home."
Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same,open-mmlab/mmdetection,2021-09-25 12:36:02,3,,6162,1007075315,"This problem occurs when modifying the network model FPN. Do you know how to solve it?
 Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
adding export_onnx for QueryInst/Sparse RCNN,open-mmlab/mmdetection,2021-09-20 03:06:41,2,enhancement#community help wanted,6135,1000526115,"Currently, I'm working on this function and I'd like to contribute my work to our community.

**Describe the feature**

**Motivation**
A clear and concise description of the motivation of the feature.
Ex1. It is inconvenient when [....].
Ex2. There is a recent paper [....], which is very helpful for [....].

**Related resources**
If there is an official code release or third-party implementations, please also provide the information here, which would be very helpful.

**Additional context**
Add any other context or screenshots about the feature request here.
If you would like to implement the feature and create a PR, please leave a comment here and that would be much appreciated.
"
Include model details in the model zoo page,open-mmlab/mmdetection,2021-09-19 19:05:34,7,enhancement,6134,1000392818,"**Describe the feature**
For each model included in the [model zoo](https://mmdetection.readthedocs.io/en/latest/model_zoo.html), include the model details (learning schedule, train time, inference time, box AP,  mask AP (for segmentation tasks), etc.) in the model zoo page, rather than in linked pages.
**Motivation**
One of the main reasons for having a model zoo page is to be able to quickly compare different models in terms of performance, such as train time, inference time, box AP, etc. This is easy to do with, e.g., the [Detectron 2 model zoo](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md) since the model zoo page collects these quantities for each model in a series of tables. Instead, [MMDetection model zoo](https://mmdetection.readthedocs.io/en/latest/model_zoo.html)  is really inconvenient in this respect, because you have to navigate to each model's page in order to see these details, and thus model comparison becomes needlessly complicated.

**Related resources**
None.

**Additional context**
None."
[Feature Request] More documentation on creating config files?,open-mmlab/mmdetection,2021-09-19 12:17:32,3,,6133,1000297567,"I have noticed that the beginners to this amazing library are often confused by lack of documentation on `config` which is arguably the most important aspect for training these models.

For example, suppose I wish to use config file from Swin (https://github.com/SwinTransformer/Swin-Transformer-Object-Detection/blob/master/configs/swin/cascade_mask_rcnn_swin_small_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_coco.py) from quick googling, it seems that I need to modify these parameters:-

```py
cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py')
 # Modify dataset type and path
 cfg.dataset_type = 'KittiTinyDataset'
 cfg.data_root = 'kitti_tiny/'
 cfg.data.test.type = 'KittiTinyDataset'
 cfg.data.test.data_root = 'kitti_tiny/'
 cfg.data.test.ann_file = 'train.txt'
 cfg.data.test.img_prefix = 'training/image_2'
 cfg.data.train.type = 'KittiTinyDataset'
 cfg.data.train.data_root = 'kitti_tiny/'
 cfg.data.train.ann_file = 'train.txt'
 cfg.data.train.img_prefix = 'training/image_2'
 cfg.data.val.type = 'KittiTinyDataset'
 cfg.data.val.data_root = 'kitti_tiny/'
 cfg.data.val.ann_file = 'val.txt'
 cfg.data.val.img_prefix = 'training/image_2'
 # modify num classes of the model in box head
 cfg.model.roi_head.bbox_head.num_classes = 3
 # We can still use the pre-trained Mask RCNN model though we do not need to
 # use the mask branch
 cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'
 # Set up working dir to save files and logs.
 cfg.work_dir = './tutorial_exps'
 # The original learning rate (LR) is set for 8-GPU training.
 # We divide it by 8 since we only use one GPU.
 cfg.optimizer.lr = 0.02 / 8
 cfg.lr_config.warmup = None
 cfg.log_config.interval = 10
 # Change the evaluation metric since we use customized dataset.
 cfg.evaluation.metric = 'mAP'
 # We can set the evaluation interval to reduce the evaluation times
 cfg.evaluation.interval = 12
 # We can set the checkpoint saving interval to reduce the storage cost
 cfg.checkpoint_config.interval = 12
 # Set seed thus the results are more reproducible
 cfg.seed = 0
 set_random_seed(0, deterministic=False)
 cfg.gpu_ids = range(1) 
```

Suppose I want to know all parameters which are available for me to modify the `config` file - where I can obtain this resource?
How can I verify that I have completed most of the required parameters in the config file required for training?

This is not a complaint, just that if there can be documentation (a single line would do) on each config aspect and how it might change, it would be wonderful to know what I am doing wrong and most importantly - **how my config file would look like**, to update it and use my own custom COCO dataset?

A prime example is this:-
```py
# Modify dataset type and path
cfg.data_root = '/kaggle/working/mmdetection/data/coco'
cfg.data.train.type = 'COCODataset'
cfg.data.train.data_root = 'train'
cfg.data.train.img_prefix = './ID'
cfg.data.val.type = 'COCODataset'
cfg.data.val.data_root = 'val'
cfg.data.val.img_prefix = './ID'
```
Suppose I have a dir structure of this:-
```py
mmdetection
-| data
---| coco
-----| annotations
-------| train.json
-------| val.json
-----| train
-------| ID_2v.jpg
-------| .....
-----| val
-------| ID_1v.jpg
-------| .....
```
how can I verify this is correct, unless I have seen an example of this, or at least the documentation describes exactly `img_prefix` works? is it like `ID*`?

<h2>Second:-</h2>
Small sample code on how to edit configs would be great too

```py
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
```
in this case, are we supposed to do `cfg.train_pipieline[1].with_mask = False`? 
I know that trial and error can be done here, but its still pretty good if small samples are there to explain/give a hint how its supposed to be done 👍 


Thanks for reading this and understanding,
Cheers,
Neel04"
Training pipeline transformation order?,open-mmlab/mmdetection,2021-09-14 20:27:50,1,enhancement#community help wanted#good first issue,6106,996404707,"I'm training various model heads (HTC, cascade mask-rcnn, etc.) with the CBNetV2 backbone (implementation [here](https://github.com/VDIGPKU/CBNetV2)) on a custom coco-format dataset with only bboxes. I'm using the following training and testing pipelines:

```python
albu_train_transforms = [
    dict(
        type='ShiftScaleRotate',
        shift_limit=0.0625,
        scale_limit=0.0,
        rotate_limit=0,
        interpolation=1,
        p=0.5),
    dict(
        type='RandomBrightnessContrast',
        brightness_limit=[0.1, 0.3],
        contrast_limit=[0.1, 0.3],
        p=0.2),
    dict(
        type='OneOf',
        transforms=[
            dict(
                type='RGBShift',
                r_shift_limit=10,
                g_shift_limit=10,
                b_shift_limit=10,
                p=1.0),
            dict(
                type='HueSaturationValue',
                hue_shift_limit=20,
                sat_shift_limit=30,
                val_shift_limit=20,
                p=1.0)
        ],
        p=0.1),
    dict(type='JpegCompression', quality_lower=85, quality_upper=95, p=0.2),
    dict(type='ChannelShuffle', p=0.1),
    dict(
        type='OneOf',
        transforms=[
            dict(type='Blur', blur_limit=3, p=1.0),
            dict(type='MedianBlur', blur_limit=3, p=1.0)
        ],
        p=0.1),
]
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),
    dict(
        type='Resize',
        img_scale=[(1600, 400), (1600, 1400)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.),
    dict(type='Pad', size_divisor=32),
    dict(
        type='Albu',
        transforms=albu_train_transforms,
        bbox_params=dict(
            type='BboxParams',
            format='pascal_voc',
            label_fields=['gt_labels'],
            min_visibility=0.0,
            filter_lost_elements=True),
        keymap={
            'img': 'image',
            'gt_bboxes': 'bboxes',
        },
        update_pad_shape=False,
        skip_img_without_anno=True),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_bboxes', 'gt_labels'])
]
```
```python
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1600, 1400),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize',**img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
```

And variations of it (e.g. sometimes I remove the Albumentations sequence). However, it seems to be very sensitive to the order of operations--for example, when I used the below pipelines, the training loss decayed nicely but the test bbox evaluation metrics were all zero and the generated detections look all random...

```python
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='LoadAnnotations',
        with_bbox=True,
        with_mask=False,
        with_seg=False),
    dict(
        type='Resize',
        img_scale=[(800.0, 200.0), (800.0, 700.0)],
        multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='SegRescale', scale_factor=0.125),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
```
```python
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(800.0, 700.0),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
```
However, once I switched 'RandomFlip' and 'Resize' in the training pipeline (the order of which seems to vary between different configs?) and made the image size its original 1600x400-1400 range (from e.g. [this config](https://github.com/VDIGPKU/CBNetV2/blob/main/configs/cbnet/cascade_mask_rcnn_cbv2_swin_base_patch4_window7_mstrain_400-1400_adamw_3x_coco.py)), it worked.

It seems to be a data pipeline issue. I couldn't find any documentation on the allowed order of data transformations, and from reading the comments on the operations in the [transforms.py](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/pipelines/transforms.py) script, it seems like you can just sequence them pretty much randomly (besides starting with loading and ending with collecting). But that doesn't explain why it keeps having these seemingly arbitrary issues."
About deformable detr,open-mmlab/mmdetection,2021-09-11 04:34:43,8,enhancement,6082,993745598,"RuntimeError: ""ms_deform_attn_forward_cuda"" not implemented for 'Half'

Does deformable detr not support fp16?
"
Import slow problem,open-mmlab/mmdetection,2021-09-10 02:15:48,2,community discussion,6072,992823682,"import mmdet.apis is extremely slower than other packages.

```
python -c ""import logging""  0.03s user 0.01s system 98% cpu 0.044 total
python -c ""import torch""  0.75s user 0.56s system 135% cpu 0.961 total
python -c ""import torchvision""  0.82s user 0.55s system 130% cpu 1.046 total
python -c ""import detectron2""  0.74s user 0.36s system 100% cpu 1.099 total
python -c ""import detectron2.modeling""  0.93s user 0.42s system 108% cpu 1.244 total
python -c ""import detectron2.engine""  1.21s user 0.34s system 104% cpu 1.475 total
python -c ""import mmcv""  1.37s user 0.64s system 105% cpu 1.911 total
python -c ""import mmdet""  1.21s user 0.60s system 106% cpu 1.709 total
python -c ""import mmdet.models""  4.15s user 0.96s system 110% cpu 4.638 total
python -c ""import mmdet.apis""  4.52s user 0.95s system 108% cpu 5.021 total
```

package | time(s)
-- | --
logging | 0.044
torch | 0.961
torchvision | 1.046
detectron2 | 1.099
detectron2.modeling | 1.244
detectron2.engine | 1.475
mmcv | 1.911
mmdet | 1.709
mmdet.models | 4.638
mmdet.apis | 5.021

![image](https://user-images.githubusercontent.com/10473170/132788296-51932a2b-4c07-4f0d-a98d-c8383b41f7c8.png)
"
CenterMask implementation,open-mmlab/mmdetection,2021-09-02 08:35:10,1,enhancement#community help wanted,6015,986373100,"As a better and more interesting model than YOLOACT, It would be really nice to have [CenterMask](https://arxiv.org/pdf/1911.06667.pdf) model implemented."
[ONNX export] Support image normalization in graph,open-mmlab/mmdetection,2021-08-26 23:12:57,1,enhancement#deployment,5958,980717352,"**Describe the feature**
Add option to include the image normalization preprocessing step in the ONNX graph.

**Motivation**
In order to export models to truly portable ONNX files, the preprocessing steps need to be included in the graph as much as possible. There is also a computational efficiency aspect, as normalization would happen on the GPU (e.g. see https://github.com/open-mmlab/mmdetection/issues/4488)

**Related resources**
A potential implementation would consist in passing the `normalize_cfg` to the `forward` method so that it can be used to normalize the image. I.e. adding the block below at https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/detectors/base.py#L169
```
            onnx_input = img[0]
            normalize_cfg = kwargs.get('normalize_cfg', None)
            if normalize_cfg:
                mean = torch.tensor(normalize_cfg['mean'])[..., None, None]
                std = torch.tensor(normalize_cfg['std'])[..., None, None]
                onnx_input = (img[0] - mean) / std
            return self.onnx_export(onnx_input, img_metas[0])
```

Then `tools/deployment/pytorch2onnx.py` can be modified accordingly to support the option.

**Additional context**
If approved, I'd be happy to open a PR with the changes above (and/or alternative/additional suggestions)."
How to add the parameter gt_bboxes_ignore in model,open-mmlab/mmdetection,2021-08-20 14:24:18,2,usage,5927,975658963,"I am using maxiouassigner but I need gt_bboxes_ignore parameter when I use it, I don't know how to pass this parameter to forward_trian()."
"ImportError: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running",open-mmlab/mmdetection,2021-08-14 03:49:55,5,,5885,970812955,"
**Error traceback**


```none
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/multiprocessing/spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/multiprocessing/spawn.py"", line 114, in _main
    prepare(preparation_data)
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/multiprocessing/spawn.py"", line 225, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/multiprocessing/spawn.py"", line 277, in _fixup_main_from_path
    run_name=""__mp_main__"")
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/zhanghong/mmdetection/tools/train.py"", line 15, in <module>
    from mmdet.apis import set_random_seed, train_detector
  File ""/home/zhanghong/mmdetection/mmdet/apis/__init__.py"", line 1, in <module>
    from .inference import (async_inference_detector, inference_detector,
  File ""/home/zhanghong/mmdetection/mmdet/apis/inference.py"", line 11, in <module>
    from mmdet.datasets import replace_ImageToTensor
  File ""/home/zhanghong/mmdetection/mmdet/datasets/__init__.py"", line 2, in <module>
    from .cityscapes import CityscapesDataset
  File ""/home/zhanghong/mmdetection/mmdet/datasets/cityscapes.py"", line 16, in <module>
    from .coco import CocoDataset
  File ""/home/zhanghong/mmdetection/mmdet/datasets/coco.py"", line 14, in <module>
    from .api_wrappers import COCO, COCOeval
  File ""/home/zhanghong/mmdetection/mmdet/datasets/api_wrappers/__init__.py"", line 1, in <module>
    from .coco_api import COCO, COCOeval
  File ""/home/zhanghong/mmdetection/mmdet/datasets/api_wrappers/coco_api.py"", line 6, in <module>
    from pycocotools.coco import COCO as _COCO
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/site-packages/pycocotools/coco.py"", line 49, in <module>
    import matplotlib.pyplot as plt
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2500, in <module>
    switch_backend(rcParams[""backend""])
  File ""/home/zhanghong/anaconda3/envs/openmmlab/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 288, in switch_backend
    newbackend, required_framework, current_framework))
ImportError: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running
.
```

**Bug fix**
If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
ImportError: cannot import name 'deform_conv_cuda' from 'ops.dcn.src' (unknown location),open-mmlab/mmdetection,2021-07-24 12:06:31,5,,5681,952062467,我在使用mmdet是找不到deform_conv_cuda，这个应该是c++的文件加载失败，请问怎么修改，有人遇到相同的问题吗
mmdetection v1.0 cascade rcnn模型 转成v2后finetune问题,open-mmlab/mmdetection,2021-07-24 08:56:26,1,,5679,952033809,"mmdetection v1.0 cascade rcnn模型 转成v2后finetune，与直接在mmdet v1.0 finetune对比，发现v2版本 loss刚开始为8.1，第一个epoch后map 为74.8。

`2021-07-24 16:36:45,713 - mmdet - INFO - Epoch [1][100/14009]	lr: 4.653e-05, eta: 13 days, 22:43:14, time: 2.868, data_time: 0.802, memory: 25562, loss_rpn_cls: 0.0145, loss_rpn_bbox: 0.0251, s0.loss_cls: 4.3144, s0.acc: 45.0299, s0.loss_bbox: 0.1248, s1.loss_cls: 2.4995, s1.acc: 35.3806, s1.loss_bbox: 0.2038, s2.loss_cls: 1.3757, s2.acc: 26.1961, s2.loss_bbox: 0.1660, loss: 8.7236, grad_norm: 40.1798
INFO:mmdet:Epoch [1][100/14009]	lr: 4.653e-05, eta: 13 days, 22:43:14, time: 2.868, data_time: 0.802, memory: 25562, loss_rpn_cls: 0.0145, loss_rpn_bbox: 0.0251, s0.loss_cls: 4.3144, s0.acc: 45.0299, s0.loss_bbox: 0.1248, s1.loss_cls: 2.4995, s1.acc: 35.3806, s1.loss_bbox: 0.2038, s2.loss_cls: 1.3757, s2.acc: 26.1961, s2.loss_bbox: 0.1660, loss: 8.7236, grad_norm: 40.1798
`
v1.0 直接finetune，刚开始loss为0.6，第一个epoch map为78，已经收敛
`2021-05-31 12:09:32,203 - INFO - Epoch [1][100/15757]	lr: 0.00005, eta: 2 days, 18:25:47, time: 1.899, data_time: 0.274, memory: 16455, loss_rpn_cls: 0.0119, loss_rpn_bbox: 0.0203, s0.loss_cls: 0.1438, s0.acc: 94.4299, s0.loss_bbox: 0.0904, s1.loss_cls: 0.0782, s1.acc: 93.8354, s1.loss_bbox: 0.1670, s2.loss_cls: 0.0401, s2.acc: 93.6331, s2.loss_bbox: 0.1367, loss: 0.6883`
两次训练数据都一样，v1的config如下
```
model= dict(
    type='CascadeRCNN',
    num_stages=3,
    #pretrained='/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/Models/pretrain_models/open-mmlab/resnext101_64x4d-ee2c6f71.pth',
    backbone=dict(
        type='ResNeXt',
        depth=101,
        groups=64,
        base_width=4,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_scales=[8, 16, 32, 64],
        anchor_ratios=[0.5, 1.0, 2.0],
        anchor_strides=[4, 8, 16, 32, 64],
        target_means=[.0, .0, .0, .0],
        target_stds=[1.0, 1.0, 1.0, 1.0],
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),
    bbox_roi_extractor=dict(
        type='SingleRoIExtractor',
        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),
        out_channels=256,
        featmap_strides=[4, 8, 16, 32]),
    bbox_head=[
        dict(
            type='SharedFCBBoxHead',
            num_fcs=2,
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=11,
            target_means=[0., 0., 0., 0.],
            target_stds=[0.1, 0.1, 0.2, 0.2],
            reg_class_agnostic=True,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
        dict(
            type='SharedFCBBoxHead',
            num_fcs=2,
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=11,
            target_means=[0., 0., 0., 0.],
            target_stds=[0.05, 0.05, 0.1, 0.1],
            reg_class_agnostic=True,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),
        dict(
            type='SharedFCBBoxHead',
            num_fcs=2,
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=11,
            target_means=[0., 0., 0., 0.],
            target_stds=[0.033, 0.033, 0.067, 0.067],
            reg_class_agnostic=True,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
    ])
# model training and testing settings
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.4,
                neg_iou_thr=0.4,
                min_pos_iou=0.4,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ],
    stage_loss_weights=[1, 0.5, 0.25])
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05, nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05), max_per_img=100),
    keep_all_stages=False)
# dataset settings
dataset_type = 'VOCDataset'

img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=[(1000, 800),(1600,1200)], keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1600, 1200),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
data = dict(
    imgs_per_gpu=3,
    workers_per_gpu=16,
    train=dict(
        type=dataset_type,
        ann_file='/media/senter/disk2/ShuDianTongDao/00_TrainData/imagesets/20210531_train.txt',
        img_prefix='/media/senter',
        pipeline=train_pipeline),
    val=dict(
        type=dataset_type,
        ann_file='/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/ShuDianYinHuanDataset/00_DatasetComplete/01TrainTestDataset/EvaluateDataset20200912-useless/FillNiaoHai16781/images.txt',
        img_prefix='/media/senter',
        pipeline=test_pipeline),
    test=dict(
        type=dataset_type,
        ann_file='/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/ShuDianYinHuanDataset/00_DatasetComplete/01TrainTestDataset/EvaluateDataset20200912-useless/FillNiaoHai16781/images.txt',
        img_prefix='/media/senter',
        pipeline=test_pipeline))
# optimizer
optimizer = dict(type='SGD', lr=0.0001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=1.0 / 3,
    step=[3, 5])
checkpoint_config = dict(interval=1)
# yapf:disable
log_config = dict(
    interval=100,
    hooks=[
        dict(type='TextLoggerHook'),
        dict(type='TensorboardLoggerHook')
    ])
# yapf:enable
# runtime settings
total_epochs = 8
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = '/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/Models/trained_models/ShuDianTongDao/20210531_cascade_rcnn_x101_64x4d_fpn_old_10cls'
load_from = None#'/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/Models/trained_models/ShuDianTongDao/20210227_cascade_rcnn_x101_64x4d_fpn_old_10cls/epoch_13.pth'
resume_from = '/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/Models/trained_models/ShuDianTongDao/20210531_cascade_rcnn_x101_64x4d_fpn_old_10cls/epoch_1.pth'
workflow = [('train', `1)]`
```
v2 config为
```
`model = dict(
    type='CascadeRCNN',
    pretrained=
    '/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/Models/pretrain_models/open-mmlab/resnext101_64x4d-ee2c6f71.pth',
    backbone=dict(
        type='ResNeXt',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        style='pytorch',
        groups=64,
        base_width=4),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8, 16, 32, 64],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='CascadeRoIHead',
        num_stages=3,
        stage_loss_weights=[1, 0.5, 0.25],
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=10,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=10,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=10,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0.0, 0.0, 0.0, 0.0],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]))
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
		nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_per_img=2000,
        nms=dict(type='nms', iou_threshold=0.7),
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.4,
                neg_iou_thr=0.4,
                min_pos_iou=0.4,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='OHEMSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ],
    stage_loss_weights=[1, 0.5, 0.25])
test_cfg = dict(
    rpn=dict(
		nms_across_levels=False,
        nms_pre=1000,
		nms_post=1000,
        max_per_img=1000,
        nms=dict(type='nms', iou_threshold=0.7),
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05),
        max_per_img=100),
    keep_all_stages=False)
dataset_type = 'VOCDataset'
data_root = '/media/senter'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        type='Resize',
        img_scale=[(1000, 800), (1600, 1200)],
        #multiscale_mode='range',
        keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1600, 1200),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=3,
    workers_per_gpu=16,
    train=dict(
        type='VOCDataset',
        ann_file=
        '/media/senter/disk2/ShuDianTongDao/00_TrainData/imagesets/20210608_train_.txt',
        img_prefix='/media/senter',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                type='Resize',
                img_scale=[(1000, 800), (1600, 1200)],
                #multiscale_mode='range',
                keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='VOCDataset',
        ann_file=
        '/media/senter/0a0d2450-80d0-4ab4-87c4-45613e982e6b/ShuDianYinHuanDataset/00_DatasetComplete/01TrainTestDataset/EvaluateDataset20200912-useless/FillNiaoHai16781/images.txt',
        img_prefix='/media/senter',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1600, 1200),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='VOCDataset',
        ann_file='home/bj/mmdetection_202105/images2.txt',
        img_prefix='/media/senter',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1600, 1200),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric='mAP')
optimizer = dict(type='SGD', lr=0.0001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    step=[10, 15])
runner = dict(type='EpochBasedRunner', max_epochs=30)
checkpoint_config = dict(interval=1)
log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = ""2020_mm_v44.pth""
resume_from = None#""2020_mm_v44.pth""
workflow = [('train', 1)]
work_dir = './work_dirs/jxgs_from_v44'
gpu_ids = range(0, 4)
```"
ImportError: undefined symbol,open-mmlab/mmdetection,2021-07-21 07:57:16,8,reimplementation,5663,949433710,"I got a problem like:
`ImportError: /data2/hongpingting/anaconda3/lib/python3.7/site-packages/mmdet-2.1.0+unknown-py3.7-linux-x86_64.egg/mmdet/ops/corner_pool/corner_pool_ext.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceISt7complexIdEEEPKNS_6detail12TypeMetaDataEv`
and have no idea how to fix it"
How to adjust stride in RPN_head in fpn,open-mmlab/mmdetection,2021-07-21 04:55:25,1,,5661,949322922,
TensorRT does not show any speed improvement ???,open-mmlab/mmdetection,2021-07-21 01:38:52,1,deployment,5658,949240506,"Hello, I try to convert `cascade_rcnn_r50 to tensorRT`, and comparing the speed. However, I don;t see any improvement at all in term of speed, while the accuracy slightly drops ?

I use the `deployment/test.py` for TensorRT, and `tools/test.py` for Pytorch model, evaluated on COCO Val2017. 
+ Pytorch: 5000/5000, **15.6 task/s,** elapsed: 320s. mAP **40.3** 
+ TensorRT : 5000/5000, **15.8 task/s**, elapsed: 316s. mAP: **40.1**"
OHEM missing  2 required positional arguments: 'num' and 'pos_fraction',open-mmlab/mmdetection,2021-07-09 17:09:08,0,,5578,940935844,"When I am using OHEM, the error occured as follows: 
TypeError: OHEMSampler: __init__() missing 2 required positional arguments: 'num' and 'pos_fraction'
 
I just followed the tutorial in [https://github.com/open-mmlab/mmdetection/blob/master/configs/faster_rcnn/faster_rcnn_r50_fpn_ohem_1x_coco.py] (url)

So confusing ! "
Integrating Kornia batch augmentation pipeline.,open-mmlab/mmdetection,2021-07-09 02:43:04,3,community discussion,5572,940364055,"Hi mmdet maintainers,

Thanks for the repo. We are the project lead of [Kornia](https://github.com/kornia/kornia), and thinking of the possibilities of integrating Kornia augmentation pipeline into mmdet.

From the feature-level, we can provide:
- GPU/CPU/TPU-based batch data augmentation.
- More augmentation options, like [PatchSequential](https://kornia-tutorials.readthedocs.io/en/latest/data_patch_sequential.html).
- Pytorch-native deployable image processing using torch.jit / onnx.
- Differentiability, which has already been adopted in applications like in GANs.

Motivation:
mmdet data pipeline happened outside the PyTorch computation graph and computed on CPUs in a image-by-image manner, which might increase the deployment difficulties and time legacy whilst processing large batches. With Kornia, it is much easier to include test time image processing pipelines into the model whilst export and deployment. Also, the image preprocessing could enjoy the GPU accerlations provided by PyTorch.

Our proposal:
- As demonstrated above, our whole pipeline happens differently than common CPU-based augmentations. Thus, we probably need to introduce a ```batch_pipeline``` module for such operations. Commonly:
```python
kornia_aug = ...
for image, label in dataloaders:
    image = kornia_aug(image)
    logits = model(image)
```
The user interface shall be like:
```
model = dict(
    preprocessing=dict(
        dict(type=""RandomHorizontalFlip"", p=0.5),
        dict(type=""ColorJitter"", hue=0.1, saturation=0.3),
    ),
    backbone=dict(
        depth=101,
        init_cfg=dict(
            type='Pretrained',
            checkpoint='open-mmlab://detectron2/resnet101_caffe')))
```
- we might include the test-time data processing into the exported model as well.

"
Low localization accuracy for Grid RCNN Model,open-mmlab/mmdetection,2021-07-08 06:32:27,0,reimplementation,5565,939519447,"Hi,
Here i am trying to get more precise object localization. And i used grid rcnn config (grid_rcnn_x101_64x4d_fpn_gn-head_2x_coco.py) to train the model.
I  bench marked the location accuracy of grid rcnn with faster rcnn by assuming that grid rcnn will give more accurate position of the object than faster rcnn. But in the experiment,faster rcnn's location accuracy was better than grid rcnn.
So to get better localization accuracy,do i need to edit any parameters in grid rcnn config for custom dataset?"
Are there evaluations of detections on images and mAP on wandblogger?,open-mmlab/mmdetection,2021-07-03 09:15:17,2,,5517,936212969,I cannot find tutorials or if mmdetection produces results of images with annotations when evaluating while training as well as mAP metric. 
Assign GPU to null,open-mmlab/mmdetection,2021-06-25 16:33:23,0,,5458,930330612,How to assign GPU parameter to null when testing demo image?
mIoU evaluation metric,open-mmlab/mmdetection,2021-06-14 12:14:45,1,,5352,920352169,"Hello everyone,

I am trying to compute the mean intersection over union in mmdetection (instance segmentation). 

What I did so far is :

1. Sort the detection by score detection and assign every groundtruth to the best detection. With that, I computed the IoU of every pair ground truth - best detection. Finally, using the function [mean_iou from mmsegmentation](https://mmsegmentation.readthedocs.io/en/latest/_modules/mmseg/core/evaluation/metrics.html?highlight=mean%20iou) I got the mean IoU.
2. Using the iou computed by the cocoeval.py [(self.ious),](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py) I iterated through all the instances and kept the instance with the highest IoU per object. Finally I computed the mean of this kept IoUs

The issue here is that the mIoU values seem very high, e.g (using the first method):

**- Model 1:**
-  mAP50 : 0.456
-  mIoU: 0.64

**- Model 2:**
-  mAP50 : 0.77
-  mIoU: 0.78

**- Model 3:**
-  mAP50 : 0.665
-  mIoU: 0.89
 
Am I approaching the problem wrong? Should I keep all the instances without filtering the best? How should I treat the instances? I have found some information about this problem in semantic segmentation but not in instance segmentation.


Thank you in advance!


"
Adding a regression head to regress a new feature,open-mmlab/mmdetection,2021-06-11 02:59:18,4,community help wanted#reimplementation,5337,918156264,"I am using **mmdetection project** for disease detection. I have converted my dataset in COCO Format and added a new field in the annotation file, called ""severity_score""(value ranges from 0 to 1). This I want to feed along with bounding box coordinates and class labels. I have run the following command to run **mmdetection's** FasterRCNN and FCOS models:

python tools/train.py configs/fcos/fcos_r50_caffe_fpn_gn-head_1x_coco.py
and
python tools/train.py configs/faster_rcnn/faster_rcnn_r101_fpn_2x_coco.py

Now I am trying to feed my dataset to regress a new feature in the annotation file. I understand I need to modify 
https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/roi_heads/bbox_heads/bbox_head.py.
But **mmdetection** seems a big project I am not quite getting to feed a new field from ground truth and add a new regression head which files should I look into. Any suggestion regarding this would be greatly appreciated.
Also, for a beginner, what kind of model would be convenient i.e. anchor free or anchor models? I would like this feature in the existing code, without registering new heads/modules if possible.


@hellock please if you have some time, I would be thankful for your help. 

"
How to update the config file for other ResNet backbones ?,open-mmlab/mmdetection,2021-06-06 16:04:23,3,Doc,5306,912874133,"As for the ResNet101 config file provided I assumed it would just update the `pretrained` file and the `depth` argument to work on other backbones with ResNet.
```python
_base_ = './faster_rcnn_r50_fpn_1x_coco.py'
model = dict(pretrained='torchvision://resnet101', backbone=dict(depth=101))
```
But it seems that you have to update the config file **by hand** for others. In particular, the argument `in_channels` in `model.neck` 

I would like to train my model on the ResNet **18**, **34** and **152**. What exactly do I need to change in the config file?  
In particular, how do I know which `in_channels` list to put in my config file ?

With just:
```python
base = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'
cfg = Config.fromfile(base)

cfg.model.pretrained = 'torchvision://resnet18'
cfg.model.backbone.depth = 18
cfg.load_from = 'checkpoints/resnet18-5c106cde.pth'
```
 a `RuntimeError: Given groups=1, weight of size [256, 1, 1, 1], expected input[2, 64, 320, 320] to have 1 channels, but got 64 channels instead` error appears. 

This seems to be a recurring question on the framework. Thanks !"
Get error when use albumentations: It seems that scikit-image has not been built correctly.,open-mmlab/mmdetection,2021-06-04 15:55:17,7,,5293,911644330,"- [x] : I have read the checklist.

# bug description

background: The server I am using is not connected to the Internet therefore cannot use `pip`,  so, I packed the Albu in local (linux os):

```py
pip install -U albumentations

pip download albumentations -d \tmp
```

then, I uploaded the `\tmp` to my server and run 

```sh
pip install albumentations-1.0.0-py3-none-any.whl --no-index --find-links=/home/ljw/pyenv/
```

then has shown :

```sh
Successfully installed albumentations-1.0.0
```

and In the same environment, I could use `albumentations`:

```py
(GPU-info) [ljw@gpu05 ~]$ python
Python 3.7.9 (default, Aug 31 2020, 12:42:55)
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import albumentations
>>> albumentations.__version__
'1.0.0'
```

But in mmdetection: 

```py
try:
    import albumentations
    from albumentations import Compose
except ImportError:
    corrupt = None
    albumentations, Compose = None, None
```

above script got error: `albumentations is not installed`. Then I have tried to remove the import block from `try`:

```py
import albumentations
from albumentations import Compose
```

But it got error:

```sh
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 124, in <module>
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 124, in <module>
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 124, in <module>
    import skimage
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 124, in <module>
    _raise_build_error(e)
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 104, in _raise_build_error
    _raise_build_error(e)
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 104, in _raise_build_error
    _raise_build_error(e)
  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 104, in _raise_build_error
        %s"""""" % (e, msg))_raise_build_error(e)

    ImportError  File ""/home/anaconda3/envs/GPU-info/lib/python3.7/site-packages/skimage/__init__.py"", line 104, in _raise_build_error
%s"""""" % (e, msg)): 
dlopen: cannot load any more object with static TLS
It seems that scikit-image has not been built correctly.
```

In the same virtual environment, I can't import `albumentations` when use `mmdetection`, but can import `albumentations` when not use `mmdetection` (so poor English). How to fix it? and here is my package version and [script](https://github.com/muyuuuu/mmdetection/blob/master/configs/faster_rcnn/mynet.py):

![image](https://user-images.githubusercontent.com/43681138/120829337-ddf8d700-c58f-11eb-97f8-5d29b0a0e372.png)
"
"In two stage model, will RPN's regression output, bbox_preds, be in the computation graph of downstream BBoxHead?",open-mmlab/mmdetection,2021-05-27 11:45:49,1,,5235,903604293,"According to the following dataflow
1, (rpn_bbox_preds, anchor) -> proposals
2, (proposals, gt_bboxes) -> samping_results
3, (samping_results, target_gt_bboxes) -> target_delta
4, (BBoxHead's bbox_preds, target_delta) -> loss_bbox
It seems rpn_bbox_preds will recieve gradient from loss_bbox.backward(), and I do not seem any place in code base where proposals are detached. Am I missing something or it should be just like this?"
how to get the proposal pkl files ?  rpn_r50_fpn_1x_val2017.pkl,open-mmlab/mmdetection,2021-05-22 12:00:31,1,,5203,898788278,"how to do that?
#2649

`You can test a RPN model on the dataset to get the proposals.
`
@hellock
 Could be more detailed?"
How to inference with model.half,open-mmlab/mmdetection,2021-05-18 12:59:38,3,,5182,894353654,"```import torch
# torch.backends.cudnn.benchmark = True
# torch.backends.cudnn.enabled = True

from mmdet.apis import init_detector, inference_detector, show_result_pyplot
import mmcv
# Load model
config_file = 'cascade_mask_rcnn_hrnetv2p_w32_20e.py'
checkpoint_file = 'ICDAR.19.Track.B2.Modern.table.structure.recognition.v2.pth'
model = init_detector(config_file, checkpoint_file, device='cuda')
model.half()
img = ""demo.png""

# Run Inference
result = inference_detector(model, img)

# Visualization results
show_result_pyplot(model, img, result, score_thr=0.85)
```


Hey i am using this code to reduce memory usage with fp16 but getting following error.

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-4-d6c2e135ce10> in <module>()
----> 1 get_ipython().run_cell_magic('time', '', '# Test a single image \nimg = ""demo.png""\n\n# Run Inference\nresult = inference_detector(model, img)\n\n# Visualization results\nshow_result_pyplot(model, img, result, score_thr=0.85)')

14 frames
<decorator-gen-53> in time(self, line, cell, local_ns)

<timed exec> in <module>()

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
    418                             _pair(0), self.dilation, self.groups)
    419         return F.conv2d(input, weight, self.bias, self.stride,
--> 420                         self.padding, self.dilation, self.groups)
    421 
    422     def forward(self, input: Tensor) -> Tensor:

RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same
```"
Deployment of trained model to the Raspberry Pi computer,open-mmlab/mmdetection,2021-05-17 13:56:53,3,community help wanted,5176,893360622,"Hello,

I was wondering if anyone has deployed or knows a method to deploy a trained mmdet model to the raspberry pi computer for inference on video frames from the picamera?

Talal"
DetectoRS segmentation,open-mmlab/mmdetection,2021-05-14 12:20:02,4,,5161,891876441,"Greetings! 
I am quite new to mmdetection. I wonder how do I set up the config in order to train it for the instance segmentation task? 
Currently my config looks like this, but it works just for the detection.
```
_base_ = 'detectors/detectors_cascade_rcnn_r50_1x_coco.py'
model = dict(
    roi_head=dict(bbox_head=[
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=2,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.1, 0.1, 0.2, 0.2]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=2,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.05, 0.05, 0.1, 0.1]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,
                               loss_weight=1.0)),
            dict(
                type='Shared2FCBBoxHead',
                in_channels=256,
                fc_out_channels=1024,
                roi_feat_size=7,
                num_classes=2,
                bbox_coder=dict(
                    type='DeltaXYWHBBoxCoder',
                    target_means=[0., 0., 0., 0.],
                    target_stds=[0.033, 0.033, 0.067, 0.067]),
                reg_class_agnostic=True,
                loss_cls=dict(
                    type='CrossEntropyLoss',
                    use_sigmoid=False,
                    loss_weight=1.0),
                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))
        ]))

dataset_type = 'COCODataset'
classes = ('class1','class2')
data = dict(
    train=dict(
        img_prefix='data/train_images/',
        classes=classes,
        ann_file='data/train/instances_train2017.json'),
    val=dict(
        img_prefix='data/test_images/',
        classes=classes,
        ann_file='data/val/instances_val2017.json'),
    test=dict(
        img_prefix='data/test_images/',
        classes=classes,
        ann_file='data/val/instances_val2017.json'))
```"
Can we use pretrained ResNext Backbone for a model with 4 channel Images as Input.,open-mmlab/mmdetection,2021-05-13 16:23:50,0,,5156,891182700,I have a dataset consisting of 4 -channel images and its corresponding annotations. Just wanted to ask if it would be right to use the pretrained backbone ResNext Model?
FCOS v2,open-mmlab/mmdetection,2021-05-08 02:21:23,1,community help wanted,5124,880113927,"Is there any plan for FCOS v2?
Thanks."
How can I get console log text?,open-mmlab/mmdetection,2021-05-07 03:52:09,0,,5117,878411505,"hello, I am studying with mmdetection package.
And I have one question.

how can I get log text in console?
"
Pseudo label training,open-mmlab/mmdetection,2021-05-06 15:55:49,3,,5116,877657760,"Is it possible to use this library and train with pseudo labels? 
Instead of training the model on robust labels, train the model with weak labels instead.
Thanks"
Using train pipeline on evaluation data,open-mmlab/mmdetection,2021-05-06 14:28:34,1,,5115,877559594,"I have noticed in [mmdet/api/train.py](https://github.com/open-mmlab/mmdetection/blob/931d96e5441b99227dce101ebace5d60c2853830/tools/train.py) that the 'val' pipeline is override by train pipeline when workflow consist evaluation period.
https://github.com/open-mmlab/mmdetection/blob/931d96e5441b99227dce101ebace5d60c2853830/tools/train.py#L167
Why?
e.g. when using EvalHook the pipeline is unchanged.
https://github.com/open-mmlab/mmdetection/blob/931d96e5441b99227dce101ebace5d60c2853830/mmdet/apis/train.py#L140

Thaks"
when test image size not equal to train image size,open-mmlab/mmdetection,2021-04-30 08:00:31,1,,5094,872136046,"I trained a mmdetection model with train image size of 512x512, it works fine when I test the model using image size with 512x512.
But when I test image with larger size of 2000x2000 (not scaled to 512x512 and the image scale in config file is set 2000x2000), the detection results looks weird, 1), there are many FP detection like [774, 0, 837, 0, 0.5], the bbox is a line and the score is always 0.5. 2) the pred_bbox is much larger that of 512x512 image (e.g. 50 pixels in 512x512 versus 200 pixels in 2000x2000, seems four times larger),

I wonder if the test image size should match exactly with the train image size?"
How do I configure WBF to mmdetection,open-mmlab/mmdetection,2021-04-30 07:19:27,0,reimplementation,5093,872105644,How do I configure WBF to mmdetection
"when i use the faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py,i meet problem.",open-mmlab/mmdetection,2021-04-29 13:23:24,1,,5087,871011591,"Thanks for your error report and we appreciate it a lot.
but when i use the config in dcn, like faster_rcnn_r50_fpn_dconv_c3-c5_1x_coco.py.
like these words will come true:
2021-04-29 09:01:28,611 - root - INFO - DeformConv2dPack module.backbone.layer3.3.conv2 is upgraded to version 2.
2021-04-29 09:01:28,612 - root - INFO - DeformConv2dPack module.backbone.layer3.4.conv2 is upgraded to version 2.
2021-04-29 09:01:28,613 - root - INFO - DeformConv2dPack module.backbone.layer3.5.conv2 is upgraded to version 2.
2021-04-29 09:01:28,614 - root - INFO - DeformConv2dPack module.backbone.layer4.0.conv2 is upgraded to version 2.
2021-04-29 09:01:28,615 - root - INFO - DeformConv2dPack module.backbone.layer4.1.conv2 is upgraded to version 2.
2021-04-29 09:01:28,616 - root - INFO - DeformConv2dPack module.backbone.layer4.2.conv2 is upgraded to version 2.
2021-04-29 09:01:28,934 - root - INFO - DeformConv2dPack module.backbone.layer2.0.conv2 is upgraded to version 2.
2021-04-29 09:01:28,936 - root - INFO - DeformConv2dPack module.backbone.layer2.1.conv2 is upgraded to version 2.
2021-04-29 09:01:28,937 - root - INFO - DeformConv2dPack module.backbone.layer2.2.conv2 is upgraded to version 2.
2021-04-29 09:01:28,938 - root - INFO - DeformConv2dPack module.backbone.layer2.3.conv2 is upgraded to version 2.
2021-04-29 09:01:28,939 - root - INFO - DeformConv2dPack module.backbone.layer3.0.conv2 is upgraded to version 2.
2021-04-29 09:01:28,940 - root - INFO - DeformConv2dPack module.backbone.layer3.1.conv2 is upgraded to version 2.
2021-04-29 09:01:28,941 - root - INFO - DeformConv2dPack module.backbone.layer3.2.conv2 is upgraded to version 2.
2021-04-29 09:01:28,942 - root - INFO - DeformConv2dPack module.backbone.layer3.3.conv2 is upgraded to version 2.
2021-04-29 09:01:28,942 - root - INFO - DeformConv2dPack module.backbone.layer3.4.conv2 is upgraded to version 2.
my version is: mmcv-full==1.2.1, mmdet==2.7.0. torch==1.4.0
the code can run,but i want to make these characters disappear?
hope you can give me some advice, thank you.
"
SomeThing Weird About The Yolact SegHead,open-mmlab/mmdetection,2021-04-20 02:00:44,1,community discussion,5023,862292246,"Hi, When I look the code in mmdet/models/dense_heads/yolact_head.py/

@HEADS.register_module()
class YOLACTSegmHead(nn.Module):
    def get_targets(self, segm_pred, gt_masks, gt_labels):
        """"""Compute semantic segmentation targets for each image.

        Args:
            segm_pred (Tensor): Predicted semantic segmentation map
                with shape (num_classes, H, W).
            gt_masks (Tensor): Ground truth masks for each image with
                the same shape of the input image.
            gt_labels (Tensor): Class indices corresponding to each box.

        Returns:
            Tensor: Semantic segmentation targets with shape
                (num_classes, H, W).
        """"""
        if gt_masks.size(0) == 0:
            return None
        num_classes, mask_h, mask_w = segm_pred.size()
        with torch.no_grad():
            downsampled_masks = F.interpolate(
                gt_masks.unsqueeze(0), (mask_h, mask_w),
                mode='bilinear',
                align_corners=False).squeeze(0)
            downsampled_masks = downsampled_masks.gt(0.5).float()
            segm_targets = torch.zeros_like(segm_pred, requires_grad=False)
            for obj_idx in range(downsampled_masks.size(0)):
                segm_targets[gt_labels[obj_idx] - 1] = torch.max(
                    segm_targets[gt_labels[obj_idx] - 1],
                    downsampled_masks[obj_idx])
            return segm_targets

Why using gt_labels[obj_idx]-1? When using coco datasets provided by mmdet, the category label has converted to [0,79], I think there is no need to -1 here. Can someone explain it?"
Refactor the get_seg_masks in FCNMaskHead,open-mmlab/mmdetection,2021-04-18 09:13:07,0,,4999,860613887,"**Describe the Modification**

**Motivation**
For now, the `get_seg_masks` function in `FCNMaskHead` directly returns instance masks that are organized by classes like `list[list[np.ndarray]]`, while the the most outside list has length of N (N is the number of the classes). Thus, the tensors of mask predictions are not returned but are sometimes necessary or convenient for some other post-processing, e.g., in panoptic segmentation.

**Modification**
Let `get_seg_masks` directly return the tensor of mask prediction, and explicitly return the ndarray masks organized by classes outside the function.
This refactoring should be conducted with the post-process refactoring in https://github.com/open-mmlab/mmdetection/pull/4912
"
how to use the trained model on cpu？,open-mmlab/mmdetection,2021-04-16 07:04:17,1,,4985,859527234,"how to inference a picture with mmdetection trained model on a none mmcv and none mmdetection and none gpu but with opencv， pytorch and cpu computer？

how to use simply pytorch transform to preprocess image and pytorch load model to inference a image？thanks a lot"
"why the input of single_level_roi_extractor roi extractor parameters  rois.shape=(2048,5) while bs=4, it doesn't have batch_size dimension,but feats always(b,c,w,h) has batch_size dimension",open-mmlab/mmdetection,2021-04-15 14:30:22,1,community discussion,4975,858939839,"![image](https://user-images.githubusercontent.com/37873318/114885915-a32bbb80-9df6-11eb-8a62-c4a7b8dec22c.png)
why the input of single_level_roi_extractor roi extractor parameters  rois.shape=(2048,5) while bs=4, it doesn't have batch_size dimension,but feats always(b,c,w,h) has batch_size dimension?
Please reply to me, I'm so confused.
"
Issue with custom model inference,open-mmlab/mmdetection,2021-04-15 13:11:53,3,,4974,858867070,"Hi Team,

![image](https://user-images.githubusercontent.com/5962751/114873997-2996cc00-9dfc-11eb-946e-97187d4d4ce7.png)

I'm trying to train a faster_rcnn on Clevr dataset. I have bounding box annotations for images which I transformed to coco style as proposed [here](https://mmdetection.readthedocs.io/en/latest/2_new_data_model.html). 

My config looks something like this:- 
```_base_ = [
    '../_base_/models/faster_rcnn_r50_fpn_clevr.py',
    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'
]
model = dict(
    roi_head=dict(
        bbox_head=dict(num_classes=48)))

classes = ('gray_rubber_cube', 'gray_rubber_sphere', 'gray_rubber_cylinder', 'gray_metal_cube', 'gray_metal_sphere', 'gray_metal_cylinder', 'red_rubber_cube', 'red_rubber_sphere', 'red_rubber_cylinder', 'red_metal_cube', 'red_metal_sphere', 'red_metal_cylinder', 'blue_rubber_cube', 'blue_rubber_sphere', 'blue_rubber_cylinder', 'blue_metal_cube', 'blue_metal_sphere', 'blue_metal_cylinder', 'green_rubber_cube', 'green_rubber_sphere', 'green_rubber_cylinder', 'green_metal_cube', 'green_metal_sphere', 'green_metal_cylinder', 'brown_rubber_cube',
           'brown_rubber_sphere', 'brown_rubber_cylinder', 'brown_metal_cube', 'brown_metal_sphere', 'brown_metal_cylinder', 'purple_rubber_cube', 'purple_rubber_sphere', 'purple_rubber_cylinder', 'purple_metal_cube', 'purple_metal_sphere', 'purple_metal_cylinder', 'cyan_rubber_cube', 'cyan_rubber_sphere', 'cyan_rubber_cylinder', 'cyan_metal_cube', 'cyan_metal_sphere', 'cyan_metal_cylinder', 'yellow_rubber_cube', 'yellow_rubber_sphere', 'yellow_rubber_cylinder', 'yellow_metal_cube', 'yellow_metal_sphere', 'yellow_metal_cylinder')
# use caffe img_norm
img_norm_cfg = dict(
    mean=[103.530, 116.280, 123.675], std=[1.0, 1.0, 1.0], to_rgb=False)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
dataset_type = 'CocoDataset'
data_root = 'data/object_recognition/clevr/'
data_image = 'data/object_recognition/CLEVR_v1.0/images/'
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/train_clevr_coco.json',
        img_prefix=data_image + 'train/',
        pipeline=train_pipeline,
        classes=classes),
    val=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/test_clevr_coco.json',
        img_prefix=data_image + 'val/',
        pipeline=test_pipeline,
        classes=classes),
    test=dict(
        type=dataset_type,
        ann_file=data_root + 'annotations/test_clevr_coco.json',
        img_prefix=data_image + 'val/',
        pipeline=test_pipeline,
        classes=classes))

# wandb implementation details
project = 'clevr'
name = ""Baseline faster rcnn""
entity = 'aksrajawat'
notes = 'lr 0.01 trying to pretrain on clever dataset with coco type'
group = 'faster_rcnn_r50_fpn'
# yapf:disable
log_config = dict(
    interval=1,
    hooks=[
        dict(type='WandbLoggerHook', init_kwargs=dict(
            name=name, project=project, entity=entity, notes=notes, group=group)),
        # dict(type='TensorboardLoggerHook'),
    ])

# runtime settings
checkpoint_config = dict(interval=1)
work_dir = './data/object_recognition/models/faster_rcnn_r50_fpn_0_01_LR/'


runner = dict(type='EpochBasedRunner', max_epochs=30)
evaluation = dict(interval=1, metric='bbox')
# optimizer
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
# optimizer_config = dict(_delete_=True, grad_clip=dict(max_norm=35, norm_type=2))
optimizer_config = dict(grad_clip=None)
# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[8, 15])
```

While post-training successful training, I am trying to run inference on the trained model. Image attached at the beginning of the post.

What's more worrisome is, post failed inference with my model I tried running `demo notebook` and the results were not visible in that too.
Any help would be appreciated.

Thanks in advance

"
How to get the output of each layer?,open-mmlab/mmdetection,2021-04-08 11:32:23,4,enhancement#community help wanted#v-3.x,4922,853373281,"In inference stage,I want to get the output of certain layer(for example the output of rpn). Could you tell me how to do?"
what is the 'get_cat_ids' in xml_style dataset used for?,open-mmlab/mmdetection,2021-04-02 01:16:44,0,community discussion,4882,848890286,"I don't find the 'get_cat_ids' function is used in all datasets besides coco style, isn't it?
By the way, I want to ask if the label is start from 0 instead of 1(background 0) in the custom datasets ?and why?"
why performance of mask-rcnn-r50-fpn with syncBN in mmdetection is poor than that in detectron2?,open-mmlab/mmdetection,2021-04-01 02:55:11,7,community discussion#reimplementation,4874,847779006,"Recently I have run some transferring experiment about self-supervised learning,  which using self-supervised model as pre-trained of mask-rcnn-r50. Most paper conduct these transferring experiment in detectron2, when I conduct these transferring experiment in mmdetection, I found the performance  is poor than that in detectron2.
Here is my environment: 
    python3.7,
    pytorch-1.6,
    torchvision-0.7,
    mmdetection2.10.0,
    mmcv-1.2.7,
the config file I used is here:
		# model settings
		model = dict(
			type='MaskRCNN',
			pretrained='torchvision://resnet50',
			backbone=dict(
				type='ResNet',
				depth=50,
				num_stages=4,
				out_indices=(0, 1, 2, 3),
				frozen_stages=-1,
				norm_cfg=dict(type='SyncBN', requires_grad=True),
				norm_eval=False,
				style='pytorch'),
			neck=dict(
				type='FPN',
				in_channels=[256, 512, 1024, 2048],
				out_channels=256,
				norm_cfg=dict(type='SyncBN', requires_grad=True),
				num_outs=5),
			rpn_head=dict(
				type='RPNHead',
				in_channels=256,
				feat_channels=256,
				anchor_generator=dict(
					type='AnchorGenerator',
					scales=[8],
					ratios=[0.5, 1.0, 2.0],
					strides=[4, 8, 16, 32, 64]),
				bbox_coder=dict(
					type='DeltaXYWHBBoxCoder',
					target_means=[.0, .0, .0, .0],
					target_stds=[1.0, 1.0, 1.0, 1.0]),
				loss_cls=dict(
					type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
				loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
			roi_head=dict(
				type='StandardRoIHead',
				bbox_roi_extractor=dict(
					type='SingleRoIExtractor',
					roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
					out_channels=256,
					featmap_strides=[4, 8, 16, 32]),
				bbox_head=dict(
					type='ConvFCBBoxHead',
					num_shared_convs=4,
					num_shared_fcs=1,
					in_channels=256,
					fc_out_channels=1024,
					roi_feat_size=7,
					num_classes=80,
					norm_cfg=dict(type='SyncBN', requires_grad=True),
					bbox_coder=dict(
						type='DeltaXYWHBBoxCoder',
						target_means=[0., 0., 0., 0.],
						target_stds=[0.1, 0.1, 0.2, 0.2]),
					reg_class_agnostic=False,
					loss_cls=dict(
						type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
					loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
				mask_roi_extractor=dict(
					type='SingleRoIExtractor',
					roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
					out_channels=256,
					featmap_strides=[4, 8, 16, 32]),
				mask_head=dict(
					type='FCNMaskHead',
					num_convs=4,
					in_channels=256,
					conv_out_channels=256,
					num_classes=80,
					norm_cfg=dict(type='SyncBN', requires_grad=True),
					loss_mask=dict(
						type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
			# model training and testing settings
			train_cfg=dict(
				rpn=dict(
					assigner=dict(
						type='MaxIoUAssigner',
						pos_iou_thr=0.7,
						neg_iou_thr=0.3,
						min_pos_iou=0.3,
						match_low_quality=True,
						ignore_iof_thr=-1),
					sampler=dict(
						type='RandomSampler',
						num=256,
						pos_fraction=0.5,
						neg_pos_ub=-1,
						add_gt_as_proposals=False),
					allowed_border=-1,
					pos_weight=-1,
					debug=False),
				rpn_proposal=dict(
					nms_pre=2000,
					max_per_img=1000,
					nms=dict(type='nms', iou_threshold=0.7),
					min_bbox_size=0),
				rcnn=dict(
					assigner=dict(
						type='MaxIoUAssigner',
						pos_iou_thr=0.5,
						neg_iou_thr=0.5,
						min_pos_iou=0.5,
						match_low_quality=True,
						ignore_iof_thr=-1),
					sampler=dict(
						type='RandomSampler',
						num=512,
						pos_fraction=0.25,
						neg_pos_ub=-1,
						add_gt_as_proposals=True),
					mask_size=28,
					pos_weight=-1,
					debug=False)),
			test_cfg=dict(
				rpn=dict(
					nms_pre=1000,
					max_per_img=1000,
					nms=dict(type='nms', iou_threshold=0.7),
					min_bbox_size=0),
				rcnn=dict(
					score_thr=0.05,
					nms=dict(type='nms', iou_threshold=0.5),
					max_per_img=100,
					mask_thr_binary=0.5))
		)
		dataset_type = 'CocoDataset'
		data_root = 'data/coco/'
		img_norm_cfg = dict(
			mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
		train_pipeline = [
			dict(type='LoadImageFromFile'),
			dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
			dict(type='Resize',
				 img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736), (1333, 768), (1333, 800)],
				 multiscale_mode='value',
				 keep_ratio=True),
			dict(type='RandomFlip', flip_ratio=0.5),
			dict(type='Normalize', **img_norm_cfg),
			dict(type='Pad', size_divisor=32),
			dict(type='DefaultFormatBundle'),
			dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks']),
		]
		test_pipeline = [
			dict(type='LoadImageFromFile'),
			dict(
				type='MultiScaleFlipAug',
				img_scale=(1333, 800),
				flip=False,
				transforms=[
					dict(type='Resize', keep_ratio=True),
					dict(type='RandomFlip'),
					dict(type='Normalize', **img_norm_cfg),
					dict(type='Pad', size_divisor=32),
					dict(type='ImageToTensor', keys=['img']),
					dict(type='Collect', keys=['img']),
				])
		]
		data = dict(
			samples_per_gpu=2,
			workers_per_gpu=2,
			train=dict(
				type=dataset_type,
				ann_file=data_root + 'annotations/instances_train2017.json',
				img_prefix=data_root + 'train2017/',
				pipeline=train_pipeline),
			val=dict(
				type=dataset_type,
				ann_file=data_root + 'annotations/instances_val2017.json',
				img_prefix=data_root + 'val2017/',
				pipeline=test_pipeline),
			test=dict(
				type=dataset_type,
				ann_file=data_root + 'annotations/instances_val2017.json',
				img_prefix=data_root + 'val2017/',
				pipeline=test_pipeline))
		evaluation = dict(metric=['bbox', 'segm'])
		# optimizer
		optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)
		optimizer_config = dict(grad_clip=None)
		# learning policy
		lr_config = dict(
			policy='step',
			warmup='linear',
			warmup_iters=1000,
			warmup_ratio=0.001,
			step=[8, 11])
		runner = dict(type='EpochBasedRunner', max_epochs=12)
		checkpoint_config = dict(interval=1)
		# yapf:disable
		log_config = dict(
			interval=50,
			hooks=[
				dict(type='TextLoggerHook'),
				# dict(type='TensorboardLoggerHook')
			])
		# yapf:enable
		custom_hooks = [dict(type='NumClassCheckHook')]
		dist_params = dict(backend='nccl')
		log_level = 'INFO'
		load_from = None
		resume_from = None
		workflow = [('train', 1)]

I have conduct some experiments, here is the performance:
1、using ImageNet pre-trained model without SyncBN
|    framework     |    Backbone     | box AP | mask AP | 
|    :-------------:  | :-------------  : | :----:     | :-----:     | 
|   mmdetection  |    R-50-FPN     | 38.80    | 35.02     |
|    detectron2     |    R-50-FPN     | 38.90    | 34.90     | 

2、using ImageNet pre-trained model with SyncBN
|    framework    |    Backbone     | box AP | mask AP | 
|  :-------------:   | :-------------:   | :----:     | :-----:     | 
|   mmdetection |    R-50-FPN     | 38.90    | 35.40     |
|   detectron2     |    R-50-FPN     | 39.80    | 36.02     | 

3、using SWAV self-supervised pre-trained model with SyncBN, link of SWAV  is https://dl.fbaipublicfiles.com/vissl/model_zoo/swav_in1k_rn50_800ep_swav_8node_resnet_27_07_20.a0a6b676/model_final_checkpoint_phase799.torch
|    framework     |    Backbone     | box AP | mask AP | 
| :-------------:     | :-------------:   | :----:     | :-----:     | 
|   mmdetection  |    R-50-FPN     |  40.60   | 37.00     |
|   detectron2      |    R-50-FPN     |  41.84   | 37.88     | 

4、using SWAV self-supervised pre-trained model with SyncBN with different number gpus
|    framework     |    num_gpus      |    Backbone     | box AP | mask AP | 
| :-------------:     | :-------------:     | :-------------:   | :----:     | :-----:      | 
|   mmdetection  |        8                 |    R-50-FPN     | 40.60  | 37.00       |
|   mmdetection  |        2                 |    R-50-FPN     | 37.10  | 34.50       | 


We can see that, when without syncBN, perfermance is consistent in two framework. But when with SyncBN, performance in mmdetection is always poor than detectron2. In addition, when number gpus is different, performancce seems unstable in mmdetection.
I don't know how to resolve it, can you give me some suggestiones? 



  "
Export model in Torchscript format,open-mmlab/mmdetection,2021-03-30 20:58:46,3,enhancement,4863,845256027,"It seems ONNX format is supported for faster rcnn, yolo and ssd. Is it possible to export these models to TorchScript format. "
Can I get the bbox before NMS?,open-mmlab/mmdetection,2021-03-27 13:18:41,2,enhancement,4840,842502806,"Hi,

I would like to get the bounding boxes before NMS to ensemble result. Is there any way to do it? 
Thank you"
when validate the model，the gtbox num is wrong。,open-mmlab/mmdetection,2021-03-26 16:04:47,1,community help wanted,4837,842099788,"After training with my own customized dataset, the number of gt-boxes in each category is smaller than the actual numbers. I don't know where the problem is. In addition, my dataset format is VOC.


Here are the output of the program.
[>>] 433/433, 6.6 task/s, elapsed: 66s, ETA:     0s
---------------iou_thr: 0.5---------------
2021-03-24 17:30:32,946 - mmdet - INFO - 
+-----------+------+------+--------+-------+
| class     | gts  | dets | recall | ap    |
+-----------+------+------+--------+-------+
| person    | 853  | 4520 | 0.892  | 0.750 |
| bus       | 175  | 924  | 0.749  | 0.452 |
| bicycle   | 52   | 947  | 0.827  | 0.436 |
| car       | 1820 | 4828 | 0.903  | 0.827 |
| motorbike | 101  | 965  | 0.871  | 0.621 |
+-----------+------+------+--------+-------+
| mAP       |      |      |        | 0.617 |
+-----------+------+------+--------+-------+


And this is the true gt-boxes nums
car:2579
bus:242
person:1113
motorbike:150
bicycle:68"
Add support for IterDet,open-mmlab/mmdetection,2021-03-24 15:58:54,1,community help wanted,4830,839876221,"Supporting IterDet would sound like a good idea, which works for both 1- and 2-stage detection models.

**Motivation**
It seems like a powerful post-processing/joint-training step with crowded scenes and densely packed object detection.

**Related resources**
Original article: https://arxiv.org/pdf/2005.05708.pdf
GitHub code w. existing adaptations from mmdet: https://github.com/saic-vul/iterdet

**Additional context**
What would be the best stage to introduce this?"
cudaErrorInvalidDevice: invalid device ordinal,open-mmlab/mmdetection,2021-03-24 06:30:43,3,community help wanted,4824,839401482,"2021-03-24 14:22:12,241 - mmdet - INFO - Start running, host: ai-root@HA-root, work_dir: /home/ai-root/shenl/mmdetection/work_dirs/faster_rcnn_r50_fpn_1x_coco
2021-03-24 14:22:12,241 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
Traceback (most recent call last):
  File ""tools/train.py"", line 187, in <module>
    main()
  File ""tools/train.py"", line 176, in main
    train_detector(
  File ""/home/ai-root/shenl/mmdetection/mmdet/apis/train.py"", line 170, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py"", line 125, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py"", line 50, in train
    self.run_iter(data_batch, train_mode=True)
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py"", line 29, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/mmcv/parallel/data_parallel.py"", line 67, in train_step
    return self.module.train_step(*inputs[0], **kwargs[0])
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/detectors/base.py"", line 247, in train_step
    losses = self(**data)
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py"", line 84, in new_func
    return old_func(*args, **kwargs)
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/detectors/base.py"", line 181, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/detectors/two_stage.py"", line 150, in forward_train
    rpn_losses, proposal_list = self.rpn_head.forward_train(
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/dense_heads/base_dense_head.py"", line 54, in forward_train
    losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/dense_heads/rpn_head.py"", line 72, in loss
    losses = super(RPNHead, self).loss(
  File ""/home/ai-root/anaconda3/envs/mmlab/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py"", line 164, in new_func
    return old_func(*args, **kwargs)
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/dense_heads/anchor_head.py"", line 459, in loss
    cls_reg_targets = self.get_targets(
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/dense_heads/anchor_head.py"", line 339, in get_targets
    results = multi_apply(
  File ""/home/ai-root/shenl/mmdetection/mmdet/core/utils/misc.py"", line 29, in multi_apply
    return tuple(map(list, zip(*map_results)))
  File ""/home/ai-root/shenl/mmdetection/mmdet/models/dense_heads/anchor_head.py"", line 224, in _get_targets_single
    sampling_result = self.sampler.sample(assign_result, anchors,
  File ""/home/ai-root/shenl/mmdetection/mmdet/core/bbox/samplers/base_sampler.py"", line 95, in sample
    neg_inds = self.neg_sampler._sample_neg(
  File ""/home/ai-root/shenl/mmdetection/mmdet/core/bbox/samplers/random_sampler.py"", line 78, in _sample_neg
    return self.random_choice(neg_inds, num_expected)
  File ""/home/ai-root/shenl/mmdetection/mmdet/core/bbox/samplers/random_sampler.py"", line 54, in random_choice
    perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]
RuntimeError: radix_sort: failed on 1st step: cudaErrorInvalidDevice: invalid device ordinal
"
Error happens when change the roi_head of cascade_rcnn to double_heads ,open-mmlab/mmdetection,2021-03-24 02:51:44,2,reimplementation,4821,839292400,"**Describe the issue**

I want to change the roi_head from cascadeRoIHead to double_heads.  However it occured error""TypeError: cfg must be a dict, but got <class 'NoneType'>"".

**Reproduction**

1. What command or script did you run?

`CUDA_VISIBLE_DEVICES=8,9 bash tools/dist_train.sh configs/rsj_nightowls/rsj_cascade_rcnn_double_heads.py 2`


2. What config dir you run?

cascade rcnn

2021-03-24 10:40:17,858 - mmdet - INFO - Config:
```
model = dict(
    type='CascadeRCNN',
    pretrained='torchvision://resnet50',
    backbone=dict(
        type='ResNet',
        depth=50,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        frozen_stages=1,
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=True,
        style='pytorch'),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=5),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(
            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),
    roi_head=dict(
        type='DoubleHeadRoIHead',
        reg_roi_scale_factor=1.3,
        bbox_head=dict(
            _delete_=True,
            type='DoubleConvFCBBoxHead',
            num_convs=4,
            num_fcs=2,
            in_channels=256,
            conv_out_channels=1024,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=3,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=2.0),
            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=2.0))))
train_cfg = dict(
    rpn=dict(
        assigner=dict(
            type='MaxIoUAssigner',
            pos_iou_thr=0.7,
            neg_iou_thr=0.3,
            min_pos_iou=0.3,
            match_low_quality=True,
            ignore_iof_thr=-1),
        sampler=dict(
            type='RandomSampler',
            num=256,
            pos_fraction=0.5,
            neg_pos_ub=-1,
            add_gt_as_proposals=False),
        allowed_border=0,
        pos_weight=-1,
        debug=False),
    rpn_proposal=dict(
        nms_across_levels=False,
        nms_pre=2000,
        nms_post=2000,
        max_num=2000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=[
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.6,
                neg_iou_thr=0.6,
                min_pos_iou=0.6,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False),
        dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.7,
                min_pos_iou=0.7,
                match_low_quality=False,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            pos_weight=-1,
            debug=False)
    ])
test_cfg = dict(
    rpn=dict(
        nms_across_levels=False,
        nms_pre=1000,
        nms_post=1000,
        max_num=1000,
        nms_thr=0.7,
        min_bbox_size=0),
    rcnn=dict(
        score_thr=0.05,
        nms=dict(type='nms', iou_threshold=0.5),
        max_per_img=100))
dataset_type = 'NightOwlsDataset'
data_root = '/data/sj_data/data/Nightowls/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(type='Resize', img_scale=(1024, 640), keep_ratio=True),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size_divisor=32),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 640),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=2,
    train=dict(
        type='NightOwlsDataset',
        ann_file='/data/sj_data/data/Nightowls/nightowls_training.json',
        img_prefix='/data/sj_data/data/Nightowls/nightowls_training/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(type='Resize', img_scale=(1024, 640), keep_ratio=True),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size_divisor=32),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='NightOwlsDataset',
        ann_file='/data/sj_data/data/Nightowls/nightowls_validation.json',
        img_prefix='/data/sj_data/data/Nightowls/nightowls_validation/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 640),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='NightOwlsDataset',
        ann_file='/data/sj_data/data/Nightowls/nightowls_validation.json',
        img_prefix='/data/sj_data/data/Nightowls/nightowls_validation/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 640),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size_divisor=32),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric='bbox')
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.001,
    step=[2, 5])
total_epochs = 10
checkpoint_config = dict(interval=1)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
work_dir = '/data/sj_data/work_dir/cascade_rcnn_resnet50_double_head_2x2_0.01_nightowls'
gpu_ids = range(0, 2)
```


3. Did you make any modifications on the code or config? Did you understand what you have modified?
I only change the lr_sche. But it doesn't matter.

4. What dataset did you use?

NightOwls Dataset

**Environment**

sys.platform: linux
Python: 3.7.10 (default, Feb 26 2021, 18:47:35) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7,8,9: GeForce RTX 2080 Ti
CUDA_HOME: /usr/local/cuda-10.0
NVCC: Cuda compilation tools, release 10.0, V10.0.130
GCC: gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0
PyTorch: 1.6.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.5.0 (Git Hash e2ac1fac44c5078ca927cb9b90e1b3066a0b2ed0)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.7.0
OpenCV: 4.4.0
MMCV: 1.2.4
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.10.0+79c8dd5


**Results**
**The error is below:**
Traceback (most recent call last):
  File ""tools/train.py"", line 187, in <module>
    main()
  File ""tools/train.py"", line 161, in main
    test_cfg=cfg.get('test_cfg'))
  File ""/home/sj_ren/mmdetection/mmdet/models/builder.py"", line 77, in build_detector
    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))
  File ""/home/sj_ren/mmdetection/mmdet/models/builder.py"", line 34, in build
    return build_from_cfg(cfg, registry, default_args)
  File ""/opt/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 171, in build_from_cfg
    return obj_cls(**args)
  File ""/home/sj_ren/mmdetection/mmdet/models/detectors/cascade_rcnn.py"", line 25, in __init__
    pretrained=pretrained)
  File ""/home/sj_ren/mmdetection/mmdet/models/detectors/two_stage.py"", line 43, in __init__
    self.roi_head = build_head(roi_head)
  File ""/home/sj_ren/mmdetection/mmdet/models/builder.py"", line 59, in build_head
    return build(cfg, HEADS)
  File ""/home/sj_ren/mmdetection/mmdet/models/builder.py"", line 34, in build
    return build_from_cfg(cfg, registry, default_args)
  File ""/opt/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 171, in build_from_cfg
    return obj_cls(**args)
  File ""/home/sj_ren/mmdetection/mmdet/models/roi_heads/double_roi_head.py"", line 13, in __init__
    super(DoubleHeadRoIHead, self).__init__(**kwargs)
  File ""/home/sj_ren/mmdetection/mmdet/models/roi_heads/base_roi_head.py"", line 26, in __init__
    self.init_bbox_head(bbox_roi_extractor, bbox_head)
  File ""/home/sj_ren/mmdetection/mmdet/models/roi_heads/standard_roi_head.py"", line 24, in init_bbox_head
    self.bbox_roi_extractor = build_roi_extractor(bbox_roi_extractor)
  File ""/home/sj_ren/mmdetection/mmdet/models/builder.py"", line 49, in build_roi_extractor
    return build(cfg, ROI_EXTRACTORS)
  File ""/home/sj_ren/mmdetection/mmdet/models/builder.py"", line 34, in build
    return build_from_cfg(cfg, registry, default_args)
  File ""/opt/anaconda3/envs/open-mmlab/lib/python3.7/site-packages/mmcv/utils/registry.py"", line 140, in build_from_cfg
    raise TypeError(f'cfg must be a dict, but got {type(cfg)}')
TypeError: cfg must be a dict, but got <class 'NoneType'>


**Issue fix**

If you have already identified the reason, you can provide the information here. If you are willing to create a PR to fix it, please also leave a comment here and that would be much appreciated!
"
a bug in distributed training of grid r-cnn,open-mmlab/mmdetection,2021-03-12 12:53:06,4,bug,4760,830090508,"grid r-cnn单卡训练没有问题，但是只要多卡或者分布式训练（use dist_train.sh）就会卡在第一次迭代的地方，不报错，也不打印损失，陷入了死循环，把程序杀掉后，显卡仍被占用内存。。。然后其他的r-cnn config使用分布式训练也没有问题。
请问这是什么问题？
"
"lugin: Could not load the Qt platform plugin ""xcb""",open-mmlab/mmdetection,2021-03-11 07:28:49,3,community help wanted,4751,828867470,"<img width=""572"" alt=""屏幕快照 2021-03-11 下午3 26 32"" src=""https://user-images.githubusercontent.com/47246206/110750969-1b9bcd00-81f8-11eb-89af-afef613b04e1.png"">
this is old questions, but the comments couldn't help me
so how could I do? I'm looking forward your reply!"
Add support for SAM (Sharpness Awareness Minimization) based optimization,open-mmlab/mmdetection,2021-03-10 16:05:27,2,enhancement#community help wanted,4747,827968944,"**Add support for [SAM based optimization](https://github.com/davda54/sam)**

**Motivation**
In Deep Learning we use optimization algorithms such as SGD/Adam to achieve convergence in our model, which leads to finding the global minima, i.e a point where the loss of the training dataset is low. But several kinds of research such as [Zhang et al](https://arxiv.org/abs/1611.03530) have shown, many networks can easily memorize the training data and have the capacity to readily overfit, To prevent this problem and add more generalization, Researchers at Google have published a new paper called [Sharpness Awareness Minimization](https://arxiv.org/abs/2010.01412) which provides State of the Art results on CIFAR10 and other datasets.

**Related resources**
Pytorch based implementation: https://github.com/davda54/sam
"
Customized data augmentation pipeline based on albumentations,open-mmlab/mmdetection,2021-02-21 16:54:19,3,community help wanted,4651,812898623,"Hello, 

I am trying to incorporate the copy and paste effect with mmdetection based on https://github.com/conradry/copy-paste-aug, which is implemented as a subclass of albumentation's DualTransform class. I basically copied copy_paste.py under mmdet/datasets/pipelines and followed the sample albu config to set up my config file, this is how my config file looks like:
(only the data transform part)
`from mmdet.datasets.pipelines import CopyPaste`
.....
.....
```
albu_train_transforms = [
    dict(
        type='ShiftScaleRotate',
        shift_limit=0.0625,
        scale_limit=0.0,
        rotate_limit=0,
        interpolation=1,
        p=0.5),
    dict(
        type='RandomBrightnessContrast',
        brightness_limit=[0.1, 0.3],
        contrast_limit=[0.1, 0.3],
        p=0.2),
    dict(
        type='OneOf',
        transforms=[
            dict(
                type='RGBShift',
                r_shift_limit=10,
                g_shift_limit=10,
                b_shift_limit=10,
                p=1.0),
            dict(
                type='HueSaturationValue',
                hue_shift_limit=20,
                sat_shift_limit=30,
                val_shift_limit=20,
                p=1.0)
        ],
        p=0.1),
    dict(type='JpegCompression', quality_lower=85, quality_upper=95, p=0.2),
    dict(type='ChannelShuffle', p=0.1),
    dict(
        type='OneOf',
        transforms=[
            dict(type='Blur', blur_limit=3, p=1.0),
            dict(type='MedianBlur', blur_limit=3, p=1.0)
        ],
        p=0.1),
    dict(
        type=CopyPaste,
        blend=True, 
        sigma=1, 
        pct_objects_paste=0.5, 
        p=1
    ),
]
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),
    dict(type='Pad', size_divisor=32),
    dict(
        type='Albu',
        transforms=albu_train_transforms,
        bbox_params=dict(
            type='BboxParams',
            format='coco',
            label_fields=['gt_labels'],
            min_visibility=0.0,
            filter_lost_elements=True),
        keymap={
            'img': 'image',
            'gt_masks': 'masks',
            'gt_bboxes': 'bboxes',
        },
        update_pad_shape=False,
        skip_img_without_anno=True),
    dict(type='Normalize', **img_norm_cfg),
    dict(type='DefaultFormatBundle'),
    dict(
        type='Collect',
        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'],
        meta_keys=('filename', 'ori_shape', 'img_shape', 'img_norm_cfg',
                   'pad_shape', 'scale_factor'))
]
```

where CopyPaste is the customized data augmentation. At first I set type='CopyPaste', it gave me error as 'CopyPaste' is not a built-in attribute of albumentations. Then I set type=CopyPaste so that it's a class type. Since given in the source code (albu_builder function in transforms.py):
```
def albu_builder(self, cfg):
        """"""Import a module from albumentations.
        It inherits some of :func:`build_from_cfg` logic.
        Args:
            cfg (dict): Config dict. It should at least contain the key ""type"".
        Returns:
            obj: The constructed object.
        """"""

        assert isinstance(cfg, dict) and 'type' in cfg
        args = cfg.copy()

        obj_type = args.pop('type')
        if mmcv.is_str(obj_type):
            if albumentations is None:
                raise RuntimeError('albumentations is not installed')
            obj_cls = getattr(albumentations, obj_type)
        elif inspect.isclass(obj_type):
            obj_cls = obj_type
        else:
            raise TypeError(
                f'type must be a str or valid type, but got {type(obj_type)}')

        if 'transforms' in args:
            args['transforms'] = [
                self.albu_builder(transform)
                for transform in args['transforms']
            ]

        return obj_cls(**args)

```
if `inspect.isclass(CopyPaste) `returns `True`, it should add the customized augmentation effect to the pipeline. But , but I always get a parsing error. Can you help me with this? Really appreciated!"
Training with Pascal VOC and using mAP does not include mAP on wandb,open-mmlab/mmdetection,2021-02-04 18:00:27,7,community help wanted,4589,801513944,"I currently have this on my wandb
command issed: `python tools/train.py config/custom/custom.py`

Validation part in wandb does not include mAP
![image](https://user-images.githubusercontent.com/8830319/106935056-dba77d00-6755-11eb-9c7e-36570df6c857.png)

Is there a way to include mAP here?"
Images normalization using too much cpu during inference,open-mmlab/mmdetection,2021-01-20 06:25:57,1,enhancement,4488,789678802,"I am working on model optimization on mmdetection. We found that normalization of pictures using too much cpu during inference.  We have rewritten this module and move it to gpu. Then CPU utilization has dropped significantly by about 4 times(Inf time: 20fps).
```
torch.from_numpy(img_rescale.astype(np.float32)).to(self.device).sub_(self.mean).mul_(self.stdinv)
```
However, the whole images preprocessing has to be reconstructed. Is there any plan to add an official support?"
CocoDataset uses annotation 'area' property to decide whether bbox should be used,open-mmlab/mmdetection,2021-01-12 13:54:41,4,enhancement,4436,784254082,"Thanks for your error report and we appreciate it a lot.

**Describe the bug**

Line 146 and 147 of `mmdet/datasets/coco.py` https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/coco.py#L146 is:

```
if ann['area'] <= 0 or w < 1 or h < 1:
    continue
```

If the area property is 0, or does not exist, then this code will skip over the adding of bbox information to gt_bboxes, and the bbox here will not be used. Area is calculated from segmentation information according to MS COCO format guidelines, and has no relation to bounding boxes, and so this code should not be using area to filter for valid bboxes.

If the annotation is bounding boxes only, this code stops that bounding box from being used.

**Bug fix**
I will create a PR for this if you agree that this is a bug and should not behave this way. My proposed solution is to move the first condition of `ann['area'] <= 0` to further down in the code to where the mask information is added to gt_masks_ann."
Add option so `simple_test` simply returns the output of `self.bbox_head.get_bboxes`. ,open-mmlab/mmdetection,2020-12-01 20:13:10,1,enhancement,4215,754692309,"**Describe the feature**

Currently with single stage models simple test is written as such: 

```python
    def simple_test(self, img, img_metas, rescale=False):
        x = self.extract_feat(img)
        outs = self.bbox_head(x)
        bbox_list = self.bbox_head.get_bboxes(
            *outs, img_metas, rescale=rescale)
        # skip post-processing when exporting to ONNX
        if torch.onnx.is_in_onnx_export():
            return bbox_list

        bbox_results = [
            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)
            for det_bboxes, det_labels in bbox_list
        ]
        return bbox_results[0]
```

I would like to be able to have access the simple `return bbox_list` when I'm not in a ONNX export. Perhaps there can be some option in `test_cfg` perhaps called `""enable_postprocessing""` or something along those lines. 

A similar change could be made to `StandardRoIHead.simple_test` and `FCNMaskHead.get_seg_masks`, etc...

**Motivation**
It is inconvenient when my pytorch results are inconsistent with my ONNX results. That is the main reason I'm writing this issue.

It is inconvenient when my results are automatically converted to numpy, when I would rather have them remain as torch tensors.

"
Problems with cityscapes evaluation,open-mmlab/mmdetection,2020-10-31 08:08:31,8,community help wanted,4037,733664558,"Hello
I have a problem with cityscapes evaluation
I have downloaded cityscapes dataset and prepare it with this:

pip install cityscapesscripts
python tools/convert_datasets/cityscapes.py ./data/cityscapes --nproc 8 --out_dir ./data/cityscapes/annotations

And pretrained model of faster-rcnn from[ here](https://github.com/open-mmlab/mmdetection/tree/master/configs/cityscapes)
Then I changed test config in cityscapes_detection.py to val config, like this:

    test=dict(
        type=dataset_type,
        ann_file=data_root +
        'annotations/instancesonly_filtered_gtFine_val.json',
        img_prefix=data_root + 'leftImg8bit/val/',
        pipeline=test_pipeline))

And when i trying to evaluate with this command:
python tools/test.py configs/cityscapes/faster_rcnn_r50_fpn_1x_cityscapes.py checkpoints/faster_rcnn_r50_fpn_1x_cityscapes_20200502-829424c0.pth  --eval bbox
I get this results:

 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.653
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.172
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.409
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.614
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.462
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.462
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.209
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.465
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.692
**the mAP_75 equals -1.**
What should i do to fix it?"
Rethinking positional arguments in the API,open-mmlab/mmdetection,2020-10-15 14:24:30,3,enhancement,3939,722385396,"This is less of a feature request, and more along the lines of: observations of issues I've had when working with the mmdet API and suggestions for how some of these might be refactored to improve the overall package. 

One of the biggest challenges of with working with mmdet so far has been its widespread use of positional arguments. This comes in two flavors: function signatures and return values.


### The current structure

As an example consider `forward_train` function in `base_dense_head.py` and its use of the `get_bboxes` function:


The signature for `get_boxes` looks like:
```python
def get_bboxes(self, cls_scores, bbox_preds, img_metas, cfg=None, rescale=False):
    ...
```

And the head forward function looks somewhat like this:

```python
    def forward(self, x):
        # do stuff to translate backbone features into box+scores
        return cls_scores, bbox_preds
```



The `forward_train` function currently looks something like this:

```python
    def forward_train(self,
                      x,
                      img_metas,
                      gt_bboxes,
                      gt_labels=None,
                      gt_bboxes_ignore=None,
                      proposal_cfg=None,
                      **kwargs):
        outs = self(x)
        if gt_labels is None:
            loss_inputs = outs + (gt_bboxes, img_metas)
        else:
            loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)
        losses = self.loss(*loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)
        if proposal_cfg is None:
            return losses
        else:
            proposal_list = self.get_bboxes(*outs, img_metas, cfg=proposal_cfg)
            return losses, proposal_list
```


Imagine if you want to extend `self.forward` to return anything other than a
tuple `Tuple[cls_scores, bbox_preds]`. You have to create a custom `get_boxes`
function that has arguments in an order that agree with the disparate forward
function. Perhaps for some people thinking in terms of ordered items is easy,
but for me, this is incredibly hard to manage. I would like to suggest an
alternative.



### The alternative proposal

Imagine if instead of returning a tuple the `forward` function returned a
dictionary where the keys were standardized instead of the positions of the
values.

```python
    def forward(self, x):
        # do stuff to translate backbone features into box+scores
        outs = {
            'cls_scores': cls_scores,
            'bbox_preds': bbox_preds,
        }
        return outs
```

Now, the `get_bboxes` function doesn't need to care about what particular head
was used. It can simply accept the `output` dictionary and assert that it
contains particular keys that that variant of `get_bboxes` needs. (Note this
might allow the head to produce other auxiliary information used in the loss,
but not in the construction of boxes)

```python
def get_bboxes(self, output, img_metas, cfg=None, rescale=False):
    # This get_bboxes function requires cls_scores and bbox_pred, but 
    # its ok if your network produces other things as well
    cls_scores = output['cls_scores']
    bbox_preds = output['bbox_preds']
    # output may have an item 'keypoint_preds' but we aren't forced to care
    # about it if we don't specifically need it.
    ...
```

We can extend this pattern further, so in addition to the `forward` function
producing a dictionary, the `forward_train` function will produce a dictionary
as well.

```python
    def forward_train(self,
                      x,
                      img_metas,
                      gt_bboxes,
                      gt_labels=None,
                      gt_bboxes_ignore=None,
                      proposal_cfg=None,
                      **kwargs):
        outs = self(x)
        losses = self.loss(outs, gt_bboxes=gt_bboxes, gt_labels=gt_labels,
                           img_metas=img_metas, gt_bboxes_ignore=gt_bboxes_ignore)
        train_outputs = {
            'losses': losses,
        }
        if proposal_cfg is not None:
            proposal_list = self.get_bboxes(*outs, img_metas, cfg=proposal_cfg)
            train_outputs['proposal_list'] = proposal_list

        return train_outputs
```

This has less conditionals and a consistent return type. 

This means that function that use forward train can seamlessly switch between
setting `proposal_cfg` and getting the boxes or just getting the loss because
the return value have consistent types and access patterns in both modes. If you 
do need a conditional it can be based on the return value instead of having to
remember the inputs. 

We could go even further and abstract the labels into a argument called `truth`
that should contain keys: `gt_bboxes`, and optionally `gt_labels` and
`gt_bboxes_ignore`, and perhaps that might look like:


```python
    def forward_train(self, x, img_metas, truth, proposal_cfg=None, **kwargs):
        outs = self.forward(x)
        losses = self.loss(outs, img_metas=img_metas, truth=truth)
        train_outputs = {
            'losses': losses,
        }
        if proposal_cfg is not None:
            proposal_list = self.get_bboxes(*outs, img_metas, cfg=proposal_cfg)
            train_outputs['proposal_list'] = proposal_list
        return train_outputs
```

### Discussion

IMO this pattern produces much more readable and extensible code. We can:

* Return arbitrary outputs from our forward function

* Add arbitrary target information to the truth dictionary and conditionally
  handle it in our custom loss. 

* Use simpler calling patterns that explicitly extract information from
  returned containers based on standard (easy for humans to remember) string
  keywords rather than standard (hard for humans to remember) integer positions. 

* Use semantically meaningful labels to allow for easier introspection of our
  code at runtime.

I think having a standard set of keywords is much more extensible
than sticking to positional based arguments.

There is a small issue of speed. Unpacking dictionaries is slower than
unpacking tuples, but I don't think it will be noticeable difference given that
every python attribute lookup is a dictionary lookup anyway. 

This is a rather large API change, but I think the reliance of positional based
arguments is stifling further development of new and exciting networks. I think
there might be a way to gradually introduce these changes such that it
maintains a decent level of backwards compatibility as well, but I'm not 100%
sure on this.

I've played around with variants of this and it works relatively well, the main
issue I had was the widespread use of `multi_apply`, which could likely be
changed to assume the containers returned by forward functions are dictionaries
instead of tuples. 


### Summary

In summary I want to propose replacing positional based APIs with keyword based
APIs. The main purpose of making this issue is for me to gauge the interest of
the core devs. If there is interest I may work on developing this idea further
and look into implementations that are amenable to a smooth transition such
that backwards compatibility is not broken.
"
How to implement two branches for faster-rcnn that shares the same weights?,open-mmlab/mmdetection,2020-10-13 14:08:14,0,community help wanted,3920,720308112,"I want to change the structures of faster-rcnn, for example add extra branch that shares the same weights with original branch in the encoding stage. 
In mmdet/apis/train.py, I build another data_loader2, then change the  last line ""runner.run(data_loaders, data_loader2, cfg.workflow, cfg.tital_epochs)"". In mmdet/detectors/two_stage.py, then change the line ""def forward_train(self, img, img_metas, img2, img_meta2, ......,)"". However, I get this error "" forward() missing 2 required positional arguments: 'img2' and 'img_meta2' """
How do I deal with situations where there is no object in the detected results?,open-mmlab/mmdetection,2020-09-08 03:24:27,2,question#Doc,3710,695514746,"https://github.com/open-mmlab/mmdetection/blob/f240bf9d4999854c4eb5cccbfae5053443db4a84/mmdet/core/evaluation/mean_ap.py#L308

If there is no target in the detected result, an error will be reported here.

---

Can you give me some advice? Note that there is no object in the ground truth."
"len() of a numpy.ndarray causes ""len() of unsized object"" error.",open-mmlab/mmdetection,2020-08-22 06:35:22,3,enhancement,3602,683941985,"The `process_polygons()` function <https://github.com/open-mmlab/mmdetection/blob/9596b9a4c916ae601f9a8a641c3a0ea47265abec/mmdet/datasets/pipelines/loading.py#L268-L271>
uses the buitin function `len()` to a `numpy.ndarray` object, leading to a `
TypeError: len() of unsized object` error.



This issue is related to #3599 where I tried to output polygon-style annotations to the `FCNMaskHead`.
"
"At the beginning of training, the GPU memory occupied was very large.",open-mmlab/mmdetection,2020-08-21 09:03:54,2,community discussion,3596,683413144,"At the beginning of training, the GPU memory occupied was very large, 10G, and it became 7G when the training was stable, which made me unable to set a larger batch. Is there any solution?   My GPU's memory is 11G."
Add recent backbones [Xception/AOGNet?],open-mmlab/mmdetection,2020-07-09 23:56:43,9,community help wanted,3268,654417444,"**Describe the feature**
Some new recent backbones seem to provide good performance but are not as widely used as the implemented ones.

It would be nice to have also some other backbones such as Xception and possibly AOGNet (not sure about the later with the licence)

I wasn't sure if it was worth splitting and making more ticket or keeping them together as AOGNet might be a far shoot due to the current licencing.

**Motivation**
These newer backbones are quite recent and adoption in open modular toolbox would encourage experimentation.

**Related resources**
[Xception]
[Unofficial simple implementation]: https://github.com/hoya012/pytorch-Xception
[Unofficial implementation]: https://github.com/jfzhang95/pytorch-deeplab-xception/blob/master/modeling/backbone/xception.py

[Official AOGNet]: https://github.com/iVMCL/AOGNet-v2


"
"After an update, acc is 100% and loss is 0 on first epoch when training on COCO format db",open-mmlab/mmdetection,2020-07-09 09:37:14,11,community help wanted,3259,653917363,"Hello,
This is a very strange thing, I did not change the DB just update mmdet and mmcv. It does not matter which config I use, it is always the same result. 

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 381M/381M [00:18<00:00, 22.0MB/s]
2020-07-09 09:32:00,637 - mmdet - INFO - Start running, host: root@bda79040bf7f, work_dir: /root/mmdetection/work_dirs/faster_rcnn_x101_64x4d_fpn_1x_coco
2020-07-09 09:32:00,637 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs
2020-07-09 09:33:08,193 - mmdet - INFO - Epoch [1][50/380]      lr: 1.978e-03, eta: 1:41:32, time: 1.351, data_time: 0.173, memory: 6574, loss_rpn_cls: 0.0017, loss_rpn_bbox: 0.0000, loss_cls: 0.0019, acc: 99.9717, loss_bbox: 0.0000, loss: 0.0036

Also at the end of the first epoch I get this:

```
2020-07-09 09:28:15,996 - mmdet - INFO - Evaluating bbox...
Loading and preparing results...
2020-07-09 09:28:15,996 - mmdet - ERROR - The testing results of the whole dataset is empty.

```

Thanks, 
"
DCN on Jetson TX2,open-mmlab/mmdetection,2020-06-16 09:20:15,4,community help wanted,3041,639509885,"Hi

I am trying to use the deformable convolutions from this repo on a Jetson TX2. Compilation was successful and I can also run them from Python. However, for every call of the DCN I get the following error:
`error in deformable_im2col: too many resources requested for launch`

I was wondering if there are any setting in the `.cu` files that I can change to fix this error?

**Minimal reproducible example**
``` Python
# Execute from parent directory of ops folder

import torch
from ops.dcn import DeformConvPack

device = torch.device('cuda')
dcn = DeformConvPack(in_channels=256,
                     out_channels=256,
                     kernel_size=(3, 3),
                     padding=1).to(device)
input = torch.Tensor(16, 256, 26, 20).to(device)
output = dcn(input)
```

**Environment**

- Jetson TX2 with JetPack 4.3
- Python 3.6.9
- Pytorch 1.4
- Torchvision 0.5

Since I only wanted to install DCNs instead of the whole repo, I used a reduced `setup.py` (copied from this repo):
```Python
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='mmdet',
    ext_modules=[
        CUDAExtension('deform_conv_cuda', [
            'src/deform_conv_cuda.cpp',
            'src/deform_conv_cuda_kernel.cu'
        ]),
        CUDAExtension('deform_pool_cuda', [
            'src/deform_pool_cuda.cpp',
            'src/deform_pool_cuda_kernel.cu'
        ])
    ],
    cmdclass={
        'build_ext': BuildExtension
    })
```


**Bug fix**
After a quick search on Google I found [this](https://github.com/pytorch/pytorch/issues/7680) PyTorch issue which seems related. Unfortunately I have no experience with CUDA at all, so I am not sure if this helps."
Roadmap of MMDetection,open-mmlab/mmdetection,2020-06-07 17:07:13,86,community help wanted#good first issue#community discussion,2931,633583273,"We keep this issue open to collect feature requests from users and hear your voice. Our monthly release plan will be updated in different issues.

In this issue, you can either:
1. Suggest a new feature by leaving a comment.
2. Vote for a feature request with :+1: or be against with :-1:. (Remember that developers are busy and cannot respond to all feature requests, so vote for your most favorable one!)
3. Tell us that you would like to help implement one of the features in the list or review the PRs. (This is the greatest things to hear about!)

We also released our TODO list on the [project page](https://github.com/open-mmlab/mmdetection/projects). Most of the TODO items are described in their corresponding issues (those labeled by Dev-RD) with detailed requirement documentation. Feel free to leave a message in the issue of any item and create a PR if you are interested in any of the item.
"
Any plans for adding support for AmoebaNet?,open-mmlab/mmdetection,2020-06-05 09:03:19,1,community help wanted,2910,631434595,"**Describe the feature**
Supporting AmoebaNet as one of the backbones for object detection.

**Motivation**
Models like NAS-FPN benefit a lot from AmoebaNet-based backbones, as shown in paper-with-code leaderboards: https://paperswithcode.com/paper/learning-data-augmentation-strategies-for#code
 
**Related resources**
https://cloud.google.com/tpu/docs/tutorials/amoebanet

**Additional context**
According to the official paoper, the # of parameters regarding AmoebaNet-A is close to a ResNeXt-101, so it is feasible for normal research.
https://arxiv.org/pdf/1802.01548.pdf
"
Imbalanced and small quantity dataset.Which model can I try?,open-mmlab/mmdetection,2020-06-02 01:05:54,3,community discussion,2873,628825153,"My custom dataset has 90 categories and 1300 images in which half number is nature images,the rest specimen.The dataset is imbalanced among classes. I havn't augmented each image 10 times to create 13000 images. I split train and val set with ratio 8:2. I had a try to employ retinanet with resnet(18,50,101)and ssd300.But overfitting appears so that train set performs well but validation set is bad . How can I do my experiment? Which model can I try? Please help me if you have solved similar situation ,Thanks!"
EfficientDet implementation,open-mmlab/mmdetection,2019-12-18 02:07:05,7,,1827,539408676,"Do author plan to implement EfficientDet ""EfficientDet: Scalable and Efficient Object Detection"" into mmdet framework
That would be great!
Thanks"
make mmdetection compatible with jetson tx2 and xavier.,open-mmlab/mmdetection,2019-10-20 20:41:27,12,enhancement#community help wanted,1563,509649682,"**Describe the feature**
Make mmdetection compatible with jetson tx2 and xavier. 

**Motivation**
Nvidia Jetson is the most used SBC for image processing in the world.

**Related resources**
https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/

Regards,
Felipe D.
"
Adding TTFNet,open-mmlab/mmdetection,2019-09-25 18:12:53,5,feature request,1450,498443834,"**Describe the feature**
It would be great to add [TTFNet](https://arxiv.org/abs/1909.00700) to the available models.

**Motivation**
TTFNet is the fastest detector at training time and one of the fastest at test time. Especially having a model that trains very fast would be a great addition I think.

**Related resources**
The [TTFNet implementation](https://github.com/ZJULearning/ttfnet) is based on mmdetection so it would hopefully not be too hard for the authors to integrated it into the original mmdetection codebase.

**Additional context**
Tagging the authors here: @liuzili97 @Turoad
If you would like to implement the feature and create a PR, please leave a comment here and that would be much appreciated.
"
CPU and GPU memory consumption during inference,open-mmlab/mmdetection,2019-08-12 07:48:25,7,community help wanted,1170,479508954,"# DOUBT
While running the demo notebook for MaskRCNN model. I want to know why the script uses : 
1. Only 1.4 GB GPU memory ?
2. **2.95 GB** CPU memory ?
3. The notebook loads more than **20 CPU threads** ?
4. Is there a way to control the total CPU threads and memory? 

# STEPS TO REPRODUCE 
config_file = '../configs/mask_rcnn_r50_fpn_1x.py'
checkpoint_file = '../checkpoints/mask_rcnn_r50_fpn_2x_20181010-41d35c05.pth'
model = init_detector(config_file, checkpoint_file, device='cuda:0')
result = inference_detector(model, img)"
Keyboard Interruption checkpoint saving?,open-mmlab/mmdetection,2018-11-03 17:06:03,3,enhancement,95,377073521,"Hi.

Do you plan to implement checkpoint saving and then exiting on keyboard interruption officially? Now the program just ignores keyboard interruptions. Great appreciate on that if so."
Secret Loss is not being optimized,tancik/StegaStamp,2022-08-29 14:30:08,0,,53,1354403407,"Hi Tancik, 

I have utilized your repository for training multiple models with various hyper-parameter changes, however, one thing that is common in all of my trainings is that the secret loss (cross entropy) isnt being optimized in all of them or there is a minute optimization resulting in a really low str_acc. 

Do you know what could be the possible issue for that? "
How to train the model on 256x256?,tancik/StegaStamp,2022-05-29 03:10:25,0,,52,1251774760,I'm trying to train this model on 256x256 but it always fails to decode... Did anyone try this image size and could you share the training parameters? Thank you!
what does  the Cos dropoff means?,tancik/StegaStamp,2022-03-30 11:41:13,0,,51,1186332447,`falloff_speed = 4 # Cos dropoff that reaches 0 at distance 1/x into image`  i am confused with the meanning or the function of this code. Could you offer me some explanation? thx.
How could I transform my picture like what displayed in the paper?,tancik/StegaStamp,2022-03-28 13:24:46,0,,50,1183439481,"![图片](https://user-images.githubusercontent.com/66240251/160402796-7ebb50e6-6862-410a-887b-fc3b7e9e6169.png)
"
Is the loss of the discriminator negative?,tancik/StegaStamp,2022-03-18 02:56:56,0,,49,1173130942,"Hello, I have been studying your paper recently. I don't quite understand the role of discriminator, because I find that G_loss is used in total loss, so what is the role of D_loss? And is the loss of discriminator negative? Thank you very much!"
How to fine tune the model?,tancik/StegaStamp,2022-03-15 08:56:24,0,,48,1169368912,How to fine tune the model through the trained CKPT model
according to bash scripts/base.sh EXP_NAME，Model test after training,tancik/StegaStamp,2022-03-14 08:46:13,4,,47,1168060731,"After training the saved PB model, when a picture is encrypted, printed and photographed, it cannot be decrypted"
How to convert encoded_image from tensor to numpy ？,tancik/StegaStamp,2022-03-01 11:56:53,2,,45,1155248984,"The encoded_image in the network has always been operated in the form of a tensor. When I want to make some changes to the noise layer, I find that I cannot directly assign a value to the tensor, so I want to convert the encoded_image to the form of an array first. I use sess.run and .eval() both failed. Is there any solution? Thank you very much! ! ! ! ! ! "
Which parameter or loss is the most important for the decodability of the printed image?,tancik/StegaStamp,2022-02-19 11:45:20,0,,44,1144672251,"Hello, @tancik
Did you find out which parameter affects decodability of the printed image the most when doing experiments?
I tried to train a new model with some image which is not natural, but the some of the printed images can't be decoded."
The encoded image can only be 400*400？,tancik/StegaStamp,2022-01-12 02:13:20,4,,43,1099799179,"ValueError: Cannot feed value of shape (1, 200, 200, 3) for Tensor 'input_hide:0', which has shape '(?, 400, 400, 3)'
How to achieve the encoded image pixels and the encoded pixels consistent？"
Detector dataset,tancik/StegaStamp,2021-12-20 03:24:06,0,,42,1084315711,"Thank you for your amazing work. 

What is the dataset used in training the detector?
"
Why StegaStamp can only encode 56bits (7 characters) with ECC?,tancik/StegaStamp,2021-12-07 11:08:17,1,,41,1073220462,"Dear tancik:

I got the following error when running `encode_image.py`:
```
Error: Can only encode 56 bits (7 characters) with ECC.
```
I'm confused about why it can only encode 56 bits (7 characters) with ECC.

Thanks in advance! 😄 
"
Detecting and decoding an image ,tancik/StegaStamp,2021-09-25 10:36:52,0,,40,1007050535,"Hi everyone,
I am trying to add a image switch to decode.py file to detect and decode the hidden physical images. Anyone has done it before or any suggestions ?"
Problem with parameter setting .,tancik/StegaStamp,2021-09-14 08:43:08,4,,38,995753587,"What does YUV_scales stand for? Why did you set y,u, and v to 1,100,100, respectively?
How do I adjust these three parameters? 

Many thanks"
save_video ,tancik/StegaStamp,2021-07-20 11:28:31,0,,36,948562080,Hello guys. could anyone tell me what kind of file form should I create for save_video FILENAME?
how to reducer the params for fc layer in decoder?,tancik/StegaStamp,2020-10-25 04:09:41,1,,25,728935341,"hi @tancik 
since the `fc layer` in `decoder` is too larger (13*13 * 128 * 500 = 10,816,000), so i want use a conv(dim=1024) to increase the dimension and then use `tf.reduce_mean(x, axis=[1, 2])`, but in the first 1w iterations, the `bit_acc` always gets around 0.5, and the `str_acc` gets zero. so how to reduce the params for the `fc layer`, at the same time, it can get good results?"
test_vid.mp4,tancik/StegaStamp,2020-07-27 06:48:35,4,,21,666025328,Where is test_vid.mp4？
The full detector model,tancik/StegaStamp,2020-07-24 09:49:31,1,,20,665054782,"Could you guys give the original detector model please?
I mean the one you released is not robust."
does tensorflow 1.13.2 have problem?,tancik/StegaStamp,2020-07-16 02:13:40,0,,19,657786444,"my test environment:
``` bash
docker pull tensorflow/tensorflow:1.13.2-gpu-py3
```
and I found the lastest saved model 
``` bash
root@3ec62ce01961:~/StegaStamp# du -sm saved_models/7/*
208     saved_models/7/saved_model.pb
0       saved_models/7/variables
```
but the released model is 
``` bash
(tf1.13) [phoenixkiller@ultrapower stegastamp_pretrained]$ du -sm *
1	saved_model.pb
207	variables
```
Does I got pb model?
And I still could not get the performance same as release model.
Should I change tensorflow version to 1.13.1?"
could you share training parameters?,tancik/StegaStamp,2020-07-15 00:01:15,18,,17,656967870,"I try to train model, bug could not get the same performance like the released model.
Could you share training parameters or told me how to adjust parameters.

Thanks."
feeding a new natural image without secret,tancik/StegaStamp,2020-06-17 13:13:39,1,,16,640432513,"I am wondering when you feed a natural image without residual secret into the decoder network.

what result will the decoder produce?

"
dataset ,tancik/StegaStamp,2020-06-12 13:50:41,5,,15,637764336,Could you send the dataset to me ?  I cannot get the data.
import lpips.lpips_tf failed.,tancik/StegaStamp,2020-05-11 07:57:08,3,,14,615672696,"Failed in models.py line 3: import lpips.lpips_tf.
No module named 'lpips_tf'.
"
How to get the value of sins in train_CVPPP.ipynb?,kulikovv/harmonic,2021-06-22 12:46:47,0,,6,927194775,"In train_CVPPP.ipynb: 
_sins = [[-24.22295570373535, 0.0, 0.4452361464500427],
 [-14.779047012329102, 0.0, 1.2561423778533936],
 [-16.399198532104492, 0.0, -0.3734317719936371],
 [18.362571716308594, 0.0, 0.7659217715263367],
 [-0.6603534817695618, 0.0, 0.24005144834518433],
 [-33.7341423034668, 0.0, -0.4350433051586151],
 [0.0, 4.286965370178223, 0.8109257817268372],
 [0.0, -19.908288955688477, 0.614355206489563],
 [0.0, 22.987812042236328, 0.28104516863822937],
 [0.0, 7.108828067779541, 0.4827950894832611],
 [0.0, 23.66850471496582, 0.21264752745628357],
 [0.0, -22.332250595092773, 1.0007625818252563]]_

Is it initialized randomly?"
sin function,kulikovv/harmonic,2021-04-20 11:58:22,0,,5,862740747,"how to train the parameter in sin function. I don't find it in the project



"
CVPPP ground truth data,kulikovv/harmonic,2021-01-24 13:03:15,3,,4,792791617,"Hi,

I read the _train_CVPPP.ipynb_ file and I'm a bit confused about the ground truth data - especially about the following:
`edges = sorted([join(basepath, f) for f in listdir(basepath) if f.endswith('_edge.png')])`

I downloaded the dataset _LSC CVPPP2017_ and checked its content.
However it contains only the follwing ground truth data:
- centers
- foreground (fg)
- rgb (this is not supposed to be ground truth but input data)
- count
- label

So there is not any _*edge.png_ file. I'd appreciate if could provide information about where you got this file from and what it actually contains (e.g. binary image of the edges of each leave or binary image of the edges of the whole plant).

Thanks a lot!"
CPU forward gives `IndexError` for diagonal matrices,nnaisense/pytorch_sym3eig,2022-05-05 21:17:52,0,,2,1227184962,"Applying Sym3Eig to diagonal matrices gives `IndexError: select(): index 1 out of range for tensor of size [1, 3] at dimension 0`.

Simple reproduction: 
```
m = torch.diag(torch.rand(3)).unsqueeze(0)
e, v = Sym3Eig.apply(m)
```"
model layers about conv_head and fc_head,wuyuebupt/doubleheadsrcnn,2022-03-10 08:53:09,0,,10,1164933239,"In roi_box_predictors.py, I didn't find the code of conv_head layers, it seems that there is only fc_head layer. Can you tell me where I can find these two heads code. Thanks a lot.
```
class FPNPredictorNeighbor(nn.Module):
    def __init__(self, cfg):
        super(FPNPredictorNeighbor, self).__init__()
        num_classes = cfg.MODEL.ROI_BOX_HEAD.NUM_CLASSES
        representation_size = cfg.MODEL.ROI_BOX_HEAD.NONLOCAL_OUT_CHANNELS

        self.cls_score = nn.Linear(representation_size, num_classes)
        num_bbox_reg_classes = 2 if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG else num_classes
        self.bbox_pred = nn.Linear(representation_size, num_bbox_reg_classes * 4)


        nn.init.normal_(self.cls_score.weight, std=0.01)
        nn.init.normal_(self.bbox_pred.weight, std=0.001)
        for l in [self.cls_score, self.bbox_pred]:
            nn.init.constant_(l.bias, 0)

        ## fc layer
        representation_size_fc = cfg.MODEL.ROI_BOX_HEAD.MLP_HEAD_DIM
        self.cls_score_fc = nn.Linear(representation_size_fc, num_classes)
        self.bbox_pred_fc = nn.Linear(representation_size_fc, num_bbox_reg_classes * 4)

        nn.init.normal_(self.cls_score_fc.weight, std=0.01)
        nn.init.normal_(self.bbox_pred_fc.weight, std=0.001)
        for l in [self.cls_score_fc, self.bbox_pred_fc]:
            nn.init.constant_(l.bias, 0)
```"
"I found you did not use the self.cls_nonlocal() and self.reg_nonlocal in roi_box_feature_extractors.py.The number of blocks ""self.cls_num_stack"" and ""self.reg_num_stack"" are zero",wuyuebupt/doubleheadsrcnn,2021-08-31 08:10:13,1,,9,983571191,"codes are here:https://github.com/wuyuebupt/doubleheadsrcnn/blob/master/maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_feature_extractors.py.
And another question:I found no difference between the reg and cls operations. All the network structure parameters are the same except some details."
What is meaning of 'evalutation-flags' ?,wuyuebupt/doubleheadsrcnn,2021-06-20 15:57:35,3,,8,925629035,"Thank you for your work.

When I run 'test_net.py', GeneralizedRCNN in generalized_rcnn.py return list with len=4.

and each 'list' (4 number of) in returned list has different num_boxes.

When I revise augument '--evalutation-flags' from [1, 1, 1, 1] (default) to [1], returned result has 4 lists.

What is meaning of 'evalutation-flags' ? I just want to use 'double-head-ext' only.

![img](https://user-images.githubusercontent.com/59550240/122680639-2cae9e00-d22b-11eb-8f6d-7230a5cff6c5.png)
"
Regreesion and Classification Heads in Code,wuyuebupt/doubleheadsrcnn,2021-06-04 15:49:33,3,,7,911639654,Could you please advise where in this project I can find the classification and regression heads? How the regression head gets bounding box coordinates as inputs to regress the object's bounding box coordinates in the next frame. 
Can you describe the differences between class Pooler and class PoolerNeighbor?,wuyuebupt/doubleheadsrcnn,2020-12-10 13:18:51,1,,5,761234589,Thanks for your nice work. I find that you use the class PoolerNeighbor instead of the class Pooler. It seems that the PoolerNeighbor only expand proposals. Can you describe the differences between class Pooler and class PoolerNeighbor?
How to build maskrcnn-benchmark?,wuyuebupt/doubleheadsrcnn,2020-10-05 06:49:15,1,,4,714566655,"Hi, 

Could you help me with the step for building maskrcnn-benchmark? It says in your ```INSTALL.md``` file to run the commands below. 

My question is, before doing this, should I replace the ```maskrcnn_benchmark``` (with underscore) folder inside ```maskrcnn-benchmark``` (with dash) with the ```maskrcnn_benchmark``` (with underscore) folder in your repository? Or should I proceed with the instructions below as is, without incorporating your own ```maskrcnn_benchmark``` folder?

Here are the commands from https://github.com/wuyuebupt/doubleheadsrcnn/blob/master/INSTALL.md:

```cd $INSTALL_DIR
git clone https://github.com/facebookresearch/maskrcnn-benchmark.git
cd maskrcnn-benchmark
# the following will install the lib with
# symbolic links, so that you can modify
# the files if you want and won't need to
# re-build it
python setup.py build develop

"
data preparation,dmsm/DeepParametricShapes,2020-12-30 06:43:13,1,,2,776296582,"Hi, I'm really interested in your work, I want to retrain the 3d model by myself, could you release the ready-to-use datasets?"
How can we arrange the dataset to train the model?,facebookresearch/you2me,2022-02-14 16:33:12,0,,11,1137552122,"Hi, thank you for the code and contribution.

Could you please tell me how we can arrange the datasets (kinect, panoptic)? I am not understanding well how we should create the directories and subdirectories of homographies, openpose, and images, because, when I run train.py it asks me for sequences that do not exist in the dataset. I am trying with the panoptic dataset. 

Thank you."
Where can we find the weights of the trained model?,facebookresearch/you2me,2022-02-14 16:27:58,0,,10,1137545490,"Hi, thanks for the code and paper. Is it possible to get the weights of the trained model? 

Thank you"
How to generate directory of openpose predictions,facebookresearch/you2me,2020-10-09 09:18:12,0,,7,717990725,"In the **Test** and **Train** the readme.md file asks to generate **directory of openpose predictions**. Can you please tell how to do that ?
"
QUESTION ABOUT TESTING,YuankaiQi/REVERIE,2022-11-07 02:04:44,1,,17,1437658959,"Hi Yuankai,

I used the Recurrent VLN BERT to train a sequence-sequence agent and got the best val unseen model. However, while I test it on the website on test split, it underperforms the val unseen results from a large margin (-8% on SR and SPL). Is it normal? "
bbox  category,YuankaiQi/REVERIE,2022-07-14 11:42:30,0,,14,1304666641,"Can you please inform me where is the object_label-object_name (eg : 73-""light"")dictionary file  in dataset REVERIE?"
Something wrong when building docker,YuankaiQi/REVERIE,2022-01-20 13:49:49,2,,11,1109325579,"Hi, Yuankai.
I was doing `docker build -t reverie .` and it got the following error.
```
Reading package lists...
Reading package lists...
Building dependency tree...
Reading state information...
E: Unable to locate package python3.6
E: Couldn't find any package by glob 'python3.6'
E: Couldn't find any package by regex 'python3.6'
The command '/bin/sh -c apt-get update && apt-get install -y python3.6' returned a non-zero code: 100
```

It seems that `RUN add-apt-repository ppa:deadsnakes/ppa` in the dockerfile didn't work. Is there any solution?

Very appreciated to your reply."
Bounding box extraction,YuankaiQi/REVERIE,2021-11-23 19:42:48,0,,10,1061647410,"Hi!

I'm interested in using the bounding boxes annotated in REVERIE. I noticed that the categories are similar to the ones in the metadata of Matterport3D. Also, some objects are not annotated (such as doors).

Could you explain to me how were the bounding boxes annotated in REVERIE?

Thanks in advance,
Benjamin"
Pruning Problem of MobileNetV2 for CIFAR100 dataset,cmu-enyac/LeGR,2022-07-13 11:01:20,0,,11,1303271084,"Hello @RudyChin ,

Thank you for your brilliant work.

I am facing some problems when I used the CIFAR100 dataset. For CIFAR10 it works well. I got the same result that you showed in the paper for CIFAR10.

But for CIFAR100 dataset, When I select  0.9 pruning away for 60 epochs even 1000 epochs, it just prunes till 80% flops where targeted flops should be 26.58M for 90% prunes. I tried in a multiple-way to change the hyper-parameters but it does not work.

Would you please, give me some suggestions that how I can solve this problem?

Here are my  used hyper-parameters for your reference,

Namespace(datapath='./data', dataset='torchvision.datasets.CIFAR100', epoch=1000, name='prune_90', model='./mbnetv2c100-best.pth', batch_size=128, lr=0.01, lbda=3e-09, prune_away=0.9, constraint='flops', large_input=False, no_grow=False, pruner='FilterPrunerMBNetV2')

Thank you and eagerly waiting for your valuable reply."
Question about using multi-gpu,cmu-enyac/LeGR,2020-08-05 03:00:45,0,,5,673215264,"Hello Ruby,

I am modifying your code to try to run your code on multi-gpu using DataParallel. For the Cifar10 example, I changed the code in __main__ function as below:

```
    device_id = []
    device_id.append(0)
    device_id.append(1)
    model = nn.DataParallel(model, device_ids=device_id).cuda()

    # if isinstance(model, nn.DataParallel):
    #     model = model.module

```
But it reports error:
```
Traceback (most recent call last):            
   File ""legr.py"", line 302, in <module>                                                                                                                                                                                                             
       legr.prune(args.name, args.model, args.long_ft, (1-(prune_away)/100.))                                                                                                                                                                        
  File ""legr.py"", line 173, in prune                                                                                                                                                                                                                
     acc = test(self.model, self.test_loader, device=self.device)                                                                                                                                                                                  
  File ""/home/workspace/test_gpu/utils/drivers.py"", line 142, in test                                                                                                                                                                      
    output = model(batch)                                                                                                                                                                                                                         
  File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__                                                                                                                                     
    result = self.forward(*input, **kwargs)                                                                                                                                                                                                       
  File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 143, in forward                                                                                                                              
    outputs = self.parallel_apply(replicas, inputs, kwargs)                                                                                                                                                                                       
  File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in parallel_apply                                                                                                                       
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])                                                                                                                                                              
  File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 83, in parallel_apply                                                                                                                       
    raise output                                                                                                                                                                                                                                  
  File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 59, in _worker                                                                                                                              
    output = module(*input, **kwargs)                                                                                                                                                                                                             
  File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__                                                                                                                                     
    result = self.forward(*input, **kwargs)                                                                                                                                                                                                       
 File ""/home/workspace/test_gpu/model/resnet_cifar10.py"", line 87, in forward                                                                                                                                                             
   x = self.features(x)                                                                                                                                                                                                                          
 File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__                                                                                                                                     
   result = self.forward(*input, **kwargs)                                                                                                                                                                                                       
 File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward                                                                                                                                    
   input = module(input)                                                                                                                                                                                                                         
 File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__                                                                                                                                     
   result = self.forward(*input, **kwargs)                                                                                                                                                                                                       
 File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 92, in forward                                                                                                                                    
    input = module(input)                                                                                                                                                                                                                         
 File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 489, in __call__                                                                                                                                     
    result = self.forward(*input, **kwargs)                                                                                                                                                                                                       
 File ""/home/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 320, in forward                                                                                                                                        
   self.padding, self.dilation, self.groups)                                                                                                                                                                                                   
 RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)  
```
I tried to comment out `legr.pruner.forward(torch.zeros((1,3,dummy_size, dummy_size), device=device))` in main function, then the `test` function in line 173 will not report error. So I wonder whether the `forward` function in `fp_resnet.py` modify the original model so that the original model cannot be tested using multi-gpu?

When I tried the ResNet50 on ImageNet using multi-gpu, it reports the similar error. 

Could you also please advise how I can use multiple GPUs to run your code?

Thank you!

"
may i ask?  what's the difference between x2 and #2 in the paper at Table1 ？,ToyotaResearchInstitute/packnet-sfm,2020-01-16 08:06:12,0,,2,550640841,"i tend to accout for that #2 means layer2 and x2 means layer2's outputs,is that right ?  thankyo"
Code is missing,ToyotaResearchInstitute/packnet-sfm,2019-06-12 04:36:03,4,,1,455007807,"Could you please provide the source code and pretrained netowork, used in the demo video:
https://youtu.be/-N8QFtL3ees

I would love to integrate it in http://ossdc.org, and test it against the datsets/videos listed here:
https://github.com/OSSDC/OSSDC-VisionBasedACC/issues/19

Thanks,
Marius"
Tiling Texture,lzqsd/InverseRenderingOfIndoorScene,2022-10-15 13:16:45,0,,12,1410164323,"Hi, thank you for your good work.

I was wondering how I can extract the Diffuse Albedo, Normal, and Roughness that's tileable out of the model after running it? I tried running the model, but the output images are not tileable.

Thanks"
About download link,lzqsd/InverseRenderingOfIndoorScene,2022-07-08 03:10:43,2,,11,1298422367,"Hi, may I get the access of download link here? I have sent email to OpenRoomsDataset@gmail.com, but do not get the response. My email is zsybul@hust.edu.cn. Waiting for your reply, thanks."
render result is blur,lzqsd/InverseRenderingOfIndoorScene,2022-01-19 03:38:31,2,,9,1107604727,"@lzqsd  I download the pretrained model and use testReal.py to render img, but the result is very blur(input img is clear)
![image](https://user-images.githubusercontent.com/23699630/150059466-d9a68dbe-23ca-4c18-a8eb-33c5ae156260.png)
![image](https://user-images.githubusercontent.com/23699630/150059481-b33df7c4-70de-4209-a642-540a4f17b400.png)
![image](https://user-images.githubusercontent.com/23699630/150059526-dec48ee5-f0ec-4d84-8b7b-86c1786ba689.png)
![image](https://user-images.githubusercontent.com/23699630/150059542-731058bb-291e-4b6e-86a4-9646b8370354.png)
"
"File name(dataset/main_xml1\scene0299_00\immask_8.png,.....) does not exist",lzqsd/InverseRenderingOfIndoorScene,2021-08-10 07:51:43,6,,4,964696426,"Hi, I encountered the following problems
![image](https://user-images.githubusercontent.com/69796850/128830436-5d92500f-d650-4c39-a124-09b868190442.png)
this problem is caused by this
![image](https://user-images.githubusercontent.com/69796850/128827129-06f55d1f-6641-43d2-aff2-14518741bff1.png)
I have downloaded OpenRooms dataset  and put them in the ./dataset folder, there are no pictures of  png format in main_xml1 folder. Now, I don't know where are the pictures of png format from."
[Question] Would be possible to use IES files for light sources?,lzqsd/InverseRenderingOfIndoorScene,2020-10-22 10:23:40,0,,3,727257470,"I do not know whether you have considered to functionality of loading IES files for the corresponding light sources in the scene, but I believe that this would be quite a nice addition.

Thanks."
Using another renderer,lzqsd/InverseRenderingOfIndoorScene,2020-09-04 18:52:57,4,,2,693495489,"Hi!

How hard do you think it would be to translate the Optix Renderer scene description files to Mitsuba scene description files? 

I think it would be interesting to be able to render this dataset using Mitsuba and some of the extensions available for Mitsuba such as [MitsubaToF](https://github.com/cmu-ci-lab/MitsubaToFRenderer) (for transient rendering) and [MitsubaCLT](https://github.com/cmu-ci-lab/mitsuba_clt) (for computational light transport imaging systems), and maybe even Mitsuba 2 (for differentiable rendering).

Thanks! 
Felipe"
Availability of the openrooms dataset and tools,lzqsd/InverseRenderingOfIndoorScene,2020-08-20 08:56:38,4,,1,682543394,"Hi guys,

Do you have any estimation about when you will be able to release the openrooms dataset and tools?

Thank you."
transfer model dataset,neuroailab/VIE,2021-01-26 08:43:57,0,,24,794044943,"Hi, UCF101 contains train splits from 1 to 3 and test splits from 1 to 3, which train split and test split did you use to train and evaluate transfer model?"
Tensorflow version?,neuroailab/VIE,2020-10-29 04:15:00,2,,17,731983052,"hello dear author, what is your Tensorflow version?"
TwoPathway models,neuroailab/VIE,2020-09-20 09:06:45,3,,11,705073885,"Can't find any codes for TwoPathway models, Are there any clues about them available?"
no models files,neuroailab/VIE,2020-09-08 06:15:10,6,,9,695577903,"hello, when i start training directly, i run the run_training.sh, but it shows there is no model.ckpt-50000 file. Can you help me solve  this problem? Thanks!"
meaning of  val_log result,neuroailab/VIE,2020-09-02 11:31:34,5,,8,690944379,"Hello I got some result on the dataset.

topn: {'top1_10NN': 0.080424786}
I know it's not the accuracy. I read your paper,but don't know the meaning of **Nearest neighbor validation performances**.

What does it mean?


"
AttributeError: 'Clustering' object has no attribute 'obj',neuroailab/VIE,2020-09-01 13:59:43,2,,7,690168138,"Hello! It's me again. Thanks for your help,I can start training in my dataset. (The IR result accuracy is only 10%. )  Whatever, I continue training in with LA.   However , it comes an error.
![F BFILAI1 AUTDKFZWDFE6](https://user-images.githubusercontent.com/35788879/91861416-5c41e900-ec9f-11ea-97bd-8e0415f428ef.png)


I didn't find its difinition. Is it an easy-solving bug?"
How to get kinetics_train.csv,neuroailab/VIE,2020-07-15 08:39:33,2,,3,657172004,"Hi chengxu,

Thanks a lot for your excellent work and open resource code! Here is my problem.
When I want to download kinetics like your version, it seems that you did not provide the kinetics_train.csv. 
![image](https://user-images.githubusercontent.com/42595629/87523301-5c6b3280-c6b9-11ea-88a1-816c01fb7f7d.png)
Kinetics-400 contains two folders ""train"" and ""val"" in my mind which exsits no kinetics_train.csv like you said. How should I get it?

Looking forward for your reply and thanks!
"
Any plan for a PyTorch version,neuroailab/VIE,2020-07-14 06:52:33,2,,2,656374679,"Hey, great work! Do you plan to release a PyTorch version? 

Best."
want to know the meaning in InstanceModel._softmax,neuroailab/VIE,2020-06-01 15:05:49,1,,1,628508034,"Hi everyone, first thanks a lot for your outstanding Paper and open resource code!
I am trying to train some videos without annotated, and get the video level representation, e.g., get a embedding likes BERT doing (I am a newer in CV).  When after reading your code, I am confused in this line https://github.com/neuroailab/VIE/blob/ac85fabe934977d0afdb21b4210edd69d8a75ba9/tf_model/model/instance_model.py#L206
I don't know what `2876934.2 / 1281167 * self.instance_data_len` really means, is it a kind of scale?
Thanks a lot :D"
about code,marvin-eisenberger/smooth-shells,2020-09-29 01:45:11,0,,1,710685779,"Dear professor，
sorry for bother you, I can not understand your demo code, could you describe your readme document in detail, and is there your all code, I need to recurrence your code, hope you can help me. Looking forward to your early reply. "
"Please publish a lightweight video model, such as X3D /TSM or R(2+1)18 etc....",facebookresearch/R2Plus1D,2022-05-10 06:58:00,0,,136,1230704028,"Dear author:
    Thanks for shareing the great large scale pretrained models. Since the dataset is not avaiable for downloading, could you kindle train and publish a lightweight models, such as X3D or TSM?? thank you very much."
questions about input,facebookresearch/R2Plus1D,2021-12-04 07:47:47,1,,135,1071151267,how many clips are used for average prediction for ucf101 and hmdb5d1?
Not able to reproduce reported results on Pytorch,facebookresearch/R2Plus1D,2021-09-14 07:57:37,0,,134,995710400,"I am unable to reproduce the results for `ir-CSN-152` and `ip-CSN-152` pretrained on Sports1M ft Kinetics and IG-65M ft Kinetics. When tested on Kinetics 400 val, the `Acc@1` and `Acc@5` are around 40% and 65% respectively. Table 5 in the paper indicates 82% and 95%. Are there issues with the Pytorch models?"
Calculating weights for Gradient Blending for new datasets,facebookresearch/R2Plus1D,2021-07-28 16:21:05,0,,133,955026937,"Hi,
Are there any additional resources about calculating the normalizing constants for the blending weights from a practical implementation point of view apart from the literature (what makes training multi-modal classification networks hard?).

The weights for the datasets used are given : https://github.com/facebookresearch/VMZ/blob/master/c2/tutorials/gradient_blending.md
But I couldn't find any code where their calculation is implemented.

But more importantly how to find them for a new dataset? The proof of the proposition for gradient blending is not helpful in this regard. Also going through the repo, they are considered an argument: https://github.com/facebookresearch/VMZ/blob/4cc542d58c1c560571e1e962426e858e00de28d2/c2/tools/train_net.py#L255

in the ""add_weighted_loss()"" method and I couldn't find any helper functions which might have been used to calculate them.

Thank you for your response."
ucf101 ip_csn_152 training/test dataset config,facebookresearch/R2Plus1D,2021-04-23 19:38:27,0,,131,866381322,"Hi,

Thanks for making this open source! I am a little confused on the ucf 101 dataset config. Is the following correspondence correct? should I leave args.val_file and args.train_file as default? 

```
def dataset_load_defaults(args):
    if args.dataset == ""ucf101"":
        args.traindir = ""/data/richardkxu/UCF101/UCF101""  # unrar from  UCF101.rar
        args.valdir = ""/data/richardkxu/UCF101/UCF101""  # unrar from  UCF101.rar

        args.val_file = ""/checkpoint/bkorbar/DATASET_TV/ucf101_train_16fms.pth""
        args.train_file = ""/checkpoint/bkorbar/DATASET_TV/ucf101_train_16fms.pth""
       
        args.annotation_path = (
            ""/data/richardkxu/UCF101/ucfTrainTestlist/""  # unzip from UCF101TrainTestSplits-RecognitionTask.zip
        )
```

With the above ucf101 config, I have encountered the following dataset error when running: `train.py --model ip_csn_152` or `""train.py --model ip_csn_152""`. It seems like the size of the train and test dataset are both 0. How can I resolve this error?

```
/home/richardkxu/anaconda3/envs/csn-env/lib/python3.7/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn(""video_reader video backend is not available"")
Not using distributed mode

torch version:  1.4.0
torchvision version:  0.5.0
2021-04-23 12:23:38.081063: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-04-23 12:23:38.081079: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Loading data
	 Loading datasets
	 Loading train data

	 Loading validation data

/home/richardkxu/Documents/csn-fork/pt/vmz/models/csn.py:62: UserWarning: Unrecognized pretraining dataset, continuing with randomly initialized network. Available pretrainings: {avail_pretrainings}
  UserWarning,
Creating model
<generator object Module.parameters at 0x7f76799176d0>
Start training
Traceback (most recent call last):
  File ""/home/richardkxu/Documents/csn-fork/pt/vmz/func/train.py"", line 306, in <module>
    train_main(args)
  File ""/home/richardkxu/Documents/csn-fork/pt/vmz/func/train.py"", line 273, in train_main
    args.apex,
  File ""/home/richardkxu/Documents/csn-fork/pt/vmz/func/train.py"", line 38, in train_one_epoch
    for data in metric_logger.log_every(data_loader, print_freq, header):
  File ""/home/richardkxu/Documents/csn-fork/pt/vmz/common/log.py"", line 171, in log_every
    for obj in iterable:
  File ""/home/richardkxu/anaconda3/envs/csn-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 279, in __iter__
    return _MultiProcessingDataLoaderIter(self)
  File ""/home/richardkxu/anaconda3/envs/csn-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 746, in __init__
    self._try_put_index()
  File ""/home/richardkxu/anaconda3/envs/csn-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 861, in _try_put_index
    index = self._next_index()
  File ""/home/richardkxu/anaconda3/envs/csn-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 339, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File ""/home/richardkxu/anaconda3/envs/csn-env/lib/python3.7/site-packages/torch/utils/data/sampler.py"", line 200, in __iter__
    for idx in self.sampler:
  File ""/home/richardkxu/Documents/csn-fork/pt/vmz/common/sampler.py"", line 117, in __iter__
    idxs = torch.cat(idxs)
RuntimeError: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors.  Available functions are [CUDATensorId, CPUTensorId, VariableTensorId]

Process finished with exit code 1

```
"
torchvision video_reader video backend is not available,facebookresearch/R2Plus1D,2021-04-23 19:02:24,0,,130,866359776,"Hi,

Thanks for making this open source! I have followed the pytorch installation steps here: https://github.com/facebookresearch/VMZ/blob/master/pt/INSTALL.md. However, I have encountered the following error when I tried to do training on ucf 101 or simply load "" torchvision.set_video_backend('video_reader')"":

```
/home/richardkxu/anaconda3/envs/csn/lib/python3.7/site-packages/torchvision/__init__.py:64: UserWarning: video_reader video backend is not available
  warnings.warn(""video_reader video backend is not available"")

av                        7.0.1            py37h82f89c2_2    conda-forge
ffmpeg                    4.2.2                h20bf706_0  
torch                     1.4.0                    pypi_0    pypi
torchvision               0.5.0                    pypi_0    pypi
cudatoolkit               10.2.89              hfd86e86_1  
```

I have `CUDA=10.2` on my system. I am wondering how to solve the above error? Do I have to use `torchvision==0.5.0` or newer version is fine? Do I need to compile torchvision from source (but usually `av` + `torchvision` should work)? Thank you!"
ir_csn_152 size mismatch: Error(s) in loading state_dict for VideoResNet,facebookresearch/R2Plus1D,2021-03-12 08:19:34,5,,129,829870348,"I follow the installation guide to install VMZ locally. When I try to load the pretrained ir_csn_152 model to PyTorch: 
`from vmz.models import ir_csn_152`
`model = ir_csn_152(pretraining=""ig65m_32frms"")`
I received an error saying that there is a size mismatch with fc.weight and fc.bias. I assumed this is a bug where the finetuned model was given the pretrained model weights or vice versa. 
![image](https://user-images.githubusercontent.com/33640876/110912081-1d73a680-8346-11eb-9736-97397fa87462.png)
"
Facing errors-> 'VideoResNet' object has no attribute 'compile'  'add' 'summary' 'fit' ,facebookresearch/R2Plus1D,2021-01-14 16:38:43,0,,127,786139853,
performance on UCF and HMDB (split1) ,facebookresearch/R2Plus1D,2020-11-27 07:51:54,0,,125,752053615,"Thanks for sharing exciting work! But I have some questions as follow:

UCF and HMDB are popular datasets for action recognition, but your work doesn't report these results. (except R2+1D report pre-training on Kinetics)
I3D has greatly improved on HMDB and UCF since pre-training on Kinetics. So how about R2+1D and CSN after pre-training on big datasets, especially in IG65M.

I am attempting to finetune on UCF and HMDB by your Initialization weights by Kinetics and IG65M. So can you provide me some experimental results?
Thanks"
DistributedDataParallel error when train the model in one server with multi-gpus?,facebookresearch/R2Plus1D,2020-10-19 02:30:51,1,,124,724211030,"Hi，when I run the train codes:

python tools/train_net.py --name=/mnt/codes/ckpts/trains --model=ir_csn_152 --resume_from_model=/mnt/codes/weights/pre_trained_weights/irCSN_152_ig65m_from_scratch_f125286141.pth --dataset=ucf101 --traindir=/mnt/codes/dataset/UCF-101/ --nodes=1 --batch-size=1 --workers=8 --epochs=45 --finetune=True --num_finetune_classes=43

meeting the error:
AssertionError: DistributedDataParallel with multi-device module only works with CUDA devices, but module parameters locate in {device(type='cuda', index=0), device(type='cpu')}.

should i change the opts or some codes in pt/vmz/func/train.py?how to solve the problem?

the log.out:
submitit INFO (2020-10-16 10:04:53,290) - Starting with JobEnvironment(job_id=2027, hostname=b33760dbd191, local_rank=0(1), node=0(1), global_rank=0(1))
submitit INFO (2020-10-16 10:04:53,291) - Loading pickle: /mnt/codes/ckpts/trains/2027/2027_submitted.pkl
Process group: 1 tasks, rank: 0
| distributed init (rank 0): file:///mnt/codes/ckpts/trains/aa2610db20af40c0bdadaad8882d434f_init
Namespace(annotation_path='', apex=False, apex_opt_level='O1', batch_size=1, crop_size=112, dataset='ucf101', device='cuda', dist_backend='nccl', dist_url='file:///mnt/codes/ckpts/trains/aa2610db20af40c0bdadaad8882d434f_init', distributed=True, epochs=45, eval_only=False, fc_lr=0.1, finetune='True', fold=1, gpu=0, l1_lr=0.001, l2_lr=0.001, l3_lr=0.001, l4_lr=0.001, lr=0.01, lr_gamma=0.1, lr_milestones=[20, 30, 40], lr_warmup_epochs=10, model='ir_csn_152', momentum=0.9, name='/mnt/codes/ckpts/trains', nodes=1, num_classes=400, num_finetune_classes=43, num_frames=16, output_dir='/mnt/codes/ckpts/trains/2027', partition='dev', pretrained='', print_freq=10, rank=0, resume='', resume_from_model='/mnt/codes/weights/irCSN_152_ig65m_from_scratch_f125286141.pth', scale_h=128, scale_w=174, start_epoch=0, sync_bn=False, train_bs_multiplier=5, train_file='', traindir='/mnt/codes/dataset/UCF-101/', val_clips_per_video=1, val_file='', valdir='/mnt/codes/dataset/val_tmp/', weight_decay=0.0001, workers=8, world_size=1)
torch version:  1.6.0
torchvision version:  0.7.0



"
Feature extraction tutorial error,facebookresearch/R2Plus1D,2020-10-16 16:22:10,3,modules:c2,123,723358935,"`python3 data/create_video_db.py --list_file sample_videos.csv --output_file my_lmdb_data --use_list 1 --use_video_id 1 --use_start_frame 1`

yields

```
TypeError
'/home/dario/temp/datasets/ADD/Anomaly-Videos/Abuse/Abuse001_x264.mp4' has type str, but expected one of: bytes
```
at  https://github.com/facebookresearch/VMZ/blob/master/c2/data/create_video_db.py#L86

```
                if not use_list:
                    with open(file_name, mode='rb') as file:
                        video_data = file.read()
                else:
                    video_data = file_name

                tensor_protos = caffe2_pb2.TensorProtos()
                video_tensor = tensor_protos.protos.add()
                video_tensor.data_type = 4  # string data
>>              video_tensor.string_data.append(video_data)
```
`video_data` is indeed a `str`, since `video_data = file_name` given `use_list`


I'm uncertain what to do now. Do you need more information? Can you reproduce this?"
VMZ. Pytorch models,facebookresearch/R2Plus1D,2020-07-15 07:33:33,8,modules:pt,117,657131147,"Hi followed the install instructions to get vmz locally.

I wanted to try out some of the pretrained models like  




$  from vmz.models import r2plus1d_34 
$ r2plus1d_34(pretraining='sports1m_32frms')

posted below is the tail output of the model:
 `
  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
  (fc): Linear(in_features=512, out_features=400, bias=True)
`
this seems like its the finetuned weights on kinetics 400 using pretrained weights, as the fc output should have been 487 from Sports1m instead of 400. I figure this is a bug in matching args to urls in . [utils.py](https://github.com/facebookresearch/VMZ/blob/master/pt/vmz/models/utils.py)

similarly, you have key mismatch running 
$ r2plus1d_34(pretraining='ig65m_32frms')

looking up the model_urls at line 7 of [utils.py](https://github.com/facebookresearch/VMZ/blob/master/pt/vmz/models/utils.py) it works when i run with 

$ r2plus1d_34(pretraining='65m_32frms')
`
    )
  )
  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
  (fc): Linear(in_features=512, out_features=400, bias=True)
)
`
I might be off since I am new to video CNN's, just wanted to point this out. I am interested in using pretrained weights from sport1m and IG65m on my own video datasets using newer CNN's like r2plus1d_34 "
Tutorial for pytorch feature extraction,facebookresearch/R2Plus1D,2020-07-14 18:15:21,1,modules:pt,116,656804158,"Hi,
Thanks for sharing the code. Would you please add a tutorial for feature extraction using the pytorch version? What is the ""shared folder"" and the ""checkpoint"" directory?"
Does VMZ work with opencv 4.0+,facebookresearch/R2Plus1D,2020-06-10 00:36:13,1,modules:general,114,635843267,"Quick question, does VMZ work with opencv 4.0+? It is really hard to install opencv 3.4 on my environment."
Unrecognized arguments --pretrained_model in the script finetune_ucf101.sh,facebookresearch/R2Plus1D,2020-05-28 19:18:26,1,,113,626749547,"When I try to run the finetune_ucf101.sh script, it throws unrecognized pretrained_model. 
I searched in train_net.py I didn't see such arguments.
What is the way to load the pre-trained model? "
Feature extraction fails with [E video_decoder.cc:151] Insufficient data to determine video format,facebookresearch/R2Plus1D,2020-05-07 01:57:03,5,modules:c2,110,613719022,"I'm using a pre-trained C3D model from [here](https://github.com/facebookresearch/VMZ#r21d-152) to extract feature for a single video clip but it fails with the following error:
**`[E video_decoder.cc:151] Insufficient data to determine video format`**

I have noted the suggestions from [this issue](https://github.com/facebookresearch/VMZ/issues/13) and picked a large video with 7051 frames.
Logically my video `len` should pass the condition from [this line](https://github.com/pytorch/pytorch/blob/f5b3125af715fa47cb6c95cb5e274adb97c6149c/caffe2/video/video_decoder.cc#L146)
so I think there may be an issue with how I prepare my csv file for lmdb creation. Here is what my csv file looks like:

## csv file

`org_video,label,start_frm,video_id`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,1,0`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,17,1`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,33,2`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,49,3`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,65,4`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,81,5`
`...`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,1537,96`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,1553,97`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,1569,98`
`/path/to/my/video/v_P3vu94B0KGY.mp4,0,1585,99`

**I have also tried starting the video_id from 0 and from 1** but both return the same error.
I'd appreciate if you can provide any insights on what what may be causing this.

## create_video_db command

Here is the command I used to create lmdb for my video:
`python3 data/create_video_db.py \`
`--list_file=/path/to/artifacts/v_P3vu94B0KGY.csv `
`--output_file=/path/to/artifacts/my_lmdb_data \`
`--use_list=1 \`
`--use_video_id=1 \`
`--use_start_frame=1`

## Feature Extraction command

and this is my feature extraction command:
`python3 tools/extract_features.py \`
`--test_data=/path/to/artifacts/my_lmdb_data \`
`--model_name=r2plus1d \`
`--model_depth=152 \`
`--batch_size=4 \`
`--load_model_path=/path/to/VMZ/pretrained_models/r2plus1d_152_ft_kinetics_from_sports1m_f128957437.pkl \`
`--output_path=/path/to/artifacts/my_features.pkl \`
`--features=softmax,final_avg,video_id \`
`--sanity_check=0 \`
`--use_local_file=1 \`
`--num_labels=400 \`
`--num_gpus=1 \`
`--use_cudnn=0 &> error.log`

## Environment
- Ubuntu 18.04.4 LTS
- Python 3.6.9
- NVIDIA-SMI 418.39
- Driver Version: 418.39
- CUDA Version: 10.1


## Error log

`[E init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.`
`[E init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.`
`[E init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.`
`INFO:feature_extractor:Namespace(batch_size=4, bottleneck_multiplier=1.0, channel_multiplier=1.0, clip_length_of=8, clip_length_rgb=16, clip_per_video=1, crop_per_clip=1, crop_size=112, db_type='pickle', decode_type=2, do_flow_aggregation=0, features='softmax,final_avg,video_id', flow_data_type=0, frame_gap_of=2, get_start_frame=0, get_video_id=0, gpus=None, input_type=0, load_model_path='/path/to/VMZ/pretrained_models/r2plus1d_152_ft_kinetics_from_sports1m_f128957437.pkl',`
`model_depth=152, model_name='r2plus1d', multi_label=0, num_channels=3, num_decode_threads=4, num_gpus=1, num_iterations=-1, num_labels=400, output_path='temp_output/zv_features.pkl', sampling_rate_of=2, sampling_rate_rgb=1, sanity_check=0, save_h5=0, scale_h=128, scale_w=171, test_data='temp_output/zv_lmdb_data', use_convolutional_pred=0, use_cudnn=0, use_dropout=0, use_local_file=1, use_pool1=0, video_res_type=0)`
`INFO:model_builder:Validated: r2plus1d with 152 layers`
`INFO:model_builder:with input 16x112x112`
`INFO:feature_extractor:Running on GPUs: range(0, 1)`
`WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.`
`INFO:data_parallel_model:Parallelizing model for devices: range(0, 1)`
`INFO:data_parallel_model:Create input and model training operators`
`WARNING:data_parallel_model:`
`WARNING:data_parallel_model:############# WARNING #############`
`WARNING:data_parallel_model:Model Extract Features/<caffe2.python.cnn.CNNModelHelper object at 0x7fee974de940> is used for testing/validation but`
`WARNING:data_parallel_model:has init_params=True!`
`WARNING:data_parallel_model:This can conflict with model training.`
`WARNING:data_parallel_model:Please ensure model = ModelHelper(init_params=False)`
`WARNING:data_parallel_model:####################################`
`WARNING:data_parallel_model:`
`INFO:data_parallel_model:Model for GPU : 0`
`INFO:model_helper:outputing rgb data`
`INFO:model_builder:creating r2plus1d, depth=152...`
`INFO:video_model:in: 64 out: 64`
`INFO:video_model:in: 64 out: 64`
`INFO:video_model:in: 64 out: 256`
`INFO:video_model:in: 256 out: 64`
`INFO:video_model:in: 64 out: 64`
`INFO:video_model:in: 64 out: 256`
`INFO:video_model:in: 256 out: 64`
`INFO:video_model:in: 64 out: 64`
`INFO:video_model:in: 64 out: 256`
`INFO:video_model:in: 256 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 128`
`INFO:video_model:in: 128 out: 128`
`INFO:video_model:in: 128 out: 512`
`INFO:video_model:in: 512 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 256`
`INFO:video_model:in: 256 out: 256`
`INFO:video_model:in: 256 out: 1024`
`INFO:video_model:in: 1024 out: 512`
`INFO:video_model:in: 512 out: 512`
`INFO:video_model:in: 512 out: 2048`
`INFO:video_model:in: 2048 out: 512`
`INFO:video_model:in: 512 out: 512`
`INFO:video_model:in: 512 out: 2048`
`INFO:video_model:in: 2048 out: 512`
`INFO:video_model:in: 512 out: 512`
`INFO:video_model:in: 512 out: 2048`
`INFO:data_parallel_model:Parameter update function not defined --> only forward`
`INFO:model_loader:copying conv1_middle_w`
`INFO:model_loader:copying conv1_middle_spatbn_relu_s`
`INFO:model_loader:copying conv1_middle_spatbn_relu_b`
`INFO:model_loader:copying conv1_w`
`INFO:model_loader:copying conv1_spatbn_relu_s`
`INFO:model_loader:copying conv1_spatbn_relu_b`
`INFO:model_loader:copying comp_0_conv_1_w`
`INFO:model_loader:copying comp_0_spatbn_1_s`
`INFO:model_loader:copying comp_0_spatbn_1_b`
`INFO:model_loader:copying comp_0_conv_2_middle_w`
`INFO:model_loader:copying comp_0_spatbn_2_middle_s`
`INFO:model_loader:copying comp_0_spatbn_2_middle_b`
`INFO:model_loader:copying comp_0_conv_2_w`
`INFO:model_loader:copying comp_0_spatbn_2_s`
`INFO:model_loader:copying comp_0_spatbn_2_b`
`INFO:model_loader:copying comp_0_conv_3_w`
`INFO:model_loader:copying comp_0_spatbn_3_s`
`INFO:model_loader:copying comp_0_spatbn_3_b`
`INFO:model_loader:copying shortcut_projection_0_w`
`INFO:model_loader:copying shortcut_projection_0_spatbn_s`
`INFO:model_loader:copying shortcut_projection_0_spatbn_b`
`INFO:model_loader:copying comp_1_conv_1_w`
`INFO:model_loader:copying comp_1_spatbn_1_s`
`INFO:model_loader:copying comp_1_spatbn_1_b`
`INFO:model_loader:copying comp_1_conv_2_middle_w`
`INFO:model_loader:copying comp_1_spatbn_2_middle_s`
`INFO:model_loader:copying comp_1_spatbn_2_middle_b`
`INFO:model_loader:copying comp_1_conv_2_w`
`INFO:model_loader:copying comp_1_spatbn_2_s`
`INFO:model_loader:copying comp_1_spatbn_2_b`
`INFO:model_loader:copying comp_1_conv_3_w`
`INFO:model_loader:copying comp_1_spatbn_3_s`
`INFO:model_loader:copying comp_1_spatbn_3_b`
`INFO:model_loader:copying comp_2_conv_1_w`
`INFO:model_loader:copying comp_2_spatbn_1_s`
`INFO:model_loader:copying comp_2_spatbn_1_b`
`INFO:model_loader:copying comp_2_conv_2_middle_w`
`INFO:model_loader:copying comp_2_spatbn_2_middle_s`
`INFO:model_loader:copying comp_2_spatbn_2_middle_b`
`INFO:model_loader:copying comp_2_conv_2_w`
`INFO:model_loader:copying comp_2_spatbn_2_s`
`INFO:model_loader:copying comp_2_spatbn_2_b`
`INFO:model_loader:copying comp_2_conv_3_w`
`INFO:model_loader:copying comp_2_spatbn_3_s`
`INFO:model_loader:copying comp_2_spatbn_3_b`
`INFO:model_loader:copying comp_3_conv_1_w`
`INFO:model_loader:copying comp_3_spatbn_1_s`
`INFO:model_loader:copying comp_3_spatbn_1_b`
`INFO:model_loader:copying comp_3_conv_2_middle_w`
`INFO:model_loader:copying comp_3_spatbn_2_middle_s`
`INFO:model_loader:copying comp_3_spatbn_2_middle_b`
`INFO:model_loader:copying comp_3_conv_2_w`
`INFO:model_loader:copying comp_3_spatbn_2_s`
`INFO:model_loader:copying comp_3_spatbn_2_b`
`INFO:model_loader:copying comp_3_conv_3_w`
`INFO:model_loader:copying comp_3_spatbn_3_s`
`INFO:model_loader:copying comp_3_spatbn_3_b`
`INFO:model_loader:copying shortcut_projection_3_w`
`INFO:model_loader:copying shortcut_projection_3_spatbn_s`
`INFO:model_loader:copying shortcut_projection_3_spatbn_b`
`INFO:model_loader:copying comp_4_conv_1_w`
`INFO:model_loader:copying comp_4_spatbn_1_s`
`INFO:model_loader:copying comp_4_spatbn_1_b`
`INFO:model_loader:copying comp_4_conv_2_middle_w`
`INFO:model_loader:copying comp_4_spatbn_2_middle_s`
`INFO:model_loader:copying comp_4_spatbn_2_middle_b`
`INFO:model_loader:copying comp_4_conv_2_w`
`INFO:model_loader:copying comp_4_spatbn_2_s`
`INFO:model_loader:copying comp_4_spatbn_2_b`
`INFO:model_loader:copying comp_4_conv_3_w`
`INFO:model_loader:copying comp_4_spatbn_3_s`
`INFO:model_loader:copying comp_4_spatbn_3_b`
`INFO:model_loader:copying comp_5_conv_1_w`
`INFO:model_loader:copying comp_5_spatbn_1_s`
`INFO:model_loader:copying comp_5_spatbn_1_b`
`INFO:model_loader:copying comp_5_conv_2_middle_w`
`INFO:model_loader:copying comp_5_spatbn_2_middle_s`
`INFO:model_loader:copying comp_5_spatbn_2_middle_b`
`INFO:model_loader:copying comp_5_conv_2_w`
`INFO:model_loader:copying comp_5_spatbn_2_s`
`INFO:model_loader:copying comp_5_spatbn_2_b`
`INFO:model_loader:copying comp_5_conv_3_w`
`INFO:model_loader:copying comp_5_spatbn_3_s`
`INFO:model_loader:copying comp_5_spatbn_3_b`
`INFO:model_loader:copying comp_6_conv_1_w`
`INFO:model_loader:copying comp_6_spatbn_1_s`
`INFO:model_loader:copying comp_6_spatbn_1_b`
`INFO:model_loader:copying comp_6_conv_2_middle_w`
`INFO:model_loader:copying comp_6_spatbn_2_middle_s`
`INFO:model_loader:copying comp_6_spatbn_2_middle_b`
`INFO:model_loader:copying comp_6_conv_2_w`
`INFO:model_loader:copying comp_6_spatbn_2_s`
`INFO:model_loader:copying comp_6_spatbn_2_b`
`INFO:model_loader:copying comp_6_conv_3_w`
`INFO:model_loader:copying comp_6_spatbn_3_s`
`INFO:model_loader:copying comp_6_spatbn_3_b`
`INFO:model_loader:copying comp_7_conv_1_w`
`INFO:model_loader:copying comp_7_spatbn_1_s`
`INFO:model_loader:copying comp_7_spatbn_1_b`
`INFO:model_loader:copying comp_7_conv_2_middle_w`
`INFO:model_loader:copying comp_7_spatbn_2_middle_s`
`INFO:model_loader:copying comp_7_spatbn_2_middle_b`
`INFO:model_loader:copying comp_7_conv_2_w`
`INFO:model_loader:copying comp_7_spatbn_2_s`
`INFO:model_loader:copying comp_7_spatbn_2_b`
`INFO:model_loader:copying comp_7_conv_3_w`
`INFO:model_loader:copying comp_7_spatbn_3_s`
`INFO:model_loader:copying comp_7_spatbn_3_b`
`INFO:model_loader:copying comp_8_conv_1_w`
`INFO:model_loader:copying comp_8_spatbn_1_s`
`INFO:model_loader:copying comp_8_spatbn_1_b`
`INFO:model_loader:copying comp_8_conv_2_middle_w`
`INFO:model_loader:copying comp_8_spatbn_2_middle_s`
`INFO:model_loader:copying comp_8_spatbn_2_middle_b`
`INFO:model_loader:copying comp_8_conv_2_w`
`INFO:model_loader:copying comp_8_spatbn_2_s`
`INFO:model_loader:copying comp_8_spatbn_2_b`
`INFO:model_loader:copying comp_8_conv_3_w`
`INFO:model_loader:copying comp_8_spatbn_3_s`
`INFO:model_loader:copying comp_8_spatbn_3_b`
`INFO:model_loader:copying comp_9_conv_1_w`
`INFO:model_loader:copying comp_9_spatbn_1_s`
`INFO:model_loader:copying comp_9_spatbn_1_b`
`INFO:model_loader:copying comp_9_conv_2_middle_w`
`INFO:model_loader:copying comp_9_spatbn_2_middle_s`
`INFO:model_loader:copying comp_9_spatbn_2_middle_b`
`INFO:model_loader:copying comp_9_conv_2_w`
`INFO:model_loader:copying comp_9_spatbn_2_s`
`INFO:model_loader:copying comp_9_spatbn_2_b`
`INFO:model_loader:copying comp_9_conv_3_w`
`INFO:model_loader:copying comp_9_spatbn_3_s`
`INFO:model_loader:copying comp_9_spatbn_3_b`
`INFO:model_loader:copying comp_10_conv_1_w`
`INFO:model_loader:copying comp_10_spatbn_1_s`
`INFO:model_loader:copying comp_10_spatbn_1_b`
`INFO:model_loader:copying comp_10_conv_2_middle_w`
`INFO:model_loader:copying comp_10_spatbn_2_middle_s`
`INFO:model_loader:copying comp_10_spatbn_2_middle_b`
`INFO:model_loader:copying comp_10_conv_2_w`
`INFO:model_loader:copying comp_10_spatbn_2_s`
`INFO:model_loader:copying comp_10_spatbn_2_b`
`INFO:model_loader:copying comp_10_conv_3_w`
`INFO:model_loader:copying comp_10_spatbn_3_s`
`INFO:model_loader:copying comp_10_spatbn_3_b`
`INFO:model_loader:copying comp_11_conv_1_w`
`INFO:model_loader:copying comp_11_spatbn_1_s`
`INFO:model_loader:copying comp_11_spatbn_1_b`
`INFO:model_loader:copying comp_11_conv_2_middle_w`
`INFO:model_loader:copying comp_11_spatbn_2_middle_s`
`INFO:model_loader:copying comp_11_spatbn_2_middle_b`
`INFO:model_loader:copying comp_11_conv_2_w`
`INFO:model_loader:copying comp_11_spatbn_2_s`
`INFO:model_loader:copying comp_11_spatbn_2_b`
`INFO:model_loader:copying comp_11_conv_3_w`
`INFO:model_loader:copying comp_11_spatbn_3_s`
`INFO:model_loader:copying comp_11_spatbn_3_b`
`INFO:model_loader:copying shortcut_projection_11_w`
`INFO:model_loader:copying shortcut_projection_11_spatbn_s`
`INFO:model_loader:copying shortcut_projection_11_spatbn_b`
`INFO:model_loader:copying comp_12_conv_1_w`
`INFO:model_loader:copying comp_12_spatbn_1_s`
`INFO:model_loader:copying comp_12_spatbn_1_b`
`INFO:model_loader:copying comp_12_conv_2_middle_w`
`INFO:model_loader:copying comp_12_spatbn_2_middle_s`
`INFO:model_loader:copying comp_12_spatbn_2_middle_b`
`INFO:model_loader:copying comp_12_conv_2_w`
`INFO:model_loader:copying comp_12_spatbn_2_s`
`INFO:model_loader:copying comp_12_spatbn_2_b`
`INFO:model_loader:copying comp_12_conv_3_w`
`INFO:model_loader:copying comp_12_spatbn_3_s`
`INFO:model_loader:copying comp_12_spatbn_3_b`
`INFO:model_loader:copying comp_13_conv_1_w`
`INFO:model_loader:copying comp_13_spatbn_1_s`
`INFO:model_loader:copying comp_13_spatbn_1_b`
`INFO:model_loader:copying comp_13_conv_2_middle_w`
`INFO:model_loader:copying comp_13_spatbn_2_middle_s`
`INFO:model_loader:copying comp_13_spatbn_2_middle_b`
`INFO:model_loader:copying comp_13_conv_2_w`
`INFO:model_loader:copying comp_13_spatbn_2_s`
`INFO:model_loader:copying comp_13_spatbn_2_b`
`INFO:model_loader:copying comp_13_conv_3_w`
`INFO:model_loader:copying comp_13_spatbn_3_s`
`INFO:model_loader:copying comp_13_spatbn_3_b`
`INFO:model_loader:copying comp_14_conv_1_w`
`INFO:model_loader:copying comp_14_spatbn_1_s`
`INFO:model_loader:copying comp_14_spatbn_1_b`
`INFO:model_loader:copying comp_14_conv_2_middle_w`
`INFO:model_loader:copying comp_14_spatbn_2_middle_s`
`INFO:model_loader:copying comp_14_spatbn_2_middle_b`
`INFO:model_loader:copying comp_14_conv_2_w`
`INFO:model_loader:copying comp_14_spatbn_2_s`
`INFO:model_loader:copying comp_14_spatbn_2_b`
`INFO:model_loader:copying comp_14_conv_3_w`
`INFO:model_loader:copying comp_14_spatbn_3_s`
`INFO:model_loader:copying comp_14_spatbn_3_b`
`INFO:model_loader:copying comp_15_conv_1_w`
`INFO:model_loader:copying comp_15_spatbn_1_s`
`INFO:model_loader:copying comp_15_spatbn_1_b`
`INFO:model_loader:copying comp_15_conv_2_middle_w`
`INFO:model_loader:copying comp_15_spatbn_2_middle_s`
`INFO:model_loader:copying comp_15_spatbn_2_middle_b`
`INFO:model_loader:copying comp_15_conv_2_w`
`INFO:model_loader:copying comp_15_spatbn_2_s`
`INFO:model_loader:copying comp_15_spatbn_2_b`
`INFO:model_loader:copying comp_15_conv_3_w`
`INFO:model_loader:copying comp_15_spatbn_3_s`
`INFO:model_loader:copying comp_15_spatbn_3_b`
`INFO:model_loader:copying comp_16_conv_1_w`
`INFO:model_loader:copying comp_16_spatbn_1_s`
`INFO:model_loader:copying comp_16_spatbn_1_b`
`INFO:model_loader:copying comp_16_conv_2_middle_w`
`INFO:model_loader:copying comp_16_spatbn_2_middle_s`
`INFO:model_loader:copying comp_16_spatbn_2_middle_b`
`INFO:model_loader:copying comp_16_conv_2_w`
`INFO:model_loader:copying comp_16_spatbn_2_s`
`INFO:model_loader:copying comp_16_spatbn_2_b`
`INFO:model_loader:copying comp_16_conv_3_w`
`INFO:model_loader:copying comp_16_spatbn_3_s`
`INFO:model_loader:copying comp_16_spatbn_3_b`
`INFO:model_loader:copying comp_17_conv_1_w`
`INFO:model_loader:copying comp_17_spatbn_1_s`
`INFO:model_loader:copying comp_17_spatbn_1_b`
`INFO:model_loader:copying comp_17_conv_2_middle_w`
`INFO:model_loader:copying comp_17_spatbn_2_middle_s`
`INFO:model_loader:copying comp_17_spatbn_2_middle_b`
`INFO:model_loader:copying comp_17_conv_2_w`
`INFO:model_loader:copying comp_17_spatbn_2_s`
`INFO:model_loader:copying comp_17_spatbn_2_b`
`INFO:model_loader:copying comp_17_conv_3_w`
`INFO:model_loader:copying comp_17_spatbn_3_s`
`INFO:model_loader:copying comp_17_spatbn_3_b`
`INFO:model_loader:copying comp_18_conv_1_w`
`INFO:model_loader:copying comp_18_spatbn_1_s`
`INFO:model_loader:copying comp_18_spatbn_1_b`
`INFO:model_loader:copying comp_18_conv_2_middle_w`
`INFO:model_loader:copying comp_18_spatbn_2_middle_s`
`INFO:model_loader:copying comp_18_spatbn_2_middle_b`
`INFO:model_loader:copying comp_18_conv_2_w`
`INFO:model_loader:copying comp_18_spatbn_2_s`
`INFO:model_loader:copying comp_18_spatbn_2_b`
`INFO:model_loader:copying comp_18_conv_3_w`
`INFO:model_loader:copying comp_18_spatbn_3_s`
`INFO:model_loader:copying comp_18_spatbn_3_b`
`INFO:model_loader:copying comp_19_conv_1_w`
`INFO:model_loader:copying comp_19_spatbn_1_s`
`INFO:model_loader:copying comp_19_spatbn_1_b`
`INFO:model_loader:copying comp_19_conv_2_middle_w`
`INFO:model_loader:copying comp_19_spatbn_2_middle_s`
`INFO:model_loader:copying comp_19_spatbn_2_middle_b`
`INFO:model_loader:copying comp_19_conv_2_w`
`INFO:model_loader:copying comp_19_spatbn_2_s`
`INFO:model_loader:copying comp_19_spatbn_2_b`
`INFO:model_loader:copying comp_19_conv_3_w`
`INFO:model_loader:copying comp_19_spatbn_3_s`
`INFO:model_loader:copying comp_19_spatbn_3_b`
`INFO:model_loader:copying comp_20_conv_1_w`
`INFO:model_loader:copying comp_20_spatbn_1_s`
`INFO:model_loader:copying comp_20_spatbn_1_b`
`INFO:model_loader:copying comp_20_conv_2_middle_w`
`INFO:model_loader:copying comp_20_spatbn_2_middle_s`
`INFO:model_loader:copying comp_20_spatbn_2_middle_b`
`INFO:model_loader:copying comp_20_conv_2_w`
`INFO:model_loader:copying comp_20_spatbn_2_s`
`INFO:model_loader:copying comp_20_spatbn_2_b`
`INFO:model_loader:copying comp_20_conv_3_w`
`INFO:model_loader:copying comp_20_spatbn_3_s`
`INFO:model_loader:copying comp_20_spatbn_3_b`
`INFO:model_loader:copying comp_21_conv_1_w`
`INFO:model_loader:copying comp_21_spatbn_1_s`
`INFO:model_loader:copying comp_21_spatbn_1_b`
`INFO:model_loader:copying comp_21_conv_2_middle_w`
`INFO:model_loader:copying comp_21_spatbn_2_middle_s`
`INFO:model_loader:copying comp_21_spatbn_2_middle_b`
`INFO:model_loader:copying comp_21_conv_2_w`
`INFO:model_loader:copying comp_21_spatbn_2_s`
`INFO:model_loader:copying comp_21_spatbn_2_b`
`INFO:model_loader:copying comp_21_conv_3_w`
`INFO:model_loader:copying comp_21_spatbn_3_s`
`INFO:model_loader:copying comp_21_spatbn_3_b`
`INFO:model_loader:copying comp_22_conv_1_w`
`INFO:model_loader:copying comp_22_spatbn_1_s`
`INFO:model_loader:copying comp_22_spatbn_1_b`
`INFO:model_loader:copying comp_22_conv_2_middle_w`
`INFO:model_loader:copying comp_22_spatbn_2_middle_s`
`INFO:model_loader:copying comp_22_spatbn_2_middle_b`
`INFO:model_loader:copying comp_22_conv_2_w`
`INFO:model_loader:copying comp_22_spatbn_2_s`
`INFO:model_loader:copying comp_22_spatbn_2_b`
`INFO:model_loader:copying comp_22_conv_3_w`
`INFO:model_loader:copying comp_22_spatbn_3_s`
`INFO:model_loader:copying comp_22_spatbn_3_b`
`INFO:model_loader:copying comp_23_conv_1_w`
`INFO:model_loader:copying comp_23_spatbn_1_s`
`INFO:model_loader:copying comp_23_spatbn_1_b`
`INFO:model_loader:copying comp_23_conv_2_middle_w`
`INFO:model_loader:copying comp_23_spatbn_2_middle_s`
`INFO:model_loader:copying comp_23_spatbn_2_middle_b`
`INFO:model_loader:copying comp_23_conv_2_w`
`INFO:model_loader:copying comp_23_spatbn_2_s`
`INFO:model_loader:copying comp_23_spatbn_2_b`
`INFO:model_loader:copying comp_23_conv_3_w`
`INFO:model_loader:copying comp_23_spatbn_3_s`
`INFO:model_loader:copying comp_23_spatbn_3_b`
`INFO:model_loader:copying comp_24_conv_1_w`
`INFO:model_loader:copying comp_24_spatbn_1_s`
`INFO:model_loader:copying comp_24_spatbn_1_b`
`INFO:model_loader:copying comp_24_conv_2_middle_w`
`INFO:model_loader:copying comp_24_spatbn_2_middle_s`
`INFO:model_loader:copying comp_24_spatbn_2_middle_b`
`INFO:model_loader:copying comp_24_conv_2_w`
`INFO:model_loader:copying comp_24_spatbn_2_s`
`INFO:model_loader:copying comp_24_spatbn_2_b`
`INFO:model_loader:copying comp_24_conv_3_w`
`INFO:model_loader:copying comp_24_spatbn_3_s`
`INFO:model_loader:copying comp_24_spatbn_3_b`
`INFO:model_loader:copying comp_25_conv_1_w`
`INFO:model_loader:copying comp_25_spatbn_1_s`
`INFO:model_loader:copying comp_25_spatbn_1_b`
`INFO:model_loader:copying comp_25_conv_2_middle_w`
`INFO:model_loader:copying comp_25_spatbn_2_middle_s`
`INFO:model_loader:copying comp_25_spatbn_2_middle_b`
`INFO:model_loader:copying comp_25_conv_2_w`
`INFO:model_loader:copying comp_25_spatbn_2_s`
`INFO:model_loader:copying comp_25_spatbn_2_b`
`INFO:model_loader:copying comp_25_conv_3_w`
`INFO:model_loader:copying comp_25_spatbn_3_s`
`INFO:model_loader:copying comp_25_spatbn_3_b`
`INFO:model_loader:copying comp_26_conv_1_w`
`INFO:model_loader:copying comp_26_spatbn_1_s`
`INFO:model_loader:copying comp_26_spatbn_1_b`
`INFO:model_loader:copying comp_26_conv_2_middle_w`
`INFO:model_loader:copying comp_26_spatbn_2_middle_s`
`INFO:model_loader:copying comp_26_spatbn_2_middle_b`
`INFO:model_loader:copying comp_26_conv_2_w`
`INFO:model_loader:copying comp_26_spatbn_2_s`
`INFO:model_loader:copying comp_26_spatbn_2_b`
`INFO:model_loader:copying comp_26_conv_3_w`
`INFO:model_loader:copying comp_26_spatbn_3_s`
`INFO:model_loader:copying comp_26_spatbn_3_b`
`INFO:model_loader:copying comp_27_conv_1_w`
`INFO:model_loader:copying comp_27_spatbn_1_s`
`INFO:model_loader:copying comp_27_spatbn_1_b`
`INFO:model_loader:copying comp_27_conv_2_middle_w`
`INFO:model_loader:copying comp_27_spatbn_2_middle_s`
`INFO:model_loader:copying comp_27_spatbn_2_middle_b`
`INFO:model_loader:copying comp_27_conv_2_w`
`INFO:model_loader:copying comp_27_spatbn_2_s`
`INFO:model_loader:copying comp_27_spatbn_2_b`
`INFO:model_loader:copying comp_27_conv_3_w`
`INFO:model_loader:copying comp_27_spatbn_3_s`
`INFO:model_loader:copying comp_27_spatbn_3_b`
`INFO:model_loader:copying comp_28_conv_1_w`
`INFO:model_loader:copying comp_28_spatbn_1_s`
`INFO:model_loader:copying comp_28_spatbn_1_b`
`INFO:model_loader:copying comp_28_conv_2_middle_w`
`INFO:model_loader:copying comp_28_spatbn_2_middle_s`
`INFO:model_loader:copying comp_28_spatbn_2_middle_b`
`INFO:model_loader:copying comp_28_conv_2_w`
`INFO:model_loader:copying comp_28_spatbn_2_s`
`INFO:model_loader:copying comp_28_spatbn_2_b`
`INFO:model_loader:copying comp_28_conv_3_w`
`INFO:model_loader:copying comp_28_spatbn_3_s`
`INFO:model_loader:copying comp_28_spatbn_3_b`
`INFO:model_loader:copying comp_29_conv_1_w`
`INFO:model_loader:copying comp_29_spatbn_1_s`
`INFO:model_loader:copying comp_29_spatbn_1_b`
`INFO:model_loader:copying comp_29_conv_2_middle_w`
`INFO:model_loader:copying comp_29_spatbn_2_middle_s`
`INFO:model_loader:copying comp_29_spatbn_2_middle_b`
`INFO:model_loader:copying comp_29_conv_2_w`
`INFO:model_loader:copying comp_29_spatbn_2_s`
`INFO:model_loader:copying comp_29_spatbn_2_b`
`INFO:model_loader:copying comp_29_conv_3_w`
`INFO:model_loader:copying comp_29_spatbn_3_s`
`INFO:model_loader:copying comp_29_spatbn_3_b`
`INFO:model_loader:copying comp_30_conv_1_w`
`INFO:model_loader:copying comp_30_spatbn_1_s`
`INFO:model_loader:copying comp_30_spatbn_1_b`
`INFO:model_loader:copying comp_30_conv_2_middle_w`
`INFO:model_loader:copying comp_30_spatbn_2_middle_s`
`INFO:model_loader:copying comp_30_spatbn_2_middle_b`
`INFO:model_loader:copying comp_30_conv_2_w`
`INFO:model_loader:copying comp_30_spatbn_2_s`
`INFO:model_loader:copying comp_30_spatbn_2_b`
`INFO:model_loader:copying comp_30_conv_3_w`
`INFO:model_loader:copying comp_30_spatbn_3_s`
`INFO:model_loader:copying comp_30_spatbn_3_b`
`INFO:model_loader:copying comp_31_conv_1_w`
`INFO:model_loader:copying comp_31_spatbn_1_s`
`INFO:model_loader:copying comp_31_spatbn_1_b`
`INFO:model_loader:copying comp_31_conv_2_middle_w`
`INFO:model_loader:copying comp_31_spatbn_2_middle_s`
`INFO:model_loader:copying comp_31_spatbn_2_middle_b`
`INFO:model_loader:copying comp_31_conv_2_w`
`INFO:model_loader:copying comp_31_spatbn_2_s`
`INFO:model_loader:copying comp_31_spatbn_2_b`
`INFO:model_loader:copying comp_31_conv_3_w`
`INFO:model_loader:copying comp_31_spatbn_3_s`
`INFO:model_loader:copying comp_31_spatbn_3_b`
`INFO:model_loader:copying comp_32_conv_1_w`
`INFO:model_loader:copying comp_32_spatbn_1_s`
`INFO:model_loader:copying comp_32_spatbn_1_b`
`INFO:model_loader:copying comp_32_conv_2_middle_w`
`INFO:model_loader:copying comp_32_spatbn_2_middle_s`
`INFO:model_loader:copying comp_32_spatbn_2_middle_b`
`INFO:model_loader:copying comp_32_conv_2_w`
`INFO:model_loader:copying comp_32_spatbn_2_s`
`INFO:model_loader:copying comp_32_spatbn_2_b`
`INFO:model_loader:copying comp_32_conv_3_w`
`INFO:model_loader:copying comp_32_spatbn_3_s`
`INFO:model_loader:copying comp_32_spatbn_3_b`
`INFO:model_loader:copying comp_33_conv_1_w`
`INFO:model_loader:copying comp_33_spatbn_1_s`
`INFO:model_loader:copying comp_33_spatbn_1_b`
`INFO:model_loader:copying comp_33_conv_2_middle_w`
`INFO:model_loader:copying comp_33_spatbn_2_middle_s`
`INFO:model_loader:copying comp_33_spatbn_2_middle_b`
`INFO:model_loader:copying comp_33_conv_2_w`
`INFO:model_loader:copying comp_33_spatbn_2_s`
`INFO:model_loader:copying comp_33_spatbn_2_b`
`INFO:model_loader:copying comp_33_conv_3_w`
`INFO:model_loader:copying comp_33_spatbn_3_s`
`INFO:model_loader:copying comp_33_spatbn_3_b`
`INFO:model_loader:copying comp_34_conv_1_w`
`INFO:model_loader:copying comp_34_spatbn_1_s`
`INFO:model_loader:copying comp_34_spatbn_1_b`
`INFO:model_loader:copying comp_34_conv_2_middle_w`
`INFO:model_loader:copying comp_34_spatbn_2_middle_s`
`INFO:model_loader:copying comp_34_spatbn_2_middle_b`
`INFO:model_loader:copying comp_34_conv_2_w`
`INFO:model_loader:copying comp_34_spatbn_2_s`
`INFO:model_loader:copying comp_34_spatbn_2_b`
`INFO:model_loader:copying comp_34_conv_3_w`
`INFO:model_loader:copying comp_34_spatbn_3_s`
`INFO:model_loader:copying comp_34_spatbn_3_b`
`INFO:model_loader:copying comp_35_conv_1_w`
`INFO:model_loader:copying comp_35_spatbn_1_s`
`INFO:model_loader:copying comp_35_spatbn_1_b`
`INFO:model_loader:copying comp_35_conv_2_middle_w`
`INFO:model_loader:copying comp_35_spatbn_2_middle_s`
`INFO:model_loader:copying comp_35_spatbn_2_middle_b`
`INFO:model_loader:copying comp_35_conv_2_w`
`INFO:model_loader:copying comp_35_spatbn_2_s`
`INFO:model_loader:copying comp_35_spatbn_2_b`
`INFO:model_loader:copying comp_35_conv_3_w`
`INFO:model_loader:copying comp_35_spatbn_3_s`
`INFO:model_loader:copying comp_35_spatbn_3_b`
`INFO:model_loader:copying comp_36_conv_1_w`
`INFO:model_loader:copying comp_36_spatbn_1_s`
`INFO:model_loader:copying comp_36_spatbn_1_b`
`INFO:model_loader:copying comp_36_conv_2_middle_w`
`INFO:model_loader:copying comp_36_spatbn_2_middle_s`
`INFO:model_loader:copying comp_36_spatbn_2_middle_b`
`INFO:model_loader:copying comp_36_conv_2_w`
`INFO:model_loader:copying comp_36_spatbn_2_s`
`INFO:model_loader:copying comp_36_spatbn_2_b`
`INFO:model_loader:copying comp_36_conv_3_w`
`INFO:model_loader:copying comp_36_spatbn_3_s`
`INFO:model_loader:copying comp_36_spatbn_3_b`
`INFO:model_loader:copying comp_37_conv_1_w`
`INFO:model_loader:copying comp_37_spatbn_1_s`
`INFO:model_loader:copying comp_37_spatbn_1_b`
`INFO:model_loader:copying comp_37_conv_2_middle_w`
`INFO:model_loader:copying comp_37_spatbn_2_middle_s`
`INFO:model_loader:copying comp_37_spatbn_2_middle_b`
`INFO:model_loader:copying comp_37_conv_2_w`
`INFO:model_loader:copying comp_37_spatbn_2_s`
`INFO:model_loader:copying comp_37_spatbn_2_b`
`INFO:model_loader:copying comp_37_conv_3_w`
`INFO:model_loader:copying comp_37_spatbn_3_s`
`INFO:model_loader:copying comp_37_spatbn_3_b`
`INFO:model_loader:copying comp_38_conv_1_w`
`INFO:model_loader:copying comp_38_spatbn_1_s`
`INFO:model_loader:copying comp_38_spatbn_1_b`
`INFO:model_loader:copying comp_38_conv_2_middle_w`
`INFO:model_loader:copying comp_38_spatbn_2_middle_s`
`INFO:model_loader:copying comp_38_spatbn_2_middle_b`
`INFO:model_loader:copying comp_38_conv_2_w`
`INFO:model_loader:copying comp_38_spatbn_2_s`
`INFO:model_loader:copying comp_38_spatbn_2_b`
`INFO:model_loader:copying comp_38_conv_3_w`
`INFO:model_loader:copying comp_38_spatbn_3_s`
`INFO:model_loader:copying comp_38_spatbn_3_b`
`INFO:model_loader:copying comp_39_conv_1_w`
`INFO:model_loader:copying comp_39_spatbn_1_s`
`INFO:model_loader:copying comp_39_spatbn_1_b`
`INFO:model_loader:copying comp_39_conv_2_middle_w`
`INFO:model_loader:copying comp_39_spatbn_2_middle_s`
`INFO:model_loader:copying comp_39_spatbn_2_middle_b`
`INFO:model_loader:copying comp_39_conv_2_w`
`INFO:model_loader:copying comp_39_spatbn_2_s`
`INFO:model_loader:copying comp_39_spatbn_2_b`
`INFO:model_loader:copying comp_39_conv_3_w`
`INFO:model_loader:copying comp_39_spatbn_3_s``
``INFO:model_loader:copying comp_39_spatbn_3_b`
`INFO:model_loader:copying comp_40_conv_1_w`
`INFO:model_loader:copying comp_40_spatbn_1_s`
`INFO:model_loader:copying comp_40_spatbn_1_b`
`INFO:model_loader:copying comp_40_conv_2_middle_w`
`INFO:model_loader:copying comp_40_spatbn_2_middle_s`
`INFO:model_loader:copying comp_40_spatbn_2_middle_b`
`INFO:model_loader:copying comp_40_conv_2_w`
`INFO:model_loader:copying comp_40_spatbn_2_s`
`INFO:model_loader:copying comp_40_spatbn_2_b`
`INFO:model_loader:copying comp_40_conv_3_w`
`INFO:model_loader:copying comp_40_spatbn_3_s`
`INFO:model_loader:copying comp_40_spatbn_3_b`
`INFO:model_loader:copying comp_41_conv_1_w`
`INFO:model_loader:copying comp_41_spatbn_1_s`
`INFO:model_loader:copying comp_41_spatbn_1_b`
`INFO:model_loader:copying comp_41_conv_2_middle_w`
`INFO:model_loader:copying comp_41_spatbn_2_middle_s`
`INFO:model_loader:copying comp_41_spatbn_2_middle_b`
`INFO:model_loader:copying comp_41_conv_2_w`
`INFO:model_loader:copying comp_41_spatbn_2_s`
`INFO:model_loader:copying comp_41_spatbn_2_b`
`INFO:model_loader:copying comp_41_conv_3_w`
`INFO:model_loader:copying comp_41_spatbn_3_s`
`INFO:model_loader:copying comp_41_spatbn_3_b`
`INFO:model_loader:copying comp_42_conv_1_w`
`INFO:model_loader:copying comp_42_spatbn_1_s`
`INFO:model_loader:copying comp_42_spatbn_1_b`
`INFO:model_loader:copying comp_42_conv_2_middle_w`
`INFO:model_loader:copying comp_42_spatbn_2_middle_s`
`INFO:model_loader:copying comp_42_spatbn_2_middle_b`
`INFO:model_loader:copying comp_42_conv_2_w`
`INFO:model_loader:copying comp_42_spatbn_2_s`
`INFO:model_loader:copying comp_42_spatbn_2_b`
`INFO:model_loader:copying comp_42_conv_3_w`
`INFO:model_loader:copying comp_42_spatbn_3_s`
`INFO:model_loader:copying comp_42_spatbn_3_b`
`INFO:model_loader:copying comp_43_conv_1_w`
`INFO:model_loader:copying comp_43_spatbn_1_s`
`INFO:model_loader:copying comp_43_spatbn_1_b`
`INFO:model_loader:copying comp_43_conv_2_middle_w`
`INFO:model_loader:copying comp_43_spatbn_2_middle_s`
`INFO:model_loader:copying comp_43_spatbn_2_middle_b`
`INFO:model_loader:copying comp_43_conv_2_w`
`INFO:model_loader:copying comp_43_spatbn_2_s`
`INFO:model_loader:copying comp_43_spatbn_2_b`
`INFO:model_loader:copying comp_43_conv_3_w`
`INFO:model_loader:copying comp_43_spatbn_3_s`
`INFO:model_loader:copying comp_43_spatbn_3_b`
`INFO:model_loader:copying comp_44_conv_1_w`
`INFO:model_loader:copying comp_44_spatbn_1_s`
`INFO:model_loader:copying comp_44_spatbn_1_b`
`INFO:model_loader:copying comp_44_conv_2_middle_w`
`INFO:model_loader:copying comp_44_spatbn_2_middle_s`
`INFO:model_loader:copying comp_44_spatbn_2_middle_b`
`INFO:model_loader:copying comp_44_conv_2_w`
`INFO:model_loader:copying comp_44_spatbn_2_s`
`INFO:model_loader:copying comp_44_spatbn_2_b`
`INFO:model_loader:copying comp_44_conv_3_w`
`INFO:model_loader:copying comp_44_spatbn_3_s`
`INFO:model_loader:copying comp_44_spatbn_3_b`
`INFO:model_loader:copying comp_45_conv_1_w`
`INFO:model_loader:copying comp_45_spatbn_1_s`
`INFO:model_loader:copying comp_45_spatbn_1_b`
`INFO:model_loader:copying comp_45_conv_2_middle_w`
`INFO:model_loader:copying comp_45_spatbn_2_middle_s`
`INFO:model_loader:copying comp_45_spatbn_2_middle_b`
`INFO:model_loader:copying comp_45_conv_2_w`
`INFO:model_loader:copying comp_45_spatbn_2_s`
`INFO:model_loader:copying comp_45_spatbn_2_b`
`INFO:model_loader:copying comp_45_conv_3_w`
`INFO:model_loader:copying comp_45_spatbn_3_s`
`INFO:model_loader:copying comp_45_spatbn_3_b`
`INFO:model_loader:copying comp_46_conv_1_w`
`INFO:model_loader:copying comp_46_spatbn_1_s`
`INFO:model_loader:copying comp_46_spatbn_1_b`
`INFO:model_loader:copying comp_46_conv_2_middle_w`
`INFO:model_loader:copying comp_46_spatbn_2_middle_s`
`INFO:model_loader:copying comp_46_spatbn_2_middle_b`
`INFO:model_loader:copying comp_46_conv_2_w`
`INFO:model_loader:copying comp_46_spatbn_2_s`
`INFO:model_loader:copying comp_46_spatbn_2_b`
`INFO:model_loader:copying comp_46_conv_3_w`
`INFO:model_loader:copying comp_46_spatbn_3_s`
`INFO:model_loader:copying comp_46_spatbn_3_b`
`INFO:model_loader:copying comp_47_conv_1_w`
`INFO:model_loader:copying comp_47_spatbn_1_s`
`INFO:model_loader:copying comp_47_spatbn_1_b`
`INFO:model_loader:copying comp_47_conv_2_middle_w`
`INFO:model_loader:copying comp_47_spatbn_2_middle_s`
`INFO:model_loader:copying comp_47_spatbn_2_middle_b`
`INFO:model_loader:copying comp_47_conv_2_w`
`INFO:model_loader:copying comp_47_spatbn_2_s`
`INFO:model_loader:copying comp_47_spatbn_2_b`
`INFO:model_loader:copying comp_47_conv_3_w`
`INFO:model_loader:copying comp_47_spatbn_3_s`
`INFO:model_loader:copying comp_47_spatbn_3_b`
`INFO:model_loader:copying shortcut_projection_47_w`
`INFO:model_loader:copying shortcut_projection_47_spatbn_s`
`INFO:model_loader:copying shortcut_projection_47_spatbn_b`
`INFO:model_loader:copying comp_48_conv_1_w`
`INFO:model_loader:copying comp_48_spatbn_1_s`
`INFO:model_loader:copying comp_48_spatbn_1_b`
`INFO:model_loader:copying comp_48_conv_2_middle_w`
`INFO:model_loader:copying comp_48_spatbn_2_middle_s`
`INFO:model_loader:copying comp_48_spatbn_2_middle_b`
`INFO:model_loader:copying comp_48_conv_2_w`
`INFO:model_loader:copying comp_48_spatbn_2_s`
`INFO:model_loader:copying comp_48_spatbn_2_b`
`INFO:model_loader:copying comp_48_conv_3_w`
`INFO:model_loader:copying comp_48_spatbn_3_s`
`INFO:model_loader:copying comp_48_spatbn_3_b`
`INFO:model_loader:copying comp_49_conv_1_w`
`INFO:model_loader:copying comp_49_spatbn_1_s`
`INFO:model_loader:copying comp_49_spatbn_1_b`
`INFO:model_loader:copying comp_49_conv_2_middle_w`
`INFO:model_loader:copying comp_49_spatbn_2_middle_s`
`INFO:model_loader:copying comp_49_spatbn_2_middle_b`
`INFO:model_loader:copying comp_49_conv_2_w`
`INFO:model_loader:copying comp_49_spatbn_2_s`
`INFO:model_loader:copying comp_49_spatbn_2_b`
`INFO:model_loader:copying comp_49_conv_3_w`
`INFO:model_loader:copying comp_49_spatbn_3_s`
`INFO:model_loader:copying comp_49_spatbn_3_b`
`INFO:model_loader:copying last_out_L400_w`
`INFO:model_loader:copying last_out_L400_b`
`INFO:model_loader:copying conv1_middle_spatbn_relu_rm`
`INFO:model_loader:copying conv1_middle_spatbn_relu_riv`
`INFO:model_loader:copying conv1_spatbn_relu_rm`
`INFO:model_loader:copying conv1_spatbn_relu_riv`
`INFO:model_loader:copying comp_0_spatbn_1_rm`
`INFO:model_loader:copying comp_0_spatbn_1_riv`
`INFO:model_loader:copying comp_0_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_0_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_0_spatbn_2_rm`
`INFO:model_loader:copying comp_0_spatbn_2_riv`
`INFO:model_loader:copying comp_0_spatbn_3_rm`
`INFO:model_loader:copying comp_0_spatbn_3_riv`
`INFO:model_loader:copying shortcut_projection_0_spatbn_rm`
`INFO:model_loader:copying shortcut_projection_0_spatbn_riv`
`INFO:model_loader:copying comp_1_spatbn_1_rm`
`INFO:model_loader:copying comp_1_spatbn_1_riv`
`INFO:model_loader:copying comp_1_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_1_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_1_spatbn_2_rm`
`INFO:model_loader:copying comp_1_spatbn_2_riv`
`INFO:model_loader:copying comp_1_spatbn_3_rm`
`INFO:model_loader:copying comp_1_spatbn_3_riv`
`INFO:model_loader:copying comp_2_spatbn_1_rm`
`INFO:model_loader:copying comp_2_spatbn_1_riv`
`INFO:model_loader:copying comp_2_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_2_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_2_spatbn_2_rm`
`INFO:model_loader:copying comp_2_spatbn_2_riv`
`INFO:model_loader:copying comp_2_spatbn_3_rm`
`INFO:model_loader:copying comp_2_spatbn_3_riv`
`INFO:model_loader:copying comp_3_spatbn_1_rm`
`INFO:model_loader:copying comp_3_spatbn_1_riv`
`INFO:model_loader:copying comp_3_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_3_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_3_spatbn_2_rm`
`INFO:model_loader:copying comp_3_spatbn_2_riv`
`INFO:model_loader:copying comp_3_spatbn_3_rm`
`INFO:model_loader:copying comp_3_spatbn_3_riv`
`INFO:model_loader:copying shortcut_projection_3_spatbn_rm`
`INFO:model_loader:copying shortcut_projection_3_spatbn_riv`
`INFO:model_loader:copying comp_4_spatbn_1_rm`
`INFO:model_loader:copying comp_4_spatbn_1_riv`
`INFO:model_loader:copying comp_4_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_4_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_4_spatbn_2_rm`
`INFO:model_loader:copying comp_4_spatbn_2_riv`
`INFO:model_loader:copying comp_4_spatbn_3_rm`
`INFO:model_loader:copying comp_4_spatbn_3_riv`
`INFO:model_loader:copying comp_5_spatbn_1_rm`
`INFO:model_loader:copying comp_5_spatbn_1_riv`
`INFO:model_loader:copying comp_5_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_5_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_5_spatbn_2_rm`
`INFO:model_loader:copying comp_5_spatbn_2_riv`
`INFO:model_loader:copying comp_5_spatbn_3_rm`
`INFO:model_loader:copying comp_5_spatbn_3_riv`
`INFO:model_loader:copying comp_6_spatbn_1_rm`
`INFO:model_loader:copying comp_6_spatbn_1_riv`
`INFO:model_loader:copying comp_6_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_6_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_6_spatbn_2_rm`
`INFO:model_loader:copying comp_6_spatbn_2_riv`
`INFO:model_loader:copying comp_6_spatbn_3_rm`
`INFO:model_loader:copying comp_6_spatbn_3_riv`
`INFO:model_loader:copying comp_7_spatbn_1_rm`
`INFO:model_loader:copying comp_7_spatbn_1_riv`
`INFO:model_loader:copying comp_7_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_7_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_7_spatbn_2_rm`
`INFO:model_loader:copying comp_7_spatbn_2_riv`
`INFO:model_loader:copying comp_7_spatbn_3_rm`
`INFO:model_loader:copying comp_7_spatbn_3_riv`
`INFO:model_loader:copying comp_8_spatbn_1_rm`
`INFO:model_loader:copying comp_8_spatbn_1_riv`
`INFO:model_loader:copying comp_8_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_8_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_8_spatbn_2_rm`
`INFO:model_loader:copying comp_8_spatbn_2_riv`
`INFO:model_loader:copying comp_8_spatbn_3_rm`
`INFO:model_loader:copying comp_8_spatbn_3_riv`
`INFO:model_loader:copying comp_9_spatbn_1_rm`
`INFO:model_loader:copying comp_9_spatbn_1_riv`
`INFO:model_loader:copying comp_9_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_9_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_9_spatbn_2_rm`
`INFO:model_loader:copying comp_9_spatbn_2_riv`
`INFO:model_loader:copying comp_9_spatbn_3_rm`
`INFO:model_loader:copying comp_9_spatbn_3_riv`
`INFO:model_loader:copying comp_10_spatbn_1_rm`
`INFO:model_loader:copying comp_10_spatbn_1_riv`
`INFO:model_loader:copying comp_10_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_10_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_10_spatbn_2_rm`
`INFO:model_loader:copying comp_10_spatbn_2_riv`
`INFO:model_loader:copying comp_10_spatbn_3_rm`
`INFO:model_loader:copying comp_10_spatbn_3_riv`
`INFO:model_loader:copying comp_11_spatbn_1_rm`
`INFO:model_loader:copying comp_11_spatbn_1_riv`
`INFO:model_loader:copying comp_11_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_11_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_11_spatbn_2_rm`
`INFO:model_loader:copying comp_11_spatbn_2_riv`
`INFO:model_loader:copying comp_11_spatbn_3_rm`
`INFO:model_loader:copying comp_11_spatbn_3_riv`
`INFO:model_loader:copying shortcut_projection_11_spatbn_rm`
`INFO:model_loader:copying shortcut_projection_11_spatbn_riv`
`INFO:model_loader:copying comp_12_spatbn_1_rm`
`INFO:model_loader:copying comp_12_spatbn_1_riv`
`INFO:model_loader:copying comp_12_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_12_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_12_spatbn_2_rm`
`INFO:model_loader:copying comp_12_spatbn_2_riv`
`INFO:model_loader:copying comp_12_spatbn_3_rm`
`INFO:model_loader:copying comp_12_spatbn_3_riv`
`INFO:model_loader:copying comp_13_spatbn_1_rm`
`INFO:model_loader:copying comp_13_spatbn_1_riv`
`INFO:model_loader:copying comp_13_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_13_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_13_spatbn_2_rm`
`INFO:model_loader:copying comp_13_spatbn_2_riv`
`INFO:model_loader:copying comp_13_spatbn_3_rm`
`INFO:model_loader:copying comp_13_spatbn_3_riv`
`INFO:model_loader:copying comp_14_spatbn_1_rm`
`INFO:model_loader:copying comp_14_spatbn_1_riv`
`INFO:model_loader:copying comp_14_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_14_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_14_spatbn_2_rm`
`INFO:model_loader:copying comp_14_spatbn_2_riv`
`INFO:model_loader:copying comp_14_spatbn_3_rm`
`INFO:model_loader:copying comp_14_spatbn_3_riv`
`INFO:model_loader:copying comp_15_spatbn_1_rm`
`INFO:model_loader:copying comp_15_spatbn_1_riv`
`INFO:model_loader:copying comp_15_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_15_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_15_spatbn_2_rm`
`INFO:model_loader:copying comp_15_spatbn_2_riv`
`INFO:model_loader:copying comp_15_spatbn_3_rm`
`INFO:model_loader:copying comp_15_spatbn_3_riv`
`INFO:model_loader:copying comp_16_spatbn_1_rm`
`INFO:model_loader:copying comp_16_spatbn_1_riv`
`INFO:model_loader:copying comp_16_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_16_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_16_spatbn_2_rm`
`INFO:model_loader:copying comp_16_spatbn_2_riv`
`INFO:model_loader:copying comp_16_spatbn_3_rm`
`INFO:model_loader:copying comp_16_spatbn_3_riv`
`INFO:model_loader:copying comp_17_spatbn_1_rm`
`INFO:model_loader:copying comp_17_spatbn_1_riv`
`INFO:model_loader:copying comp_17_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_17_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_17_spatbn_2_rm`
`INFO:model_loader:copying comp_17_spatbn_2_riv`
`INFO:model_loader:copying comp_17_spatbn_3_rm`
`INFO:model_loader:copying comp_17_spatbn_3_riv`
`INFO:model_loader:copying comp_18_spatbn_1_rm`
`INFO:model_loader:copying comp_18_spatbn_1_riv`
`INFO:model_loader:copying comp_18_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_18_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_18_spatbn_2_rm`
`INFO:model_loader:copying comp_18_spatbn_2_riv`
`INFO:model_loader:copying comp_18_spatbn_3_rm`
`INFO:model_loader:copying comp_18_spatbn_3_riv`
`INFO:model_loader:copying comp_19_spatbn_1_rm`
`INFO:model_loader:copying comp_19_spatbn_1_riv`
`INFO:model_loader:copying comp_19_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_19_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_19_spatbn_2_rm`
`INFO:model_loader:copying comp_19_spatbn_2_riv`
`INFO:model_loader:copying comp_19_spatbn_3_rm`
`INFO:model_loader:copying comp_19_spatbn_3_riv`
`INFO:model_loader:copying comp_20_spatbn_1_rm`
`INFO:model_loader:copying comp_20_spatbn_1_riv`
`INFO:model_loader:copying comp_20_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_20_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_20_spatbn_2_rm`
`INFO:model_loader:copying comp_20_spatbn_2_riv`
`INFO:model_loader:copying comp_20_spatbn_3_rm`
`INFO:model_loader:copying comp_20_spatbn_3_riv`
`INFO:model_loader:copying comp_21_spatbn_1_rm`
`INFO:model_loader:copying comp_21_spatbn_1_riv`
`INFO:model_loader:copying comp_21_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_21_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_21_spatbn_2_rm`
`INFO:model_loader:copying comp_21_spatbn_2_riv`
`INFO:model_loader:copying comp_21_spatbn_3_rm`
`INFO:model_loader:copying comp_21_spatbn_3_riv`
`INFO:model_loader:copying comp_22_spatbn_1_rm`
`INFO:model_loader:copying comp_22_spatbn_1_riv`
`INFO:model_loader:copying comp_22_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_22_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_22_spatbn_2_rm`
`INFO:model_loader:copying comp_22_spatbn_2_riv`
`INFO:model_loader:copying comp_22_spatbn_3_rm`
`INFO:model_loader:copying comp_22_spatbn_3_riv`
`INFO:model_loader:copying comp_23_spatbn_1_rm`
`INFO:model_loader:copying comp_23_spatbn_1_riv`
`INFO:model_loader:copying comp_23_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_23_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_23_spatbn_2_rm`
`INFO:model_loader:copying comp_23_spatbn_2_riv`
`INFO:model_loader:copying comp_23_spatbn_3_rm`
`INFO:model_loader:copying comp_23_spatbn_3_riv`
`INFO:model_loader:copying comp_24_spatbn_1_rm`
`INFO:model_loader:copying comp_24_spatbn_1_riv`
`INFO:model_loader:copying comp_24_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_24_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_24_spatbn_2_rm`
`INFO:model_loader:copying comp_24_spatbn_2_riv`
`INFO:model_loader:copying comp_24_spatbn_3_rm`
`INFO:model_loader:copying comp_24_spatbn_3_riv`
`INFO:model_loader:copying comp_25_spatbn_1_rm`
`INFO:model_loader:copying comp_25_spatbn_1_riv`
`INFO:model_loader:copying comp_25_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_25_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_25_spatbn_2_rm`
`INFO:model_loader:copying comp_25_spatbn_2_riv`
`INFO:model_loader:copying comp_25_spatbn_3_rm`
`INFO:model_loader:copying comp_25_spatbn_3_riv`
`INFO:model_loader:copying comp_26_spatbn_1_rm`
`INFO:model_loader:copying comp_26_spatbn_1_riv`
`INFO:model_loader:copying comp_26_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_26_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_26_spatbn_2_rm`
`INFO:model_loader:copying comp_26_spatbn_2_riv`
`INFO:model_loader:copying comp_26_spatbn_3_rm`
`INFO:model_loader:copying comp_26_spatbn_3_riv`
`INFO:model_loader:copying comp_27_spatbn_1_rm`
`INFO:model_loader:copying comp_27_spatbn_1_riv`
`INFO:model_loader:copying comp_27_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_27_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_27_spatbn_2_rm`
`INFO:model_loader:copying comp_27_spatbn_2_riv`
`INFO:model_loader:copying comp_27_spatbn_3_rm`
`INFO:model_loader:copying comp_27_spatbn_3_riv`
`INFO:model_loader:copying comp_28_spatbn_1_rm`
`INFO:model_loader:copying comp_28_spatbn_1_riv`
`INFO:model_loader:copying comp_28_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_28_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_28_spatbn_2_rm`
`INFO:model_loader:copying comp_28_spatbn_2_riv`
`INFO:model_loader:copying comp_28_spatbn_3_rm`
`INFO:model_loader:copying comp_28_spatbn_3_riv`
`INFO:model_loader:copying comp_29_spatbn_1_rm`
`INFO:model_loader:copying comp_29_spatbn_1_riv`
`INFO:model_loader:copying comp_29_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_29_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_29_spatbn_2_rm`
`INFO:model_loader:copying comp_29_spatbn_2_riv`
`INFO:model_loader:copying comp_29_spatbn_3_rm`
`INFO:model_loader:copying comp_29_spatbn_3_riv`
`INFO:model_loader:copying comp_30_spatbn_1_rm`
`INFO:model_loader:copying comp_30_spatbn_1_riv`
`INFO:model_loader:copying comp_30_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_30_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_30_spatbn_2_rm`
`INFO:model_loader:copying comp_30_spatbn_2_riv`
`INFO:model_loader:copying comp_30_spatbn_3_rm`
`INFO:model_loader:copying comp_30_spatbn_3_riv`
`INFO:model_loader:copying comp_31_spatbn_1_rm`
`INFO:model_loader:copying comp_31_spatbn_1_riv`
`INFO:model_loader:copying comp_31_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_31_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_31_spatbn_2_rm`
`INFO:model_loader:copying comp_31_spatbn_2_riv`
`INFO:model_loader:copying comp_31_spatbn_3_rm`
`INFO:model_loader:copying comp_31_spatbn_3_riv`
`INFO:model_loader:copying comp_32_spatbn_1_rm`
`INFO:model_loader:copying comp_32_spatbn_1_riv`
`INFO:model_loader:copying comp_32_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_32_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_32_spatbn_2_rm`
`INFO:model_loader:copying comp_32_spatbn_2_riv`
`INFO:model_loader:copying comp_32_spatbn_3_rm`
`INFO:model_loader:copying comp_32_spatbn_3_riv`
`INFO:model_loader:copying comp_33_spatbn_1_rm`
`INFO:model_loader:copying comp_33_spatbn_1_riv`
`INFO:model_loader:copying comp_33_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_33_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_33_spatbn_2_rm`
`INFO:model_loader:copying comp_33_spatbn_2_riv`
`INFO:model_loader:copying comp_33_spatbn_3_rm`
`INFO:model_loader:copying comp_33_spatbn_3_riv`
`INFO:model_loader:copying comp_34_spatbn_1_rm`
`INFO:model_loader:copying comp_34_spatbn_1_riv`
`INFO:model_loader:copying comp_34_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_34_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_34_spatbn_2_rm`
`INFO:model_loader:copying comp_34_spatbn_2_riv`
`INFO:model_loader:copying comp_34_spatbn_3_rm`
`INFO:model_loader:copying comp_34_spatbn_3_riv`
`INFO:model_loader:copying comp_35_spatbn_1_rm`
`INFO:model_loader:copying comp_35_spatbn_1_riv`
`INFO:model_loader:copying comp_35_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_35_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_35_spatbn_2_rm`
`INFO:model_loader:copying comp_35_spatbn_2_riv`
`INFO:model_loader:copying comp_35_spatbn_3_rm`
`INFO:model_loader:copying comp_35_spatbn_3_riv`
`INFO:model_loader:copying comp_36_spatbn_1_rm`
`INFO:model_loader:copying comp_36_spatbn_1_riv`
`INFO:model_loader:copying comp_36_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_36_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_36_spatbn_2_rm`
`INFO:model_loader:copying comp_36_spatbn_2_riv`
`INFO:model_loader:copying comp_36_spatbn_3_rm`
`INFO:model_loader:copying comp_36_spatbn_3_riv`
`INFO:model_loader:copying comp_37_spatbn_1_rm`
`INFO:model_loader:copying comp_37_spatbn_1_riv`
`INFO:model_loader:copying comp_37_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_37_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_37_spatbn_2_rm`
`INFO:model_loader:copying comp_37_spatbn_2_riv`
`INFO:model_loader:copying comp_37_spatbn_3_rm`
`INFO:model_loader:copying comp_37_spatbn_3_riv`
`INFO:model_loader:copying comp_38_spatbn_1_rm`
`INFO:model_loader:copying comp_38_spatbn_1_riv`
`INFO:model_loader:copying comp_38_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_38_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_38_spatbn_2_rm`
`INFO:model_loader:copying comp_38_spatbn_2_riv`
`INFO:model_loader:copying comp_38_spatbn_3_rm`
`INFO:model_loader:copying comp_38_spatbn_3_riv`
`INFO:model_loader:copying comp_39_spatbn_1_rm`
`INFO:model_loader:copying comp_39_spatbn_1_riv`
`INFO:model_loader:copying comp_39_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_39_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_39_spatbn_2_rm`
`INFO:model_loader:copying comp_39_spatbn_2_riv`
`INFO:model_loader:copying comp_39_spatbn_3_rm`
`INFO:model_loader:copying comp_39_spatbn_3_riv`
`INFO:model_loader:copying comp_40_spatbn_1_rm`
`INFO:model_loader:copying comp_40_spatbn_1_riv`
`INFO:model_loader:copying comp_40_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_40_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_40_spatbn_2_rm`
`INFO:model_loader:copying comp_40_spatbn_2_riv`
`INFO:model_loader:copying comp_40_spatbn_3_rm`
`INFO:model_loader:copying comp_40_spatbn_3_riv`
`INFO:model_loader:copying comp_41_spatbn_1_rm`
`INFO:model_loader:copying comp_41_spatbn_1_riv`
`INFO:model_loader:copying comp_41_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_41_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_41_spatbn_2_rm`
`INFO:model_loader:copying comp_41_spatbn_2_riv`
`INFO:model_loader:copying comp_41_spatbn_3_rm`
`INFO:model_loader:copying comp_41_spatbn_3_riv`
`INFO:model_loader:copying comp_42_spatbn_1_rm`
`INFO:model_loader:copying comp_42_spatbn_1_riv`
`INFO:model_loader:copying comp_42_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_42_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_42_spatbn_2_rm`
`INFO:model_loader:copying comp_42_spatbn_2_riv`
`INFO:model_loader:copying comp_42_spatbn_3_rm`
`INFO:model_loader:copying comp_42_spatbn_3_riv`
`INFO:model_loader:copying comp_43_spatbn_1_rm`
`INFO:model_loader:copying comp_43_spatbn_1_riv`
`INFO:model_loader:copying comp_43_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_43_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_43_spatbn_2_rm`
`INFO:model_loader:copying comp_43_spatbn_2_riv`
`INFO:model_loader:copying comp_43_spatbn_3_rm`
`INFO:model_loader:copying comp_43_spatbn_3_riv`
`INFO:model_loader:copying comp_44_spatbn_1_rm`
`INFO:model_loader:copying comp_44_spatbn_1_riv`
`INFO:model_loader:copying comp_44_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_44_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_44_spatbn_2_rm`
`INFO:model_loader:copying comp_44_spatbn_2_riv`
`INFO:model_loader:copying comp_44_spatbn_3_rm`
`INFO:model_loader:copying comp_44_spatbn_3_riv`
`INFO:model_loader:copying comp_45_spatbn_1_rm`
`INFO:model_loader:copying comp_45_spatbn_1_riv`
`INFO:model_loader:copying comp_45_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_45_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_45_spatbn_2_rm`
`INFO:model_loader:copying comp_45_spatbn_2_riv`
`INFO:model_loader:copying comp_45_spatbn_3_rm`
`INFO:model_loader:copying comp_45_spatbn_3_riv`
`INFO:model_loader:copying comp_46_spatbn_1_rm`
`INFO:model_loader:copying comp_46_spatbn_1_riv`
`INFO:model_loader:copying comp_46_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_46_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_46_spatbn_2_rm`
`INFO:model_loader:copying comp_46_spatbn_2_riv`
`INFO:model_loader:copying comp_46_spatbn_3_rm`
`INFO:model_loader:copying comp_46_spatbn_3_riv`
`INFO:model_loader:copying comp_47_spatbn_1_rm`
`INFO:model_loader:copying comp_47_spatbn_1_riv`
`INFO:model_loader:copying comp_47_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_47_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_47_spatbn_2_rm`
`INFO:model_loader:copying comp_47_spatbn_2_riv`
`INFO:model_loader:copying comp_47_spatbn_3_rm`
`INFO:model_loader:copying comp_47_spatbn_3_riv`
`INFO:model_loader:copying shortcut_projection_47_spatbn_rm`
`INFO:model_loader:copying shortcut_projection_47_spatbn_riv`
`INFO:model_loader:copying comp_48_spatbn_1_rm`
`INFO:model_loader:copying comp_48_spatbn_1_riv`
`INFO:model_loader:copying comp_48_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_48_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_48_spatbn_2_rm`
`INFO:model_loader:copying comp_48_spatbn_2_riv`
`INFO:model_loader:copying comp_48_spatbn_3_rm`
`INFO:model_loader:copying comp_48_spatbn_3_riv`
`INFO:model_loader:copying comp_49_spatbn_1_rm`
`INFO:model_loader:copying comp_49_spatbn_1_riv`
`INFO:model_loader:copying comp_49_spatbn_2_middle_rm`
`INFO:model_loader:copying comp_49_spatbn_2_middle_riv`
`INFO:model_loader:copying comp_49_spatbn_2_rm`
`INFO:model_loader:copying comp_49_spatbn_2_riv`
`INFO:model_loader:copying comp_49_spatbn_3_rm`
`INFO:model_loader:copying comp_49_spatbn_3_riv`
`INFO:data_parallel_model:Creating checkpoint synchronization net`
`INFO:data_parallel_model:Run checkpoint net`
`[E video_decoder.h:269] Error opening video file`
`@^@^HI^@^@^H ^@^@^K4^@^@^Gå^@^@^H¶^@^@^K[^@^@^Hw^@^@^G«^@^@`
`\^@^@^GË^@^@`
`J^@^@^Fã^@^@`
`...  <82>^@^@^E.^@^@^E>^@^@^F^H^@^@^C,^@^@^C<8f>^@^@^Eê^@^@^Ee^@^@^DG^@^@^E¨^@^@^GT^@^@^E<84>^@^@^Gç^@^@+µ^@^@^EG^@^@^Hæ^@^@^FÇ^@^@`
`^W^@^@^G^W^@^@^H^Z^@^@`
`¹^@^@^G^C^`
`[E video_decoder.cc:151] Insufficient data to determine video format`
`free(): invalid pointer`
"
Windows 10 environment?,facebookresearch/R2Plus1D,2020-02-27 07:45:13,0,,106,571884020,"Can I use this project in Windows 10 environment?
If not, can you give me a guide on how to change the configuration?"
About video list for feature extraction,facebookresearch/R2Plus1D,2020-02-18 07:57:58,1,,104,566712503,"I have some questions about video list file for feature extraction.
I saw your answer of previous issue where https://github.com/facebookresearch/VMZ/issues/11#issue-331330979, but I'm still not sure about my understanding.

There are four categories in video list : **org_video, label, start_frm, video_id**
And I want to use extract_features.py for untrimmed video datasets.
Here is my questions.

1) Does ""Clip"" you used correspond to ""snippet"" meaning several successive frames?
2) As I understand, I should use different **video_id** for different clips in one video for feature extraction of untrimmed video. Right?
3) When using video_id, Is the **start_frm** meaning the target frame of **label**?
And that's why you use 1 frame clips in tutorial.

Thank you."
Completely incapable of getting the installation requirements working,facebookresearch/R2Plus1D,2019-11-29 23:26:58,2,,101,530507579,"After following the [steps for Ubuntu 16](https://github.com/facebookresearch/VMZ/blob/master/tutorials/Installation_guide.md), verbatim, I consistently get this: 

```
from caffe2.python import core
WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.
CRITICAL:root:Cannot load caffe2.python. Error: No module named caffe2_pybind11_state
```
I would like some input. 

**Edit**: Basically, ignore these instructions if you have trouble. Your source should be: 

https://caffe2.ai/docs/getting-started.html?platform=ubuntu&configuration=compile



"
How does jitter_scales parameter actually work?,facebookresearch/R2Plus1D,2019-11-24 19:03:09,0,,100,527741165,"I suppose this parameter should be set in order to do spatial jittering, which is stated in the original paper. However, through a detailed check of the provided code, I feel confused about how it works. Sorry if it is a stupid question but please help me understand it correctly.

In train_net.py, jitter_scales is directly passed into AddVideoInput():
https://github.com/facebookresearch/VMZ/blob/6c925c47b7d6545b64094a083f111258b37cbeca/tools/train_net.py#L329

Then, in AddVideoInput, all the parameters are passed into model.net.VideoInput() using **kwargs:
https://github.com/facebookresearch/VMZ/blob/6c925c47b7d6545b64094a083f111258b37cbeca/lib/utils/model_helper.py#L133

The model should be defined by CNNModelHelper and VideoInput object is given here:
https://github.com/pytorch/pytorch/blob/3990e9d1ca23e39bfa65fe1d907ddb9f9dbf0919/caffe2/python/cnn.py#L66

where we can see it is actually defined in brew.py:
https://github.com/pytorch/pytorch/blob/cca247635c6edb323176eeac7a18d3e9ab71c558/caffe2/python/brew.py#L63

and we will see video_input method is provided in tools.py:
https://github.com/pytorch/pytorch/blob/3990e9d1ca23e39bfa65fe1d907ddb9f9dbf0919/caffe2/python/helpers/tools.py#L31

It gets back, like a dead circle...So at which piece of code is jitter_scales really used?

If we check the C++ API of Caffe2, video_input_op indeed exists but there's no parameter called jitter_scales:
https://caffe2.ai/doxygen-c/html/video__input__op_8h_source.html
"
Could you update the clip level accuracy on Kinetics of those models?,facebookresearch/R2Plus1D,2019-10-08 13:03:28,2,,96,504031204,thanks!
finetune ir-csn with my own dataset,facebookresearch/R2Plus1D,2019-09-29 08:50:48,5,,91,499882037,"Ignoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.
Ignoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.
Ignoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.
[E init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.
[E init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.
[E init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.
INFO:train_net:Namespace(base_learning_rate=0.0001, batch_size=8, bottleneck_multiplier=1.0, channel_multiplier=1.0, clip_length_of=8, clip_length_rgb=32, conv1_temporal_kernel=3, conv1_temporal_stride=1, crop_size=224, cudnn_workspace_limit_mb=64, db_type='pickle', display_iter=10, do_flow_aggregation=0, epoch_size=100000, file_store_path='/tmp', flow_data_type=0, frame_gap_of=2, gamma=0.1, get_video_id=0, gpus='0', input_type=0, is_checkpoint=0, jitter_scales='128,160', load_model_path='/usr/VMZ-master-1/irCSN_152_ft_kinetics_from_ig65m_f126851907.pkl', model_depth=152, model_name='ir-csn', multi_label=0, num_channels=3, num_decode_threads=4, num_epochs=8, num_gpus=1, num_labels=5, pred_layer_name=None, profiling=0, sampling_rate_of=2, sampling_rate_rgb=2, save_model_name='simple_c3d', scale_h=256, scale_w=342, step_epoch=2, test_data='/usr/VMZ-master/datasetv/pig_test', train_data='/usr/VMZ-master/datasetv/pig_train', use_cudnn=1, use_dropout=0, use_local_file=1, use_pool1=1, video_res_type=1, weight_decay=0.005)
INFO:model_builder:Validated: ir-csn with 152 layers
INFO:model_builder:with input 32x224x224
INFO:train_net:Running on GPUs: [0]
INFO:train_net:Using epoch size: 100000
WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.
INFO:train_net:Training set has 723 examples
INFO:data_parallel_model:Parallelizing model for devices: [0]
INFO:data_parallel_model:Create input and model training operators
INFO:data_parallel_model:Model for GPU : 0
INFO:model_helper:outputing rgb data
INFO:model_builder:creating ir-csn, depth=152...
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 256
INFO:video_model:in: 256 out: 64
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 256
INFO:video_model:in: 256 out: 64
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 256
INFO:video_model:in: 256 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 512
INFO:video_model:in: 512 out: 512
INFO:video_model:in: 512 out: 2048
INFO:video_model:in: 2048 out: 512
INFO:video_model:in: 512 out: 512
INFO:video_model:in: 512 out: 2048
INFO:video_model:in: 2048 out: 512
INFO:video_model:in: 512 out: 512
INFO:video_model:in: 512 out: 2048
INFO:data_parallel_model:Adding gradient operators
INFO:data_parallel_model:Add gradient all-reduces for SyncSGD
INFO:data_parallel_model:Post-iteration operators for updating params
INFO:data_parallel_model:Add initial parameter sync
WARNING:data_parallel_model:------- DEPRECATED API, please use data_parallel_model.OptimizeGradientMemory() ----- 
WARNING:memonger:NOTE: Executing memonger to optimize gradient memory
INFO:memonger:Memonger memory optimization took 0.0483040809631 secs
INFO:train_net:----- Create test net ----
WARNING:root:[====DEPRECATE WARNING====]: you are creating an object from CNNModelHelper class which will be deprecated soon. Please use ModelHelper object with brew module. For more information, please refer to caffe2.ai and python/brew.py, python/brew_test.py for more information.
INFO:train_net:Testing set has 278 examples
INFO:data_parallel_model:Parallelizing model for devices: [0]
INFO:data_parallel_model:Create input and model training operators
WARNING:data_parallel_model:
WARNING:data_parallel_model:############# WARNING #############
WARNING:data_parallel_model:Model ir-csn_test/<caffe2.python.cnn.CNNModelHelper object at 0x7fec6c3cd350> is used for testing/validation but
WARNING:data_parallel_model:has init_params=True!
WARNING:data_parallel_model:This can conflict with model training.
WARNING:data_parallel_model:Please ensure model = ModelHelper(init_params=False)
WARNING:data_parallel_model:####################################
WARNING:data_parallel_model:
INFO:data_parallel_model:Model for GPU : 0
INFO:model_helper:outputing rgb data
INFO:model_builder:creating ir-csn, depth=152...
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 256
INFO:video_model:in: 256 out: 64
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 256
INFO:video_model:in: 256 out: 64
INFO:video_model:in: 64 out: 64
INFO:video_model:in: 64 out: 256
INFO:video_model:in: 256 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 128
INFO:video_model:in: 128 out: 128
INFO:video_model:in: 128 out: 512
INFO:video_model:in: 512 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 256
INFO:video_model:in: 256 out: 256
INFO:video_model:in: 256 out: 1024
INFO:video_model:in: 1024 out: 512
INFO:video_model:in: 512 out: 512
INFO:video_model:in: 512 out: 2048
INFO:video_model:in: 2048 out: 512
INFO:video_model:in: 512 out: 512
INFO:video_model:in: 512 out: 2048
INFO:video_model:in: 2048 out: 512
INFO:video_model:in: 512 out: 512
INFO:video_model:in: 512 out: 2048
INFO:data_parallel_model:Parameter update function not defined --> only forward
WARNING:caffe2.python.workspace:Original python traceback for operator `9` in network `ir-csn_test` in exception above (most recent call last):
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/train_net.py"", line 586, in <module>
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/train_net.py"", line 581, in main
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/train_net.py"", line 401, in Train
WARNING:caffe2.python.workspace:  File ""/root/pytorch/build/caffe2/python/data_parallel_model.py"", line 39, in Parallelize_GPU
WARNING:caffe2.python.workspace:  File ""/root/pytorch/build/caffe2/python/data_parallel_model.py"", line 237, in Parallelize
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/train_net.py"", line 275, in create_model_ops
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/models/model_builder.py"", line 129, in build_model
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/models/r3d_model.py"", line 179, in create_model
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/models/r3d_model.py"", line 311, in create_r3d
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/models/builder/video_model.py"", line 297, in add_bottleneck
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/models/builder/video_model.py"", line 145, in add_conv
WARNING:caffe2.python.workspace:  File ""/usr/VMZ-master-1/tools/models/builder/video_model.py"", line 211, in add_channelwise_conv
WARNING:caffe2.python.workspace:  File ""/root/pytorch/build/caffe2/python/brew.py"", line 108, in scope_wrapper
WARNING:caffe2.python.workspace:  File ""/root/pytorch/build/caffe2/python/helpers/conv.py"", line 164, in conv_nd
WARNING:caffe2.python.workspace:  File ""/root/pytorch/build/caffe2/python/helpers/conv.py"", line 123, in _ConvBase
Traceback (most recent call last):
  File ""/usr/VMZ-master-1/tools/train_net.py"", line 586, in <module>
    main()
  File ""/usr/VMZ-master-1/tools/train_net.py"", line 581, in main
    Train(args)
  File ""/usr/VMZ-master-1/tools/train_net.py"", line 404, in Train
    workspace.CreateNet(test_model.net)
  File ""/root/pytorch/build/caffe2/python/workspace.py"", line 181, in CreateNet
    StringifyProto(net), overwrite,
  File ""/root/pytorch/build/caffe2/python/workspace.py"", line 215, in CallWithExceptionIntercept
    return func(*args, **kwargs)
RuntimeError: [enforce fail at context.h:48] option.device_type() == PROTO_CPU. 1 vs 0
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x78 (0x7fed19c32178 in /usr/local/lib/libc10.so)
frame #1: <unknown function> + 0x2686d70 (0x7fecdcf03d70 in /usr/local/lib/libtorch.so)
frame #2: <unknown function> + 0x2723fec (0x7fecdcfa0fec in /usr/local/lib/libtorch.so)
frame #3: <unknown function> + 0x3aff5ee (0x7fecde37c5ee in /usr/local/lib/libtorch.so)
frame #4: std::_Function_handler<std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (caffe2::OperatorDef const&, caffe2::Workspace*), std::unique_ptr<caffe2::OperatorBase, std::default_delete<caffe2::OperatorBase> > (*)(caffe2::OperatorDef const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, caffe2::OperatorDef const&, caffe2::Workspace*&&) + 0x23 (0x7fed1a4b5433 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #5: <unknown function> + 0x236c25c (0x7fecdcbe925c in /usr/local/lib/libtorch.so)
frame #6: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0x328 (0x7fecdcbea528 in /usr/local/lib/libtorch.so)
frame #7: caffe2::dag_utils::prepareOperatorNodes(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x2ad (0x7fecdcbda06d in /usr/local/lib/libtorch.so)
frame #8: caffe2::AsyncNetBase::AsyncNetBase(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x24d (0x7fecdcbb670d in /usr/local/lib/libtorch.so)
frame #9: caffe2::AsyncSchedulingNet::AsyncSchedulingNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x9 (0x7fecdcbbb5b9 in /usr/local/lib/libtorch.so)
frame #10: <unknown function> + 0x23410ae (0x7fecdcbbe0ae in /usr/local/lib/libtorch.so)
frame #11: std::_Function_handler<std::unique_ptr<caffe2::NetBase, std::default_delete<caffe2::NetBase> > (std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*), std::unique_ptr<caffe2::NetBase, std::default_delete<caffe2::NetBase> > (*)(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*&&) + 0x23 (0x7fecdcbbdf83 in /usr/local/lib/libtorch.so)
frame #12: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x4a5 (0x7fecdcbb0495 in /usr/local/lib/libtorch.so)
frame #13: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0x103 (0x7fecdcc2fe23 in /usr/local/lib/libtorch.so)
frame #14: caffe2::Workspace::CreateNet(caffe2::NetDef const&, bool) + 0x91 (0x7fecdcc30d61 in /usr/local/lib/libtorch.so)
frame #15: <unknown function> + 0x57906 (0x7fed1a4ad906 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #16: <unknown function> + 0x57bd2 (0x7fed1a4adbd2 in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
frame #17: <unknown function> + 0x99e3d (0x7fed1a4efe3d in /root/pytorch/build/caffe2/python/caffe2_pybind11_state_gpu.so)
<omitting python frames>
frame #33: __libc_start_main + 0xe7 (0x7fed1e897b97 in /lib/x86_64-linux-gnu/libc.so.6)
"
Can we use the features extracted by the R(2+1)D-34 caffe model as input for a classifier built in Keras?,facebookresearch/R2Plus1D,2019-09-19 07:54:19,8,,86,495633885,"For one of my projects, I would like to build a simple feed forward neural network in Keras. I was wondering if it is possible to use the features extracted by the given script written in caffe2 as input for a keras model. "
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte,facebookresearch/R2Plus1D,2019-01-05 08:55:15,2,,62,396147732,"I'm using python3.6 and run the extract_features.py, but with error of UnicodeDecodeError, I changed the code 'import cPickle as pickle' to 'import pickle' in lib/utils/model_loader.py as there is no cPickle in python3, does this make de mistake?

the error info:
File ""tools/extract_features.py"", line 328, in <module>
    main()
  File ""tools/extract_features.py"", line 323, in main
    ExtractFeatures(args)
  File ""tools/extract_features.py"", line 153, in ExtractFeatures
    root_gpu_id=gpus[0]
  File ""/home/ubuntu/demos/VMZ/lib/utils/model_loader.py"", line 79, in LoadModelFromPickleFile
    blobs = pickle.load(fopen)
  File ""/home/ubuntu/miniconda3/envs/vmz/lib/python3.6/codecs.py"", line 321, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
the test accuracy of the same test dataset is different when using the same model,facebookresearch/R2Plus1D,2018-10-22 02:49:35,3,,53,372380776,"Hi,
@dutran , thank you for your great work.
I finetuned the r2plus1d model on my own dataset using train_net.py, then, I got the best test accuracy 0.72 and the corresponding model r2plus1d_3.mdl. However, when I use the same test dataset and the r2plus1d_3.mdl to run test_net.py, the test accuracy is low. It is about 0.2. And I also tried to extract the features using extract_features.py and then got the test accuracy using dense_prediciton_aggregation.py. The test accuracy is low too, it is at most 0.12.
It makes me feel confused. Why is the test accuracy so different?
I know the value of the decode_type may influence the test accuracy, but I wonder if there are any other reasons that could affect the test accuracy? Could you give me some advice? Thank you."
Training .mdl to Predictor,facebookresearch/R2Plus1D,2018-06-27 03:47:28,0,,18,336067442,"Hey team,

Thanks for all your awesome work in Spatio-Temporal CNNs! I had a couple quick questions about going from a trained model (.mdl format) to a standalone predictor.

I was able to successfully build the environment and train the R(2+1)D model on my own dataset but now that I've got my trained .mdl file I've been struggling to find documentation on how to go from the trained/training model that takes in LMDB's to a 'predictor' model that can take in plain np array inputs.

I've currently got my .mdl loaded into an init_net and predict_net as suggested by much of the documentation I was able to find:

init_net proto:
```
input: ""!!PREDICTOR_DBREADER""
output: ""gpu_0/conv1_middle_w""
output: ""gpu_0/conv1_middle_spatbn_relu_s""
output: ""gpu_0/conv1_middle_spatbn_relu_b""
output: ""gpu_0/conv1_w""
output: ""gpu_0/conv1_spatbn_relu_s""
output: ""gpu_0/conv1_spatbn_relu_b""
output: ""gpu_0/comp_0_conv_1_middle_w""
output: ""gpu_0/comp_0_spatbn_1_middle_s""
...
name: """"
type: ""Load""
```
first two layers in predict_net proto:
```
input: ""r2plus1d_train_init/CreateDB""
output: ""gpu_0/data""
output: ""gpu_0/label""
name: ""data""
type: ""VideoInput""
arg {
... bunch of VideoInput args ...
}
device_option {
  device_type: 1
  cuda_gpu_id: 0
}
```
```
input: ""gpu_0/data""
output: ""gpu_0/data""
name: """"
type: ""StopGradient""
device_option {
  device_type: 1
  cuda_gpu_id: 0
}
```
```
{convolutional layers, etc...}
```

My questions are:
a) how would I go about modifying the net(s) to allow for individual predictions from raw np arrays? Ex:
```
data = np.random.rand(N, C, H, W) // aka np arrays of video frames
workspace.FeedBlob(""gpu0_data"", data)
```

and b) how would I switch from GPU to CPU execution? Is there something like workspace.RunAllOnCPU() for example?

Thanks!"
BFT for MobileNetV3,keivanalizadeh/ButterflyTransform,2021-10-05 01:38:12,0,,2,1015792996,"Hi, thanks for your wonderful work. I use BFT to replace pointwise convolution in MobileNetV3 for object detection. I train the model from scratch. But the AP goes down a lot compared with MobileNetV3"
About dataset and dataroot,nnaisense/conditional-style-transfer,2021-05-14 09:42:16,0,,4,891777683,"Hello, I want to use your code to train my own dataset, but I am not sure about the meaning of trainA, trainB, valA, valB, etc in the dataroot, and where should my dataset be placed?"
About the test results,nnaisense/conditional-style-transfer,2020-12-21 09:51:11,0,,3,772025174,"I have run your code using the comand `python test.py --checkpoints_dir=./samples/models --name GanAuxPretrained --model gan_aux --netG=resnet_residual --netD=disc_noisy
--gpu_ids=0 --num_style_samples=1 --loadSize=512 --fineSize=512 --knn=5 --peer_reg=bidir --epoch=200 --content_folder=./samples/data/content_imgs
--style_folder=./samples/data/style_imgs --output_folder=output`.
Then I got the results with strange color.I just have no idea why my results is so bizarre. 
![image](https://user-images.githubusercontent.com/53161080/102763509-ba145100-43b4-11eb-8f5e-bdcf84bd6f4a.png)
"
RuntimeError,nnaisense/conditional-style-transfer,2020-11-13 05:54:21,2,,2,742172030,"When I was training the model, I had this problem:
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [256, 256, 3, 3]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
How to solve this problem?
Look forward to your reply."
Where is the code for K-nearest neighbor?,kevaldoshi17/NVIDIA_AICITY,2021-08-24 08:43:16,0,,13,977857647,"I read your thesis ""Fast Unsupervised Anomaly Detection in Traffic Videos"".
I was curious about parameters(eg. k, l) for K-nearest neighbors distance but I can't find that code in your repo.
Plz let me know."
How to generate the part 2.json on Segmentation maps ?,kevaldoshi17/NVIDIA_AICITY,2021-04-05 13:30:28,0,,6,850368359,"@kevaldoshi17 
**2.Run pretrained Yolo v3 model on the original_images folder and save it as part1.json. To reduce complexity, we divided the task into two parts and saved it as part1.json and part2.json.**

This step specifies how to generate the part1.json but not the part2.json"
How to generate Result.json?,kevaldoshi17/NVIDIA_AICITY,2020-07-16 15:27:40,8,,3,658305092,"In the background modelling part, after running yolov3, we are suppsed to save it as result.json. Usually yolov3 outputs results as .txt file and it does not look like the result.json provided in the repo. Is there anything extra need to be done to get result.json file?"
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED,lordfjw/OptimalGradCheckpointing,2022-08-17 21:34:45,0,,5,1342313738,"```
...
File ""/home/Foo/miniforge3/envs/pytor/lib/python3.6/site-packages/torch/nn/functional.py"", line 1923, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
```

Got this error when trying to run the following command the README.md:

```
python benchmark.py --arch resnet101 --device cuda:0
```"
Can't use run_segment with apex.amp,lordfjw/OptimalGradCheckpointing,2021-11-11 07:57:42,4,,4,1050677335,"I use code like this
```
run_segment = optimal_grad_checkpointing(model, inp)
run_segment, optimizer = apex.amp.initialize(run_segment, optimizer, opt_level=""02"", verbosity=0)
...
output = run_segment(images)
```
and get the error
```
output = run_segment(images)
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py"", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 911, in forward
    return graph_forward(x, **self.info_dict)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 838, in graph_forward
    output = checkpoint(segment_checkpoint_forward(op), input)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/checkpoint.py"", line 155, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File ""/opt/conda/lib/python3.6/site-packages/torch/utils/checkpoint.py"", line 74, in forward
    outputs = run_function(*args)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 807, in custom_forward
    outputs = segment(*inputs)
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 911, in forward
    return graph_forward(x, **self.info_dict)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 840, in graph_forward
    output = op(input)
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 349, in forward
    return self._conv_forward(input, self.weight)
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py"", line 346, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.cuda.HalfTensor) and weight type (torch.cuda.FloatTensor) should be the same
```
It would be effective to combine Optimal Gradient Checkpointing with apex.amp or torch.cuda.amp"
Problem with PyTorch Version 1.10,lordfjw/OptimalGradCheckpointing,2021-11-10 11:02:36,2,,3,1049683973,"Hi, I am trying to reproduce the results. It works correctly with PyTorch 1.5, but with PyTorch 1.10 - `Parsing Computation Graph with torch.jit failed` and with manual parse_graph function it takes up twice as much GPU memory.
Output with PyTorch Version 1.10.0a0+0aef44c (nvcr.io/nvidia/pytorch:21.10-py3 docker container):
```
Processing resnet101, Input size (32, 3, 224, 224)--------------------
Parsing Computation Graph
Parsing Computation Graph with torch.jit failed, revert to manual parse_graph function
Building Division Tree
Getting Max Terms
Solving Optimal for Each Max Term
100%|████████████████████████████████████████████████████| 330/330 [00:02<00:00, 138.06it/s]
Solving optimal gradient checkpointing takes 2.7020 s
/opt/conda/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(""None of the inputs have requires_grad=True. Gradients will be None"")
Parsed graph forward check passed
Run graph forward check passed
Parsed graph backward check passed
Run graph backward check passed
/opt/conda/lib/python3.8/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
100%|█████████████████████████████████████████████████████| 100/100 [00:24<00:00,  4.12it/s]
100%|█████████████████████████████████████████████████████| 100/100 [00:30<00:00,  3.25it/s]
Average Iteration Time: Checkpointing 0.3082 s, Regular 0.2427 s, overhead 26.99%
Average Peak Memory: Checkpointing 5251.0508 MB, Regular 8157.9248 MB, Memory Cut off 35.63%
Average Intermediate Tensors: Checkpointing 1023.0098 MB, Regular 3929.8838 MB, Memory Cut off 73.97%
```

Output after commenting the ""try"" at https://github.com/lordfjw/OptimalGradCheckpointing/blob/main/benchmark.py#L167
```
Processing resnet101, Input size (32, 3, 224, 224)--------------------
Parsing Computation Graph
Traceback (most recent call last):
  File ""benchmark.py"", line 212, in <module>
    main(arch, device)
  File ""benchmark.py"", line 168, in main
    G, source, target = parse_computation_graph(net, inputs)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 34, in parse_computation_graph
    computation_graph, input_node_ids, output_node_ids = parse_raw_computation_graph_from_jit(module, inputs)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 55, in parse_raw_computation_graph_from_jit
    computation_graph, _, input_node_ids, output_node_ids = build_computation_graph_recursively(module, inputs, inputs_nodes_ids=None, outputs_nodes_ids=None, cur_node_idx=None)
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 412, in build_computation_graph_recursively
    internal_node_dicts = [parse_node_str(n) for n in graph_nodes]
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 412, in <listcomp>
    internal_node_dicts = [parse_node_str(n) for n in graph_nodes]
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 162, in parse_node_str
    shape = [int(s) for s in shape_str.split(', ')]
  File ""/working_dir/OptimalGradCheckpointing/graph.py"", line 162, in <listcomp>
    shape = [int(s) for s in shape_str.split(', ')]
ValueError: invalid literal for int() with base 10: 'strides=[2048'
```

Output with PyTorch Version 1.5.0a0+8f84ded (nvcr.io/nvidia/pytorch:20.03-py3 docker container)
```
Processing resnet101, Input size (32, 3, 224, 224)--------------------
Parsing Computation Graph
/opt/conda/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn(""The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad ""
Building Division Tree
Getting Max Terms
Solving Optimal for Each Max Term
100%|██████████████████████████████████████████████████| 350/350 [00:03<00:00, 95.77it/s]
Solving optimal gradient checkpointing takes 4.1945 s
/opt/conda/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(""None of the inputs have requires_grad=True. Gradients will be None"")
Parsed graph forward check passed
Run graph forward check passed
Parsed graph backward check passed
Run graph backward check passed
100%|█████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.02it/s]
100%|█████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.80it/s]
Average Iteration Time: Checkpointing 0.2623 s, Regular 0.1983 s, overhead 32.28%
Average Peak Memory: Checkpointing 1524.6592 MB, Regular 4306.1680 MB, Memory Cut off 64.59%
Average Intermediate Tensors: Checkpointing 1145.3750 MB, Regular 3926.8838 MB, Memory Cut off 70.83%
```
"
How to auto parsing?,lordfjw/OptimalGradCheckpointing,2021-11-02 09:59:29,1,,2,1042101838,"I run the benchmark.pu with the following warnings.
python benchmark.py --arch resnet18 --device cuda:0
Parsing Computation Graph with torch.jit failed, revert to manual parse_graph function"
Thank you & two minor issues,lordfjw/OptimalGradCheckpointing,2021-08-11 13:53:08,1,,1,966763402,"Hi,

thank you so much for providing this code!
Using the automatic computation graph parser, I was able to use the optimal gradient checkpoints during model training without writing much additional code. Now I can train with almost 2x larger batches, which is very helpful for my application!

I just had a few minor issues when running the code and I want to quickly mention them here in case anyone else experiences the same:

- In graph.py line 429: The assertion `len(input_node_names) == len(inputs_nodes_ids)` fails because the list `inputs_nodes_ids` contains `None`. It works after removing all `None` from the list (`inputs_nodes_ids = [i for i in inputs_nodes_ids if i is not None]`). However, I'm not sure if doing this could have any adverse effects??
- In graph.py line 159: Parsing the shape string fails because the `node_type` looks sth like `""Float(2, 1024, strides=[1024, 1], requires_grad=0, device=cuda:0)""` (I used pytorch v1.9).
Quick fix: 
```
if 'strides' in node_type: 
    shape_str = node_type.split('(')[-1].split(', strides')[0]
else:
    shape_str = node_type.split('(')[-1].split(')')[0]
```
- I also had to add a few lines of code to the `get_python_module_from_node_op` function (in graph.py) in order to handle `'prim:ListUnpack', 'aten::constant_pad_nd', 'aten::squeeze'`, but this was straightforward based on the examples in your code :)

Thank you again for making my life easier!"
Ask for Code,aupendu/sr-uncertainty,2022-05-07 08:43:44,0,,4,1228570939,Thank you for your work. I am very curious about the implementation. When will the code be released? Thank you.
code release,aupendu/sr-uncertainty,2022-04-15 02:54:45,0,,3,1205199033,Thank you for your work. I am very curious about the implementation. When will the code be released? Thank you. 
code release,aupendu/sr-uncertainty,2021-10-29 08:46:22,0,,2,1039326849,Many thanks for your work. I am sorry to ask when the code will be released. Many thanks.
code release,aupendu/sr-uncertainty,2021-07-04 10:48:08,0,,1,936429151,Many thanks for your work. I am sorry to ask when the code will be released. Many thanks.
Enquiry,AemikaChow/DATASOURCE,2021-11-28 03:53:48,4,,2,1065180960,"Hello, thanks for your amazing review.
I wonder if you still conduct some task or project related fashion AI?
Best"
Question about the results,facebookresearch/DomainBed,2022-11-03 06:11:05,0,,116,1434162692,"Hi, thanks for your great work!

I'm wondering that are those results released on your paper coming from the script [sweep.py](https://github.com/facebookresearch/DomainBed/blob/main/domainbed/scripts/sweep.py)?

Another question is that, for example, when we train a model for PACS using DomainBed with env0 as the test_env, should we use the highest result in env0_in_acc or just the last one or anything else?

Thanks."
"Error ""Object of type int64 is not JSON serializable""",facebookresearch/DomainBed,2022-11-02 06:58:44,0,,115,1432604163,"When I run train.py with the MLDG algorithm, an error saying that ""Object of type int64 is not JSON serializable"" occurs. At line 
     File ""/home/zx/DomainBed/domainbed/scripts/train.py"", line 257, in <module>
         f.write(json.dumps(results, sort_keys=True) + ""\n"")

After debugging I find it comes from the ""_hparam('n_meta_test', 2, lambda r:  r.choice([1, 2]))"" line in the hparams_registry file. It is because that r.choice returns a numpy.int64 type data, which may lead to an error sometimes. I think we should change it to ""_hparam('n_meta_test', 2, lambda r:  int(r.choice([1, 2])))"" to transfer the selected number to the int type"
[Question] Pre-trained models for quick evaluation,facebookresearch/DomainBed,2022-11-01 13:05:06,0,,114,1431394992,Are there any pre-trained models to be found for quick evaluation and comparison with other algos?
Issue in using InfiniteDataLoader with Lightning,facebookresearch/DomainBed,2022-10-31 00:11:50,0,,113,1429070641,"I'm trying to use `InfiniteDataLoader` in `lib.fast_data_loader` with the Pytorch Lightning Module
```
train_loaders = [InfiniteDataLoader(
    dataset=env,
    weights=env_weights,
    batch_size=batch_size,
    num_workers=dataset.N_WORKERS)
    for i, (env, env_weights) in enumerate(in_splits)
    if i not in test_envs + val_envs]
    
trainer.fit(algorithm, train_loaders)
```

The training loop goes on forever in this case. If I add a `__len__` function similar to `FastDataLoader` in `InfiniteDataLoader` class, the problem is solved. However, I wanted to ask if this would cause issues with the expected behaviour of the `InfiniteDataLoader`."
ColoredMNIST with IRM Implementation is different from the original IRM papers,facebookresearch/DomainBed,2022-10-19 02:31:54,1,,112,1414162635,"The ColoredMNIST with IRM implementation uses the following MLP network in original papers, which is very different from the Conv network implementation in DomainBed, and the results are also quite different (acc=66.9 on test_env[0.9] in original IRM paper; acc=10.1 on test_env[0.9] in this paper). Could you please let me know if this different implementation is reasonable for comparison?

```python
class MLP(nn.Module):
  def __init__(self):
    super(MLP, self).__init__()
    if flags.grayscale_model:
      lin1 = nn.Linear(14 * 14, flags.hidden_dim)
    else:
      lin1 = nn.Linear(2 * 14 * 14, flags.hidden_dim)
    lin2 = nn.Linear(flags.hidden_dim, flags.hidden_dim)
    lin3 = nn.Linear(flags.hidden_dim, 1)
    for lin in [lin1, lin2, lin3]:
      nn.init.xavier_uniform_(lin.weight)
      nn.init.zeros_(lin.bias)
    self._main = nn.Sequential(lin1, nn.ReLU(True), lin2, nn.ReLU(True), lin3)
  def forward(self, input):
    if flags.grayscale_model:
      out = input.view(input.shape[0], 2, 14 * 14).sum(dim=1)
    else:
      out = input.view(input.shape[0], 2 * 14 * 14)
    out = self._main(out)
    return out
```

The `binary_cross_entropy_with_logits` is used in the original IRM implementation; however, the general conv network is used in this repository. Please let me know if there is a reason for this. Under this implementation, it seems the IRM may not be suitable; thus, ColoredMNIST[0.9]'s benchmark score is 10.1 as opposed to 66.9 in the IRM paper."
Enquiry about hyper-parameter tuning using sweep,facebookresearch/DomainBed,2022-09-11 07:38:00,1,,111,1368861344,"Hi authors, 

Thank you for providing the community with this fabulous test bed. I found this repo extremely helpful to my research about domain generalization. When reading the code in `sweep.py` and `train.py`, there is a line that I am not very sure about its rationale.

When we tune hyper parameters by calling `sweep.py`, we should keep the generated parameter the same across different trials. However, current seed of random hyper parameter generation in `train.py` is set to be `misc.seed_hash(args.hparams_seed, args.trial_seed)` (line79), which means that the set of hyper parameters will only be given one chance in the run to show its performance during sweep. I am wondering if a seed `misc.seed_hash(args.hparams_seed)` will be enough when we call `train.py` via `sweep.py`, as we should test the randomly generated set of hyper parameters in the same number of trials as we do for the default hyper parameters.

I will really appreciate it if you could enlighten me on my above question about the hyper-parameter tuning in `sweep.py`.

Thank you!"
About model test,facebookresearch/DomainBed,2022-08-23 14:17:18,1,,110,1348034981,"Hi Authors,

What command should be run to get the experimental results of the test phase using the trained model？"
About error bars reported in paper,facebookresearch/DomainBed,2022-07-28 08:39:30,0,,103,1320592235,"Hi. Thanks for the nice work!
I am truly motivated by the work and trying to use it as baseline for my work.
While analyzing the code and the paper, I have a question about the standard deviation reported in the paper. 
According to the paper, std is calculated by running 3 experiments with entirely different random choices.
Can I follow the way you did it by changing hparams_seed, seed, trial_seed in your code?
Thank you!"
Skorch compatibility,easeml/datascope,2022-08-07 14:24:34,1,,7,1331031855,"Hi, as far as I understand, Datascope is compatible with any scikit-learn pipeline. I'm using PyTorch and skorch (library that wraps PyTorch) to make my classifier scikit-learn compatible.

I'm currently getting the following error when trying to compute the score:
```
ValueError                                Traceback (most recent call last)
[<ipython-input-49-2e03ddd68d36>](https://localhost:8080/#) in <module>()
----> 1 importances.score(test_data, test_labels)

3 frames
[/usr/local/lib/python3.7/dist-packages/datascope-0.0.3-py3.7-linux-x86_64.egg/datascope/importance/importance.py](https://localhost:8080/#) in score(self, X, y, **kwargs)
     38         if isinstance(y, DataFrame):
     39             y = y.values
---> 40         return self._score(X, y, **kwargs)

[/usr/local/lib/python3.7/dist-packages/datascope-0.0.3-py3.7-linux-x86_64.egg/datascope/importance/shapley.py](https://localhost:8080/#) in _score(self, X, y, **kwargs)
    285         units = np.delete(units, np.where(units == -1))
    286         world = kwargs.get(""world"", np.zeros_like(units, dtype=int))
--> 287         return self._shapley(self.X, self.y, X, y, self.provenance, units, world)
    288 
    289     def _shapley(

[/usr/local/lib/python3.7/dist-packages/datascope-0.0.3-py3.7-linux-x86_64.egg/datascope/importance/shapley.py](https://localhost:8080/#) in _shapley(self, X, y, X_test, y_test, provenance, units, world)

    314             )
    315         elif self.method == ImportanceMethod.NEIGHBOR:
--> 316             return self._shapley_neighbor(X, y, X_test, y_test, provenance, units, world, self.nn_k, self.nn_distance)
    317         else:
    318             raise ValueError(""Unknown method '%s'."" % self.method)

[/usr/local/lib/python3.7/dist-packages/datascope-0.0.3-py3.7-linux-x86_64.egg/datascope/importance/shapley.py](https://localhost:8080/#) in _shapley_neighbor(self, X, y, X_test, y_test, provenance, units, world, k, distance)
    507             assert isinstance(X_test, spmatrix)
    508             X_test = X_test.todense()
--> 509         distances = distance(X, X_test)
    510 
    511         # Compute the utilitiy values between training and test labels.

sklearn/metrics/_dist_metrics.pyx in sklearn.metrics._dist_metrics.DistanceMetric.pairwise()

ValueError: Buffer has wrong number of dimensions (expected 2, got 4)

```

Here's a snippet of my code: 

```
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

net = reset_model(seed = 0) # gives scikit-learn compatible skorch model

pipeline = Pipeline([(""model"", net)])

pipeline.fit(train_dataset, train_labels)
y_pred = pipeline.predict(test_dataset)

plot_loss(net)
accuracy_dirty = accuracy_score(y_pred, test_labels)
print(""Pipeline accuracy in the beginning:"", accuracy_dirty)
```
The above works fine, and I'm able to compute the accuracy of my baseline model.

However, when trying to run `importances.score(test_data, test_labels)` I'm getting the error mentioned above.
```
from datascope.importance.common import SklearnModelAccuracy
from datascope.importance.shapley import ShapleyImportance

net = reset_model(seed = 0)
pipeline = Pipeline([(""model"", net)])

utility = SklearnModelAccuracy(pipeline)
importance = ShapleyImportance(method=""neighbor"", utility=utility)
importances = importance.fit(train_data, train_labels)
importances.score(test_data, test_labels)
```

Here's the shape of my data:
```
train_data.shape, train_labels.shape
((2067, 3, 224, 224), (2067,))

test_data.shape, test_labels.shape
((813, 3, 224, 224), (813,))
```

Would be happy is someone could point me in the right direction! Not sure if this error is skorch related or the images are not supported yet? Thanks :) "
Is regression supported?,easeml/datascope,2022-06-17 07:39:06,1,,6,1274684763,"Looking at the code my understanding is that only classification is possible. (SklearnModelAccuracy)
I tried using SklearnModelUtility with MSE as a metric but this leads to a NotImplementedError.

Thanks!
"
"""compute_all_importances_cy"" data type mismatch",easeml/datascope,2022-05-29 11:55:12,5,,3,1251868267,"Thanks for this great library. I followed the instruction in readme.md file and run the setup.

I get the following error when trying to test on the notebook ""DataScope-Demo-1.ipynb"":

```
...\datascope\importance\shapley.py in compute_shapley_1nn_mapfork(distances, utilities, provenance, units, world, null_scores, simple_provenance)
    219     n_test = distances.shape[1]
    220     null_scores = null_scores if null_scores is not None else np.zeros((1, n_test))
--> 221     all_importances = compute_all_importances_cy(unit_distances, unit_utilities, null_scores)
    222 
    223     # Aggregate results.

datascope/importance/shapley_cy.pyx in datascope.importance.shapley_cy.compute_all_importances_cy()

ValueError: Buffer dtype mismatch, expected 'int_t' but got 'long long'
```

It seems there is an issue with type when calling the **compute_all_importances_cy** function. It expects integer but receives float(double?).

I tried to modify **compute_all_importances_cy** in **shapley_cy.pyx** but I had no luck to fix this bug."
About .pkl file visualization,hding2455/DSC,2022-03-10 06:36:03,1,,15,1164813898,"Hello, I have two questions to ask. I use ""python tools/test_ins.py configs/dsc/fast_dsc_r50_fpn_1x_coco.py f_dsc_r50_fpn_1x_coco.pth --out results.pkl --eval bbox segm
"" to test the COCO dataset, but the generated .pkl file, what should I do to achieve its visualization?
I am going to do the GPU training data set ""python tools/train.py configs/dsc/fast_dsc_r50_fpn_1x_coco.py"". Will this directly generate the .pth model.
Thank you for your contribution to the published paper"
evaluating model on Cityscapes,hding2455/DSC,2022-02-19 17:28:54,8,,14,1144800682,"Hello, I started training the model using the Cityscapes dataset (I have also converted the annotations to COCO style) and I wanted to run the test.py file. I have used both cityscapes and bbox & segm as eval args, both together and separately, but I keep obtaining the following results:
With cityscapes arg:

###############################
what           :             AP         AP_50%
###############################
person         :            nan            nan
rider          :            nan            nan
car            :            nan            nan
truck          :            nan            nan
bus            :            nan            nan
train          :            nan            nan
motorcycle     :            nan            nan
bicycle        :            nan            nan
average        :            nan            nan

With bbox (and segm) args:

Evaluating bbox...
Loading and preparing results...
DONE (t=1.14s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.55s).
Accumulating evaluation results...
DONE (t=0.68s).
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000

However, while training, at the end of every epoch, after performing the eval, I get normal looking results, eg. 0.390 AP.
This is the command that I run:
`python tools/test.py configs/dsc/dsc_r50_fpn_1x_cityscapes.py  work_dirs/dsc_r50_fpn_1x_cityscapes/epoch_8.pth --out  results_city_ep8.pkl --eval cityscapes bbox segm`
Are there other arguments that I should add to the command? Thanks!"
Multiple Issue..Difficult to use this,hding2455/DSC,2021-12-24 10:48:06,4,,10,1088311077,"@hding2455 
I was unable to run this repo,even with the current version that you use here.
Issue 1
1) mmdet.version was missing , i resolved it
2) issue 2 , CONWS already registered, i resolved it 
3) unresolvable cannot import  .cornor_pool_ext from corner_pool.py 

With new version there is difference between file  of yours and SamplingResults of version after your version of MMDET

Please suggest how to run.. I tried all possible bits but finally got stuck here.

"
Input for the config_file,fredfung007/snlt,2022-10-17 07:44:52,2,,18,1411075865,"Hello,
I had gone through your whole paper again and again, and I had also gone through all the issues. 
But, I am still pretty struggling with running your code. You told us the config file should be the path of the yaml document under the experiments folder. But, I am still pretty confused about how we actually get this or generate this yaml document. What should this yaml document contain? Should we find it from some of the datasets or do we need to manually generate it? How do we generate it if it is the second case?

Could you please go through one simple example using any one of the datasets so that we can learn from this?

Looking forward to your response. Greatly appreciate your hard-working on such brilliant work!"
about yaml file,fredfung007/snlt,2022-07-07 17:03:07,0,,17,1297782609,"Hello. How to get the config file of the experiment, there is no  experiments folder and  test.yaml .  Could you upload the experiments/test.yaml file ?"
about data json file,fredfung007/snlt,2022-06-27 04:36:25,0,,16,1285226380,"can you release the '/lasotcrops/lasot.json' file and other data config file? thanks
"
"could you tell me the language model of bert is which one ,cased or uncased?And which of the two results is better？",fredfung007/snlt,2022-03-15 01:58:21,1,,15,1169092245,
License?,lolemacs/avagrad,2020-03-24 17:13:28,0,,2,587132050,"I'm interested in using Avagrad for some research that may have commercial applications, so it would be great if you could clarify the license. One can never be too careful. Thanks!"
关于数据集的问题,guochengqian/PU-GCN,2022-09-28 01:09:52,0,,17,1388577349,您好，我想请问您放在google drive 里面的PU1K数据集压缩包，我解压出来之后看到数据集的后缀名中包含add_pugan的字样，我想请问下，这个数据集的表达意思是不是您在这个数据集中不仅仅是添加了PU1K数据集而且还添加了PUGAN数据集呢？
"Empty ""HDF5"" file in PU1K/train",guochengqian/PU-GCN,2022-09-26 12:50:32,1,,16,1386028214,"I have downloaded the model that has been provided in this repository and plan to train it using the PU1K data that you have linked to. The "".h5"" file provided in the training set, however, is empty, I have tried to view it using HDFview and pandas, in both cases no data is found.
I assume the ""h5"" file is actually empty as I receive a 'No such file or directory' message when I try to train the model in an Nvidia/cuda:10-devel-ubuntu18.04 docker container.
Please let me know if there is any alternative means to access the training data for your model.
![Screenshot from 2022-09-26 14-43-02](https://user-images.githubusercontent.com/26639730/192280625-5e712d89-9af4-4fbb-b65a-7f4b4c82cffd.png)
![Screenshot from 2022-09-26 14-43-31](https://user-images.githubusercontent.com/26639730/192280630-e6c351ed-d9cc-4862-acfa-62e7c9b4a880.png)
![Screenshot from 2022-09-26 14-43-57](https://user-images.githubusercontent.com/26639730/192280632-bf0fa92d-4dd2-4257-9be9-152a01180abe.png)
"
Where is the test data in PU-GAN dataset?,guochengqian/PU-GCN,2022-08-25 13:40:03,0,,15,1350920124,"Hello, I only found the test data in [](https://drive.google.com/open?id=1BNqjidBVWP0_MUdMTeGy1wZiR6fqyGmC). However, there is only 27 files with .off format. How can I get the test data in PU-GAN dataset?
Thank you very much for your time!"
test for kitti raw,guochengqian/PU-GCN,2022-08-01 11:19:11,1,,14,1324255823,"Hello, I am running this excellent project on kitti raw point cloud using provided pretrained model.
Strange hollows exisit in the upsampled clouds. Is it deal to cropping strategies or something else?
To reproduce:
`bash test_realscan.sh pretrain/pu1k-punet ./data/real_scan_kitti_pugcn 0 --model punet --upsampler original
bash test_realscan.sh pretrain/pu1k-mpu ./data/real_scan_kitti_pugcn 0 --model mpu --upsampler duplicate
bash test_realscan.sh pretrain/pu1k-pugcn  ./data/real_scan_kitti_pugcn 0 --model pugcn --k 20`

using the 0th frame of 2011-09-26_0048 from kitti raw sequence.
![original](https://user-images.githubusercontent.com/27678247/182136938-91709f4b-d597-436c-be06-4f06a7e1149f.png)
![upsampling](https://user-images.githubusercontent.com/27678247/182136956-ebf8f7c7-b0a7-4069-9a57-f4ff12b19fd0.png)

Thank you!"
There was a problem during test compilation,guochengqian/PU-GCN,2022-05-09 12:24:00,1,,13,1229627285,"-- This program requires the CGAL library, and will not be compiled.
-- Configuring done
-- Generating done
-- Build files have been written to: /data/home/scy0124/run/PU-GCN-master/evaluation_code
/var/spool/slurmd/job27981/slurm_script: line 14: ./evaluation: No such file or directory"
P2F metric return Nan,guochengqian/PU-GCN,2022-04-08 05:48:35,0,,11,1196857367,"First of all, thank you so much for the code
I have a problem using the code, so I'm asking you a question.
In the evaluation_code, the p2f value between my output point cloud and ground truth point cloud is Nan.
I took a print and found that **pred_map_points[i] = shortest_paths.point(location.first,location.second);** was Nan. I wonder if you've ever experienced this before. Also, what should I do if I had one?"
Some Questions about Metric Calculation and PU-GAN Dataset,guochengqian/PU-GCN,2021-12-26 06:50:48,3,,10,1088710750,"Hi, thanks for your work. While I have some questions about metric calculation and PU-GAN dataset. The first one is the hausdorff distance, your implementation [here](https://github.com/guochengqian/PU-GCN/blob/master/evaluate.py#L157) seems not consistent with the [definition](https://en.wikipedia.org/wiki/Hausdorff_distance), which means it should be calculated as follow: `hd_value = np.max(A, B)` but not `hd_value = np.max(A+B)`. I also check the code in PU-GAN ([here](https://github.com/liruihui/PU-GAN/blob/master/evaluate.py#L158)) and DIS-PU ([here](https://github.com/liruihui/Dis-PU/blob/main/evaluate.py#L158)), it seems you all calculate this metric mistakenly. The second is the point-to-surface distance, it seems that you only calculate the unidirectional point-to-surface distance [here ](https://github.com/guochengqian/PU-GCN/blob/master/evaluation_code/evaluation.cpp#L224), but not the symmetrical point-to-surface and surface-to-point distance, just as chamfer distance. And I think the symmetric calculation is more convincing. The third is that I find the test set of PU-GAN dataset only contains mesh file, so does it mean each paper using PU-GAN dataset generates the input point cloud file and groundtruth point cloud file by their own? If so, how do you generate the input and groundtruth test pair for PU-GAN dataset? I also notice your PU1K dataset provide test point cloud file, but the loss of test point cloud file in PU-GAN really makes me confused. Thanks a lot, hope for your reply."
how to attack long video?,anonymous-p/Flickering_Adversarial_Video,2021-02-24 02:33:03,0,,14,815023388,
how to convert my source mp4 video file to npy file which for attack?,anonymous-p/Flickering_Adversarial_Video,2021-02-20 12:50:05,6,,12,812604389,"vidcap = cv2.VideoCapture('/home/xkjs/NDisk/zys/srcz/ertong1.AVI.mp4')
success, image = vidcap.read()
count = 0
success = True
frames = []
while success:
    image = cv2.resize(image, (224, 224))
    frames.append(image)
    success, image = vidcap.read()
    count += 1
print(count, "" frames extracted"")
# into shape (1,90,224,224,3)
frames = np.array(frames).reshape((1, 1705, 224, 224, 3)) "
how to convert a video,anonymous-p/Flickering_Adversarial_Video,2020-08-17 17:34:04,3,,5,680406085,"how can i convert video to read by the pkl

and vice versa to convert .pkl to video 

please give me the code as soon as possible  i was struggling for more than a week"
Where is the code for Distillation Loss?,kinredon/umt,2022-05-08 18:46:40,0,,13,1228959425,"Hi, I am confused that I can't find the code for Distillation Loss in `umt_train.py`, could you tell me where is it？"
Details about CycleGAN training,kinredon/umt,2022-04-07 10:00:13,1,,12,1195813215,"Hi, thanks for your excellent work! I am currently trying to reproduce your result on Cityscapes->Foggy Cityscapes task. I would wonder how do you use CycleGAN to transfer styles between these two dataset. Would you mind sharing some details about it like the parameters and the scripts? Thanks in advance!"
Can torch1.8 run this project?,kinredon/umt,2022-03-16 13:37:10,1,,10,1171037097,"Thank you for providing such an excellent project, I have a high CUDA version, can torch 1.8 run this project?"
"Hello, Thankyou for this repository. Can you please explain the difference between 'city_cycle_foggy_voc' and 'foggy_cycle_city_voc'",kinredon/umt,2022-03-09 13:39:14,3,,8,1163952884,
About the loss of fake dataset,kinredon/umt,2021-10-04 02:37:35,0,,5,1014664164,"Can you upload the loss of adaptation training here for comparison.  The loss of loss_rpn_cls_s_fake, loss_rcnn_cls_s_fake and loss_rcnn_box_s_fake seem not converged in my training log, they are fluctuating.
What can be wrong in the training procedure?"
a bug,mmSir/GainedVAE,2022-09-13 07:47:04,0,,5,1371009888,"您好！感谢您提供的代码。
我使用相同的环境按照指示训练模型，遇到了如下错误，请教一下该怎么解决。
![A{{E1Y6U2QB3E8`KM{D ~KH](https://user-images.githubusercontent.com/84566085/189841471-02c14b49-b58b-484a-8d56-515bfe736402.png)
似乎是没有在pybind11包中查找到pybind11_builtins，是包的版本问题吗？
谢谢！"
mask conv,mmSir/GainedVAE,2022-08-31 06:30:49,0,,4,1356856616,"Hello. You don't use mask conv. could you tell me the reason?
"
scale table,mmSir/GainedVAE,2022-08-11 02:38:07,0,,3,1335400044,"您好，想请教您一个问题。

请问 GaussianConditional 里面预设的 scale table 是基于什么考虑设计的呀？
为啥要设置 scale table？为什么是里面这64个值呢？"
Some questions,mmSir/GainedVAE,2022-06-02 09:15:43,3,,2,1257917640,"你好，可以请教几个问题吗
1.损失部分为什么mse_loss 要乘以255的平方，其他的压缩论文好像都是直接乘以lambda系数
![image](https://user-images.githubusercontent.com/66156211/171596758-72af943b-28df-47fa-83b8-92a22bfeea31.png)
2. 这篇论文我有几个地方不太理解，它这个variable rate主要是体现在同时训练多个lambda参数得到不同的rate还是插值部分得到不同rate
3. 你最终的rd曲线结果是运行哪个文件得到的，我没找到测试文件"
The reported result of CutMix in Table.1,alldbi/SuperMix,2022-06-06 12:34:33,0,,7,1261783302,"Hi,

In the original paper of CutMix, it reports top-1 accuracy as 78.6 (21.4 top-1 error) on the ImageNet, however, in the Table.1 of your SuperMix, the corresponding result of CutMix is only 77.2. Could I know what makes the difference? Thanks."
Incorrect cross entropy?,kekmodel/MPL-pytorch,2022-09-24 12:33:40,0,,34,1384658109,"https://github.com/kekmodel/MPL-pytorch/blob/7fb5b40cd53179bf4c09ef0f916815c3272d3e9d/main.py#L197

At this point hard_pseudo_label is a batch size by 1 array of integer indexes, not a one hot encoding.

Is this correct when calculating cross entropy?"
Why do we expand labels?,kekmodel/MPL-pytorch,2022-09-22 11:06:54,0,,33,1382262439,"https://github.com/kekmodel/MPL-pytorch/blob/7fb5b40cd53179bf4c09ef0f916815c3272d3e9d/data.py#L138

Hey all, 

I've been working my way through this implementation and cannot work out why the expand labels options exists? It seems that even if the labels aren't expanded the data loader will loop anyway.

Can anyone explain why this is needed?"
Binary classification using MLP,kekmodel/MPL-pytorch,2022-08-01 12:29:45,0,,32,1324341259,"Hi 
Thank you for your valuable repository. How to change the loss function for binary classification? "
Model accuracy is reduced with the own dataset,kekmodel/MPL-pytorch,2022-04-16 05:39:25,0,,30,1206022112,"When I used my dataset for training, the model's accuracy dropped from 96% to 51%. can you help me, thank you"
Typo,kekmodel/MPL-pytorch,2022-03-31 03:10:50,0,,29,1187433294,"Thanks for the great code. One minor typo: 
https://github.com/kekmodel/MPL-pytorch/blob/main/data.py#L147"
why should add a batch normalize layer after the third res-block?,kekmodel/MPL-pytorch,2021-12-08 02:23:25,0,,24,1073940102,"In the forward of class WideResNet，there is a BN normalizing the output of the last res-block and if I comment this line, the loss in the model will become NaN after few steps (in a high learining rate). I want to ask whether the case just appear in MPL or I should note it in other algorithm?"
"CE(pseudo hard label, teacher_us)",kekmodel/MPL-pytorch,2021-08-15 08:13:30,1,,21,971078360,"<img width=""694"" alt=""image"" src=""https://user-images.githubusercontent.com/85744300/129471726-635c3e17-fc55-4a1a-8cae-36ea28c005e0.png"">
I have a small question about the above code. 
 
The aim of the cross entropy is to make the two different distributions be similar with each other. 
I understood the first CE loss but not the second CE loss, which confuses me a lot. 
In line 218, the latter argument of CE should follow the former argument.
In this case is the purpose of CE loss to make the raw output of teacher model to be sharpened? 
"
TypeError: object.__new__() takes exactly one argument (the type to instantiate),kekmodel/MPL-pytorch,2021-06-04 12:23:34,1,,17,911445880,"I have already successfully run the original code. To try to use new datasets, I inherit torch.utils.data.Dataset and create subclass Webface:
![HJZ1N%7GF8YR 4DU1JLFFN9](https://user-images.githubusercontent.com/34671700/120799659-9fecba80-c571-11eb-8b60-ad6989a810da.png)
![ECMY27X4@~G~Z_E8F4O7A2](https://user-images.githubusercontent.com/34671700/120799775-be52b600-c571-11eb-9344-f83b54a5ceb2.png)
I want to use webface as unlabeled dataset, however when I instantiate a webface object, I get a type error:
![CVQ6Y PW HKEFH)7JO_6 UG](https://user-images.githubusercontent.com/34671700/120800248-4d5fce00-c572-11eb-981a-e19208f0a254.png)
How to solve this bug? The error code is from a auto-generated file when I created an anaconda virtual environment."
CIFAR-10-4K Number of Steps,kekmodel/MPL-pytorch,2021-04-22 17:16:29,0,,15,865222235,"For CIFAR-10-4K, you have the model train for 300,000 steps, while the original paper trains for 1,000,000 steps. What is the reason for stopping earlier? 

![image](https://user-images.githubusercontent.com/68761152/115758303-f8267f00-a36c-11eb-92bb-d1ebef833868.png)
"
Errors,kekmodel/MPL-pytorch,2021-03-30 16:21:14,1,,14,844825756,"i encountered this error while running my code with GPU
```WARNING:__main__:Process rank: 0, device: cuda:0, distributed training: True, 16-bits training: True
2021-03-30 16:11:36.205545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Killing subprocess 1237
Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py"", line 340, in <module>
    main()
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py"", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File ""/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py"", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'drive/MyDrive/MPL/main.py', '--local_rank=0', '--name=hateful_memes_with_MPL', '--expand-labels', '--amp']' died with <Signals.SIGKILL: 9>.```

do you know the cause of this error ? thanks"
The number of unlabeled data for CIFAR-10.,kekmodel/MPL-pytorch,2021-03-27 08:40:18,0,,13,842453940,"""For CIFAR-10 [34], 4,000 labeled examples are kept as labeled
data while 41,000 examples are used as unlabeled data""

Why did you use whole data for the unlabeled data? 
```
def x_u_split(args, labels):
    label_per_class = args.num_labeled // args.num_classes
    labels = np.array(labels)
    labeled_idx = []
    # unlabeled data: all training data
     ...
```
"
Models,kekmodel/MPL-pytorch,2021-03-23 12:53:26,1,,12,838695797,what is the model you used in this MPL finetune method?
dot_product = s_loss_old - s_loss_new but s_loss_new - s_loss_old?,kekmodel/MPL-pytorch,2021-03-03 16:13:08,26,,6,821256780,"Hello, thanking for your pytorch implement of  MPL. I think the dot_prodoct should be s_loss_old - s_loss_new but s_loss_new - s_loss_old for the reason here 
<a href=""https://ibb.co/GtGnvDQ""><img src=""https://i.ibb.co/mXdbCK5/image.png"" alt=""image"" border=""0""></a><br /><a target='_blank' href='https://freeonlinedice.com/'>just flip a coin</a><br />
 Am I wrong?"
Teacher Gradients,kekmodel/MPL-pytorch,2021-02-18 17:37:51,12,,2,811317186,"Hi,
I understand in the reference implementation, the MPL loss on the teacher does nothing essentially. To fix this we use hard labels rather than soft ones.

For this purpose, I believe we should not be detaching `t_logits_us` here:
https://github.com/kekmodel/MPL-pytorch/blob/main/main.py#L208"
Loss curve,mahmoudnafifi/Exposure_Correction,2022-10-04 18:54:25,2,,24,1396709239,"Hi, @mahmoudnafifi!

Could you please share your loss curve during training? For both, disc and generator

Thanks in advance!"
implementation in Python,mahmoudnafifi/Exposure_Correction,2022-06-29 12:33:58,4,,22,1288658138,"hey @mahmoudnafifi 

This is a wonderful work, but since the implementation is in MATlab the reach is limited.
I was thinking of reimplementing this research paper in python, before doing that I thought first check with you if you are fine with it or not. I also dropped a mail few weeks back.

So will that be fine with you ?

I will definitely put citation, refence to original repo and research paper"
have artifacts on generated LR,ShuhangGu/DASR,2022-08-09 03:34:00,0,,20,1332660847,"I trained the model using my own dataset. 
I have some artifacts on LR after using the create_dataset_modify.py
Anyone has this too?
![artifact](https://user-images.githubusercontent.com/25168884/183558201-54b9dc6c-0912-4d4c-8320-3e7ed46b214d.png)

"
在加载模型时会占用很多时间,ShuhangGu/DASR,2022-06-16 08:15:45,1,,19,1273216789,"在初始化加载模型时
`Loading model from: /home/yuzun/project/MRI_multiModel_SR/DASR/codes/PerceptualSimilarity/models/weights/v0.1/alex.pth`
会卡十几分钟的时间
请问怎么解决呢"
关于数据集RealSR version3,ShuhangGu/DASR,2022-04-20 06:33:36,0,,18,1209224952,您好，从RealSR官方代码给的version3数据集链接下载得到的测试集只有50个HR-LR图像对，没有100个图像对，请问您是从哪下的
"no module named ""pytorch_wavelets""",ShuhangGu/DASR,2021-12-07 12:59:48,3,,17,1073324687,"您好，**from pytorch_wavelets import DWTForward**报错，找不到pytorch_wavelets包，用pip install也找不到，该怎么解决？
谢谢！"
Performance Reproducibility,ShuhangGu/DASR,2021-11-02 15:56:38,1,,16,1042541940,"Hello,
Using the provided models, I am not able to reproduce the numbers provided in the paper.

AIM19 results:
         - psnr: 21.7883
         - ssim: 0.5731
         - lpips: 0.3466

RealSR Results:
         - psnr: 25.7862
         - ssim: 0.7528
         - lpips: 0.2656
         
But according to the paper (Table 3), the performance should be:
AIM19: 21.600 | 0.5640 | 0.336 
RealSR: 26.782 | 0.7822 | 0.228

Could you please double-check the provided models, or maybe I am missing something?"
关于训练SRN,ShuhangGu/DASR,2021-10-29 03:44:26,7,,15,1039138125,"在按照train_DASR.json训练SRN的时候出现错误：
  File ""/work/DASRGAN/SRN/models/__init__.py"", line 23, in create_model
    raise NotImplementedError('Model [{:s}] not recognized.'.format(model))
NotImplementedError: Model [DASR_FS_ESRGAN_patchGAN] not recognized.
就是说没有DASR_FS_ESRGAN_patchGAN这个model，init.py里面有sr/srgan/srragan/De_Resnet/De_patch_wavelet_GAN/DASR/DASR_Adaptive_Model。想问一下SRN训练的时候用的哪个model？"
AIM_DASR_TDSR_G.pth模型的训练细节,ShuhangGu/DASR,2021-09-09 07:23:43,0,,13,991892142,您好，在使用AIM_DASR_TDSR_G模型测试AIM19的验证集时发现，输入noisy images(LR)运行的输出效果很差，但输出Clean Images(GT)得到的结果却很好，想请问您训练这个模型的数据集是如何组成的。谢谢
作者您好，train完DSN后，进行Generate LR-HR这步时报错。,ShuhangGu/DASR,2021-08-09 08:20:56,2,,11,963756961,"使用以下代码进行DSN的训练：
                  python train.py --dataset aim2019 --artifacts tdsr \
                                         --generator DeResnet --discriminator FSD \
                                          --norm_layer Instance \
                                          --filter wavelet --cat_or_sum cat  \
                                          --batch_size 8 --num_workers 8 --crop_size 256 \
                                          --save_path 0603_DeResnet+wavcat+FSD+wtex0.03
然后使用以下代码进行Generate LR-HR pairs and domain-distance maps时：
                 python create_dataset_modified.py --dataset aim2019 \
                              --checkpoint /home/DSN_experiments/0603_DeResnet+wavcat+FSD+wtex0.03/checkpoints/last_iteration.tar 
                                         --generator DeResnet --discriminator FSD  --filter avg_pool\
                                         --name 0603_DSN_LRs_aim2019
出现以下的报错:
![微信图片_20210809162010](https://user-images.githubusercontent.com/35997309/128677607-db2d2453-411f-4d8d-9344-e1d28a528825.jpg)
请问您当时有遇到这样的情况吗？代码我还没有修改过。"
Something wrong with the pretrained AIM_DeResnet_FSD.tar,ShuhangGu/DASR,2021-07-19 04:02:37,2,,10,947236987,"Thank you for your excellent job about the unsupervised SR. However, the pretrained model (AIM_DeResnet_FSD.tar,https://drive.google.com/file/d/1egzDbeL3UXeDwrjIapIL2HMtHEDImLSr/view?usp=sharing) can't be unzipped. I don't know what's wrong with it. Could you help me about this issue? Thank you in advance for your help."
About sharing results images,ShuhangGu/DASR,2021-07-08 02:39:34,0,,9,939411436,"Dear authors,

Thank you very much for sharing the great work!

While I know that pre-trained models and codes are available to reproduce the core contents of the paper,

it would be much convenient to make quick comparisons if results images can be directly downloaded from the repository."
How to reproduce the same visual results as in your paper？,ShuhangGu/DASR,2021-04-15 07:09:33,10,,8,858566700,"When training the models with the given parameters, we can get the best SR visual results almost like this:
![image](https://user-images.githubusercontent.com/82566391/114824980-387b7f80-9df8-11eb-8bc5-8cb5ce4cfd2d.png)
They are not as clear as your results. And as the number of iterations increases, there will be more artifacts like this：
![image](https://user-images.githubusercontent.com/82566391/114826013-86dd4e00-9df9-11eb-9701-e4bb4d5c4389.png)
So is our reproduction normal or is there something wrong？"
How to test unpaired dataset?,ShuhangGu/DASR,2021-01-26 13:08:41,1,,7,794217892,"Hello,
recently I'm reproducing your work, and have successfully completed the training process.
When Test, I have got a problem.

i) The json file of test says the model = ""sr"", which means it lacks the ability to test datasets like Set5, BSD100 according to the SR_model.py , How to deal with such need?
I wander If there are built-in method about this situation or just resize the imgs by myself?
ii) In the json file , has a key named: pretrain_model_D_wlt, where is the weight_map comes from?


Thanks for your work and looking forward to your quick reply : )"
About CameraSR dataset,ShuhangGu/DASR,2020-12-15 02:25:16,7,,6,767096019,"Thanks for the awesome work!
As paper descriptied, CameraSR dataset result is trained on the Nicon subset (and DIV2K) and tested on the iphoneX subset.
Please tell me if I have misunderstood. "
Trained DASR models,ShuhangGu/DASR,2020-10-14 15:01:58,0,,5,721539353,"Where can I find fully trained model checkpoints?

Cause `CameraSR DeResnet` URL seems to be broken."
Source codes missing for SRN,ShuhangGu/DASR,2020-10-05 17:06:11,2,,4,715012795,There is no 'codes' folder in the SRN directory
About path: target: '/media/4T/Dizzy/AIM/AIM_datasets/DIV2K_train_HR',ShuhangGu/DASR,2020-09-15 15:22:19,4,,3,702027974,is it the original DIV2K_train_HR? 
About DSN,ShuhangGu/DASR,2020-07-12 03:12:25,2,,2,655321261,"For the design of DSN, how do you prevent DSN from completely becoming a ""bicubic"" down sampler, because when DSN is a ""bicubic"" down sampler, the loss will be very small."
When will the code be released?,ShuhangGu/DASR,2020-04-26 01:27:27,2,,1,606892630,Thanks for your work. when will the code be released?
Is there a Pytorch version?,wy1iu/OPT,2021-12-03 08:46:28,0,,1,1070353594,"Hi, thanks for your great work. 

Is there a pytorch version ?"
Allow more algorithm to use prior knowledge,huawei-noah/trustworthyAI,2022-09-28 05:06:26,3,,79,1388734588,"Just like the example of PC algorithm,[PC demo.py](https://github.com/huawei-noah/trustworthyAI/blob/1868f3a3c15b9e6131263a8c9a1ccb1f59b3c045/gcastle/example/pc/pc_demo.py)
 I hope you can provide interfaces to allow more algorithms to adopt a priori knowledge.
![image](https://user-images.githubusercontent.com/49603007/192692097-bba01b21-904a-44d9-82e4-47e9f27025c2.png)"
task of data generation,huawei-noah/trustworthyAI,2022-09-15 03:46:17,1,,76,1373872190,"Hi. When I use the GUI web for the task of data generation. I found that the edges in the graph are not equal to those in the configuration parameters. When I change the seed while keeping the n_nodes and n_edges identical, the edges in the graph may also be changed. So What's the effect of seed? Thanks."
PC算法的find_skeleton为什么不先计算所有数据的相关系数矩阵,huawei-noah/trustworthyAI,2022-03-16 08:16:52,0,,55,1170683287,而是在CITest里面重复计算每组数据相关快系数
error in convert_graph_int_to_adj_mat,huawei-noah/trustworthyAI,2022-01-17 04:58:42,7,,43,1105374240,"Hello,

I am trying to use [Causal Disocvery RL](https://github.com/huawei-noah/trustworthyAI/tree/master/Causal_Structure_Learning/Causal_Discovery_RL) on bnlearn benchmarks. I encounter the error in convert_graph_int_to_adj_mat function.

The input to this function is 
```
[-1903318164   235405414   101482606   495790951   201853294   378349935
 -1634426101 -1718146065   134742090          64   134742086   134742084
   446475107   470616428 -1785775892 -1768316434   201884524   134217728
   201949548 -1903613075   470286702   101187694 -1734505621   503843118
 -2070074547   134217838   513518542   503875886   235405386   445754223
           0      524358   236432367   134742086   134217792   134217792
   503908622]
```

And the error message follows:
```
Traceback (most recent call last):
  File ""main.py"", line 337, in <module>
    main()
  File ""main.py"", line 285, in main
    graph_batch = convert_graph_int_to_adj_mat(graph_int)
  File ""/home/user/Causal_Discovery_RL/src/helpers/analyze_utils.py"", line 156, in convert_graph_int_to_adj_mat
    for curr_int in graph_int], dtype=int)
  File ""/home/user/Causal_Discovery_RL/src/helpers/analyze_utils.py"", line 156, in <listcomp>
    for curr_int in graph_int], dtype=int)
ValueError: invalid literal for int() with base 10: '-'
```"
CORL处理非线性因果发现问题,huawei-noah/trustworthyAI,2022-01-10 11:48:16,4,,42,1097802838,请问如果是处理非线性问题应该如何修改代码，只修改reg_type为“GPR”似乎在variable selection时有个pruning_cam方法没有定义，请问该如何解决，谢谢！
Separate repo for gCastle ?,huawei-noah/trustworthyAI,2021-12-02 17:53:55,1,,39,1069828885,Would it make sense to have gCastle in its own repo? Just wondering.
fin-data,huawei-noah/trustworthyAI,2021-10-22 08:55:09,0,,33,1033341886,"can I loader the fin-data? just the csv of (open low high close volume).
if not, how can I deal with it?"
Can I add restrictions on causal structure？,huawei-noah/trustworthyAI,2021-09-25 04:30:27,2,,32,1006957122,"When learning causal structure, can I set a parameter to specify which causal edges exist or do not exist?"
The Comparison for Vector-Valued Sample Data with NOTEARS,huawei-noah/trustworthyAI,2021-07-29 15:02:05,0,,26,955935164,"Hi there,

Thanks for your great work for causal structure learning. I'm currently reading your paper (_A Graph Autoencoder Approach to Causal Structure Learning_) and have a little bit confusion so far.

For part **4.2 Vector-Valued Case**, I don't quite understand how the comparison made with the NOTEARS code? I mean, the NOTEARS only designed for handling the Scalar-Valued variable. But in this part, the variable dimension are set to 5, but the NOTEARS code as far as I check, it can't set the variable dimension. Am I missing something? 

Thanks for any reply!"
Is there more information on the real dataset?,huawei-noah/trustworthyAI,2021-07-09 02:27:06,0,,25,940358246,"I'm interested in the [dataset from real telecommunication networks](/huawei-noah/trustworthyAI/tree/master/Causal_Structure_Learning/Datasets).
I have some questions, of which I failed to find the answers in neither this repo nor the [dataset repo](https://github.com/zhushy/causal-datasets/tree/master/Real_Dataset).

- How did the experts obtain the underlying causal relationships?
  - Have the experts conducted randomized controlled trials to verify each causal relationship?
- Is there a published paper or a public website that describe the dataset and the way to create it?
- There are 57 kinds of alarms in the causal graph, but only 55 columns in the processed data. What's wrong with the missing A_21 and A_34?

I notice that the result part is still empty after more than a year since the last edition.
The listed two paper, in NIPS 2019 and ICLR 2020, did not use this dataset for evaluation.

- It will be interesting if the experiment result related to this dataset could be summarized.
- How should we cite this dataset for research?"
ttpm.py line 343 : what is the meaning of  mu[i] * self._T,huawei-noah/trustworthyAI,2021-07-05 03:39:26,0,,23,936649784,"Hi,
I'm wondering what's the meaning of  `mu[i] * self._T ` in the first part of likelihood. It's not consistent with the paper, which should be lambda*delta t"
Error in  eval-action-recg-linear.py,facebookresearch/AVID-CMA,2022-08-10 09:03:30,0,,13,1334308292,"Hello.
Thank you for your excellent work and release of the code.

When I run python a code `eval-action-recg-linear.py configs/benchmark/kinetics/8x224x224-linear.yaml configs/main/avid/kinetics/Cross-N1024.yaml`, some errors occur in the ""run_phase"" function below.

Below is the location where the error code occurred and its output.
The print is placed before and after the point where the error occurred.

error code
```
def run_phase(phase, loader, model, optimizer, epoch, args, cfg, logger):
    
    .
    .
    .

    total_loss = 0.
        for ft in feature_names:
            if phase == 'test_dense':
                confidence = softmax(logits[ft]).view(batch_size, clips_per_sample, -1).mean(1)
                target_tiled = target.unsqueeze(1).repeat(1, clips_per_sample).view(-1)
                loss = criterion(logits[ft], target_tiled)
            else:
                confidence = softmax(logits[ft])
                print(logits[ft])
                print(confidence)
                print(target)
                print('---------------')
                loss = criterion(logits[ft], target)
                print(loss)
            total_loss += loss

            with torch.no_grad():
                acc1, acc5 = metrics_utils.accuracy(confidence, target, topk=(1, 5))
                loss_meters[ft].update(loss.item(), target.size(0))
                top1_meters[ft].update(acc1[0].item(), target.size(0))
                top5_meters[ft].update(acc5[0].item(), target.size(0))
```

output


```
==============================   Test DB   ==============================
/home/haruka-asanuma/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Kinetics dataset
 - Root: /home/haruka-asanuma/kinetics-datasets//val
 - Subset: val
 - Num videos: 19881
 - Num samples: 497025
 - Example video: /home/haruka-asanuma/kinetics-datasets//val/EzAPYkvFxO0_000355_000365.mp4

==============================   Dense DB   ==============================
Kinetics dataset
 - Root: /home/haruka-asanuma/kinetics-datasets//val
 - Subset: val
 - Num videos: 19881
 - Num samples: 198810
 - Example video: /home/haruka-asanuma/kinetics-datasets//val/EzAPYkvFxO0_000355_000365.mp4


test: Epoch 12
tensor([[-0.4370, -2.4042, -0.4472,  ..., -0.2500, -2.4469,  0.0503],
        [-1.2650, -3.2401, -0.3476,  ...,  0.3754, -3.4689,  1.7811],
        [-1.1321, -4.3601, -1.2206,  ..., -1.4128, -2.2114,  1.1148],
        ...,
        [-0.7892, -3.8467, -1.6175,  ..., -0.8994, -1.7921,  0.7191],
        [-0.6565, -2.5375,  1.0547,  ..., -0.4171, -2.0549,  1.8172],
        [ 0.4295, -2.2780, -1.2851,  ..., -1.4298, -0.6527,  0.6132]],
       device='cuda:0')
tensor([[4.6641e-04, 6.5228e-05, 4.6168e-04,  ..., 5.6230e-04, 6.2498e-05,
         7.5932e-04],
        [1.8080e-04, 2.5084e-05, 4.5248e-04,  ..., 9.3242e-04, 1.9954e-05,
         3.8025e-03],
        [1.2092e-04, 4.7928e-06, 1.1068e-04,  ..., 9.1322e-05, 4.1092e-05,
         1.1437e-03],
        ...,
        [9.6051e-05, 4.5151e-06, 4.1955e-05,  ..., 8.6029e-05, 3.5234e-05,
         4.3407e-04],
        [3.6984e-04, 5.6375e-05, 2.0471e-03,  ..., 4.6987e-04, 9.1348e-05,
         4.3887e-03],
        [8.1401e-04, 5.4299e-05, 1.4655e-04,  ..., 1.2680e-04, 2.7584e-04,
         9.7820e-04]], device='cuda:0')
tensor([ 5289, 12931,  6744,  1889,  8891, 12426, 18293,  7740,  7090,   737,
         3557, 14430, 10911, 16911,  6179,  6593,     3,  3685,  2417,  7936,
         1608, 12762,  2093,  9390, 14598,  8638, 18721, 12621, 16732,  2105,
        17101,   970, 19066,  8943, 14840,  3280, 14341, 17823, 10266, 15639,
         5584, 16445, 18098, 12328,  1673, 14808,  8710, 15231,  1479,  5327,
        13081,   289, 18751,  7479,  4119,  5000, 18878,  2995,  7328,  1425,
         2274, 19566, 13748, 11803,  7019,  2386,  9278, 17079,  4871, 15091,
         7056,  9649,  3191, 14395,  7064, 14885,  6459,  5782, 14693, 16240,
         1542,  8260,   362, 10696, 15122,  8446, 17733,  8491,  1144,  1154,
        13995, 19726,  5870, 15542, 17442, 13262,  3836, 11575, 14413,  3346,
         3094,  4787,  9883, 16552, 17225, 14576, 16234, 15394, 19573, 16467,
         3717, 14414,  1112,  7343,   154,  1852, 12860, 15433,  7661,  6824,
         5161, 17000, 12098,  1200, 19550, 16283,  2416, 16018],
       device='cuda:0')
---------------

/opt/conda/conda-bld/pytorch_1656352660876/work/aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1656352660876/work/aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1656352660876/work/aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1656352660876/work/aten/src/ATen/native/cuda/Loss.cu:271: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.

```


This means that this function is not working well.
Why this happened? How did you make the score in READ ME ??

Thank you in advance.

"
Could you share the codes of linear probing AVID model on ESC50 and DCASE dataset?,facebookresearch/AVID-CMA,2022-06-22 09:29:57,0,,12,1279849563,"Hi, Could you share the codes of linear probing AVID model on ESC50 and DCASE dataset? There are only codes for fine-tuning AVID model on HMDB51 and UCF101 dataset and linear probing AVID model on Kinetics dataset."
How to save and load checkpoints in mult-cards multi-machines mode?,facebookresearch/AVID-CMA,2022-01-06 07:27:40,0,,9,1095044380,"Hello! wonderfull project! Here I have a question about how to save and  load checkpoints when training in multi-cards and multi-machines? I suppose that the original codes simply save the trained models in all nodes (I'm not sure if I understand) without specifying which node to save.  Generally, training in multi-cards and multi-machines using ddp solely save the models of master nodes, by judging whether it is a master node like:
`if dist.get_rank() == 0:`

Moreover, when training using DDP, how to load the saved checkpoints. I notice that in original codes, the way to load the trained models is:
```
ckp = torch.load(checkpoint, map_location='cpu')
nn.DataParallel(model).load_state_dict(ckp['model'])
```
Is DDP the same？

Looking forward to your reply. Thanks"
Quetion about clips_per_video=10 in training Kinetics,facebookresearch/AVID-CMA,2021-12-30 09:54:30,1,,8,1090995565,"Hi

Thank you for your excellent work and release of the code.

There is one thing in the code I am very confused about: why do you set clips_per_video=10 in your training script on Kinetics400.
If I have not misunderstood, this will only make repeat the samples 10 times, and thus you trained the model for 30 epochs, which has the same effect as training for 300 epochs, as stated in your paper.

Did setting clips_per_video=10 result in a faster convergence in your training?

Thank you in advance for answering the question!

Bests,




 "
Using Cross-AVID for Audio-Video Synchronization,facebookresearch/AVID-CMA,2021-11-17 14:06:05,0,,7,1056172253,"Hi,

The work presented in this paper is fascinating and I thank you for releasing the code as well.

I have read the paper several times and have gone through the code as well, and I just wanted to find out if Cross AVID can be used for performing audio-visual lip synchronization, and if so, how? By audio visual lip-synchronization, I mean that the model encourages the video and audio embeddings to be close-by in the embedding space when the audio is in-sync with the mouth movements in the video and vice versa.

From my understanding, this was not the focus of the paper or the code because my understanding is that the model is instead encouraged to match the corresponding memory features from the memory bank (this is also what is defined as the 'target' in the paper).



My understanding is that, in [this](https://github.com/facebookresearch/AVID-CMA/blob/4538e404c79eb1b7459122a31508d918038705fd/criterions/avid.py#L57) portion of the code and [this ](https://github.com/facebookresearch/AVID-CMA/blob/4538e404c79eb1b7459122a31508d918038705fd/criterions/avid.py#L70) code portion, instead of working with positive memories, we would instead be using the computed embeddings from [here](https://github.com/facebookresearch/AVID-CMA/blob/4538e404c79eb1b7459122a31508d918038705fd/criterions/avid.py#L52)? 

If you could advise on how cross-AVID can be adapted to perform audio-visual lip-sync synchronization, and whether my understanding is correct, it would be highly appreciated.

Thanks!
"
Pretrained model with R(2+1) D backbone from Table 10,facebookresearch/AVID-CMA,2021-08-27 19:52:42,1,,6,981550298,"Thanks for releasing the code and pretrained models of your amazing work ""Audio-Visual Instance Discrimination with Cross-Modal Agreement"".   I noticed that you used different architectures for R(2+1)D in different experiments as shown in Table 9 and Table 10. Can you please release/share the kinetics pretrained model where you used the architecture of Table 10 for the R(2+1) D backbone? 
I am working on self-supervised learning and want to include your paper in my current project. For comparison purposes, I want to use the same backbone as done by previous works."
Discrepancy of Accuracy Results,facebookresearch/AVID-CMA,2021-08-08 10:00:01,0,,5,963396770,"When I retested AVID+Kinetics+UCF Video@1 Acc. twice, I got 84.48 and 85.28. Both were lower than the 86.9 in the table. The difference between the results of the retest and the difference between the results of the retest and the table are large, even greater than the difference of adding the CMA. I'm very curious about what to pay attention to in the retest to get a result close to the table."
Is there any sample code for audio data evaluation?,facebookresearch/AVID-CMA,2021-05-16 11:23:48,0,,4,892650556,"It seems like this repo only provide evaluation scripts for these 3 video datasets (Kinetics-400, UCF and HMDB).
Would you provide the scripts of experiments on audio datasets such as ESC-50? 
Or could you describe how to reproduce the experimental results on ESC-50 in original paper?
Thanks a lot."
Why the params of MobileDets in Jetson Xavier GPU is so large?,inacmor/mobiledets-yolov4-pytorch,2021-10-17 05:30:14,0,,1,1028221939,"`----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 219, 220]             896
       BatchNorm2d-2         [-1, 32, 219, 220]              64
             ReLU6-3         [-1, 32, 219, 220]               0
            Conv2d-4          [-1, 8, 219, 220]             256
       BatchNorm2d-5          [-1, 8, 219, 220]              16
             ReLU6-6          [-1, 8, 219, 220]               0
            Conv2d-7         [-1, 16, 219, 220]           1,152
       BatchNorm2d-8         [-1, 16, 219, 220]              32
             ReLU6-9         [-1, 16, 219, 220]               0
           Conv2d-10         [-1, 16, 219, 220]             256
      BatchNorm2d-11         [-1, 16, 219, 220]              32
       TuckerConv-12         [-1, 16, 219, 220]               0
           Conv2d-13        [-1, 128, 110, 110]          18,432
      BatchNorm2d-14        [-1, 128, 110, 110]             256
            ReLU6-15        [-1, 128, 110, 110]               0
         Identity-16        [-1, 128, 110, 110]               0
           Conv2d-17         [-1, 32, 110, 110]           4,096
      BatchNorm2d-18         [-1, 32, 110, 110]              64
     EdgeResidual-19         [-1, 32, 110, 110]               0
           Conv2d-20          [-1, 8, 110, 110]             256
      BatchNorm2d-21          [-1, 8, 110, 110]              16
            ReLU6-22          [-1, 8, 110, 110]               0
           Conv2d-23          [-1, 8, 110, 110]             576
      BatchNorm2d-24          [-1, 8, 110, 110]              16
            ReLU6-25          [-1, 8, 110, 110]               0
           Conv2d-26         [-1, 32, 110, 110]             256
      BatchNorm2d-27         [-1, 32, 110, 110]              64
       TuckerConv-28         [-1, 32, 110, 110]               0
           Conv2d-29          [-1, 8, 110, 110]             256
      BatchNorm2d-30          [-1, 8, 110, 110]              16
            ReLU6-31          [-1, 8, 110, 110]               0
           Conv2d-32          [-1, 8, 110, 110]             576
      BatchNorm2d-33          [-1, 8, 110, 110]              16
            ReLU6-34          [-1, 8, 110, 110]               0
           Conv2d-35         [-1, 32, 110, 110]             256
      BatchNorm2d-36         [-1, 32, 110, 110]              64
       TuckerConv-37         [-1, 32, 110, 110]               0
           Conv2d-38          [-1, 8, 110, 110]             256
      BatchNorm2d-39          [-1, 8, 110, 110]              16
            ReLU6-40          [-1, 8, 110, 110]               0
           Conv2d-41          [-1, 8, 110, 110]             576
      BatchNorm2d-42          [-1, 8, 110, 110]              16
            ReLU6-43          [-1, 8, 110, 110]               0
           Conv2d-44         [-1, 32, 110, 110]             256
      BatchNorm2d-45         [-1, 32, 110, 110]              64
       TuckerConv-46         [-1, 32, 110, 110]               0
           Conv2d-47          [-1, 256, 55, 55]          73,728
      BatchNorm2d-48          [-1, 256, 55, 55]             512
            ReLU6-49          [-1, 256, 55, 55]               0
         Identity-50          [-1, 256, 55, 55]               0
           Conv2d-51           [-1, 64, 55, 55]          16,384
      BatchNorm2d-52           [-1, 64, 55, 55]             128
     EdgeResidual-53           [-1, 64, 55, 55]               0
           Conv2d-54          [-1, 512, 55, 55]         294,912
      BatchNorm2d-55          [-1, 512, 55, 55]           1,024
            ReLU6-56          [-1, 512, 55, 55]               0
         Identity-57          [-1, 512, 55, 55]               0
           Conv2d-58           [-1, 64, 55, 55]          32,768
      BatchNorm2d-59           [-1, 64, 55, 55]             128
     EdgeResidual-60           [-1, 64, 55, 55]               0
           Conv2d-61          [-1, 512, 55, 55]         294,912
      BatchNorm2d-62          [-1, 512, 55, 55]           1,024
            ReLU6-63          [-1, 512, 55, 55]               0
         Identity-64          [-1, 512, 55, 55]               0
           Conv2d-65           [-1, 64, 55, 55]          32,768
      BatchNorm2d-66           [-1, 64, 55, 55]             128
     EdgeResidual-67           [-1, 64, 55, 55]               0
           Conv2d-68          [-1, 256, 55, 55]         147,456
      BatchNorm2d-69          [-1, 256, 55, 55]             512
            ReLU6-70          [-1, 256, 55, 55]               0
         Identity-71          [-1, 256, 55, 55]               0
           Conv2d-72           [-1, 64, 55, 55]          16,384
      BatchNorm2d-73           [-1, 64, 55, 55]             128
     EdgeResidual-74           [-1, 64, 55, 55]               0
           Conv2d-75          [-1, 512, 28, 28]         294,912
      BatchNorm2d-76          [-1, 512, 28, 28]           1,024
            ReLU6-77          [-1, 512, 28, 28]               0
         Identity-78          [-1, 512, 28, 28]               0
           Conv2d-79          [-1, 128, 28, 28]          65,536
      BatchNorm2d-80          [-1, 128, 28, 28]             256
     EdgeResidual-81          [-1, 128, 28, 28]               0
           Conv2d-82          [-1, 512, 28, 28]         589,824
      BatchNorm2d-83          [-1, 512, 28, 28]           1,024
            ReLU6-84          [-1, 512, 28, 28]               0
         Identity-85          [-1, 512, 28, 28]               0
           Conv2d-86          [-1, 128, 28, 28]          65,536
      BatchNorm2d-87          [-1, 128, 28, 28]             256
     EdgeResidual-88          [-1, 128, 28, 28]               0
           Conv2d-89          [-1, 512, 28, 28]         589,824
      BatchNorm2d-90          [-1, 512, 28, 28]           1,024
            ReLU6-91          [-1, 512, 28, 28]               0
         Identity-92          [-1, 512, 28, 28]               0
           Conv2d-93          [-1, 128, 28, 28]          65,536
      BatchNorm2d-94          [-1, 128, 28, 28]             256
     EdgeResidual-95          [-1, 128, 28, 28]               0
           Conv2d-96          [-1, 512, 28, 28]         589,824
      BatchNorm2d-97          [-1, 512, 28, 28]           1,024
            ReLU6-98          [-1, 512, 28, 28]               0
         Identity-99          [-1, 512, 28, 28]               0
          Conv2d-100          [-1, 128, 28, 28]          65,536
     BatchNorm2d-101          [-1, 128, 28, 28]             256
    EdgeResidual-102          [-1, 128, 28, 28]               0
          Conv2d-103         [-1, 1024, 28, 28]       1,179,648
     BatchNorm2d-104         [-1, 1024, 28, 28]           2,048
           ReLU6-105         [-1, 1024, 28, 28]               0
        Identity-106         [-1, 1024, 28, 28]               0
          Conv2d-107          [-1, 128, 28, 28]         131,072
     BatchNorm2d-108          [-1, 128, 28, 28]             256
    EdgeResidual-109          [-1, 128, 28, 28]               0
          Conv2d-110         [-1, 1024, 28, 28]       1,179,648
     BatchNorm2d-111         [-1, 1024, 28, 28]           2,048
           ReLU6-112         [-1, 1024, 28, 28]               0
        Identity-113         [-1, 1024, 28, 28]               0
          Conv2d-114          [-1, 128, 28, 28]         131,072
     BatchNorm2d-115          [-1, 128, 28, 28]             256
    EdgeResidual-116          [-1, 128, 28, 28]               0
          Conv2d-117         [-1, 1024, 28, 28]       1,179,648
     BatchNorm2d-118         [-1, 1024, 28, 28]           2,048
           ReLU6-119         [-1, 1024, 28, 28]               0
        Identity-120         [-1, 1024, 28, 28]               0
          Conv2d-121          [-1, 128, 28, 28]         131,072
     BatchNorm2d-122          [-1, 128, 28, 28]             256
    EdgeResidual-123          [-1, 128, 28, 28]               0
          Conv2d-124         [-1, 1024, 28, 28]       1,179,648
     BatchNorm2d-125         [-1, 1024, 28, 28]           2,048
           ReLU6-126         [-1, 1024, 28, 28]               0
        Identity-127         [-1, 1024, 28, 28]               0
          Conv2d-128          [-1, 128, 28, 28]         131,072
     BatchNorm2d-129          [-1, 128, 28, 28]             256
    EdgeResidual-130          [-1, 128, 28, 28]               0
          Conv2d-131          [-1, 512, 14, 14]         589,824
     BatchNorm2d-132          [-1, 512, 14, 14]           1,024
           ReLU6-133          [-1, 512, 14, 14]               0
        Identity-134          [-1, 512, 14, 14]               0
          Conv2d-135          [-1, 128, 14, 14]          65,536
     BatchNorm2d-136          [-1, 128, 14, 14]             256
    EdgeResidual-137          [-1, 128, 14, 14]               0
          Conv2d-138          [-1, 512, 14, 14]         589,824
     BatchNorm2d-139          [-1, 512, 14, 14]           1,024
           ReLU6-140          [-1, 512, 14, 14]               0
        Identity-141          [-1, 512, 14, 14]               0
          Conv2d-142          [-1, 128, 14, 14]          65,536
     BatchNorm2d-143          [-1, 128, 14, 14]             256
    EdgeResidual-144          [-1, 128, 14, 14]               0
          Conv2d-145          [-1, 512, 14, 14]         589,824
     BatchNorm2d-146          [-1, 512, 14, 14]           1,024
           ReLU6-147          [-1, 512, 14, 14]               0
        Identity-148          [-1, 512, 14, 14]               0
          Conv2d-149          [-1, 128, 14, 14]          65,536
     BatchNorm2d-150          [-1, 128, 14, 14]             256
    EdgeResidual-151          [-1, 128, 14, 14]               0
          Conv2d-152          [-1, 512, 14, 14]         589,824
     BatchNorm2d-153          [-1, 512, 14, 14]           1,024
           ReLU6-154          [-1, 512, 14, 14]               0
        Identity-155          [-1, 512, 14, 14]               0
          Conv2d-156          [-1, 128, 14, 14]          65,536
     BatchNorm2d-157          [-1, 128, 14, 14]             256
    EdgeResidual-158          [-1, 128, 14, 14]               0
          Conv2d-159         [-1, 1024, 14, 14]         131,072
     BatchNorm2d-160         [-1, 1024, 14, 14]           2,048
            ReLU-161         [-1, 1024, 14, 14]               0
          Conv2d-162         [-1, 1024, 14, 14]           9,216
     BatchNorm2d-163         [-1, 1024, 14, 14]           2,048
            ReLU-164         [-1, 1024, 14, 14]               0
        Identity-165         [-1, 1024, 14, 14]               0
          Conv2d-166          [-1, 384, 14, 14]         393,216
     BatchNorm2d-167          [-1, 384, 14, 14]             768
     InvertedResidual-168          [-1, 384, 14, 14]               0`

Total params: 11,690,672
Trainable params: 11,690,672
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 2.23
Forward/backward pass size (MB): 527.29
Params size (MB): 44.60
Estimated Total Size (MB): 574.11


The original paper says the params in all the platforms are less than 10M. "
tabular data/ noisy instances ,XinshaoAmosWang/ProSelfLC-CVPR2021,2022-05-09 08:49:44,1,,9,1229386789,"Hi,
thanks for sharing your implementation. I have two questions about it:

1.	Does it also work on tabular data?
2.	Is it possible to identify the noisy instances (return the noisy IDs or the clean set)?

Thanks!
"
Further research: Not All Knowledge Is Created Equal https://arxiv.org/abs/2106.01489,XinshaoAmosWang/ProSelfLC-CVPR2021,2021-06-04 21:43:37,0,,2,911863805,"Not All Knowledge Is Created Equal
Ziyun Li, Xinshao Wang, Haojin Yang, Di Hu, Neil M. Robertson, David A. Clifton, Christoph Meinel

arXiv: https://arxiv.org/abs/2106.01489

Mutual knowledge distillation (MKD) improves a model by distilling knowledge from another model. However, not all knowledge is certain and correct, especially under adverse conditions. For example, label noise usually leads to less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge misleads the learning rather than helps. This problem can be handled by two aspects: (i) improving the reliability of a model where the knowledge is from (i.e., knowledge source's reliability); (ii) selecting reliable knowledge for distillation. In the literature, making a model more reliable is widely studied while selective MKD receives little attention. Therefore, we focus on studying selective MKD and highlight its importance in this work.

Concretely, a generic MKD framework, Confident knowledge selection followed by Mutual Distillation (CMD), is designed. The key component of CMD is a generic knowledge selection formulation, making the selection threshold either static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special cases: zero knowledge and all knowledge, leading to a unified MKD framework. We empirically find CMD-P performs better than CMD-S. The main reason is that a model's knowledge upgrades and becomes confident as the training progresses.

Extensive experiments are present to demonstrate the effectiveness of CMD and thoroughly justify the design of CMD. For example, CMD-P obtains new state-of-the-art results in robustness against label noise.

"
Dataset preparation instructions are needed.,dvlab-research/gfs-seg,2022-08-17 03:11:16,2,,2,1341106858,"Hello, great work! I find that many txt files like ""train_contain_crowd_data_list.txt"" and ""voc2012/list/val.txt"" are hard to find. I suggest that the dataset preparation instructions are needed.

```
    raise (RuntimeError(""Image list file do not exist: "" + data_list + ""\n""))
RuntimeError: Image list file do not exist: /home/zhuotaotian/smtxlun2/dataset/voc2012/list/val.txt
```"
data load error,dvlab-research/gfs-seg,2022-06-25 11:16:50,3,,1,1284554466,"Thank you for your excellent work

the error in dataset.py  line 95
 rand_select_idx = raw_select_list[random.randint(0,len(raw_select_list)-1)]

while at the first round,the length of raw_select_list is zero ,so the random function report error ?how to salve this error.
"
"Implementing the script ""run_experiment.py"" prints the ""error"" message for every image",yihongsun/bayesian-amodal,2022-05-12 08:29:11,3,,1,1233631744,
Why are two driving images used in training?,itsyoavshalev/Image-Animation-with-Perturbed-Masks,2022-10-31 02:22:14,0,,7,1429147256,"Hello,
Thanks for your excellent work and for making it open-source! I have a little doubt about the source code. Why are two driving images used in training? 
there,  it does not seem to mention in the paper. 
![image](https://user-images.githubusercontent.com/41572723/198917933-a864b24a-1bd9-4ed3-ab84-ae11696fb9c6.png)"
How about cross domain generation?,itsyoavshalev/Image-Animation-with-Perturbed-Masks,2022-08-08 05:40:50,0,,6,1331358511,"Excellent work, thank you for sharing code. But is it possible to handle domain shift, like drive cat/dog face expression by human input video? Any suggestions, thanks."
can you provided the checkpoint,itsyoavshalev/Image-Animation-with-Perturbed-Masks,2022-06-29 08:33:12,0,,5,1288361783,can you provided the checkpoint？？
can you provided the checkpoint??,itsyoavshalev/Image-Animation-with-Perturbed-Masks,2022-06-18 09:05:54,0,,4,1275739314,
Question about the code,shiyuanh/tane,2022-08-05 17:38:29,0,,2,1330199278,"Great work!
I would like to be able to cite your paper in my own work. However, I have encountered some confusion while reading your code. 
1. Equation 6 in the paper does not seem to appear in the code, but instead is a[ calibration module](https://github.com/shiyuanh/TANE/blob/fe089d477ba9387dd2d008d8872cdc84f8ad77ed/architectures/AttnClassifier.py#L19) that also uses the attention mechanism.
2. Using the example of training command for 5-way 1-shot FSOR does not seem to yield the correct ATT, ATT-G or SEMAN-G results for FSOR. According to the description in the bottom left corner of page 4 of the paper, the agg parameter should be set to mlp instead of avg.
3. The current code seems to support only a single negative class estimate.

Looking forward to your reply, best wishes!"
Release of the code?,shiyuanh/tane,2022-04-07 06:32:57,0,,1,1195581324,Hey there! Will you release the code soon? Thanks!
ModuleNotFoundError: No module named 'transformers',Vision-CAIR/VisualGPT,2022-06-22 21:00:51,0,,11,1280897669,"I fount the above error while runing the shell command in colab.

`!python /content/VisualGPT/train_visualGPT.py --batch_size 50 --head 12 --tau 0.2 --features_path /content/drive/MyDrive/coco_detections.hdf5 --annotation_folder /content/annotations --lr 1e-4 --gpt_model_type gpt --random_seed 42 --log_file logs/log --exp_name experiment_log --lr 1e-4 --decoder_layer 12 --optimizer_type adamw  --gradient_accumulation_steps 2 --train_percentage 0.001 --split_train_data`"
loss_kd_neg计算的意义,HikariTJU/LD,2022-09-27 08:24:55,3,,52,1387337583,"https://github.com/HikariTJU/LD/blob/2dda5c043e733c7cc40f6ce41fb37d0e76c44eeb/mmdet/models/dense_heads/ld_head.py#L267 

请教下 这一项loss恒为0，为什么还要计算呢
"
关于Two-Stage Detector Distillation 的实现问题,HikariTJU/LD,2022-09-18 21:58:41,1,,51,1377189472,"**Describe the issue**
您好，感谢您的很棒的工作。我发现在detector/中您包含了two-stage-kd.py 文件，而且在head 下也有rpn_gfl_head.py， 请问已经实现了LD在two-stage detector上的应用嘛？如果可以的话，是否可以share一下对应的config 文件或者解答如何使用您提供的内容实现two-stage detector的蒸馏，比如Faster-Rcnn。十分感谢！



"
关于回归的变量离散化选择,HikariTJU/LD,2022-09-18 18:33:03,1,,50,1377141207,"您好，感谢您的很棒的工作。我想将LD用到其他检测器中，关于把regression换成离散化的概率这部分有一个疑惑想要问问作者。我看检测器每一个scale得到的regression都通过相同的`integral`将概率转化成lrtb值，那么得到的lrtb都是一样的。所以最终不同feature scale的lrtb范围都是相同的，为什么不把每个scale的lrtb按照比例放缩范围呢？以及如果feature比较大，这样的实现得到的lrtb范围是否有可能小于GT lrtb的范围呢？

https://github.com/HikariTJU/LD/blob/2dda5c043e733c7cc40f6ce41fb37d0e76c44eeb/mmdet/models/dense_heads/ld_head.py#L200"
请问是否可以使用其他预训练教师模型,HikariTJU/LD,2022-09-12 04:19:59,3,,48,1369255753,"我用FCOS-GFL-R101的配置预训练了一个教师模型，并用教师训练了学生模型，基本达到了您论文的结果。
我想尝试使用其他的预训练模型训练教师，不知道可不可以，另外代码是不是需要修改某些部分，麻烦帮忙看一下，谢谢。

我对 LD/configs/ld/ld_r50_fcos_r101_1x.py 进行了更改，
teacher_config='configs/fcos/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco.py',
teacher_ckpt='configs/ld/fcos_r101_caffe_fpn_gn-head_mstrain_640-800_2x_coco-511424d6.pth'
并下载了mmdetection提供的预训练模型。"
Where is definition of `LocalizationDistillationLoss`?,HikariTJU/LD,2022-09-07 10:59:13,4,,47,1364485099,"For something like `self.loss_ld = build_loss(dict(type='LocalizationDistillationLoss'...))`
I seem not to find the class definition of it in this codebase..."
AP landscape,HikariTJU/LD,2022-08-29 15:19:15,2,,45,1354477924,Exciting and excellent work! We are really interested in your AP landscape. Will you released the code or pseudo-code for testing AP landscape？
config问题,HikariTJU/LD,2022-08-19 03:16:30,6,,44,1343881384,文章Table4和Table5所述的LD应该是指Main KD + Main LD + VLR LD吧，我看config文件里ld_r18_gflv1_r101_fpn_coco_1x.py只应用了Main LD，但在ld_r50_gflv1_r101_fpn_coco_1x.py中应用了Main KD + Main LD + VLR LD。其他的config需要自己配置编写对吧。谢谢！
关于分类蒸馏的问题,HikariTJU/LD,2022-08-01 10:45:21,2,,43,1324212029,想问一下这里是吧多个二分类sigmoid输出转化为softmax下的了吗？这样是否不考虑背景分类的蒸馏
TypeError: KnowledgeDistillationSingleStageDetector: __init__() got an unexpected keyword argument 'output_feature',HikariTJU/LD,2022-07-01 09:32:10,7,,40,1291161463,"![G{R_Y4HJ 5%)AXBWHK%DKAB](https://user-images.githubusercontent.com/108521153/176868044-d13440d8-bffc-4bc3-9ba1-475253471e68.png)
训练的时候遇到了问题，很抱歉打扰您，实在不知道该怎么办"
请问LD中对预测框的概率分布的回归是用的类似GFocal中一个卷积层进行学习的吗,HikariTJU/LD,2022-06-22 07:04:32,2,,39,1279659934,
如何对yolox进行知识蒸馏,HikariTJU/LD,2022-05-31 13:13:20,5,,38,1253851997,我看config里面没有yolox的，如果我想对yolox进行蒸馏是不是需要仿照着写一个配置文件
How can I train my own VOC datasets?,HikariTJU/LD,2022-05-12 06:33:36,16,,36,1233511757,"我想尝试你们的方法在我自己数据集的效果，但是我遇到了很多问题。
1、我自己的数据集只有两类，不能使用提供的预训练模型
2、我想使用gfl_r101_fpn_voc.py重新训练一个教师网络，遇到错误如下所示。
![1652337053(1)](https://user-images.githubusercontent.com/68552295/168006325-68c265bc-1fcc-4278-a560-f75301d7f8a2.png)
另外，add new datasets的链接是失效的。感谢解答！"
Whiat TOOL of Feature Visualization Do YOU USE?,HikariTJU/LD,2022-05-10 03:30:21,1,,35,1230558461,"![WeChat Image_20220510112741](https://user-images.githubusercontent.com/68552295/167536918-69a60cb6-c132-4fca-b1aa-8254c0145ef3.png)
您好，请问您上面的特征图可视化工具使用的是什么呀？十分感谢"
关于GI(General Instance) Feature-based Distillation 实现的问题,HikariTJU/LD,2022-05-01 15:17:51,8,,33,1222189984,"我注意到您代码里的实现：
idx_out = torch.ops.torchvision.nms(gibox, giscore, 0.3)[:10]
return idx_out


gi_idx = self.get_gi_region(soft_label, cls_score, anchors,
                                        bbox_pred, soft_targets, stride)
gi_teacher = teacher_x[gi_idx]
gi_student = x[gi_idx]
loss_im = self.loss_im(gi_student, gi_teacher)

并没有像GI 论文里那样使用GIbox  ROIAlign 进行FM。

这样的话只选了10个FPN像素进行FM，而且经过NMS  这10个GI box像素附近的点也大概率被抑制掉了，这些像素应该也是信息量比较大的像素。

请问你们这样实现是为什么呢？有试过原文的做法吗，效果怎么样？"
关于ld_loss计算的问题,HikariTJU/LD,2022-04-21 03:11:13,1,,29,1210438664,"实现中ld_loss没有除以avg_factor
如果batch_size比较大的话，ld_loss会波动很大
我训练的时候ld_loss会从刚开始1左右上升到10多，又慢慢下降。

我看您推荐的batch_size是2，如果要batch_size比较大该怎么设置呢？把ld_loss的权重 0.25 除以batchsize*2?
#28 #7 "
关于GI(General Instance)论文的实现问题,HikariTJU/LD,2022-04-06 02:10:04,2,,25,1193918423,您好，在看您论文和代码的时候，发现您实现了GI(General Instance)，但是我好像只找到了特征蒸馏部分，没有找到response-based蒸馏和relation-based蒸馏，请问您有实现这两个部分吗？如果有，方便提供一下吗？非常感谢。
bug while running the sample model training,HikariTJU/LD,2022-02-03 08:49:48,2,,21,1122807493,"Hello again,
I tried to run the training code that is provided in the Readme.md file using 1 gpu. 

`./tools/dist_train.sh configs/ld/ld_r50_gflv1_r101_fpn_coco_1x.py 1`

The only modification I did in the config file, is to precise that I want to run the program for one epoch: 

`runner = dict(
            type='EpochBasedRunner',
            max_epochs= 1
            )
`

I am getting an error while trying to save the checkpoint after the training. This is the complete bug: 

```
2022-02-03 02:45:24,914 - mmdet - INFO - Epoch [1][58550/58633] lr: 2.500e-03, eta: 0:00:46, time: 0.5
58, data_time: 0.004, memory: 4122, loss_cls: 0.7503, loss_bbox: 0.3822, loss_dfl: 0.2494, loss_ld: 0.
2703, loss_ld_vlr: 0.4174, loss_kd: 0.2851, loss_kd_neg: 0.0000, loss_im: 0.3847, loss: 2.7392
2022-02-03 02:45:52,665 - mmdet - INFO - Epoch [1][58600/58633] lr: 2.500e-03, eta: 0:00:18, time: 0.5
55, data_time: 0.004, memory: 4122, loss_cls: 0.7615, loss_bbox: 0.3864, loss_dfl: 0.2457, loss_ld: 0.
2216, loss_ld_vlr: 0.3467, loss_kd: 0.2940, loss_kd_neg: 0.0000, loss_im: 0.3496, loss: 2.6055
2022-02-03 02:46:16,693 - mmdet - INFO - Saving checkpoint at 1 epochs
[                                                  ] 0/5000, elapsed: 0s, ETA:Traceback (most recent c
all last):
  File ""./tools/train.py"", line 187, in <module>
    main()
  File ""./tools/train.py"", line 183, in main
    meta=meta)
  File ""/home/edouard/eden/work/codes/LD/mmdet/apis/train.py"", line 170, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py""
, line 125, in run
    epoch_runner(data_loaders[i], **kwargs)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py""
, line 54, in train
    self.call_hook('after_train_epoch')
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/mmcv/runner/base_runner.py"", line 
308, in call_hook
    getattr(hook, fn_name)(self)
  File ""/home/edouard/eden/work/codes/LD/mmdet/core/evaluation/eval_hooks.py"", line 276, in after_trai
n_epoch
    gpu_collect=self.gpu_collect)
  File ""/home/edouard/eden/work/codes/LD/mmdet/apis/test.py"", line 97, in multi_gpu_test
    result = model(return_loss=False, rescale=True, **data)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 
550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/torch/nn/parallel/distributed.py"", line 458, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py"", line 84, in new_func
    return old_func(*args, **kwargs)
  File ""/home/edouard/eden/work/codes/LD/mmdet/models/detectors/base.py"", line 183, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File ""/home/edouard/eden/work/codes/LD/mmdet/models/detectors/base.py"", line 160, in forward_test
    return self.simple_test(imgs[0], img_metas[0], **kwargs)
  File ""/home/edouard/eden/work/codes/LD/mmdet/models/detectors/single_stage.py"", line 120, in simple_test
    *outs, img_metas, rescale=rescale)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/mmcv/runner/fp16_utils.py"", line 164, in new_func
    return old_func(*args, **kwargs)
      File ""/home/edouard/eden/work/codes/LD/mmdet/models/dense_heads/anchor_head.py"", line 583, in get_bboxes
    scale_factors, cfg, rescale)
  File ""/home/edouard/eden/work/codes/LD/mmdet/models/dense_heads/gfl_head.py"", line 560, in _get_bboxes
    cfg.max_per_img)
  File ""/home/edouard/eden/work/codes/LD/mmdet/core/post_processing/bbox_nms.py"", line 187, in multiclass_nms
    return dets, labels[keep]
IndexError: index 8663 is out of bounds for dimension 0 with size 100
Traceback (most recent call last):
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/torch/distributed/launch.py"", line 263, in <module>
    main()
  File ""/home/edouard/anaconda3/envs/LD/lib/python3.7/site-packages/torch/distributed/launch.py"", line 259, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/home/edouard/anaconda3/envs/LD/bin/python', '-u', './tools/train.py', '--local_rank=0', 'configs/ld/ld_r50_gflv1_r101_fpn_coco_1x.py', '--launcher', 'pytorch']' returned non-zero exit status 1.
```

PS. all is installed as recommended in the readme file.

Thank you very much for you help 
"
文件缺失,HikariTJU/LD,2021-10-03 14:16:54,2,,15,1014409869,能提供一下gfl_r101_fpn_1x_voc.py文件吗？
question on the model settings in Tab. 5,xingyizhou/UniDet,2022-07-14 08:45:27,1,,19,1304463059,"Hi Xingyi, thanks for your wonderful work! I have a question regarding the model setting, in Tab. 5, for the `naive merge`, how is the merge conducted? and for the `retrained`, in my understanding, the box labels of multiple datasets are converted to the unified space and used for training a unified model, but still, the objects in one dataset may not be labeled in another, so we could still face such background confusion in supervision? is my understanding correct? 

Thanks in advance!"
Custom labels,xingyizhou/UniDet,2022-06-08 12:39:34,0,,18,1264680539,"Hi

Thanks for releasing UniDet!

Is there a way to specify custom labels similar to Detic (using clip) ?

Thanks,
Subham"
Question about RPN,xingyizhou/UniDet,2022-04-25 07:47:41,1,documentation,17,1214108618,"Hello, I am very interested in your work, but I have a question to ask you. In the RPN stage, a proposal may be a positive sample in dataset A but a negative sample in dataset B. Will this produce ambiguity? Thanks！
"
Mapillary object detection dataset,xingyizhou/UniDet,2022-02-28 14:50:38,0,documentation,16,1154184162,"Thank you for that great work. I would like to know which dataset of mapillary datasets has you used to train your model. I have check the website and I am not sure which one contains the object detection bounding boxes.

Could you please provide a link to download the dataset.

"
Why some labels are duplicated in the unified detector?,xingyizhou/UniDet,2021-11-22 06:31:21,1,documentation,14,1059748540,"For example, nightstand appears twice in the labels file `learned_mAP.csv`:
```
line255: _nightstand_,,,nightstand,
line693: Nightstand__,/m/02z51p,Nightstand,,
```
But some labels from different datasets are normalized to one label. Why?
Is it due to the `When there are different label granularities, we
keep them all in our label-space, and expect to predict all of
them` in the paper?"
import error 'FastRCNNOutputs' from 'detectron2.modeling.roi_heads.fast_rcnn'  ,xingyizhou/UniDet,2021-11-17 05:43:46,2,,13,1055728684,"```
$ python projects/UniDet/demo/demo.py --config-file projects/UniDet/configs/Unified_learned_OCI_R50_6x.yaml --input images/34501842524_3c858b3080_k.jpg --opts MODEL.WEIGHTS models/Unified_learned_OCI_R50_6x.pth
Traceback (most recent call last):
  File ""projects/UniDet/demo/demo.py"", line 18, in <module>
    from unidet.config import add_unidet_config
  File ""projects/UniDet/unidet/__init__.py"", line 17, in <module>
    from .modeling.roi_heads.custom_roi_heads import CustomROIHeads, CustomCascadeROIHeads
  File ""projects/UniDet/unidet/modeling/roi_heads/custom_roi_heads.py"", line 22, in <module>
    from .custom_fast_rcnn import CustomFastRCNNOutputLayers
  File ""projects/UniDet/unidet/modeling/roi_heads/custom_fast_rcnn.py"", line 16, in <module>
    from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs
ImportError: cannot import name 'FastRCNNOutputs' from 'detectron2.modeling.roi_heads.fast_rcnn' (/ProjectRoot/pyprojects/venv/test2/lib/python3.7/site-packages/detectron2/modeling/roi_heads/fast_rcnn.py)
```

I install detectron2 v0.6 built from `https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html` . The reason is detectron2 change its codebase, while ours is older one. So i follow the `https://issueexplorer.com/issue/facebookresearch/unbiased-teacher/44` to fix the bug perfectly. I hope this can help someone who faces it."
Does the model use data augmentation strategies when training?,xingyizhou/UniDet,2021-09-17 06:48:48,1,enhancement,10,999006019,"Thank you for your hard work and spiritual welfare. Does the model use data augmentation strategies when training? How to increase the data augmentation strategy?

Thank you very much."
"[Learning a unified label space]: FileNotFoundError: [Errno 2] No such file or directory for ""json.load(open(unified_label_file, 'r'))""",xingyizhou/UniDet,2021-07-24 22:25:07,3,documentation,9,952157477,"Hi,

thank you for this awesome idea on this nice multi-dataset-object-detectror.

I've trained a ""Partitioned detector"" on coco, oid and obejcts365-V2. (for V2, because MEGVII have updated their dataset, we could not download v1 anymore.)

When I tried the second step: Learning a unified label space:
I followed the tutorial and run:

python projects/UniDet/train_net.py  \\
--config-file projects/UniDet/Partitioned_COI_R50_2x.yaml  \\
--num-gpus 1 \\
--eval-only MULTI_DATASET.UNIFIED_EVAL True

It raised a ""FileNotFoundError"". When  I checked config file, I found that the key ""UNIFIED_LABEL_FILE"" in the config file has been set as ' ' (None), as shown in images below.

Could you give me some hints about how could I generate this this file or label space?

many thanks!

FileNotFoundError:


![fileNotFoundErroe_unified_label](https://user-images.githubusercontent.com/87916250/126882321-13a1b4ab-5cdb-446c-8ec2-8002132f5264.jpg)

config.yaml:


![UNIFIELD_LABEL_FILE](https://user-images.githubusercontent.com/87916250/126882627-02ca1e42-3e50-496c-b8c3-b33aa41f7c4e.png)
"
Cannot import SplAtConv2d,xingyizhou/UniDet,2021-06-29 09:02:04,3,documentation,8,932391750,"Hi Xingyi,

Thanks for sharing your models. However, when I tried to run inferences with model 'Unified_learned_OCIM_RS200_6x+2x', I got an exception on 'from .splat import SplAtConv2d'. As I cannot find 'splat' in the codebase, I wonder if the splat.py file is missing or I missed something when installing the dependencies?

Wenchang"
How to display label name like the paper? objects365 - Dog  like this,xingyizhou/UniDet,2021-06-10 11:09:55,1,documentation,7,917219503,"## 📚 Documentation

* Links to the relevant documentation/comment:
I want to detect a lot of objects and need to get specific category names. plz tell me how to get that"
how to deal with machine-generated labels of openimages ?,xingyizhou/UniDet,2021-06-08 01:52:21,5,documentation,6,914147039,how to deal with machine-generated labels of openimages ?
AssertionError: Attribute 'thing_classes' in the metadata of 'objects365_train' cannot be set to a different value!,xingyizhou/UniDet,2021-06-02 08:07:40,2,documentation,5,909237390,"## 📚 Documentation
I arrange the dataset as guided in the documentation, but get this error when trying to train split detectors.
* Links to the relevant documentation/comment:
"
AttributeError: 'CustomFastRCNNOutputs' object has no attribute '_log_accuracy',xingyizhou/UniDet,2021-04-19 11:18:31,3,,4,861200497,"AttributeError: 'CustomFastRCNNOutputs' object has no attribute '_log_accuracy'
when i run train ""python projects/UniDet/train_net.py --config-fil│
e projects/UniDet/configs/Base-CRCNN-COCO.yaml --num-gpus 8""
"
Loss without gradients,xingyizhou/UniDet,2021-03-24 08:41:36,0,documentation,3,839487584,"Great work!
I don't really understand how in the unified version you apply the loss on all the detection heads, even when there are no gradients because we didn't pass the input through them. What am I missing and where can I find it in the code?"
Convert annotations of OpenImages to COCO format,xingyizhou/UniDet,2021-03-08 09:56:00,7,documentation,2,824392580,"Hi Xingyi,

Thanks for you wonderful and easy to use work! I have prepare OpenImages dataset.

However, when I use projects/UniDet/tools/convert_datasets/convert_oid.py to convert training annotations.

There is an error:
sets train
converting train data
Traceback (most recent call last):
  File ""projects/UniDet/tools/convert_datasets/convert_oid.py"", line 276, in <module>
    original_image_annotations = csvread(os.path.join(base_dir, 'annotations', image_label_sourcefile))
NameError: name 'image_label_sourcefile' is not defined

'image_label_sourcefile' is not defined when converting training annotations, I don't know how to solve it?
Could you give me the solution?
Thanks

* Links to the relevant documentation/comment:
https://github.com/xingyizhou/UniDet/blob/master/projects/UniDet/unidet_docs/DATASETS.md"
camera setting,facebookresearch/neural_3d_video,2022-09-18 08:57:09,0,,20,1376990156,"Hi. We want to build a camera system with similar settings. The question is, is the accessory-like object on top of each GoPro in the picture part of the timecode synchronizer system? We would appreciate it if you could tell us what method you used to match the timecode. 
And in the learning process, was there a purpose for the central camera to look like Azure Kinect? Or is it something that can be omitted when building a dataset? We want to shoot with a dslr camera instead of a gopro. I wonder if this difference in picture quality will affect the picture quality of the result."
reduce batch size or number of iterations when using smaller images?,facebookresearch/neural_3d_video,2022-09-08 07:39:30,2,,19,1365702029,"In order to train a dynerf in a reasonable amount of time on my hardware (nvidia 3090) I assume that I  should reduce the image size and reduce either the number of iterations or the batch size (number of rays per iteration). I can fit about 2500 rays on my GPU, but I can accumulate gradients to make a larger effective batch size if needed.
So, tu put it clear, I would want to train in about 4 days using your dataset with only one 3090, so I reduced the images to 333x250 pixels and then I can:

- reduce batch size to 2500 -3000 and maintain the number of iterations at 300k +250k +100k as explained in the supplementary material
- Reduce the number of iterations to approx 35k + 30k +12k but maintain the batch size at approx 24k

Can you provide some insight on what would work better? Thanks in advance!"
About latent code?,facebookresearch/neural_3d_video,2022-08-27 04:15:19,2,,14,1352935984,"I would like to ask about the specific implementation details of the latent code, what network did he use to get it?"
How exactly does importance sampling works?,facebookresearch/neural_3d_video,2022-07-21 13:17:07,0,,9,1313246454,"I have 2 doubts regarding importance sampling.
The first one is  about **how much rays do you pick per iteration**. If i understood it correctly, **you would pick as many rays as there are pixels for a given frame (accounting for all the views**), as a regular nerf would do, but sampling according to the generated weights instead of simply shuffling all the rays. So, to make it clear, if there are 10 views and each view is 100x100 pixels, a nerf would pick all 100x100x10 rays in a random order and DyNeRF instead would pick 100x100x10 rays (the same amount) with each ray having a **probability of being picked proportional to its weight** (computed using global median or temporal difference). Is this right?

The second one is about temporal difference. If we have 300 frames and we use a frame difference of 25, for a frame at time _T_ we would **measure the difference** between that and the one at _T+25_ and compute the weight for **frame _T_**. But what about **frames from 275 to 300?**

Thanks in advance, any help you can provide would be greatly appreciated!"
How do I find the synced frames in the dataset,facebookresearch/neural_3d_video,2022-07-11 22:20:32,8,,5,1301300896,"Are datasets synced frame by frames? If not, How do I align the synced frames across the videos? 
I extracted the first frames from a set (coffee_martini). They don't look synced. 

![cam00](https://user-images.githubusercontent.com/56043227/178368300-9ad46df9-53af-4651-83d4-c54ff6131c70.png)
![cam13](https://user-images.githubusercontent.com/56043227/178368324-7af23668-3962-48c3-8672-f4ec55bbaba5.png)

"
[abstract_nas] how to reuse received chekpoints,google-research/google-research,2022-11-06 14:44:35,0,,1336,1437428590,"Hello, I'm searching with abstract Nas and get some interesting results, after searching I receive directory with data for every generation and `checkpoint.npz` and config files. How can I reuse this data to get tensorflow model back?"
[jax_nerf] ndc code,google-research/google-research,2022-11-04 12:14:23,0,,1335,1435986895,"could you help explain following code?
https://github.com/google-research/google-research/blob/9b84ad0a60e4fdff0b52b7e3581f9208478597eb/jaxnerf/nerf/datasets.py#L45"
Alistar,google-research/google-research,2022-11-02 13:15:34,0,,1333,1433130220,
[Clay] How to run the project to denoise the Rico dataset,google-research/google-research,2022-11-01 19:01:21,0,,1332,1431933338,"Hi! Thanks a lot for releasing the code and Clay dataset to denoise the Rico dataset.
However, I don't know how to run the project in the terminal of my laptop (MacBook). Could you please provide an example of shell command and the python environment (pip packages versions) required for the project?
Thank you again!"
[jax_dft] colab demo expired,google-research/google-research,2022-11-01 01:45:25,0,,1331,1430726279,"I try to reproduce the result of PRL paper using the colab [demo](https://colab.research.google.com/github/google-research/google-research/blob/master/jax_dft/examples/training_neural_xc_functional.ipynb), but at certain import code block, the jaxlib and cuda version can't match.

Just re-run the colab script from the beginning to reproduce the error."
Si,google-research/google-research,2022-10-29 22:03:39,0,,1330,1428475535,
[layout-blt] may need the split rate of train/val/test for Rico & Publaynet dataset to reproduce the BLT results,google-research/google-research,2022-10-27 01:25:42,0,,1329,1424890893,"Thanks for releasing codes and the interesting paper a lot! 
I follow the paper to keep the max element number as 25 in Rico, and 22 in Publaynet. While there is no split rate given in paper, so I split the Rico with 0.85:0.05:0.10 for train/val/test.  I re-train the BLT model with this data, but the metrics results calculated by the code in utils/metrics.py seems a little strange, especially for Overlap. So I'm here to ask for help. Thanks for your attention~
![image](https://user-images.githubusercontent.com/44337029/198168495-1277c861-9fab-4463-bb00-d5ab2fbc473d.png)
![image](https://user-images.githubusercontent.com/44337029/198168791-cab44fb6-4b83-44a7-b2ec-f6a8b0c83436.png)

"
#Photochat dataset questions:,google-research/google-research,2022-10-22 15:42:01,2,,1328,1419369995,"Hey, I am a student do some researches on VD. I noticed that you've proposed a great dataset called PhotoChat for multi-model  dialog generative system.
However, it seems that both the url provided and id are invalid for me now. Could you give me some tips how can I solve it without download the whole image challenge dataset with is 500G+(too expensive for me).
```
train/0a5e922c15ffb08d https://c6.staticflickr.com/1/156/400565484_fdfaa551e7_o.jpg
train/048fe12fa832143f https://c8.staticflickr.com/1/29/96515447_4ac8050343_o.jpg
train/080a8a8692a66687 https://farm8.staticflickr.com/6082/6042280582_c7a5d88b51_o.jpg
train/07f372b1aba8b165 https://farm6.staticflickr.com/7416/9782494465_1f1a8a315f_o.jpg
train/a593754ce22e87da https://c3.staticflickr.com/4/3488/3697088360_5b402b5c66_o.jpg
train/0617ddfd18b1343f https://c8.staticflickr.com/6/5565/15132241305_540a42cf78_o.jpg
```
@XiaoxueZang 
Best regards,
Zeyu Li"
Quantized embedding,google-research/google-research,2022-10-17 13:19:40,0,,1325,1411571893,Is it possible to get quantized embedding from scann library after calibration? 
[Poem] Partially-Visible Pose Retrieval Model Checkpoint,google-research/google-research,2022-10-17 10:59:04,0,,1324,1411367150,"Hello,
Is it possible to get the model checkpoints from dedicated models that were trained for each visibility pattern with synthetic occlusions using different keypoint occlusion augmentation training strategies (as stated in section 5.5 of the paper)?"
ul2 Token is unknown(UNK).,google-research/google-research,2022-10-16 16:27:06,0,,1323,1410540799,"```
input_string = ""[NLU] 吾輩は猫である <extra_id_0>""                                               
inputs = tokenizer(input_string, return_tensors=""pt"", add_special_tokens=False).input_ids.to(""cuda"")
outputs = model.generate(inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
```
↓
<pad><extra_id_0> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>. <unk> <unk>..<unk>..<unk>..<unk>..<unk>..<unk>..<extra_id_1>..</s>
"
[Scann]A bug in scann.scann_ops_pybind.load_searcher(self.searcher_savedir),google-research/google-research,2022-10-16 05:11:31,0,,1322,1410376570,"I have built my searcher, but i have no idea why it can not run when using scann.scann_ops_pybind.load_searcher(self.searcher_savedir)

![image](https://user-images.githubusercontent.com/101795593/196019259-d65ce558-90c6-4721-bef5-32162fa0fa8c.png)


what mean is **'parse_npy_header: failed to find header keyword: 'fortran_order''**"
DMON-performance,google-research/google-research,2022-10-14 18:17:24,0,,1320,1409697161,"I experimented with the DMoN (path: graph_embedding/dmon) with several datasets, which are the focus of the DMoN's publication. Especially with Amazon PC and Amazon Photo datasets, I get far worse results than the mentioned results, while other state-of-the-art models show satisfiable performance. Does anyone get similar results with me? Or can anyone manage to get similar results as the reference paper? Thanks in advance!"
[layout-bert] can't run train script any data type,google-research/google-research,2022-10-13 00:51:50,0,,1319,1406979087,"Thanks for the very interesting paper.

I try to train script, but got an error.

```
main.py --config configs/bert_layout_publaynet_config.py --workdir exp

Traceback (most recent call last):
  File ""main.py"", line 87, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 77, in main
    trainer.train()
  File ""*/layout-blt/trainers/base_trainer.py"", line 255, in train
    train_ds, eval_ds, _, vocab_size, pos_info = input_pipeline.get_all_dataset(
  File ""*/layout-blt/input_pipeline.py"", line 149, in get_all_dataset
    train_ds, vocab_size, pos_info = get_dataset(batch_size, dataset_folder,
  File ""*/layout-blt/input_pipeline.py"", line 113, in get_dataset
    dataset = LayoutDataset(dataset_name, ds_path, add_bos, shuffle)
  File ""*/layout-blt/input_pipeline.py"", line 192, in __init__
    self.data = _normalize_entries(data, shuffle)
  File ""*/layout-blt/input_pipeline.py"", line 65, in _normalize_entries
    children = document[""children""]
TypeError: string indices must be integers

```

There is no key named children in publaynet. Are there any plans to provide code to generate train/val/test.json appropriate for each data?
"
TypeError: Can't call __hash__ on modules that hold variables.,google-research/google-research,2022-10-10 11:28:58,1,,1318,1403020978,"Hi 
I'm trying to run the [colab](https://colab.research.google.com/drive/1TjCWS2_Q0HJKdi9wA2OSY7avmFUQYGje?usp=sharing) notebook mentioned in porject page, for dreamfields.
But I am getting the below error, while running training step. What can be done to overcome it?

![image](https://user-images.githubusercontent.com/110157037/194856242-729cbf85-2055-45ad-81d8-e5f6f0262ab7.png)
"
[RegNeRF] LPIPS metric,google-research/google-research,2022-10-07 20:02:42,0,,1316,1401615753,"Thank you for your amazing work and I successfully reproduced the results of RegNeRF.
However, I kept trying to reproduce the LPIPS results but could not made the same or similar results of the paper.
Could you share the code for LPIPS evaluation?

Thank you in advance !"
[ScaNN] Question about ScaNN builder according to distance_measure,google-research/google-research,2022-10-07 05:01:33,0,,1315,1400622726,"scann.scann_ops_pybind.builder(db, num_neighbors, **distance_measure**)

There are dot_product and squared_l2 options for distance_measure, and I ask how quantization, clustering, ah, search and etc are performed according to each option.

I can't find any related documents..."
[smurf] 'Heavy augmentation' settings,google-research/google-research,2022-10-06 19:26:41,0,,1314,1400193750,"It seems like the 'heavy augmentation' settings have not been shared. 

The following probabilities for augmentations are set to zero:
probability_relative_scale = 0.,
probability_rotation = 0.0,
probability_relative_rotation = 0.0,
probability_crop_offset = 0.0,

Does this mean that these augmentations were not used for the results mentioned in the final paper?"
"ERROR: raise RuntimeError(f""Backend '{platform}' failed to initialize: """,google-research/google-research,2022-10-06 00:42:13,0,,1312,1398553463,"https://colab.research.google.com/drive/1208hU7pVYW3iG8nPKDoZU2aSssaJfWp1?authuser=4#scrollTo=6uQJRMw-NLKQ

hello, I get this error when executing, I did the previous steps correctly, I'm a newbie in this, I attach a screenshot

[https://ibb.co/dQF956g](url)"
ERROR:,google-research/google-research,2022-10-06 00:39:22,0,,1311,1398551195,
question about removal flare,google-research/google-research,2022-10-04 13:07:11,0,,1309,1396248215,"亲爱的作者，您好，不知道您能不能把数据集放在中国一份，目前我不知道可以用什么方法打开Google数据集。
"
工学計算,google-research/google-research,2022-10-01 13:00:11,0,,1308,1393392879,
dataset for BLT - Layout,google-research/google-research,2022-09-26 22:09:38,0,,1304,1386787781,"I  couldn't find the code for generating the dataset format used in this paper. Would you mind share an example?
Thanks in advance and congrats for the excellent work!

Flavio."
AttributeError: module 'jaxlib.pocketfft' has no attribute 'pocketfft',google-research/google-research,2022-09-23 19:05:35,0,,1292,1384198514,"After installing the requirements.txt, I tried to run the code , but failed with the error ""AttributeError: module 'jaxlib.pocketfft' has no attribute 'pocketfft'"". How can I install pocketfft ?"
How can I calculate the optical flow of my own video,google-research/google-research,2022-09-19 08:29:42,0,,1290,1377592698,"I calculated the optical flow of the billiard_clip.mp4 as described in the run.sh file, but when I'm running my own video, can I just change the code inside convert_video_to_dataset_test.py to generate a TFRecord file? How can I modify the running code to calculate the optical flow in my own video?"
[kws_streaming] export stream_state_internal savedmodel stuck at  keras.engine.functional.reconstruct_from_config,google-research/google-research,2022-09-15 08:04:54,1,,1280,1374107141,"system: ubuntu 20.04
tensorflow:  2.9.1

Based on the [experiments/](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments) , I have success in train & export saved model of non_stream .
Tf2.9.1 is the only version I tried can train the models with some little fixed , I can't find  tf_nightly-2.3.0.dev20200515 in the [experiments/](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments)  。
But when i tried to export saved model of stream_state_internal.
I found it stuck at [reconstruct_from_config](https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/python/keras/engine/functional.py#L1114) , the node_index_map is  always empty ,so node_index_map return always None. It keeps add node into unprocessed_nodes.
I print some variables .
The unprocessed_nodes is 
```
{<kws_streaming.layers.speech_features.SpeechFeatures object at 0x7ff1804fb730>:[[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056d940>]], 
<kws_streaming.layers.lstm.LSTM object at 0x7ff1804fbbb0>: 
[[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056dc40>]], 
<kws_streaming.layers.stream.Stream object at 0x7ff1804fbb20>:
 [[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056dc10>]], 
<keras.layers.regularization.dropout.Dropout object at 0x7ff180229e50>:
 [[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056d400>]], 
<keras.layers.core.dense.Dense object at 0x7ff18069bf40>: 
[[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056d430>]]}
```

And my export script is:
```
python -m kws_streaming.train.model_train_eval --data_url '' \
--data_dir kws_streaming/data2 \
--train_dir  kws_streaming/lstm_peep/ \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 20 \
--resample 0.15 \
--alsologtostderr \
--train 0 \
--lr_schedule 'exp' \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 10 \
--frequency_masks_number 2 \
--frequency_mask_max_size 5 \
lstm \
--lstm_units 500 \
--return_sequences 0 \
--use_peepholes 1 \
--num_proj 200 \
--dropout1 0.3 \
--units1 '' \
--act1 '' \
--stateful 0
```

How to solve it ?
Thanks!

"
[Poem] Clarification for Video Alignment,google-research/google-research,2022-09-14 20:01:57,0,,1279,1373518214,"Hello,
I have question about the description in the sectioni 4.3 of the paper
""In all the following experiments in this section, we compute our Pr-VIPE embeddings on single video frames and use the negative logarithm of the matching probability (7) as the distance between two frames. Then we apply temporal averaging within an atrous kernel of size 7 and rate 3 around the two center frames and use this averaged distance as the frame matching distance. Given the matching distance, we use standard dynamic time warping (DTW) algorithm to align two action sequences by minimizing the sum of frame matching distances.""


Why we need matching distance as input for DTW to align two video sequences? Do you mean the distance function used in DTW is different from the default( L2 distance), and we should use matching distance function you describe above?
 


And also in section 4.3.2 
""We measure the alignment quality of our embeddings quantitatively using Kendall’s Tau [12], which reflects how well an embedding model can be applied to align unseen sequences if we use nearest neighbor in the embedding space to match frames for video pairs.""

As far as I understand, you use KNN to align two video sequences, which is somehow different from 4.3 description.


Thank you!


"
Felix - how is the non-autoregressivity implemented?,google-research/google-research,2022-09-13 13:00:08,0,,1277,1371419640,"Hi everyone, 
I have been reading the code of Felix (Flexible text-editing model), but I cannot understand how the non-autoregressivity is implemented. 

Where in the code is it implemented? Does it have to do with the classes BertEncoder and BertPretrainerV2? "
【compositional_rl】code release,google-research/google-research,2022-09-13 11:13:29,2,,1276,1371277571,"When will the code be released, thank you!"
[Reg-NeRF] About the effect of applying geometry regularization in rendering training images,google-research/google-research,2022-09-08 10:21:35,0,,1274,1365986661,"Hi, @yilei , thank you for open-soucing Reg-NeRF!

I am interested in the effect of applying geometry regularization in rendering patches of unobserved viewpoint. May I ask have you tried to apply this regularization term in rendering patches from training images, and how about the effect?

Hope for the reply,
Thanks!"
How can i inference image?,google-research/google-research,2022-09-08 04:01:42,0,,1273,1365491768,May i ask about how to inference some image ?
[UL2] Smaller model checkpoint request,google-research/google-research,2022-09-06 09:31:38,1,,1272,1362991737,"I'd love to use UL2, however the 20B model is far too big for my application. Section 4 of the paper mentions that multiple smaller models were trained. Could we please get access? Just the best performing model that was used as a base design for the 20B model would be fine. Additionally, a model around the size of T5-small would be great."
How to compile scann old version (1.2.6) from source,google-research/google-research,2022-09-05 08:19:22,0,,1271,1361573887,Hi. How i can compile scann 1.2.6 version from sources. Were i can get old versions of scann sources?
How to deal with padding tokens in F-Net implementation,google-research/google-research,2022-09-04 14:24:24,0,,1270,1361164651,"In the F-Net implementation the padding tokens are not considered an issue https://github.com/google-research/google-research/blob/4d0e1709b347e33ac95fb557a180271bf057ee9e/f_net/layers.py#L107. 

However, the logic of the operation is to do an FFT on the hidden dimension, and then the time dimension. padding/zeroes influences the output of the FFT so it feels like this is an oversight and may influence the resultant values? I.e. the same example, in two different batches will have a different FFT outcome, if those batches have a different max sequence length (which everything gets padded to). "
dreamfields' README.md is not matching the Dockerfile.,google-research/google-research,2022-09-04 05:38:48,0,,1269,1361049036,"https://github.com/google-research/google-research/blob/4d0e1709b347e33ac95fb557a180271bf057ee9e/dreamfields/README.md?plain=1#L17

in Dockerfile, it is not `nvcr.io/nvidia/tensorflow:21.11-tf2-py3`, but `nvcr.io/nvidia/tensorflow:21.12-tf2-py3`."
picture is blank,google-research/google-research,2022-09-03 17:12:21,0,,1268,1360917913,"Why does the image after the sampler that is the next step, namely the color step 2 and 3, the resulting image is a blank."
"When will the code be released, thank you!",google-research/google-research,2022-09-03 09:14:33,0,,1267,1360804837,
is there a method to integrate the ScANN into android demo??,google-research/google-research,2022-09-02 09:30:34,0,,1266,1359941944,
Two-phere parameterization of light fields,google-research/google-research,2022-09-01 11:30:01,0,,1265,1358696282,"In ""Light Field Neural Rendering"",
Could you provide the code for the Two-phere parameterization of the light field and the experiments in the 360° scene?"
Your Results in New Super-Resolution Benchmarks,google-research/google-research,2022-08-31 17:59:47,0,,1264,1357675553,"Hello,

[MSU Graphics & Media Lab Video Group](https://videoprocessing.ai/) has recently launched two new Super-Resolution Benchmarks.
* [Video Upscalers Benchmark: Quality Enhancement](https://videoprocessing.ai/benchmarks/video-upscalers.html) determines the best upscaling methods for increasing video resolution and improving visual quality.
* [Super-Resolution for Video Compression benchmark](https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html) aims to test Super-Resolution methods on compressed videos and select the best model for each video codec standard.

Your method achieved 13th place in [Video Upscalers Benchmark: Quality Enhancement](https://videoprocessing.ai/benchmarks/video-upscalers.html) in 'Animation 2x' category and 1st place in [Super-Resolution for Video Compression Benchmark](https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html) in 'VVC compression' category. We congratulate you on your result and look forward to your future work!

We would be grateful for your feedback on our work."
SNERG: How can I run the viewer code?,google-research/google-research,2022-08-30 19:54:30,0,,1263,1356231796,Looking for a way to view the baked model online. I couldn't find any document to use the viewer code. Could you help me on this?
[Multi-Game DT] Can you release the other model checkpoints?,google-research/google-research,2022-08-30 11:53:34,0,,1262,1355647372,"Hi,

It seems like only the 200M parameter model checkpoint is available.

```
tf.io.gfile.listdir(""gs://rl-infra-public/multi_game_dt"")
# ['checkpoint_38274228.pkl']
```

Can you release a checkpoint for the 40M parameter model? (10M parameter model ckpt would be nice as well)"
tensorflow.compat.v1 could not be resolved with Tensorflow 2.8.2,google-research/google-research,2022-08-30 05:06:22,0,,1261,1355172896,"The code below cannot be resolved under Tensorflow 2.8.2
import tensorflow.compat.v1 as tf

"
Export the model as ply or obj ?,google-research/google-research,2022-08-29 16:50:42,0,,1260,1354590272,"I want to used 3D object.
Anyone have an idea about ""How to export the model as Mesh ply or obj format ?""

Thankful for response in advance.
"
Train scann searcher: Killed.,google-research/google-research,2022-08-24 14:26:47,0,,1251,1349535930,"I tried to train a searcher based on the code from [here](https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/scripts/train_searcher.py#L117-L119)

But I got ：
```
Finished loading of retrieval database of length 2616145.
(2616145, 768)
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
Initializing scaNN searcher with the following values:
k: 20
metric: dot_product
reorder_k: 40
anisotropic_quantization_threshold: 0.2
dims_per_block: 2
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
Start training searcher....
N samples in pool is 2616145
Using using partioning, asymmetric hashing search and reordering.
Partitioning params:
num_leaves: 1617
num_leaves_to_search: 80
[libprotobuf WARNING external/com_google_protobuf/src/google/protobuf/text_format.cc:339] Warning parsing text-format research_scann.ScannConfig: 43:11: text format contains deprecated field ""min_cluster_size""
Killed
```
When I reduce the dataszie to, say `(616145, 768)`, it works.
How can I train searcher using all the embeddings?"
Retrain KWS model with new data in one class,google-research/google-research,2022-08-23 03:53:41,3,,1249,1347289930,Currently I'm using the att_rnn model for training the customized speech command models. Is there any approaches to  retrain the model with the new data from a checkpoint that can improve the overall accuracy?  
[MultigameDT] Unable to initialize multigame decision transformer on Colab,google-research/google-research,2022-08-19 18:58:52,1,,1247,1344801323,"Hi,

I was trying to play with the multigame decision transformer notebook on colab (i.e. https://github.com/google-research/google-research/blob/master/multi_game_dt/Multi_game_decision_transformers_public_colab.ipynb) but the runtime crashes whenever the line of code `init_params, init_state = model_fn.init(rng, dummy_datapoint)` is executed. The log did not report anything informational at the time of the crash:

Aug 19, 2022, 11:48:19 AM | WARNING | WARNING:root:kernel 402f5699-0e26-4cef-a58c-dfd68b89e706 restarted
-- | -- | --
Aug 19, 2022, 11:48:19 AM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports

The model parameters seem to be loaded successfully by displaying
> loading checkpoint from: gs://rl-infra-public/multi_game_dt/checkpoint_38274228.pkl
Number of model parameters: 1.98e+08

However, there was a warning that said:

Aug 19, 2022, 11:46:29 AM | WARNING | 2022-08-19 18:46:29.161081: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""NOT_FOUND: Could not locate the credentials file."". Retrieving token from GCE failed with ""NOT_FOUND: Error executing an HTTP request: HTTP response code 404"".
-- | -- | --

To provide more information for debugging, I'm a Colab Pro user and I used TPU runtime with standard memory allocation. The TPU runtime with high RAM allocation crashed too. The notebook is unmodified and I ran it cell by cell. I also tried restarting the runtime after installing all packages but it did not work as well.

I'd really appreciate if someone could look into the error and help fix it. Thank you!"
Broken link: UL2's T5 Config,google-research/google-research,2022-08-19 08:19:58,0,,1245,1344113245,"In the checkpoint section of the following page:
https://github.com/google-research/google-research/tree/master/ul2 T5 config links lead to the page https://storage.googleapis.com/scenic-bucket/ul2/ul220b/config.gin that returns a ""NoSuchKey"" error."
FNet cannot import name 'optim' from 'flax' when running unit tests,google-research/google-research,2022-08-18 14:19:23,0,,1244,1343179190,"I have tried following the instructions to run FNet here: https://github.com/google-research/google-research/tree/master/f_net

But I run into error: 
```
ImportError: cannot import name 'optim' from 'flax'
```
when running 
```
python3 -m unittest discover -s f_net -p '*_test.py'
```

and can't seem to fix it by trying to roll back to a version of flax where optim is not deprecated"
Demogen,google-research/google-research,2022-08-16 11:48:24,4,,1238,1340232012,"I am trying to reproduce the results of the ""Predicting the Generalization Gap in Deep Networks with Margin Distributions"" paper using the Demogen project. The code seems to be developed using TF1, and I can not run it on TF2. I have changed some parts of the code, but it still has a problem with tensor2tensor library, and the error is: 
    _kwargs = spec_.kwargs.copy()
AttributeError: 'NoneType' object has no attribute 'copy'

Could you please tell me how I can solve this problem and make this code compatible with TF2?

"
[SCANN] support for large (out-of-memory) dataset,google-research/google-research,2022-08-15 19:22:36,0,,1237,1339390291,"I wonder if SCANN supports large dataset in general. For example, to train the sift1B (1 billion x 128 dim) dataset on a small machine with 64 GB of memory, I need to memmap the dataset and pass it to the scann's builder. However, seems SCANN only supports to load the dataset fully in memory before it can start training. 

Is there any way to train large dataset given constraint memory (like faiss)? Or is SCANN designed for rather small datasets only?  

```
Traceback (most recent call last):
  File ""/data/scann_test/scann_sift1B.py"", line 68, in <module>
    searcher = scann.scann_ops_pybind.builder(xb, topK, ""squared_l2"").tree(
  File ""/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_builder.py"", line 243, in build
    return self.builder_lambda(self.db, config, self.training_threads, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 78, in builder_lambda
    return create_searcher(db, config, training_threads, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 86, in create_searcher
    scann_pybind.ScannNumpy(db, scann_config, training_threads))
TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:
    1. scann_pybind.ScannNumpy(arg0: str, arg1: str)
    2. scann_pybind.ScannNumpy(arg0: numpy.ndarray[numpy.float32], arg1: str, arg2: int)

Invoked with: memmap([[2.9440201e-20, 1.8216880e-44, 2.8302559e+26, ..., 3.4590261e-11,
```"
How can I set seed on this TFT model?,google-research/google-research,2022-08-15 03:02:33,0,,1236,1338474104,
[HitNet] Predicted disparity differs between the paper and the pre-trained model ,google-research/google-research,2022-08-14 13:27:47,0,,1235,1338246808,"In the [paper](https://arxiv.org/abs/2007.12140), Figure 1 shows a predicted disparity of an image of [plants](https://vision.middlebury.edu/stereo/data/scenes2014/):
![disparity-in-paper](https://user-images.githubusercontent.com/523148/184538505-8afccb8c-a728-469e-92d4-161cad3c735a.png)

In the [pre-trained model](https://github.com/google-research/google-research/tree/master/hitnet),  the script `predict_middlebury.sh` produces a different image:
<img src=""https://user-images.githubusercontent.com/523148/184538977-d4298ef5-0781-4968-8e3f-c11be9909c39.jpg"" width=""337"" height=""251"">
False-colored image is created by replacing the [function `encode_image_as_16bit_png`](https://github.com/google-research/google-research/blob/master/hitnet/predict.py#L268) with

```python
def save(data, filename):
  image = data
  import numpy as np
  image = (image-image.min())/(image.max()-image.min()) * 255
  image = image.astype(np.uint8)
  import cv2
  image = cv2.applyColorMap(image, cv2.COLORMAP_JET)
  image = image[...,::-1]
  from PIL import Image
  image = Image.fromarray(image)
  image.save(filename)
```
TensorFlow version is 2.6.2.
Tried with CPU and GPU backends.
Tried with all middlebury models, which have max disparities 160, 288 and 400.
"
protoattend for adult census income data,google-research/google-research,2022-08-10 08:38:40,0,,1233,1334278645,"I am trying to apply protoattend on adult census income data as mentioned in the paper. But I am having issues in building a LSTM encoder for that.

can anyone help me on that"
pip instructions on pip site and on README.md,google-research/google-research,2022-08-05 15:31:14,0,,1229,1330083737,"pip install -r rouge/requirements.txt didn't seem to work for me.  I get:
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'

I then tried pip install -r rouge-score/requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge-score/requirements.txt'

And tried pip install -r rouge_score/requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge_score/requirements.txt'

Please advise or update the instructions on https://pypi.org/project/rouge-score/ and https://github.com/google-research/google-research/tree/master/rouge

Thanks!"
MBPP split / README,google-research/google-research,2022-08-05 08:33:31,2,,1227,1329627087,"I noticed that the [MBPP dataset on the Hugging Face hub](https://huggingface.co/datasets/mbpp) only contains a single ""test"" split encompassing the whole dataset, instead of a train-test split. While writing an [issue](https://github.com/huggingface/datasets/issues/4795) to correct this, it became apparent that the splits used for the experiments of the [""Program Synthesis with Large Language Models"" paper](https://arxiv.org/abs/2108.07732) were not completely clear, at least to me.

The paper mentions a four-way split of the full variant of MBPP in subsection 2.1:
> In the experiments described later in the paper, we hold out 10 problems for **few-shot prompting**, another 500 as our **test** dataset (which is used to evaluate both few-shot inference and fine-tuned models), 374 problems for **fine-tuning**, and the rest for **validation**.

The paper doesn't explicitly state the task ID ranges of the splits, but the [README.md from this repo](https://github.com/google-research/google-research/blob/master/mbpp/README.md), which is referenced in the paper, does:
> We specify a train and test split to use for evaluation. Specifically:
> 
> * Task IDs 11-510 are used for evaluation.
> * Task IDs 1-10 and 511-1000 are used for training and/or prompting. We typically used 1-10 for few-shot prompting, although you can feel free to use any of the training examples.

I.e. compared to the paper, the few-shot, train and validation splits are combined into one split, with a soft suggestion of using the first ten for few-shot prompting.

It is not explicitly stated whether the 374 fine-tuning samples mentioned in the paper have Task ID 511 to 784 or 601 to 974 or whether they were randomly sampled from Task ID 511 to 974. The total number of samples is misstated as 1000 instead of 974.

Regarding the edited variant of the dataset, the paper states the following:
> For evaluations involving the edited dataset, we perform comparisons with 100 problems that appear in both the original and edited dataset, using the same held out 10 problems for few-shot prompting and 374 problems for fine-tuning. 

The language here doesn't appear to be very precise, as among the 10 few-shot problems, those with Task ID 1, 5 and 10 are not part of the edited variant, and many from the task_id range from 511 to 974 are missing (e.g. Task ID 511 to 553).
I suppose the idea the Task ID ranges for each split remain the same, even if some of the task_ids are not present, but I think this should be stated explicitly (if it is the case).

"
Stereo Depth Estimation,google-research/google-research,2022-08-03 11:09:37,0,,1225,1327056048,"Hello, I'm researching about long-range stereo vision systems Cause I'm planning to estimate the depth of an object that is 1000m away. I have a question, Do you think this task is possible if we use the far-field lens in our research? What would be the limitations? even if we use an extra lens with a large focal length.
"
[ScaNN] SIGILL when installing PIP module,google-research/google-research,2022-08-03 01:11:10,10,,1224,1326556271,"Receiving the following backtrace when I try to `import scann` in a Python project on Ubuntu 20.04:

```
#0  0x00007fff9f1e9209 in google::protobuf::UninterpretedOption::UninterpretedOption() ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#1  0x00007fff9f1c482b in InitDefaultsscc_info_UninterpretedOption_google_2fprotobuf_2fdescriptor_2eproto() ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#2  0x00007fff9f277d43 in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.9425925494154413262] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#3  0x00007fff9f277d2f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.9425925494154413262] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#4  0x00007fff9f277d2f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.9425925494154413262] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
```

It looks like the error comes from this file effectively, when defining TensorFlow operations: https://github.com/google-research/google-research/blob/master/scann/scann/scann_ops/cc/ops/scann_ops.cc

Here's a current view of the `requirements.txt` file:

```
numpy
protobuf<3.20,>=3.9.2
pyarrow
scann>=1.2.6
```

Here's the result of `pip freeze` for the virtual environment:

```
absl-py==1.2.0
astunparse==1.6.3
cachetools==5.2.0
certifi==2022.6.15
charset-normalizer==2.1.0
flatbuffers==2.0
gast==0.5.3
google-auth==2.9.1
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.47.0
h5py==3.7.0
idna==3.3
importlib-metadata==4.12.0
keras==2.8.0
Keras-Preprocessing==1.1.2
libclang==14.0.6
Markdown==3.4.1
MarkupSafe==2.1.1
numpy==1.23.1
oauthlib==3.2.0
opt-einsum==3.3.0
protobuf==3.19.4
pyarrow==8.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
requests==2.28.1
requests-oauthlib==1.3.1
rsa==4.9
scann==1.2.6
six==1.16.0
tensorboard==2.8.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.8.2
tensorflow-estimator==2.8.0
tensorflow-io-gcs-filesystem==0.26.0
termcolor==1.1.0
typing-extensions==4.3.0
urllib3==1.26.11
Werkzeug==2.2.1
wrapt==1.14.1
zipp==3.8.1
```"
[ScaNN] Issue while loading the saved 'Searcher',google-research/google-research,2022-08-02 16:18:54,0,,1223,1326088266,"I have followed the [steps](https://github.com/google-research/google-research/issues/355) to save the `ScannSearcher` using numpy `serialize` method, but am getting the following error while loading back the searcher. Any solutions?
```
F ./scann/utils/reordering_helper.h:90] Cannot enable exact reordering when the original dataset is empty.
Aborted (core dumped)
```
"
"I want to create a digital project which will work within Google, Yelp, Glassdoor and other platforms.",google-research/google-research,2022-07-29 14:36:46,0,,1220,1322332190,"### The idea is based on optimizing the interaction between users and sites' support. This is necessary for users to work for the platforms and for the platforms to work for the users.

At the moment, the most important thing that worries me is the way to optimize the user's communication with the sites. Yelp/Google/Glassdoor/etc have a huge audience and it's impossible to cover all this flow of users and their opinions or reports only by internal instruments.

**Profit for the user**: to pay money and not waste time on solving your specific (or not so) problem.

**Profit for sites**: the effectiveness of solving external issues + templating requests according to the criteria of the site itself.

_So, if someone likes the idea, please share your fascinating experience communicating with the support of one or another site._

"
"Question on ""dual_dice repository"" ",google-research/google-research,2022-07-29 01:05:27,0,,1217,1321643660,"How can we obtain the **ground truth** of target policy.
How can we compute the **RMSE error** as in Figure 1, 2, 3, 4, 5 in the paper https://arxiv.org/abs/1906.04733

As described in README: ""The print-out of the Target (oracle) rewards is an estimate! In a perfect world this would be calculated with num_trajectories=\infty and max_trajectory_length=\infty. In practice, to get a better estimate, just set these to a large value (e.g., 1000).""

So, did you treat the **estimated Target (oracle) rewards** as ground truth?
I think this is not reasonable.

Can you give some suggestions?
Thanks a lot!

"
pairwise_accuracy,google-research/google-research,2022-07-28 01:02:46,0,,1215,1320267197,"I am able to run the demo code mentioned in the repo (graph_embedding/dmon) but after I compute pairwise_accuracy, it's only about 11%. I use the accuracy computing code in the repo."
Datasets:,google-research/google-research,2022-07-27 11:52:26,0,,1214,1319450234,"Thank you for this work.

I don't find the code for generating Kitti and EuROC dataset format used in this paper : 
Depth from Video in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras."
ops_benchmark.cc missing from automl_zero repository,google-research/google-research,2022-07-23 20:03:13,0,,1213,1315735789,"Hi,

In https://github.com/google-research/google-research/blob/master/automl_zero/compute_cost.cc#L32
> ""To add a new op compute cost here, first run **ops_benchmark.cc**""

Where can I find this benchmark code you used?
Thanks!"
rouge-score==0.0.7,google-research/google-research,2022-07-22 06:58:18,4,,1212,1314463379,"File ""/root/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f/rouge.py"", line 21, in <module>
    from rouge_score import rouge_scorer, scoring
ImportError: cannot import name 'rouge_scorer' from 'rouge_score' (unknown location)


Version 0.0.4 works perfectly fine."
Dual Pixel on Google Pixel 6 Pro,google-research/google-research,2022-07-21 20:08:19,0,,1211,1313746168,"https://github.com/google-research/google-research/tree/master/dual_pixels#android-app-to-capture-dual-pixel-data

I'm trying to get this example to run on a Pixel 6 Pro. After updating some constants (see below) I kept getting the following error:

`W/CameraDevice-JV-0: Stream configuration failed due to: createSurfaceFromGbp:390: Camera 0: No supported stream configurations with format 0x20 defined, failed to create output stream`

Modifications:
```
  private static final int DP_WIDTH = 2040;
  private static final int DP_HEIGHT = 768;
```

Plus I updated build script to work with more recent version of Gradle.

Any ideas?"
intrinsics prediction,google-research/google-research,2022-07-21 13:53:52,0,,1210,1313299994,"Thank you for this work

I don't find the camera intrinsics evaluation code (Table5)."
what's the reference of multiple_user_representations,google-research/google-research,2022-07-21 08:34:19,0,,1209,1312885189,"@andrewluchen hi，I'm interested in your project https://github.com/google-research/google-research/tree/master/multiple_user_representations. However, I'm confused about the method Iterative density weighting. Would you please share the related paper. Thx a lot."
"FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 2 from 1",google-research/google-research,2022-07-21 06:34:50,2,,1208,1312729824,"@rybakov  I need a help from you.
When I set the preprocess = 'mfcc', I could not convert the model to streaming mode correctly.  tf version=2.7.0.  Do you know the reason for this?
Thanks!

The error message is like:

W0720 23:15:13.229021 22708 stream.py:258] There is no need to use Stream on time dim with size 1
W0720 23:15:13.579084 22708 stream.py:258] There is no need to use Stream on time dim with size 1
W0720 23:15:13.660865 22708 test.py:646] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 2 from 1 for '{{node streaming/average_pooling2d/AvgPool}} = AvgPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 2, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/dropout_1/Identity)' with input shapes: [1,1,1,64].
2022-07-20 23:15:13.662452: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0720 23:15:13.663857 22708 test.py:449] tflite stream model state external with reset_state 1
I0720 23:15:33.224785 22708 model_train_eval.py:291] FAILED to run TFLite streaming: Model provided has model identifier '}☻"
EuROC,google-research/google-research,2022-07-20 13:55:26,0,,1207,1311160097,"Hello, thank you for this work
Where I can find the EuROC  training database?"
[MUSIQ] Error in Image Dimensionality,google-research/google-research,2022-07-18 18:23:29,6,,1206,1308350788,"@junjiek 

I get the following error when I run inference with only some images.  Some images are fine.  My guess is that the image dimensions are somehow not compatible.  I noticed that the issue comes on images that are tall and narrow but not always.

Does the images have to be a specific dimension to have them do inference correctly?.

`
Image size: 728x2048
Image ratio: 0.35546875

Traceback (most recent call last):

  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 267, in <module>
    main()
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 251, in main
    pred = run_model_single_image(model_config, num_classes, pp_config, params, image_data)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 160, in run_model_single_image
    image = prepare_image(image_path, pp_config)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 137, in prepare_image
    data = pp_fn(data)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 301, in _preprocess_fn
    image = get_multiscale_patches(image, **preprocessing_kwargs)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 210, in get_multiscale_patches
    out = _extract_patches_and_positions_from_image(resized_image, patch_size, patch_stride, hse_grid_size,
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 164, in _extract_patches_and_positions_from_image
    out = tf.concat([p, spatial_p, scale_p, mask_p], axis=2)
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,28,3072] vs. shape[1] = [1,21,1] [Op:ConcatV2] name: concat
`

I get another error with a different image shape.
`
Image size: 2480x3509
Image ratio: 0.7067540609860359

Traceback (most recent call last):

  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 267, in <module>
    main()
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 243, in main
    pred_mos = run_model_single_image(model_config, num_classes, pp_config, params, image_path)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 160, in run_model_single_image
    image = prepare_image(image_path, pp_config)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 137, in prepare_image
    data = pp_fn(data)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 301, in _preprocess_fn
    image = get_multiscale_patches(image, **preprocessing_kwargs)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 210, in get_multiscale_patches
    out = _extract_patches_and_positions_from_image(resized_image, patch_size, patch_stride, hse_grid_size,
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 144, in _extract_patches_and_positions_from_image
    p = tf.reshape(p, [n_crops, -1, patch_size * patch_size * c])
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 35840 values, but the requested shape requires a multiple of 3072 [Op:Reshape]
`

These image dimensions I get a successful inference.

`
Image size: 2550x3301
Image ratio: 0.7724931838836716

36.122936 36.12_2de0e5a9-67b8-4011-bb1e-163a85863314-page-1.jpeg
`

Here is another one that processed successfully.
`
Image size: 2550x3301
Image ratio: 0.7724931838836716

35.829983 35.83_2550590.jpeg
`

Can you help?
"
"Hi, I have one question how we can give our known 2D keypoints to the pr-vipe model.",google-research/google-research,2022-07-18 09:20:12,0,,1205,1307643631,"Hi, I have one question how we can give our known 2D keypoints to the pr-vipe model.

_Originally posted by @B-rkh-verma in https://github.com/google-research/google-research/issues/611#issuecomment-1186964327_"
"[poem]  After the inference model outputs the result vector, is it necessary to normalize the feature vector before distance calculation",google-research/google-research,2022-07-16 11:58:10,1,,1204,1306789234,"After the inference model outputs the result vector, is it necessary to normalize the feature vector before distance calculation"
Install ROUGE from source: `AttributeError: module 'tokenize' has no attribute 'open'`,google-research/google-research,2022-07-14 09:24:26,0,,1203,1304509084,"Since there hasn't been a release of `rouge_score` in a while (#1199) I tried to install the package from source with:

```bash
pip install git+https://github.com/google-research/google-research.git#subdirectory=rouge
```

Which returns:
```bash
(env)  ~/git/tmp  pip install git+https://github.com/google-research/google-research.git#subdirectory=rouge
Collecting git+https://github.com/google-research/google-research.git#subdirectory=rouge
  Cloning https://github.com/google-research/google-research.git to /private/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/pip-req-build-4q0gbc1a
  Running command git clone --filter=blob:none --quiet https://github.com/google-research/google-research.git /private/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/pip-req-build-4q0gbc1a
  Resolved https://github.com/google-research/google-research.git to commit f3033edb77b3f4549858482c91db6131a7a7af11
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [42 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 14, in <module>
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/setuptools/__init__.py"", line 16, in <module>
          import setuptools.version
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/setuptools/version.py"", line 1, in <module>
          import pkg_resources
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 84, in <module>
          __import__('pkg_resources.extern.packaging.requirements')
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/packaging/requirements.py"", line 10, in <module>
          from pkg_resources.extern.pyparsing import (  # noqa
        File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
        File ""<frozen importlib._bootstrap>"", line 565, in module_from_spec
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/extern/__init__.py"", line 52, in create_module
          return self.load_module(spec.name)
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/extern/__init__.py"", line 37, in load_module
          __import__(extant)
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/__init__.py"", line 140, in <module>
          from .core import __diag__, __compat__
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 5663, in <module>
          _escapedPunc = Word(_bslash, r""\[]-*.$+^?()~ "", exact=2).set_parse_action(
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 677, in set_parse_action
          self.parseAction = [_trim_arity(fn) for fn in fns]
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 677, in <listcomp>
          self.parseAction = [_trim_arity(fn) for fn in fns]
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 286, in _trim_arity
          _trim_arity_call_line = (_trim_arity_call_line or traceback.extract_stack(limit=2)[-1])
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/traceback.py"", line 211, in extract_stack
          stack = StackSummary.extract(walk_stack(f), limit=limit)
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/traceback.py"", line 366, in extract
          f.line
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/traceback.py"", line 288, in line
          self._line = linecache.getline(self.filename, self.lineno).strip()
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py"", line 30, in getline
          lines = getlines(filename, module_globals)
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py"", line 46, in getlines
          return updatecache(filename, module_globals)
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py"", line 136, in updatecache
          with tokenize.open(fullname) as fp:
      AttributeError: module 'tokenize' has no attribute 'open'
      [end of output]
```

The issue is that there is a file `tokenize.py` in the root directory which interferes with Python's `tokenize` module. Are there any workarounds for this besides renaming the file before installation?"
run.sh has wrong path for requirements.txt,google-research/google-research,2022-07-11 03:46:41,0,,1200,1300165986,"Running run.sh errors with the following output:
```
+ source ./bin/activate
+++ '[' ./bin/activate = ./run.sh ']'
+++ deactivate nondestructive
+++ unset -f pydoc
+++ '[' -z '' ']'
+++ '[' -z '' ']'
+++ hash -r
+++ '[' -z '' ']'
+++ unset VIRTUAL_ENV
+++ '[' '!' nondestructive = nondestructive ']'
+++ VIRTUAL_ENV=/home/<user>/source/google-research/muzero
+++ '[' linux-gnu = cygwin ']'
+++ '[' linux-gnu = msys ']'
+++ export VIRTUAL_ENV
+++ _OLD_VIRTUAL_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/<user>/source/google-research/muzero/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ export PATH
+++ '[' -z '' ']'
+++ '[' -z '' ']'
+++ _OLD_VIRTUAL_PS1=
+++ '[' x '!=' x ']'
++++ basename /home/<user>/source/google-research/muzero
+++ PS1='(muzero) '
+++ export PS1
+++ alias pydoc
+++ true
+++ hash -r
++ pip install -r muzero/requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'muzero/requirements.txt'
```
Given the contents of the script, it looks like it was designed to be run from the parent folder:
```set -e
set -x

virtualenv -p python3 .
source ./bin/activate

pip install -r muzero/requirements.txt
python -m muzero.core_test
```

Doing so, however, reveals an issue with requirements.txt:
```ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)
ERROR: No matching distribution found for tensorflow==2.4.1
```
It sounds like you're assuming the virtualenv will be created from an old version of python?
"
question about removal flare,google-research/google-research,2022-07-06 02:20:50,0,,1194,1295023162,"First of all, thank you for sharing the idea.  The way of making data mentioned in the paper is very good, but I wonder if there is more theoretical explanation for this way?  This paper is only mentioned in Chapter 4: 'The additive nature of light implies we can model flare
as an additive artifact on top of the “ideal” image' . After subtracting F from the If I shot, there will be a purple artifact around the light source.  I am very confused. Could you please tell me the reason why you made the data like this?"
[caltrain] data download access denied,google-research/google-research,2022-07-05 03:19:25,3,,1193,1293734513,"Hello,

I followed the instructions from the **caltrain** README to download the data, but I am unable to download the data due to an access denied error.

**Data download instructions followed:**
```
DATA_DIR='./caltrain/data' # This is the default value if omitted below
python -m caltrain.download_data --data_dir=${DATA_DIR}
```

**Access denied error:** 
```
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object.</Details>
</Error>
```
I would appreciate any help on this issue. Thanks! :) 
"
[smurf] The datasets of multiframe training,google-research/google-research,2022-07-04 12:19:59,0,,1192,1293095039,"`python3 -m smurf.multiframe_training.main -- --input_dir=<path **multiframe records**> --output_dir=<path to output directory>`

Which way(eg .py files) can generate **multiframe records**？I can't find answear in the README.md or code.

Thanks~"
[ScaNN] Is it possible to build ScaNN without tensorflow?,google-research/google-research,2022-07-04 10:19:53,1,,1191,1292954401,"Hello,
I need to minimize docker image size, I don't use tf_serving, I only use like this:
```python
searcher = scann.scann_ops_pybind.builder(search_index, 2, ""dot_product"").score_brute_force().build()
neighbours, similarities = searcher.search_batched([person_vector])
```
So is it possible to build ScaNN without tensorflow by removing part of the code or is ScaNN completely dependent on tensorflow?
Thank you in advance."
It takes too long to compile the train and eval process when reproducing muNet on 8 A100 GPUs,google-research/google-research,2022-06-30 07:39:36,0,,1188,1289727703,"Hi there,

I am reproducing the muNet on 8 A100 GPUs. Compared to running it on Colab TPUv2 8 cores,  it takes too long to compile each child model. XLA also reminds me that it takes too long and suggests that there may be a bug. So, an [issue](https://github.com/google/jax/issues/11271) is also posted on the JAX repository, but so far no one respond. Maybe it's better to post it here. To be clear, I will add more main info here.

- The script I am using is [munet](https://colab.research.google.com/github/google-research/google-research/blob/master/muNet/muNet.ipynb), the experiment running is the smallest one, 'ViT tiny 3 layers / characters benchmark’. 
- Packages are installed in the way specified by munet script.
- The Cuda version is 11.4, then cudnn version is 8.2, python version is 3.9
- it only happens when running on 8 gpus, running it on one gpu is fine
- I am running it with Slurm, so maybe slurm does not give enough process to Python? The command is srun --partition=xxx --gres=gpu:8 -N1 -n1 --ntasks-per-node=1 --cpus-per-task=32 python munet_30June.py. I give 32 cores to this task. It should be fine.

Any hint is welcome. Thanks in advance."
how to train from scrach for a low-resource langauge ,google-research/google-research,2022-06-29 20:05:31,0,,1186,1289247204,"I am dealing with a low-resource langauge - the bert model is ok but it is too slow; there is also hardware limitation in the field that makes me to consider with mobile BERT. I wonder how I can train this mobile BERT from scrach ? any resources ?
Thank you for your good work."
imagen,google-research/google-research,2022-06-28 09:09:27,0,,1184,1287035647,"hello, can youplease give me imagen pls pls pls :C I really want to :(
depressedintokyoo@gmail.com

I want to create a unique and world's first nft collection of cats, they will have uniqueness and all that! I'll make a revolution with imagen
I know you haven't allowed commercial use yet, but I think you will in the future
I've been your fan since the 1st version that was on github
(I generated about 10k images in it)
I will not sell them, I really want to start developing the collection"
score_multi() method is missing?,google-research/google-research,2022-06-27 21:12:51,0,,1183,1286391566,"I can create a RougeScorer called scorer, and even call scorer.score().

However, I want to use scorer.score_multi(), which is shown in the RougeScorer class... but then I get an error saying that RougeScorer object has no attribute 'score_multi'. What could be the case here?


For context, I am using google collab."
[RegNeRF] bug in regnerf?,google-research/google-research,2022-06-23 12:58:27,1,,1176,1282368593,"Hi, I think i found a bug in regnerf datasets.py row 1131, the code should be in the for-loops?
https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/regnerf/internal/datasets.py#L1131"
poem - Pr-vipe checkpoints,google-research/google-research,2022-06-23 04:11:38,2,,1175,1281699168,Pr-vipe checkpoint file.
Proper setup tools support for model_pruning ,google-research/google-research,2022-06-22 22:29:19,0,,1174,1281109061,Please add a proper python-build or setuptools support for https://github.com/google-research/google-research/tree/master/model_pruning. For an ArchLinux user it is simply a shame to install pip packages!
Single view MPI,google-research/google-research,2022-06-16 00:36:40,6,,1161,1272905163,"Hello, I'm interested in your single_view_mpi code. And when I see it, there's no train code in single_view_mpi directory. Could you provide me train code of single_view_mpi?? "
[MUSIQ] Has anyone had any success in running a prediction with the MUSIQ IQA code posted here?,google-research/google-research,2022-06-15 15:51:36,10,,1160,1272433211,"@junjiek or @andrewluchen

- I have downloaded the code for MUSIQ IQA and also the checkpoint models. 
- I have updated the libraries to overcome the deprecated version of TensorFlow by upgrading to version 2.9.1
- I ran the prediction with the KONIQ checkpoint and it processes the image initially.

I run into the following error when the application starts to do the prediction in line 143 in original code line 141 in my revised code.

```
Traceback (most recent call last):
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 174, in <module>
    app.run(main)
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 169, in main
    pred_mos = run_model_single_image(model_config, FLAGS.num_classes, pp_config, params, FLAGS.image_path)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 141, in run_model_single_image
    model = model_mod.Model.partial(num_classes=num_classes, train=False, **model_config)
AttributeError: type object 'Model' has no attribute 'partial'
```

The model_mod.Model class is referenced from the multiscale_transformer.py script in the model directory.  When I go into that file there is no Class method for the Model Class called partial.  

Is this app posted by Google Research missing some code to run the prediction or am I misunderstaning something here and not setting it up right?

Thanks."
Meta Back-Translation Code Not Exit,google-research/google-research,2022-06-15 15:32:04,0,,1159,1272409093,"Hello!
I read a paper: [meta back-translation](https://arxiv.org/pdf/2102.07847v1.pdf), but I could not find the code.
Link code from paper does not exit.
https://github.com/google-research/google-research/tree/master/meta_back_translation
So, Where can I find it?
Thank so!"
[kws_streaming] How to realize BC-ResNet running in streaming mode,google-research/google-research,2022-06-15 08:12:01,2,,1158,1271841651,"@rybakov 
Hi,
I am preparing to use BC-ResNet to implement streaming inference. When command below used(padding is same), it seems unable to stream inference. 
If padding is set to causal, dimension inconsistency will occur in streaming inference。
Whether `kws_streaming/models/bc_resnet.py` needs to be modified to enable streaming inference?

Thank you

```
$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/bc_resnet_1/ \
--mel_upper_edge_hertz 7500 \
--mel_lower_edge_hertz 125 \
--how_many_training_steps 100,100,100,100,30000,30000,20000,10000,5000,5000 \
--learning_rate 0.001,0.002,0.003,0.004,0.005,0.002,0.0005,1e-5,1e-6,1e-7 \
--window_size_ms 30.0 \
--window_stride_ms 10.0 \
--mel_num_bins 40 \
--dct_num_features 0 \
--resample 0.1 \
--alsologtostderr \
--train 1 \
--use_spec_augment 0 \
--time_masks_number 2 \
--time_mask_max_size 25 \
--frequency_masks_number 2 \
--frequency_mask_max_size 7 \
--pick_deterministically 1 \
bc_resnet \
--sub_groups 5 \
--last_filters 32 \
--first_filters 16 \
--paddings 'same' \
--dilations '(1,1),(2,1),(4,1),(8,1)' \
--strides '(1,1),(1,2),(1,2),(1,1)' \
--blocks_n '2, 2, 4, 4' \
--filters '8, 12, 16, 20' \
--dropouts '0.1, 0.1, 0.1, 0.1' \
--pools '1, 1, 1, 1' \
--max_pool 0
```

> tflite Final test accuracy, non stream model = 96.50% (N=4890)
> tf stream model state external with reset_state 1
> TF Final test accuracy of stream model state external = 0.00% (N=10)
> tf stream model state external with reset_state 0
> TF Final test accuracy of stream model state external = 0.00% (N=10)"
"TypeError: Expected int64 passed to parameter 'y' of op 'Less', got 100000.0 of type 'float' instead. Error: Expected int64, got 100000.0 of type 'float' instead.",google-research/google-research,2022-06-15 06:23:29,0,,1157,1271728429,Please wrap `delta_r_warmup` at line 202 of darc.agent.py with `int()`: `int(delta_r_warmup)`.
[smurf] How to train on own dataset?,google-research/google-research,2022-06-11 05:52:13,0,,1154,1268173169,How to train the SMURF model on custom datasets? 
Error when installing: TypeError: chown() missing 1 required positional argument: 'numeric_owner' #17,google-research/google-research,2022-06-08 07:21:28,0,,1152,1264309847,"When I install easy_install, I got the following error on windows 10:
        >(tf) D:\Download\ez_setup-0.9\ez_setup-0.9>python ez_setup.py
        Extracting in C:\Users\ADMINI~1\AppData\Local\Temp\tmp4yuz2578
        Traceback (most recent call last):
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 485, in <module>
            main(sys.argv[1:])
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 481, in main
            _install(tarball)
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 74, in _install
            _extractall(tar)
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 467, in _extractall
            self.chown(tarinfo, dirpath)
        TypeError: TarFile.chown() missing 1 required positional argument: 'numeric_owner'

Anybody can help me?

Thanks,"
[RealFormer] Pre-trained checkpoints?,google-research/google-research,2022-06-02 22:12:07,0,,1150,1258840407,Wondering if the fine-tuned checkpoints for RealFormer are available anywhere? Particular interested in the model fine-tuned on HotpotQA
Failed to import metagraph,google-research/google-research,2022-05-31 22:02:50,0,,1142,1254461728,"I was trying to run 02_inference.ipynb, in the section with respect to run non streaming inference with TFLite, and i got the error Failed to import metagrap as shown in the image, will you be able to help correct this problem? Thank's for your time.
![erro](https://user-images.githubusercontent.com/64547325/171291490-38c33d4c-f1fe-4b30-b429-7c40aee0ae17.png)

"
how to export meshes and textures?,google-research/google-research,2022-05-31 21:25:48,0,,1141,1254433048,"
Hello all, we are trying to export our complete 3d models in order to open it in Blender or similar softwares. Do you know how to export obj, fbx or any other mesh format, including textures or vertex colors?
Thank you
Mattia
"
Perhaps input is empty or misspecified,google-research/google-research,2022-05-31 00:23:13,0,,1138,1253205558,"I keep getting the following problem when trying to use the train function. I've tried everything I can think of.

INFO:tensorflow:Saving dict for global step 0: absolute_difference = 0.0, bass_spectrogram = 0.0, drums_spectrogram = 0.0, global_step = 0, loss = 0.0, other_spectrogram = 0.0, vocals_spectrogram = 0.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: musdb_model\model.ckpt-0
WARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.
INFO:tensorflow:Loss for final step: None.
INFO:spleeter:Model training done

I am running on Windows 10. I get the same issue with whatever version of Python/Tensorflow I use. This is my command line:

python -m spleeter train -p configs/musdb_config.json -d C:\Python\spleeter\configs --verbose

Is there anything anyone can think of for me to try?

I added extra print statements to try to track down. Here is the output:

We are in:  __main__
Entered entrypoint:  __main__
Someone called spleeter_callback:   __main__
Let us train:  __main__
INFO:spleeter:Just imported TF
Did this instantiate_pre?
We are in class InstrumentDatasetBuilder spleeter.dataset
We are in class class DatasetBuilder spleeter.dataset
Did this instantiate?
INFO:tensorflow:Using config: {'_model_dir': 'musdb_model', '_tf_random_seed': 3, '_save_summary_steps': 5, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.45
}
, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Ready to jump?......
Just jumped?......
INFO:spleeter:Start model training
estimator........:  <tensorflow_estimator.python.estimator.estimator.EstimatorV2 object at 0x0000028688343E50>
train_spec.......:  TrainSpec(input_fn=functools.partial(<function get_training_dataset at 0x00000286ABE050D0>, {'train_csv': 'configs/musdb_train.csv', 'validation_csv': 'configs/musdb_validation.csv', 'model_dir': 'musdb_model', 'mix_name': 'mix', 'instrument_list': ['vocals', 'drums', 'bass', 'other'], 'sample_rate': 44100, 'frame_length': 4096, 'frame_step': 1024, 'T': 512, 'F': 1024, 'n_channels': 2, 'n_chunks_per_song': 40, 'separation_exponent': 2, 'mask_extension': 'zeros', 'learning_rate': 0.0001, 'batch_size': 4, 'training_cache': 'cache/training', 'validation_cache': 'cache/validation', 'train_max_steps': 200000, 'throttle_secs': 1800, 'random_seed': 3, 'save_checkpoints_steps': 1000, 'save_summary_steps': 5, 'model': {'type': 'unet.unet', 'params': {'conv_activation': 'ELU', 'deconv_activation': 'ELU'}}}, <spleeter.audio.ffmpeg.FFMPEGProcessAudioAdapter object at 0x0000028688343DC0>, 'C:\\Python\\spleeter\\configs'), max_steps=200000, hooks=(), saving_listeners=())
evaluation_spec..:  EvalSpec(input_fn=functools.partial(<function get_validation_dataset at 0x00000286AD9AA9D0>, {'train_csv': 'configs/musdb_train.csv', 'validation_csv': 'configs/musdb_validation.csv', 'model_dir': 'musdb_model', 'mix_name': 'mix', 'instrument_list': ['vocals', 'drums', 'bass', 'other'], 'sample_rate': 44100, 'frame_length': 4096, 'frame_step': 1024, 'T': 512, 'F': 1024, 'n_channels': 2, 'n_chunks_per_song': 40, 'separation_exponent': 2, 'mask_extension': 'zeros', 'learning_rate': 0.0001, 'batch_size': 4, 'training_cache': 'cache/training', 'validation_cache': 'cache/validation', 'train_max_steps': 200000, 'throttle_secs': 1800, 'random_seed': 3, 'save_checkpoints_steps': 1000, 'save_summary_steps': 5, 'model': {'type': 'unet.unet', 'params': {'conv_activation': 'ELU', 'deconv_activation': 'ELU'}}}, <spleeter.audio.ffmpeg.FFMPEGProcessAudioAdapter object at 0x0000028688343DC0>, 'C:\\Python\\spleeter\\configs'), steps=None, name=None, hooks=(), exporters=(), start_delay_secs=120, throttle_secs=1800)

INFO:tensorflow:Not using Distribute Coordinator.
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. 
        Checkpoint frequency is determined based on RunConfig arguments: 
            save_checkpoints_steps 1000 or save_checkpoints_secs None.
WARNING:tensorflow:From C:\Users\windo\anaconda3\envs\g397\lib\site-packages\tensorflow\python\training\training_util.py:
                235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be 
                    removed in a future version.
Instructions for updating:
Use Variable.read_value. 
Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.

We are in get_training_dataset:  spleeter.dataset
Loaded from:  C:\Python\spleeter\spleeter\dataset.py
Here is df:  
                                              mix_path  ...    duration
0    train\A Classic Education - NightOwl\mixture.wav  ...  171.247166
1                 train\ANiMAL - Clinic A\mixture.wav  ...  237.865215
2               train\ANiMAL - Easy Tiger\mixture.wav  ...  205.473379
3           train\Actions - Devil's Words\mixture.wav  ...  196.626576
4      train\Actions - South Of The Water\mixture.wav  ...  176.610975
..                                                ...  ...         ...
81                train\Triviul - Dorothy\mixture.wav  ...  187.361814
82  train\Voelund - Comfort Lives In Belief\mixtur...  ...  209.908390
83            train\Wall Of Death - Femme\mixture.wav  ...  238.933333
84     train\Young Griffo - Blood To Bone\mixture.wav  ...  254.397823
85            train\Young Griffo - Facade\mixture.wav  ...  167.857052

[86 rows x 6 columns]
dataset....................... <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
Loaded from:  C:\Python\spleeter\spleeter\utils\tensor.py
dataset:   <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
dataset before:  <ShuffleDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: (), start: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64, start: tf.float64}>
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
INFO:tensorflow:Calling model_fn.
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for vocals_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
WARNING:tensorflow:From C:\Users\windo\anaconda3\envs\g397\lib\site-packages\tensorflow\python\keras\layers\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for drums_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for bass_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for other_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in:  spleeter.model
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from musdb_model\model.ckpt-0
WARNING:tensorflow:From C:\Users\windo\anaconda3\envs\g397\lib\site-packages\tensorflow\python\training\saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...
INFO:tensorflow:Saving checkpoints for 0 into musdb_model\model.ckpt.
INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...
We are in get_validation_dataset:  spleeter.dataset
Loaded from:  C:\Python\spleeter\spleeter\dataset.py
Here is df:  
                                              mix_path  ...    duration
0                 train\ANiMAL - Rockshow\mixture.wav  ...  165.511837
1        train\Actions - One Minute Smile\mixture.wav  ...  163.375601
2   train\Alexander Ross - Goodbye Bolero\mixture.wav  ...  418.632562
3   train\Clara Berry And Wooldog - Waltz For My V...  ...  175.240998
4        train\Fergessen - Nos Palpitants\mixture.wav  ...  198.228753
5           train\James May - On The Line\mixture.wav  ...  256.092880
6    train\Johnny Lokke - Promises & Lies\mixture.wav  ...  285.814422
7                train\Leaf - Summerghost\mixture.wav  ...  231.804807
8              train\Meaxic - Take A Step\mixture.wav  ...  282.517188
9   train\Patrick Talbot - A Reason To Leave\mixtu...  ...  259.552653
10        train\Skelpolu - Human Mistakes\mixture.wav  ...  324.498866
11      train\Traffic Experiment - Sirens\mixture.wav  ...  421.279637
12             train\Triviul - Angelsaint\mixture.wav  ...  236.704218
13           train\Young Griffo - Pennies\mixture.wav  ...  277.803537

[14 rows x 6 columns]
dataset....................... <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
Loaded from:  C:\Python\spleeter\spleeter\utils\tensor.py
dataset:   <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
dataset before:  <MapDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: (), start: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64, start: tf.float64}>
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
INFO:tensorflow:Calling model_fn.
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for vocals_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for drums_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for bass_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for other_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2022-05-29T18:24:50
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from musdb_model\model.ckpt-0
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Inference Time : 4.72633s
INFO:tensorflow:Finished evaluation at 2022-05-29-18:24:54
INFO:tensorflow:Saving dict for global step 0: absolute_difference = 0.0, bass_spectrogram = 0.0, drums_spectrogram = 0.0, global_step = 0, loss = 0.0, other_spectrogram = 0.0, vocals_spectrogram = 0.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: musdb_model\model.ckpt-0
WARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.
INFO:tensorflow:Loss for final step: None.
INFO:spleeter:Model training done"
Is the time stamp in predict outputs of the tft model incorrect?,google-research/google-research,2022-05-28 12:34:04,0,,1136,1251619998,"In the function format_outputs(prediction):

about **line 1240** in the tft/libs/tft_model.py:

flat_prediction = pd.DataFrame(
          prediction[:, :, 0],
          columns=[
              **'t+{}'.format(i)**
              for i in range(self.time_steps - self.num_encoder_steps)
          ])

should be:

flat_prediction = pd.DataFrame(
          prediction[:, :, 0],
          columns=[
              **'t+{}'.format(i+1)**
              for i in range(self.time_steps - self.num_encoder_steps)
          ])

or the **line 1244**:

flat_prediction['forecast_time'] = time[:, **self.num_encoder_steps - 1**, 0]

should be:

flat_prediction['forecast_time'] = time[:, **self.num_encoder_steps**, 0]?


Thanks and welcome anybody to review this issue!

"
"CuBERT: About pre-training BERT to CuBERT model, is there any instruction.",google-research/google-research,2022-05-27 05:56:29,0,,1135,1250373125,"Hello. I'm trying to pre-train BERT model following the CuBERT paper to create CuBERT model for C++ language.
On the CuBERT GitHub page, it explains only that they modify run_pretraining.py from BERT. But how much did you change run_pretraining.py to work with CuBERT tokenizer? "
Scann- Big Ann benchmarks,google-research/google-research,2022-05-25 10:22:58,0,,1123,1247884250,"This is not an issue but a question rather. I had been evaluating Scann few months back and came across this project: https://big-ann-benchmarks.com/ . I spent some time in coming up with the framework code for Scann and the PR can be found here https://github.com/vamossagar12/big-ann-benchmarks/tree/t1/scann. 
I couldn't get the time to continue on this further but before I did that, I realized I should ask it here.
I am not a google employee so I am not quite sure if I should be proceeding with submitting for these evaluations without the consent of researchers at google. There are some deadlines for submission etc so wanted to understand if anybody from google research would be interested in participating in this ? If yes, then would it be a collaborative effort where I can also chime in or somebody from your side would pick this up? And if it's a no, then well I can also pause this effort :) "
Update and Fix Issue of YouTube Android App  version 17.19.34 ,google-research/google-research,2022-05-24 17:03:38,0,,1107,1246863641,"In The YouTube Android App When a User Visit The App And Click **Any Video** to Watching And **After Watching Any Video** Then go to Watch **Short Video** There **Show Recently video Inside Every Short Video** There is an Issue. Please See and Fix The Issue.
![Youtube_Issue_17 19 34](https://user-images.githubusercontent.com/101797831/170092107-fbb3dae1-92ec-4005-a6fa-a5f4ae77f091.png)
"
Error when trying to install dependencies via bnn_hmc/requirements.txt,google-research/google-research,2022-05-24 13:56:02,0,,1106,1246599912,"I cannot install the dependencies using requirements.txt. I have tried both conda and pip.

Running ```pip install -r requirements.txt``` produces the following error: 

```
ERROR: Invalid requirement: '_libgcc_mutex=0.1=main' (from line 4 of izmailov/requirements.txt)
Hint: = is not a valid operator. Did you mean == ?
```

Running ```conda create --name <env> --file <this file>``` produces the following error:

```
Collecting package metadata (current_repodata.json): done
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - future==0.18.2=pypi_0
  - tabulate==0.8.9=pypi_0
  - wrapt==1.12.1=pypi_0
  - markdown==3.3.4=pypi_0
  - opt-einsum==3.3.0=pypi_0
  - pyasn1-modules==0.2.8=pypi_0
  - google-auth-oauthlib==0.4.4=pypi_0
  - certifi==2020.12.5=py38h06a4308_0
  - tk==8.6.10=hbc83047_0
  - promise==2.3=pypi_0
  - typing-extensions==3.7.4.3=pypi_0
  - keras-preprocessing==1.1.2=pypi_0
  - sqlite==3.35.4=hdfb4753_0
  - google-pasta==0.2.0=pypi_0
  - attrs==20.3.0=pypi_0
  - libstdcxx-ng==9.1.0=hdf63c60_0
  - jmp==0.0.2=pypi_0
  - libgcc-ng==9.1.0=hdf63c60_0
  - ld_impl_linux-64==2.33.1=h53a641e_7
  - dm-tree==0.1.6=pypi_0
  - absl-py==0.12.0=pypi_0
  - grpcio==1.32.0=pypi_0
  - idna==2.10=pypi_0
  - ncurses==6.2=he6710b0_1
  - importlib-resources==5.1.2=pypi_0
  - readline==8.1=h27cfd23_0
  - tensorboard-data-server==0.6.0=pypi_0
  - tensorflow-metadata==0.30.0=pypi_0
  - python==3.8.8=hdb3f193_5
  - chex==0.0.6=pypi_0
  - jax==0.2.12=pypi_0
  - h5py==2.10.0=pypi_0
  - toolz==0.11.1=pypi_0
  - xz==5.2.5=h7b6447c_0
  - numpy==1.19.5=pypi_0
  - zlib==1.2.11=h7b6447c_3
  - pyasn1==0.4.8=pypi_0
  - requests-oauthlib==1.3.0=pypi_0
  - oauthlib==3.1.0=pypi_0
  - protobuf==3.15.8=pypi_0
  - flatbuffers==1.12=pypi_0
  - tqdm==4.60.0=pypi_0
  - pip==21.1=pypi_0
  - astunparse==1.6.3=pypi_0
  - chardet==4.0.0=pypi_0
  - gast==0.3.3=pypi_0
  - google-auth==1.30.0=pypi_0
  - optax==0.0.6=pypi_0
  - six==1.15.0=pypi_0
  - urllib3==1.26.4=pypi_0
  - tensorflow-estimator==2.4.0=pypi_0
  - tensorboard-plugin-wit==1.8.0=pypi_0
  - jaxlib==0.1.65+cuda112=pypi_0
  - requests==2.25.1=pypi_0
  - rsa==4.7.2=pypi_0
  - scipy==1.6.3=pypi_0
  - setuptools==52.0.0=py38h06a4308_0
  - tensorflow==2.4.1=pypi_0
  - tensorboard==2.5.0=pypi_0
  - dill==0.3.3=pypi_0
  - ca-certificates==2021.4.13=h06a4308_1
  - werkzeug==1.0.1=pypi_0
  - cachetools==4.2.2=pypi_0
  - tensorflow-datasets==4.2.0=pypi_0
  - googleapis-common-protos==1.53.0=pypi_0
  - termcolor==1.1.0=pypi_0
  - dm-haiku==0.0.5.dev0=pypi_0
  - libffi==3.3=he6710b0_2
  - openssl==1.1.1k=h27cfd23_0

Current channels:

  - https://repo.anaconda.com/pkgs/main/win-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/win-64
  - https://repo.anaconda.com/pkgs/r/noarch
  - https://repo.anaconda.com/pkgs/msys2/win-64
  - https://repo.anaconda.com/pkgs/msys2/noarch
  - https://conda.anaconda.org/conda-forge/win-64
  - https://conda.anaconda.org/conda-forge/noarch
  - https://conda.anaconda.org/anaconda/win-64
  - https://conda.anaconda.org/anaconda/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
```

Any help is appreciated, thanks!"
"""Entity Linking in 100 Languages"" Model F+ implementation",google-research/google-research,2022-05-24 09:18:23,0,,1105,1246250450,"Hello!

I was very impressed by the results achieved in the ""Entity Linking in 100 Languages"" paper. I wanted to replicate the results you have achieved on Mewsli-9 with Model F+ and use this in my research. However, I could not find implementation of Model F+ and its training process anywhere. Is it possible for you to publish the source code, please?

Thanks!"
[VATT] Pretraining VATT on a custom dataset,google-research/google-research,2022-05-24 04:47:57,0,,1104,1245977462,"Hi @hassanhub , first of all great work on VATT and thanks a lot for open sourcing its code. I'm currently trying to pretrain VATT on my own dataset. So far, I have converted my raw videos and text to TFRecords and have also made changes to `vatt/data/datasets/toy_dataset.py` as described [here](https://github.com/google-research/google-research/tree/master/vatt#data).

Can you please help me figure out what more do I need to do to get the pretraining running on my dataset? Would really be grateful!"
Complete gin files for finetuning UL2 Unifying Language Learning Paradigms,google-research/google-research,2022-05-23 22:44:45,2,,1101,1245772299,"Hi! I am wondering what gin files are needed to fine-tune UL2. I tried the following gin file, adapted from https://github.com/google-research/t5x/blob/main/t5x/examples/t5/t5_1_1/examples/small_wmt_finetune.gin. However, I met the problem of `NameError: 't5_architecture' was not provided by an import statement.`. I guess we will also need some gin files from FlaxFormer, right? Could you provide the complete gin file list for fine-tuning UL2? Thanks!

```
from __gin__ import dynamic_registration

import __main__ as train_script
from t5.data import mixtures
from t5x import models
from t5x import partitioning
from t5x import utils

include ""ul2.gin""
include ""t5x/configs/runs/finetune.gin""

USE_CACHED_TASKS = False
MIXTURE_OR_TASK_NAME = ""wmt_t2t_ende_v003""
TASK_FEATURE_LENGTHS = {""inputs"": 256, ""targets"": 256}
TRAIN_STEPS = 2651000
DROPOUT_RATE = 0.0
INITIAL_CHECKPOINT_PATH = ""gs://scenic-bucket/ul2/ul220b/checkpoint_2650000/""
# `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained
# using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be
# set to `pretraining batch_size` * `target_token_length`. For T5 and T5.1.1:
# `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.
LOSS_NORMALIZING_FACTOR = 233472
```"
Image not found in multimodalchat,google-research/google-research,2022-05-21 07:39:53,0,,1099,1243902585,271 images not found in multimodalchat
wrong TensorFlow hub link in README.md in non_semantic_speech_benchmark/trillsson,google-research/google-research,2022-05-20 05:51:39,0,,1098,1242654242,"in README.md in 
https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/trillsson

The Models are in TensorFlow hub [here](https://github.com/google-research/google-research/blob/master/non_semantic_speech_benchmark/trillsson/?).

The above suppose to link to TensorFlow hub, but it linked back to the repo instead"
"About felix ,i have this question about the prediction",google-research/google-research,2022-05-19 04:14:04,1,,1097,1241182989,"I followed the code in the prediction part, I have given this parameter (label_map_file), but it shows as None.
This is my predict_run.sh code:
![2028fdf385b81f61b23ad70b69fcf20](https://user-images.githubusercontent.com/80745053/169202666-5aa85f1c-18c3-4b91-9c0b-ea1d7a45aad5.png)

This is the error:

![d8d399d69ce3696cb33603419c71be1](https://user-images.githubusercontent.com/80745053/169202104-4627ad35-06c1-4fb8-8122-75b0a13940f0.png)

Hope you can help me，Thanks!


"
[regnerf] no RealNVP NLL loss,google-research/google-research,2022-05-19 03:56:32,1,,1096,1241173147,"Hi, I read the code of regnerf, and I found there is no NLL loss, which is different from the [released paper](https://drive.google.com/file/d/1S_NnmhypZjyMfwqcHg-YbWSSYNWdqqlo/view). Is this term not important? As shown the ablation study (Table 3), the appearance regularizer slightly improves the performance. Is it the reason that the code excludes the flow model? Thank you!"
https://jazeerapaints.com/saudi-ar/issues/issuecomment-792187934/,google-research/google-research,2022-05-17 23:30:13,1,,1093,1239261206,
dreamfields: gpu consumption,google-research/google-research,2022-05-15 18:16:17,0,,1092,1236357816,"Hi,
is there a method to decrease the training GPU memory consumption?

Thanks"
" cannot load tmp, resulted in recycle training ",google-research/google-research,2022-05-13 08:38:42,0,,1090,1234917382,"![image](https://user-images.githubusercontent.com/72790108/168244466-729de0b4-4d37-4bf4-816d-b375a192559a.png)
question: Cannot load from D:\Anaconda\project\nn\Time_forest\lstm_transformer\google-research-master\tft\expt_settings\..\outputs\saved_models\volatility\main\tmp, skipping ...
In script_hyperparam_opt.py, training has finished but cannot load tmp, resulted in recycle training ,I do not know about reason. Thanks "
AutoML Zero Build did not complete successfully,google-research/google-research,2022-05-10 14:40:16,2,,1089,1231270545,"Running on Windows 10 Version 20H2 (OS Build 19042.1645)

Installed Bazel, cloned git, ran:

`./run_demo.sh`

and get the following output:

> Starting local Bazel server and connecting to it...
> INFO: Repository rules_cc instantiated at:
>   C:/users/jack/documents/google-research/automl_zero/WORKSPACE:26:13: in <toplevel>
> Repository rule http_archive defined at:
>   C:/users/jack/_bazel_jack/n5bafl7x/external/bazel_tools/tools/build_defs/repo/http.bzl:353:31: in <toplevel>
> ERROR: An error occurred during the fetch of repository 'rules_cc':
>    Traceback (most recent call last):
>         File ""C:/users/jack/_bazel_jack/n5bafl7x/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 100, column 45, in _http_archive_impl
>                 download_info = ctx.download_and_extract(
> Error in download_and_extract: java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> ERROR: C:/users/jack/documents/google-research/automl_zero/WORKSPACE:26:13: fetching http_archive rule //external:rules_cc: Traceback (most recent call last):
>         File ""C:/users/jack/_bazel_jack/n5bafl7x/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 100, column 45, in _http_archive_impl
>                 download_info = ctx.download_and_extract(
> Error in download_and_extract: java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> ERROR: Skipping ':run_search_experiment': no such package '@rules_cc//cc': java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> WARNING: Target pattern parsing failed.
> ERROR: no such package '@rules_cc//cc': java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> INFO: Elapsed time: 23.970s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (0 packages loaded)
> FAILED: Build did NOT complete successfully (0 packages loaded)
>     currently loading:
>     Fetching C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc; Extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip"
tabular data/ noisy instances ,google-research/google-research,2022-05-09 10:57:51,0,,1088,1229531734,"Hi,
thanks for sharing your implementation. I have two questions about it:

1.	Does it also work on tabular data?
2.	Is it possible to identify the noisy instances (return the noisy IDs or the clean set)?

Thanks!"
How to get in touch regarding a security concern,google-research/google-research,2022-05-07 15:00:47,0,,1087,1228649083,"Hello 👋

I run a security community that finds and fixes vulnerabilities in OSS. A researcher (@dirac231) has found a potential issue, which I would be eager to share with you.

Could you add a `SECURITY.md` file with an e-mail address for me to send further details to? GitHub [recommends](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository) a security policy to ensure issues are responsibly disclosed, and it would help direct researchers in the future.

Looking forward to hearing from you 👍

(cc @huntr-helper)"
Dreamfields custom data training ,google-research/google-research,2022-05-07 13:05:53,0,,1086,1228623728,"Hey @ajayjain, are there utilities available in dreamfields project to train model on custom dataset of text-mesh pairs.

For example, I want to train on Amazon products Dataset: https://amazon-berkeley-objects.s3.amazonaws.com/index.html"
The data and code of etcsum have been in the release process for 2 years and have not yet been released,google-research/google-research,2022-05-04 10:31:49,0,,1084,1225189784,@joshuahm The data and code have been in the release process for 2 years and have not yet been released
Build ScaNN on Macos M1?,google-research/google-research,2022-05-01 11:40:50,2,,1082,1222127187,"Building Scann from source on MacOs M1 fails with 
```
Original error was: dlopen(/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/_multiarray_umath.cpython-38-darwin.so' (no such file), '/usr/lib/_multiarray_umath.cpython-38-darwin.so' (no such file)
Is numpy installed?
ERROR: /Users/sagarrao/gitprojects/google-research/scann/scann/scann_ops/cc/python/BUILD.bazel:8:17: //scann/scann_ops/cc/python:scann_pybind.so depends on @local_config_python//:python_headers in repository @local_config_python which failed to fetch. no such package '@local_config_python//': Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>
    from numpy.core._multiarray_umath import (
ImportError: dlopen(/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/_multiarray_umath.cpython-38-darwin.so' (no such file), '/usr/lib/_multiarray_umath.cpython-38-darwin.so' (no such file)
```
after a lot of other steps which needed to be done. Anybody knows any workaround for this?"
Error on Colab,google-research/google-research,2022-05-01 08:23:31,1,,1081,1222072243,"I try to run ""infinite nature"" on colab but I have several errors:
1-  ""ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.
albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.""
3- ""---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
[<ipython-input-3-497b3f26180e>](https://localhost:8080/#) in <module>()
     13 config.set_training(False)
     14 model_path = ""ckpt/model.ckpt-6935893""
---> 15 render_refine, style_encoding = infinite_nature_lib.load_model(model_path)
     16 initial_rgbds = [
     17     pickle.load(open(""autocruise_input1.pkl"", ""rb""))['input_rgbd'],

9 frames
<__array_function__ internals> in prod(*args, **kwargs)

[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in __array__(self)
    747   def __array__(self):
    748     raise NotImplementedError(""Cannot convert a symbolic Tensor ({}) to a numpy""
--> 749                               "" array."".format(self.name))
    750 
    751   def __len__(self):

NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array."""
unexpected keyword 'visibility' in call to _config_setting_group,google-research/google-research,2022-04-22 07:26:44,1,,1074,1211934211,"Hi, I tried to run run_demo.sh. First it complains about no prefix ""rules-cc-master"" and suggests ""rules-cc-main"". The same for ""googletest-master"", so I changed them in WORKSPACE. Then I get the following error. Could someone help me on this? Thanks.

ERROR: /home/xiaoan/.cache/bazel/_bazel_xiao/a739b7a7b94483f081eeb0f6b7bbd028/external/com_google_absl/absl/BUILD.bazel:100:1: unexpected keyword 'visibility' in call to _config_setting_group(name, match_any = [], match_all = [])
ERROR: Analysis of target '//:run_search_experiment' failed; build aborted: Analysis failed
"
Question about KIP in kip directory,google-research/google-research,2022-04-22 06:40:33,0,,1073,1211891650,"* After doing data distillation on one dataset, can update function preserve the data distillation capability?"
infinite_nature:  can you provide training code？,google-research/google-research,2022-04-20 12:09:20,0,,1070,1209595857,
DALL-E 2 & Diffusion models,google-research/google-research,2022-04-19 01:51:50,0,,1069,1207620512,"Hi,

Can we leverage advancements made in DALL-E 2 training strategies and diffusion models in this project somehow?

Best,
Rakesh"
[cache_replacement] take long time to train,google-research/google-research,2022-04-15 10:56:48,0,,1068,1205488499,"I try to run this code : 
![image](https://user-images.githubusercontent.com/54568460/163562947-c3fae365-5104-4a8b-927f-b5da3e2cb708.png)

But it's take more than 10 hours and just train 36.000 / 1 mil .


![10h](https://user-images.githubusercontent.com/54568460/163563088-f3c75f10-1a1b-47aa-bef8-811168b52404.PNG)


"
Can we use tft for classification use cases?,google-research/google-research,2022-04-15 02:35:14,1,,1067,1205190763,I want to adapt TFT for my time series classification use case but there is no loss function available for such use case. Only quantile loss is available. Is there any way I can use TFT for binary classification?
"screen2words:  when I run create_tf_example_main.py, it reports an error ""TypeError: run_pipeline() missing 1 required positional argument: 'options'"".",google-research/google-research,2022-04-10 16:58:14,0,,1065,1199111482,Could you tell me the version of python and the package used? Maybe this is causing the error
Generating DMVR CSV Files for MSRVTT and YouCook2,google-research/google-research,2022-04-08 20:35:31,0,,1064,1197795050,"Hello! I am trying to replicate VATT.

I am trying to generate DMVR records as required for the two datasets you used for text-to-video retreival. I understand how to generate the YouCook2 csv files for this because the original annotations provide a start and end time and caption. 

However, I am struggling to understand how you generated the CSV file required for MSRVTT. 

In order to keep the replication accurate to your implementation, I was hoping you could provide such a file or reference on how to generate such a file for the MSRVTT dataset. 

Thanks!"
Best representation for 3D  one-hot sparse np.array,google-research/google-research,2022-04-06 13:09:03,0,,1061,1194593232,"@cll27 After reading the article ""On the Generalization of Representations in Reinforcement Learning"" I wonder what is the best way to represent  observation  which is 3D on-hot sparse np.array with size about (300,200,1000) and values 0 or 1.
The array represents solution of school timetable -  with 300 rooms, 200 groups and 1000 timeslots.
I thought to use 3D keras  convolution layers to prepare dense representation but I will be grateful for any sugestion.

Peter"
"dreamfields:     raise ValueError(""a must be an integer or 1-dimensional"")",google-research/google-research,2022-04-06 05:15:22,2,,1060,1194044758,"  File ""/home/google-research/dreamfields/dream/lib/python3.6/site-packages/jax/_src/random.py"", line 599, in choice
    raise ValueError(""a must be an integer or 1-dimensional"")
jax._src.traceback_util.UnfilteredStackTrace: ValueError: a must be an integer or 1-dimensional
"
[implicit_pdf] Paper results not reproducible,google-research/google-research,2022-04-06 02:20:28,0,,1059,1193923917,"Hi,

Can you please share the training command (with hyper-parameters) for reproducing numbers in Table 1 of the main paper?

I'm unable to reproduce reported results for implicit_pdf on the SYMSOL1 dataset. The paper (arxiv version) specifies the following hyper-parameters for reproducing results (Section S8):

- ""One network was trained for all five shapes of SYMSOL I"" (`--symsol_shapes symsol1`)
- ""... with a batch size of 128 images""
- ""... for 100,000 steps""
- ""three positional encoding terms were used for the query""
- ""four fully connected layers of 256 units with ReLU activation for the MLP""
- Note that the paper also says that it renders 100k images for each shape. However, the `symmetric_solids` dataset in tfds only contains 50k images each, and that's what I train on.

The corresponding training run command is: 
```bash
python -m implicit_pdf.train --symsol_shapes symsol1 \
 --number_fourier_components 3 \
 --batch_size 128 \
 --number_training_iterations 100000 \
 --head_network_specs 256 --head_network_specs 256 --head_network_specs 256 --head_network_specs 256 
```

However, the trained model seems to be overfitting as `gt_log_likelihood` starts reducing for `cyl` and `cone` after ~3k iterations. Please see the [uploaded tensorboard logs](https://tensorboard.dev/experiment/PLJkBENPR6aqG7VwJqzWAQ/). Reducing the depth of the MLP network to the default 2 layers didn't help either. 

Thanks and Regards,
Shubham"
[ScaNN] How to customize similarity metric.  ,google-research/google-research,2022-04-05 14:38:04,1,,1058,1193247561,"Hello, 
Thank you very much for this great contribution. 
I have just implemented your example.py file in the below link and I wonder how to implement my own similarity metric in this code. 

https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb
Thank you in advance."
[FNet] Is FNet stable with mixed precision training?,google-research/google-research,2022-04-04 22:45:18,0,,1057,1192390877,Just wanted to ask whether FNet is stable when using mixed precision training?
calculate ddgk graph embedding with GPU,google-research/google-research,2022-03-28 08:42:46,0,,1052,1183095717,"Hi!
How can I use GPU to get the ddgk graph embdding?
I have tried the following code:
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"".
The GPU memory usage is very high but the GPU-Util is close to 0.
I have also tried to replace multiprocessing.pool with 'for' loop, but it doesn't work either.
My version of tensorflow is 2.1.0.
Thanks for your reply!"
Cubert: How to get the embedding source code,google-research/google-research,2022-03-23 06:30:23,4,,1044,1177659584,"Dear @maniatis 

I fine-tuned Cubert in my own dataset, and I would like to get the embedding of source code (function)?"
When will the d3pm add the code about text space?,google-research/google-research,2022-03-21 08:42:28,1,,1039,1175063587,
An error about run_predict_image.py,google-research/google-research,2022-03-21 01:58:38,0,,1038,1174790534,"Hi，
When I run file ‘run_predict_image.py’, the error is reported as follows

File ""/home/xj/Dataset/WXQ/labelingcocodataset/musiq/run_predict_image.py"", line 144, in run_model_single_image
    model = model_mod.Model.patial(num_classes=num_classes, train=False, **model_config)
AttributeError: type object 'Model' has no attribute 'patial'"
[smurf] How to get the reported performance,google-research/google-research,2022-03-20 14:10:46,3,,1036,1174561854,"Thank you for the good work of SMURF. I am trying to replicate the reported performance on the training set of the Sintel dataset, namely 1.71 EPE on Clean and 2.58 EPE on Final.

I used apply_smurf.py on the sintel sequences but cannot get the same performance. Could you please provide some instructions on what commands to run?

Thank you."
google-research/musiq/  Partial function missing,google-research/google-research,2022-03-17 11:22:23,1,,1034,1172251947,"When I use this file, it prompts an error 
AttributeError: type object 'Model' has no attribute 'partial'

"
Sign Language Detection Inference Time,google-research/google-research,2022-03-16 11:29:40,2,,1032,1170895467,"This question is for the [paper](https://arxiv.org/abs/2008.04637) corresponding to this: https://github.com/google-research/google-research/tree/master/sign_language_detection part.
1] Would it be right to conclude from Table-1 (of the paper) that the inference times in Table-1 (0.35 ms in the worst case) can be ignored since in actual applications, the bottleneck latency will be due to the pose-estimation algorithm (> 300 ms)?

2] The paper mentions that the OpenPose's latency per frame is 300 ms. How does the demo then support real-time processing at 25 fps (i.e. < 40 ms delay per frame)? Is the 300 ms latency for heavy ResNet based versions?"
some questions about data-free distillation,google-research/google-research,2022-03-15 07:56:10,0,,1031,1169313277,"In ""Large-Scale Generative Data-Free Distillation"", the resnet34 pre-trained on imagenet32 achieves 60% top-1 auuuracy on val set. We attempt to train the model on imagenet32, however, only get about 35% top 1 accuracy. Could you provide the pre-trained weight of resnet34, or some suggests. thx!"
ipagnn: non-hashable static arguments,google-research/google-research,2022-03-11 20:22:16,1,,1025,1166830690,"`example` is a dict of numpy arrays, neither of which are hashable
https://github.com/google-research/google-research/blob/master/ipagnn/adapters/common_adapters.py#L108
"
about loss function,google-research/google-research,2022-03-10 12:11:03,0,,1024,1165142911,"Hi, appreciate of your great work! But i have some confuse about loss function.

if we have a minibath data: A， A1，A2，A3，B，C，D，E. Ai belongs to one class, and B, C, D, E belongs other 4 class.

the paper said eq2 let more positive data pair contribute to loss function, and positive pair (such as A•A1, A•A2, A•A3)  becomes lager, negtive pair  (such as: A•B, A•C, A•D) becomes smaller.

The eq2 indicate that the numerator inside log still only have one positive data pair and the denominator have all positive and negtive pair each calculate.

If A is anchor, A1，A2，A3 are positive data, and B，C，D，E are negtive data. the part of eq2 is： 
（log（A•A1 / ( A•A1 + A•A2 + A•A3 + A•B + A•C + A•D + A•E）+ 
    log（A•A2 / ( A•A1 + A•A2 + A•A3 + A•B + A•C + A•D + A•E）+ 
    log（A•A3 / ( A•A1 + A•A2 + A•A3 + A•B + A•C + A•D + A•E））/ 3 


then, when loss is going down, the numerator (such as A•A1) in log is going to get bigger and the denominator is going to get samller. Is that correct? It seems like every elements in denominator is going to get smaller expect A•A1. So that means A•A2, A•A3 in denominator are going to get smaller？Logically, the values of A•A2, A•A3 should be larger.

I'm confused about this.
looking forward to your replies! 
"
<AssembleNet> I have a question about assembleNet / + ,google-research/google-research,2022-03-10 10:00:34,0,,1023,1165005473,"hello 

I have a question while studying the assembleNet.

I wonder where the loss function for learning is on the code

If there is no loss function, I wonder why there is no loss function.

thank you"
ColTran: FID calculation,google-research/google-research,2022-03-07 14:20:33,3,,1020,1161482856,"Thanks for providing the codes. I'd like to know how to calculate the fid between the colorization and ground truth.
In your paper, you state that generate 5000 colorizations and there is no overlap between colorization and ground truths. So how the two datasets (colorization and ground truth) are splitted."
How to obtain the variable selection weights corresponding to different quantile prediction?,google-research/google-research,2022-03-07 13:23:48,0,,1019,1161414783,"Hello, I obtained ""attention_weights"" with the following command: 
weights = libs.tft_model.TemporalFusionTransformer(dictMerged).get_attention(raw_data)

I now have three problems:
(1) tft_ model. py explains that ""decoder_self_attn"" in ""attention_weights"" is the time attention weight. What is the time attention weight?
(2) Are ""static_flags"", ""historical_flags"" and ""future_flags"" in ""attention_weights"" the result of variable selection network output?
(I don't know why I can't upload pictures. These attributes are mentioned in line 1020 of tft_model. py)
(3) According to the above question (2), how to obtain the weight of each variable corresponding to P10, P50 and P90?


"
Bug occurred in graph_embedding/persona,google-research/google-research,2022-03-07 08:18:34,0,,1018,1161052727,"When I run the function `Splitter` of `splitter.py`, `unsupported operand type(s) for +: 'int' and 'str'` occurred in line 267 where the code is `iter=iterations` , how can I solve it?
Thanks!"
Google Collab; AlreadyExistsError,google-research/google-research,2022-03-06 19:41:43,0,,1017,1160697397,"Hello, im getting the following error after these lines in the import cell of the google collab:

""import tensorflow as tf
tf.config.experimental.set_visible_devices([], ""GPU"")""



![2022-03-06_20h38_04](https://user-images.githubusercontent.com/58479885/156939295-5bd14151-7fe6-4f5a-b89a-f989e34d44f5.png)

Any help would be much appreciated, 
Thanks 
"
Temporal Fusion Transformers generate attention weights,google-research/google-research,2022-03-06 12:29:50,0,,1015,1160598420,"Hello,

I also have a problem while getting the attention weights from the model with code:
model.get_attention(train)
after I load the model generated by:
!python3 -m script_train_fixed_params $EXPT $OUTPUT_FOLDER $USE_GPU
from model_folder

I got the following errors:
![image](https://user-images.githubusercontent.com/68455198/156923199-9b15570d-0ecd-4dd5-8311-880a8d88802f.png)

May I know how to solve this problem?

Thanks,
Erica"
[Cluster GCN][Amazon2M Dataset] Raw text for the labels?,google-research/google-research,2022-03-04 06:18:36,0,,1014,1159289395,"Hi there,

Refering to this issue [https://github.com/snap-stanford/ogb/issues/303](https://github.com/snap-stanford/ogb/issues/303). I was wondering if you have the **original text data** for all the 47 labels in Amazon2M dataset?

Thanks."
Internal state tflite,google-research/google-research,2022-03-03 16:43:18,3,,1013,1158641683,"Hi, 

Im looking to produce a tflite model with internal streaming state. However, it looks like the code only produces external state streaming tflite models. Is there a limitation with doing this, or is this just something that has not been implemented yet? 

Thanks,
Brett Young 

@rybakov"
《Transformer Quality in Linear Time》-[Submitted on 21 Feb 2022]  ,google-research/google-research,2022-03-03 07:54:38,0,,1012,1158084334,"https://arxiv.org/abs/2202.10447

Excuse me， When will the code and pre training model of this paper be submitted?
thanks！
"
"PERFORMANCE, Computational costs, Background process",google-research/google-research,2022-03-01 08:37:18,0,,1010,1155033955,"Dear machine learning Aces, 

What happens when I set the epochs setting to 1 ? 
I have been trying to execute the electricity model with the script_hyperparam_opt. 

I know I am not using a GPU, but it takes forever... However, my PC parameters are not too bad too...
![image](https://user-images.githubusercontent.com/100216305/156133314-5ce735a5-1cc0-41d4-81d4-c412bbdcaa2e.png)

I really dont get what is happening in the background, when it says (see below) and runs the same multiple times:

None
*** Fitting TemporalFusionTransformer ***
Getting batched_data
Using cached training data
Using cached validation data
Using keras standard fit
Train on 450000 samples, validate on 50000 samples
273152/450000 [=================>............] - ETA: 3:30:42 - loss: 0.4841
"
[SCANN] Is it possible to dynamically increase the database?,google-research/google-research,2022-02-24 13:13:10,1,,1007,1149274342,I've looked for it but I've not found any documentation about scann rather than just initializing and searching on it. Is it possible to add new data and create new clusters?
TFT model  experiments data kaggle retail dataset  favorita process error,google-research/google-research,2022-02-17 08:09:45,1,,1006,1141002035,"When I try to train tft model with kaggle retail dataset  favorita, I got the following error running script_download_data.py 
process_favorita function
![image](https://user-images.githubusercontent.com/22293352/154432588-a6b02c64-eb99-4043-88ea-f7d9c23aa202.png)
"
Simon,google-research/google-research,2022-02-12 10:36:14,0,,1003,1133839905,
[VATT] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.,google-research/google-research,2022-02-11 08:07:11,5,,997,1132029062,"Hi, @hassanhub. I encountered this problem and  failed to solve it after searching for several solutions to similar(just look like similar) issues.

As you mentioned in #962, I modified the `objectives.py` for conducting a multi-gpu pre-training experiment and encountered this problem. Then I tried to inplement a single-gpu version and also encountered this problem.

The error information is as:
```
2022-02-11 15:07:20.266179: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.266326: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.266722: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.268130: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.268490: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
Traceback (most recent call last):
  File ""main.py"", line 97, in <module>
    app.run(main)
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 93, in main
    return executor.run(mode=params.mode)
  File ""experiments/base.py"", line 439, in run
    self.train()
  File ""experiments/base.py"", line 280, in train
    metrics = train_step(data_iterator, num_iterations)
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
         [[{{node strided_slice_3}}]]
         [[while/body/_1/while/IteratorGetNext]]
         [[while/body/_1/while/model/tx_mlp_fac/backbone_stack/audio_module/wat_base/RandomShuffle/_72]]
  (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
         [[{{node strided_slice_3}}]]
         [[while/body/_1/while/IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_step_97988]

Function call stack:
train_step -> train_step

2022-02-11 15:07:20.269632: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.269706: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.271021: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.271261: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.271735: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.274001: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.274614: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.275279: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.275625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.276116: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds

I mainly modified the `base.py` and `objectives.py`. Since I'm not familiar with Tensorflow framework, the code base_and_objectives.tar.gz may exists a lot of  ridiculous problems. Do you have some idea about this problem?

Thanks."
Capturing Dual Pixel Images using Google Pixel 4a Smart Phone,google-research/google-research,2022-02-10 12:54:24,0,,993,1130023687," Following the link, https://github.com/google-research/google-research/tree/master/dual_pixels#android-app-to-capture-dual-pixel-data
 the app to capture dual pixel imagery is installed. While I was capturing dual pixel data using Google pixel 4a , I could see the message "" xxxxxxx is saved"" coming . As no google pixel camera is with SD card, the captured image(s) are not being directed to the path ""/sdcard/Android/data/com.google.reseach.pdcapture/files""

The issues Image(s) are not getting saved in the specified directory."
[Scaling Efficiently: Insights from Pre-training and Finetuning Transformers] Some checkpoints seem to be incomplete or incorrectly uploaded,google-research/google-research,2022-02-09 12:06:00,8,,986,1128455369,"Hey @vanzytay,

Thanks a lot for uploading all of your newest T5 checkpoints to GCP. 
It's really amazing that you guys are taking the time to make the weights available to the community. Especially, the smaller versions `{small, mini, tiny}` are extremely useful IMO.

I've started an automatic conversion of these checkpoints to Transformers (see: https://huggingface.co/NewT5/) to make them even more available. The idea is to convert them to all frameworks we offer: PyTorch, Tensorflow, and JAX and give them a bit more visibility - hope this is ok for you! Before releasing them we would move them to the Google org on the Hub.

The conversion is going well so far - however I've noticed the that the following checkpoints seem to be incomplete or missing:
1. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_lg_h8_l16_law_03-21-23-05 has no checkpoints
2. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_lg_h8_l32_law_03-21-23-06
3. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_3B_l28_law_04-04-11-05 

For 1. there are no checkpoints at all.
For 2. & 3. it seems like only the checkpoints up to step 306000 are available - should I just take those for the conversion or are the 524288 ones available somewhere?

"
hitnet problem of dilated convolution,google-research/google-research,2022-02-09 07:27:46,0,,985,1128154241,"The author of hitnet used dilated convolution in the last three propagations of the paper, but the model you provided did not use dilated convolution. Why? Is it because this works better ?

"
A bug in ScannInterface::Serialize(std::string path) ?,google-research/google-research,2022-02-08 11:01:42,1,,984,1127101478,"Hi,

I have a question about the scann source code.

In the scann.cc, the `ScannInterface::Serialize(std::string path)`, there is a [circle](https://github.com/google-research/google-research/blob/master/scann/scann/scann_ops/cc/scann.cc#:~:text=for%20(const%20auto,datapoint_to_token.npy%22%2C%20datapoint_to_token)) to serialize `datapoint_to_token` into file `datapoint_to_token.npy`:

The method `VectorToNumpy()` is defined in the scann/utils/io_npy.h, it uses the `OpenSourceableFileWriter` class to write data into file datapoint_to_token.npy.
The `OpenSourceableFileWriter` constructor is:
```
OpenSourceableFileWriter::OpenSourceableFileWriter(absl::string_view filename)
    : fout_(std::string(filename), std::ofstream::binary) {}
```
The `fout_` is a std::ofstream, it is not appended mode. The `datapoint_to_token.npy` will be overwritten by the circle for multiple times, is it expected?"
coltran colab notebook link down,google-research/google-research,2022-02-08 05:29:20,3,,982,1126812484,It seems the link to the coltran colab notebook (https://tinyurl.com/coltranfewclicks) has become invalid. Could you fix it?
Lidar to trainable dataset pipeline,google-research/google-research,2022-02-07 17:56:51,0,,981,1126315017,"Hi, I couldn't find pipeline to convert *.las file to trainable dataset format. How are we supposed bro train on custom datasets without this feature?"
VATT Pytorch Version,google-research/google-research,2022-02-07 06:31:00,2,,978,1125571461,"Hi, Thanks for sharing your VATT work! Will you provide PyTorch version of the VATT code and checkpoints? That will be more helpful, thanks!"
NaN loss in proposed loss,google-research/google-research,2022-02-04 11:12:29,0,,972,1124083494,"in non_decomp/ for the paper ""Teaching overparametrised models...' by harikrishna et al, in the execution of CSL loss, there seems to be an issue with the loss function, becomes NaN after a few epochs.
"
Dream fields colab notebook jax 32 bit / 64 bit warning,google-research/google-research,2022-01-31 07:01:05,1,,964,1119044236,"@ajayjain

It's working regardless but I noticed that this warning is shown:

```
/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3662: 
UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32.
To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable.
See https://github.com/google/jax#current-gotchas for more.
  lax._check_user_dtype_supported(dtype, ""zeros"")
```"
Dream fields colab notebook matlibplot version issue,google-research/google-research,2022-01-31 05:19:17,0,,963,1118987293,"@ajayjain 

https://colab.research.google.com/drive/17GtPqdUCbG5CsmTnQFecPpoq_zpNKX7A?usp=sharing#scrollTo=5NN3LHPaYORI

In the Colab notebook Cell 3 does not fix the matlibplot version problem. Moving the cell to the top, running it, and restarting restarting the runtime fixes the issue though.

```
# Upgrade packages to deal with version mismatch. Restart after running this cell.
!pip install --upgrade matplotlib numpy"
[VATT]No OpKernel was registered to support Op 'AllToAll',google-research/google-research,2022-01-31 04:44:37,2,,962,1118968219,"Hi @hassanhub, sorry for the interruption.
I have followed the instruction with the command 'python -m vatt.main --task=pretrain --mode=train --model_dir=PATH/TO/RUN --model_arch=tx_fac --strategy_type=mirrored' on the GPU environement.
The TPU configuration is set to None.
However it got the error as following, which seems related to the TPU.
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'AllToAll' used by {{node StatefulPartitionedCall/AllToAllGather}} with these attrs: [split_count=4, concat_dimension=1, split_dimension=0, T=DT_FLOAT]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>
Do you  the idea for the error?"
Windows 10 - Python int too large to convert to C long ,google-research/google-research,2022-01-28 16:57:54,1,,961,1117631976,"if you get this error,   look in wit.py and wit_kaggle.py in the lib/site-packages/tensorflow_datasets/vision_language

wit.py and wit_kaggle.py need this fix

import ctypes as ct
csv.field_size_limit(int(ct.c_ulong(-1).value // 2))
#csv.field_size_limit(sys.maxsize)                                                                                                                              
limit1 = csv.field_size_limit()
""0x{0:016X}"".format(limit1)


fixes it.   Wrong ""Long"" type apparently.
"
Process Darmstadt raw data to RGB color image,google-research/google-research,2022-01-27 02:34:45,0,,960,1115701860,"I tried reading the raw data and converting it to RGGB Bayer domain, like dnd_denoise.py does, and feeding the data to process.py to get an RGB color image, but I get weird colors. However, if I swap the flip direction of the two bayer_pattern cases, I get the correct color.

I'm just wondering if it's because we do the transpose on the image matrix but not on the bayer_pattern or I do something wrong?
Below is part of the code I used to convert the raw data to an RGB image.

```
img = h5py.File(filename, 'r')
Inoisy = np.float32(np.array(img['Inoisy']).T)
bayer_pattern = np.asarray(info[info['camera'][0][i]]['pattern']).tolist()

if (bayer_pattern == [[1, 2], [2, 3]]):
    pass
elif (bayer_pattern == [[2, 1], [3, 2]]):
    Inoisy = np.flipud(Inoisy)                <==== Here I change the flip direction
elif (bayer_pattern == [[2, 3], [1, 2]]):
    Inoisy = np.fliplr(Inoisy)                <==== Here I change the flip direction
else:
    print('Warning: assuming unknown Bayer pattern is RGGB.')

height, width = Inoisy.shape
channels = []
for yy in range(2):
    for xx in range(2):
        noisy_crop_c = Inoisy[yy:height:2, xx:width:2].copy()
        channels.append(noisy_crop_c)
channels = np.stack(channels, axis=-1)

Inoisy = process.process(channels, red_gains, blue_gains, cam2rgbs)

```
"
[Question] Can I use it for ANN of 200M vectors of 256D?,google-research/google-research,2022-01-20 19:37:11,0,,953,1109693024,"Hi there,

I am trying to use ScANN (with both backends, including TF with GPU) on my data. The data consists of 200M vectors of 256 Dims. Even if I use big EC2 instance of 744G RAM I still get Out of memory exception, even if I set all hyperparameters to extremely small values:

```
    searcher = scann\
        .scann_ops.builder(all_embeddings, 50, ""dot_product"")\
        .tree(num_leaves=60, 
              num_leaves_to_search=1, 
              training_sample_size=300)\
        .score_ah(2, anisotropic_quantization_threshold=0.2)\
        .reorder(100)
```

The embeddings are already loadded into memory, but after build() starts to work I see that it just starts taking more and more RAM using 1 core only (so I guess actual training haven't started yet). 

The same situation happens if I use GPU, but it first loads something in GPU, but then fails.

Does ScANN actually supports data of this size? If yes, what can I do to successfully run it?"
Is there plan to support N-D (N >= 4) sparse conv? ,google-research/google-research,2022-01-18 23:41:55,0,,949,1107471584,"if not, hint/pointer to how to extend is also greatly appreciated!"
"tensorflow.keras.layers TextVectorization: adapt() with output_mode=""tf_idf"" (GPU only) throws InvalidArgumentError: INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string ",google-research/google-research,2022-01-17 18:04:57,1,,947,1106136417,"**Behaviour observed with**:
* Python 3.7 Tensorflow 2.6, Py 3.7 TF 2.7, Py 3.8 TF 2.7, Py 3.9.9 TF 2.7.0, 
* GPUs: 
    * GeForce GTX 1660 SUPER, compute capability: 7.5, 5944MiB, CUDA 11.2
    * Tesla M60, compute capability: 5.2, 8129MiB, CUDA 11.5

**Error Message**:
```
InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
	 [[map/while/loop_body_control/_21/_51]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_adapt_step_1288]

Function call stack:
adapt_step -> adapt_step
```
**ReprEx**:
```
import os, pathlib
import tensorflow as tf
from tensorflow import keras

base_dir = pathlib.Path(""RepRex"")
os.makedirs(base_dir / ""neg"", exist_ok = True)
os.makedirs(base_dir / ""pos"", exist_ok = True)

with open(base_dir / ""pos"" / ""sample1.txt"", ""w"") as f:
    f.write(""The food was excellent.\n"")

with open(base_dir / ""neg"" / ""sample1.txt"", ""w"") as f:
    f.write(""The wine was awful, we will never go to that place again.\n"")


batch_size = 2
ds = keras.utils.text_dataset_from_directory(""RepRex"", batch_size=batch_size)
text_only_ds = ds.map(lambda x, y: x)   # Prepare a dataset that only yields raw text inputs (no labels).

text_vectorization = keras.layers.TextVectorization(max_tokens=20000, output_mode=""tf_idf"")
print(""Running adapt() with 'tf_idf' on CPU"") # ok
with tf.device('/cpu:0'):
    text_vectorization.adapt(text_only_ds)  
    
text_vectorization2 = keras.layers.TextVectorization(max_tokens=20000, output_mode=""count"")
print(""Running adapt() with 'count' on GPU"") # ok
with tf.device('/gpu:0'):
    text_vectorization2.adapt(text_only_ds)  

text_vectorization3 = keras.layers.TextVectorization(max_tokens=20000, output_mode= ""multi_hot"")
print(""Running adapt() with 'multi_hot' on GPU"") # ok
with tf.device('/gpu:0'):
    text_vectorization3.adapt(text_only_ds)  

text_vectorization4 = keras.layers.TextVectorization(max_tokens=20000, output_mode=""tf_idf"")
print(""Running adapt() with 'tf_idf' on GPU"") # error
with tf.device('/gpu:0'):
    text_vectorization4.adapt(text_only_ds)  
 
```

** ReprEx output**:
```
Found 2 files belonging to 2 classes.
Running adapt() with 'tf_idf' on CPU
Running adapt() with 'count' on GPU
WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe5d80cec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Running adapt() with 'multi_hot' on GPU
WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe5d80ceaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Running adapt() with 'tf_idf' on GPU
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/tmp/ipykernel_610484/3457753437.py in <module>
     37 print(""Running adapt() with 'tf_idf' on GPU"")
     38 with tf.device('/gpu:0'):
---> 39     text_vectorization4.adapt(text_only_ds)
     40 

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py in adapt(self, data, batch_size, steps)
    242       with data_handler.catch_stop_iteration():
    243         for _ in data_handler.steps():
--> 244           self._adapt_function(iterator)
    245           if data_handler.should_sync:
    246             context.async_wait()

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     56   try:
     57     ctx.ensure_initialized()
---> 58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     59                                         inputs, attrs, num_outputs)
     60   except core._NotOkStatusException as e:

InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
	 [[map/while/loop_body_control/_21/_51]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_adapt_step_1288]

Function call stack:
adapt_step -> adapt_step
```"
Google Landmark Federated split is missing files,google-research/google-research,2022-01-17 12:46:04,2,,946,1105812743,"Hi!
I've started working with the gldv2 dataset for FL using the split provided [here](https://github.com/google-research/google-research/tree/master/federated_vision_datasets).
In particular, I'm using the [dataset loader](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/gldv2/load_data) from tensorflow federated.

I've noticed that roughly a 10% of images are missing: [this](https://github.com/tensorflow/federated/blob/da2ca219e75e3317c43e10a83d07b635aad9cfd2/tensorflow_federated/python/simulation/datasets/gldv2.py#L108) exception is raised when creating the clients tfrecords.
It looks that some filenames in the federated splits are not matching filenames from the original dataset. 

I've collected the missing files in a [json file](https://drive.google.com/file/d/1HbMKi8LrWCwldhl7H4DuZmEJcLBSavgq/view?usp=sharing) that you can check (list of dict - one element per client - with fields `user_id`, `total`, `found`, `missing`, `missing_files`)

@hang-qi I'm keeping bugging you sorry :)"
"flare_removal requires keras, what is the version of keras",google-research/google-research,2022-01-17 03:36:13,1,,945,1105331171,"flare_removal requires keras, what is the version of keras"
[Hitnet] Right Disparity Image,google-research/google-research,2022-01-12 20:43:35,0,,942,1100772212,"Thanks for your amazing work. I was trying to generate the right disparity image from the given demo files. The code throws an error 
KeyError: ""The name 'graph/secondary_output_disparity:0' refers to a Tensor which does not exist. The operation, 'graph/secondary_output_disparity', does not exist in the graph.""
To double-check, I printed the names of all the tensors in the graph and was not able to find a key that matches the word ""secondary"". 
Can you please let me know how I can get the right disparity image?
Thanks in advance."
"[scalable_shampoo] Default parameters of distributed_shampoo don't work, pmap/pjit mode is forced",google-research/google-research,2022-01-11 18:43:12,3,,941,1099504618,"Trying to call `distributed_shampoo` (as the version in the JAX folder doesn't support Optax) without setting any parameter other than `learning_rate` and `block_size` results in this error:
```py
    974     else:
--> 975       to_pad = -num_statistics % num_devices_for_pjit
    976       padded_statistics = [pad_matrix(stat, max_size) for stat in statistics]
    977       padded_statistics.extend([

TypeError: unsupported operand type(s) for %: 'int' and 'NoneType'
```

Forcing me to set a value for `num_devices_for_pjit`, while in the source these comments are written at [line 580](https://github.com/google-research/google-research/blob/03cb03de1583d48940ff087987fabffe85710f98/scalable_shampoo/optax/distributed_shampoo.py#L580):
```py
    ### Only set following 3 params in pjit/spmd mode.
    ### WARNING: Experimental
    mesh_axis_names=None,
    num_devices_for_pjit=None,
    shard_optimizer_states=False,
```
Passing `num_devices_for_pjit = 1` also brings about another error, now caused by what I think is an error in the code at [line 1012](https://github.com/google-research/google-research/blob/03cb03de1583d48940ff087987fabffe85710f98/scalable_shampoo/optax/distributed_shampoo.py#L1012), `if not batch_axis_name:` then enters a branch that uses the **PJIT** mode for matrix inverse pth root, while I think that logically if `batch_axis_name` isn't set pjit mode shouldn't be enabled (As commented on [line 578](https://github.com/google-research/google-research/blob/03cb03de1583d48940ff087987fabffe85710f98/scalable_shampoo/optax/distributed_shampoo.py#L578)).

Going ahead and trying to patch this doesn't seem to make some progress and this is where I ended up, I hope something can be done in being able to run the Optax optimizer **WITHOUT** needing to configure everything in terms of batches.

If this isn't possible I hope there's a chance for the non distributed optimizer to be adapted to Optax."
[depth_from_video_in_the_wild]: A question about occlusion aware mask,google-research/google-research,2022-01-11 14:31:00,1,,940,1099233309,"https://github.com/google-research/google-research/blob/742c8e68a411247264ef338a430a95be9e700faf/depth_and_motion_learning/consistency_losses.py#L95

I just can't understand why the occlusion mask is generated by comparing between `frame1transformed_depth.depth` and `frame2depth_resampled`. Since `frame1transformed_depth` is at frame2's location, `frame2depth_resampled` is at frame1's location, what's the meaning of compare the depth at the same pixel when they are at different location? Does not compare `frame1depth` and `frame2depth_resampled` at the same pixel more reasonable?"
[concept_explanations] can’t open google cloud link,google-research/google-research,2022-01-09 16:11:14,0,,939,1097236112,"Hi, @chihkuanyeh  
When I click the link ""https://console.cloud.google.com/storage/browser/concept_discovery""
I got the following error:
  _Additional permissions required to list objects in this bucket. Ask a bucket owner to grant you 'storage.objects.list' permission._ 
How can I fix it.

Thank you in advance."
Queries on Meta Pseudo Labels,google-research/google-research,2022-01-05 15:54:45,0,,937,1094498684,"Hi,

I have been looking into the Meta Pseudo Labels paper https://arxiv.org/abs/2003.10580 and checking out the corresponding code herein. First of all, thanks for the great work and sharing the code. I have a couple of questions regarding the batch size used in the paper.

1. The paper mentions (Section 3.3 Implementation Details) about using a batch size of 4096 images for labelled data and 32768 for unlabelled data in a single iteration step. I was thinking about the impact if the Student is updated using pseudo labels on unlabelled data with a batch size of 4096 and repeat the process 8 times (effective batch size of 32768) and then update the Teacher with the feedback of Student's performance on 4096 labelled images. Could you please share your thoughts on this, or if you tried any experiments with this strategy?
2. How would the method perform if trained with a smaller batch size, e.g. 256, 512 etc? Would its performance be close to the paper results or drop down significantly?

It would be really great if you could provide some insights into the above queries on batch size or if you have some results with these that you could please share.

Looking forward to your reply.

Thank You !!

Anuj"
"I
Iresch",google-research/google-research,2022-01-04 15:34:25,1,,936,1093471854,
tf3d & yocto,google-research/google-research,2022-01-03 16:57:51,0,,934,1092666621,Are there any yocto recipes for this framework?
kws_streaming dataset for 12 labels,google-research/google-research,2022-01-01 01:43:17,1,,932,1091711933,"Hi,

There are 35 labels audio data in speech_commands_v0.02.tar.gz but I can't see the setting which label will be used for training and test.
When we want to do [12 labels experiment](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md), should we set which labels are used? or Should they be set automatically correctly?
(I've seen [12 labels experiment](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md) have been done because the final layer has 12 output)
And if we want to 12 labels experiment, do dataset actually need 35 labels data?

Thank you"
Name misses letter A,google-research/google-research,2021-12-30 07:17:57,1,,928,1090921687,"https://github.com/google-research/google-research/blob/bbee4fa1d4459c2505052d689cb8ecca4f1360a7/poem/run.sh#L57

TEMPORL_PR_VIPE_TRAIN_DIR->TEMPORAL_PR_VIPE_TRAIN_DIR"
keyword streaming --split_data 0 _silence_  class ,google-research/google-research,2021-12-29 22:25:07,2,,927,1090776721,"I've noticed when using a custom data split with this repo, theres no silence or unknown audio added to the dataset, similar to how it is added in when the default subfolder organization is used. It's simple to add an unknown class by just adding a 'unknown' folder in the dataset with a bunch of random audio files, however, this solution doesn't add silence clips, and it's tedious to make a directory full of silence clips. I've added a simple way to add silence in my own local copy of this repo. Was wondering if the authors of this repo would be interested in me turning this option into something that can be controlled with command line flags, and making a pull request for this feature? 
@rybakov

Thanks,
Brett"
[Source code of SNIP],google-research/google-research,2021-12-29 10:51:56,0,,926,1090434689,"Hi, i could not find the source code of SNIP, where is metioned in the paper ""Pruning Redundant Mappings in Transformer Models
via Spectral-Normalized Identity Prior"".

Could you release the code about this paper(SNIP), Thanks!"
[VATT] Construct and load datasets using DMVR,google-research/google-research,2021-12-27 02:47:06,18,,925,1088919192,"Hi, @hassanhub. I have some problem with DMVR.

I followed the [dmvr example](https://github.com/deepmind/dmvr/tree/master/examples) to generate the dataset.
And changed the corresponding code so the code can read the correct path of data.

But when I run the vatt evaluation process, it would raise following error:

> tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
>   (0) INVALID_ARGUMENT:  Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
>          [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
>          [[MultiDeviceIteratorGetNextFromShard]]
>          [[RemoteCall]]
>          [[IteratorGetNext_3]]
>          [[model/video_module/vit_base/spatio_temporal_embeddings/strided_slice_5/_1]]
>   (1) INVALID_ARGUMENT:  Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
>          [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
>          [[MultiDeviceIteratorGetNextFromShard]]
>          [[RemoteCall]]
>          [[IteratorGetNext_3]]
>   (2) CANCELLED:  Function was cancelled before it was started
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_evaluation_step_21087]
> 
> Function call stack:
> evaluation_step -> evaluation_step -> evaluation_step

I located the error at line150 in file ./vatt/data/loading.py where the object generate by command `factory.make_dataset()` cannot be read. The minimize pseudocode is as follow:

```
dataset =  factory.make_dataset(
              shuffle=self.shuffle,
              num_epochs=self.num_epochs,
              batch_size=per_replica_batch_size,
              padded_batch=False,
              drop_remainder=True,
              keep_key=False,
              override_preprocess_fn=None,
              )
iter_data = iter(dataset)
next(iter_data) // this would trigger the error
```

-------------------------------------------------------

Here is the pipeline I used to generate the data file. I write some info in the README.txt.
[dmvr_example.tar.gz](https://github.com/google-research/google-research/files/7769292/dmvr_example.tar.gz)

I cannot identify whether I miss used the dmvr, or I miss code the vatt data reader.

What should I do?

Thanks
"
[FELIX] Warnings raise when train felix,google-research/google-research,2021-12-22 07:27:34,0,,923,1086533531,"
![image](https://user-images.githubusercontent.com/9492778/147052569-67ec9e3c-4e23-4fc9-8fd1-5a0de32b71ba.png)
"
[widget-caption][screen2words] Could you please provide the requirements for running?,google-research/google-research,2021-12-20 13:22:58,0,,922,1084788164,"Hi, I am a beginner in UI understanding and found your amazing work, but I have some trouble reproducing your work.
I tried to run your code on the recent environment (`python 3, tensorflow==2.7.0, apache_beam==2.34.0`), but I got an error:
```
Traceback (most recent call last):
  File ""E:/github/google-research/widget_caption/create_tf_example_main.py"", line 364, in <module>
    app.run(main)
  File ""D:\Anaconda3\envs\tf25\lib\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""D:\Anaconda3\envs\tf25\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""E:/github/google-research/widget_caption/create_tf_example_main.py"", line 360, in main
    runners.DataflowRunner().run_pipeline(pipeline)
TypeError: run_pipeline() missing 1 required positional argument: 'options'
```

It seems that `run_pipeline` in the recent `apache_beam` (`2.34.0`) takes two input parameters: `pipeline` and `options`. 
I checked the `apache_beam` API documentation and found that until version 2.9.0, `run_pipeline` only needs the parameter `pipeline`, but version 2.9.0 and previous versions only support python 2.7 but not python 3, which makes me confused. 
Could you please provide the requirements for running?"
Issues of parameter setting in 'meta pseudo labels',google-research/google-research,2021-12-19 08:14:36,0,,921,1084040157,"I use google colab-tpu to run the official MPL code, but i got some questions.
Question 1: i can not find the parameter 'num_cores_per_replica' in main.py(line 71). And the paremeter is not defined in flag_utils.py.
Question 2: i donot know which tpu_lib is imported in main.py(line 66).
thank you very much."
[smurf] can't get pre-trained model checkpoints,google-research/google-research,2021-12-17 08:50:39,1,,918,1083023149,"Hi @AustinCStone 
When I click the link at: https://console.cloud.google.com/storage/browser/gresearch/smurf/kitti
I got nothing but a blank page like this:
![image](https://user-images.githubusercontent.com/58680686/146515861-0e8cf497-5b4f-4f21-b61c-74d66bd791b9.png)

Then I use: gsutil cp -r gs://gresearch/smurf* /tmp/smurf/
I got an error:
root@mygpu:~/smurf# gsutil cp -r gs://gresearch/smurf* /tmp/smurf/
Can't locate LWP/UserAgent.pm in @INC (you may need to install the LWP::UserAgent module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.22.1 /usr/local/share/perl/5.22.1 /usr/lib/x86_64-linux-gnu/perl5/5.22 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.22 /usr/share/perl/5.22 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base) at /usr/bin/gsutil line 26.
BEGIN failed--compilation aborted at /usr/bin/gsutil line 26.

Is there any other way to get the pre-trained model?
Thank you in advance."
Add to index,google-research/google-research,2021-12-15 01:38:44,0,,916,1080466567,Is there an equivalent of faiss index.add() in ScaNN?
Running ETC model using GPU,google-research/google-research,2021-12-10 18:32:49,0,,915,1077122934,"How can I run ETC (Extended Transformer Construction) model using the graphic card (GPU) on my own system (windows 10). I mean, I am not going to use google Cloud.
I just use the WikiHop fine-tuning section of the model. The section employs **apache beam** and I think it does not work well. Because, I got an error which is shown in the following highlighted line ([data_utils.py](https://github.com/google-research/google-research/blob/master/etcmodel/models/wikihop/data_utils.py))


![image](https://user-images.githubusercontent.com/42575196/145620192-1c54acea-114c-4a83-88fd-f97dafd24f8f.png)


error:
```
raise JSONDecodeError(""Expecting value"", s, err.value) from None
RuntimeError: json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1) [while running 'DevParseExample']
```

I think, I got the error because I did not use google cloud. I appreciate to inform me if you have any idea.
"
[MUSIQ] Will you also release the training code?,google-research/google-research,2021-12-10 18:22:37,6,,914,1077115074,"@junjiekCopybara-Service
@junjiek

I am impressed with the approach and work done with MUSIQ for image quality assessment.  I would like to use it to expand the capabilities and even train with additional data for specific IQA instances.

At the moment it appears that this code is for doing a prediction using one of the model checkpoints created from the competitive datasets.  I would like to train the approach on some unique datasets and train it further.

Although I think I may be able to come up with the code to train a model based on the paper, I wanted to see if you would be willing to release your training scripts as well so I could train new datasets quicker.  Thanks."
Bug in bnn_hmc repo,google-research/google-research,2021-12-07 22:55:02,1,,913,1073819364,"We are trying to reproduce the results presented in the paper ""![What Are Bayesian Neural Network Posteriors Really Like?](https://arxiv.org/abs/2104.14421)"". However, when we tried to run the script, we found that it throws an error. After examining the code, we found that the problem is in https://github.com/google-research/google-research/blob/730deec43de1980d70eeb5d0a2d93e723d334fc7/bnn_hmc/utils/train_utils.py#L204
![图片](https://user-images.githubusercontent.com/34415979/145117704-17245884-98e1-4c16-8654-2bfe4a66acd9.png)
The order of arguments of this function is not consistent with how it is called in https://github.com/google-research/google-research/blob/730deec43de1980d70eeb5d0a2d93e723d334fc7/bnn_hmc/run_hmc.py#L122
And it also uses the wrong order of arguments when it calls `_perdevice_log_prob_and_grad` https://github.com/google-research/google-research/blob/730deec43de1980d70eeb5d0a2d93e723d334fc7/bnn_hmc/utils/train_utils.py#L147
"
Knowledge distillation on regression task,google-research/google-research,2021-12-07 07:06:02,0,,911,1072991229,"I noticed the conclusion in your paper - ""In contrast, Single -> Multi knowledge distillation improves or matches the performance of the other methods on all tasks except STS, the only regression task in GLUE. **We believe distillation does not work well for regression tasks because there is no distribution over classes passed on by the teacher to aid learning**"". 


Did you test the knowledge distillation technique on other regression tasks?  E.g., noise reduction.

Looking forward to your reply.

B.R.
"
tf3d with tensorflow 2.7.0?,google-research/google-research,2021-12-05 04:42:57,0,,910,1071371135,"Hi,
I wonder whether tf3d will become available in tensorflow 2.7.0 or future versions.

Sparse conv is often used for point cloud recognition and is very important. I think many researchers will want to use it with latest tensorflow. However, currently, tf3d is only supported for tensorflow 2.3.

I read its build codes(WORKSPACE), and found comments for future works. For example,
[https://github.com/google-research/google-research/blob/4a383cf4eef1144b5c711802393fd91407f38323/tf3d/ops/WORKSPACE#L77](https://github.com/google-research/google-research/blob/4a383cf4eef1144b5c711802393fd91407f38323/tf3d/ops/WORKSPACE#L77)

So, I expect tf3d will be updated. Do you have such a plan?

Thank you."
Temporal Fusion Transformers - Val Losses don't improve at all,google-research/google-research,2021-11-26 12:13:46,0,,904,1064410861,"I'm experimenting with the 4 dataset in the original paper. One thing that I noticed is that val losses usually gets it's best value in first couple of epochs and don't improve or stays the same. Is this normal? What is the point of training more than 2-3 epochs than? Also predefined eary calls usually stops the network between 10-20 epochs. The paper says that the model is trained for 6 hours on a V100 for electiricty dataset. Is there a reason why I am not seeing val loss improvements, or this is normal and that 6 hours training time is with hparam search included. I'd appreciate if you help me understand, thanks."
[MobileBERT] How to train IB-BERT before using MobileBERT,google-research/google-research,2021-11-23 23:43:27,0,,903,1061821788,"I tried to make a model by using MobileBERT.
And I'm struggling to train IB-BERT first.
But There is no any information about IB-BERT.
So I'm asking how to train IB-BERT,
If only way to train IB-BERT is fixing BERT, could you explain me how to reduce dimension and add layers."
Notebook Error When Using Color Upsampler,google-research/google-research,2021-11-20 15:21:15,2,,902,1059166921,"Hi,

My input is a single .png format image, whenever I finish running stage1, stage2 (color upsampler) produces this error:

Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/google-research/coltran/custom_colorize.py"", line 244, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/content/google-research/coltran/custom_colorize.py"", line 223, in main
    out = model.sample(bit_cond=prev_gen, gray_cond=gray_64)
  File ""/content/google-research/coltran/models/upsampler.py"", line 115, in sample
    logits = self.upsampler(bit_cond, gray_cond, training=False)
  File ""/content/google-research/coltran/models/upsampler.py"", line 101, in upsampler
    channel = tf.concat((channel, gray_embed), axis=-1)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [5,64,64,512] vs. shape[1] = [1,64,64,512] [Op:ConcatV2] name: concat



What could be causing this?
Thank you

"
[Python 3.8.12 and 3.9.6]: Could not find a version that satisfies the requirement scann (from versions: none),google-research/google-research,2021-11-20 13:56:57,0,,901,1059150561,"Getting this error when trying to install `scann` from `pip`:
```
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```

Tried with both Python 3.9.6 and 3.8.12.

`pip freeze`:
```
absl-py==0.15.0
astunparse==1.6.3
atomicwrites==1.4.0
attrs==21.2.0
autopep8==1.6.0
cachetools==4.2.4
certifi==2021.10.8
charset-normalizer==2.0.7
clang==5.0
cycler==0.11.0
dython==0.6.8
flatbuffers==1.12
gast==0.4.0
gensim==4.1.2
google-auth==2.3.3
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
greenlet==1.1.2
grpcio==1.42.0
h5py==3.1.0
idna==3.3
importlib-metadata==4.8.2
joblib==1.1.0
keras==2.7.0
Keras-Preprocessing==1.1.2
kiwisolver==1.3.2
Markdown==3.3.6
matplotlib==3.4.3
numpy==1.19.5
oauthlib==3.1.1
opt-einsum==3.3.0
pandas==1.3.2
pathlib-mate==1.0.1
Pillow==8.4.0
protobuf==3.19.1
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycodestyle==2.8.0
pyparsing==3.0.6
python-dateutil==2.8.2
pytz==2021.3
requests==2.26.0
requests-oauthlib==1.3.0
rsa==4.7.2
scikit-learn==1.0.1
scikit-plot==0.3.7
scipy==1.7.2
seaborn==0.11.2
six==1.15.0
smart-open==5.2.1
SQLAlchemy==1.4.27
tensorboard==2.7.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.0
tensorflow==2.6.0
tensorflow-estimator==2.7.0
tensorflow-recommenders==0.6.0
termcolor==1.1.0
threadpoolctl==3.0.0
toml==0.10.2
typing-extensions==3.7.4.3
urllib3==1.26.7
uszipcode==0.2.6
Werkzeug==2.0.2
wrapt==1.12.1
zipp==3.6.0
```"
Which tf version should I use in google-research/tft?,google-research/google-research,2021-11-19 04:29:13,1,,900,1058119649,Which exact tensorflow version should I use? Minor errors appear due to the version issue. There's no info in _google-research/tft/requirements.txt_
Loading exported goemotions model,google-research/google-research,2021-11-16 16:31:24,0,,898,1055082983,"Hi Team,

I followed your instructions listed [here](https://github.com/google-research/google-research/tree/master/goemotions) and was able to train a model on an uncased BERT-base. After the training process, I exported the model which resulted in a new folder containing:

```
variables/
saved_model.pb
```

I then tried several loading mechanisms, e.g. with Keras, but in every case I observed errors like:

```
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
```

It seems that there are additional ressources that have to be loaded to actually use the exported model. 

Can you briefly explain how to do so?

Kind regards
Julian"
Coltran: Need help on Flow for training and evaluation on custom dataset,google-research/google-research,2021-11-16 10:14:27,15,,896,1054693140,"- ### How long do I need to train on a custom dataset? 
- ### How do I know if the training is complete?
- ### How to get the result/output for a custom dataset?
- ### How to calculate the FID?

I am using this Notebook, for training on the cityscapes dataset.
[Link to Colab Notebook](https://colab.research.google.com/drive/1hG5t2djdrToWGyQdvGJ3Sf_DkDfWi5jE?usp=sharing)

I trained the model on colab (till GPU resource got exhausted), got around 23 checkpoints.
```
I1115 14:34:04.833563 140478266419072 run.py:286] Saved checkpoint to /content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt/model-20
I1115 14:37:47.372999 140478266419072 run.py:282] Loss: 0.549 bits/dim, Speed: 0.449 steps/second
I1115 14:41:29.732624 140478266419072 run.py:282] Loss: 0.507 bits/dim, Speed: 0.450 steps/second
I1115 14:45:12.382166 140478266419072 run.py:282] Loss: 0.507 bits/dim, Speed: 0.449 steps/second
I1115 14:48:54.532032 140478266419072 run.py:282] Loss: 0.497 bits/dim, Speed: 0.450 steps/second
I1115 14:52:36.791089 140478266419072 run.py:282] Loss: 0.529 bits/dim, Speed: 0.450 steps/second
I1115 14:52:40.168719 140478266419072 run.py:286] Saved checkpoint to /content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt/model-21
I1115 14:56:22.615087 140478266419072 run.py:282] Loss: 0.530 bits/dim, Speed: 0.450 steps/second
I1115 15:00:05.021047 140478266419072 run.py:282] Loss: 0.522 bits/dim, Speed: 0.450 steps/second
I1115 15:03:47.351305 140478266419072 run.py:282] Loss: 0.489 bits/dim, Speed: 0.450 steps/second
I1115 15:07:29.647610 140478266419072 run.py:282] Loss: 0.528 bits/dim, Speed: 0.450 steps/second
I1115 15:11:11.850597 140478266419072 run.py:282] Loss: 0.550 bits/dim, Speed: 0.450 steps/second
I1115 15:11:15.348102 140478266419072 run.py:286] Saved checkpoint to /content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt/model-22
I1115 15:14:58.002671 140478266419072 run.py:282] Loss: 0.542 bits/dim, Speed: 0.449 steps/second
I1115 15:18:40.301140 140478266419072 run.py:282] Loss: 0.501 bits/dim, Speed: 0.450 steps/second
I1115 15:22:22.593688 140478266419072 run.py:282] Loss: 0.528 bits/dim, Speed: 0.450 steps/second
I1115 15:26:05.144472 140478266419072 run.py:282] Loss: 0.499 bits/dim, Speed: 0.449 steps/second
```
- How long / which step / loss value should I continue to train?
- Any parameters which need to be changed? (I did change the batch size as mentioned here #838)

Next, after training on a custom dataset, 
how to evaluate the model or obtain the results of colorized/recolorized images?

I used this cmd, but I guess it works only for imagenet dataset.

```
python -m coltran.run --config=coltran/configs/colorizer.py --mode=eval_valid --logdir=$LOGDIR --dataset=custom --data_dir=$EVAL_DATA_DIR 
```

Now, I am trying to use this (see the notebook for the next 2 steps)

```
python -m coltran.custom_colorize --config=coltran/configs/colorizer.py --logdir=$LOGDIR --img_dir=$IMG_DIR --store_dir=$STORE_DIR --mode=$MODE
```

Can someone please tell me if I am following the correct commands for getting the output?
A step-by-step guide would be appreciated. I am getting confused about which flow to follow.

- Is there any recommended way or package to calculate the FID scores to compare the results with the paper?

Also, the paper mentions 3 different coloured outputs for one input bnw image. 
![image](https://user-images.githubusercontent.com/42700922/141965832-818ac5a5-ee6e-4649-9794-e85d9434a555.png)
How to get such results?


"
Where is the cosine distance implementation?,google-research/google-research,2021-11-15 07:07:49,0,,891,1053288996,"Thanks for the great work, i m wondering where is the cosine distance implementation you mention in appendix c.3? Also, it seem like you use a Taylor approximation way to calculate the dot product, is there a none Taylor approximation way to calculate the dot product? where one has higher acc?"
"typo in README -- ""reter-agreement""",google-research/google-research,2021-11-14 16:41:47,0,,890,1052984130,"surely ""rater-agreement""?"
Publish scann sources to PyPI?,google-research/google-research,2021-11-14 02:57:04,0,,885,1052830416,"Hi all

I'm looking to [package](https://github.com/conda-forge/staged-recipes/pull/16944) scann for conda-forge, which would make installation a lot easier (ex. #779 #782). However, there are no sources available on PyPI, which makes this process much harder.

In fact, the best (from the POV of a packager) would be to split off scann into its own repository, but I'd be happy with the sources already. :)"
Hi ,google-research/google-research,2021-11-13 01:03:44,0,,884,1052504725,
[kws_streaming] Seperate folder to test the model,google-research/google-research,2021-11-11 05:57:11,1,,883,1050602687,"@rybakov 
I have a dataset _folder1_ which contains `_background_noise_ ` folder and several other folders corresponding to `wanted_words` and some folders which doesn't belong to `wanted_words`
I have a CRNN model which was trained on the dataset _folder1_ with split_data = 1 (so code automatically splits data in test ,valid, train sets and creates two extra labels `_unknown_` and `_silence_`  + `wanted_words`).

Now, I have dataset _folder2_  with similar structure as _folder1_  which contains other wave files which I intent to test the model. 
I want to run some tests from [test.py](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py) to check the accuracy of dataset _folder2_ 
How should I use the functions 
```
test.tf_non_stream_model_accuracy(flags, ...)                # (the flags was used from the previously trained model)
test.tf_stream_state_internal_model_accuracy(flags, ..)
```

to check the accuracy on the dataset _folder2_. What changes in flags do I need to make?
"
module 'tensorflow' has no attribute 'variable_scope',google-research/google-research,2021-11-11 00:31:21,0,,882,1050452835,"I tried to run the TFT model using the electricity dataset, but I got this error
module 'tensorFlow has no attribute 'variable_scope.'
I updated tf.variable_scope   ----> tf.compat.v1.variable_scope
 Also, I used the automatic update mechanism, but without luck.
Any help."
"Hi, How can I get more information about google pathway？",google-research/google-research,2021-11-10 22:30:56,0,,881,1050372803,"I am so interested in google pathways and already have some work on a similar net.
I can't get the registered SMS for Twitter, looking forward to your reply.

https://github.com/FFiot/HWlayer_V2
https://github.com/FFiot/HWnet"
[fisher_brc] Question about running behavior cloning,google-research/google-research,2021-11-09 23:35:48,0,,879,1049237346,"Hi, 
    Thanks for sharing the repo. as mentioned in the paper that FisherBRC is the same as BRAC. Both need behavior cloning to be run first. However, in the README documentation, it only mentioned running `train_offline.py`.  And it is different from BRAC, in BRAC, if BC is not run first BRAC cannot be run. Nevertheless, I could directly run `fbrc` without bc run before. 
    Can you clarify the usage?"
No negative Data Values estimated by DVRL,google-research/google-research,2021-11-09 16:26:04,1,,878,1048811888,"![image](https://user-images.githubusercontent.com/47161914/140963451-4fbd894a-08f8-4d49-8718-c65894cf47e2.png)

I ran DVRL with fashion mnist, with 10000 flipped labels I introduced. I would expect some fraction of these to crop up as negative data values (harm the predicting model) but all are positive. Can negative values be output by this set-up?

![image](https://user-images.githubusercontent.com/47161914/140963874-2bc74902-2edc-4a56-95f9-2b5fc75818bf.png)

"
When comparing the representations of different architectures with CKA which axis corresponds to which model?,google-research/google-research,2021-11-09 00:13:39,0,,876,1048049162,"Hi @simonster , when using CKA across models, which axis corresponds to which model on the generated heatmap, based on the function signature `update_state_across_models(self, activations1, activations2)`?

In case its relevant, i'm plotting with
```
plt.imshow(heatmap)
plt.gca().invert_yaxis()
```

Thanks!"
Prediction with the TFT,google-research/google-research,2021-11-06 20:19:35,0,,875,1046589189,"Hey, 
I am trying to use the temporal fusion transformer for time series prediction. 
I would like to predict exact futurevalues, not only the P10, P50 and P90-values. Till now, i haven't found a possibility to do so. 
Is there any opportunity to predict the series? 

My idea so far was, to switch the loss to ""mse"" and change the data preparation. But I could not find a answer to the following question: Where and why are the target values processed to the shape ""3"" in z-direction? 

Thanks for your help!
Hinnerk8"
Temporal Fusion Transformers for Multi-horizon Time Series Forecasting (tft),google-research/google-research,2021-11-05 20:59:08,5,,874,1046240083,"Hi, 
I have a problem with training the model using GPU, I got this error (module 'tensorflow' has no attribute 'ConfigProto') even If I select the (tf_device=""CPU"") it gives me the same error.
I replaced tf.ConfigProto by tf.compat.v1.ConfigProto
Also, I tried--->  import tf.compat.v1 as tf instead of ---> import tensorflow as tf

Any suggestion?"
[Fisher BRC] question about experiment results.,google-research/google-research,2021-11-05 02:20:20,0,,873,1045392053,"In Table 1 of your paper, how can you directly take results from other papers? How do you know the set of random seeds is the same as theirs? DRL is notorious for its non-reproducibility, slight change on the set of random seeds could lead to a great difference in performance. This problem is addressed in several papers, could you justify this?"
MuZero actor freezes,google-research/google-research,2021-11-03 15:24:50,0,,872,1043740843,"Hi!

thanks for releasing your implementation of MuZero. I'm trying to run MuZero on my laptop. According to your instructions, it should work with SEED RL however the instructions are not clear enough. It's not obvious where to put each file and which scripts to run.

I managed to run MuZero with SEED RL in some way, but it's not working. All libraries load, variables are initialized, the training loop starts, but the actor freezes on GRPC call of initial_inference function. Here are logs from [learner](https://pastebin.com/aGQNNSdi) and [actor](https://pastebin.com/vgpm2KhQ)

Could you provide more detailed instructions on how to run MuZero with SEED RL?

I found out that requirements.txt of MuZero is missing three libraries (inflection, tf-agents, ale-py). This occurs when I try to run MuZero on Atari. I ran `pip install -r muzero/requirements.txt` inside [this](https://github.com/google-research/seed_rl/blob/master/docker/Dockerfile.atari) docker image and the code for MuZero did not load correctly unless I installed those 3 libraries.

Could you provide an output of `pip freeze` of a python env in which MuZero runs correctly? I'm providing a `pip freeze` of my env [here](https://pastebin.com/2hXitfAZ)."
collect2: error: ld returned 1 exit status for fully_dynamic_submodular_maximization,google-research/google-research,2021-11-01 09:59:01,0,,870,1040988149,"I cloned the entire repository (both on my system and in Gitpod) and executed:  
`gcc -std=c++14 dynamic_submodular_maximization_main.cc`  
from its directory.  

gcc version:
```
gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
```
I am always getting the following error. Please help.
```
/usr/bin/ld: /tmp/ccAA286T.o: in function `insertInOrderThenDeleteLargeToSmall(SubmodularFunction&, Algorithm&)':
dynamic_submodular_main.cc:(.text+0x316): undefined reference to `SubmodularFunction::DeltaAndIncreaseOracleCall(int) const'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x340): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x34b): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: /tmp/ccAA286T.o: in function `main':
dynamic_submodular_main.cc:(.text+0x6d6): undefined reference to `OurSimpleAlgorithm::OurSimpleAlgorithm(double)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x6f1): undefined reference to `OurSimpleAlgorithm::OurSimpleAlgorithm(double)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x700): undefined reference to `std::allocator<char>::allocator()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x71d): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x736): undefined reference to `GraphUtility::GraphUtility(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x745): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x754): undefined reference to `std::allocator<char>::~allocator()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x90f): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x914): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9ae): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9b3): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9c6): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9d0): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9db): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0xb7b): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0xb93): undefined reference to `std::allocator<char>::~allocator()'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__static_initialization_and_destruction_0(int, int)':
dynamic_submodular_main.cc:(.text+0xcd8): undefined reference to `std::ios_base::Init::Init()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0xced): undefined reference to `std::ios_base::Init::~Init()'
/usr/bin/ld: /tmp/ccAA286T.o: in function `RandomHandler::CheckRandomNumberGenerator()':
dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x1f): undefined reference to `RandomHandler::generator_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x31): undefined reference to `RandomHandler::generator_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x53): undefined reference to `std::cerr'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x58): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x67): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SubmodularFunction::~SubmodularFunction()':
dynamic_submodular_main.cc:(.text._ZN18SubmodularFunctionD0Ev[_ZN18SubmodularFunctionD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Algorithm::~Algorithm()':
dynamic_submodular_main.cc:(.text._ZN9AlgorithmD0Ev[_ZN9AlgorithmD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SieveStreaming::SieveStreaming()':
dynamic_submodular_main.cc:(.text._ZN14SieveStreamingC2Ev[_ZN14SieveStreamingC5Ev]+0x1f): undefined reference to `vtable for SieveStreaming'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SimpleGreedy::SimpleGreedy()':
dynamic_submodular_main.cc:(.text._ZN12SimpleGreedyC2Ev[_ZN12SimpleGreedyC5Ev]+0x1f): undefined reference to `vtable for SimpleGreedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Greedy::Greedy()':
dynamic_submodular_main.cc:(.text._ZN6GreedyC2Ev[_ZN6GreedyC5Ev]+0x1f): undefined reference to `vtable for Greedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void runExperimentForAlgorithms<double (SubmodularFunction&, Algorithm&, int), int>(double ( const&)(SubmodularFunction&, Algorithm&, int), SubmodularFunction&, std::vector<std::reference_wrapper<Algorithm>, std::allocator<std::reference_wrapper<Algorithm> > > const&, std::vector<int, std::allocator<int> > const&, int)':
dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4c): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x51): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x87): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <char, std::char_traits<char>, std::allocator<char> >(std::basic_ostream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x96): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0xa2): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x129): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x12e): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x164): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <char, std::char_traits<char>, std::allocator<char> >(std::basic_ostream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x173): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x17f): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x217): undefined reference to `std::cerr'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x21c): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x22f): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x239): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x244): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x250): undefined reference to `RandomHandler::generator_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x286): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x2da): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x34a): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x34f): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x39f): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3a4): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3b3): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3de): undefined reference to `std::ostream::operator<<(double)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3ed): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x400): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x40a): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x40f): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x41d): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x422): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x46e): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x473): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x482): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4ab): undefined reference to `std::ostream::operator<<(long)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4ba): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4cd): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4d7): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4dc): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x51b): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x539): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<std::pair<double, int>, std::allocator<std::pair<double, int> > >::_M_realloc_insert<double, int&>(__gnu_cxx::__normal_iterator<std::pair<double, int>*, std::vector<std::pair<double, int>, std::allocator<std::pair<double, int> > > >, double&&, int&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x279): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x2e1): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x2ed): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<double, std::allocator<double> >::_M_realloc_insert<double const&>(__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, double const&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<double, std::allocator<double> >::_M_realloc_insert<double>(__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, double&&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<std::pair<double, int>, std::allocator<std::pair<double, int> > >::_M_check_len(unsigned long, char const*) const':
dynamic_submodular_main.cc:(.text._ZNKSt6vectorISt4pairIdiESaIS1_EE12_M_check_lenEmPKc[_ZNKSt6vectorISt4pairIdiESaIS1_EE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<double, std::allocator<double> >::_M_check_len(unsigned long, char const*) const':
dynamic_submodular_main.cc:(.text._ZNKSt6vectorIdSaIdEE12_M_check_lenEmPKc[_ZNKSt6vectorIdSaIdEE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<int, std::allocator<int> >::_S_check_init_len(unsigned long, std::allocator<int> const&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIiSaIiEE17_S_check_init_lenEmRKS0_[_ZNSt6vectorIiSaIiEE17_S_check_init_lenEmRKS0_]+0x62): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<std::reference_wrapper<Algorithm>, std::allocator<std::reference_wrapper<Algorithm> > >::_S_check_init_len(unsigned long, std::allocator<std::reference_wrapper<Algorithm> > const&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorISt17reference_wrapperI9AlgorithmESaIS2_EE17_S_check_init_lenEmRKS3_[_ZNSt6vectorISt17reference_wrapperI9AlgorithmESaIS2_EE17_S_check_init_lenEmRKS3_]+0x62): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<double>::deallocate(double*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIdE10deallocateEPdm[_ZN9__gnu_cxx13new_allocatorIdE10deallocateEPdm]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::pair<double, int> >::deallocate(std::pair<double, int>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt4pairIdiEE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorISt4pairIdiEE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<SieveStreaming::SingleThresholdSieve>::deallocate(SieveStreaming::SingleThresholdSieve*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIN14SieveStreaming20SingleThresholdSieveEE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorIN14SieveStreaming20SingleThresholdSieveEE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<int>::deallocate(int*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim[_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::unique_ptr<SubmodularFunction, std::default_delete<SubmodularFunction> > >::deallocate(std::unique_ptr<SubmodularFunction, std::default_delete<SubmodularFunction> >*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt10unique_ptrI18SubmodularFunctionSt14default_deleteIS2_EEE10deallocateEPS5_m[_ZN9__gnu_cxx13new_allocatorISt10unique_ptrI18SubmodularFunctionSt14default_deleteIS2_EEE10deallocateEPS5_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<long, std::allocator<long> >::_M_realloc_insert<long>(__gnu_cxx::__normal_iterator<long*, std::vector<long, std::allocator<long> > >, long&&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_[_ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_[_ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_[_ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::pair<double, int> >::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<double>::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<int>::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::reference_wrapper<Algorithm> >::deallocate(std::reference_wrapper<Algorithm>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::reference_wrapper<Algorithm> >::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<long>::deallocate(long*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIlE10deallocateEPlm[_ZN9__gnu_cxx13new_allocatorIlE10deallocateEPlm]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<long, std::allocator<long> >::_M_check_len(unsigned long, char const*) const':
dynamic_submodular_main.cc:(.text._ZNKSt6vectorIlSaIlEE12_M_check_lenEmPKc[_ZNKSt6vectorIlSaIlEE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node_base*>::deallocate(std::__detail::_Hash_node_base**, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::pair<double, int>* std::__uninitialized_copy<false>::__uninit_copy<std::move_iterator<std::pair<double, int>*>, std::pair<double, int>*>(std::move_iterator<std::pair<double, int>*>, std::move_iterator<std::pair<double, int>*>, std::pair<double, int>*)':
dynamic_submodular_main.cc:(.text._ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_[_ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_]+0x7f): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_[_ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_]+0x97): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_[_ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_]+0xa3): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node<std::pair<int const, int>, false> >::deallocate(std::__detail::_Hash_node<std::pair<int const, int>, false>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKiiELb0EEEE10deallocateEPS6_m[_ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKiiELb0EEEE10deallocateEPS6_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node<int, false> >::deallocate(std::__detail::_Hash_node<int, false>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeIiLb0EEEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeIiLb0EEEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<long>::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x10): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x18): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x20): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x28): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x30): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x38): more undefined references to `__cxa_pure_virtual' follow
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTI9Algorithm[_ZTI9Algorithm]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTI18SubmodularFunction[_ZTI18SubmodularFunction]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SieveStreaming::~SieveStreaming()':
dynamic_submodular_main.cc:(.text._ZN14SieveStreamingD2Ev[_ZN14SieveStreamingD5Ev]+0x13): undefined reference to `vtable for SieveStreaming'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SieveStreaming::~SieveStreaming()':
dynamic_submodular_main.cc:(.text._ZN14SieveStreamingD0Ev[_ZN14SieveStreamingD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SimpleGreedy::~SimpleGreedy()':
dynamic_submodular_main.cc:(.text._ZN12SimpleGreedyD2Ev[_ZN12SimpleGreedyD5Ev]+0x13): undefined reference to `vtable for SimpleGreedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SimpleGreedy::~SimpleGreedy()':
dynamic_submodular_main.cc:(.text._ZN12SimpleGreedyD0Ev[_ZN12SimpleGreedyD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Greedy::~Greedy()':
dynamic_submodular_main.cc:(.text._ZN6GreedyD2Ev[_ZN6GreedyD5Ev]+0x13): undefined reference to `vtable for Greedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Greedy::~Greedy()':
dynamic_submodular_main.cc:(.text._ZN6GreedyD0Ev[_ZN6GreedyD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `OurSimpleAlgorithm::~OurSimpleAlgorithm()':
dynamic_submodular_main.cc:(.text._ZN18OurSimpleAlgorithmD2Ev[_ZN18OurSimpleAlgorithmD5Ev]+0x13): undefined reference to `vtable for OurSimpleAlgorithm'
/usr/bin/ld: /tmp/ccAA286T.o: in function `OurSimpleAlgorithm::~OurSimpleAlgorithm()':
dynamic_submodular_main.cc:(.text._ZN18OurSimpleAlgorithmD0Ev[_ZN18OurSimpleAlgorithmD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `GraphUtility::~GraphUtility()':
dynamic_submodular_main.cc:(.text._ZN12GraphUtilityD2Ev[_ZN12GraphUtilityD5Ev]+0x13): undefined reference to `vtable for GraphUtility'
/usr/bin/ld: /tmp/ccAA286T.o: in function `GraphUtility::~GraphUtility()':
dynamic_submodular_main.cc:(.text._ZN12GraphUtilityD0Ev[_ZN12GraphUtilityD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<OurSimpleAlgorithm::OurSimpleAlgorithmSingleThreshold>::deallocate(OurSimpleAlgorithm::OurSimpleAlgorithmSingleThreshold*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIN18OurSimpleAlgorithm33OurSimpleAlgorithmSingleThresholdEE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorIN18OurSimpleAlgorithm33OurSimpleAlgorithmSingleThresholdEE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<unsigned long>::deallocate(unsigned long*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorImE10deallocateEPmm[_ZN9__gnu_cxx13new_allocatorImE10deallocateEPmm]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > >::deallocate(std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> >*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt13unordered_setIiSt4hashIiESt8equal_toIiESaIiEEE10deallocateEPS7_m[_ZN9__gnu_cxx13new_allocatorISt13unordered_setIiSt4hashIiESt8equal_toIiESaIiEEE10deallocateEPS7_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.local.DW.ref.__gxx_personality_v0[DW.ref.__gxx_personality_v0]+0x0): undefined reference to `__gxx_personality_v0'
collect2: error: ld returned 1 exit status
```"
Question about ieg,google-research/google-research,2021-11-01 08:29:54,7,,869,1040911003,"Hi,
Thank you very much about all the sharings! I have some problems with launching ieg. I have downloaded the code and run as required.

And I got the error:`AttributeError: 'NoneType' object has no attribute 'shape'`

when trying to execute the line in datasets.py:
`self.probe_size = x_probe.shape[0]`

from this:
```
出

    if not self.split_probe and x_probe is not None:
      # Usually used for supervised comparison.
      tf.logging.info('Merge train and probe')
      x_train = np.concatenate([x_train, x_probe], axis=0)
      y_train = np.concatenate([y_train, y_probe], axis=0)
      y_gold = np.concatenate([y_gold, y_probe], axis=0)

    conf_mat = sklearn_metrics.confusion_matrix(y_gold, y_train)
    conf_mat = conf_mat / np.sum(conf_mat, axis=1, keepdims=True)
    print('Corrupted confusion matirx\n {}'.format(conf_mat))
    x_test, y_test = shuffle_dataset(x_test, y_test)
    self.train_dataset_size = x_train.shape[0]
    self.val_dataset_size = x_test.shape[0]
    if self.split_probe:
      self.probe_size = x_probe.shape[0]
```
The reason for the error is that `x_probe = None`, then `self.probe_size = x_probe.shape[0]`, where the value of `self.split_probe` is True.

So I tried to assign the value of `self.split_probe` to False, but another error occurred：
`AttributeError: 'CIFAR' object has no attribute 'probe_dataflow'`

when trying to execute the line in model.py:
```
probe_ds = self.dataset.probe_dataflow.repeat().batch(
        self.batch_size,
        drop_remainder=True).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
```

I kindly ask for your further help in solving the problem.
Best Regards."
MobileBERT with SQUAD 1.1 Quantized INT8 Error,google-research/google-research,2021-10-30 08:25:31,0,,868,1040087059,"I download the quantize-aware training int8 model(saved_model.pb) from repo `google-research/google-research`.And I use run_squad.py which offered in the repo with tensorflow 1.15. `https://github.com/google-research/google-research/tree/master/mobilebert/run_squad.py`.I also try to convert this model with tensorflow 2.2/2.3/2.5/2.6/2.8-nightly.But it didn't work anymore.
Could you please provide some information to help convert the model?"
[FELIX] Recent commits result in breaking change,google-research/google-research,2021-10-28 12:23:18,2,,867,1038452962,"Hi,

I would like to report that, the recent commits to FELIX (https://github.com/google-research/google-research/commit/22119bcefacaf7ae696d3c9cb57f9760e135a622), and to `tensorflow/models` (https://github.com/tensorflow/models/commit/9b23daf9daaa1b44aee32e9f2e3b7ee5c9462b11#diff-163581806ba7ae5f9a9dc1f68f99befae3e361d4e702f5a31d0fceb4ae9a45bf) result in a breaking change in the import of `layers` modules.

FELIX asks for `tf-models-official==2.4.0` and imports `official.nlp.modeling.layers.position_embedding import PositionEmbedding`, but such class is not present in versions `2.4.0`, nor `2.5.0` and `2.6.0` (latest release in Aug.).
It is only present in the current master version of the repository (https://github.com/tensorflow/models/blob/ac7f9e7f2d0508913947242bad3e23ef7cae5a43/official/nlp/modeling/layers/__init__.py#L37) which is not released yet.

So for the FELIX codes to work, one should either use the old version of the codes or manually clone the master version.

"
"mentormix "" Controlled Noisy Web Labels""  ID image wrong",google-research/google-research,2021-10-27 14:07:50,0,,866,1037470231,"
The image ID of split part dont match id of mini-imagenet-annotations

So is not possible use this dataset. 

Someone can help me?

Thx"
Do input matrices for CKA need to be normalized by the forbenius norm?,google-research/google-research,2021-10-26 16:56:10,0,,865,1036535939,"Dividing by the forbenius norm has broken [basic sanity checks](https://github.com/brando90/ultimate-anatome/issues/3) I expected for CCA. 

Does it also break things for CKA? What is the right normalization for CKA? Is dividing by the standard devation better?"
realformer tf checkpoint convert to torch model,google-research/google-research,2021-10-26 15:21:45,15,,864,1036441019,"hi, i want  convert realformer tf checkpoint to torch model, but get a error 
```
torch.nn.modules.module.ModuleAttributeError: 'LayerNorm' object has no attribute 'beta'
```
I think this is caused by changing the bert structure. 
i change convert code，but got a another error
```
AssertionError:
(torch.Size([21128, 768]), (30522, 768))
```

convert code:
```
for name, array in zip(names, arrays):
        name = name.split('/')
        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
        # which are not required for using pretrained model
        if name[-1] in [""adam_v"", ""adam_m""]:
            print(""Skipping {}"".format(""/"".join(name)))
            continue
        pointer = model
        for m_name in name:
            if re.fullmatch(r'[A-Za-z]+_\d+', m_name):
                l = re.split(r'_(\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'kernel':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'output_bias' or l[0] == 'beta':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'output_weights' or l[0] == 'gamma':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        if m_name[-11:] == '_embeddings':
            pointer = getattr(pointer, 'weight')
        elif m_name == 'kernel':
            array = np.transpose(array)
        try:
            assert pointer.shape == array.shape # error
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        print(""Initialize PyTorch weight {}"".format(name))
        pointer.data = torch.from_numpy(array)
```
Whether the vocabulary size is inconsistent？
"
Data for smith,google-research/google-research,2021-10-24 14:35:45,4,,862,1034432203,I want to know how to transfer Gwikimatch which only include urls into tfrecord ?
"Are you retrained other models, e.g., EDVR, RSDN, with compressed videos in paper COMISR?   ",google-research/google-research,2021-10-24 03:03:13,0,,861,1034311099,"Some questions in the paper COMISR: Compression-Informed Video Super-Resolution.
The results are incredible. But I don't think the results of other models are containing so many artifacts like your exhibition in your paper if you retrain these models with compressed videos. I used to retrain EDVR and RSDN with compressed videos on CRF37, and the results are just over-smoothed instead of full of artifacts. Is it because something is wrong with my knowledge about that or you just use the trained model provided by their authors?"
failed: sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures,google-research/google-research,2021-10-19 17:53:17,0,,851,1030588580,"i'm stuck at step 6 sparse_convo_py_test in ops compilation folders 
here is the log please tell me what to do
2021-10-19 17:43:59.277603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-19 17:43:59.277846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-10-19 17:43:59.278272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
INFO:tensorflow:time(__main__.SparseConvOpTest.test_spar_conv_op): 0.48s
I1019 17:43:59.404408 140462161450752 test_util.py:1973] time(__main__.SparseConvOpTest.test_spar_conv_op): 0.48s
[  FAILED  ] SparseConvOpTest.test_spar_conv_op
======================================================================
ERROR: test_spar_conv_op (__main__.SparseConvOpTest)
SparseConvOpTest.test_spar_conv_op
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/39e26583fdab04cdafc9b2ab1c2b6d71/execroot/__main__/bazel-out/k8-opt/bin/sparse_conv_ops_py_test.runfiles/__main__/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py"", line 32, in test_spar_conv_op
    dtype=tf.float32)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

----------------------------------------------------------------------
Ran 2 tests in 0.487s

FAILED (errors=1, skipped=1)
"
Order of Training,google-research/google-research,2021-10-19 14:41:58,0,,850,1030397842,"Thanks for your awesome work! 

I am confused about the order of training, which is described in the first paragraph in Part 4 of your paper as this ""In order to approximate regret, we use the difference between the payoffs of two agents acting under the same environment conditions. Assume we are given a fixed environment with parameters fixed policy for the protagonist agent, and we then train a second antagonist agent, πA, to optimality in this environment. Then, the difference between the reward obtained by the antagonist, and the protagonist.""

So, the env_adversary will first generate an env based on the fixed policy of the Protagonist, then the Antagonist will be trained to optimality on this env. After training the Antagonist, we will compute the reward obtained by the Antagonist (already been trained) and Protagonist (not yet been trained) on this env. However, in your implementation, I could not find the order of training follows this way. What I find is that in the run() function you run the three agents in order, and the Antagonist is not first being trained (https://github.com/google-research/google-research/blob/5a4d95be1ca8ca4335d42f5a0326d79262c6992e/social_rl/adversarial_env/adversarial_driver.py#L118). 

Could you please explain this a bit to me? Really appreciate it.


"
Questions about the MuZero implementation,google-research/google-research,2021-10-15 18:29:35,0,,847,1027697939,"Hi, 

Thanks for open-sourcing your MuZero implementation. I was wondering if you've benchmarked the implementation, and whether it reproduces some of the original Atari results. Also, are there any plans to implement ReAnalyse? cc @galmacky @mochi1219 @sgirgin "
The model-definition/training/prediction imprementations of `dual_pixels`.,google-research/google-research,2021-10-15 07:23:56,0,,846,1027156774,"Hi, I have a question about the `dual_pixels` dir.
It is the official imprementations of the paper [""Learning Single Camera Depth Estimation using Dual-Pixels""](https://arxiv.org/abs/1904.05822).

Maybe, I could see only the following functions.

- Evaluation for the results predicted previously.
- Android app to capture dual-pixel photos using some Google smartphone.

I could NOT see the following functions.

- Model/DNN-design definition.
- Prediction for other dual-pixel photos.
- Training/finetuning using other datasets.

Why they have not been published?
or
If they have been published at another repository, would you teach me it?

Thanks for your great works!"
tokenizers missing in requirements.txt,google-research/google-research,2021-10-12 13:52:56,0,,844,1023852135,"I'd like to know which version of tokenizers you use, couldn't find it in requirements.txt

The following import fails:

`from tokenizers import BertWordPieceTokenizer`

ModuleNotFoundError: No module named 'tokenizers'"
kws_streaming 依赖问题,google-research/google-research,2021-10-11 06:08:46,8,,843,1022299528,您好，我在跑kws_streaming这个项目时，　依赖包tf_nightly==2.3.0.dev20200515无法安装，请问怎么解决呢
Tf3d instance segmentation,google-research/google-research,2021-09-29 23:01:52,0,,839,1011519683,"[https://github.com/google-research/google-research/blob/25b69734731eee63b1da062562ddfb6a77171a26/tf3d/instance_segmentation/scripts/scannet_scene/run_train_locally.sh#L40](https://github.com/google-research/google-research/blob/25b69734731eee63b1da062562ddfb6a77171a26/tf3d/instance_segmentation/scripts/scannet_scene/run_train_locally.sh#L40)

Hello,

Any chance that you know what happened here. Under the task of instance segmentation, it's using the config/model of semantic segmentation. 

I am trying to understand the code here. And this line is very confusing to me. Further clarification would be appreciated!

All the best,
Weiwei.
 "
[BLUR] Missing attribute 'GRADIENT_GENOME' in BLUR code,google-research/google-research,2021-09-28 17:30:26,0,,837,1010024775,"Good morning, I'm trying to run the example code in BLUR project called Learning_Boolean_Functions_Using_BLUR.ipynb, but when I try to run Gradient descent on genome section, I get this error: AttributeError: module 'blur.genome_util' has no attribute 'GRADIENT_GENOME'. I read the genome_util.py file and there's NO a Gradient_genome method or process or whatever there, and I can't continue with the execution. ¿What can I do? ¿Where did that attribute go? I'll really appreciate help because is for my homework. Thanks."
[POEM] Implementation of Video Alignment and Action Recognition?,google-research/google-research,2021-09-28 06:51:18,1,,835,1009355381,"Hi, ive gone through the repo and it seems that only the pose invariant embedding algorithm is implemented. Is there any implementation of video alignment and action recognition algo as well? Thanks..."
"Can't build TF3D with Sparse Ops, numpy version mismatch",google-research/google-research,2021-09-27 18:10:57,0,,834,1008446612,"This references https://github.com/google-research/google-research/commit/e4154f628b12c819c8b1383d0535774f69383e12

I followed the instructions on how to set up TF3D and TF3D ops as described in https://github.com/google-research/google-research/blob/master/tf3d/doc/setup.md and  https://github.com/google-research/google-research/tree/master/tf3d/ops/setup.md and got as far as step 6 in setup.md and try to run ""configure.sh"" when it gives this error message.  

![image](https://user-images.githubusercontent.com/4442406/134961287-989d3e9b-b011-49cd-91cd-1aee3e3c0996.png)

Please note that the existing version of numpy provided in the tensorflow/tensorflow:2.3.0-custom-op-gpu-ubuntu16 docker image is  numpy-1.18.5, and tensorflow 2.3.0 requires numpy version <1.19.0.  If I try to force a higher version of numpy, it still fails.  "
representation similarity (CKA) on neural networks,google-research/google-research,2021-09-27 08:28:27,0,,833,1007854752,"Hi, 
The CKA described [here](https://github.com/google-research/google-research/blob/master/representation_similarity/Demo.ipynb) works on n x f matrices. I am not sure how to calculate it for neural network layers. For example, if we take all elements of activation tensors as features, then the dimension would be N x (C*H*W) which is huge for working matrices to fit into memory. Other ways can be to take spatial mean(N x C) or channel mean(N x H*W). Does someone know what preprocessing is done to calculate CKA on neural network like resnets"
[DVRL] seems not data-agnostic,google-research/google-research,2021-09-25 12:00:55,1,,831,1007067901," Hello, I am trying to use DVRL and apply it to the data evaluation for text classification.

Besides the blog and adult dataset, I add another two public datasets: sst-2, AG News for experiments.
For simplicity and consistency, I use pre-trained sentence encoder to project each sample of text into embedding, which is of 1024 dimension.(I have validate the effectiveness of the projector and if I use a simple one-layer network on top of it, the accuracy is ok, about 0.8+).

Other settings follow the script of https://github.com/google-research/google-research/blob/master/dvrl/main_data_valuation.py

I use AUC to measure the performance of DVRL, because clean samples are expected to be ranked higher than corrupted ones, in terms of the scores from DVE network. AUC measures the ranking ability. (the marks of clean samples are 1 while 0 for corrupted samples, whose labels are distorted intentionally)

This is my implementation: https://github.com/yananchen1989/topic_classification_augmentation/blob/main/dvrl_repo/main_data_valuation.py  

I run each dataset multiple times, with the exactly same settings.

```
for i in {1..12}
do
	#seed=$RANDOM

    python -u main_data_valuation.py --inner_iterations 100 --batch_size 256 \
        --batch_size_predictor 256 --iterations ${1} \
        --learning_rate 0.01  --perf_metric accuracy \
       --train_no 512 --valid_no 64 --dsn ${2} > dvrl.${2}.${1}.${i}.log 
done
```

However, I find that the auc is not stable each time.
For the AG and SST-2 datasets, here is the metrics of all trials.

![image](https://user-images.githubusercontent.com/26405281/134842551-5dc96346-b0e1-494a-a19c-645401df9c5e.png)

The blog and adult datasets bear the same outcomes.

My question is how to avoid this unstability and what is the cause for that ?

Thanks. 
"
[Meta Pseudo Labels] Question on dot_product scalar value for Teacher network,google-research/google-research,2021-09-24 11:56:20,0,,829,1006386797,"Hi @hyhieu, thanks for sharing this neat idea with academia.

Currently, I am applying the idea of MPL for my own project, and having trouble with computing the gradients for the 
teacher network.

In the original paper, the gradient for updating the teacher network is defined as below. 
![image](https://user-images.githubusercontent.com/20310517/134666637-93dd867f-a101-4873-9024-755cea27dc9d.png)

From the fruitful [discussion from other researchers,](https://github.com/google-research/google-research/issues?q=mpl), I found that `dot_product` term ('h' in the paper) is related to the Taylor series approximation. 

However, unlike the paper [implementation](https://github.com/google-research/google-research/blob/7c88de21b15ceb0161ac3b3a604a437c873aefce/meta_pseudo_labels/training_utils.py#L424) , the term is 
subtracted in the Taylor is computed via 'labeled' samples and labels, while the original paper claims that term should be computed on the unlabeled samples with pseudo labels.

I assume the code implementation is correct, but the paper still makes me suspicious about the term.
I wonder if the paper has typos in explaining the term 'h'.

Thanks in advance."
Cannot find language/table_to_text module?,google-research/google-research,2021-09-24 04:35:11,0,,827,1006070778,How do I find this module? Is there an implementation code for PARENT？
File,google-research/google-research,2021-09-20 21:31:32,0,,825,1001443794,
Question about TCC,google-research/google-research,2021-09-20 12:48:52,1,,821,1000930481,"Hi, 
Thank you very much about all the sharings! I am currently trying to work on TCC - Temporal Cycle-Consistency Learning - but there is a technical question I can't figure out.
What exactly does the network sees? At first I thought it was pairs of videos of the same class, but the loss takes in account 1 batch of samples.

It seems that the loss is computed over all pairs of samples in the batch, but what is the condition on the samples of the batch? In particular, in the paper it is said that ""The core contribution of this work is a self-supervised
approach to learn an embedding space where two similar
video sequences can be aligned temporally."". So I would find it weird to align 2 videos that aren't similar (like pouring/tennis serve).

Do you know where in the code can I find information about such stuff? I am not really familiar with tensorflow :("
Problem with compiling tf3d 3D sparse ops,google-research/google-research,2021-09-14 14:25:56,0,,817,996085737,"I have been trying to implement the sparse ops for a 3D object detection but get stuck in step 5, I kindly ask for your further help in solving the problem.
Best Regards. 
![20210912_045039](https://user-images.githubusercontent.com/72381301/133275944-61b74305-905d-48d7-983c-9c3dfbc5c86d.jpg)
![Uploading 20210911_033838.jpg…]()
"
[ptopk_patch_selection] flax.nn is deprecated need upgrade to flax.linen,google-research/google-research,2021-09-14 06:55:04,0,,816,995658726,"flax.nn is deprecated but ptopk_patch_selection still use it, so it is better upgrade the code to falx.linen"
[KWS_Streaming] Trained Model CRNN,google-research/google-research,2021-09-11 10:18:59,1,,814,993806250,@rybakov Can you please provide me the trained model files of CRNN model of KWS_Streaming. I couldn't train it due to colab and PC limitations. I need it urgently please.
[Felix] Why each model's train loss will rise  in the last epoch?,google-research/google-research,2021-09-08 02:52:58,3,,811,990628746,"When I use felix model training Discofuse dataset, the model's train loss will rise in the last epoch no matter which size of epoch I chosen.  My training parameters : num_training_examples = 450,000 ,num_eval_examples = 20,000, batch_size = 32, max_seq_length=128, lr = 0.00003"
error training svdf_resnet on custom data,google-research/google-research,2021-09-03 16:13:09,31,,809,987923534,"Hi @rybakov , 

I am trying to follow the steps for svdf_resnet [here for quantization](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_30k_12_labels.md) with the below command on custom data. 

Below is the command used:
```
$CMD_TRAIN \
--wanted_words 'srewai' \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/svdf_resnet/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 80 \
--dct_num_features 40 \
--resample 0.15 \
--time_shift_ms 100 \
--feature_type 'mfcc_op' \
--fft_magnitude_squared 1 \
--preprocess 'raw' \
--train 1 \
--lr_schedule 'exp' \
svdf_resnet \
--block1_memory_size '7' \
--block2_memory_size '7' \
--block3_memory_size '11,11' \
--block1_units1 '32' \
--block2_units1 '50' \
--block3_units1 '50,128' \
--blocks_pool '2,2,1' \
--use_batch_norm 1 \
--bn_scale 1 \
--activation 'relu' \
--svdf_dropout 0.0 \
--svdf_pad 1 \
--svdf_use_bias 0 \
--dropout1 0.0 \
--units2 '64' \
--flatten 0
```
The model summary looks like this.
```
Instructions for updating:
Colocations handled automatically by placer.
W0903 18:00:45.072718 140373190989632 deprecation.py:347] From /srewai-venv/lib/python3.6/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(100, 16000)]       0           []                               
__________________________________________________________________________________________________
speech_features (SpeechFeature  (100, 49, 40)        0           ['input_1[0][0]']                
s)                                                                                                
__________________________________________________________________________________________________
dense_1 (Dense)                 (100, 49, 32)        1280        ['speech_features[0][0]']        
__________________________________________________________________________________________________
svdf_1_0 (Svdf)                 (100, 49, 32)        1632        ['speech_features[0][0]']        
__________________________________________________________________________________________________
batch_normalization_1 (BatchNo  (100, 49, 32)        128         ['dense_1[0][0]']                
rmalization)                                                                                      
__________________________________________________________________________________________________
add (Add)                       (100, 49, 32)        0           ['svdf_1_0[0][0]',               
                                                                  'batch_normalization_1[0][0]']  
__________________________________________________________________________________________________
activation (Activation)         (100, 49, 32)        0           ['add[0][0]']                    
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (100, 24, 32)        0           ['activation[0][0]']             
__________________________________________________________________________________________________
dense_3 (Dense)                 (100, 24, 50)        1600        ['max_pooling1d[0][0]']          
__________________________________________________________________________________________________
svdf_2_0 (Svdf)                 (100, 24, 50)        2150        ['max_pooling1d[0][0]']          
__________________________________________________________________________________________________
batch_normalization_3 (BatchNo  (100, 24, 50)        200         ['dense_3[0][0]']                
rmalization)                                                                                      
__________________________________________________________________________________________________
add_1 (Add)                     (100, 24, 50)        0           ['svdf_2_0[0][0]',               
                                                                  'batch_normalization_3[0][0]']  
__________________________________________________________________________________________________
activation_1 (Activation)       (100, 24, 50)        0           ['add_1[0][0]']                  
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (100, 11, 50)        0           ['activation_1[0][0]']           
__________________________________________________________________________________________________
svdf_3_0 (Svdf)                 (100, 11, 50)        3250        ['max_pooling1d_1[0][0]']        
__________________________________________________________________________________________________
dense_6 (Dense)                 (100, 11, 128)       6400        ['max_pooling1d_1[0][0]']        
__________________________________________________________________________________________________
svdf_3_1 (Svdf)                 (100, 11, 128)       8320        ['svdf_3_0[0][0]']               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNo  (100, 11, 128)       512         ['dense_6[0][0]']                
rmalization)                                                                                      
__________________________________________________________________________________________________
add_2 (Add)                     (100, 11, 128)       0           ['svdf_3_1[0][0]',               
                                                                  'batch_normalization_6[0][0]']  
__________________________________________________________________________________________________
activation_2 (Activation)       (100, 11, 128)       0           ['add_2[0][0]']                  
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (100, 9, 128)        0           ['activation_2[0][0]']           
__________________________________________________________________________________________________
global_average_pooling1d (Glob  (100, 128)           0           ['max_pooling1d_2[0][0]']        
alAveragePooling1D)                                                                               
__________________________________________________________________________________________________
dropout (Dropout)               (100, 128)           0           ['global_average_pooling1d[0][0]'
                                                                 ]                                
__________________________________________________________________________________________________
dense_7 (Dense)                 (100, 64)            8256        ['dropout[0][0]']                
__________________________________________________________________________________________________
dense_8 (Dense)                 (100, 3)             195         ['dense_7[0][0]']                
==================================================================================================
Total params: 33,923
Trainable params: 32,983
Non-trainable params: 940
__________________________________________________________________________________________________
I0903 18:00:45.455425 140373190989632 train.py:71] None
```

But it fails at the time of converting into tflite stream. Here is the error:

```
2021-09-03 18:22:54.631382: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2021-09-03 18:22:54.634525: E tensorflow/core/grappler/grappler_item_builder.cc:669] Init node svdf_1_0/dense/kernel/Assign doesn't exist in graph
I0903 18:22:54.659212 140426826090304 lite.py:1723] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2021-09-03 18:22:54.663914: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.
2021-09-03 18:22:54.663933: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.
2021-09-03 18:22:54.698313: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1855] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):
Flex ops: FlexAudioSpectrogram, FlexMfcc
Details:
	tf.AudioSpectrogram(tensor<16000x1xf32>) -> (tensor<1x49x513xf32>) : {device = """", magnitude_squared = true, stride = 320 : i64, window_size = 640 : i64}
	tf.Mfcc(tensor<1x49x513xf32>, tensor<i32>) -> (tensor<1x49x40xf32>) : {dct_coefficient_count = 40 : i64, device = """", filterbank_channel_count = 80 : i64, lower_frequency_limit = 2.000000e+01 : f32, upper_frequency_limit = 7.600000e+03 : f32}

******snipped****

I0903 18:01:06.509099 140373190989632 test.py:510] tflite test accuracy, non stream model = 74.63% 200 out of 221
I0903 18:01:06.552580 140373190989632 test.py:514] tflite Final test accuracy, non stream model = 75.11% (N=221)
I0903 18:01:06.553761 140373190989632 model_train_eval.py:257] run TFlite streaming model accuracy evaluation
W0903 18:01:07.091468 140373190989632 test.py:560] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 3 from 1 for '{{node streaming/max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=""NHWC"", explicit_paddings=[], ksize=[1, 3, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/max_pooling1d/ExpandDims)' with input shapes: [1,1,1,32].
I0903 18:01:07.091856 140373190989632 test.py:364] tflite stream model state external with reset_state 1
I0903 18:01:07.342433 140373190989632 model_train_eval.py:282] FAILED to run TFLite streaming: Mmap of '7' at offset '0' failed with error '22'.

```
Below is my environment :

```
Package                       Version
----------------------------- -------------------
absl-py                       0.13.0
argon2-cffi                   21.1.0
astunparse                    1.6.3
async-generator               1.10
attrs                         21.2.0
backcall                      0.2.0
bleach                        4.1.0
cached-property               1.5.2
cachetools                    4.2.2
certifi                       2021.5.30
cffi                          1.14.6
charset-normalizer            2.0.4
cycler                        0.10.0
dataclasses                   0.8
decorator                     5.0.9
defusedxml                    0.7.1
dm-tree                       0.1.6
entrypoints                   0.3
flatbuffers                   1.12
gast                          0.4.0
google-auth                   1.35.0
google-auth-oauthlib          0.4.5
google-pasta                  0.2.0
graphviz                      0.17
grpcio                        1.39.0
h5py                          3.1.0
idna                          3.2
importlib-metadata            4.8.1
ipykernel                     5.5.5
ipython                       7.16.1
ipython-genutils              0.2.0
ipywidgets                    7.6.4
jedi                          0.18.0
Jinja2                        3.0.1
jsonschema                    3.2.0
jupyter                       1.0.0
jupyter-client                7.0.2
jupyter-console               6.4.0
jupyter-core                  4.7.1
jupyterlab-pygments           0.1.2
jupyterlab-widgets            1.0.1
keras-nightly                 2.7.0.dev2021083107
Keras-Preprocessing           1.1.2
kiwisolver                    1.3.1
libclang                      11.1.0
Markdown                      3.3.4
MarkupSafe                    2.0.1
matplotlib                    3.3.4
mistune                       0.8.4
nbclient                      0.5.4
nbconvert                     6.0.7
nbformat                      5.1.3
nest-asyncio                  1.5.1
notebook                      6.4.3
numpy                         1.19.5
oauthlib                      3.1.1
opt-einsum                    3.3.0
packaging                     21.0
pandocfilters                 1.4.3
parso                         0.8.2
pexpect                       4.8.0
pickleshare                   0.7.5
Pillow                        8.3.1
pip                           21.2.4
prometheus-client             0.11.0
prompt-toolkit                3.0.20
protobuf                      3.17.3
ptyprocess                    0.7.0
pyasn1                        0.4.8
pyasn1-modules                0.2.8
pycparser                     2.20
pydot                         1.4.2
Pygments                      2.10.0
pyparsing                     2.4.7
pyrsistent                    0.18.0
python-dateutil               2.8.2
pyzmq                         22.2.1
qtconsole                     5.1.1
QtPy                          1.10.0
requests                      2.26.0
requests-oauthlib             1.3.0
rsa                           4.7.2
scipy                         1.5.4
Send2Trash                    1.8.0
setuptools                    57.4.0
six                           1.15.0
tb-nightly                    2.6.0a20210806
tensorboard-data-server       0.6.1
tensorboard-plugin-wit        1.8.0
tensorflow-addons             0.14.0
tensorflow-model-optimization 0.6.0
termcolor                     1.1.0
terminado                     0.11.1
testpath                      0.5.0
tf-estimator-nightly          2.7.0.dev2021083108
tf-nightly                    2.7.0.dev20210806
tornado                       6.1
traitlets                     4.3.3
typeguard                     2.12.1
typing-extensions             3.7.4.3
urllib3                       1.26.6
wcwidth                       0.2.5
webencodings                  0.5.1
Werkzeug                      2.0.1
wheel                         0.37.0
widgetsnbextension            3.5.1
wrapt                         1.12.1
zipp                          3.5.0
```
The data folder has 1-sec recordings as below:

```
data folder:
_background_noise_  bed  bird  cat  dog  happy  srewai  house  marvin   sheila  tree  wow
 ```

Please note that svdf works fine for me.


"
Depth_from_video_in_the_wild: distorted camera model,google-research/google-research,2021-09-01 05:17:38,0,,803,984633563,"Hi,

In the paper of ""Depth_from_video_in_the_wild"", you mentioned the calibration results from the EUROC dataset where you use a pinhole+radial distortion model to model the fisheye camera in the dataset. 

However, in the [code](https://github.com/google-research/google-research/blob/45eca1118642cc1257824856ac6e1ab0aa7bf299/depth_from_video_in_the_wild/motion_prediction_net.py#L23) itself, there seems to be a standard pinhole model predicted.

It doesn't seem trivial to me how to add the extra two distortion parameters into the model, because the unprojection operation does not have a closed-form solution and involves finding the root of a high-order polynomial. I appreciate if you could enlighten how do you implement that.

Thanks,
JD"
When will the musiq network code be open source,google-research/google-research,2021-08-27 08:31:24,4,,800,981008328,When will the musiq network code be open source
 No module named 'kws_streaming',google-research/google-research,2021-08-27 04:05:29,8,,798,980849477,"I am trying https://github.com/google-research/google-research/blob/master/kws_streaming/colab/00_check_data.ipynb in colab.
I am getting error as :
ModuleNotFoundError                       Traceback (most recent call last)

<ipython-input-16-40e192295e39> in <module>()
      1 
----> 2 from kws_streaming.models import models
      3 from kws_streaming.models import utils
      4 from kws_streaming.layers.modes import Modes
      5 import tensorflow as tf

ModuleNotFoundError: No module named 'kws_streaming'
"
Not able to install tf_trees,google-research/google-research,2021-08-25 15:36:01,0,,796,979324497,"I am trying to add tf_trees to my existing installation of tensorflow, but it is not working. when I try to import tf_trees, it says it is not installed.

I have installed tensorflow from pip and here are the details. so i tried to install it binary way. And downloaded tf_trees folder from repository through DownGit

bash-4.2$ python3
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux


bash-4.2$ uname -a
Linux inlpc9 5.10.9-1.el7.elrepo.x86_64 #1 SMP Mon Jan 18 17:47:08 EST 2021 x86_64 x86_64 x86_64 GNU/Linux


bash-4.2$ pip show tensorflow
Name: tensorflow
Version: 2.6.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/jshah/.local/lib/python3.8/site-packages
Requires: absl-py, google-pasta, wrapt, numpy, keras, wheel, tensorboard, gast, opt-einsum, flatbuffers, termcolor, grpcio, astunparse, h5py, keras-preprocessing, clang, typing-extensions, six, protobuf, tensorflow-estimator
Required-by: "
[felix] Can we get tensorflow1.x code?,google-research/google-research,2021-08-25 02:50:16,1,,794,978651252,"the requirement of felix code confuse me and is not compatible with my environment.
Can we get origin tensorflow1.x code? Thanks a lot."
Can't load TRILL TF2 model with MirroredStrategy,google-research/google-research,2021-08-24 16:47:58,0,,793,978294988,"Can't load TF2 TRILL model (non semantic speech benchmanrk) with MirroredStrategy.

```
hub.load(tfhub_model_path)
*** tensorflow.python.framework.errors_impl.InvalidArgumentError:  assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.]
	 [[node Assert/Assert (defined at /ec2-user-home/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_hub/module_v2.py:106) ]] [Op:__inference_restored_function_body_6478]

Function call stack:
restored_function_body
```

Works fine outside of the MirroredStrategy. Any solution?"
installing scann on osx,google-research/google-research,2021-08-24 16:05:58,1,,792,978258944,"Hi, I'm trying to install scann on OSX 11.5.2 (20G95) in the context of Anaconda jupyter notebooks 6.1.4

```
❯ pip install -q scann
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```

Here's all my current tensorflow related versions:

```
tensorboard                        2.4.1
tensorboard-plugin-wit             1.8.0
tensorflow                         2.4.1
tensorflow-datasets                4.4.0
tensorflow-estimator               2.4.0
tensorflow-hub                     0.12.0
tensorflow-metadata                0.29.0
tensorflow-recommenders            0.5.1
```

has anyone had any luck installing on OSX?  Any hints about what I might be able to fix this?"
variational_dropout,google-research/google-research,2021-08-24 07:17:49,0,,791,977785910,"Hi, I have a question about layer_utils.py which in google-research/state_of_sparsity/layers/utils.
I'm getting an error when I run this file,
![1629788769(1)](https://user-images.githubusercontent.com/72150671/130572235-62ed33f5-2a9d-40b1-b047-01895ecb825d.png)
So, I looked at the code in this file and found that line 21 contained commands in the TF2.0 environment, and line 22 ('tensorflow.contrib') contained in TF1.0. 


21     import tensorflow.compat.v1 as tf 
22     from tensorflow.contrib.eager.python import tfe as contrib_eager
23
24         from tensorflow.contrib.layers.python.layers import utils as layer_utils

here are my packages,

absl-py==0.12.0
numpy==1.19.5
six==1.15.0
tensorflow==2.6.0
tensor2tensor==1.15.7
Is there a problem with the version of my packages? What version do you use?
"
[TFT] Variable selection trainable parameters,google-research/google-research,2021-08-20 16:09:37,0,,788,975743149,"Hello,

Can you please clarify your variable selection method, that is used in Temporal Fusion Transformer?

It seems that it has an enormously large number of trainable parameters for electricity dataset:
number of entities is 370, and hidden_layer_size is 160.
So in input we will have tensor of size (batch_size, n_timestaps, 370), and after ```get_tft_embeddings``` method tensor will be converted to (batch_size, n_timestaps, 160, 370). When the tensor is flattened it will have shape (batch_size, n_timestaps, 59200), and GRN layer has to process about 30 million of trainable parameters (59200 * 160 * 4). If we add sizes of GRN layers from each individual feature (160 * 160 * 4), the total number of parameters will be close to 70 million, which seems quite big in comparison with LSTM and MultiHeadAttention sizes (200k and 100k respectively).
"
continuous-action environments for dual_dice,google-research/google-research,2021-08-19 08:26:25,1,,787,974407616,"@eladeban,
      Recently I try to reproduce dual_dice algorithm on Reacher, but I can't get the results as you presented in the paper. The phenomena is quite strange: the estimate average step reward is quite close to the behavior policy's step reward rather than the target policy's. Since there is no source code for continuous action setting, could you help to provide more details? 
     i) when calculating \nu(s',a'),  I use (1/N)*\sum_{i=1}^{N}nu(s',\pi(s')) and try N=1,10,100 respectively, and the results are the same. How you calculate it?
    ii) For this setting, do you choose Fenchel conjugate trick or not?
   iii) Is any other tips I need to know?

   Look forward to your reply.
   Best wishes.
                       
"
Is there a sample code for DSelect-k?,google-research/google-research,2021-08-19 03:05:17,0,,786,974218887,"I want to try DSelect-k on my case, but I only see the implement layer with keras. 
Can you provide examples? Thanks
"
[scann]How do I cluster my data with scann,google-research/google-research,2021-08-17 03:40:13,0,,784,972270494,I have data from bert.How do I cluster my data with scann?
Manylinux wheel fails to import scann with tensorflow 2.5,google-research/google-research,2021-08-16 18:42:08,0,,782,972004439,"The manylinux wheel appears to have an incompatibility with the tensorflow we run in the cluster. I've created a python virtual environment with TF 2.5 and numpy 1.19.2. Then downloaded the manylinux wheel 3.7 and pip installed it, with the following error on import:

<pre>
import scann
2021-08-16 16:27:37.804869: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jshleap/building/3.7/lib/python3.7/site-packages/scann/__init__.py"", line 2, in <module>
    from scann.scann_ops.py import scann_ops
  File ""/home/jshleap/building/3.7/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops.py"", line 26, in <module>
    ""cc/_scann_ops.so""))
  File ""/home/jshleap/building/3.7/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/jshleap/building/3.7/lib/python3.7/site-packages/scann/scann_ops/cc/_scann_ops.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb
</pre>"
Error about convert MobileBERT to TFLite ,google-research/google-research,2021-08-13 09:20:29,1,,780,970221404,"@saberkun @renjie-liu 
I would like to convert mobilebert to tflite format.
I use the quantized weight you offered in the repo with tensorflow 1.15. https://github.com/google-research/google-research/tree/master/mobilebert#pre-trained-checkpoints 

But the error occurs when I try to convert the model to int8 tflite model. 
I use the flag: `--use_post_quantization  --activation_quantization`

Log:
```
2021-08-13 09:03:07.437436: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-08-13 09:03:09.818900: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2021-08-13 09:03:09.818948: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 7077 nodes (-6520), 7317 edges (-6524), time = 1308.49194ms.
2021-08-13 09:03:09.818955: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 7077 nodes (0), 7317 edges (0), time = 426.431ms.
Traceback (most recent call last):
  File ""run_squad_ptq.py"", line 653, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""run_squad_ptq.py"", line 642, in main
    tflite_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0
```

I also try to convert the model in Tensorflow 2.3/2.4/2.6. In those versions, the model can convert successfully to tflite.
However, a runtime error occurs during inference.
```
RuntimeError: tensorflow/lite/kernels/dequantize.cc:75 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 2 (DEQUANTIZE) failed to prepare.
 ```
I've referred to #325. It still does not work.
I can convert model weights to integers, but whenever I want to convert activation error happened.

Another question: why the model which I converts to fp32 tflite model cannot show in Netron? Your models in tfhub can do. Are there any differences between tfhub models and https://github.com/google-research/google-research/tree/master/mobilebert#pre-trained-checkpoints? "
No matching distribution found for scann,google-research/google-research,2021-08-12 19:22:50,27,,779,969417469,"I try to install scann using _`pip install scann`_, it give me the following error:

_ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann_

Here is package version in my env:

**tensorflow=2.6.0
tensorflow recommender=v0.5.2
tensorflow dataset=4.4.0**

It can install the scann on Colab and I used the same env in my local computer, BUT it did not work. Any suggestions are appreciated. "
"typo of variable ""munchuasen_term"" in m_iqn.py",google-research/google-research,2021-08-12 18:11:20,0,,778,969310581,"This is nothing serious, just raise this so you know. Fixing it would help to debug."
"tf3d use my pointcloud data gives tensorflow.python.framework.errors_impl.InvalidArgumentError: Need minval < maxval, got 0 >= 0",google-research/google-research,2021-08-09 10:12:16,1,,777,963842474,"I am trying to feed my point cloud data into the semantic segmentation sparseconvunet by modifying run_train_local.py and adding new siemens_spec.py, siemens_dataset_frame.py and siemens_train.gin. Waymo semantic segmentation can train smoothly but the siemens one is not working. 

I get stuck on this error: 

```
I0809 09:53:35.545849 140474592175936 train.py:117] Model fit starting for 100 epochs, 10                                                                    0 step per epoch, total batch size:2
I0809 09:53:36.200485 140474592175936 callback_utils.py:370] Saving ckpt for epoch: 0 at                                                                     /tmp/tf3d_experiment/seg_siemens_001/model
Epoch 1/100
Traceback (most recent call last):
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/runpy.py"", line 193, in _run_module_                                                                    as_main
    ""__main__"", mod_spec)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/sdl/assem/users/yvu2cv/tf3d/train.py"", line 222, in <module>
    app.run(main)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/absl/app.py"", line 303                                                                    , in run
    _run_main(main, args)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/absl/app.py"", line 251                                                                    , in _run_main
    sys.exit(main(argv))
  File ""/sdl/assem/users/yvu2cv/tf3d/train.py"", line 177, in main
    train(strategy=strategy, write_path=write_path)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/gin/config.py"", line 1                                                                    069, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/gin/utils.py"", line 41                                                                    , in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/gin/config.py"", line 1                                                                    046, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/sdl/assem/users/yvu2cv/tf3d/train.py"", line 124, in train
    verbose=1 if FLAGS.run_functions_eagerly else 2)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 806, in train_function
    return step_function(self, iterator)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 795, in step_function
    data = next(iterator)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/dist                                                                    ribute/input_lib.py"", line 649, in __next__
    return self.get_next()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/dist                                                                    ribute/input_lib.py"", line 694, in get_next
    self._iterators[i].get_next_as_list_static_shapes(new_name))
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/dist                                                                    ribute/input_lib.py"", line 1474, in get_next_as_list_static_shapes
    return self._iterator.get_next()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/data                                                                    /ops/multi_device_iterator_ops.py"", line 581, in get_next
    result.append(self._device_iterators[i].get_next())
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/data                                                                    /ops/iterator_ops.py"", line 825, in get_next
    return self._next_internal()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/data                                                                    /ops/iterator_ops.py"", line 764, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/eage                                                                    r/context.py"", line 2105, in execution_mode
    executor_new.wait()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/eage                                                                    r/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Need minval < maxval, got 0                                                                     >= 0
         [[{{node random_uniform}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
  In call to configurable 'train' (<function train at 0x7fc2c2df4048>)

```

Does anyone have clue on what caused this? Any help will be appreciated! "
Zn,google-research/google-research,2021-07-30 08:16:56,0,,774,956518479,Hdhdh
Extra unnamed column added when using DVRL preprocess_data function,google-research/google-research,2021-07-28 21:31:36,0,,772,955250243,"I am splitting my data into train, test, valid csvs and looking to load them to use with the DVRL package, but an extra column is being added at the front which leads to almost perfect accuracy (around 99.5%). Does anyone know what might be causing this?

![Snip20210728_9](https://user-images.githubusercontent.com/58536665/127398668-a16084be-dffe-4626-b518-5368f1868ab2.png)
"
Negative loss values for adaptive loss,google-research/google-research,2021-07-27 21:13:35,1,,771,954271442,"Hello, I have used adaptive loss implementation on a neural network, however after training a model long enough, I am getting negative loss values. Any help/suggestion would be highly appreciated! Please let me know if you need additional info

*Model definition -* 

####### define model
best_hyperparameter_space = {""gru_up"": 64, 
                             ""up_dropout"": 0.2,
                             ""learning_rate"": 0.004,
                             ""batch_size"": 1024}

def many_to_one_model(params):
  input_1 = tf.keras.Input(shape =(1, 53), name='input_1')

  input_2 = tf.keras.Input(shape=(1, 19), name='input_2') # this one is 
  
  input_3 = tf.keras.Input(shape=(1, 130), 
                                   name='input_3')
  input_3_flatten = Flatten()(input_3)
  input_3_flatten = RepeatVector(1)(input_3_flatten)
  
  concat_outputs = Concatenate()([input_1, 
                                  input_2, 
                                  input_3_flatten])

  output_1 = GRU(units=int(params['gru_up']), 
                              kernel_initializer=tf.keras.initializers.he_uniform(),
                              activation='relu')(concat_outputs)
  output_1 = Dropout(rate=float(params['up_dropout']))(output_1)
  output_1 = Dense(units=1, 
                               activation='linear', 
                               name='output_1')(output_1)

  model = tf.keras.models.Model(inputs=[input_1, 
                                        input_2, 
                                        input_3], 
                                outputs=[output_1],
                                name='many_to_one_model')

  return model


*Model summary -*

Model: ""many_to_one_model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 1, 130)]     0                                            
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 130)          0           input_3[0][0]                    
__________________________________________________________________________________________________
input_1 (InputLayer)            [(None, 1, 53)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 1, 19)]      0                                            
__________________________________________________________________________________________________
repeat_vector_5 (RepeatVector)  (None, 1, 130)       0           flatten_5[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1, 202)       0           input_1[0][0]                    
                                                                 input_2[0][0]                    
                                                                 repeat_vector_5[0][0]            
__________________________________________________________________________________________________
gru_5 (GRU)                     (None, 64)           51456       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 64)           0           gru_5[0][0]                      
__________________________________________________________________________________________________
output_1 (Dense)                (None, 1)            65          dropout_5[0][0]                  
==================================================================================================
Total params: 51,521
Trainable params: 51,521
Non-trainable params: 0
__________________________________________________________________________________________________


*Adaptive loss implementation -*

####### Create the initial model object
model = many_to_one_model(best_hyperparameter_space)

####### Define robust loss function
adaptive_lossfun = robust_loss.adaptive.AdaptiveLossFunction(num_channels=1, float_dtype=np.float32)

variables = (list(model.trainable_variables) + list(adaptive_lossfun.trainable_variables))

####### Get dynamic param for learning rate
optimizer_call = getattr(tf.keras.optimizers, ""Adam"")         # Can update to meet needs
optimizer = optimizer_call(learning_rate=best_hyperparameter_space[""learning_rate""], amsgrad=True)

mlflow_callback = LambdaCallback()
for epoch in range(750):
  def lossfun():
    ####### Stealthily unsqueeze to an (n,1) matrix, and then compute the loss.
    ####### A matrix with this shape corresponds to a loss where there's one shape
    ####### and scale parameter per dimension (and there's only one dimension for
    ####### this data).
    aa = y_train_up - model([train_cat_ip, train_num_ip, ex_train_num_ip])
    mean_calc = tf.reduce_mean(adaptive_lossfun(aa))
    return mean_calc

  optimizer.minimize(lossfun, variables)

  loss = lossfun()
  alpha = adaptive_lossfun.alpha()[0, 0]
  scale = adaptive_lossfun.scale()[0, 0]
  print('{:<4}: loss={:+0.5f}  alpha={:0.5f}  scale={:0.5f}'.format(epoch, loss, alpha, scale))
  mlflow_callback.on_batch_end(epoch, mlflow.log_metrics({""loss"":loss.numpy(), 
                                                          ""alpha"":alpha.numpy(), 
                                                          ""scale"":scale.numpy()}, epoch))


*Loss, alpha and scale vs epochs graph -*

![image](https://user-images.githubusercontent.com/34693127/127228166-a490381b-1d3c-4603-b0cd-50488976f26c.png)
"
need DEMO script to use TF3D with camera and lidar,google-research/google-research,2021-07-27 10:46:48,0,,770,953751864,"Hello,

I have successfully completed the installation of Tensorflow 3D but I am unable to use the libraries through my depth camera and Lidar. Has anyone been able to use them and if so, how?
Can someone also share a demo that explains how to use datasets and for example object detection?

Thank you
"
[HITNet] Code stops running on Google colab after creating a 'Tensorflow device',google-research/google-research,2021-07-26 11:50:08,0,,769,952818821,"I tried to run the code on Google Colab and this is the output that I got -
```
2021-07-26 11:24:31.361832: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-26 11:24:32.907777: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-07-26 11:24:32.970575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:32.971227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-07-26 11:24:32.971284: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-26 11:24:33.154572: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-07-26 11:24:33.154694: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-07-26 11:24:33.259957: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-07-26 11:24:33.312886: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-07-26 11:24:33.559715: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10
2021-07-26 11:24:33.617689: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-07-26 11:24:33.623997: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-07-26 11:24:33.624181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.624909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.627938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-07-26 11:24:33.628872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.629559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-07-26 11:24:33.629649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.630282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.630836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-07-26 11:24:33.632529: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-26 11:24:38.774730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-07-26 11:24:38.774779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-07-26 11:24:38.774795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-07-26 11:24:38.774971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:38.775604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:38.776196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:38.776739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2021-07-26 11:24:38.776822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
```

The code literally stops after the last execution. There are no errors thrown at all. What can be the solution to this?

On a side note, when will the pre-trained model for the paper be released? My work just requires inference results for images on pre-trained models so it would be great if the authors can release the pre-trained implementation anytime soon."
frechet_audio_distance: Fatal Python error: Segmentation fault,google-research/google-research,2021-07-26 06:27:51,2,,768,952551717,"Hello,
I am trying to apply FAD but facing the Fatal Python error: Segmentation fault here.
Currently I am using tf 2.5.0 and apache-beam 2.24.0 with python 3.7.

I am stuck on the step > **Compute embeddings and eastimate multivariate Gaussians**
`python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats`

I found the problem **Fatal Python error** happened in `def create_pipeline()` of `create_embeddings_beam.py` :

> if files_input_list:
    examples = (
        pipeline
        | 'Read File List' >> ReadFromText(files_input_list)
        | 'Read Files' >> beam.ParDo(ReadWavFiles()))

Cause I am really not an expert using apache so couldn't figure it out what's going on. 

Also by googling it, it seems something wrong with stack memory which may cause the **Segmentation fault** problem, 
so I have tried to increase the stack that my operating system allocates for the python process ended by `ulimit -s 262140`.
But it's still not working.

I will be appreciate if someone has any ideas.
Thank you!!
"
Can 4 GPU Titan RTX train the model Coltran？,google-research/google-research,2021-07-21 08:06:39,1,,767,949441386,Can 4 GPU Titan RTX train the model Coltran(Colorization Transformer)？Thank you for answer!
Question about CuBERT,google-research/google-research,2021-07-20 08:12:38,0,,766,948410843,"Hello!
I have some problems with launching CuBERT. I've loaded the dataset and model for the variable misuse task using methods from bert (run_classifier.py).

And I got the error: `""ValueError: The initializer passed is not valid. It should be a callable with no arguments and the shape should not be provided or an instance of 'tf.keras.initializers.*' and 'shape' should be fully defined."" `

when trying to execute the line: 
`result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)`

from this:
```
from cubert_processors import VarmisuseProcessor
from bert.run_classifier import convert_single_example, file_based_convert_examples_to_features, file_based_input_fn_builder
from bert.run_classifier import PaddingInputExample

processor = VarmisuseProcessor()
eval_batch_size = batch_size
eval_examples = processor.get_test_examples(data_dir)
num_actual_eval_examples = len(eval_examples)

label_list = processor.get_labels()
convert_single_example(0, eval_examples[1], label_list, max_seq_length, full_tokenizer)

if is_use_tpu:
    while len(eval_examples) % eval_batch_size != 0:
        eval_examples.append(PaddingInputExample())

eval_file = os.path.join(output_dir, ""eval.tf_record"")
file_based_convert_examples_to_features(
    eval_examples, label_list, max_seq_length, full_tokenizer, eval_file)

tf.logging.info(""***** Running evaluation *****"")
tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                len(eval_examples), num_actual_eval_examples,
                len(eval_examples) - num_actual_eval_examples)
tf.logging.info(""  Batch size = %d"", eval_batch_size)

# This tells the estimator to run through the entire set.
eval_steps = None
# However, if running eval on the TPU, you will need to specify the
# number of steps.
if is_use_tpu:
    assert len(eval_examples) % eval_batch_size == 0
    eval_steps = int(len(eval_examples) // eval_batch_size)

eval_drop_remainder = True if is_use_tpu else False
eval_input_fn = file_based_input_fn_builder(
    input_file=eval_file,
    seq_length=max_seq_length,
    is_training=False,
    drop_remainder=eval_drop_remainder)

# evaluate all checkpoints; you can use the checkpoint with the best dev accuarcy
steps_and_files = []
model_dir = saved_model_path
filenames = tf.gfile.ListDirectory(model_dir)
for filename in filenames:
    if filename.endswith("".index""):
        ckpt_name = filename[:-6]
        cur_filename = os.path.join(model_dir, ckpt_name)
        global_step = int(cur_filename.split(""-"")[-1])
        tf.logging.info(""Add {} to eval list."".format(cur_filename))
        steps_and_files.append([global_step, cur_filename])
steps_and_files = sorted(steps_and_files, key=lambda x: x[0])
output_eval_file = os.path.join(output_dir, ""eval_results_albert_zh.txt"")
print(""output_eval_file:"", output_eval_file)
tf.logging.info(""output_eval_file:"" + output_eval_file)
with tf.gfile.GFile(output_eval_file, ""w"") as writer:
    for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):
        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)

        tf.logging.info(""***** Eval results %s *****"" % (filename))
        writer.write(""***** Eval results %s *****\n"" % (filename))
        for key in sorted(result.keys()):
            tf.logging.info(""  %s = %s"", key, str(result[key]))
            writer.write(""%s = %s\n"" % (key, str(result[key])))
```

Could someone tell me what I'm doing wrong?
"
[scann]compiling error: proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:cc_toolchain' does not have mandatory provider 'ProtoInfo',google-research/google-research,2021-07-15 03:34:04,2,,764,944965474,"SCANN is not compiled because of error
proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:cc_toolchain' does not have mandatory provider 'ProtoInfo'"
[smurf] [uflow],google-research/google-research,2021-07-12 04:57:47,1,,759,941669833,"Would it be possible to create a simple custom training workflow for smurf and uflow?

Both these modules run into some or the other issues for a custom training.."
Questions about TFT,google-research/google-research,2021-07-07 09:32:01,0,,757,938689250,"Hi there, when I use TFT for custom datasets, I'm facing two questions, hope can find it out here, many thanks.
Q1: how to set the parameters ""total_time_steps"" and ""num_encoder_steps"" for custom datasets?
Q2: If I'm using some sales data, and different item has different time length, eg item1 sales 30days, item2 sales 20days, can TFT handle this?
Thanks"
2.7 Exabyte of storage ,google-research/google-research,2021-07-03 08:34:54,0,,755,936205984,"Install sosumi and replace common and find Jan Sloot code
https://files.fm/u/82zpbg9yc
https://uptobox.com/f9ugkg0vim8e
https://mega.nz/folder/3TgkXBLS#b9brcWFuyVaqhjx47FVZUQ
![Screenshot at 2021-07-02 18-16-46.png](https://user-images.githubusercontent.com/75278757/124348444-3f8d7f00-dbea-11eb-911e-f6b66fce20c3.png)![20210703_070714.jpg](https://user-images.githubusercontent.com/75278757/124348453-44eac980-dbea-11eb-82c4-1e2f69100397.jpg)![Screenshot at 2021-07-02 18-16-32.png](https://user-images.githubusercontent.com/75278757/124348460-4c11d780-dbea-11eb-9b72-2b78b98cff42.png)"
Charformer question,google-research/google-research,2021-07-01 14:53:53,0,,754,934932512,"I can't understand how intra-block positional embeddings can save position of each character after block operations.
Can someone explain idea about it considering discussion in lucidrains repo [embeddings](https://github.com/lucidrains/charformer-pytorch/issues/1#issue-934034665)
"
Performer: `jnp.max` used in `nonnegative_softmax_kernel_feature_creator`,google-research/google-research,2021-07-01 12:40:41,2,,753,934801897,"Hi! Thanks for the amazing works!
I have one question in  `nonnegative_softmax_kernel_feature_creator`.
I don't understand why `jnp.max` terms are used in `nonnegative_softmax_kernel_feature_creator` as below. I cannot find the corresponding equations in the original paper. Is it just for normalization? Is there a mathematical background? 
Thank you!

https://github.com/google-research/google-research/blob/c249ee982c9ca3bb0cca4788758435c87c71fc7d/performer/fast_attention/jax/fast_attention.py#L101-L109

"
[HITNet] Run model on Middlebury,google-research/google-research,2021-07-01 08:32:08,3,,751,934569218,"Hi, I didn't get the reported accuracy according to your sh script and model. Where might be the problems? thanks for your help!
On half resolution. The printed metrics are:
Images processed:
15
psm_epe bad_0.1 bad_0.5 bad_1.0 bad_2.0 bad_3.0
[ 2.07099784 85.09916146 42.11792107 23.01151438 12.87441467  9.49672834]"
SMURF: tensorflow.python.framework.errors_impl.FailedPreconditionError: /data/TF_Sintel/test; Is a directory,google-research/google-research,2021-06-30 08:51:32,4,,750,933458249,"I have encountered an issue like this: 

tensorflow.python.framework.errors_impl.FailedPreconditionError: /data/TF_Sintel/test; Is a directory
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]] [Op:IteratorGetNext]

in smurf/smurf_trainer.py(321)train_eval()
distributed_inputs = train_it.next()

Could you provide me some suggestions on this issue?"
Incompatible function arguments when importing flax.nn: ptopk_patch_selection,google-research/google-research,2021-06-28 15:38:50,4,,749,931703941,"To replicate:
```console
conda create -n ptopk python=3.8
pip install -r requirements.txt
python image_classification.py --config <some config> --workdir /tmp
```

Getting incompatible function arguments error when image_classification.py tries to import flax.nn:

```console
TypeError: jit(): incompatible function arguments. The following argument types are supported:
    1. (fun: function, cache_miss: function, get_device: function, static_argnums: List[int], static_argnames: List[str] = [], donate_argnums: List[int] = [], cache: jaxlib.xla_extension.jax_jit.CompiledFunctionCache = None) -> object

Invoked with: <function _rfft_transpose at 0x7f22f751fdc0>, <function _cpp_jit.<locals>.cache_miss at 0x7f22f751fe50>, <function _cpp_jit.<locals>.get_device_info at 0x7f22f751fee0>, <function _cpp_jit.<locals>.get_jax_enable_x64 at 0x7f22f751ff70>, <function _cpp_jit.<locals>.get_jax_disable_jit_flag at 0x7f22f7526040>, (0, 2)
```

OS Info:
Ubuntu 20.04.1 LTS"
This is not good,google-research/google-research,2021-06-28 06:19:03,0,,748,931214018,
The function does not raise such an exception (typo-minor),google-research/google-research,2021-06-25 08:58:36,0,,745,929980423,https://github.com/google-research/google-research/blob/d3e4296414efe111427d71a082ff454edb7d8ed5/direction_net/pano_utils/geometry.py#L123
Cjdj,google-research/google-research,2021-06-24 17:10:00,0,,744,929439126,
UFlow Training on custom data set  failed with error : train_ds = train_datasets[0]   IndexError: list index out of range ,google-research/google-research,2021-06-24 12:32:53,0,,743,929181867,"Followed the instructions in the readme, which was not successful, seems like training on custom data isn't supported "
failed; build aborted: Analysis of target '@com_google_protobuf//:cc_toolchain' failed,google-research/google-research,2021-06-23 12:33:28,1,,742,928208465,"
When run the automl-zero , I meet the following error:

execute command: `bash ./run_demo.sh`

The errror is : 
```bash 
ERROR: /home/gitlab-runner/.cache/bazel/_bazel_gitlab-runner/80373f2a2f278aa1c4548a331a0e2355/external/com_google_protobuf/BUILD:873:21: in blacklisted_protos attribute of proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:_internal_wkt_protos_genrule' does not have mandatory providers: 'ProtoInfo'
ERROR: Analysis of target '//:run_search_experiment' failed; build aborted: Analysis of target '@com_google_protobuf//:cc_toolchain' failed
INFO: Elapsed time: 0.109s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    Fetching @eigen_archive; fetching
```"
[Meta Pseudo Labels] Is there exits GPU version code?,google-research/google-research,2021-06-17 09:44:43,0,,738,923716185,Is there exits GPU version code? 
Re creating the results  described in the paper,google-research/google-research,2021-06-16 14:29:00,0,,737,922691218,I was running the script using provided Case Bert Base model and train and test data for the GoEmotion model (Bert_classifier.py). After I run the calculate_metric.py script it does not generate the results described in the paper. Is there any change that need to be made to recreate that result?  
Trainable parameters in Low-Rank Factorization!,google-research/google-research,2021-06-14 06:28:45,0,,736,920057208,"Hi,

I'm trying to run the [cifar10 example](https://github.com/google-research/google-research/tree/master/graph_compression/compression_lib/examples/cifar10) in _graph_compression_ directory related to the [Matrix Compression Library](https://github.com/google-research/google-research/tree/master/graph_compression)
Based on the [doc](https://drive.google.com/file/d/1843aNpKx_rznpuh9AmEshgAKmISVdpJY/view) in Low-Rank Approximation we divide an original weight matrix (A) into two low rank matrix B and C. 
According to the results by applying Low-Rank Approximation on cifar10 and determining rank=200 we can reduce parameters from **1,067,584** to **680512**.
But when I execute the example and print the summary of layers and number of parameters I have this result:
**1,639,690**
It means we still have the original weight matrix (A) in our graph.
How can we fix this problem?

Thanks.
Miladdona
 "
CuBERT fine-tuning parameters,google-research/google-research,2021-06-13 21:29:44,0,,734,919885265,"Hi there,

Thank you for the amazing work and for making the fine-tuning datasets online.  I was wondering if it would be possible to make cubert's fine-tuning parameters available to us?  Looking at the paper, it said,

> We pre-train CuBERT with the default configuration of the BERT Large model, one model per example length (128, 256, 512, and 1,024 subword tokens) with batch sizes of 8,192, 4,096, 2,048, and 1,024 respectively, and the default BERT learning rate of 1 × 10−4. Fine-tuned models also used the same batch sizes as for pre-training, and BERT’s default learning rate (5 × 10−5). For both, we gradually warm up the learning rate for the first 10 % of examples, which is BERT’s default value.

What do you mean by with batch sizes 8,192, 4,096, 2,048, and 1,024?  This seems not to match what I found on this page https://github.com/google-research/bert/blob/master/README.md.  Also, the paper said the evaluation is done on either V100 or P100.  Could you also specify the memory limit on those GPUs?

For the exception dataset, I can only achieve an accuracy of 75%, which is 4% away from what was reported, so I would greatly appreciate any help, as we find this work very interesting!  Thank you in advance!"
Why did you choose to have a folder structure with all repos in one folder? ,google-research/google-research,2021-06-09 09:39:36,0,,729,916001627,Why did you choose to have a folder structure with all repos in one folder?  When you could have everything modular in distinct repos? 
"Why cannot find the code for ""AutoDropout: Learning Dropout Patterns to Regularize Deep Networks"" ? ",google-research/google-research,2021-06-07 03:49:48,1,,727,913060405,"Hi, I want to refer to the implementation of AutoDropout, but it seems that I cannot find it here. The paper said that code is published in this website, so how can I touch it ? "
Feature request : Colab record video of animation,google-research/google-research,2021-06-05 06:30:19,0,,726,912119136,I am amazed by the amazing results of the model. I think it would be great if there is an option of recording the animation video.
[HITNet] Run model on GPU,google-research/google-research,2021-06-04 12:10:28,0,,725,911436132,"I'm new to TensorFlow. Can I run the model on GPU, your code can be run successfully on the CPU. However, when I change the device to GPU, It crashes. "
[FAVOR+] Recover the attention score in Performer,google-research/google-research,2021-06-01 13:29:50,0,,723,908295939,"In Appendices to the Paper, you have a figure that visualises the attention scores. How is it possible to recover those? "
Accuracy without finetune on CIFAR10,google-research/google-research,2021-06-01 02:48:17,0,,722,907819491,"I am implementing this paper myself. Without fine-tuning, the student trained on pseudo-label (CIFAR-10-4000) only achieves an accuracy of about 50%. Is this the true status or this is my implementation issue? What is the official accuracy without fine-tuning?
Thank you so much!"
Nebulous. Bothering to argument ,google-research/google-research,2021-05-31 17:52:27,0,,721,907637902,
why rescaling to a default boundary factor?,google-research/google-research,2021-05-31 06:24:38,0,,719,907114203,"In the file datasets.py, line 287, you scale the pose matrix to a default boundary matrix:

        # Rescale according to a default bd factor.
        scale = 1. / (bds.min() * .75)
        poses[:, :3, 3] *= scale
        bds *= scale

why?"
About output shape of TRILL-distilled v3 model,google-research/google-research,2021-05-30 17:06:18,0,,718,906819719,"I want to use trill-distilled model v3 as a feature extractor for an input for an other classification model.

When I make some test with the model I found a strange behavior about the output size. 

If the input vector is in shape in in between:         (1,) - (18320,) model gives TensorShape([1, 2048])
If the input vector is in shape in in between: (18320,) - (21040,) model gives TensorShape([2, 2048])
If the input vector is in shape in in between: (21040,) - (23760,) model gives TensorShape([3, 2048])
If the input vector is in shape in in between: (23760,) - (26480,) model gives TensorShape([4, 2048])
If the input vector is in shape in in between: (26480,) - (29200,) model gives TensorShape([5, 2048])

Its adds up one 2048 element long row for increasing input vector size by 2720. Each increment adds an other feature row as output.

My question is why is the first output range is different from the others?

Thank you..."
Inconsistency on tabnet between paper and code,google-research/google-research,2021-05-29 17:49:41,1,,717,906524977,"Hi

For me there are two items on [https://github.com/google-research/google-research/blob/master/tabnet/tabnet_model.py](url)  that are different from what was reported in the paper [https://arxiv.org/pdf/1908.07442.pdf](url) 

On **interpretability** section the author @soarik  says that:
<img src=""https://render.githubusercontent.com/render/math?math=\sum_{c=1}^{N_d} ReLU(d_{b,c}[i])"">
But the given code in github is dividing this sum by the number of steps:
(line 177)
`# Aggregated masks are used for visualization of the`
` # feature importance attributes.`
 `         scale_agg = tf.reduce_sum(
              decision_out, axis=1, keep_dims=True) / (
                  self.num_decision_steps - 1)`
Also, for the aggregate feature importance mask, it looks like the normalization is missing. The paper states:
<img src=""https://render.githubusercontent.com/render/math?math=\frac{ \sum_{i=1}^{N_{steps}} \eta_b[i]M_{b,j}[i]} {\sum_{j=1}^{D}\sum_{i=1}^{N_{steps}} \eta_b[i]M_{b,j}[i]}"">
But the give code is:
(line 182)
`aggregated_mask_values += mask_values * scale_agg`
Inside a for looping over each step, which will give only the top part of the equation.

Am I missing something or the paper and code are really different?
"
[POEM] How to treat missing keypoints.,google-research/google-research,2021-05-26 08:20:49,12,,716,901980142,"Hi!

I'm interested in the Pr-VIPE technology, and trying to adapt it to action recognition.
I already made sample ""input.csv"" and confirmed that infer.py works well.

But, although I changed some scores of keypoint to zero assuming the keypoint is not detected, results (unnormalized_embeddings.csv) did not change.
Of course, I set  the min_input_keypoint_score_2d to greater than 0 (actually set to 0.1).

Would you show me how to treat missing keypoint?

Best."
Google,google-research/google-research,2021-05-26 04:10:16,0,,715,901726907,
Google,google-research/google-research,2021-05-26 04:09:07,1,,714,901726061,
Summary-level for Rouge-N,google-research/google-research,2021-05-25 18:30:59,1,,712,901200144,"According to the documentation, there is a possibility for summary-level evaluation of RougeL (using `rougeLsum` mode). But is a similar thing possible for Rouge-N variations?
Right now looking at the code, I can't help but conclude that it is not. So I was wondering, was that a conscious decision to not make it an option? Would something like that make no sense to compute? 

Ultimately, I am interested in computing Rouge-2 and Rouge-3 with 1 candidate sentence and multiple references. Right now I'm looking at calculating pairwise scores between the candidate and all references and then average out the result. 
I was wondering if the way `rougeLsum` does it would be a better choice and if it's possible using the current state of the code. "
OSError: @neural-rendering/lpips/distance/1 does not exist.,google-research/google-research,2021-05-18 13:07:17,2,,707,894361721,"When I tested the model, raised an error:
File ""/home/sy/nerf/code/jaxnerf/eval.py"", line 73, in main 
    lpips_model = tf_hub.load(LPIPS_TFHUB_PATH) 

I can not solve this problem. I hope you can give me some advice， thank you."
BAM: It's possibile to train on GPU??,google-research/google-research,2021-05-17 18:21:32,0,,705,893584979,"Hello to everyone, thanks for sharing of your work. I'm using BAM to solve new type of tasks. It'seems that the training takes place on CPU in absence of TPU. It's possible to train using GPU?"
Cannot replicate results table 1 from Distilling Effective Supervision from Severe Label Noise,google-research/google-research,2021-05-14 01:35:26,0,,702,891510443,"I clone repository at https://github.com/google-research/google-research/tree/master/ieg , set 'use_imagenet_as_eval' to False
then I run command:
CUDA_VISIBLE_DEVICES=0 python -m ieg.main --dataset=cifar10_uniform_0.2 --network_name=resnet29 --probe_dataset_hold_ratio=0.002 --checkpoint_path=${SAVEPATH}/ieg

That's all step, but in paper the results is ~0.92, when I test it is only 0.8x.

I use 1 GPU cuda 10.1 cudnn 7.6.5 with all required package in conda env.
"
[Felix] Machine Translation - APE (Automatical Post Editing),google-research/google-research,2021-05-12 10:25:09,0,,700,889936536,"How can I run the APE task?

There seems to be no example, so please contact us.
"
[TCC] Evaluation gives very high loss value,google-research/google-research,2021-05-12 10:07:25,3,,699,889914916,"Hi @debidatta

Once training is completed, I usually see that I get the training loss of around 0.005 with 60000 iterations for my settings.
But for evaluation when I run `python -m tcc.evaluate --alsologtostderr --logdir $MODEL_DIR`, I get a very high training loss like ~1.000. And I got a very small value for the kendalls tau task like 0.02. Though alignment looks working quite well. 

I am wondering what could cause this discrepancy in the training loss between training and evaluation. Any feedback would be appreciated. Thank you!"
Example of inference commands for new PG-19 checkpoints?,google-research/google-research,2021-05-11 20:13:30,0,,698,888484567,"Thanks for uploading the PG-19 checkpoints and the 'routing_transformer' scripts. 

'routing_transformer_test.py' works but I'm unclear how to run inference from the scripts as per the published paper.

I would appreciate an example to run. Using ubuntu 18.04 and tf 1.15 

Cheers"
working with posenet but not working,google-research/google-research,2021-05-09 06:06:54,0,,695,881853626,working with posenet but not working with the c++ api
Distilling Effective Supervision from Severe Label Noise,google-research/google-research,2021-05-08 13:49:03,0,,694,880847074,"In the paper, it mentions that it separate data into clean and noisy using T, and only keep clean ones to calculate unsupervised loss. However, I don't see this step in the code. Do anyone know where this step is?"
(tf3d) object detection dataset,google-research/google-research,2021-05-08 01:04:07,3,,693,880032709,"Hello,
I am planning to work on 3D object detection using tf3d.
I am curious to know is it possible to use my own custom dataset to train a object detection model? As on repository it is mentioned that currently it supports only 3 datasets (waymo, rio and scannet).
If Yes, than please can any one help me in preparing .tfrecord with which i can train tf3d object detection model? As i don't have much experience in this filed.

Right now i am able to train the tf3d object detection model with only following waymo dataset source:(gs://waymo_open_dataset_tf_example_tf3d/original_tfrecords/train-00000-of-01212.tfrecords .)

Thank you"
tf3d simplier installation process ?,google-research/google-research,2021-05-05 21:05:48,0,,689,876834089,"Hi, 
I would like to try out the sparce conv 3d layer for one of my work project 
However I am running a win10/wsl setup and it seems I wont be able to complete the tf3d/ops instructions with the nvidia docker image with GPU support unless in get windows insider build crap and I am not even sure it would work then.

Any plan for pip/conda package ?    "
Performer faster than Linear Transformer,google-research/google-research,2021-05-05 14:12:59,0,,688,876496304,"Hi,

In ""Rethinking Attention with Performers"", in the appendix, section D.5, Performers seem to be faster than Linear Transformers. I'm not sure I understand how. 

Let's assume we're in a base case  (nb_heads, nb_layers, d_ff, d), and we have a sequence of size n:
 - Linear transformer simply applie `elu(x) + 1`, thus converts out input from `(batch_size, nb_heads, sequence_length, d // nb_heads)` to `(batch_size, nb_heads, sequence_length, d // nb_heads)`
 - Performer converts input from `(batch_size, nb_heads, sequence_length, d // nb_heads)` to `(batch_size, nb_heads, sequence_length, m * l)` (m being the number of random vector we use, and l the number of kernel functions).
 
 Performers approximation should be slower, unless `m*l < d // nb_heads` right? "
Performer: seed difference or same for \omega between training and prediction?,google-research/google-research,2021-05-05 13:30:11,0,,687,876453495,"As performer need to generate <img src=""https://render.githubusercontent.com/render/math?math=\omega""> inside the attention calculation, is the generation way the same for training and inference?"
Why are the test advantages of the last task normalized?,google-research/google-research,2021-05-03 12:43:43,2,,686,874540818,"Shouldn't it be inside the for loop (1-indentation in lines 581 and 583 )?

https://github.com/google-research/google-research/blob/8c62c43029efc0c49d9dfc019bd9af11578cc1ae/norml/maml_rl.py#L581

Thank you for making the repository available. "
[roar benchmark] compute feature ranking,google-research/google-research,2021-04-29 07:33:48,0,,684,870705436,"Dear @sarahooker , @doomie 
thanks a lot for the nice work and code. 
I have a question regarding the ranking algorithm. E.g. in the `compute feature ranking()` in ""data_input.py"", the saliency map is regarded as a 3-channel map and the ranking is for each channel separately. 
It means that for one pixel ( (x,y) coordinate) in an input image,  two of the channels could be removed but one channel left. Is that correct? 
Thank you very much!

Best,"
[cache_replacement] ChampSim dataset error,google-research/google-research,2021-04-28 04:16:07,1,,683,869476575,"![I7}R~$2_O`SJ}TD6O3WI8D9](https://user-images.githubusercontent.com/38975880/116345697-3e6f5880-a81b-11eb-8bc9-4cee9d9824c9.png)
I got an error like this when I tried to get the dataset that the aurther mentioned in the Readme. Did anyone solve this pronlem?"
[depth_and_motion_learning] Details of training/evaluation setting on Waymo data set,google-research/google-research,2021-04-25 16:14:55,0,,681,867050059,"Dear authors, 

 I am trying to evaluate another unsupervised depth estimation method on Waymo dataset, and I would like to use similar if not the same setting you used for your work. I would be grateful if you could share more details about the training/evaluation setting like the following: 
 * I found that data records have also radial and tangential distortion parameters, Have you processed Waymo's data considering those distortions?
 * What is the input resolution of the model for Waymo's data? (480 * 196?)
 * Did you crop the input images so only the part of the image that has lidar ground truth is processed? (0.4 * height crop?)
 
Thank you for sharing the code of your work,
"
[depth_from_video_in_the_wild] Convolution kernel regularization,google-research/google-research,2021-04-23 07:20:56,0,,680,865825477,"Hi @gariel-google,
first of all thanks for your great work, also with [depth_and_motion_learning]. Upon porting this model to tf2, I've stumbled upon the following issue and it would be very kind if you could help me with that.

In [train.py line 64](https://github.com/google-research/google-research/blob/b12b5753dc0870af2e3c30a0ac1d0adfb3da0754/depth_from_video_in_the_wild/train.py#L64) the default weight regularization is set to 1e-2, so it seems to be active by default. Also in the CVF [paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.pdf) it is stated
![image](https://user-images.githubusercontent.com/826149/115832258-fc0cdc80-a412-11eb-9aa9-1e131fb2a20e.png)
that a l2 regularization term is used. Digging deeper in the code, special care is taken where the l2-weights are applied:
- In the Resnet encoder, the parameter is ignored.
- For the depth decoder, it is applied to each Conv2D and Conv2DTranspose layer.
- For the motion network, we apply it in the enocder, decoder, and refinements in the v2-variants, but only to the encoder in the v1-variants.

If I load the [Cityscapes + KITTI checkpoint](https://www.googleapis.com/download/storage/v1/b/gresearch/o/depth_from_video_in_the_wild%2Fcheckpoints%2Fcityscapes_kitti_learned_intrinsics.zip?generation=1566493762028542&alt=media) in my new implementation and train with the same l2-weight, I get a quite high l2-loss. Additionally, in your reference training code I didn't find a call to `tf.GraphKeys.REGULARIZATION_LOSSES`, so I'm now wondering if this l2-regularization is really active.

Thank you already in advance
xerxesr"
unable to install tf3d on widows 10 (CPU only),google-research/google-research,2021-04-20 19:03:07,0,,677,863138870,"### I have followed the instruction mentioned in (https://github.com/tensorflow/custom-op), but I face error with both bazel and makefile builders:
**bazel error msg:**
**_""bled by setting --experimental_repo_remote_exec
ERROR: error loading package '': in C:/users/windows10-desktop/_bazel_windows10-desktop/gbkcodrx/external/org_tensorflow/tensorflow/workspace.bzl: Extension file 'third_party/py/python_configure.bzl' has errors
INFO: Elapsed time: 0.164s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)""_**

**makefile error msg:
""make: *** No rule to make target 'zero_out_pip_pkg'.  Stop.""**

please find below screenshots for tf3d/ops directory:
![ops](https://user-images.githubusercontent.com/72650269/115450035-79c2c380-a22c-11eb-9833-d249aa3894f6.jpg)
"
Does CKA works on Fully Connected Layer?,google-research/google-research,2021-04-15 16:47:53,0,,676,859066928,"Dear @simonster 



Hello, I'm a graduate student who is new in deep learning.

First, please take my gratitude for your your great work.

I want to ask a question about your CKA similarity index.

Does CKA works properly on Fully Connected Layer?

I tested it on FC Layers with 2 Hidden Layers and 1 Output Layer, but the result was so bad even it cannot passed the sanity check.

I wonder did I do wrong on the test or does CKA not work properly on FC Layer.

Please give me any advise.

It will be so much helpful to me.

Thank you in advance."
[uflow] trained model checkpoints,google-research/google-research,2021-04-15 14:48:21,3,,675,858959821,"Dear authors @AustinCStone @eladeban, 

I would like to benchmark your model, however I cannot find the trained model checkpoints.
Would you be willing to share the files?

Kind Regards,
Stefano"
"sgk/transformer ""CsrSoftmax"" op Error",google-research/google-research,2021-04-15 07:42:14,1,,672,858593718,"$ bash ./benchmark.sh ../../sgk_models/transformer/sparse sparse
   ........
   No OpKernel was registered to support Op '**CsrSoftmax**' used by {{node sparse_transformer/body/decoder/layer_0/self_attention/multihead_attention/CsrSoftmax}} with these attrs: []
Registered devices: [CPU]
Registered kernels:
  device='GPU

$  $ bash ./benchmark.sh ../../sgk_models/transformer/sparse dense
 .........
 No OpKernel was registered to support Op '**FusedSoftmax**' used by {{node sparse_transformer/body/decoder/layer_0/self_attention/multihead_attention/FusedSoftmax}} with these attrs: []
Registered devices: [CPU]
Registered kernels:
  device='GPU'"
PSE on dm-control failed to run when contrastive_loss_weight > 0.,google-research/google-research,2021-04-13 23:09:56,4,,670,857385104,"Hi @agarwl ,

Thanks for uploading the PSE code. I have a question about running PSE on dm-control. The following command ran fine, i.e. when contrastive_loss_weight was set to zero.

`python -m pse.dm_control.run_train_eval
        --trial_id=1
        --seed 0
        --env_name=cartpole-swingup
        --root_dir=/tmp/drq
        --num_train_steps=10000
        --eval_interval=25
        --policy_save_interval=50
        --checkpoint_interval=100
        --gin_bindings=""load_dm_env_for_eval.action_repeat=8""
        --contrastive_loss_weight=0
        --gin_bindings=""drq_agent.train_eval.initial_collect_steps=300""
        --gin_bindings=""drq_agent.train_eval.eval_episodes_per_run=1""
        --alsologtostderr`

Then I followed the instructions to download and configure distracting_control and the dataset for cartpole-swingup and set `contrastive_loss_weight` to 1.0. The training script failed because when it isn't 0, it needs `data_dir` to load the episode data from the dataset I downloaded from the GCP bucket. It's a simple fix. No problem. But the training still failed with the following exception:

`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is Func/StatefulPartitionedCall/Losses_1/contrastive_loss/write_summary/summary_cond/StatefulPartitionedCall/input/_1059 , and the dst node is StatefulPartitionedCall/Losses_1/contrastive_loss/write_summary/summary_cond/StatefulPartitionedCall/Losses_1/contrastive_loss/write_summary/summary_cond [Op:__inference__train_27434]
`

Any idea on how to fix this? Thanks!


"
what is the tpu_platform on the readme.md?,google-research/google-research,2021-04-11 13:54:51,0,,669,855313003,
code for 2021CVPR-CV-MIM: Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization,google-research/google-research,2021-04-09 14:46:14,1,,665,854592868,May you share the code about `CV-MIM: Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization`?
why does scann limits the number of codewords of dimension d/M?,google-research/google-research,2021-04-02 16:15:03,0,,664,849312147,"Referring to the paper, it says ""With product quantization we encode each datapoint into an M dimensional codeword, each with k possible states"".
Here, k directly relates to the compression rate of the dataset so I wanted to know how ScaNN's behavior differs as k varies. However, I could not find a parameter to set k and it seems like k is fixed to 16 in the code.

Is there any reason why ScaNN only supports k=16? If it is, is it because to maximize the benefit of SIMD instruction?
Or is there other way to adjust k?"
TF3D: Non Max Suppression,google-research/google-research,2021-03-29 08:06:58,2,,660,843097986,"I'm interested in a C++ version of the 3D Non Max Suppression algorithm. The implementation in tf3d is already super nice but I'm looking for a high performance one like the 2D Non Max Suppression that already exists in TensorFlow (see [this thread](https://github.com/tensorflow/addons/issues/2434) for more precise discussion).

Have you considered creating it?"
Checkpoints of Meta Pseudo Label,google-research/google-research,2021-03-28 11:48:32,0,,659,842721515,Could you provide the checkpoints of Meta Pseudo Label in efficientnet l2 and b7?
Question about seq2act,google-research/google-research,2021-03-28 11:29:43,5,,658,842718461,"@zhouxin913 

hi，I can’t find the gbash.sh file in the seq2act project. Is this file not submitted?
![image](https://user-images.githubusercontent.com/19492817/112750429-2da0ac80-8ffb-11eb-8f01-ad3990e3620d.png)

In fact, I still have a lot of questions about the seq2act project. I would like to consult you in detail. Thank you very much.

"
tf3d setup -- download failed,google-research/google-research,2021-03-26 06:52:35,2,,654,841648730,"As this seems like another issue, I open a new one.
I am currently at step 6 according to https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md
at line ""bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures""

i get this error:
```
WARNING: Download from https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/local_config_tf/BUILD:5674:1: target 'libtensorflow_framework.so' is both a rule and a file; please choose another name for the rule
```

Is there any regional restriction for this url ```https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz``` ?
"
Getting no registered kernels ConfigureDistributedTPU for 'meta_pseudo_labels',google-research/google-research,2021-03-25 08:41:15,3,,651,840697481,"Traceback (most recent call last):
  File ""meta_pseudo_labels/main.py"", line 463, in main
    train_tpu(params, should_eval=should_eval)
  File ""meta_pseudo_labels/main.py"", line 237, in train_tpu
    set_tpu_info(params)
  File ""meta_pseudo_labels/main.py"", line 68, in set_tpu_info
    topology_proto = sess.run(tpu_init)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by node ConfigureDistributedTPU (defined at /usr/local/lib/p
ython3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [tpu_embedding_config="""", is_global_init=false, embedding_config=""""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>
         [[ConfigureDistributedTPU]]
I0322 13:53:47.967103 140539649898304 main.py:470] Failed 50 times. Retry!"
request for seq2act pretrained tuple phrase extract model,google-research/google-research,2021-03-25 05:02:57,0,,650,840560199,"Hello,

Request to upload the pretrained tuple phrase extraction model for seq2act repository.
Only pretrained grounding model is provided. 

thanks."
Unavailable Training Code,google-research/google-research,2021-03-23 13:21:34,0,,649,838721219,"Can you publish the code of the training process on github?
I need to know more details of the training process.
Thank you a lot!"
For information on FAVOR accuracy test,google-research/google-research,2021-03-22 17:52:31,0,,648,837970706,"This is not an issue I'm just wondering that the reasoning is behind https://github.com/google-research/google-research/blob/master/performer/fast_attention/jax/fast_attention_test.py#L46

1. Why are Q and K the same (byproduct of random setting random seed to 0). Interestingly even though K uses the same seed the result is different (because different shape?). 
2. Why is `nb_random_features` set to 10000. Is this just to verify the approximation works at all? Obviously the whole point of the FAVOR mechanism allows `nb_random_features` << sequence length.

Context: I think this is wonderful and I'm working on notebook(s) for https://plutojl.org/plutocon2021 with the aim explaining FAVOR in an accessible way."
Automl-zero run_demo.sh shows ERROR: Generating C++ proto_library,google-research/google-research,2021-03-22 14:21:16,5,,647,837766960,"I used a fresh ubuntu system and tried to run Automl-zero research (https://github.com/google-research/google-research/tree/master/automl_zero) done by google.

Here is the order of codes I used in a notebook (Instruction were given in the same auto-ml repository):
1. cd
2. ! sudo apt install apt-transport-https curl gnupg
3. ! sudo apt install g++
4. ! g++ --version # output: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
5. ! curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor > bazel.gpg
6. ! sudo mv bazel.gpg /etc/apt/trusted.gpg.d/
7. ! echo ""deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list
8. ! sudo apt update && sudo apt install bazel-3.4.1
9. ! sudo apt update && sudo apt full-upgrade
10. ! sudo ln -s /usr/bin/bazel-3.4.1 /usr/bin/bazel
11. ! bazel --version
12. ! git clone https://github.com/google-research/google-research.git
13. cd google-research/automl_zero
14. ! ./run_demo.sh

And after running run_demo.sh file, I am getting an error.

[211 / 443] 2 actions running
    Compiling com_google_absl/absl/flags/usage.cc; 0s processwrapper-sandbox
[212 / 443] 2 actions running
    Compiling com_google_absl/absl/flags/usage.cc; 0s processwrapper-sandbox
[212 / 443] 2 actions running
    Compiling com_google_absl/absl/flags/usage.cc; 1s processwrapper-sandbox
**ERROR**: /root/google-research/automl_zero/BUILD:30:14: **Generating C++ proto_library** //:task_proto failed (Exit 1): protoc failed: error executing command bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/k8-opt/bin' '-Itask.proto=task.proto' --direct_dependencies task.proto ... (remaining 2 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox protoc failed: error executing command bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/k8-opt/bin' '-Itask.proto=task.proto' --direct_dependencies task.proto ... (remaining 2 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
devtools/staticanalysis/pipeline/analyzers/proto_best_practices/proto/optouts.proto: File not found.
task.proto:20:1: Import ""devtools/staticanalysis/pipeline/analyzers/proto_best_practices/proto/optouts.proto"" was not found or had errors.
Target //:run_search_experiment failed to build
Use --verbose_failures to see the command lines of failed build steps.
**INFO**: Elapsed time: 231.997s, Critical Path: 14.04s
**INFO**: 216 processes: 21 internal, 195 processwrapper-sandbox.
**FAILED**: Build did NOT complete successfully
**FAILED**: Build did NOT complete successfully

Could someone help me with this?
@crazydonkey200 
@claytonkanderson"
[tf3d] - Lack of detailed documentation,google-research/google-research,2021-03-22 12:44:30,3,,646,837680690,"Hi, I open this issue to ask you if it is possible to have a sort of readme where you explain from the beginning how to train a network using the modules that you provide. For example, my ideal tutorial for semantic segmentation would be.

1 - How to prepare you dataset for the network. Which data are needed? In which format? etc..
2 - Build a simple model from scratch using sparse convolutions etc, or loading existing ones, how to feed data inside the model and how tu run a training. 
3 - Load trained weights and test the result

I know that these information can be extracted from the repository but I think that some examples with detailed documentation will be very useful for every user. I let you an example of a well designed repository that explain from the beginning how to use it (https://github.com/matterport/Mask_RCNN).

Thank you for your patience!

Francesco. "
[tf3d] NaN error for box_classification_using_center_distance_loss,google-research/google-research,2021-03-22 08:18:02,0,,645,837451007,"Hi everyone,

I am running the standard script `run_train_locally.sh` for the waymo dataset with the standard configuration of 240 Epochs and 100 Steps. However after a while (always different number of steps), the box classification value becomes NaN (see picture).

![image](https://user-images.githubusercontent.com/17144445/111959683-117aa800-8aef-11eb-908f-bb667fb3b56b.png)


However there is no error message whatsoever and the training continuous, expect that the total loss also turns into a NaN respectively. I'm not sure what causes this. Does anyone have experienced something similar ?

Thanks a lot!

Max"
help for data(AndroidHowTo/crawled_instructions.json) of seq2act project,google-research/google-research,2021-03-22 07:26:38,3,,644,837411643,"The instructions in [README.md](https://github.com/google-research/google-research/blob/master/seq2act/data_generation/README.md)  have listed all the steps about how to generate the ```AndroidHowTo```  Dataset, but the ```CommonCrawl``` Dataset is very large and difficult for me the deal with. So if it is convenient for you guys to supply the final ```crawled_instructions.json``` file? That will help a lot of people who have situations like me."
metis installation problem,google-research/google-research,2021-03-21 03:34:22,0,,642,836974630,"I just followed the instruction and installed metis=5.1.0 and networkx=1.11(python=3.6.2).
However,when I test :
`import networkx as nx
`
 `import metis` 
`G = metis.example_networkx()
`
 `(edgecuts, parts) = metis.part_graph(G, 3)`

Error occured:
`Traceback (most recent call last):
`
 ` File ""<string>"", line 1, in <module>
`
`  File ""/root/work_dir/anaconda3/envs/cluster-gcn/lib/python3.6/site-packages/metis.py"", line 765, in part_graph
`
 `   graph = networkx_to_metis(graph)
`
  `File ""/root/work_dir/anaconda3/envs/cluster-gcn/lib/python3.6/site-packages/metis.py"", line 574, in networkx_to_metis`
   ` for i in H.nodes:
`
`TypeError: 'method' object is not iterable`
could someone help?"
FVD example ,google-research/google-research,2021-03-16 23:07:23,1,,638,833261628,"Attempting to run `frechet_video_distance/example.py` I get the error:

`NotImplementedError: Cannot convert a symbolic Tensor (covariance/Size_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported`

Ubuntu 20.04.2 LTS
tensorflow version: 2.4.1

complete traceback:

```
  File ""/home/ian/projects/frame-prediction-pytorch/fid/example.py"", line 51, in <module>
    tf.app.run(main)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/ian/projects/frame-prediction-pytorch/fid/example.py"", line 40, in main
    result = fvd.calculate_fvd(
  File ""/home/ian/projects/frame-prediction-pytorch/fid/frechet_video_distance.py"", line 140, in calculate_fvd
    return tfgan.eval.frechet_classifier_distance_from_activations(real_activations, generated_activations)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow_gan/python/eval/classifier_metrics.py"", line 792, in frechet_classifier_distance_from_activations
    return _frechet_classifier_distance_from_activations_helper(
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow_gan/python/eval/classifier_metrics.py"", line 716, in _frechet_classifier_distance_from_activations_helper
    tfp.stats.covariance(activations1),)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow_probability/python/stats/sample_stats.py"", line 438, in covariance
    tf.ones([sample_ndims], tf.int32)),
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 3120, in ones
    output = _constant_if_small(one, shape, dtype, name)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 2804, in _constant_if_small
    if np.prod(shape) < 1000:
  File ""<__array_function__ internals>"", line 5, in prod
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 3030, in prod
    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 852, in __array__
    raise NotImplementedError(
NotImplementedError: Cannot convert a symbolic Tensor (covariance/Size_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

Conda environment:
```
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                      1_llvm    conda-forge
_tflow_select             2.1.0                       gpu  
absl-py                   0.12.0             pyhd8ed1ab_0    conda-forge
aiohttp                   3.7.4            py38h497a2fe_0    conda-forge
astunparse                1.6.3              pyhd8ed1ab_0    conda-forge
async-timeout             3.0.1                   py_1000    conda-forge
attrs                     20.3.0             pyhd3deb0d_0    conda-forge
blas                      1.0                         mkl    conda-forge
blinker                   1.4                        py_1    conda-forge
brotlipy                  0.7.0           py38h497a2fe_1001    conda-forge
bzip2                     1.0.8                h7f98852_4    conda-forge
c-ares                    1.17.1               h7f98852_1    conda-forge
ca-certificates           2021.1.19            h06a4308_1  
cachetools                4.2.1              pyhd8ed1ab_0    conda-forge
cairo                     1.16.0            h7979940_1007    conda-forge
certifi                   2020.12.5        py38h578d9bd_1    conda-forge
cffi                      1.14.5           py38ha65f79e_0    conda-forge
chardet                   4.0.0            py38h578d9bd_1    conda-forge
click                     7.1.2              pyh9f0ad1d_0    conda-forge
cloudpickle               1.6.0                      py_0  
comet_ml                  3.5.0                      py38    comet_ml
configobj                 5.0.6                      py_0    conda-forge
cryptography              3.4.6            py38ha5dfef3_0    conda-forge
cudatoolkit               10.1.243             h036e899_8    conda-forge
cudnn                     7.6.5.32             hc0a50b0_1    conda-forge
cupti                     10.1.168                      0  
cycler                    0.10.0                     py_2    conda-forge
dbus                      1.13.6               hfdff14a_1    conda-forge
decorator                 4.4.2              pyhd3eb1b0_0  
dill                      0.3.3              pyhd3eb1b0_0  
dulwich                   0.20.20          py38h497a2fe_0    conda-forge
everett                   1.0.2                      py_1    comet_ml
expat                     2.2.10               h9c3ff4c_0    conda-forge
ffmpeg                    4.3.1                hca11adc_2    conda-forge
fontconfig                2.13.1            hba837de_1004    conda-forge
freetype                  2.10.4               h0708190_1    conda-forge
future                    0.18.2                   py38_1  
gast                      0.4.0              pyh9f0ad1d_0    conda-forge
gettext                   0.19.8.1          h0b5b191_1005    conda-forge
glib                      2.66.7               h9c3ff4c_1    conda-forge
glib-tools                2.66.7               h9c3ff4c_1    conda-forge
gmp                       6.2.1                h58526e2_0    conda-forge
gnutls                    3.6.13               h85f3911_1    conda-forge
google-auth               1.24.0             pyhd3deb0d_0    conda-forge
google-auth-oauthlib      0.4.1                      py_2    conda-forge
google-pasta              0.2.0              pyh8c360ce_0    conda-forge
googleapis-common-protos  1.52.0           py38h06a4308_0  
graphite2                 1.3.13            h58526e2_1001    conda-forge
grpcio                    1.36.1           py38hdd6454d_0    conda-forge
gst-plugins-base          1.18.4               h29181c9_0    conda-forge
gstreamer                 1.18.4               h76c114f_0    conda-forge
h5py                      2.10.0          nompi_py38h7442b35_105    conda-forge
harfbuzz                  2.7.4                h5cf4720_0    conda-forge
hdf5                      1.10.6          nompi_h7c3c948_1111    conda-forge
icu                       68.1                 h58526e2_0    conda-forge
idna                      2.10               pyh9f0ad1d_0    conda-forge
importlib-metadata        3.7.3            py38h578d9bd_0    conda-forge
jasper                    1.900.1           h07fcdf6_1006    conda-forge
jpeg                      9d                   h36c2ea0_0    conda-forge
jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
keras-preprocessing       1.1.2              pyhd8ed1ab_0    conda-forge
kiwisolver                1.3.1            py38h1fd1430_1    conda-forge
krb5                      1.17.2               h926e7f8_0    conda-forge
lame                      3.100             h7f98852_1001    conda-forge
lcms2                     2.12                 hddcbb42_0    conda-forge
ld_impl_linux-64          2.35.1               hea4e1c9_2    conda-forge
libblas                   3.9.0                     8_mkl    conda-forge
libcblas                  3.9.0                     8_mkl    conda-forge
libclang                  11.1.0          default_ha53f305_0    conda-forge
libcurl                   7.75.0               hc4aaa36_0    conda-forge
libedit                   3.1.20191231         he28a2e2_2    conda-forge
libev                     4.33                 h516909a_1    conda-forge
libevent                  2.1.10               hcdb4288_3    conda-forge
libffi                    3.3                  h58526e2_2    conda-forge
libgcc-ng                 9.3.0               h2828fa1_18    conda-forge
libgfortran-ng            7.5.0               h14aa051_18    conda-forge
libgfortran4              7.5.0               h14aa051_18    conda-forge
libglib                   2.66.7               h3e27bee_1    conda-forge
libiconv                  1.16                 h516909a_0    conda-forge
liblapack                 3.9.0                     8_mkl    conda-forge
liblapacke                3.9.0                     8_mkl    conda-forge
libllvm11                 11.1.0               hf817b99_0    conda-forge
libnghttp2                1.43.0               h812cca2_0    conda-forge
libopencv                 4.5.1            py38h703c3c0_0    conda-forge
libpng                    1.6.37               h21135ba_2    conda-forge
libpq                     13.1                 hfd2b0eb_2    conda-forge
libprotobuf               3.15.6               h780b84a_0    conda-forge
libssh2                   1.9.0                ha56f1ee_6    conda-forge
libstdcxx-ng              9.3.0               h6de172a_18    conda-forge
libtiff                   4.2.0                hdc55705_0    conda-forge
libuuid                   2.32.1            h7f98852_1000    conda-forge
libuv                     1.41.0               h7f98852_0    conda-forge
libwebp-base              1.2.0                h7f98852_1    conda-forge
libxcb                    1.13              h7f98852_1003    conda-forge
libxkbcommon              1.0.3                he3ba5ed_0    conda-forge
libxml2                   2.9.10               h72842e0_3    conda-forge
llvm-openmp               11.0.1               h4bd325d_0    conda-forge
lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
matplotlib                3.3.4            py38h578d9bd_0    conda-forge
matplotlib-base           3.3.4            py38h0efea84_0    conda-forge
mkl                       2020.4             h726a3e6_304    conda-forge
mkl-service               2.3.0            py38h1e0a361_2    conda-forge
multidict                 5.1.0            py38h497a2fe_1    conda-forge
mysql-common              8.0.23               ha770c72_1    conda-forge
mysql-libs                8.0.23               h935591d_1    conda-forge
ncurses                   6.2                  h58526e2_4    conda-forge
nettle                    3.6                  he412f7d_0    conda-forge
ninja                     1.10.2               h4bd325d_0    conda-forge
nspr                      4.29                 h9c3ff4c_1    conda-forge
nss                       3.62                 hb5efdd6_0    conda-forge
numpy                     1.20.1           py38h18fd61f_0    conda-forge
nvidia-ml                 7.352.0                    py_0    conda-forge
oauthlib                  3.0.1                      py_0    conda-forge
olefile                   0.46               pyh9f0ad1d_1    conda-forge
opencv                    4.5.1            py38h578d9bd_0    conda-forge
openh264                  2.1.1                h780b84a_0    conda-forge
openssl                   1.1.1j               h27cfd23_0  
opt_einsum                3.3.0                      py_0    conda-forge
pcre                      8.44                 he1b5a44_0    conda-forge
pillow                    8.1.2            py38ha0e1e83_0    conda-forge
pip                       21.0.1             pyhd8ed1ab_0    conda-forge
pixman                    0.40.0               h36c2ea0_0    conda-forge
promise                   2.2.1                      py_0    powerai
protobuf                  3.15.6           py38h709712a_0    conda-forge
psutil                    5.8.0            py38h497a2fe_1    conda-forge
pthread-stubs             0.4               h36c2ea0_1001    conda-forge
py-opencv                 4.5.1            py38h81c977d_0    conda-forge
pyasn1                    0.4.8                      py_0    conda-forge
pyasn1-modules            0.2.7                      py_0    conda-forge
pycparser                 2.20               pyh9f0ad1d_2    conda-forge
pyjwt                     2.0.1              pyhd8ed1ab_0    conda-forge
pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
pyqt                      5.12.3           py38h578d9bd_7    conda-forge
pyqt-impl                 5.12.3           py38h7400c14_7    conda-forge
pyqt5-sip                 4.19.18          py38h709712a_7    conda-forge
pyqtchart                 5.12             py38h7400c14_7    conda-forge
pyqtwebengine             5.12.1           py38h7400c14_7    conda-forge
pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
pysocks                   1.7.1            py38h578d9bd_3    conda-forge
python                    3.8.8           hffdb5ce_0_cpython    conda-forge
python-dateutil           2.8.1                      py_0    conda-forge
python-flatbuffers        1.12               pyhd8ed1ab_0    conda-forge
python_abi                3.8                      1_cp38    conda-forge
pytorch                   1.7.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch
qt                        5.12.9               hda022c4_4    conda-forge
readline                  8.0                  he28a2e2_2    conda-forge
requests                  2.25.1             pyhd3deb0d_0    conda-forge
requests-oauthlib         1.3.0              pyh9f0ad1d_0    conda-forge
requests-toolbelt         0.9.1                      py_0    conda-forge
rsa                       4.7.2              pyh44b312d_0    conda-forge
scipy                     1.6.1            py38h91f5cce_0  
setuptools                49.6.0           py38h578d9bd_3    conda-forge
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.34.0               h74cdb3f_0    conda-forge
tensorboard               2.4.1              pyhd8ed1ab_0    conda-forge
tensorboard-plugin-wit    1.8.0              pyh44b312d_0    conda-forge
tensorflow                2.4.1           gpu_py38h8a7d6ce_0  
tensorflow-base           2.4.1           gpu_py38h29c2da4_0  
tensorflow-datasets       3.1.0                      py_0    powerai
tensorflow-estimator      2.4.1              pyheb71bc4_0  
tensorflow-gan            2.0.0                      py_0    powerai
tensorflow-gpu            2.4.1                h30adc30_0  
tensorflow-hub            0.8.0              pyh1d8a796_0    powerai
tensorflow-metadata       0.21.0             pyh1d8a796_0    powerai
tensorflow-probability    0.7                        py_2  
termcolor                 1.1.0                      py_2    conda-forge
tk                        8.6.10               h21135ba_1    conda-forge
torchvision               0.8.1                py38_cu101    pytorch
tornado                   6.1              py38h497a2fe_1    conda-forge
tqdm                      4.59.0             pyhd3eb1b0_1  
typing                    3.7.4.3          py38h06a4308_0  
typing-extensions         3.7.4.3                       0    conda-forge
typing_extensions         3.7.4.3                    py_0    conda-forge
urllib3                   1.26.4             pyhd8ed1ab_0    conda-forge
websocket-client          0.57.0           py38h578d9bd_4    conda-forge
werkzeug                  1.0.1              pyh9f0ad1d_0    conda-forge
wheel                     0.36.2             pyhd3deb0d_0    conda-forge
wrapt                     1.12.1           py38h497a2fe_3    conda-forge
wurlitzer                 1.0.3                      py_2    comet_ml
x264                      1!161.3030           h7f98852_0    conda-forge
xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
xorg-libice               1.0.10               h7f98852_0    conda-forge
xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
xorg-libx11               1.7.0                h7f98852_0    conda-forge
xorg-libxau               1.0.9                h7f98852_0    conda-forge
xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
xorg-libxext              1.3.4                h7f98852_1    conda-forge
xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
xorg-xproto               7.0.31            h7f98852_1007    conda-forge
xz                        5.2.5                h516909a_1    conda-forge
yarl                      1.6.3            py38h497a2fe_1    conda-forge
zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11            h516909a_1010    conda-forge
zstd                      1.4.9                ha95c52a_0    conda-forge
```"
Tensorflow 3D demos?,google-research/google-research,2021-03-16 22:34:51,1,,637,833247212,"I am unable to find a link to the demos mentioned in the tf3d setup doc:

https://github.com/google-research/google-research/blob/master/tf3d/doc/setup.md

""Now you are ready to start training or evaluation or try one of our **demos**""

Can you add a link, please?"
About tf3d requirements.txt,google-research/google-research,2021-03-14 14:34:10,8,,631,831172054,"In my practice, I found that the package version in requirements.txt is written in xxx >= yyy format, such as 'tensorflow-probability >= 0.11.1', but it seems that pip3 installs the latest version of the package by default according to requirements.txt, which resulting in an error:  'ERROR: The version of TensorFlow requires Tensorflow version >=2.4; Detected an Installation of Version 2.3.0.'.
When I manually changed >= to == and installed the specified version of the software package, for example, tensorflow-probability==0.11.1, the error was fixed. So it is recommended to specify the minimum version in requirements.txt to avoid conflicts with the tensorflow version."
ModuleNotFoundError: No module named 'meta_pseudo_labels',google-research/google-research,2021-03-13 09:24:35,2,,628,830851965,"Help, i am unable to execute the file due to this error code: ModuleNotFoundError: No module named 'meta_pseudo_labels'"
Voxelization explained,google-research/google-research,2021-03-12 14:33:09,2,,626,830175016,"I'm reading (https://arxiv.org/pdf/2004.01170.pdf) and I would like to understand in detail how do you apply Voxelization to the input point cloud, I took a look at the code but it's not easy to understand it without many comments. Then I would like to understand how to re-obtain the point cloud in output from voxels"
Problem running Scalable Shampoo,google-research/google-research,2021-03-11 19:52:30,0,,625,829473785,"I am trying to run the script from  https://github.com/google/flax/tree/master/examples/mnist   but with the shampoo optimizer from    https://github.com/google-research/google-research/blob/master/scalable_shampoo/jax/shampoo.py

The training is not working properly as it is  stuck.  I did some debugging and  found out that the line 
` return jaxpr, out_avals, consts `  in  /jax/interpreters/partial_eval.py(1204)trace_to_subjaxpr_dynamic() is not excuted and that is why training is stuck.

Any chance some one can help?


Thanks a lot
"
[Meta Pseudo Labels] Reliable student u_aug labels?,google-research/google-research,2021-03-11 18:07:43,2,,623,829401040,"I see that the student is trained on the augmented unlabeled data. Is the student able to produce reliable labels for these augmented images that it was trained on, or is it only able to produce reliable labels for the labeled and original (non-augmented) unlabeled images? Has this been evaluated?"
[ScaNN] Choosing a value for training_sample_size,google-research/google-research,2021-03-11 17:29:24,3,,622,829362817,"Hi there!

How should one go about selecting `training_sample_size` for the `tree()` and `score_ah()` methods of the `ScannBuilder` class? The hyperparameter is not mentioned in the [algorithms](https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md) section. Should one leave it as default (e.g. `100000`) or stick to a value similar to the one from the example notebook (e.g. `250000`)? Does it depend on the dataset? In my case, I would like to build a ScaNN index on word embeddings with ~4M rows and 300 features.

Thanks in advance."
"If the teacher model in paper ""meta pseudo labels"" is initialized randomly?",google-research/google-research,2021-03-11 12:37:01,0,,621,829112272,"Hi, I wonder if the teacher model in paper ""meta pseudo labels"" is initialized randomly?"
can metis install in windows? ,google-research/google-research,2021-03-11 10:36:25,0,,620,829021465,"Hello, I found that some blogs said that Metis can be installed on windows, but they failed to use cmake to configure.
Is there a Metis that has been compiled and installed directly like a python library?"
How to visualize feature map of Lidar point cloud?,google-research/google-research,2021-03-10 09:45:11,0,,616,827441030,"Hi Admin and authors,

Is there any way to visualize feature map like feature map of images?

Any help will be helpful.

Thanks in advance"
[uflow] Question about training loss and training memory requirement,google-research/google-research,2021-03-09 17:51:44,0,,615,826368139,"Hi @AustinCStone, 

Thank you for your great work. I just have a question about uflow's training loss and training memory requirement. 
1. Training loss: in your final SOTA result, the losses you used are self-sup + smoothness + occ masking + photo loss as census loss, right? But I just have one question: all these losses are applied only on model output level, i,e, level 2 in your implementation or the losses are applied on all levels?
2.  Since self-sup loss will engage after 50% of total iteration, how much memory will be required if using batch size of 1?

I am looking forward to your reply. Thank you very much!

Best,"
Problem of tf3d setup - Preparing the 3D Sparse Convolution Op,google-research/google-research,2021-03-09 09:08:31,33,,614,825576915,"Hi, I'm trying to install [tf3d](https://github.com/google-research/google-research/tree/master/tf3d) but encounter a problem of step 1 [Preparing and Compiling the Sparse Conv Op](https://github.com/google-research/google-research/tree/master/tf3d/ops). <br> My virtual environment (managed by using miniconda) is ```tensorflow==2.3.0, Python=3.7``` running on RTX-3090.<br> I tried ```
 pip3 install tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp37-cp37m-linux_x86_64.whl  ```.  <br> Then when I ``` import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops``` in python shell, I got error message as bellow:

```bash
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/media/xxx/357B56513FD6C042/dockers/tf3d/google-research/tf3d/ops/tensorflow_sparse_conv_ops/__init__.py"", line 19, in <module>
    from tensorflow_sparse_conv_ops import sparse_conv_ops
  File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/__init__.py"", line 18, in <module>
    from tensorflow_sparse_conv_ops.sparse_conv_ops import submanifold_sparse_conv3d, submanifold_sparse_conv2d
  File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/sparse_conv_ops.py"", line 27, in <module>
    resource_loader.get_path_to_datafile('_sparse_conv_ops.so'))
  File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/_sparse_conv_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb
>>> 
```
"
[HITNet] Run model on KITTI images,google-research/google-research,2021-03-08 15:04:20,12,,613,824650058,"Hi,

Thanks for publishing this amazing work.
I have downloaded the FlyingThings model using the provided script. When I feed the model with  FlyingThings data I obtain correct and sharp disparity predictions. Nevertheless, if I feed it KITTI data, I obtain incredibly wrong outputs, not even close to the expected predictions.

Is there any reason your model does not work with images from a different dataset? Do I have to apply some sort of preprocessing before feeding the data to the network?

Thanks in advance,
Sergio"
[BiGG] RuntimeError: CUDA error: no kernel image is available for execution on the device,google-research/google-research,2021-03-08 10:15:11,0,,612,824409367,"Hi, I install the BiGG in my new machine.

This is my Environment:
* RTX3090, cuda 11.1
* CentOS7
* Python3.6
* PyTorch1.8

I can run the test code normally on cpu:
`python -m bigg.unit_test.lib_test`

But I get error on gpu:
`python -m bigg.unit_test.lib_test -gpu 0`
This is Error message:
```
$ python -m bigg.unit_test.lib_test -gpu 0
Namespace(accum_grad=1, batch_exec=False, batch_size=10, bfs_permute=False, bits_compress=256, blksize=-1, data_dir='.', dev_ratio=0.2, directed=False, display=False, dist_backend='gloo', embed_dim=256, epoch_load=None, epoch_save=100, eval_folder=None, g_type=None, gpu=0, grad_clip=5, greedy_frac=0, learning_rate=0.001, max_num_nodes=-1, model_dump=None, node_order='default', num_epochs=100000, num_proc=1, num_test_gen=-1, old_model=False, param_layers=1, phase='train', pos_base=10000, pos_enc=True, rnn_layers=2, save_dir='.', seed=34, self_loop=False, share_param=True, train_method='full', train_ratio=0.8, tree_pos_enc=False)
use gpu indexed: 0
====== begin of tree_clib configuration ======
| bfs_permute = 0
| max_num_nodes = 1000000
| bits_compress = 256
| dim_embed = 256
| gpu = 0
| seed = 34
======   end of tree_clib configuration ======
venv_of_bigg/lib/python3.6/site-packages/torch/cuda/__init__.py:104: UserWarning: 
GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join(arch_list), device_name))
Traceback (most recent call last):
  File ""venv_of_bigg/bigg/bigg/unit_test/lib_test.py"", line 47, in <module>
    ll, _ = model.forward_train([0])
  File ""venv_of_bigg/bigg/bigg/model/tree_model.py"", line 470, in forward_train
    fn_hc_bot, h_buf_list, c_buf_list = self.forward_row_trees(graph_ids, list_node_starts, num_nodes, list_col_ranges)
  File ""venv_of_bigg/bigg/bigg/model/tree_model.py"", line 445, in forward_row_trees
    binary_embeds, base_feat = TreeLib.PrepareBinary()
  File ""venv_of_bigg/bigg/bigg/model/tree_clib/tree_lib.py"", line 176, in PrepareBinary
    feat = torch.cuda.FloatTensor(num_nodes + 2, self.embed_dim).fill_(0)
RuntimeError: CUDA error: no kernel image is available for execution on the device
```

Could you please help fix this error or tell me how to avoid this problem?
Thank you for your excellent work.
I am looking forward to your reply."
[POEM] Action Recognition,google-research/google-research,2021-03-08 01:55:26,17,,611,824095714,"Thanks for sharing of your work .
In the paper(View-Invariant Probabilistic Embedding forHuman Pose),the embeddings for action recogni-
tion is evaluated by using nearest neighbor search with the sequence distance . 
Can I understand in this way that some pose have already been recognized, and other pose use nearest neighbor search with the sequence distance to find the nearest already recognized pose?Does the model need to additional training?"
"Build from source failed - ""does not have mandatory provider 'ProtoInfo'""",google-research/google-research,2021-03-07 19:36:55,2,,610,823999410,"After I run configure.py and try to build blaze, I get the following error message:

```
 CC=clang-8 bazel build -c opt --features=thin_lto --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
DEBUG: Rule 'com_google_protobuf' indicated that a canonical reproducible form can be obtained by modifying arguments commit = ""52b2447247f535663ac1c292e088b4b27d2910ef"", shallow_since = ""1569016252 -0700"" and dropping [""tag""]
DEBUG: Repository com_google_protobuf instantiated at:
  /mnt/hd1/gabrielcrds/github/google-research/scann/WORKSPACE:39:15: in <toplevel>
Repository rule git_repository defined at:
  /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/com_google_protobuf/BUILD:979:21: in proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:cc_toolchain' does not have mandatory provider 'ProtoInfo'.
WARNING: /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/local_config_tf/BUILD:5785:8: target 'libtensorflow_framework.so.2' is both a rule and a file; please choose another name for the rule
DEBUG: Rule 'pybind11_bazel' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1586412825 -0700""
DEBUG: Repository pybind11_bazel instantiated at:
  /mnt/hd1/gabrielcrds/github/google-research/scann/WORKSPACE:12:15: in <toplevel>
Repository rule git_repository defined at:
  /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis of target '@com_google_protobuf//:cc_toolchain' failed
INFO: Elapsed time: 0.275s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
```

Any idea how to fix the problem?"
Brandon Dube,google-research/google-research,2021-03-07 06:14:40,0,,609,823843971,![image](https://user-images.githubusercontent.com/80189847/110230933-22d38a00-7f1d-11eb-9c28-67276b96d899.jpeg)
AutoML-Zero in Colab,google-research/google-research,2021-03-05 12:31:02,0,,606,823042535,"Hi, Can anyone please help me running the AutoML-Zero In Google Colab Notebook."
[TFT] Input data include label?,google-research/google-research,2021-03-05 06:18:14,2,,605,822785029,"Hello, I am new here and studying the TFT project. 
I encounter a problem in _batch_sampled_data() which convert time-series data input model inputs and outputs. 

Let me copy and code in here first.

```
    id_col = self._get_single_col_by_type(InputTypes.ID)
    time_col = self._get_single_col_by_type(InputTypes.TIME)
    target_col = self._get_single_col_by_type(InputTypes.TARGET)
    input_cols = [
        tup[0]
        for tup in self.column_definition
        if tup[2] not in {InputTypes.ID, InputTypes.TIME}
    ]

    for i, tup in enumerate(ranges):
      if (i + 1 % 1000) == 0:
        print(i + 1, 'of', max_samples, 'samples done...')
      identifier, start_idx = tup
      sliced = split_data_map[identifier].iloc[start_idx -
                                               self.time_steps:start_idx]
      inputs[i, :, :] = sliced[input_cols]
      outputs[i, :, :] = sliced[[target_col]]
      time[i, :, 0] = sliced[time_col]
      identifiers[i, :, 0] = sliced[id_col]

    sampled_data = {
        'inputs': inputs,
        'outputs': outputs[:, self.num_encoder_steps:, :],
        'active_entries': np.ones_like(outputs[:, self.num_encoder_steps:, :]),
        'time': time,
        'identifier': identifiers
    }
```

In the Volatility dateset, the target_col is 'log_vol' while input_cols are ['log_vol', 'open_to_close', 'days_from_start', 'day_of_week', 'day_of_month', 'week_of_year', 'month', 'Region']

It is reasonable to have 'log_vol' in input data as it is the observation of past timestep. 
However, it is not removed in the period of target. 
As you can see, the outputs is cropped but not inputs in the line of sample_data.

So, I am wonder if the label is removed somewhere else? If not, then it seems strange to include the label in inputs? 

Thank you for your attentions. "
Meta back translation system code link is not working,google-research/google-research,2021-03-04 08:46:38,1,,603,821907031,"Hello,

I read your recent article on ""Meta BACK-Translation"" but I could not find the code as stated in your article. The link provided is not working.   

_**https://github.com/google-research/google-research/tree/master/meta_back_translation**_

Is it possible to have a link to the original code ? "
dot_product = s_loss_old - s_loss_new but s_loss_new - s_loss_old?,google-research/google-research,2021-03-04 04:25:45,2,,602,821739141,"Hello, thanking for your amazing work. I have read your paper and code, then I have some questions about the dot_product in code. I think the dot_prodoct should be s_loss_old - s_loss_new but s_loss_new - s_loss_old for the reason here.
<a href=""https://ibb.co/xgbvSzc""><img src=""https://i.ibb.co/kDTngqr/image.png"" alt=""image"" border=""0""></a>
Am I wrong?
"
Extracting loglikelihood scores along with predictions on target set ,google-research/google-research,2021-03-03 17:36:27,0,,600,821331905,I have a model for the T5 CBQA task and I wish to extract the loglikelihood scores along with predictions from an exported model. How would I go about doing that?
The code is not working in Google Colab.,google-research/google-research,2021-03-03 12:49:08,0,,599,821080134,"Hi,
I have been trying to implement the code in Google Colab but not able to run it successfully. I am receiving the following error while running ./run_demo.sh command.

![image](https://user-images.githubusercontent.com/79981317/109807982-9723da00-7c4c-11eb-8126-aa520d199733.png)
"
[kws_streaming] Why I got 403 forbidden when I wget a pretrained model?,google-research/google-research,2021-03-03 08:47:22,2,,597,820890472,"Hello, 

In [kws_streaming] part I follow the tutorial and I input
wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz
it's work. but when i input
wget https://storage.googleapis.com/kws_models/models1.zip
it returns 403 forbidden. 
Does my network wrong? Or it just close the access to the model.

Thank you"
[slot_attention] Train and test subsets of CLEVRwithmasks used in Object Discovery task,google-research/google-research,2021-03-02 15:52:30,4,,595,820147322,"@tkipf Could you please upload the file lists for CLEVR6 / CLEVR10 train and test splits? It would make repro much easier!

From what I understood that the first 70K frames were used for training. How was the test split obtained? What was its size? Was filtering <= 6 objects done before or after getting the train/test splits?

Do you train only on CLEVR6 with crops?

Are crops done both in training and in testing?

Thank you!"
[FAVOR+] TypeError in def create_projection_matrix,google-research/google-research,2021-03-02 05:43:52,0,,594,819653610,"CODE:
https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py#L28

BUG:
TypeError: Expected int for argument 'seed2' not <tf.Tensor 'oos/while/while/attention_6/random_normal/mod:0' shape=() dtype=int32>.

FIX:
tf.random.normal takes seed as python int. To take seed as tensorflow tensor, please use tf.random.stateless_normal instead.

BTW:
various static Tensor.shape[x] had better be its dynamic counterpart tf.shape(tensor)[x]"
Not proper use DVRL repo?,google-research/google-research,2021-02-27 10:49:15,0,,593,817893977,"Hi, 
my case is image recognition (classifier for custom images). I adapted my custom datasets to .npz format and use notebook (https://github.com/google-research/google-research/blob/master/dvrl/main_dvrl_image_transfer_learning.ipynb). 
I have tried to use DVRL for 2 cases:
1. choosing the best described images by model,
2. choosing the best synthetic data

 
1. In this problem I have many described by model photos and some data described by human. My assumption was that data described by human has better quality than desribed by model and dvrl model should can pick up the best  photos from train set. Valid and test is from the same distribution, splited to 2 sets. Datsets:
train = 191443 - data described by model, I assume this set has lower quality than valid and train
valid = 3639 - data described by human, I assume this set has high quality
test = 8268 - data described by human, I assume this set has high quality

First of all, is it possible for dvrl to choose data which are wrong described by model? What do you mean by low quality, is it only jitter on photos, or it could be my case (wrong described photos by model, for instance wrong class)?


2. In this problem I have synthetic data generated by GAN in train dataset. Valid and test has same distribution splited to 2 sets. Valid and test are real photos, not synthetic.
 
train = 1132 - generated by gan
valid = 35 - real data
test = 40 - real data
Number of classes: 4

Does dvrl could choose high quality synthetic data, based on real photos included in valid and test set? It is possible that my valid and test set are to small?"
[tf3d] sparse_conv wheel for python 3.8,google-research/google-research,2021-02-26 09:19:37,6,,591,817184838,"Hi @HRLTY and @afathi3, could you please also provide wheel for python 3.8 for sparse convolution op? Python 3.8 is the default for Ubuntu 20.04 LTS.

Regards,"
[depth_and_motion_learning] Imagenet Checkpoint,google-research/google-research,2021-02-26 04:38:29,4,,589,817032867,The README for depth_and_motion says that you should supply a checkpoint for the depth model pretrained on imagenet.  Can you release such a model checkpoint?
How to merge subtokenized sentences back to tokens using code_to_cubert_sentences,google-research/google-research,2021-02-25 08:47:44,0,,587,816221235,"Hi,
It might be a dumb question, but how do I merge subtokenized tokens if I used the **_[gs://cubert/20201018_Java_Deduplicated/github_java_vocabulary.txt]_**  from Cubert project to subtokenize the java code in the first place?

Thank you,
Peter

"
Problem in running behavior_regularized_actor_critic,google-research/google-research,2021-02-24 16:11:32,0,,585,815624443,"Hi, 

The dependencies you detail in the `requirements.txt` are not working.

Long story short: 

1. Update the `readme.md` by saying that to run the code, `python==3.6` is needed (I actually tested in on `python3.6.9`). (This step can be made via `virtualenv --python=/usr/bin/python3.6 ~/env`)
2. Update the `requirements.txt` as follow:

```
tensorflow==2.2.0
tensorflow-probability=0.10.1
cloudpickle==1.3.0
```

Here, I detail the reason for this update.


I followed all the steps described, using virtual env and installing the prerequisites.
Doing that, I get the following error from `pip`:

```
Collecting absl-py>=0.5.0
  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)
Collecting numpy>=1.13.3
  Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)
ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0
ERROR: No matching distribution found for tensorflow==1.14.0
```

I found out that this issue is given because I was using python 3.8.
I, therefore, recreated the virtual environment using python 3.6.
Doing that, the installation of the requirements proceed fine. However, once running 

```
python -m train_online \
 --sub_dir=0 \
 --env_name=HalfCheetah-v2 \
 --eval_target=4000 \
 --agent_name=sac \
 --total_train_steps=500000 \
 --gin_bindings=""train_eval_online.model_params=(((300, 300), (200, 200),), 2)"" \
 --gin_bindings=""train_eval_online.batch_size=256"" \
 --gin_bindings=""train_eval_online.optimizers=(('adam', 0.0005),)""
```

I get a long list of warnings 

```
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
```

 followed by an error

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_online.py"", line 35, in <module>
    tf.compat.v1.enable_v2_behavior()
AttributeError: module 'tensorflow._api.v1.compat.v1.compat' has no attribute 'v1'
```

After seeing this error, I installed the newest `tensorflow==2.4.1`, which, instead, raises the issue

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_online.py"", line 32, in <module>
    from behavior_regularized_offline_rl.brac import train_eval_online
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_eval_online.py"", line 32, in <module>
    from behavior_regularized_offline_rl.brac import train_eval_utils
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_eval_utils.py"", line 25, in <module>
    from tf_agents.environments import suite_mujoco
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/environments/__init__.py"", line 26, in <module>
    from tf_agents.environments import utils
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/environments/utils.py"", line 25, in <module>
    from tf_agents.policies import random_py_policy
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/policies/__init__.py"", line 18, in <module>
    from tf_agents.policies import actor_policy
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/policies/actor_policy.py"", line 29, in <module>
    from tf_agents.networks import network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/__init__.py"", line 18, in <module>
    from tf_agents.networks import actor_distribution_network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/actor_distribution_network.py"", line 26, in <module>
    from tf_agents.networks import categorical_projection_network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/categorical_projection_network.py"", line 26, in <module>
    from tf_agents.networks import network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/network.py"", line 32, in <module>
    from tensorflow.python.keras.engine import network as keras_network  # TF internal
ImportError: cannot import name 'network'
```
To solve this, I found out that one needs to use `tensorflow-2.2.0`.

Doing that, however, I had a problem with cloudpickle:

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_online.py"", line 31, in <module>
    from behavior_regularized_offline_rl.brac import agents
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/agents.py"", line 21, in <module>
    from behavior_regularized_offline_rl.brac import bc_agent
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/bc_agent.py"", line 25, in <module>
    from behavior_regularized_offline_rl.brac import networks
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/networks.py"", line 24, in <module>
    import tensorflow_probability as tfp
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/__init__.py"", line 76, in <module>
    from tensorflow_probability.python import *  # pylint: disable=wildcard-import
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py"", line 23, in <module>
    from tensorflow_probability.python import distributions
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/distributions/__init__.py"", line 88, in <module>
    from tensorflow_probability.python.distributions.pixel_cnn import PixelCNN
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/distributions/pixel_cnn.py"", line 37, in <module>
    from tensorflow_probability.python.layers import weight_norm
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/layers/__init__.py"", line 31, in <module>
    from tensorflow_probability.python.layers.distribution_layer import CategoricalMixtureOfOneHotCategorical
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/layers/distribution_layer.py"", line 28, in <module>
    from cloudpickle.cloudpickle import CloudPickler
ImportError: cannot import name 'CloudPickler'
```

I found out that there is an incompatibility between `tensorflow-probability` and `cloudpickle`.
I've therefore switched to `tensorflow-probability-0.10.1` that aims to overcome this issue and requires `cloudpickle==1.3.0`.

This solved the problem.
"
Questions about Custom training for kws_streaming.,google-research/google-research,2021-02-20 18:15:01,1,,579,812676512,"Firstly, a big thanks for open sourcing this work.

I am following [01_train.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/01_train.ipynb) and [02_inference.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb) for training a model for keyword detection on a dataset which has only 1 class (+ unknown + silence).
I have 2 questions:
1. What size of the dataset will I require to train a model that is robust enough to be relied upon in real life? (are 3000 instances of keyword class of 1-sec audio clips good enough?) (size of unknown words?)

2. Tips, suggestions for training?"
[depth_and_motion_learning] pre-trained models,google-research/google-research,2021-02-19 09:18:54,0,,578,811850438,"Hi @uniqueness, 

thank you very much for releasing the code from the paper. Can you please also release the pre-trained models? I would be interested in using the estimated depths for the Waymo dataset.

Thank you very much in advance."
"'AutoContrast': autocontrast, NameError: name 'autocontrast' is not defined",google-research/google-research,2021-02-17 10:59:23,2,,576,810078259,"File ""/home/google-research/meta_pseudo_labels/augment.py"", line 63, in <module>
    'AutoContrast': autocontrast,
NameError: name 'autocontrast' is not defined

How can I solve it?"
Why is all this one single repository?,google-research/google-research,2021-02-15 16:17:28,0,,575,808666265,Am I missing something?
tf3d: object detection 3d,google-research/google-research,2021-02-14 22:10:54,1,,574,808058903,"Hi @HRLTY and @afathi3,
Thanks for releasing tf3d repo. 
Got some questions regarding current model architecture for 3d object detection:
* The graph layer is missing compared with paper. Why? It seems like interesting concept
* Can we expect implementation of LSTM enhancement from the second paper?

Regards,"
Instructions to load and use CuBERT pretrained and fine-tuned models,google-research/google-research,2021-02-11 07:34:16,9,,571,806149675,"Any instructions to know how to load the provided pre-trained models? I understand that these are probably just BERT models trained on Source code using different tokenizer but I can't figure out how to incorporate the Cubert tokenizer with original BERT files to load / use the pretrained models. Any help is appreciated. Also please correct me if I am wrong about my assumption above.
Thanks!"
MPI  Extrapolation: Sample input file not present,google-research/google-research,2021-02-10 13:00:05,0,,570,805492473,"Hi @eladeban,

For mpi_extrapolation, you have shown how to run the testing code. As an argument, you're passing a file (`mpi_extrapolation/examples/0.npz`), but I can't find this file in the repository. Can you kindly add this file or update the readme on the structure of this file?"
[slot_attention] Gradient instability,google-research/google-research,2021-02-09 06:38:53,1,,567,804260112,"@tkipf I'm trying slot attention with higher level features.

Hard K-means variant trains very fast, while the full attention variant with `slots = updates` is prone to gradient blow-up (probably because of softmax and maybe), even with warmup. 

It seems that the `slots = updates` variant does not do residual update in contrast to GRU (which in return requires learned parameters). Does it make sense to first try some residual / momentum `slots = alpha * slots_prev + (1 - alpha) * updates`?

Also it turns out that temperature `sqrt(slot_size)` is too high, I've had to decrease it for softmax not converging to uniform assignment.

Could you recommend what gradients / distributions should I monitor? Gradients wrt projection weights? Gradients wrt inputs?

Have you tried using more than 3 iterations in training? E.g. 10 iterations? Was it still stable?"
interpretability_benchmark: clarification regarding feature importance estimates,google-research/google-research,2021-02-08 09:40:28,2,,565,803383056,"Hi!
Thanks a lot for the great work! I am considering using ROAR in my own research but would like to clarify something first. In the paper and also in the README you mention that you use the estimated feature importance, and you write 
> A feature importance estimate is a ranking of the contribution of each input pixel to the model prediction for that image.

From my own experiments with some of the methods, e.g. Integrated Gradients, I know that the attributions can also be negative. I couldn't find any hint how negative attributions are treated. I could imagine a few possible solutions, therefore I was wondering if you could clarify how it is done. Are the contributions ranked by their absolute value, are negative values ignored, are the contributions sorted from large positive to large negative values?

Thanks a lot for your time, I would really appreciate an answer!
Best regards
Verena  Haunschmid

tagging: @sarahooker @doomie"
Question about Meta Pseudo Label’s student update,google-research/google-research,2021-02-06 13:34:18,2,,559,802704038,"Hi, thanks for your sharing of MPL paper and code and I have two question about this paper.
Firstly, during the updating of student on pseudo labeled data, is the student trained for many epochs until convergence or only sample a batch of unlabeled data to update student for one time ?
Secondly, what is the component of teacher’s update gradients in the two moon dataset experiment? Does it contains student feedback, supervised gradient and UDA gradient all together? I tried to test MPL using only supervised loss and students feed back on this toy dataset, but did not get result as good as your paper."
[TFT] Why outputs (labels) are included in inputs (data) ?,google-research/google-research,2021-02-04 07:29:06,0,,557,801013212,"Hi, 
First, I appreciate that your researches and shared codes.

I studied the TFT (Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting, Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister) to solve a time-series forecasting problem. 

The paper is well-written to understand and the codes can be found on this github.

However, (as noticed in this title) I'm very confused because the output (labels, targets) is included in input dict.
there can be static covariates, history of target, etc., as inputs to predict the future target. 
But, I found that the target (output) is included in input data. 

How can it be possible? does it make sense?

Sincerely, "
cFDSD score ,google-research/google-research,2021-02-04 07:17:20,0,,556,801006042,"Hi, good implementation in directory ""google-research/ged_tts""! 
Following usage in README, I got points like 6.158, 4.535 in audio generated by WaveRNN, and Parallel WaveGAN respectively, which are a lot higher than those in the paper ""A Spectral Energy Distance for Parallel Speech Synthesis"", do you know why? Thanks!"
"How to obtain representation from Smith, for some arbitrary doc",google-research/google-research,2021-02-03 13:13:48,3,,554,800317072,"Hi,

I have looked at the code, but it is not exactly clear on how to obtain representation. Is this something to obtain from layers.get_masked_lm_output, after the NN transformation?

In the paper you suggest that you take the first block representation, but it is not exactly clear how you extract the CLS token from the code at the document level 

Many thanks"
Can't train with customed data,google-research/google-research,2021-02-03 03:29:12,1,,552,799931343,"I tried to run the command given in the readme file for training testing split in the dataset. But I am getting the following error. Can you help me with this?

![issue_new](https://user-images.githubusercontent.com/35806906/106694185-3f209600-6602-11eb-97f0-1db20ccb4e4a.png)
"
"Depth and Motion Learning: Auto Mask, Object Motion Net, Regularization",google-research/google-research,2021-02-02 23:14:29,1,,551,799805448,"Hi @uniqueness and @gariel-google ,

Thank you for providing the code for this wonderful work! I have some questions and hopefully you will get back to me.

1. What is the general purpose and significance of this [Auto_mask](https://github.com/google-research/google-research/blob/88d2083a36cc0566585ed4e431534f9e3db0b312/depth_and_motion_learning/object_motion_nets.py#L212)? Why do you compute the mean_sq_residual_translation over all residual translations in the batches instead of independent mean_sq_residual_translation for each residual translation in the batch? If I recall correctly then the 0th dimension should have batch, and this [line](https://github.com/google-research/google-research/blob/88d2083a36cc0566585ed4e431534f9e3db0b312/depth_and_motion_learning/object_motion_nets.py#L215) will take the mean over the entire batch. 

2. I tried training your object motion model on the Kitti data set but the residual translations in some images were not sparse which affected the entire performance of the model. How can I change alpha and beta in the motion regularization to make them more sparse and constant throughout a moving object? 

3. Did you try predicting object motion without using depth information? If so, how were the results? Do you think its possible to extract motion maps without using depth since this method requires us to predict depth for 2 frames?

4. Is your method extendable to an ""n"" frame sequence like in other methods? Did you try 3 frames?

Thanks !"
[uflow] NotImplementerError with numpy==1.20.0,google-research/google-research,2021-02-02 20:48:43,0,,550,799673805,"I am using the following command:
`python -m uflow.uflow_main --train_on=""sintel:/mnt/d/Courses/CTU/Diploma/data/sintel-test-tf/test/final/"" --plot_dir=/mnt/d/Courses/CTU/Diploma/data/results/sintel-plots/ --checkpoint_dir=/mnt/d/Courses/CTU/Diploma/data/results/sintel-checkpoints/`

This gave me the following traceback with the `NonImplementedError`. This was solved by downgrading the `numpy` to 1.19.5. I guess, either the code should be updated to fix the error, or the `requirements.txt` should be updated to include only compatible numpy versions instead of `numpy>=1.18.3`, which installs the incompatible newer version.

```
.Traceback (most recent call last):
  File ""/home/iegorval/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/iegorval/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/iegorval/Diploma/google-research/uflow/uflow_main.py"", line 379, in <module>
    app.run(main)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/iegorval/Diploma/google-research/uflow/uflow_main.py"", line 345, in main
    occ_active=occ_active)
  File ""/home/iegorval/Diploma/google-research/uflow/uflow_net.py"", line 549, in train
    occ_active=occ_active)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 726, in _initialize
    *args, **kwds))
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3887, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:

    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:482 train_step  *
        weights,
    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:453 train_step_no_tf_function  *
        losses, gradients, variables = self._loss_and_grad(
    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:625 _loss_and_grad  *
        losses = self.compute_loss(
    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:668 compute_loss  *
        flows, selfsup_transform_fns = uflow_utils.compute_features_and_flow(
    /home/iegorval/Diploma/google-research/uflow/uflow_utils.py:982 compute_features_and_flow  *
        flow = teacher_flow_model(
    /home/iegorval/Diploma/google-research/uflow/uflow_model.py:209 call  *
        warp_up = uflow_utils.flow_to_warp(flow_up)
    /home/iegorval/Diploma/google-research/uflow/uflow_utils.py:40 flow_to_warp  *
        i_grid, j_grid = tf.meshgrid(
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3552 meshgrid
        mult_fact = ones(shapes, output_dtype)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3120 ones
        output = _constant_if_small(one, shape, dtype, name)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2804 _constant_if_small
        if np.prod(shape) < 1000:
    <__array_function__ internals>:6 prod
        
    /home/iegorval/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3031 prod
        keepdims=keepdims, initial=initial, where=where)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:855 __array__
        "" a NumPy call, which is not supported"".format(self.name))

    NotImplementedError: Cannot convert a symbolic Tensor (pwc_flow/meshgrid/Size:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported

```"
"if no tpu, how can I use the wt5 ",google-research/google-research,2021-02-02 07:32:03,1,,549,799017883,the link is https://github.com/google-research/google-research/tree/master/wt5
[dual_pixels]: does the dual-pixel data capturing APP support Pixel 5?,google-research/google-research,2021-02-01 09:28:17,0,,547,798158082,"For the dual_pixels project. I wonder whether I can use Google Pixel 5 for data capturing. Is there a best model for such data capturing, or any model works equally? Thanks a lot."
[Scann] Does example.py uses loss function from the paper?,google-research/google-research,2021-02-01 07:17:07,7,,546,798047870,"I'm following the code flow by running example.py and defining search as below,
`searcher = scann.ScannBuilder(normalized_dataset, 10, ""dot_product"").tree(
    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(2, anisotropic_quantization_threshold=0.2).reorder(100).create_pybind()`

I was expecting to see the part where it trains codewords with the loss function that was proposed in the paper, but I only see training codewords with GenericKmeans (original kmeans-clustering algorithm). 

Does the example.py uses loss function from the paper? If not, how can I change the searcher definition to make it use the loss function from the paper?"
what about flutter,google-research/google-research,2021-01-27 18:43:10,0,,543,795350787,
[SCANN] scann tensorflow2.3 is cpu or gpu?,google-research/google-research,2021-01-26 07:04:09,3,,542,793990282,"hi, scann compile from source, the tensorflow must be gpu version, or cpu version is ok?"
[SCANN] Any plans for C++ API?,google-research/google-research,2021-01-21 21:05:35,3,,538,791492793,"Hi everyone.
Are there any plans to add C or C++ API to SCANN? Or maybe would you consider an external PR implementing it?"
[T5X] RuntimeError with BF16 and multiple GPUs,google-research/google-research,2021-01-20 18:45:35,0,,535,790226111,"Hi there,

I really enjoyed the recent release of **T5X**.
While having no problems running it on Colab (standard notebook with one GPU), I am facing some difficulties in running the code on one of my machines. Specifically, my configuration is Ubuntu 18.04, CUDA 11, CuDNN 7, with **two GPUs**  (TITAN Xp, 12GB each).
The GPUs are recognized by JAX, and the code runs correctly up to the `pmap` related to `p_train_epoch`.

The error is the following:
`RuntimeError: Unimplemented: Requested AllReduce not implemented on GPU; replica_count: 2; operand_count: 131; IsCrossReplicaAllReduce: 1; NCCL support: 1; first operand array element-type: BF16`

By working with float32 the training is performed successfully. It appears an error related to using **jnp.float16** on **multi-GPUs**. Can you help me?

Here the complete Traceback:
```
Traceback (most recent call last):
  File ""ft_t5_small_super_glue.py"", line 84, in <module>
    train(model_dir='t5x_data', config=fine_tuning_cfg)
  File ""/workspace/linear_t5x/src/t5x_utils/training.py"", line 525, in train
    jnp.array(0, dtype=jnp.int32), 1)
  File ""/opt/conda/lib/python3.6/site-packages/jax/api.py"", line 1564, in f_pmapped
    global_arg_shapes=tuple(global_arg_shapes_flat))
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 1262, in bind
    return call_bind(self, fun, *args, **params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 1226, in call_bind
    outs = primitive.process(top_trace, fun, tracers, params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 1265, in process
    return trace.process_map(self, fun, tracers, params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 598, in process_call
    return primitive.impl(f, *tracers, **params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/interpreters/pxla.py"", line 635, in xla_pmap_impl
    *abstract_args)
  File ""/opt/conda/lib/python3.6/site-packages/jax/linear_util.py"", line 251, in memoized_fun
    ans = call(fun, *args)
  File ""/opt/conda/lib/python3.6/site-packages/jax/interpreters/pxla.py"", line 892, in parallel_callable
    compiled = xla.backend_compile(backend, built, compile_options)
  File ""/opt/conda/lib/python3.6/site-packages/jax/interpreters/xla.py"", line 349, in backend_compile
    return backend.compile(built_c, compile_options=options)
```
"
Question about Meta Pseudo Labels gradient signal,google-research/google-research,2021-01-20 08:00:29,32,,534,789736483,"Hi there,

I really enjoyed reading the paper by @hyhieu et al.! I don't have the resources to train the models, or even run the code, but I do have some questions about the code. Especially about the loss of the teacher model. If I'm understanding correctly, the final loss is defined here: 

https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L494-L496

I have a question about the MPL term. `cross_entropy['mpl'] * dot_product`.

`dot_product` seems to be a scalar (like mentioned in the paper), without gradient computation:

https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L483

`cross_entropy['mpl']` seems to be defined a few lines above:
https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487

This seems to be a cross-entropy where the logits and targets are the same (since `softmax_cross_entropy` also applies a softmax to the logits). This loss can be non-zero when the targets are not 'hard' (soft targets), however, I don't think there is any signal in that case? Since the logits and targets seem to be equal. 

My question is — if I'm understanding the code correctly — if there is no signal and it's scaled by the `dot_product`, I do not know what the value is of this term in the optimization?

Thanks again for your work!
Hans

P.S. small side question: in the paper, it is mentioned the method uses hard-targets, but I cannot find this in the code.

"
ScaNN (Anisotropic product quantization),google-research/google-research,2021-01-19 14:49:49,0,,533,789088031,"Hello developers, 
I would like to know which is the space partitioning algorithm used in ScaNN to reduce the number of instances to evaluate. I read in the docs that is a tree-based algorithm. Is it possible to have some details about it? Maybe a reference to a paper. 
"
Complete cad models of keypose,google-research/google-research,2021-01-19 06:36:10,0,,530,788749737,"There are only simplified vertex CAD files in keypose.
Is there a way to download complete cad models of keypose?"
"FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite",google-research/google-research,2021-01-14 07:10:20,2,,526,785728719,"Hi,

I tried to train a kws_streaming ""**ds_tc_resnet**"" model on my own data, but when the training is finished, I got the Error,



    W0114 14:36:01.360302 140229770733312 test.py:675] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: in user code:
    /homecode/2021/google-research/kws_streaming/layers/speech_features.py:237 call  *
        outputs = self._mfcc_op(outputs)
    /home/code/2021/google-research/kws_streaming/layers/speech_features.py:195 _mfcc_op  *
        outputs = self.data_frame(inputs)
    /home/code/2021/google-research/kws_streaming/layers/data_frame.py:114 call  *
        output, self.output_state = self._streaming_external_state(
    /home/code/2021/google-research/kws_streaming/layers/data_frame.py:201 _streaming_external_state  *
        raise ValueError('inputs.shape[1]:%d must be = self.frame_step:%d' %

    ValueError: inputs.shape[1]:320 must be = self.frame_step:160

Here is my training cmd,

CUDA_VISIBLE_DEVICES=6 python -m kws_streaming.train.model_train_eval --data_url '' --data_dir /data/audio_data/HiCar_data/RecordedSLU4TCResNet/ --batch_size 100 --train_dir ./models/RecordedSLU4StreamingDSTCResNetHiCarNoise/ --wanted_words my_wanted_words --clip_duration_ms 3000  --split_data 0  --mel_upper_edge_hertz 7600  --how_many_training_steps 40000,40000,20000,20000   --learning_rate 0.001,0.0005,0.0002,0.0001 --window_size_ms 30.0  --window_stride_ms 10.0  --mel_num_bins 60  --dct_num_features 40  --resample 0.15 --alsologtostderr  --train 1 --feature_type 'mfcc_op'  --fft_magnitude_squared 1  --use_spec_augment 1 --time_masks_number 2 --time_mask_max_size 25 --frequency_masks_number 2 --frequency_mask_max_size 7 --pick_deterministically 1   ds_tc_resnet --activation 'relu'  --dropout 0.0  --ds_filters '128,64,64,64,128,128'  --ds_repeat '1,1,1,1,1,1'  --ds_residual '0,1,1,1,0,0' --ds_kernel_size '11,13,15,17,29,1' --ds_stride '1,1,1,1,1,1' --ds_dilation '1,1,1,1,2,1'

Did I set something wrong?
Thank you."
How to use 'max_to_keep' to limit the saved checkpoints?,google-research/google-research,2021-01-14 02:03:49,0,,525,785589778,"Hi,
I tried to train the model and notice that ""model.save_weights"" in keras does not have the parameter max_to_keep parameter.
Should I delete some checkpoints by myself or could you please give some tips?
Thank you."
" When will you release the code of  ""google-research/etcsum"" ?",google-research/google-research,2021-01-12 13:19:47,0,,523,784227896,Thank you
a4@gmail.com,google-research/google-research,2021-01-11 23:05:42,0,,522,783770562,
GoEmotions religion and name masking resources,google-research/google-research,2021-01-11 14:10:50,6,,521,783402621,"Hello @ddemszky  @dattias and @eladeban,
I'm trying to apply your model to some text that is not part of the GoEmotions dataset.
To do so I need to mask the input text in the same way you do, that is replacing religions and names with the [RELIGION] and [NAME] tokens.
In the paper I read that the list of religion terms is included with the dataset, but I can't find it, maybe I'm looking in the wrong places?
Also, I'm looking for the BERT-based NER model you used for name tagging, do you have a link to the model by any chance?
Thank you very much!"
xa@gmail.com,google-research/google-research,2021-01-11 06:04:20,0,,518,783091763,
[TRILL] No Gradient error when training TRILL network,google-research/google-research,2021-01-08 02:42:27,1,,517,781798213,"Hi, thank you for your great work. I'm trying to fine-tune TRILL (https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark) but got the bug bellow:

```
LookupError: No gradient defined for operation 'map/while' (op type: StatelessWhile)
```

Here is a code to reproduce this bug:

```
import tensorflow as tf
import tensorflow_hub as hub


model = tf.keras.models.Sequential()
model.add(tf.keras.Input((128000,)))  # Input is [bs, input_length]
trill_layer = hub.KerasLayer(
    handle='https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3',
    trainable=True,
    arguments={'sample_rate': tf.constant(16000, tf.int32)},
    output_key='embedding',
    output_shape=[None, 2048]
)
model.add(trill_layer)

opt = tf.keras.optimizers.Adam(lr=0.001)

model.summary()

@tf.function
def train_step():
    with tf.GradientTape() as tape:
        logits = model(
            tf.random.uniform(shape=[1, 128000], dtype=tf.float32), 
            training=True
        )
        loss = tf.reduce_mean(logits)
    grads = tape.gradient(loss, model.trainable_variables)
    opt.apply_gradients(zip(grads, model.trainable_variables))
    tf.print(loss)


for _ in range(1000):
    train_step()
```

@joel-shor can you take a look? "
[uflow] error when evaluating model on sintel,google-research/google-research,2021-01-07 09:53:15,6,,515,781182909,"Hi, I've used the code to train model on sintel dataset. The training is not completed and I try to do evaluation on the mediate models. I use the script: 
```
python3 -m uflow.uflow_evaluator --eval_on=""sintel:uflow/data_tfrecords/sintel/test/clean"" --plot_dir=uflow/plot/sinte    l --checkpoint_dir=uflow/checkpoints/sintel
```
then I get the following error:
```
I0107 17:50:51.010394 140387742058304 uflow_evaluator.py:61] New checkpoint found: uflow/checkpoints/sintel/ckpt-526
2021-01-07 17:50:51.090070: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at example_parsing_ops.cc:476 : Invalid argument: I
nconsistent number of elements for feature flow_uv: 0 vs 1
2021-01-07 17:50:51.090382: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at iterator_ops.cc:941 : Invalid argument: Inconsis
tent number of elements for feature flow_uv: 0 vs 1
         [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
2021-01-07 17:50:51.092987: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at example_parsing_ops.cc:476 : Invalid argument: I
nconsistent number of elements for feature flow_uv: 0 vs 1
Traceback (most recent call last):
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py"", line 1897, in execution_mo
de
    yield
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 659, in _next
_internal
    output_shapes=self._flat_output_shapes)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2479, in iterat
or_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_n
ot_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent number of elements for feature flow_uv: 0 vs 1
         [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/google-research-master/uflow/uflow_evaluator.py"", line 98, in <module>
    app.run(main)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/google-research-master/uflow/uflow_evaluator.py"", line 92, in main
    evaluate()
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/google-research-master/uflow/uflow_evaluator.py"", line 66, in evaluate
    eval_results = evaluate_fn(uflow)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_data.py"", line 277, in eval_function
    width, progress_bar, plot_dir, num_plots)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/data/data_utils.py"", line 222, in evaluate
    for test_batch in it:
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 630, in __nex
t__
    return self.next()
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 665, in _next
_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mo
de
    executor_new.wait()
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent number of elements for feature flow_uv: 0 vs 1
         [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]

If you suspect this is an IPython 7.12.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with ""%tb"", or use ""%debug""
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

```
I think it's strange, because when i do evaluation on Chairs, it works well.  Could you help to fix this problem?
"
[uflow] error when training on KITTI15,google-research/google-research,2021-01-07 09:45:01,3,,514,781177580,"Hi, I found some small mistakes in `data_conversion_scripts/convert_KITTI_flow_to_tfrecords.py`, for example:

> line 71: flow_path = os.path.join(data_dir, FLAGS.subdir, 'flow_' + version, 

After fixing generating the KITTI15 tfrecords data, I tried to do training. In training,  I got the following error:
```
Restoring model from checkpoint uflow/checkpoints/kitti15.
Making eval datasets and eval functions.
Making training iterator.
Starting training loop.

Traceback (most recent call last):
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_main.py"", line 379, in <module>
    app.run(main)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_main.py"", line 357, in main
    uflow_plotting.print_log(log, epoch)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_plotting.py"", line 46, in print_log
    np.mean(log['total-loss'][-mean_over_num_steps:]))
KeyError: 'total-loss'

```
The train data loader **train_it** is empty and the training loop wasn't executed. Could you help to fix?"
Uflow Train on custom dataset,google-research/google-research,2021-01-05 10:51:00,9,,512,778864624,"Thank you for this awesome work. 

When trying to train the UFlow network on the billiard dataset, I had the following error message : Unknown data format ""custom"". 
After looking at the uflow_data.py file I found no support for 'custom' dataset format on the make_train_iterator function. 

Is there a way to avoid this error ?

Finally, kindly share any tips for training Uflow on an unsupervised dataset other than 'Flying chairs', 'Sintel', 'Kitti'.

Best regards,
Mouad Lazrak.
"
fast-attention: train well but predict badly,google-research/google-research,2020-12-29 10:21:31,7,,508,775815636,"1. I use the transformer model in models/official/nlp/transformer/transformer.py to train a seq2seq model, replacing the built-in keras attention with performer fast-attention(tensorflow version),  for example, the decoder stack is as follows
![image](https://user-images.githubusercontent.com/7539692/103272848-d76e9e00-49f8-11eb-9594-b0f57114e67a.png)
> It seams images can't be displayed: 
```
    # DecoderStack
    def build(self, input_shape):
        """"""Builds the decoder stack.""""""
        params = self.params
        for _ in range(params[""num_hidden_layers""]):
            if not params[""use_fast_attn""]:  # ==========set to True to use fast_attention=============
                self_attention_layer = attention_layer.SelfAttention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""])
                enc_dec_attention_layer = attention_layer.Attention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""])
            else:
                self_attention_layer = fast_attention.SelfAttention(  # ==========use fast_attention=============
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""],
                    kernel_transformation=fast_attention.softmax_kernel_transformation,
                    causal=True,
                    projection_matrix_type=1,
                    nb_random_features=nb_random_features,  # =============100(model hidden size is 512)============
                )
                enc_dec_attention_layer = fast_attention.Attention(  # ==========use fast_attention=============
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""],
                    kernel_transformation=fast_attention.softmax_kernel_transformation,
                    causal=False,
                    projection_matrix_type=1,
                    nb_random_features=nb_random_features,
                )
            feed_forward_network = ffn_layer.FeedForwardNetwork(
                params[""hidden_size""], params[""filter_size""], params[""relu_dropout""])

            self.layers.append([
                PrePostProcessingWrapper(self_attention_layer, params),  # x->LNorm(x)->{layer(x)=y}->Dropout(y)->y+x
                PrePostProcessingWrapper(enc_dec_attention_layer, params),
                PrePostProcessingWrapper(feed_forward_network, params)
            ])
        self.output_normalization = tf.keras.layers.LayerNormalization(
            epsilon=1e-6, dtype=""float32"")
        super(DecoderStack, self).build(input_shape)
```

2. The training stage is perfect, with good decoder outputs almost the same as the targets. An example is, if the target is **""He is playing basktball in the basketball gym""**, the output could be the same.

3. But when doing inference, the decoder outputs are very bad. The output for the above example now is **""He is playing is playing is playing is playing is playing </s>""**, even more worse with **""He He He He He He He He He He He He </s>""**.

4. I just change the mode from ""train"" to ""predict"", and use beam search. Whether setting beam_size=1 or not, the output is the same and bad.

5. When doing inference, I check the **decoder attention output(layer0)** in ""fast_attention.py>Attention>call()"", the states between beams of an sample are almost the same in step 2. 
![image](https://user-images.githubusercontent.com/7539692/103276792-7b107c00-4a02-11eb-8604-f9f72ae47bee.png)
![image](https://user-images.githubusercontent.com/7539692/103276588-ec9bfa80-4a01-11eb-9c57-c4edb4565d63.png)

> It seams images can't be displayed: 
```      
        if cache is not None:
            # Combine cached keys and values with new keys and values.
            if decode_loop_step is not None:
                cache_k_shape = cache[""k""].shape.as_list()
                indices = tf.reshape(
                    tf.one_hot(decode_loop_step, cache_k_shape[1], dtype=key.dtype),
                    [1, cache_k_shape[1], 1, 1])
                key = cache[""k""] + key * indices
                cache_v_shape = cache[""v""].shape.as_list()
                indices = tf.reshape(
                    tf.one_hot(decode_loop_step, cache_v_shape[1], dtype=value.dtype),
                    [1, cache_v_shape[1], 1, 1])
                value = cache[""v""] + value * indices
        else:
            key = tf.concat([tf.cast(cache[""k""], key.dtype), key], axis=1)
            value = tf.concat([tf.cast(cache[""v""], value.dtype), value], axis=1)
    
            # Update cache
            cache[""k""] = key
            cache[""v""] = value

        attention_output = favor_attention(query, key, value,
                                           self.kernel_transformation, self.causal,
                                           bias, projection_matrix)
        
        # the attention_output is: 
        attention_output = tf.Tensor(
        [[-9.661879   2.9591107  1.3627272 ... -5.7556434 -4.247857   5.5499997]
         [-9.661878   2.9591098  1.3627269 ... -5.7556424 -4.247855   5.55     ]
         [-9.661879   2.9591098  1.3627272 ... -5.755643  -4.247857   5.5499997]
         [-9.661878   2.9591103  1.3627266 ... -5.755642  -4.2478557  5.549999 ]
         [-9.661879   2.9591098  1.3627272 ... -5.7556434 -4.247856   5.5499997]], shape=(5, 512), dtype=float32)
```

6. Using the raw code and the built-in keras attention, everying is good."
Error building on Mac,google-research/google-research,2020-12-27 21:52:57,2,,507,775114698,"After running `configure.py` I try to build with Bazel, but I'm getting the following error:

    $ CC=clang bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
    INFO: Build option --action_env has changed, discarding analysis cache.
    WARNING: /private/var/tmp/_bazel_thdy/2bcb61de71a5e80e9c149676ae8deec4/external/local_config_tf/BUILD:5430:8: target 'ensorflow_framework.2' is both a rule and a file; please choose another name for the rule
    ERROR: /private/var/tmp/_bazel_thdy/2bcb61de71a5e80e9c149676ae8deec4/external/local_config_tf/BUILD:11:11: in srcs attribute of cc_library rule @local_config_tf//:libtensorflow_framework: '@local_config_tf//:ensorflow_framework.2' does not produce any cc_library srcs files (expected .cc, .cpp, .cxx, .c++, .C, .cu, .cl, .c, .h, .hh, .hpp, .ipp, .hxx, .h++, .inc, .inl, .tlh, .tli, .H, .tcc, .S, .s, .asm, .a, .lib, .pic.a, .lo, .lo.lib, .pic.lo, .so, .dylib, .dll, .o, .obj or .pic.o)
    ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis of target '@local_config_tf//:libtensorflow_framework' failed
    INFO: Elapsed time: 0,692s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (5 packages loaded, 319 targets configured)
        Fetching ...ogle_protobuf; Cloning tags/v3.9.2 of https://github.com/protocolbuffers/protobuf.git

This is on

    $ uname -a
    Darwin mac611211 19.6.0 Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64 x86_64"
WMT2014 train dataset size not mentioned in the paper for bertseq2seq/bert24_en_de ,google-research/google-research,2020-12-23 15:52:54,0,,504,773897677,"https://github.com/google-research/google-research/tree/master/bertseq2seq
In the above repo, can you tell on how many WMT2014 en-> de sentences the bertseq2seq/bert24_en_de was trained?"
Fast Attention TF: Unit test unstable?,google-research/google-research,2020-12-23 10:01:59,11,,503,773635927,"When executing `test_softmax_noncausal_attention_block_output` in `performer/fast_attention/tensorflow/fast_attention_test.py`
it produces a maximum error of 0.20 which is clearly below the max allowed error `max_error = 2.0`.
However, when changing `length` to a different value. The maximum relative error jumps up significantly.
The commit I'm running on is: 6f29c8099fbb1474769046276c8744521470cab9
I run on TF 1.15.
Here is a table of length vs error and unit test state:

length | relative error | unit test status
------|-----|------
2 | 0.20 | Pass
4 | 1.24 | Pass
8 | 106 | FAIL
10 | 9.7 | FAIL

Ping @xingyousong
"
Question about NSP loss of MobileBERT pretraining,google-research/google-research,2020-12-23 06:02:55,0,,501,773510712,"Hello, firstly thanks for sharing code and model. I notice ""classifier_activation"" is set to false in model config, which means the output logit of NSP won't be normalized using ""tanh"" and value will be quite large, like [3e6, 3e6]. Since I think MLM logit is normalized through the final layer normalization layer and won't be very large, is it correct to combine the small MLM loss with a huge NSP loss? or I miss some processing steps in the code?
Hope you can explain that for me, and please tell me the final loss of MLM and NSP. Thank you!"
absl.flags._exceptions.IllegalFlagValueError,google-research/google-research,2020-12-22 10:04:32,0,,500,772816095,"I got absl.flags._exceptions.IllegalFlagValueError: flag --output_dir=None: Flag --output_dir must have a value other than None.
so i put dir address.

```
`2020-12-22 18:51:53.991739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --data_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --vocab_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --bert_config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:975: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 550, in _assert_validators
    validator.verify(self)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py"", line 82, in verify
    raise _exceptions.ValidationError(self.message)
absl.flags._exceptions.ValidationError: Flag --output_dir must have a value other than None.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\google-research\goemotions\bert_classifier.py"", line 975, in <module>
    tf.app.run()
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 297, in run
    flags_parser,
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 366, in _run_init
    flags_parser=flags_parser,
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 213, in _register_and_parse_flags_with_usage
    args_to_main = flags_parser(original_argv)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\app.py"", line 31, in _parse_flags_tolerate_undef
    return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 658, in __call__
    self.validate_all_flags()
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 532, in validate_all_flags
    self._assert_validators(all_validators)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 553, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --output_dir=None: Flag --output_dir must have a value other than None.`
```



AND  after that, I got this error below... what should i do.

```
`2020-12-22 19:00:07.287395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --data_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --vocab_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --bert_config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --output_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:975: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

28 labels
Multilabel: False
Getting distance matrix...
[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W1222 19:00:09.434083  5656 module_wrapper.py:139] From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W1222 19:00:09.453032  5656 module_wrapper.py:139] From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From D:\google-research\goemotions\bert\modeling.py:113: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

W1222 19:00:09.467995  5656 module_wrapper.py:139] From D:\google-research\goemotions\bert\modeling.py:113: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

Windows fatal exception: access violation

Current thread 0x00001618 (most recent call first):
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 84 in _preread_check
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 122 in read
  File ""D:\google-research\goemotions\bert\modeling.py"", line 114 in from_json_file
  File ""D:\google-research\goemotions\bert_classifier.py"", line 773 in main
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 303 in run
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40 in run
  File ""D:\google-research\goemotions\bert_classifier.py"", line 975 in <module>
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 85 in _run_code
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 193 in _run_module_as_main`
```"
[ES-MAML] There is some missing parts in the code.,google-research/google-research,2020-12-21 08:28:18,5,,499,771968915,"Dear @xingyousong 


I tried to run your ES-MAML code, but I failed.

In declaration phase, `blackbox_maml_objects.py` does import like below.

```
from es_maml.first_order import first_order_pb2
from es_maml.first_order import first_order_pb2_grpc

from es_maml.zero_order import zero_order_pb2
from es_maml.zero_order import zero_order_pb2_grpc
```

However, there is no `first_order_pb2`, `first_order_pb2_grpc`, `zero_order_pb2` and `zero_order_pb2_grpc` in `es_maml/first_order` and `es_maml/zero_order` directories.

As result, an error occurred like below.
`ImportError: cannot import name 'first_order_pb2' from 'es_maml.first_order' (unknown location)`

Could you upload those things to provide workable ES_MAML code?

If I misunderstood the way to run your code, please give me any advise.

Thank you in advance."
The problem of MolDQN running out of memory,google-research/google-research,2020-12-21 02:10:03,0,,498,771781482,"Hello,
When I run MolDQN at about 3500episode, I found that the memory of the server has been exhausted and the program can not run. What is the reason for this? Please tell me. Thank you very much. Best wishes.

Anny
2020.12.21"
[Closed Book QA w/ T5] Decoding algorithm,google-research/google-research,2020-12-16 10:55:22,1,,497,768715332,"Thanks again for releasing all the Closed Boook QA T5 checkpoints. The models are now fully ported to hugging face and the < xl/3b models can also be used on the inference API: https://huggingface.co/google/t5-large-ssm-nq?text=how+many+episodes+in+season+2+breaking+bad%3F

At the moment we use simple greedy search as the decoding algorithm for those models, but we could switch to beam search if it gives better performance. What is your opinion here? In case you found that beam search works better, could you post your suggested hyperparameters (max_length/min_length/num_beams/length_penalty). Thanks a lot!

Hope it's ok to tag you here @adarob :-) 
"
Is it possible that you provide the pretrained teacher model IB-BERT for MobileBERT?,google-research/google-research,2020-12-16 03:29:55,3,,496,768416185,"Hi, thanks for this wonderful project. Are there any pretrained weights for IB-BERT(large) available for download, or planned to be?"
Can you creat clever business platform?,google-research/google-research,2020-12-12 14:21:08,0,,492,763883424,
[uflow] pretrained weights available?,google-research/google-research,2020-12-09 10:09:30,3,,490,760190539,"Hi,
Thanks for this nice project!
Are there any pretrained weights available for download, or planned to be?
"
Correlation_clustering algorithm,google-research/google-research,2020-12-08 19:39:24,0,,488,759724096,"I've followed all the instructions mentioned in this code repository, but when I want to run this algorithm on my dataset, I get the following error: 

IllegalFlagValueError: flag --input_graph=None: Flag --input_graph must have a value other than None.

Can someone help?

Many thanks in advance. "
[POEM] Where is the structure of H5 files?,google-research/google-research,2020-12-06 13:34:37,1,,486,757927459,"Hello!
In the ""gen_train_tfrecords.py""， the h5 files be note as
 ""
The instructions
for downloading the H5 files are located in the Github page:
https://github.com/una-dinosauria/3d-pose-baseline.
""

But I can't find the structure of  the h5 files in this page. Can you give me some examples of the h5 files?
Looking forward to your advice."
[widget_caption] Any plan to release widget caption dataset?,google-research/google-research,2020-12-03 04:12:48,2,,483,755803745,Hello. I’m very interested in the paper “Widget Captioning: Generating Natural Language Description for MobileUser Interface Elements” published in EMNLP 2020. I went to the url provided in the paper for dataset but got nothing. Any plan to release widget captioning dataset? Thanks!
[scann] anisotropic product quantization,google-research/google-research,2020-12-02 16:26:23,0,,482,755422350,"Is anisotropic product quantization (section 4.1 in [paper](https://arxiv.org/pdf/1908.10396.pdf)) implemented in scann library?
If so, it would be great if you show how to do it in code.

Thanks!"
tf3d,google-research/google-research,2020-11-30 05:39:15,12,,477,753163248,"Can you provide pretrained model on waymo dataset and some guides for  running the inference code? Although there is README file for using the sparse conv, i still stuck in runing the inference code. I want to reproduce the  reported 12ms inference time @HRLTY "
[depth_and_motion_learning] inference,google-research/google-research,2020-11-27 13:59:25,13,,474,752273262,"Hi all,

 I'm checking out the new unsupervised depth model. I can see that there is no separate inference script. Which is the proposed way to run prediction?
 - Should the depth_motion_field_model.py (function infer_depth(rgb_image, params) ) be applied and somehow  the input batch passed to it? Normalized?
 - Like at the depth_from_videos_in_the_wild,  one should use and slightly modify the inference script from struct2depth project? (signiture of infer_depth is now quite different as it was earlier with inference_depth(im_batch, sess) ).

BEst Regards
 György "
depth_from_video_in_the_wild: EuRoC groundtruth seems bad,google-research/google-research,2020-11-25 22:01:13,0,,472,751142181,"Hi, thanks for sharing your code for EuRoC depth map groundtruth generation.

I tested it and it seems that the image is not exactly aligned with generated groundtruth.

It can even be seen in the paper at fig. A1, here is visualization of image + depthmap :
![problem_depth](https://user-images.githubusercontent.com/4380424/100263180-75c89900-2f4d-11eb-96bc-5c0606641704.png)
![problem_depth_zoom](https://user-images.githubusercontent.com/4380424/100263205-7e20d400-2f4d-11eb-9ca2-dd0aa0d39ba8.png)

You can see on the zoom that background pixels are actually thought to be foreground.

You could say that this is very subtle, but the de-alignment change from video to video, when trying it on first video, V1_01_easy, the difference is actually huge :

![1403715283262142976mg](https://user-images.githubusercontent.com/4380424/100272803-07d79e00-2f5c-11eb-8d83-2735c99fbf67.png)

You can see how the cupboard is much more on the left on the depth map than on the image. Also, notice the tatami on the far left which is very dephased.

Is this an implementation problem ?
I tried to check the code and it seems good to me. Does it mean that the Odometry of EuRoC, at least for these two scenes is not perfect ? It seems to me that the position is good, but not the orientation.

To check if it is only a problem of camera extrinsics, which would explain the good position but bad orientation, I converted the EuRoC to Colmap : https://gist.github.com/ClementPinard/662c6642c4be37c5030fedaf5ad5414c

And then I used the point triangulator : https://colmap.github.io/faq.html#reconstruct-sparse-dense-model-from-known-camera-poses to know if there was a rigid transformation offset.

It turns out that the transformation is not constant, otherwise, we wouldn't get this reconstruction : 

![colma_problem2](https://user-images.githubusercontent.com/4380424/100285619-7a06ad80-2f71-11eb-8f6d-4f940211e352.png)


Here is one reconstruction when we map everything from scratch without using groundtruth pose, and you can see that the model is much sharper :

![colma_no_problem](https://user-images.githubusercontent.com/4380424/100285435-2f853100-2f71-11eb-84a7-344e9cd36e32.png)


What are your thoughts on this problem ? Thanks is advance for your help

I do concede that the subject is very broad, but criticizing EuRoC odometry is a very strong claim so I prefer a second opinion :)"
.,google-research/google-research,2020-11-25 09:00:20,0,,471,750626535,
M-DQN and M-IQN log-policy clipping differs,google-research/google-research,2020-11-21 21:58:31,0,,467,748110679,"Hello. Thank you very much for this project and your awesome work on it!

I've been reading the implementation of the M-DQN and M-IQN and found that the log-policy clipping is applied differently.
In the original paper authors clip this member between [lo, 0], where lo is hyperparameter.
In M-IQN it [looks correct](https://github.com/google-research/google-research/blob/master/munchausen_rl/agents/m_iqn.py#L310), but in the M-DQN the value is [clipped in [lo, 1]](https://github.com/google-research/google-research/blob/master/munchausen_rl/agents/m_dqn.py#L156).
Probably I'm missing something, so if this is not a bug, could you please give some details about why you use 1 as a max clipping value?

Thanks in advance!"
NNGP TensorFlow Error,google-research/google-research,2020-11-20 11:32:56,0,,466,747417635,"For NNGP-guided Neural Architecture Search at https://github.com/google-research/google-research/tree/master/nngp_nas, I get an error when I try to run your Google Colab notebook for it at https://colab.research.google.com/github/google-research/google-research/blob/master/nngp_nas/NNGP_on_NASBench101.ipynb 👍 
---->  import tensorflow_datasets as tfds
ImportError: This version of TensorFlow Datasets requires TensorFlow version >= 2.1.0; Detected an installation of version 1.15.2. Please upgrade TensorFlow to proceed.
"
question of fig.4 in the paper of performer.,google-research/google-research,2020-11-17 16:39:54,5,,465,744899155,"In the paper of performer, we know ORFs require m <= d, but the description of fig.4 is ""L=4096 and d=16"" and x axis (m) varies from 0 to 200. Is there any typo?"
ScaNN out of core?,google-research/google-research,2020-11-16 18:36:05,0,,464,744068066,"I could not tell from the paper or repo, if and how ScaNN can handle larger-than-core datasets.  Any comment?"
ENAS_LM,google-research/google-research,2020-11-13 12:45:46,0,,462,742424566,"Dears,
In ENAS_LM, may I ask which part exactly in the code the controller trains all the children (explicitly), and how to know how many children do you train in one epoch?"
Missing FEELVOS Repo,google-research/google-research,2020-11-13 11:29:18,0,,461,742382467,"Missing [FEELVOS](https://arxiv.org/pdf/1902.09513.pdf) repo, Kindly add the repo for this paper and open source it's code."
What if my own dataset have missing values at some time steps for TFT model?,google-research/google-research,2020-11-13 05:51:51,0,,460,742169329,"I want to apply TFT model to my own dataset, but there are missing values at some time steps. My question is whether the model can handle missing values? Do I need to preprocess my data to keep the time series continuous?"
Question: How Effectively Does TFT Deal with Nonstationarity?,google-research/google-research,2020-11-12 16:47:22,0,,459,741757318,"After reading through your paper and applying TFT to model COVID-19 trends across US states, I am still unclear about how effectively TFT deals with nonstationarity. My trained model appears to be doing as well as can be expected, given how aberrant the pandemic trends have been in the US; however, I'm wondering if TFT necessitates stationarity transformations in this case or if the sequential attention is able to learn to deal with this on its own fairly effectively (given that obviously the more feature engineering that is done can help improve model performance but then again part of the appeal of TFT is that one can avoid much of that work through the model).

Thanks for your thoughts. Much appreciated!"
[POEM] Code of Evaluation of the model,google-research/google-research,2020-11-10 11:06:46,35,,457,739812610,"Hi team,

Thanks a lot for the providing it publicly.

It would be great if you can provide code for evaluating the trained model and it's different applications as mentioned in the paper.

Thanks in advance for your continuous support .

Harish Naidu Chinnam "
Access to TFRC,google-research/google-research,2020-11-08 18:17:45,0,,456,738523501,"Hello all Dears,

How can we access free tpu via TFRC for pre-training BERT language Model on a specific language?

the link below explains that we have to sign up here: https://services.google.com/fb/forms/tpusignup/ , but it seems that it's a dead URL.
https://ai.googleblog.com/2017/05/introducing-tensorflow-research-cloud.html

Nothing happened By Apply now at https://www.tensorflow.org/tfrc"
Adding zeroes?,google-research/google-research,2020-11-07 22:13:16,0,,455,738326929,"https://github.com/google-research/google-research/blob/fa685638bd1d873a2f58d485655ae4edadca1426/performer/fast_self_attention/fast_self_attention.py#L84

What is the reasoning behind adding zeros here? It doesn't make much sense to me, but then again I haven't really looked much into jax. Also the data_mod_shape doesn't match the projection_matrix shape, so how would that work?

Thx in advance :) "
Ising model simulation on TPU with TF 2.x,google-research/google-research,2020-11-07 13:49:21,0,,454,738246178,"As somebody with more of a physics background than ML, the implementation [here](https://github.com/google-research/google-research/tree/master/simulation_research/ising_model) for Ising MCMC simulation on TPU is too cumbersome/lengthy with Tensorflow 1.x. Will be a lot more helpful for the physics community if somebody can rewrite with 2.x..."
[MentorMix] ask for controlled noisy web labels dataset image files,google-research/google-research,2020-11-04 06:42:38,2,,450,735856079,"Thanks for your interesting works!

It is too difficult to download the whole dataset from every single image url. Can you share me the image file?

thanks a lot!  "
SCANN - Sparse matrix,google-research/google-research,2020-11-02 01:17:54,2,,448,734112675,"Hi, does SCANN support sparse matrix? If yes, where can I find instruction for it? Thank you."
RL,google-research/google-research,2020-10-29 18:52:24,0,,443,732571025,
AudioSet COLA model release?,google-research/google-research,2020-10-29 16:55:52,0,,442,732488963,"Hi, I'm just wondering if there are plans to release the COLA model pre-trained on AudioSet. Seems like it could be useful for a lot of downstream tasks and fine-tuning. Thanks!"
【SCANN】,google-research/google-research,2020-10-29 09:31:31,2,,441,732151023,"if  I use clang-8,it works,but I have to use gcc10 to build scann and there is a error. Anyone knows how to solve. it likes the different rule between clang and gcc.

command：
CC=gcc bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg

--->
ERROR: /home/xuyao/src/google-research/scann/scann/distance_measures/one_to_one/BUILD.bazel:146:11: C++ compilation of rule '//scann/distance_measures/one_to_one:dot_product_sse4' failed (Exit 1): gcc failed: error executing command /home/xuyao/gcc9.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 58 argument(s) skipped)
scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
scann/distance_measures/one_to_one/dot_product_sse4.cc:30:62: error: expected '{' before '->' token
   30 |   auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
      |                                                              ^~
scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductByteImpl(const Byte*, const Byte*, size_t)':
scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected primary-expression before '{' token
   30 |   auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
      |                                                                          ^
scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected ',' or ';' before '{' token
scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
scann/distance_measures/one_to_one/dot_product_sse4.cc:169:64: error: expected '{' before '->' token
  169 |   auto as_m128i = [](const int8_t* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
      |                                                                ^~
scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductSse4(const tensorflow::scann_ops::DatapointPtr<signed char>&, const tensorflow::scann_ops::DatapointPtr<float>&)':

"
Share pre-train encoder model in Contrastive Learning of General-Purpose Audio Representations(COLA),google-research/google-research,2020-10-27 09:28:58,1,,437,730272871,"Can we share the pre-train encoder model so that we don't need to re-train it from scratch? 

Thanks."
why to use target values when we want to predict target target values or their quantiles?,google-research/google-research,2020-10-27 07:52:59,1,,435,730207836,"I am having difficulty in understanding that why does the `tft` code uses previous observations which are also targets? For example using electricity example, and if `total_time_steps` are 192 and `num_encoder_steps` are 168, I understand that the model uses 168 previous values to predict next 24 values (or their quantiles). But the input data i.e. `data` in line [1145](https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py#L1145) has the shape `(450000, 192, 5)` while the `labels` have shape `(450000, 24, 5)`. However the last 24 values in `data` are exactly what the 24 values of `labels` are. For validation I ran following lines of code during debugging after line [1145](https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py#L1145)

```python
for i,j in zip(data[0, -24:, 0], labels[0]):
    print(np.abs(np.subtract(i,j[0])))
```
and it only prints 0s. 

My question is if we are required to feed the target values as inputs, then what is the point of having a model? What am I missing? "
[BERTSeq2Seq]Attention Mechanism,google-research/google-research,2020-10-23 21:19:26,0,,433,728530063,"Hi Very nice work on the BERT Seq2Seq model. One question: In order for it to fit in the Seq2Seq model, you need to create attention mechansim between encoder-decoder. Bert/GPT2 is pretty much using self-attention. I cannot find any source code on this since you guys release the trained models. Can you share that piece of code/design ideas? 

For Summary task: 
I saw on the shared models that it directly outputs the summary. Can I know if there is a way to finetune the model base on our data? https://github.com/google-research/google-research/tree/master/bertseq2seq"
[repnet] multi-action detection + count,google-research/google-research,2020-10-23 16:09:00,0,,432,728342364,"Hi @debidatta 
Can RepNet be used for counting multiple actions in a video? Also would it be possible to process a video as long as 2-3 hours with sporadically repeated actions? How long would the processing take?"
[BAM] Config cannot read/decode JSON- Extra data: line 1 column 5 (char 4),google-research/google-research,2020-10-22 15:35:55,0,,431,727500973,"Hi. Thanks for the repo. Following the steps in the repo, I am trying to run command `python -m bam.run_classifier debug-model $BAM_DIR '{""debug"": true}'` as a minimal test. However, below line of code, raise `Extra data: line 1 column 5 (char 4)` error. Does anyone have any idea what should be the root cause? I tried to run other commands, still, this line raises an issue with the JSON. 

Line raining the error:
`\bam\run_classifier.py"", line 258, in main
    config = configure.Config(topdir, model_name, **json.loads(hparams))`

Error: 
`raise JSONDecodeError(""Extra data"", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 5 (char 4)`"
scann core dump when python import,google-research/google-research,2020-10-20 10:05:48,2,,430,725422686,"once python import scann, python crash & core dump with error 'Illigal instruction'. is there any problem with compablity?

system info
```
[root@k8s-new-node-2-37nir tmp]# cat /etc/os-release 
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

[root@k8s-new-node-2-37nir tmp]# uname -a
Linux k8s-new-node-2-37nir.vclound.com 4.20.7-1.el7.elrepo.x86_64 #1 SMP Wed Feb 6 13:17:46 EST 2019 x86_64 x86_64 x86_64 GNU/Linux
[root@k8s-new-node-2-37nir tmp]# python3 --version
Python 3.6.0
[root@k8s-new-node-2-37nir tmp]# pip3 --version
pip 20.2.4 from /usr/local/python36/lib/python3.6/site-packages/pip (python 3.6)
```

gdb info
```
Core was generated by `python3'.
Program terminated with signal 4, Illegal instruction.
#0  0x00007fbaaf0b1c54 in InitDefaultsscc_info_FixedPoint_scann_2fproto_2fexact_5freordering_2eproto() () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-11.el7.x86_64 libselinux-2.5-6.el7.x86_64 libstdc++-4.8.5-11.el7.x86_64 libuuid-2.23.2-33.el7.x86_64 ncurses-libs-5.9-14.20130511.el7_4.x86_64 openssl-libs-1.0.2k-19.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 readline-6.2-11.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) bt
#0  0x00007fbaaf0b1c54 in InitDefaultsscc_info_FixedPoint_scann_2fproto_2fexact_5freordering_2eproto() () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#1  0x00007fbaaf19cf23 in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#2  0x00007fbaaf19cf0f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#3  0x00007fbaaf19cf0f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#4  0x00007fbaaf19cdc0 in google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#5  0x00007fbaaf0e2545 in google::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable const*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#6  0x00007fbaffd5b1e3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2
#7  0x00007fbaffd5f8f6 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2
#8  0x00007fbaffd5aff4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#9  0x00007fbaffd5efeb in _dl_open () from /lib64/ld-linux-x86-64.so.2
#10 0x00007fbaff92cfbb in dlopen_doit () from /lib64/libdl.so.2
#11 0x00007fbaffd5aff4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#12 0x00007fbaff92d5bd in _dlerror_run () from /lib64/libdl.so.2
#13 0x00007fbaff92d051 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2
#14 0x00007fbabd53d88a in tensorflow::internal::LoadLibrary(char const*, void**) () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#15 0x00007fbabd523af7 in tensorflow::(anonymous namespace)::PosixEnv::LoadLibrary(char const*, void**) () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#16 0x00007fbabd1e30b4 in tensorflow::LoadLibrary(char const*, void**, void const**, unsigned long*) () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#17 0x00007fbac1b811b7 in TF_LoadLibrary () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```"
[Performer] Make weights publicly available,google-research/google-research,2020-10-19 17:30:29,1,,428,724821359,Thanks so much for making the code for Performer publicly available! Do you think you could also open source the weights of the pre-trained models shown in the paper?
"[ScaNN] searcher serialize fails with std::bad_alloc, plenty of space left on machine",google-research/google-research,2020-10-19 17:05:17,5,,427,724804465,"The input data was ~10M vectors of 2048 dim each, the searcher was successfully built after 14 minutes, and there were hundreds of GB to spare (several multiples of the space used by the searcher itself), but it was not possible to to save it

```
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
<ipython-input-33-3451ef45ab69> in <module>
      1 os.makedirs('.data/scann_artifacts/test', exist_ok=True)
----> 2 searcher.serialize('./data/scann_artifacts/test')

~/.conda/envs/vsms/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops_pybind.py in serialize(self, artifacts_dir)
     70 
     71   def serialize(self, artifacts_dir):
---> 72     self.searcher.serialize(artifacts_dir)
     73 
     74 

MemoryError: std::bad_alloc
```

The problem goes away if I build the searcher with the first 2M vectors."
When will the Keypose learning code be released?,google-research/google-research,2020-10-14 09:40:04,0,,420,721304569,"Keypose sample code and pretrained models work very well.
I want to train a Keypose network with a custom dataset.
So I want to know when the training code will be released.
@kkonolige "
[uq_benchmark_2019] what does mean by 'r1' in the trained weight file?,google-research/google-research,2020-10-13 01:15:16,0,,418,719775309,"In pre-trained weight file, there are 'r1', 'r2', ..., 'r4' folders. What does mean by those numbers?"
So does it use past targets (y)?,google-research/google-research,2020-10-10 02:18:47,0,,415,718508482,"The task is formalized as 
<img width=""308"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643397-83cb7300-0ae1-11eb-88d2-a235716b843f.png"">
<img width=""415"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643446-ce4cef80-0ae1-11eb-8396-82b3b8f1a0ef.png"">
the past targets y is part of the inputs.

But in the figure, it seems that past targets are not in this model.
<img width=""612"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643428-ae1d3080-0ae1-11eb-8d86-8aeebe7143ff.png"">
where
<img width=""128"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643442-c0976a00-0ae1-11eb-83a9-222f79374256.png"">
So where are the past targets?"
AttributeError: module 'tensorflow.keras.backend' has no attribute 'get_session',google-research/google-research,2020-10-09 07:13:03,1,,414,717914132,"Extracting 10 samples...
Cached data ""valid"" updated
*** Fitting TemporalFusionTransformer ***
Getting batched_data
Using cached training data
Using cached validation data
Using keras standard fit
Train on 100 samples, validate on 10 samples
2020-10-09 07:11:36.762729: W tensorflow/c/c_api.cc:326] Operation '{name:'TemporalFusionTransformer/lstm/while' id:3156 op device:{} def:{{{node TemporalFusionTransformer/lstm/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=45, _read_only_resource_inputs=[8, 9, 10], body=TemporalFusionTransformer_lstm_while_body_3416[], cond=TemporalFusionTransformer_lstm_while_cond_3415[], output_shapes=[[], [], [], [], [?,5], ..., [], [], [], [], []], parallel_iterations=32](TemporalFusionTransformer/lstm/while/loop_counter, TemporalFusionTransformer/lstm/while/maximum_iterations, TemporalFusionTransformer/lstm/time, TemporalFusionTransformer/lstm/TensorArrayV2_1, TemporalFusionTransformer/layer_normalization_4/add, TemporalFusionTransformer/layer_normalization_5/add, TemporalFusionTransformer/lstm/strided_slice, TemporalFusionTransformer/lstm/TensorArrayUnstack/TensorListFromTensor, TemporalFusionTransformer/lstm/lstm_cell/kernel, TemporalFusionTransformer/lstm/lstm_cell/recurrent_kernel, TemporalFusionTransformer/lstm/lstm_cell/bias, TemporalFusionTransformer/lstm/while/EmptyTensorList, TemporalFusionTransformer/lstm/while/EmptyTensorList_1, TemporalFusionTransformer/lstm/while/EmptyTensorList_2, TemporalFusionTransformer/lstm/while/EmptyTensorList_3, TemporalFusionTransformer/lstm/while/EmptyTensorList_4, TemporalFusionTransformer/lstm/while/EmptyTensorList_5, TemporalFusionTransformer/lstm/while/EmptyTensorList_6, TemporalFusionTransformer/lstm/while/EmptyTensorList_7, TemporalFusionTransformer/lstm/while/EmptyTensorList_8, TemporalFusionTransformer/lstm/while/EmptyTensorList_9, TemporalFusionTransformer/lstm/while/EmptyTensorList_10, TemporalFusionTransformer/lstm/while/EmptyTensorList_11, TemporalFusionTransformer/lstm/while/EmptyTensorList_12, TemporalFusionTransformer/lstm/while/EmptyTensorList_13, TemporalFusionTransformer/lstm/while/EmptyTensorList_14, TemporalFusionTransformer/lstm/while/EmptyTensorList_15, TemporalFusionTransformer/lstm/while/EmptyTensorList_16, TemporalFusionTransformer/lstm/while/EmptyTensorList_17, TemporalFusionTransformer/lstm/while/EmptyTensorList_18, TemporalFusionTransformer/lstm/while/EmptyTensorList_19, TemporalFusionTransformer/lstm/while/EmptyTensorList_20, TemporalFusionTransformer/lstm/while/EmptyTensorList_21, TemporalFusionTransformer/lstm/while/EmptyTensorList_22, TemporalFusionTransformer/lstm/while/EmptyTensorList_23, TemporalFusionTransformer/lstm/while/EmptyTensorList_24, TemporalFusionTransformer/lstm/while/EmptyTensorList_25, TemporalFusionTransformer/lstm/while/EmptyTensorList_26, TemporalFusionTransformer/lstm/while/EmptyTensorList_27, TemporalFusionTransformer/lstm/while/EmptyTensorList_28, TemporalFusionTransformer/lstm/while/EmptyTensorList_29, TemporalFusionTransformer/lstm/while/EmptyTensorList_30, TemporalFusionTransformer/lstm/while/EmptyTensorList_31, TemporalFusionTransformer/lstm/while/EmptyTensorList_32, TemporalFusionTransformer/lstm/while/EmptyTensorList_33)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
100/100 [==============================] - ETA: 0s - loss: 1.9711WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
100/100 [==============================] - 3s 33ms/sample - loss: 1.9711 - val_loss: 1.5490
Cannot load from data/saved_models/electricity/fixed/tmp, skipping ...
Using cached validation data
Optimal model found, updating
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 238, in <module>
    use_testing_mode=True)  # Change to false to use original default params
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 136, in main
    opt_manager.update_score(params, val_loss, model)
  File ""/content/google-research/tft/libs/hyperparam_opt.py"", line 227, in update_score
    model.save(self.hyperparam_folder)
  File ""/content/google-research/tft/libs/tft_model.py"", line 1307, in save
    tf.keras.backend.get_session(),
AttributeError: module 'tensorflow.keras.backend' has no attribute 'get_session'"
AttributeError: module 'tensorflow.keras.layers' has no attribute 'CuDNNLSTM,google-research/google-research,2020-10-09 07:00:47,1,,413,717907449,"Selecting GPU ID=0
*** Training from defined parameters for electricity ***
Loading & splitting data...
/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
Formatting train-valid-test splits.
Setting scalers with training data...
*** Loading hyperparm manager ***
*** Running calibration ***
Params Selected:
dropout_rate: 0.1
hidden_layer_size: 5
learning_rate: 0.001
minibatch_size: 64
max_gradient_norm: 0.01
num_heads: 4
stack_size: 1
model_folder: data/saved_models/electricity/fixed
WARNING:tensorflow:From /content/google-research/tft/script_train_fixed_params.py:121: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

Resetting temp folder...
*** TemporalFusionTransformer params ***
# dropout_rate = 0.1
# hidden_layer_size = 5
# learning_rate = 0.001
# max_gradient_norm = 0.01
# minibatch_size = 64
# model_folder = data/saved_models/electricity/fixed
# num_heads = 4
# stack_size = 1
# total_time_steps = 192
# num_encoder_steps = 168
# num_epochs = 1
# early_stopping_patience = 5
# multiprocessing_workers = 5
# column_definition = [('id', <DataTypes.REAL_VALUED: 0>, <InputTypes.ID: 4>), ('hours_from_start', <DataTypes.REAL_VALUED: 0>, <InputTypes.TIME: 5>), ('power_usage', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>), ('hour', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('day_of_week', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('hours_from_start', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('categorical_id', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>)]
# input_size = 5
# output_size = 1
# category_counts = [369]
# input_obs_loc = [0]
# static_input_loc = [4]
# known_regular_inputs = [1, 2, 3]
# known_categorical_inputs = [0]
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 238, in <module>
    use_testing_mode=True)  # Change to false to use original default params
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 124, in main
    model = ModelClass(params, use_cudnn=use_gpu)
  File ""/content/google-research/tft/libs/tft_model.py"", line 456, in __init__
    self.model = self.build_model()
  File ""/content/google-research/tft/libs/tft_model.py"", line 1006, in build_model
    = self._build_base_graph()
  File ""/content/google-research/tft/libs/tft_model.py"", line 929, in _build_base_graph
    = get_lstm(return_state=True)(historical_features,
  File ""/content/google-research/tft/libs/tft_model.py"", line 907, in get_lstm
    lstm = tf.keras.layers.CuDNNLSTM(
AttributeError: module 'tensorflow.keras.layers' has no attribute 'CuDNNLSTM"
bug (tft) - wrong num_encoder_steps for retail dataset favorita,google-research/google-research,2020-10-08 10:33:40,0,,412,717223918,"I guess we are taking 90 days history to forecast for next 30 days. So shouldn't the num_encoder_steps be 90 instead ? 
It seems for other datasets num_encoder_steps are defined correctly, but wrong for retail dataset (favorita)

tft/data_formatters/favorita.py

# Default params
  def get_fixed_params(self):
    """"""Returns fixed model parameters for experiments.""""""

    fixed_params = {
        'total_time_steps': 120,
        'num_encoder_steps': 30,
        'num_epochs': 100,
        'early_stopping_patience': 5,
        'multiprocessing_workers': 5
    }

    return fixed_params"
[uq_benchmark_2019] How to test ensemble model?,google-research/google-research,2020-10-05 11:00:18,2,,411,714738295,"I ran 'end_to_end_test.py' file, and get the results of dropout, svi, vanilla. 
I also want to get the ensemble's result, but it seems that ensemble method is not included in the 'end_to_end_test.py' file.
How can I test ensemble model?"
Self-supervised pretraining for TabNet,google-research/google-research,2020-10-03 04:28:08,0,,410,714001597,"Do you have example of using TabNet for self-supervised pretraining? Especially the ones you mentioned in the paper:
`We study self-supervised learning on Higgs and Forest Cover Type datasets. `

Thanks!"
[ScaNN] Serialize searcher throws `cannot create std::vector larger than max_size`,google-research/google-research,2020-09-24 04:23:50,0,,406,707841834,"When trying to serialize a searcher for a Dataset with 2,000,000 items and each item with a vetor length of 4,096. It will throw an error.

```
Traceback (most recent call last):
  File ""scann_run.py"", line 141, in <module>
    train()
  File ""scann_run.py"", line 116, in train
    searcher.serialize(save_target)
  File ""~/.local/lib/python3.6/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 55, in serialize
    self.searcher.serialize(artifacts_dir)
ValueError: cannot create std::vector larger than max_size()
```"
flag --vocab_file=None: Flag --vocab_file must have a value other than None.,google-research/google-research,2020-09-23 19:49:28,2,,405,707635775,"Traceback (most recent call last):
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/tungdinh/CSProjects/UW/Capstone/Research/google-research/goemotions/bert_classifier.py"", line 1037, in <module>
    tf.compat.v1.app.run()
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/app.py"", line 293, in run
    flags_parser,
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/app.py"", line 362, in _run_init
    flags_parser=flags_parser,
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/app.py"", line 212, in _register_and_parse_flags_with_usage
    args_to_main = flags_parser(original_argv)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 31, in _parse_flags_tolerate_undef
    return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow_core/python/platform/flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 636, in __call__
    self._assert_all_validators()
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 510, in _assert_all_validators
    self._assert_validators(all_validators)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 531, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --vocab_file=None: Flag --vocab_file must have a value other than None.

I'm running this program via this repo: https://github.com/tungxd96/google-research.git
First resolved error so far is to replace tensorflow v2 to tensorflow v1. Next, I'm receiving this error while running ""python -m goemotions.bert_classifier"". I found this vocab_file is assigned to None instead of a directory pointing to the vocabulary file as shown in line 75 of bert_classifier.py. I can't find any vocabulary file in google-research/goemotions/data. Am I missing any important files so far besides downloading 3 csv files in full_datasets? Has someone faced this issue before? Please show me how to solve it or if anyone has trained this model successfully, could you provide reproducing steps? Thank you"
Download the GoEmotion Dataset,google-research/google-research,2020-09-23 06:34:10,1,,404,707088451,"Hello, 

Could you please let me know where I can download the GoEmotion Dataset? 

I loaded the GoEmotion dataset from TensorFlow dataset but I am not able to decode the column ""comment_text"". 

Therefore, could you please let me know where can I download the dataset maybe in csv or tsv file format? 

Thank you 

"
[Scann] Question: Does Scann support dynamically increasing database.,google-research/google-research,2020-09-21 10:13:02,4,,401,705464435,"Hi,

I was going through the repo but couldn't find any resource on how to increase the database size once the tree is built. If I have trained the tree on 1 million data points, is it possible to add 1 million more data points??."
[ged_tts] overcomplete basis implementation?,google-research/google-research,2020-09-18 14:52:59,0,,399,704443586,"the GED paper specifies the use of an 8x overcomplete basis when computing spectrograms in appendix C.1. But it appears that a basis of fixed dimension 256 is used in every case, and that it is not possible for the bin count to vary with window length as implemented.

`ged` calls `calc_spectrograms` without changing the default value of 256:
https://github.com/google-research/google-research/blob/1c49e6ffd1dd009c21c4ff7596341c9ee4c90603/ged_tts/distance_function/spectral_ops.py#L95-L98
this same value is passed for every window length to `get_spectral_matrix` where it determines the number of filters:
https://github.com/google-research/google-research/blob/1c49e6ffd1dd009c21c4ff7596341c9ee4c90603/ged_tts/distance_function/spectral_ops.py#L71-L78"
[ScaNN]: Add option to add new dataset items,google-research/google-research,2020-09-18 08:52:14,3,,398,704207490,"Hi,

I have a large dataset with image (say ~2 million) embeddings. 

Can I add another say 100 embeddings to the scanner or I need always a new Initialization?   "
[uq-uncertainty] vanilla accuracy 20 news (even classes) unreachable,google-research/google-research,2020-09-16 10:26:27,3,,393,702646073,"The appendix of the paper mentions reaching 95.5% accuracy for 20news with the detailed hyperparameter settings. 
Whatever I do, I can never exceed 40% accuracy on the in-distribution data [even data]. 
Did they mean AUC? 
On all 20 classes I manage to reach 80% accuracy both with a textCNN and LSTM. 

I add in my Sacred config for comparison: 
```@ex.named_config
def ovadia_20news():
    """"""
    %maxdoclen 250; max_vocabulary 30000
    %The vanilla model uses a one-layer LSTM model of size 32 and a dense layer to predict the 10 class
    % probabilities based on word embedding of size 128. A dropout rate of 0.1 is applied to both the LSTM
    % layer and the dense layer for the Dropout model. The LL-SVI model replaces the last dense layer
    % with a Bayesian layer, the ensemble model aggregates 10 vanilla models, and stochastic methods
    % sample 5 predictions per example. The vanilla model accuracy for in-distribution test data is 0.955.
    """"""
    identifier = ""20news""
    data_folder = os.path.join(DATAROOT, identifier)
    out_folder = generate_out_folder(data_folder)
    task = ""document_classification""  # or regression
    max_vocabulary = 30000  # 20news to 30K e.g.
    composition = [""word""]

    model = ""lstm""
    embed_dim = 128
    projection_nodes = 32
    dropout = 0.1
    dropout_nonlinear = 0.1
    embedding_dropout = 0
    dropout_concrete = None
    weight_decay = 0  # .0001  # triggers AdamW optimizer
    max_document_len = 250

    epochs = 48
    optimizer = ""adam""
    clipnorm = 10
    learning_rate = 0.001
    steps_per_epoch = None  # could also be None

    posterior_sampling = 10
    use_aleatorics = False
    multilabel = False
    loss_fn = ""categorical_crossentropy"" if not use_aleatorics else ""attenuated_learned_loss""
    metrics = [""accuracy"", ""mse""] if not use_aleatorics else []
    ood = ['comp.graphics', 'comp.sys.ibm.pc.hardware', 'comp.windows.x', 'rec.autos', 'rec.sport.baseball', 'sci.crypt', 'sci.med', 'soc.religion.christian', 'talk.politics.mideast', 'talk.religion.misc']
```"
t5_closed_book_qa - Pre-trained models avaialble on aws/s3 ?,google-research/google-research,2020-09-15 16:55:19,0,,391,702096869,I am having trouble accessing google cloud for t5_closed_book_qa due to corporate restrictions. Any other location where I can download pre trained wieghts from ?
Can you please publicly release the raw Pouring dataset with new annotations (instead of the tfrecords one)?,google-research/google-research,2020-09-13 21:57:37,6,,390,700656074,Can you please publicly release the raw Pouring dataset with new annotations (instead of the tfrecords one)? @debidatta
Steps to build our first android app!!,google-research/google-research,2020-09-11 18:40:06,0,,389,699628070,.
Possible bug in TFT,google-research/google-research,2020-09-08 15:45:18,0,,388,695984510,"Found this piece of code in [tft_model.py](https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py#L1130)

```python
print('Getting batched_data')
    if train_df is None:
      print('Using cached training data')
      train_data = TFTDataCache.get('train')
    else:
      train_data = self._batch_data(valid_df)
```

Shouldn't that be `train_df`? "
Tuple Phrase Extraction checkpoint- Seq2act,google-research/google-research,2020-09-07 11:00:17,0,,385,694983753,"The checkpoint is only provided for grounding model, so if I want to train my own grounding model while using the Tuple Phrase pre-trained checkpoint model, what should I do?"
[mobilebert] Tensor had NaN values Error when using quantization-aware training ,google-research/google-research,2020-09-06 17:55:11,11,,384,694468877,"Hey, I am trying to retrain mobilebert. I was able to run the pretraing scrips with no problems. However, when fine-tuning on squad using the flag `--use_quantized_training=true`. I get the following error 

```
ERROR:tensorflow:Error recorded from training_loop: From /job:worker/replica:0/task:0:
Gradient for bert/encoder/layer_9/attention/output/dense/bias:0 is NaN : Tensor had NaN values
```
Here is the script I use for fine-tuning 

```
python3 run_squad.py \
	--bert_config_file=config/uncased_L-24_H-128_B-512_A-4_F-4_OPT_QAT.json \
	--data_dir=${DATA_DIR} \
	--do_train \
	--doc_stride=128 \
	--init_checkpoint={INIT_CKPT}/model.ckpt-1000.index \
	--learning_rate=4e-05 \
	--predict_file=dev.json \
	--do_lower_case \
  	--do_predict \
	--max_answer_length=30 \
	--max_query_length=64 \
	--max_seq_length=128\
	--n_best_size=20 \
	--num_train_epochs=1 \
	--output_dir=${OUTPUT_DIR} \
	--train_batch_size=32 \
	--train_file=train.json \
	--use_tpu \
	--tpu_name=${TPU_NAME} \
	--vocab_file=../bert/en-vocab.txt \
	--warmup_proportion=0.1 \
	--verbose_logging=True \
	--use_quantized_training=true
```"
[mobilebert] Question about the init_checkpoint,google-research/google-research,2020-09-06 17:40:17,0,,383,694464038,I am trying to train mobilebert from scratch on another language. The `run_pretraining.py` file contains a flag called `init_checkpoint`. Is that flag for a pretrained BERT model ? i.e do I have to train BERT first then do the distillation ?
[mentormix] How would you deal with imbalance dataset in mentormix?,google-research/google-research,2020-09-01 14:46:34,5,,380,690204916,"@roadjiang How would you deal with imbalanced dataset in Mentormix setting? Without special handling, negative samples (say a large portion of the dataset is negative) tend to have lower loss, with MentorMix where we favor sample with lower loss, we would focus much more on only negative samples.  "
[KWS] Splitting Speech Command Dataset ,google-research/google-research,2020-08-31 12:48:18,2,,378,689180971,"@rybakov

Hi Oleg, I am a little confused about how you get datasets split. For instance In V2, we have 105829 samples in total, but only 36923/4445/4890 are used for train/val/test sets. 

The script [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/input_data.py) doesn't seem to handle the number of samples.

Best regards."
EEG viewer is not compatible with latest Werkzeug,google-research/google-research,2020-08-29 03:01:59,0,,377,688445317,`ProtobufRequestMixin` has been deprecated since v0.15 and is no longer available in 1.0 and above. Would you mind updating the EEG viewer code to not use it?
ScaNN builder crashes for higher values of k,google-research/google-research,2020-08-26 18:26:46,1,,375,686526572,"Hello,

Inspired from the scann/docs/example.ipynb, I wrote the following code to search the freebase entities:

`searcher = scann.ScannBuilder(normalized_dataset, 2000, ""dot_product"").tree(
num_leaves = 1000, num_leaves_to_search = 50, training_sample_size = 3000).score_ah(
2, anisotropic_quantization_threshold = 0.2).create_pybind()`

`# normalized_dataset.shape = 15000, 400 # freebase entity embeddings with dimension 400`

With `k = 2000`, the application crashes with **corrupted unsorted chunks error**.
For `k = 20` or `k = 50`, the application runs just fine. Any clue would be appreciated.

Complete backtrace is here
`
2020-08-26 20:01:05.869102: I scann/partitioning/partitioner_factory_base.cc:58] Size of sampled dataset for training partition: 2920
2020-08-26 20:01:06.241532: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 372.329265ms.
*** Error in '/home/uji300/.pyenv/versions/py3.7/bin/python': free(): corrupted unsorted chunks: 0x0000000074268ba0 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7c619)[0x2aaaab670619]
/home/uji300/.pyenv/versions/py3.7/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so(_ZN10tensorflow9scann_ops10ScannNumpy13SearchBatchedERKN8pybind117array_tIfLi17EEEiiib+0x780)[0x2aabbc8b7d60]
/home/uji300/.pyenv/versions/py3.7/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so(+0x5611d9)[0x2aabbc8b61d9]
/home/uji300/.pyenv/versions/py3.7/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so(+0x556e5e)[0x2aabbc8abe5e]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyMethodDef_RawFastCallKeywords+0x364)[0x43b0b4]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyObject_FastCallKeywords+0x2e7)[0x43b3c7]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalFrameDefault+0x607e)[0x427d5e]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalCodeWithName+0xa36)[0x4ef436]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyFunction_FastCallKeywords+0xa5)[0x43ac15]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalFrameDefault+0x7137)[0x428e17]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalCodeWithName+0xa36)[0x4ef436]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyFunction_FastCallKeywords+0xa5)[0x43ac15]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalFrameDefault+0x7137)[0x428e17]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalCodeWithName+0xa36)[0x4ef436]
/home/uji300/.pyenv/versions/py3.7/bin/python(PyEval_EvalCode+0x23)[0x4ef533]
/home/uji300/.pyenv/versions/py3.7/bin/python(PyRun_FileExFlags+0x15e)[0x526efe]
/home/uji300/.pyenv/versions/py3.7/bin/python(PyRun_SimpleFileExFlags+0xdc)[0x5270fc]
/home/uji300/.pyenv/versions/py3.7/bin/python[0x42f71d]
/home/uji300/.pyenv/versions/py3.7/bin/python(_Py_UnixMain+0x2d)[0x42f98d]
/lib64/libc.so.6(__libc_start_main+0xf5)[0x2aaaab615c05]
/home/uji300/.pyenv/versions/py3.7/bin/python[0x42a9f4]`"
Code Of Conduct Missing in this repo,google-research/google-research,2020-08-24 14:07:30,0,,372,684700147,
"randomizer fail, bazel sched.h no such file Error in windows choco bazel",google-research/google-research,2020-08-22 15:07:03,0,,370,684012355,"```bash
ERROR: C:/home/projects/software/study/google-research/automl_zero/BUILD:533:11: C++ compilation of rule '//:randomizer' failed (Exit 2)
C:\users\seonglae\_bazel_seonglae\e4sphe5m\execroot\__main__\definitions.h(25): fatal error C1083: Cannot open include file: 'sched.h': No such file or directory
```

Output when I run `./run_demo.sh` in ` google-research/automl_zero` demo

I installed bazel with choco in windows
```bash
choco install bazel
```

Is this meaningful error? and how can I solve this problem?

"
ModuleNotFoundError: No module named 'dataset_analysis',google-research/google-research,2020-08-21 15:31:12,2,,368,683643642,"While running goemotions  bert_classifier.py:
```
Traceback (most recent call last):
  File ""bert_classifier.py"", line 50, in <module>
    from dataset_analysis.bert import modeling
ModuleNotFoundError: No module named 'dataset_analysis'
```

Where dataset_analysis module can be found ?
Thank you."
[BertSeq2Seq] ,google-research/google-research,2020-08-19 15:16:11,1,,366,681924048,"Do you have the right version of tensorflow, TF_ Hub and sentencepiece to run the model 


"
[BertSeq2Seq] Retrieve SentencePieceModel,google-research/google-research,2020-08-19 14:05:23,3,,365,681868651,"Thanks a lot for making the weights of the BertSeq2Seq models of your paper: [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) available to the public.

I'm trying to get access to the sentence piece model that was used for **bertseq2seq/roberta24_cnndm** : https://tfhub.dev/google/bertseq2seq/roberta24_cnndm/1

As far as I understand it can be found under `assets` in the unzipped folder, but I cannot find anything there. 
Is there a possibility that you can add the sentence piece models to all bertseq2seq models? 

Gently pinging @shashiongithub :-)  

Would be very thankful for any help!
"
[depth_from_video_in_the_wild]How to get the learned camera intrinsic?,google-research/google-research,2020-08-18 07:46:03,1,,362,680775104,"Thanks for your work and sharing.
I tested your pre-trained models on my own image sequence.
It seems the trajectory_inference.py only provided the relative position.
The paper said your work can learn the camera intrinsic but I can't find any part about it in this project."
How to download using cmd line checkpoint from google drive?,google-research/google-research,2020-08-16 16:59:23,0,,361,679788059,"Hello,

How to download using shell\command line googleapis links such as https://www.googleapis.com/download/storage/v1/b/gresearch/o/depth_from_video_in_the_wild%2Fcheckpoints%2Fcityscapes_kitti_learned_intrinsics.zip?generation=1566493762028542&alt=media
?"
[scann] Tensorflow with scann example needed,google-research/google-research,2020-08-14 02:40:43,2,,357,678855172,The example in `google-research/scann/docs/example.ipynb` is scann with numpy example. Is there a tensorflow example?
[scann] How to save the searcher?,google-research/google-research,2020-08-13 18:33:47,5,,355,678652031,"Is there a way to save `scann.scann_ops.py.scann_ops_pybind.ScannSearcher`? Pickle solution failed (I tried joblib). 
Thank you."
missing file cold_posterior_bnn/run_cnnlstm_experiment_small.sh,google-research/google-research,2020-08-13 15:29:37,0,,354,678529432,"Hi, 
the README says to run:

cold_posterior_bnn/run_cnnlstm_experiment_small.sh

for the small version of the cnnlstm experiment, but the script seems not to exist in the repo.

"
No parameter 'num_iterations' for configurable 'LbpSampler',google-research/google-research,2020-08-10 12:46:28,0,,350,676108713,"https://github.com/google-research/google-research/blob/39195a28fb30ecffc06f1d9d60200606dc10d1ed/grouptesting/configs/toy.gin#L105

Is it the `max_iterations` to be set instead?
```
LbpSampler.max_iterations = 100
```"
Question about speech embedding ,google-research/google-research,2020-08-09 13:36:55,0,,349,675701319,"Hi,

Thank you for your interesting work!  I have read the paper, and I want to reproduce the performance using your code. However, I have some questions about the codes.

1.   In the speech_commands.ipynb, I find that, the results are different  when I evaluate the model with different batch size. It might be casued by the batch norm, the batch norm is still in trianing mode when we evaluate the model. The same condition can be observed in the inference codes. Therefore, is this a bug？

Thanks,
Hang"
[Depth from Videos in the Wild] Problem with Training on YouTube Videos,google-research/google-research,2020-08-08 21:20:17,1,,348,675590055,"Hi, thanks for your interesting work. I am trying to train 'Depth from Videos in the Wild' on YouTube videos. The problem that I have is that even when I set 'learn_intrinsics', the current training codes needs the camera intrinsic parameter as a text file. How can I solve it? Thanks"
Question about BERT model size (transformer block number) ,google-research/google-research,2020-08-06 15:44:45,0,,346,674413542,"Hi,

Thank you for your interesting work! I have just started to learn BERT recently. I have some general questions regarding this topic.

1. Why BERT has 12 blocks? Not 11 or 13 etc. ? I couldn't find any explanation.

2. I want to compare the performance of BERT with different model size (transformer block number). Is it necessary to do distillation?  If I just train a BERT with 6 Layers without distillation, does the performance look bad?

3. Do you have to do pretraining every time you change the layer number of  BERT? Is it possible to just remove some layers in an existing pre-trained model and finetune on tasks? 


Thanks,
ZLK
"
"[scann] Attempt to include <hash_set>, which is deprecated",google-research/google-research,2020-08-05 07:22:13,0,,344,673316635,"## Description

I executed the suggested Bazel build command and the error occurred as below:

```
In file included from scann/partitioning/kmeans_tree_partitioner.cc:15:
In file included from ./scann/partitioning/kmeans_tree_partitioner.h:24:
In file included from ./scann/partitioning/kmeans_tree_like_partitioner.h:22:
./scann/partitioning/partitioner_base.h:21:10: fatal error: 'hash_set' file not found
#include <hash_set>
         ^~~~~~~~~~
1 error generated.
Target //:build_pip_pkg failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

At the [reference](https://docs.microsoft.com/en-us/cpp/standard-library/hash-set-class?view=vs-2019) of this header, they said this API is obsolete and suggesting to use `<unordered_set>`.

I checked two headers are all included on `memory_logging.h`:

```cpp
#include <hash_set>
#include <type_traits>
#include <unordered_map>
#include <unordered_set>
```

## Possible Fix

We can remove `<hash_set>` and only use `<unordered_set>` if there are no concerns about breaking any dependencies. 

If not, it will be highly appreciated if you can provide the solution to use both headers when compile. (e.g. Bazel options, Configuration file change)

## Environment

My bazel version is `bazel 3.4.1-homebrew`, executed on macOS Catalina.

"
[scann] use of undeclared identifier 'aligned_alloc',google-research/google-research,2020-08-05 07:10:01,0,,343,673309508,"## Description
I executed the suggested Bazel build command and the error occurred as below:

```
scann/oss_wrappers/scann_aligned_malloc.cc:24:10: error: use of undeclared identifier 'aligned_alloc'; did you mean 'aligned_malloc'?
  return aligned_alloc(minimum_alignment, size);
         ^~~~~~~~~~~~~
         aligned_malloc
scann/oss_wrappers/scann_aligned_malloc.cc:22:7: note: 'aligned_malloc' declared here
void *aligned_malloc(size_t size, size_t minimum_alignment) {
      ^
1 error generated.
```

At the [reference](https://en.cppreference.com/w/cpp/memory/c/aligned_alloc) of this function, aligned_alloc, I could find the definition inside <cstdlib> on C++17.
I also checked the header file, `scann_aligned_malloc.h`, and found the include statement for `<cstdlib>` is already there.

I checked my build option contains the option for setting standard version to 17, `--cxxopt=""-std=c++17""`, but error still remains.

## Environment
My bazel version is bazel 3.4.1-homebrew, executed on macOS Catalina."
[scann] TF_SHARED_LIBRARY_NAME set with typos when configure,google-research/google-research,2020-08-05 07:02:42,4,,342,673305503,"## Description

I executed `python configure.py` inside `virtualenv` constructed with Python 3.7 and saw generated `.bazelrc` and `.bazel_query.sh`.

I executed the suggested Bazel build command, `CC=clang-8 bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg`, and the error occurred as below:

```
...
ERROR: /private/var/tmp/_bazel_harryhong/6c67f240e5dd3cb3ad6beb460050957b/external/local_config_tf/BUILD:11:11: in srcs attribute of cc_library rule @local_config_tf//:libtensorflow_framework: '@local_config_tf//:ensorflow_framework.2' does not produce any cc_library srcs files (expected .cc, .cpp, .cxx, .c++, .C, .cu, .cl, .c, .h, .hh, .hpp, .ipp, .hxx, .h++, .inc, .inl, .tlh, .tli, .H, .tcc, .S, .s, .asm, .a, .lib, .pic.a, .lo, .lo.lib, .pic.lo, .so, .dylib, .dll, .o, .obj or .pic.o)
ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.406s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (5 packages loaded, 1355 targets configured)
```

I thought `@local_config_tf//:ensorflow_framework.2` is suspicious, so I modified `.bazelrc` as follows:
```diff
- build --action_env TF_SHARED_LIBRARY_NAME=""ensorflow_framework.2""
+ build --action_env TF_SHARED_LIBRARY_NAME=""libtensorflow_framework.2.dylib""
```
and build had proceeded with no error on that portion of code.
I also tried `libtensorflow_framework.2` on that env, but it also failed.

## Possible Fix

This error may produced by this code on `configure.py`:

```python
def generate_shared_lib_name(namespec):
  """"""Converts the linkflag namespec to the full shared library name.""""""
  # Assume Linux for now
  return namespec[1][3:]
```

This function may replace the prefix of the name of the shared library, but it fails on the macOS environment.
I can send a PR with a fix; capture the name of OS and branch off the logic.

## Environment

My bazel version is `bazel 3.4.1-homebrew`, executed on macOS Catalina."
depth_from_video_in_wild: distortion coeficients,google-research/google-research,2020-08-04 13:23:28,2,,340,672790276,"Hi @gariel-google ,
Your paper mentioned you leanred the distortion coeficients (two of them - quadratic and quartic), but your code has functions to apply distortion only with the quadratic coeficents, and no conv heads for it.

Did you use a 1x1 conv with 1 output, and no activation function?
Will you publish the code for the quatric distortion coeficient?

Thank you!
"
Step size and windows size of TRILL model,google-research/google-research,2020-07-29 23:15:09,2,,338,668229070,"Hi,

I have a question for TRILL model. What is the step size of feature extraction and what is the window size of each feature. 
For example, 
audio length: 2.16  feature count: 8
audio length: 3.36  feature count: 15
audio length: 6.6    feature count: 34
audio length: 1.49  feature count: 4
(Sampling rate is 16000)
What is the relation between audio length and the number of extracted feature?
Or more specifically, is there any way of knowing the timestamp of each extracted feature?

Thanks.






"
mobileBERT multilingual support,google-research/google-research,2020-07-29 07:18:32,2,,336,667612136,"Very impressed by the performance that mobileBERT delivers, is it possible to get a multi-lingual version please?"
Can not Generate RicoSCA Datasets,google-research/google-research,2020-07-28 03:37:07,3,,333,666727232,"Hello, 
This is a question  for the repo about seq2act paper.
i follow the steps in the README file under seq2act/data_generation and try to Generate RicoSCA Datasets. I can successfully run seq2act/data_generation/create_rico_sca.sh and geneate tfrecord files，but those generated tfrecord files are empty. Do i need extra step except what listed in the README file? please give me your suggestion when you are free, thank you."
Bug in tft_model,google-research/google-research,2020-07-27 09:05:58,0,,332,666117931,"In the file model_tft.py, function get_tft_embeddings(), the following piece of the code seems incorrect:

# Observed (a prioir unknown) inputs
    wired_embeddings = []
    for i in range(num_categorical_variables):
      if i not in self._known_categorical_input_idx \
        and i not in self._input_obs_loc:
        e = embeddings[i](categorical_inputs[:, :, i])
        wired_embeddings.append(e)

self._known_categorical_input_idx starts from the first categorical input of all_input, while self._input_obs_loc starts from 0 of all inputs. The condition here is incorrect.

e.g.
_column_definition = [
      ('Symbol', DataTypes.CATEGORICAL, InputTypes.ID),
      ('date', DataTypes.DATE, InputTypes.TIME),
      ('log_vol', DataTypes.REAL_VALUED, InputTypes.TARGET),
      ('open_to_close', DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),
      ('days_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),
      ('day_of_week', DataTypes.CATEGORICAL, InputTypes.OBSERVED_INPUT),
      ('day_of_month', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('week_of_year', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('month', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('Region', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),
  ]

log_vol is the target, and self._input_obs_loc = [0]
5 categorical inputs: day_of week, day_of_month, week_of_year, moth, region, and self._know_categorical_input_idx = [1, 2, 3, 4]. We made up the first categorical input 'day_of_week' to 'OBSERVED_INPUT' (unknown input).
We expected to append observed categorical input 'day_of_week' to wired_embeddings, however as the index conflict with self._input_obs_loc,  the code did not work.

Proposed fix:
# Observed (a prioir unknown) inputs
    wired_embeddings = []
    for i in range(num_categorical_variables):
      if i not in self._known_categorical_input_idx \
        and i + num_regular_variables not in self._input_obs_loc:
        e = embeddings[i](categorical_inputs[:, :, i])
        wired_embeddings.append(e)

changed 'i' to 'i + num_regular_variables'"
[depth_from_video_in_the_wild] about the checkpoint in table.4,google-research/google-research,2020-07-25 00:42:40,1,,330,665482359,"Thank you very much for your code. But I had a problem loading the weight: tf.train. latest_checkpoint (checkpoint_dir) return None Type. It seems to lack the ""checkpoint"" file in the compressed package."
Question about KWS ,google-research/google-research,2020-07-17 13:23:16,18,,327,659254084,"I find that there is [depthwise conv 1D](https://github.com/google-research/google-research/tree/master/kws_streaming/layers) in this project. However, I don't see it being used by any model. Since I am curious about this and would try it I guess, could someone explain for me why it is not used?

In addition, imo Depthwise Separable Conv may be the most efficient model for KWS due to its much less filters and memory access. Have you tried DSC+Resnet? It is worth noting that DSC+ResNet is SOTA in terms of efficiency[[1]](https://arxiv.org/pdf/2004.12200.pdf)[[2]](https://arxiv.org/pdf/2004.08531.pdf). Just wanna know the experiments you have done so that I could have some clue on how to do my research. Much appreciated!

Sorry for bothering @rybakov. I find that you are the main contributor for this project so it would be more efficient to ping you."
Can you please merge my pull request?,google-research/google-research,2020-07-16 20:49:43,0,,326,658558032,"Dear Google staff

I have created the pull request on the ""conqur"" folder. My pull request is: https://github.com/google-research/google-research/pull/315.

It has been more than a week now, do you know how can my pull be accepted?"
 mask in Scaled dot-product ,google-research/google-research,2020-07-15 06:47:37,0,,324,657106745,Can mask in Scaled dot-product be set to None?
AutoML-Zero demo fails build,google-research/google-research,2020-07-14 21:40:05,7,,323,656915435,"Running on Ubuntu 18.04, I followed the brief setup instructions.

```
gcc (Ubuntu 9.3.0-11ubuntu0~18.04.1) 9.3.0
g++ (Ubuntu 9.3.0-11ubuntu0~18.04.1) 9.3.0
bazel 3.4.1
```

After running ./run_demo.sh

```
INFO: Analyzed target //:run_search_experiment (41 packages loaded, 1469 targets configured).
INFO: Found 1 target...
ERROR: /home/clayton/tree/automl/google-research/automl_zero/BUILD:272:11: C++ compilation of rule '//:experiment_util' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 63 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
experiment_util.cc:24:10: fatal error: absl/container/node_hash_map.h: No such file or directory
   24 | #include ""absl/container/node_hash_map.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //:run_search_experiment failed to build
```

I confirmed that the file is present in the bazel files -

```
clayton@clayton-VirtualBox:~/tree/automl/google-research/automl_zero$ ls bazel-automl_zero/external/com_google_absl/absl/container/
btree_benchmark.cc  BUILD.bazel                           fixed_array_test.cc    inlined_vector_benchmark.cc              node_hash_map.h
btree_map.h         CMakeLists.txt                        flat_hash_map.h        inlined_vector_exception_safety_test.cc  node_hash_map_test.cc
btree_set.h         fixed_array_benchmark.cc              flat_hash_map_test.cc  inlined_vector.h                         node_hash_set.h
btree_test.cc       fixed_array_exception_safety_test.cc  flat_hash_set.h        inlined_vector_test.cc                   node_hash_set_test.cc
btree_test.h        fixed_array.h                         flat_hash_set_test.cc  internal

```

"
Optional train_df input for fit,google-research/google-research,2020-07-13 16:41:10,0,,321,655990404,"Should the referenced line of code batch the train data opposed to the validation data ?

https://github.com/google-research/google-research/blob/bc0310db2874b05e08fa285697c209135d9096cc/tft/libs/tft_model.py#L1130"
Specify which command line args you support in google-research - ROUGE,google-research/google-research,2020-07-12 22:11:42,0,,320,655486305,"Thanks for this native python implementation of ROUGE! 
https://github.com/google-research/google-research/tree/master/rouge

While it is great that you specify in your README that you do not support all the command line arguments supported by the original perl package rouge1.5.5,  it will greatly benefit if you also say exactly which ones you support and which ones you do not and if I can turn on/off specific arguments.

thanks!"
[depth_from_video_in_the_wild]Bad depth  results on custom data.,google-research/google-research,2020-07-11 13:34:26,3,,319,655209970,"Hello great work @gariel-google and team!
I tried training the model without any pretrained weights on about 5300 images for about 35 epochs and also provided possibly mobile masks for the moving objects as shown
![0000001418-fseg](https://user-images.githubusercontent.com/31023599/87225782-f4f96e00-c3ac-11ea-97bf-6c7ebc794771.png)

I got the following loss graph
![lossdepth_from](https://user-images.githubusercontent.com/31023599/87225705-93d19a80-c3ac-11ea-8101-0e991c9c890f.png)
But the result of depth inference is :
![0000001419](https://user-images.githubusercontent.com/31023599/87225164-b2359700-c3a8-11ea-91b4-f47502bd5e4e.png)
The result without training and directly infering on cityscapes pretrained weights provided in the repo is little better.
![0000001418](https://user-images.githubusercontent.com/31023599/87225199-e7da8000-c3a8-11ea-8922-ac5293e6f19b.png)
I took the inference code from struct2depth and replaced their model with depth_from_video_in_the_wild's model
When I plot the egomotion with the trained model, it looks ok.
Can I know what might have gone wrong while training?
Any suggestion is greatly appreciated.
Thanks.
 "
这个怎么用？,google-research/google-research,2020-07-09 09:16:33,1,,317,653903482,
AVX2 usage in scannn,google-research/google-research,2020-07-07 16:30:05,0,,314,652459606,"Dear scannn authors, 

I would like to bring your attention on recent work brining SIMD distance evaluation to FAISS. The work includes methods for doing SIMD lookup for larger tables thus allowing more precise distance (than with 16-values tables as done your work) while keeping the advantage of SIMD using recently-released AVX512-capable processors. I believe the method could be applied to scannn similarly to how QuickerADC extends QuickADC. 

See https://github.com/nlescoua/faiss-quickeradc for source code.

The corresponding paper is available as preprint: IEEE Transaction on Pattern Analysis and Machine Intelligence
https://ieeexplore.ieee.org/document/8896060
https://arxiv.org/abs/1812.09162

Best regards"
Yeah I noticed that Pull Request merge too! only 2 lines seemed to have been changed in that merge.,google-research/google-research,2020-06-29 15:55:08,2,,313,647489072,"Yeah I noticed that Pull Request merge too! only 2 lines seemed to have been changed in that merge.
However, for now I'm using TF 1.14 and things run fine.

_Originally posted by @SachitNayak in https://github.com/google-research/google-research/issues/284#issuecomment-636922075_"
 module 'tensorflow' has no attribute 'compat',google-research/google-research,2020-06-28 07:17:49,3,,311,646860008, I have upgraded tensorflow to 2.0. why would it go wrong? module 'tensorflow' has no attribute 'compat'？
"[Astronet] ValueError: could not broadcast input array from shape (4,43) into shape (4,39)",google-research/google-research,2020-06-23 23:24:49,5,,309,644209836,"### 1. The entire URL of the file you are using

https://github.com/google-research/exoplanet-ml/tree/master/exoplanet-ml/astronet

### 2. Describe the bug

I'm trying to create training, validation and test set files in ""Process Kepler Data"" section.

```
# Preprocess light curves into sharded TFRecord files using 5 worker processes.
bazel-bin/astronet/data/generate_input_records \
  --input_tce_csv_file=${TCE_CSV_FILE} \
  --kepler_data_dir=${KEPLER_DATA_DIR} \
  --output_dir=${TFRECORD_DIR} \
  --num_worker_processes=5
```

The process seems to start without any issues and it starts to create tfrecords, but after a while I get this error:
**""ValueError: could not broadcast input array from shape (4,43) into shape (4,39)""**

### 3. Steps to reproduce

**1. Setup the environment**
The following libraries are required:
tensorflow==1.15
tensorflow-probability==0.8
pandas
numpy
scipy
astropy
pydl
absl-py

Ensure that test and build pass

```
cd exoplanet-ml  # Bazel must run from a directory with a WORKSPACE file
bazel test astronet/... astrowavenet/... light_curve/... tf_util/... third_party/...
```

`bazel build astronet/...`

**2. Download Kepler data**

First, download the DR24 TCE Table in CSV format from the [NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce) and ensure the following columns are selected:

-    rowid: Integer ID of the row in the TCE table.
-    kepid: Kepler ID of the target star.
-   tce_plnt_num: TCE number within the target star.
-   tce_period: Period of the detected event, in days.
-   tce_time0bk: The time corresponding to the center of the first detected event in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days.
-  tce_duration: Duration of the detected event, in hours.
-  av_training_set: Autovetter training set label; one of PC (planet candidate), AFP (astrophysical false positive), NTP (non-transiting phenomenon), UNK (unknown).

```
# Filename containing the CSV file of TCEs in the training set.
TCE_CSV_FILE=""${HOME}/astronet/dr24_tce.csv""

# Directory to download Kepler light curves into.
KEPLER_DATA_DIR=""${HOME}/astronet/kepler/""

# Generate a bash script that downloads the Kepler light curves in the training set.
python astronet/data/generate_download_script.py \
  --kepler_csv_file=${TCE_CSV_FILE} \
  --download_dir=${KEPLER_DATA_DIR}

# Run the download script to download Kepler light curves.
./get_kepler.sh
```
### NOTE: the following script take up about 90 GB.

**3. Process Kepler data**

```
# Directory to save output TFRecord files into.
TFRECORD_DIR=""${HOME}/astronet/tfrecord""

# Preprocess light curves into sharded TFRecord files using 5 worker processes.
bazel-bin/astronet/data/generate_input_records \
  --input_tce_csv_file=${TCE_CSV_FILE} \
  --kepler_data_dir=${KEPLER_DATA_DIR} \
  --output_dir=${TFRECORD_DIR} \
  --num_worker_processes=5
```

### 4. Expected behavior

When the script finishes you will find 8 training files, 1 validation file and 1 test file in TFRECORD_DIR. The files will match the patterns train-0000?-of-00008, val-00000-of-00001 and test-00000-of-00001 respectively.

### 5. Additional context

**Traceback (most recent call last):**

File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
result = (True, func(*args, **kwds))
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 164, in _process_file_shard
example = _process_tce(tce)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 144, in _process_tce
time, flux = preprocess.process_light_curve(all_time, all_flux)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/preprocess.py"", line 72, in process_light_curve
spline = kepler_spline.fit_kepler_spline(all_time, all_flux, verbose=False)[0]
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/third_party/kepler_spline/kepler_spline.py"", line 321, in fit_kepler_spline
verbose=verbose)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/third_party/kepler_spline/kepler_spline.py"", line 216, in choose_kepler_spline
time, flux, bkspace=bkspace, maxiter=maxiter)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/third_party/kepler_spline/kepler_spline.py"", line 104, in kepler_spline
curve = bspline.iterfit(time[mask], flux[mask], bkspace=bkspace)[0]
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/pydl/pydlutils/bspline.py"", line 639, in iterfit
x2=x2work)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/pydl/pydlutils/bspline.py"", line 189, in fit
errb = cholesky_band(alpha, mininf=min_influence)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/pydl/pydlutils/bspline.py"", line 491, in cholesky_band
L[:, 0:n] = lower
ValueError: could not broadcast input array from shape (4,43) into shape (4,39)
""""""

**The above exception was the direct cause of the following exception:**

Traceback (most recent call last):
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 256, in
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/absl/app.py"", line 299, in run
_run_main(main, args)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
sys.exit(main(argv))
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 248, in main
async_result.get()
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/multiprocessing/pool.py"", line 657, in get
raise self._value
**ValueError: could not broadcast input array from shape (4,43) into shape (4,39)**

### 6. System information

I'm testing the model in conda environment

-    OS Platform and Distribution: ('CentOS Linux', '7.6.1810', 'Core')
-    TensorFlow installed from : source (https://anaconda.org/conda-forge/tensorflow)
-   TensorFlow version : 1.15.0
-    Python version: 3.7.7
-    Bazel version : 2.1.0

Thanks.
"
Skipgram metric in ROUGE,google-research/google-research,2020-06-21 15:32:00,0,,307,642585269,Can you please add Skipgram metric in ROUGE metrics
"I meet Error when run code in the project ""pairwise_fairness""",google-research/google-research,2020-06-21 11:49:38,0,,305,642547945,
Links are down in fastconvnets,google-research/google-research,2020-06-18 03:25:01,4,,299,640877503,"Hi, 
It seems that all links in the [directory](https://github.com/google-research/google-research/blob/master/fastconvnets) are down.
"
[TFT] How could I use attention weights?,google-research/google-research,2020-06-17 04:15:10,0,,298,640126965,"I got attention_weights using model.get_attention(train) after model.fit() as below.

total_time_step = 84 + 30
num_encoder_step = 84
num_static = 2
num_known_input = 10
num_observed_input = 2

For visualizing variable importance and temporal patterns,

what part should I use?

Plus, please explain some more.

For example, if I have to use attention_weights['decoder_self_attn'][0,-1,:,:],

what is the meaning of that [114,114]?

<img width=""364"" alt=""attn_weights_example"" src=""https://user-images.githubusercontent.com/49193062/84853629-8faaa980-b09a-11ea-8202-71e83aff08b1.PNG"">
"
mobilebert finetune has no unique_ids in result,google-research/google-research,2020-06-12 21:48:42,2,,295,638019451,"when I print the results, it has the following and no 'unique_ids'
dict_keys(['start_logits', 'end_logits'])
This has been run on CPU. 


INFO:tensorflow:prediction_loop marked as finished
I0612 14:17:58.848829 139625204832064 error_handling.py:101] prediction_loop marked as finished
Traceback (most recent call last):
(text omitted here ...) 
  File ""run_squad.py"", line 1439, in main
    unique_id = int(result[""unique_ids""])
KeyError: 'unique_ids'

command to use:
python3 run_squad.py \
  --bert_config_file=uncased_L-2_H-128_A-2/bert_config.json \
  --data_dir=${DATA_DIR} \
  --do_lower_case \
  --do_predict \
  --do_train \
  --doc_stride=128 \
  --init_checkpoint=${INIT_CHECKPOINT}/bert_model.ckpt \
  --learning_rate=4e-05 \
  --max_answer_length=30 \
  --max_query_length=64 \
  --max_seq_length=384 \
  --n_best_size=20 \
  --num_train_epochs=5 \
  --output_dir=${OUTPUT_DIR} \
  --predict_file=/my_path_to/dev-v1.1.json \
  --train_batch_size=32 \
  --train_file=/my_path_to/train-v1.1.json \
  --vocab_file=${INIT_CHECKPOINT}/vocab.txt \
  --warmup_proportion=0.1\
  --export_dir=export_model
"
meaning by rouge score,google-research/google-research,2020-06-11 18:33:03,2,,294,637235842,"Hi, I am new here. I want to ask a silly question:

This is the rough score I got:
ROUGE_1: 
AggregateScore(low=Score(precision=0.3997101114187033, recall=0.5115655746404962, fmeasure=0.43832121374077704), mid=Score(precision=0.40219078493377414, recall=0.514241835608697, fmeasure=0.4406308217147367), high=Score(precision=0.40483246876750517, recall=0.516888812840918, fmeasure=0.4430230464355599))

The BART paper report rouge_1 on cnn/daily dataset for 44.16. Does this score mean measure? What is meaning about ""low"", ""mid"" and ""high""?
"
Accelerate of inference for transformer weight prunning,google-research/google-research,2020-06-11 09:25:43,0,,293,636863816,"The weight prunning of transformer achieves high performance. But we also focus on the speed of inference. Whether the inference has also accelerated?

Looking forward to your reply.
Thanks!"
New,google-research/google-research,2020-06-07 08:27:56,1,,290,633190643,
MobileBERT finetuning hyperparameters,google-research/google-research,2020-06-01 23:57:26,4,,287,628802648,"In the MobileBERT paper, it's mentioned that the optimization hyperparameters (batch sizes, learning rates, number of epochs) are searched to choose the model. What are the optimal hyperparameters for the available tasks? "
Temporal Fusion Transformers ( tft ) AttributeError: module 'tensorflow' has no attribute 'ConfigProto'  incompatibility with tf 2.x,google-research/google-research,2020-05-27 04:20:18,8,,284,625364181,"I encountered the following error when running a sample program of Temporal Fusion Transformers (tft) see [here](https://aihub.cloud.google.com/u/0/p/products%2F9f39ad8d-ad81-4fd9-8238-5186d36db2ec)

Specifically this line seems to cause it:

```
import tensorflow as tf
from libs.tft_model import TemporalFusionTransformer

tf_config = utils.get_default_tensorflow_config(tf_device=""gpu"", gpu_id=0) 
```

and the stacktrace is:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-23-9367d03d4948> in <module>()
      3 
      4 # Specify GPU usage
----> 5 tf_config = utils.get_default_tensorflow_config(tf_device=""gpu"", gpu_id=0)
      6 tf.reset_default_graph()
      7 with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:

/content/repo/tft/libs/utils.py in get_default_tensorflow_config(tf_device, gpu_id)
    151     print('Selecting GPU ID={}'.format(gpu_id))
    152 
--> 153     tf_config = tf.ConfigProto(log_device_placement=False)
    154     tf_config.gpu_options.allow_growth = True
    155 

AttributeError: module 'tensorflow' has no attribute 'ConfigProto' 

```

According to my understanding this is an issue due to incompatibility with tensorflow 2.x

I haven't yet tried the stackoverflow workaround mentioned [here](https://stackoverflow.com/questions/56127592/attributeerror-module-tensorflow-has-no-attribute-configproto)

Linking a related issue discussing migration to TF2 #260 

PS: I could work on this issue if its assigned to me - just migrating the ConfigProto part in the tft/libs/utils.py file ( not possibly migrating entire tft codebase to TF2)
"
How to generate Rico SCA tfrecord (seq2act project),google-research/google-research,2020-05-26 09:46:16,10,,283,624750715,"Thank you for sharing your wonderful OSS.

https://github.com/google-research/google-research/tree/master/seq2act/data_generation
```
sh seq2act/data_generation/create_rico_sca.sh
```

I'm trying to generate a dataset using the above steps and commands.
However, the absence of these two files in data_generation directory causes an error during the script execution.

```
--vocab_file=${PWD}""/seq2act/data_generation/lower_case_vocab"" \
--input_candiate_file=${PWD}""/seq2act/data_generation/input_candidate_words.txt"" \
```
How did you create these two files(vocab file and input_candidate_words file)?
If you can't seem to release the two files yet, I'm going to try to create my own.(if possible)
I would appreciate it if you could tell me."
Implement the distributed version of dql_grasping,google-research/google-research,2020-05-24 15:03:13,0,,282,623882121,"Hello All,

I am using TensorFlow-gpu 1.15.2 python3 on Ubuntu 18.04. I noted that the dql_grasping was originally made on a distributed system but I cannot find the corresponding pipeline code. Hence I decide to make one by following [this tutorial](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md). My purpose is to run the distributed version of dql_grasping on a machine with 1 CPU and multiple GPUs (this can be extended later). However, I got the following error regarding the initialization some slim variables. Note that these TruncatedNormal variables are defined in dql_grasping/tf_modules.py

If anyone happens to have similar issue, or already have the working code of running drl_grasping in a distributed system, please let me know. Thanks!
  
File ""/home/yy/DRL/google-research-master/dql_grasping/run_train_collect_eval.py"", line 144, in <module>
    app.run(main)
  File ""/home/yy/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/yy/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/yy/DRL/google-research-master/dql_grasping/run_train_collect_eval_parallel.py"", line 124, in main
    worker_device=worker_device)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/yy/DRL/google-research-master/dql_grasping/train_collect_eval.py"", line 127, in train_collect_eval
    policy = policy_class()
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/yy/DRL/google-research-master/dql_grasping/policies.py"", line 178, in __init__
    self._greedy_policy = greedy_policy_class()
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/yy/DRL/google-research-master/dql_grasping/policies.py"", line 237, in __init__
    q_func, state_shape, use_gpu=use_gpu, checkpoint=checkpoint)
  File ""/home/yy/DRL/google-research-master/dql_grasping/policies.py"", line 107, in __init__
    self._sess.run(init_op)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal: Could not satisfy explicit device specification '' because the node node q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal (defined at /DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) placed on device Device assignments active during op 'q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal' creation:
...
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/job:ps/task:0/device:CPU:0' assigned_device_name_='' resource_device_name_='/job:ps/task:0/device:CPU:0' supported_device_types_=[GPU, CPU, XLA_CPU, XLA_GPU] possible_devices_=[]
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
Mul: GPU CPU XLA_CPU XLA_GPU 
TruncatedNormal: GPU CPU XLA_CPU XLA_GPU 
Add: GPU CPU XLA_CPU XLA_GPU 
VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  q_func/Conv/weights/Initializer/truncated_normal/shape (Const) 
  q_func/Conv/weights/Initializer/truncated_normal/mean (Const) 
  q_func/Conv/weights/Initializer/truncated_normal/stddev (Const) 
  q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal (TruncatedNormal) 
  q_func/Conv/weights/Initializer/truncated_normal/mul (Mul) 
  q_func/Conv/weights/Initializer/truncated_normal (Add) 
  q_func/Conv/weights (VarHandleOp) /job:ps/task:0/device:CPU:0
  q_func/Conv/weights/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:worker/task:0
  q_func/Conv/weights/Assign (AssignVariableOp) /job:ps/task:0/device:CPU:0
  q_func/Conv/weights/Read/ReadVariableOp (ReadVariableOp) /job:ps/task:0/device:CPU:0
  q_func/Conv/Conv2D/ReadVariableOp (ReadVariableOp) /job:ps/task:0/device:CPU:0
  save/AssignVariableOp_2 (AssignVariableOp) /job:ps/task:0/device:CPU:0

	 [[node q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal (defined at /DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]Additional information about colocations:No node-device colocations were active during op 'q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal' creation.
Device assignments active during op 'q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal' creation:
  with tf.device(None): </home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1535>
  with tf.device(): </home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/contrib/framework/python/ops/variables.py:268>
  with tf.device(_ReplicaDeviceChooser.device_function</home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/training/device_setter.py, 99>): </home/yy/DRL/google-research-master/dql_grasping/train_collect_eval.py:126>
"
"MobileBERT for QQP, RTE, STS tasks",google-research/google-research,2020-05-21 00:27:32,0,,278,622165080,"I am trying to replicate the mobileBERT paper results but the current code is missing support for QQP, RTE, and STS-B. Are there plans to update the code to include these tasks? "
It seems to have problems when I run the #run_pretraining.py#,google-research/google-research,2020-05-18 02:35:58,3,,274,619865644,"Hello, how can I solve this problem:
**The requirement.txt says I need to install tensorflow 1.15 but deterministic is an argument of TF 2.x** 

`Traceback (most recent call last):
  File ""run_pretraining.py"", line 983, in <module>
    tf.app.run()
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""run_pretraining.py"", line 863, in main
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_distill_steps)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3035, in train
    rendezvous.raise_errors()
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 136, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1188, in _train_model_default
    input_fn, ModeKeys.TRAIN))
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1025, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2987, in _call_input_fn
    return input_fn(**kwargs)
  File ""run_pretraining.py"", line 747, in input_fn
    deterministic=(not is_training))
TypeError: interleave() got an unexpected keyword argument 'deterministic'
`"
What,google-research/google-research,2020-05-17 08:52:53,1,,272,619647497,"Excuse me, I am confused about the relationship between this open source project and other open source projects under github.com/google-research organization. In other words, what kind of open source projects will be included in this large open source project? What kind of open source projects will be opened independently? Thanks."
Depth map to Point Cloud map,google-research/google-research,2020-05-14 05:27:13,0,,268,617935292,"I compared the results of Depth from wild and PSMNet.
The size of the 3D point Cloud detected by 'Depth from video in wild' was much larger than the size of the 3D Point Cloud of PSMNet.
Like this
[PSMNet]
![LiDAR](https://user-images.githubusercontent.com/33896710/81895490-4247a200-95ed-11ea-8b67-01f2786a7429.PNG)
[Depth from video in wild]
![Google](https://user-images.githubusercontent.com/33896710/81895498-4a074680-95ed-11ea-8d41-d0c18f58a4cd.PNG)

<First Question>
Does the depth image detected from depth from 'Depth from video in wild' contain the depth value or disparity value.

<Second Question>
I know that 'Depth from video in wild' learning intrinsic, but do this still detect depth values relatively?
The performance of intrinsic learning is good, so it seems that i can get a value similar to the absolute depth value.

"
Can BAM be used together with ELECTRA?,google-research/google-research,2020-05-10 11:48:23,0,,265,615374792,"Hi, I'm interested in using the multitask BAM network, where both the multitask model and the teacher models are ELECTRA. The question I have is since one of the tasks I need done is MLM while the other is classification, and they're done by different models (in ELECTRA the generator and discriminator respectively), can they be used as teachers for BAM? Because as I understand from the paper the teachers must have the exact same model archtecture as each other and as the student, and in this case the generator and discriminator are different models.

Thank you!"
"[ChemGraph MolDQN] LogP is not discounted as a reward, why?",google-research/google-research,2020-05-10 07:37:52,0,,263,615336467,"In the demonstration file at https://github.com/google-research/google-research/blob/master/mol_dqn/chemgraph/optimize_logp.py

Line 46 reads:
`return molecules.penalized_logp(molecule)`

However, this reward is not discounted for timesteps, unlike the same demonstration file for QED
https://github.com/google-research/google-research/blob/master/mol_dqn/chemgraph/optimize_qed.py

```
qed = QED.qed(molecule)
return qed * self.discount_factor ** (self.max_steps - self.num_steps_taken)
```
Was this an oversight, or executed on purpose? 

Thank you on advance"
[depth_from_video_in_the_wild]BUG:std::bad_alloc,google-research/google-research,2020-05-10 02:03:54,0,,262,615297888,"When I tried to run the code for the /data_example,the following bug showed up.Can you help me please?
-----------------------------------------------------------------------------------------------------------
2020-05-10 09:38:46.241554: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Fatal Python error: Aborted

Thread 0x00007fe4fd7fe700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe4fdfff700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe504ff9700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe5057fa700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe505ffb700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe5067fc700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe506ffd700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe5077fe700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe580f3f700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/queue.py"", line 164 in get
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 159 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe619adc740 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319 in _run_fn
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334 in _do_call
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328 in _do_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152 in _run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929 in run
  File ""train.py"", line 195 in _train
  File ""train.py"", line 155 in main
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""train.py"", line 236 in <module>
Aborted (core dumped)"
Please provide / update code bases  to TF2 atleast. ,google-research/google-research,2020-05-07 10:48:42,3,,260,613958931,"You guys are creating a huge amount of great libraries and codebases with great explanations. But TF1 is really old and tired with it's alien syntax.

I'm mostly a pyTorch user but TF2 is really fine and dandy with how similiar it is to pyTorch and normal python syntax. Since most researchers basically use pyTorch its really hard to expect a new researchers to learn read old TF1 codes.

https://github.com/google-research/google-research/tree/master/dql_grasping
https://github.com/tensorflow/tensor2tensor

2 example of great libraries/code bases stuck with TF1 forever.

Please consider them upgrading to TF2"
Real depth or disparity ?,google-research/google-research,2020-05-06 07:52:04,0,,259,613120602,"So I wanted to infer real depth from the network, and the `inference_depth()` function in `model.py` did give me some values for my test images.
1. I am unable to understand if those values are real depth or just disparity? 
2. If real depth then is the unit in meters or something else?
3. And to what range of real depth (in meters) can the n/w infer?"
"For autoML-zero, why in run_baseline.sh the search tasks and select tasks have the same held-out-pairs??",google-research/google-research,2020-04-20 07:34:11,0,,248,603011119,"For autoML-zero, why in run_baseline.sh the search tasks and select tasks have the same held-out-pairs?"
Unprocessing: link broken,google-research/google-research,2020-04-17 00:29:06,0,,246,601620518,"In the [unprocessing](https://github.com/google-research/google-research/blob/master/unprocessing/README.md) sub-repository (Section ""Evaluation on Different Real Data""), the link to the noise calibration script is broken. Could you please fix it? 

Thank you."
TFLite for speech_embedding?,google-research/google-research,2020-04-09 09:49:00,5,,242,597160862,"@kevinkilgour 

I tried to convert the model provided at https://tfhub.dev/google/speech_embedding/1 to TFLite but unfortunately failed.

### Setup:
- tensorflow 2.2.0rc2
- tensorflow-hub 0.8.0

### Conversion
I tried the following:
1. Manually downloaded the model from https://tfhub.dev/google/speech_embedding/1?tf-hub-format=compressed
2. ```
   converter = tf.lite.TFLiteConverter.from_saved_model(
    ""speech_embedding"",
    tags=[]
   )
   tflite_model = converter.convert()
   ```
### Error
I get the following error:
```
ValueError                                Traceback (most recent call last)
<ipython-input-90-2fa2b3d63f8a> in <module>
      3     tags=[]
      4 )
----> 5 tflite_model = converter.convert()
      6 
      7 # converter.experimental_new_converter = True

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)
    481               ""None is only supported in the 1st dimension. Tensor '{0}' has ""
    482               ""invalid shape '{1}'."".format(
--> 483                   _get_tensor_name(tensor), shape_list))
    484         elif shape_list and shape_list[0] is None:
    485           # Set the batch size to 1 if undefined.

ValueError: None is only supported in the 1st dimension. Tensor 'samples' has invalid shape '[None, None]'.
```

I tried as well to enable the new converter (`converter.experimental_new_converter = True`) but that didn't help either.

### Questions
1. Do you guys have a tflite version of the model that you would be willing to publish to tfhub?
2. Am I doing something wrong with the conversion? If so, please let me know.
3. Is there something you guys would need to change in the model to support TFLite? If so, would you be willing to perform that necessary change?

Thanks a lot for your help and the great work you have done with the speech_embedding model. Much appreciated.

"
TFT - Multi-class forecasting,google-research/google-research,2020-04-02 09:42:42,1,,240,592495964,"In the TFT branch, is there a reason why the multi-class is not activated? After looking at the code, it seems that the model itself can handle it. Thus, beside removing the lines throwing error and using target_col instead of [target_col] in few places, as the target column is now an array, I haven't seen much change to do to handle multi-class."
rouge scorer with stemmer may give different results from PERL implementation,google-research/google-research,2020-04-02 00:49:43,0,,239,592286307,"For the following reference/summary pair:

```
ref = ""weald basin estimated to contain 100billion barrels of oil , new report says . \n uk oil & gas investments claims the site could supply 30 % of uk 's needs . \n if the maximum amount of oil is extracted it will be worth # 600billion . \n but some experts warn the site in sussex will be difficult to exploit .""

summ = ""trillions of pounds worth of oil - as much as the entire north sea fields - lies beneath an area of england dubbed 'britain's dallas', it was claimed yesterday. \n analysis suggests there is up to 100billion barrels of the fossil fuel under the home counties. \n the potential goldmine in the weald basin, across surrey, sussex, hampshire and kent, could meet up to a third of britain's oil demand within 15 years, according to the consortium exploring the area. \n uk oil & gas investments say they have discovered 100billion barrels worth of oil reserves in the weald basin, near gatwick airport .""

from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
scores = scorer.score(ref, summ)

# scores
# {'rouge1': Score(precision=0.2727272727272727, 
# recall=0.5192307692307693, 
# fmeasure=0.3576158940397351)}

# perl implementation results:
# 1 ROUGE-1 Average_R: 0.50000 (95%-conf.int. 0.50000 - 0.50000)
# 1 ROUGE-1 Average_P: 0.26263 (95%-conf.int. 0.26263 - 0.26263)
# 1 ROUGE-1 Average_F: 0.34437 (95%-conf.int. 0.34437 - 0.34437)
```

Looking at the unigrams extracted from each, for the PERL implementation, the stemmer will stem say -> sai and says -> say, while the nltk stemmer (nltk==3.4.5) will stem both cases to 'say' and thus have a different unigram intersection. "
Little typo on the TFT branch,google-research/google-research,2020-04-01 12:49:49,0,,238,591890934,"on readme.md, Customising Scripts for New Datasets section there's _my_serach_iterations_ instead of _my_search_iterations_"
About Amazon2m dataset,google-research/google-research,2020-03-29 21:52:55,2,,237,589894497,"> Hi,
> 
> We just release the script/data for generating Amazon2M.
> Please download it [here](http://web.cs.ucla.edu/~chohsieh/data/Amazon2M.tar.gz), in which you can see how this data set is processed from the raw metadata.

Hello, the amazon 3M data on this [website](http://manikvarma.org/downloads/XC/XMLRepository.html) doesn't include metadata.json. It's not in both ""BoW Features"" and ""Raw text""

_Originally posted by @abcbdf in https://github.com/google-research/google-research/issues/69#issuecomment-605705200_"
How can I get 96.6 for Reddit dataset?,google-research/google-research,2020-03-27 18:11:17,1,,236,589315480,"Hi google research team, I ran the file run_reddit.sh provided by you without any modification and the test accuracy is 0.96172 on Reddit dataset, which has much less than the result posted in the paper (0.996). I also notice than in the paper, you said ""For Reddit, a 4-layer GCN with 128 hidden units is used"" while in .sh file, the number of hidden units is 512 as the default value. I'm wondering if it is the reason and I'm doing experiment to verify this guess."
About ClusterGCN,google-research/google-research,2020-03-24 12:31:05,3,,234,586925457,"Hello, In your paper, It is mentioned that you used pytorch, but this is a Tensorflow implementation. Could you please share the pytorch code, I am not familiar with Tensorflow.Many thanks!"
Stand-Alone Self-Attention in Vision Models: Code??,google-research/google-research,2020-03-21 14:52:16,5,,232,585514530,"When will be the code used in the paper ""Stand-Alone Self-Attention in Vision Models"" be available? The code link in the paper leads to [this page](https://github.com/google-research/google-research/tree/master/standalone_self_attention_in_vision_models) which just has a readme updated 5 months ago. "
frechet_audio_distance: Instructions not working,google-research/google-research,2020-03-16 15:52:53,6,,227,582406773,"Hi. As is the instructions for frechet_audio_distance give errors:

```
(fad) [jon@jon-thinkpad google-research]$ python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats
.... Tensorflow nagging ...
Traceback (most recent call last):
  File ""/home/jon/.conda/envs/fad/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/jon/.conda/envs/fad/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/jon/work/senselab/soundquality/google-research/frechet_audio_distance/create_embeddings_main.py"", line 26, in <module>
    from frechet_audio_distance import create_embeddings_beam
  File ""/home/jon/work/senselab/soundquality/google-research/frechet_audio_distance/create_embeddings_beam.py"", line 36, in <module>
    from frechet_audio_distance.audioset_model import AudioSetModel
  File ""/home/jon/work/senselab/soundquality/google-research/frechet_audio_distance/audioset_model.py"", line 25, in <module>
    from tensorflow_models.audioset import mel_features
ImportError: cannot import name 'mel_features'
```
This seems to be due to a change in Tensorflow models, where the vggish directory was moved.
https://github.com/tensorflow/models/commit/4079c5d9693142a406f6ff392d14e2034b5f496d#diff-bdac73c3c32a5ba3834aa4ce2b8b345e

Using an earlier version seems to work fine, however then one also have to use Tensorflow 1.15
```
svn export https://github.com/tensorflow/models/tags/v1.13.0/research/audioset tensorflow_models/audioset

pip install tensorflow==1.15
```

Btw it also seems that Apache Beam supports Python3 now. Right now frechet_audio_distance is however not compatible. At least there are some trivial errors like use of `xrange`.

Would a PR which updates to Python3 and TensorFlow 2.x be welcomed? "
try the dome but error  as follow why ?,google-research/google-research,2020-03-15 05:29:19,3,,225,581526626,"ERROR: /home/fuxiai/google-research/automl_zero/BUILD:286:1: C++ compilation of rule '//:fec_hashing' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 72 argument(s) skipped)
"
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 28 (FlexBatchMatMul) failed to prepare.,google-research/google-research,2020-03-13 07:35:34,7,,220,580423406,"Hi big bro
When I run kws_streaming.model_train_eval.py by **CNN model** .It happend the error.
Environment:
Ubuntu 18.0.4
Tensorflow 2.1.0


Skipping registering GPU devices...
2020-03-13 15:24:52.786265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-13 15:24:52.786269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-13 15:24:52.786273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-13 15:24:53.758005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-13 15:24:53.758030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      
Traceback (most recent call last):
  File ""/home/mi/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/mi/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/mi/ASR/google-research/kws_streaming/train/model_train_eval.py"", line 548, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/mi/ASR/google-research/kws_streaming/train/model_train_eval.py"", line 185, in main
    test.tflite_non_stream_model_accuracy(flags, folder, fname)
  File ""/home/mi/ASR/google-research/kws_streaming/train/test.py"", line 453, in tflite_non_stream_model_accuracy
    interpreter.invoke()
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py"", line 493, in invoke
    self._interpreter.Invoke()
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 113, in Invoke
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 28 (FlexBatchMatMul) failed to prepare.
"
Incorrect shebang lines,google-research/google-research,2020-03-11 06:03:23,0,,216,579029680,"Throughout the repo there are a variety of executable shell scripts with shebang/interpreter lines (`#!`) that are not at the start of the relevant files (usually underneath a copyright notice). These are not parsed correctly and will be ignored as the shebang is only valid as the first two bytes of the file, see [`linux/fs/binfmt_script.c:load_script`](https://github.com/torvalds/linux/blob/f35111a946548e3b34a55abbad3e9bacce6cb10f/fs/binfmt_script.c#L42) as an example implementation.

It is likely that this was not noticed due to the majority of shebang lines specifying `/bin/bash` as the interpreter: if executing the scripts under bash this would behave mostly the same. However, a few scripts specify `-e` and `-u` which will get swallowed.

Files with shebang lines in the wrong position can be found using the following invocation:
```
find . -type f -executable -exec sh -c 'sed -n -e \'2,${/#!/q1}\' {} || echo {}' \; | sort
```
when executed from the repository root. As of 5e864806, this lists:
```
./abps/run.sh
./action_gap_rl/run.sh
./algae_dice/run.sh
./attribution/run.sh
./automl_zero/run_demo.sh
./automl_zero/run_integration_test_linear.sh
./automl_zero/run_integration_test_nonlinear.sh
./automl_zero/run_integration_tests.sh
./axial/run.sh
./bam/run.sh
./behavior_regularized_offline_rl/brac/run_bcq.sh
./behavior_regularized_offline_rl/brac/run_bc.sh
./behavior_regularized_offline_rl/brac/run_collect_data.sh
./behavior_regularized_offline_rl/brac/run_dual.sh
./behavior_regularized_offline_rl/brac/run_primal.sh
./behavior_regularized_offline_rl/brac/run_train_online.sh
./behavior_regularized_offline_rl/run.sh
./bitempered_loss/run.sh
./cfq/run_experiment.sh
./cfq/run.sh
./cluster_gcn/run_ppi.sh
./cluster_gcn/run_reddit.sh
./cnn_quantization/run.sh
./cold_posterior_bnn/run_resnet_experiment.sh
./cold_posterior_bnn/run.sh
./dac/run_evaluation_worker.sh
./dac/run_training_worker.sh
./dataset_analysis/run.sh
./dble/run.sh
./dense_representations_for_entity_retrieval/parse_wikinews.sh
./dense_representations_for_entity_retrieval/run.sh
./depth_from_video_in_the_wild/run.sh
./dql_grasping/run.sh
./dreg_estimators/run.sh
./dual_pixels/run.sh
./dvrl/run.sh
./edward2_autoreparam/run.sh
./eeg_modelling/viewer.sh
./eim/run.sh
./evanet/run.sh
./experience_replay/run.sh
./explaining_risk_increase/run.sh
./extrapolation/run.sh
./generalized_rates/run.sh
./genomics_ood/run.sh
./graph_compression/run.sh
./graph_embedding/watch_your_step/run.sh
./group_agnostic_fairness/run.sh
./hmc_swindles/run.sh
./hmc_swindles/scripts/fetch_datasets.sh
./hyperbolic_discount/run.sh
./igt_optimizer/run.sh
./interpretability_benchmark/run.sh
./large_margin/run.sh
./learnreg/run.sh
./meta_learning_without_memorization/pose_code/run.sh
./meta_reward_learning/semantic_parsing/run.sh
./meta_reward_learning/textworld/run.sh
./moew/run.sh
./moment_advice/run.sh
./m_theory/run.sh
./neutra/run.sh
./nigt_optimizer/run.sh
./nopad_inception_v3_fcn/run.sh
./norml/run.sh
./opt_list/run.sh
./playrooms/rooms/playroom/textures/download.sh
./poly_kernel_sketch/run.sh
./probabilistic_vqvae/run.sh
./pruning_identified_exemplars/run.sh
./psycholab/run.sh
./recursive_optimizer/run.sh
./rl4circopt/run.sh
./rllim/run.sh
./robust_loss/run.sh
./rouge/run.sh
./sm3/run.sh
./soft_sort/run.sh
./solver1d/run.sh
./stacked_capsule_autoencoders/eval_mnist_coupled.sh
./stacked_capsule_autoencoders/eval_mnist.sh
./stacked_capsule_autoencoders/run_constellation.sh
./stacked_capsule_autoencoders/run_mnist.sh
./stacked_capsule_autoencoders/run.sh
./stacked_capsule_autoencoders/setup_virtualenv.sh
./state_of_sparsity/run.sh
./storm_optimizer/run.sh
./sufficient_input_subsets/run.sh
./tabnet/run.sh
./tcc/dataset_preparation/download_pouring_data.sh
./tcc/run.sh
./tf_trees/run.sh
./tft/run.sh
./towards_gan_benchmarks/run.sh
./truss_decomposition/compile.sh
./truss_decomposition/run.sh
./uncertainties/scripts/train_local.sh
./uq_benchmark_2019/run.sh
./video_structure/run.sh
./wiki_split_bleu_eval/run.sh
``` "
when are you going to release ELECTRA,google-research/google-research,2020-03-06 14:21:29,0,,213,576963008,Thanks a lot!
Code for „Pairwise Fairness for Ranking and Regression“,google-research/google-research,2020-03-05 07:27:13,1,,212,576046394,"Hi everyone,

I am currently reading the paper „Pairwise Fairness for Ranking and Regression“ and wanted to run the experiments. 

Unfortunately in the repository I can only find a notebook for running a toy example. The paper claims that the code for the experiments is available so I am wondering where I can find it or if it will be uploaded? 

Thank you,
Marius "
DIDI Dataset with textual context NO Label (text /non-text) of each stroke?,google-research/google-research,2020-03-04 03:14:17,0,,211,575107687,"Thankyou very much for shrae the big Dataset . However, There is No Label of each stroke (About this stroke is text or non-text) in DIdi Datset with textual context ? I want to use this dataset to do text-non-text classifaction, how can i get the label of each stroke? Thankyou ! "
[Dual_Pixels] How to save files in ImageFormat.RAW10 and with metadata ,google-research/google-research,2020-03-03 16:03:26,1,,210,574771996,How to save files in ImageFormat.RAW10 and metadata through the Java application?
depth_from_video_in_the_wild : generate depth image of EuRoc MAV dataset from pre-trained model ?,google-research/google-research,2020-02-29 10:19:00,7,,208,573268144,"@gariel-google 
Sorry for opening a new issue about rendering the depth image from the checkpoint you provide of EuRoc MAV dataset  by the the sequence of image, since I can't find any script specific for inference the depth image for EuRoc MAV dataset.

Wish you could provide some instruction about it, thanks in advanced!"
When will Meelna scarping code be released?,google-research/google-research,2020-02-28 23:22:03,0,,207,573060054,
Tabnet requirements.txt outdated due to old tf version? module 'tensorflow.compat.v1' has no attribute 'contrib' when running experiment_covertype,google-research/google-research,2020-02-25 09:18:22,2,,204,570422297,"Hi,
Running the second instruction in the readme (should be python -m experiment_covertype - without the .py), I get the error: 

```
W0225 10:16:24.084449 140481427662656 ag_logging.py:145] Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fc40296fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fc40296fd90>>: AssertionError: Bad argument number for Name: 3, expecting 4
Traceback (most recent call last):
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/natalia/repos/google-research/tabnet/experiment_covertype.py"", line 200, in <module>
    app.run(main)
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/natalia/repos/google-research/tabnet/experiment_covertype.py"", line 99, in main
    feature_train_batch, reuse=False, is_training=True)
  File ""/home/natalia/repos/google-research/tabnet/tabnet_model.py"", line 201, in encoder
    mask_values = tf.contrib.sparsemax.sparsemax(mask_values)
AttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'
```"
"[Meena] How many steps the final model was trained for, possible discrepancy in paper",google-research/google-research,2020-02-19 17:25:46,0,,202,567716478,"I have been trying to understand for how many steps (ie. gradient updates) the final Meena model (`10.2`) perplexity was trained for as it's not directly mentioned in the paper.

I think it's indirectly mentioned twice that points to around `2.5M` updates:

1) [Section: 3.3] Batch size: 4M tokens, Total tokens seen: 10T. so gradient updates comes around 2.5M

2) [Section: 3.3] Step time: 1 second, Total training time: 30 days, So number of steps: `~2.59M`.


But if above is true, there's seems to be some discrepancy or ambiguity with the following in the paper:

1) [Section: 3.2]
```
Our largest
(i.e., maximum memory usage) Evolved Transformer scored `10.2` perplexity and our largest vanilla Transformer scored perplexity 10.7 for the same number of training steps (738k).   
```
Which seems to indicate `738K` steps.


Am I interpreting the line in section 3.2 wrong? Can you please clarify this?

Thanks"
TCC: images_to_tfrecords.py on Penn Action dataset,google-research/google-research,2020-02-10 07:27:19,1,,197,562372633,"I am trying to reproduce the TCC results on the Penn Action dataset. While scripts have been provided for the Pouring dataset, they have not been provided for the Penn Action dataset. 

To be able to train on the Penn Action dataset, I downloaded the dataset and then tried running images_to_tfrecords.py. However, I get the following error:

```
2020-02-08 23:00:23.276804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""images_to_tfrecords.py"", line 201, in <module>
    app.run(main)
  File ""/home/arjung2/.conda/envs/tcc/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/arjung2/.conda/envs/tcc/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""images_to_tfrecords.py"", line 197, in main
    FLAGS.expected_segments)
  File ""images_to_tfrecords.py"", line 188, in create_tfrecords
    FLAGS.action_label, frame_labels)
  File ""../../tcc/dataset_preparation/dataset_utils.py"", line 111, in write_seqs_to_tfrecords
    frame_labels_string=frame_labels_string)
  File ""../../tcc/dataset_preparation/dataset_utils.py"", line 80, in get_example
    context_features_dict = {'name': bytes_feature([name]),
  File ""../../tcc/dataset_preparation/dataset_utils.py"", line 39, in <lambda>
    bytes_feature = lambda v: feature(bytes_list=tf.train.BytesList(value=v))
TypeError: '1579' has type str, but expected one of: bytes
```

Interestingly, when I run the code multiple times, I get different numbers in the last line -- it is not deterministic. 

Any ideas about what might be going on? Thanks!"
TCC: Annotations for Penn Action Dataset,google-research/google-research,2020-02-09 04:08:51,2,,194,562118528,Have annotations for the Penn Action dataset (for evaluation purposes) been released yet?
Question about Meena Model Architecture,google-research/google-research,2020-02-06 08:21:48,1,,193,560850358,"Hello, I was wondering how you ends up with 1 ET, 13 ET blocks on models.

From the paper,

> The best performing Meena model is an Evolved Transformer (ET) (So et al., 2019) seq2seq model with 2.6B parameters, which includes 1 ET encoder block and 13 ET decoder blocks.

Does it means you also find Meena structure using evolutionary NAS structure? 
I would like to know how you get 1 ET and 13 ET structures since the numbers are not normal for me.

Thanks."
"""Unicode objects must be encoded before hashing"" error in line 55 google-research/stochastic_to_deterministic/hashing .py",google-research/google-research,2020-01-31 15:32:31,0,,188,558200910,"I tried out the stochastic_to_deterministic research and the implementation  in the hashing.py ,but there seems to be an error in the syntax in line 55 : 
feature_sum_hex = [hashlib.md5(s).hexdigest() for s in feature_sum_str]
 The error stack trace:
Traceback (most recent call last):

  File ""<ipython-input-2-9c4701d54322>"", line 1, in <module>
    runfile('G:/s2d.py', wdir='G:')

  File ""G:\anaconda\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""G:\anaconda\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""G:/s2d.py"", line 115, in <module>
    app.run(main)

  File ""G:\anaconda\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)

  File ""G:\anaconda\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))

  File ""G:/s2d.py"", line 106, in main
    hash_val = compute_hash(features, hash_matrix, hash_vector)

  File ""G:/s2d.py"", line 63, in compute_hash
    feature_sum_hex = [hashlib.md5(s).hexdigest() for s in feature_sum_str]

  File ""G:/s2d.py"", line 63, in <listcomp>
    feature_sum_hex = [hashlib.md5(s).hexdigest() for s in feature_sum_str]

TypeError: Unicode-objects must be encoded before hashing

I have been using the required versions of absl and numpy which are specified in the requirements.txt


Resolution: The error is resolved by changing the syntax to 's.encode()' .I am submitting a PR for the same issue. "
Are you planning to release Meena code?,google-research/google-research,2020-01-29 14:18:21,30,,187,556891978,"Hey, you did it again making us all excited about new promising research in NLP with the ""Towards a Human-like Open-Domain Chatbot"" paper and conversation transcripts. Any plans to publish the source code? "
test set information aware in tft,google-research/google-research,2020-01-17 14:26:23,7,,183,551445267,"Take the experiment of electricity for example,   I notice that in the second window of the test set of each id in your code, it has the conditioning range with the ground truth of the values of 2014-09-01 instead of the prediction results, which equals to leak the test set information.

https://github.com/google-research/google-research/blob/1fec1abdc5a54d6f2ac8b5ef3420b36d4e6509f0/tft/libs/tft_model.py#L1220-L1229"
Integrating with OSS-Fuzz,google-research/google-research,2020-01-13 15:25:54,0,,180,549003050,"Greetings google-research developers and contributors,

We’re reaching out because your project is an important part of the open source ecosystem, and we’d like to invite you to integrate with our [fuzzing](https://www.owasp.org/index.php/Fuzzing) service, [OSS-Fuzz]( https://opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html ). OSS-Fuzz is a free fuzzing infrastructure you can use to identify security vulnerabilities and stability bugs in your project. OSS-Fuzz will:

- Continuously run at scale all the fuzzers you write.
- Alert you when it finds issues.
- Automatically close issues after they’ve been fixed by a commit.

Many widely used [open source projects]( https://github.com/google/oss-fuzz/tree/master/projects ) like OpenSSL, FFmpeg, LibreOffice, and ImageMagick are fuzzing via OSS-Fuzz, which helps them find and remediate [critical issues]( https://bugs.chromium.org/p/oss-fuzz/issues/list?can=1&q=status%3AFixed%2CVerified+Type%3ABug%2CBug-Security+-component%3AInfra+ ). 

Even though typical integrations can be done in < 100 LoC, we have a [reward program]( https://www.google.com/about/appsecurity/patch-rewards/ ) in place which aims to recognize folks who are not just contributing to open source, but are also working hard to make it more secure.

We want to stress that anyone who meets the eligibility criteria and integrates a project with OSS-Fuzz is eligible for a reward.

If you're not interested in integrating with OSS-Fuzz, it would be helpful for us to understand why—lack of interest, lack of time, or something else—so we can better support projects like yours in the future.

If we’ve missed your question in our [FAQ]( https://google.github.io/oss-fuzz/faq/ ), feel free to reply or reach out to us at oss-fuzz-outreach@googlegroups.com.


Thanks!

Tommy
OSS-Fuzz Team
"
Depth-from-video-in-the-wild:run trajectory_inference.py error,google-research/google-research,2020-01-10 03:07:09,0,,179,547838448,"when I run trajectory_inference.py,I got a problem:
    ValueError: Cannot feed value of shape (1, 370, 416, 3) for Tensor 'image1:0', which has shape '(1, 128, 416, 3)'.
How can I solve it?"
can u add the mnist_ood model as the paper describe(Likelihood Ratios for Out-of-Distribution),google-research/google-research,2020-01-05 12:29:16,0,,176,545411233,
schema baseline model can't reproduce the results in the article,google-research/google-research,2019-12-30 03:57:18,0,,174,543716187,You reported results on SGD-S dev set to be 0.776 in Avg GA and 0.486 in Joint GA. I tried to reproduce your results and ran your code and it gave 0.742 Avg GA and 0.371 Joint GA. It's a large gap. I use tensorflow 1.12 so I made some necessary fixes to your code. I ran your code on one GPU. 
Cluster-GCN: How to achieve mirco-F1=0.966 on Reddit?,google-research/google-research,2019-12-14 06:11:56,1,,171,537873391,"I run the code of Cluster-GCN with the default parameters in run_reddit.sh, but the micro-F1 I get is 0.962. What's the optimized parameters for Reddit dataset?  "
"Cluster-GCN：metis install unsuccessful, is there any tips? help ",google-research/google-research,2019-12-12 07:52:55,5,,170,536813075,
I wanna see the experiment of artificial data,google-research/google-research,2019-12-08 13:31:56,0,,169,534542253,"Hi, I've read and I'm really interested in your nips paper.

I wanna see the code of the experiment of artificial data that was used for checking the validity of ROAR framework.

I would like you to tell me where it is.
Thank you."
How to see masked_lm_loss & sentence_order_loss per iteration step during train?,google-research/google-research,2019-12-05 07:43:02,0,,166,533180025,"I want to see masked_lm_loss & sentence_order_loss per iteration step(ex 1,000) with total loss during train on log.
and how plot these on tensorboard?

Are there any good ways?

"
Depth from Video in the Wild fails with NaN due to CUDA bug,google-research/google-research,2019-12-03 09:18:46,13,,162,531838076,"It appears that 
https://github.com/google-research/google-research/blob/b13a2d98f590495331e27b86268b000413ecb50d/depth_from_video_in_the_wild/transform_utils.py#L111
can produce arbitrarily wrong gradients on trans_vec1 which then back-propagate to corrupt the depth estimation network with NaN weights. 

The issue is caused by a `CUDA 10.0` bug https://github.com/tensorflow/tensorflow/issues/31166 which is fixed in `TF 2 nightly` by upgrading to `CUDA 10.1`, however this network requires `TF 1.x`. One workaround would be to recompile `TF 1.15` with `CUDA 10.1` or to use CPU for all calculations.

But in any case, the network as-is won't train on GPU for more than a few steps before going NaN with any released TF version. So I'm writing this issue mainly to document that for others.

"
[dual_pixels] pgm files not saved on my pixel 3a ,google-research/google-research,2019-11-29 16:44:40,1,,158,530429782,"I tried to build and run the dual_pixel on my Google Pixel 3a, and the preview of dual pixel works.
but no matter how many times I push the ""capture"" button, the pgm file are not generated.

and I try to add the following codes, and I found `onCaptureFailed` occurs on my Pixel 3a.
Can anybody help me to make it works on my phone? Thank you.

```
     try {
       final CaptureRequest.Builder captureBuilder =
           cameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
       captureBuilder.addTarget(imageReader.getSurface());
       captureBuilder.addTarget(new Surface(viewfinderTextureView.getSurfaceTexture()));
       captureBuilder.set(CaptureRequest.CONTROL_MODE, CameraMetadata.CONTROL_MODE_AUTO);
-      cameraCaptureSession.capture(captureBuilder.build(), null, cameraHandler);
+
+      CameraCaptureSession.CaptureCallback captureCallback = new CameraCaptureSession.CaptureCallback() {
+        @Override
+        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request, TotalCaptureResult result) {
+          super.onCaptureCompleted(session, request, result);
+        }
+
+        @Override
+        public void onCaptureStarted(@NonNull CameraCaptureSession session, @NonNull CaptureRequest request, long timestamp, long frameNumber) {
+          super.onCaptureStarted(session, request, timestamp, frameNumber);
+          Log.d(TAG, ""onCaptureStarted"");
+        }
+
+        @Override
+        public void onCaptureFailed(@NonNull CameraCaptureSession session, @NonNull CaptureRequest request, @NonNull CaptureFailure failure) {
+          super.onCaptureFailed(session, request, failure);
+          Log.d(TAG, ""onCaptureFailed:"" + failure.getReason());
+          Log.d(TAG, ""onCaptureFailed:"" + failure.toString());
+        }
          cameraCaptureSession.capture(captureBuilder.build(), captureCallback, cameraHandler);
```

except the above code, I also get many messages in the logcat, but I do not know if that are related to this issue or not.

```
2019-11-29 23:53:14.685 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1556 SetMetadataByTag() Invalid slot; cannot set metadata tag 80210000
2019-11-29 23:53:14.685 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1720 PublishMetadataList() Invalid slot, cannot publish metadata list
2019-11-29 23:53:14.718 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1556 SetMetadataByTag() Invalid slot; cannot set metadata tag 80210000
2019-11-29 23:53:14.718 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1720 PublishMetadataList() Invalid slot, cannot publish metadata list
```

```
2019-11-29 23:53:11.553 828-828/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1556 SetMetadataByTag() Invalid slot; cannot set metadata tag 80090003
2019-11-29 23:53:11.553 828-828/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1720 PublishMetadataList() Invalid slot, cannot publish metadata list
2019-11-29 23:53:11.553 828-828/? E/CamX: [ERROR][SENSOR ] camxsensornode.cpp:2174 LoadPDlibrary() PD library CreateLib failure result=0, m_pPDLib=0x73e8735040
```

```
2019-11-29 23:53:17.664 848-1016/? I/sensors-hal: activate_physical_sensor:155, com.google.sensor.camera_vsync/7 en=0
2019-11-29 23:53:17.665 848-1016/? I/sensors-hal: activate_physical_sensor:166, com.google.sensor.camera_vsync/7 en=0 completed
2019-11-29 23:53:17.665 828-828/? I/GoogSensorSync: ~GoogSensorSync 72 Total failure/sync for camera 0: 0/58
```"
Universal Sentence Encoder missing?,google-research/google-research,2019-11-29 05:24:32,0,,156,530161448,"Hi,

There is published paper and a model in TF Hub, but the training code seems to be missing from this repository."
Regarding data generation in depth_from_video_in_the_wild,google-research/google-research,2019-11-27 08:51:24,1,,152,529201994,"I am doing research in unsupervised depth estimation. And thanks for the preliminary code in 'depth from video in the wild'.

In this project, it is necessary to generate data before training, which is similar in another project of the author [https://github.com/tensorflow/models/tree/master/research/vid2depth](https://github.com/tensorflow/models/tree/master/research/vid2depth). However, this project assume that the data has not only image file and cam file, but also segment file, which is different from the vid2depth project. Without the segment file, the training cannot be conducted.

May I request the data generating code in the 'depth from video in the wild' project? Thank you very much."
What is net_structure_type?,google-research/google-research,2019-11-19 16:45:34,0,,142,525130638,What is the use of the parameter net_structure_type in the model function builder for the language model?
Fine tuning TFHub models,google-research/google-research,2019-11-18 10:55:04,0,,139,524288970,"I tried V2 first, it doesn't work (with TF 1.15), there is an earlier report about missing Einsum op.

I tried V1 then for Cola task, after training it shows 

eval_accuracy = 0.6912752
eval_loss = 0.6295214
global_step = 6413
loss = 0.62965536

However if I run prediction, it always predicts label ""1"", resulting test file looks really strange:

0.2420986	0.7579014
0.24209863	0.7579013
0.24209863	0.7579013
0.24209863	0.7579013
...

So did anyone tried those TFHub models? They don't look like usual bert ones (no .ckpt files), so probably some conversion/preparation is required?"
Error in calculating pvals for n-gram probabilities in create_pretraining_data.py,google-research/google-research,2019-11-16 22:05:06,0,,136,523901883,"Lines 468 - 475.

If `FLAGS.favor_shorter_ngram` is True, then pvals actually favors longer n-grams.

Line 474 should be changed to:

`if not FLAGS.favor_shorter_ngram:`"
[depth_from_video_in_the_wild] cannot load from ImageNet ckpt for training,google-research/google-research,2019-11-15 05:53:50,13,,130,523270654,It seems that the batchnorm weights were missing from the checkpoint. Loading from the checkpoint gives the error `Not found: Key conv1/bn/beta not found in checkpoint` @gariel-google 
[bam]How the teacher annealing is implemented in this code? ,google-research/google-research,2019-11-14 09:11:43,1,,128,522719970,"I noticed in the paper that the lambda hyperparameter is used to balance the model prediction and the true label.Also noticed that the lambda hyperparameter is linearly increased during the trainning,since the paper did not explain the linearly increasing lambda strategy,I am looking ofrward to find it in the code.However, I did not find the hyperparamter setting of lambda and the implementation of teacher annealing.All I noticed is that in the configure.py,there is a lambda=0.5,but this is used when there is no teacher annealing.
Anyone knows how the teacher annealing is implemented in the code?Or has the similiart problem with me?"
"[bam] bam/task_specific/classification/classification_tasks.py"", line 136, in featurize . self._distill_inputs[eid]) KeyError: 97",google-research/google-research,2019-11-12 11:53:14,1,,124,521504712,"I changed several config code position (includes `bert_dir` and `self.bert_config_file`) on `configure.Config`, so I can distill 12 layer teacher model's output to 3 layer student model by use another `bert_config_file`.

I could train teacher model successfully.
But when I train the student model of distillation, error occurs.

command for train teacher model:
`python -m bam.run_classifier news-model-teacher $BAM_DIR '{""debug"": false, ""task_names"": [""news""], ""pretrained_model_name"": ""chinese_L-12_H-768_A-12"", ""learning_rate"": 2e-5, ""num_train_epochs"": 3.0, ""distill"": false, ""max_seq_length"": 512, ""train_batch_size"": 4, ""save_checkpoints_steps"": 10000}'`

command for train student model:
`python -m bam.run_classifier news-model-student $BAM_DIR '{""debug"": false, ""task_names"": [""news""], ""pretrained_model_name"": ""chinese_L-12_H-768_A-12"", ""learning_rate"": 2e-5, ""num_train_epochs"": 3.0, ""distill"": true, ""bert_config_file"": ""/home/work/repo/google-research-master/bam/bam_dir/pretrained_models/chinese_L-12_H-768_A-12/bert_config_3_layer.json"", ""teachers"": {""news"": ""news-model-teacher""}, ""max_seq_length"": 512, ""train_batch_size"": 4, ""save_checkpoints_steps"": 10000}'`

error message:
```
Traceback (most recent call last):
  File ""/home/work/anaconda2/envs/py3-tf.1.12/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/work/anaconda2/envs/py3-tf.1.12/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/data/repo/google-research-master/bam/run_classifier.py"", line 276, in <module>
    tf.app.run()
  File ""/home/work/anaconda2/envs/py3-tf.1.12/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv))
  File ""/data/repo/google-research-master/bam/run_classifier.py"", line 253, in main
    model_runner = ModelRunner(config, tasks)
  File ""/data/repo/google-research-master/bam/run_classifier.py"", line 152, in __init__
    sizes) = self._preprocessor.prepare_train()
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 63, in prepare_train
    return self._serialize_dataset(self._tasks, True, ""train"")
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 108, in _serialize_dataset
    self.serialize_examples(examples, is_training, tfrecords_path)
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 127, in serialize_examples
    tf_example = self._example_to_tf_example(example, is_training)
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 136, in _example_to_tf_example
    example, is_training))
  File ""/data/repo/google-research-master/bam/task_specific/classification/classification_tasks.py"", line 136, in featurize
    self._distill_inputs[eid])
KeyError: 97
```"
How do you get the training time on each epoch using TPUEstimator?,google-research/google-research,2019-11-10 06:46:14,1,,122,520562245,"I am able to see INFO:tensorflow:loss = 134.62343, step = 97

but not the time.
"
【TCC】-json file of Penn-Action,google-research/google-research,2019-11-05 17:23:26,10,,116,517913905,"Hello, sorry, I am sorry to disturb you. I hope that you can provide the frame tag json file of Penn-Action, because the task of re-labeling the data set is really huge. Maybe you can share some tfrecord files that have been marked, I hope to get your reply. Thank you。@debidatta"
How much time it take to build a model?,google-research/google-research,2019-11-04 12:50:21,0,,112,517130429,
How to run the source code successfully of  'depth_from_video_in_the_wild',google-research/google-research,2019-11-04 10:00:12,6,,111,517049088,"After downloaded the source code, I tried to run the `run.sh`,
![Screenshot from 2019-11-04 18-58-14](https://user-images.githubusercontent.com/38068286/68112644-1f00b380-ff35-11e9-8b64-bd972d448940.png)
I always get this error shown in this picture. I am new to TF, any cues would be highly appreciated.
"
[depth_from_video_in_the_wild] How to train with image sequences only?,google-research/google-research,2019-11-03 23:50:01,12,,107,516909542,"From the paper, I understand that we can train the network by providing the only image files, but the code is asking for the camera matrix and the mask image for moving objects. Can you help to modify so that I can train using only image files?

Regards"
Cluster-GCN：why the input_dim becomes FLAGS_hidden1*2?,google-research/google-research,2019-11-03 12:39:38,2,,104,516822160,"I have the problem with the code below? why the input_dim becomes FLAGS_hidden1*2?the input_dim should equal to the last layer's output_dim??

for _ in range(self.num_layers - 2):
      self.layers.append(
          layers.GraphConvolution(
              input_dim=FLAGS.hidden1 * 2,
              output_dim=FLAGS.hidden1,
              placeholders=self.placeholders,
              act=tf.nn.relu,
              dropout=True,
              sparse_inputs=False,
              logging=self.logging,
              norm=self.norm,
              precalc=False))
"
How to set up 'train_pattern' and 'test_pattern',google-research/google-research,2019-11-03 09:25:43,0,,102,516801715,"Sorry, I have a silly question. I can't run the codes, because there is always something wrong with the dataset. My directory is the following structure.
![image](https://user-images.githubusercontent.com/23647489/68083020-f0180e00-fe5e-11e9-9d62-6ced2ead0333.png)

Synthesize_MotionBlur
   -dataset
      --train
        ---1
           ----frame0.bmp
           ----frame1.bmp 
           ----blurred.bmp
     --test
   -result
   -train
     --dataset.py
     --estimator.py
     --network.py
     --train.py

And in the train.py, I set the 'train_pattern': `'../dataset/train/*'`, set the 'test_pattern': `'../dataset/test/*'`.  
![image](https://user-images.githubusercontent.com/23647489/68083055-3b322100-fe5f-11e9-83b8-0eba6b20d4ad.png)

> 2019-11-03 17:21:48.735950: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRandomAccessFile failed to Create/Open: ..\dataset\train\1\blurred.jpg/frame_0.jpg : 系统找不到指定 的路径。
> ; No such process
> 2019-11-03 17:21:48.744391: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRandomAccessFile failed to Create/Open: ..\dataset\train\1\frame_0.jpg/frame_0.jpg : 系统找不到指定 的路径。
> ; No such process
> 2019-11-03 17:21:48.756171: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRandomAccessFile failed to Create/Open: ..\dataset\train\1\frame_0.jpg/frame_1.jpg : 系统找不到指定 的路径。
> ; No such process
> 2019-11-03 17:21:48.768635: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRa


I can't understand why this happended, please help me with this. How should I set up the file directory correctly?"
why weight decay in _loss function is only applied to the first layer,google-research/google-research,2019-10-31 08:31:04,0,,95,515251841,"  def _loss(self):
    """"""Construct the loss function.""""""
    # Weight decay loss
    if FLAGS.weight_decay > 0.0:
      for var in self.layers[0].vars.values():
        self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)"
Depth_in_the_wild: Stardard KITTI odometry input and output,google-research/google-research,2019-10-31 06:42:16,0,,94,515203405,"Hey,

Thanks for your nice work and open sources. However, It would be better if you provide a script that works on the standard KITTI odometry dataset? For example, it takes the ""dataset_dir"" as input and outputs the trajectory file in KITTI format (n x 12). 

Thanks,
Jiawang"
Tcc: result cannot be same as the one in paper,google-research/google-research,2019-10-29 01:41:26,4,,88,513644353,"I train the tcc model and evaluate 'classification'. But the result is 68.8/68.6(0.5/1.0), which is much worse than the result in paper(91.43/91.82). Is there any wrong for my training?

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from easydict import EasyDict as edict


CONFIG = edict()


CONFIG.DECODER = False

CONFIG.LOGDIR = '/tmp/alignment_logs/'

CONFIG.DATASETS = [
    # 'baseball_pitch',
    # 'baseball_swing',
    # 'bench_press',
    # 'bowling',
    # 'clean_and_jerk',
    # 'golf_swing',
    # 'jumping_jacks',
    # 'pushups',
    # 'pullups',
    # 'situp',
    # 'squats',
    # 'tennis_forehand',
    # 'tennis_serve',
    'pouring',
]


CONFIG.PATH_TO_TFRECORDS = '/home/cxu-serve/p1/zkou2/ad/'


CONFIG.TRAINING_ALGO = 'alignment'

CONFIG.IMAGE_SIZE = 224  # For ResNet50



CONFIG.TRAIN = edict()
CONFIG.TRAIN.MAX_ITERS = 150000

CONFIG.TRAIN.BATCH_SIZE = 2

CONFIG.TRAIN.NUM_FRAMES = 20
CONFIG.TRAIN.VISUALIZE_INTERVAL = 200

CONFIG.EVAL = edict()

CONFIG.EVAL.BATCH_SIZE = 2

CONFIG.EVAL.NUM_FRAMES = 20

CONFIG.EVAL.VAL_ITERS = 20
CONFIG.EVAL.TASKS = [
    # 'algo_loss',
    'classification',
    # 'kendalls_tau',
    # 'event_completion',
    # 'few_shot_classification'
]

CONFIG.EVAL.FRAMES_PER_BATCH = 25
CONFIG.EVAL.KENDALLS_TAU_STRIDE = 5  # 2 for Pouring, 5 for PennAction
CONFIG.EVAL.KENDALLS_TAU_DISTANCE = 'sqeuclidean'  # cosine, sqeuclidean
CONFIG.EVAL.CLASSIFICATION_FRACTIONS = [0.1, 0.5, 1.0]
CONFIG.EVAL.FEW_SHOT_NUM_LABELED = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
CONFIG.EVAL.FEW_SHOT_NUM_EPISODES = 50

CONFIG.MODEL = edict()

CONFIG.MODEL.EMBEDDER_TYPE = 'conv'

CONFIG.MODEL.BASE_MODEL = edict()

CONFIG.MODEL.BASE_MODEL.NETWORK = 'Resnet50_pretrained'

CONFIG.MODEL.BASE_MODEL.LAYER = 'conv4_block3_out'

CONFIG.MODEL.TRAIN_BASE = 'only_bn'
CONFIG.MODEL.TRAIN_EMBEDDING = True

CONFIG.MODEL.RESNET_PRETRAINED_WEIGHTS = '/home/cxu-serve/u1/zkou2/Code/tcc/repo/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5'


CONFIG.MODEL.VGGM = edict()
CONFIG.MODEL.VGGM.USE_BN = True

CONFIG.MODEL.CONV_EMBEDDER_MODEL = edict()

CONFIG.MODEL.CONV_EMBEDDER_MODEL.CONV_LAYERS = [
    (256, 3, True),
    (256, 3, True),
]
CONFIG.MODEL.CONV_EMBEDDER_MODEL.FLATTEN_METHOD = 'max_pool'

CONFIG.MODEL.CONV_EMBEDDER_MODEL.FC_LAYERS = [
    (256, True),
    (256, True),
]
CONFIG.MODEL.CONV_EMBEDDER_MODEL.CAPACITY_SCALAR = 2
CONFIG.MODEL.CONV_EMBEDDER_MODEL.EMBEDDING_SIZE = 128
CONFIG.MODEL.CONV_EMBEDDER_MODEL.L2_NORMALIZE = False
CONFIG.MODEL.CONV_EMBEDDER_MODEL.BASE_DROPOUT_RATE = 0.0
CONFIG.MODEL.CONV_EMBEDDER_MODEL.BASE_DROPOUT_SPATIAL = False
CONFIG.MODEL.CONV_EMBEDDER_MODEL.FC_DROPOUT_RATE = 0.1
CONFIG.MODEL.CONV_EMBEDDER_MODEL.USE_BN = True


CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL = edict()

CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.CONV_LAYERS = [(512, 3, True),
                                                   (512, 3, True)]

CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.GRU_LAYERS = [
    128,
]
CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.DROPOUT_RATE = 0.0
CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.USE_BN = True

CONFIG.MODEL.L2_REG_WEIGHT = 0.00001


CONFIG.ALIGNMENT = edict()
CONFIG.ALIGNMENT.CYCLE_LENGTH = 2
CONFIG.ALIGNMENT.LABEL_SMOOTHING = 0.1
CONFIG.ALIGNMENT.SOFTMAX_TEMPERATURE = 0.1

CONFIG.ALIGNMENT.LOSS_TYPE = 'classification'
CONFIG.ALIGNMENT.NORMALIZE_INDICES = True
CONFIG.ALIGNMENT.VARIANCE_LAMBDA = 0.001
CONFIG.ALIGNMENT.FRACTION = 1.0
CONFIG.ALIGNMENT.HUBER_DELTA = 0.1
CONFIG.ALIGNMENT.SIMILARITY_TYPE = 'l2' 

CONFIG.ALIGNMENT.STOCHASTIC_MATCHING = False


CONFIG.SAL = edict()
CONFIG.SAL.DROPOUT_RATE = 0.0

CONFIG.SAL.FC_LAYERS = [(128, True), (64, True), (2, False)]
CONFIG.SAL.SHUFFLE_FRACTION = 0.75

CONFIG.SAL.NUM_SAMPLES = 8
CONFIG.SAL.LABEL_SMOOTHING = 0.0

CONFIG.ALIGNMENT_SAL_TCN = edict()

CONFIG.ALIGNMENT_SAL_TCN.ALIGNMENT_LOSS_WEIGHT = 0.33
CONFIG.ALIGNMENT_SAL_TCN.SAL_LOSS_WEIGHT = 0.33

CONFIG.CLASSIFICATION = edict()
CONFIG.CLASSIFICATION.LABEL_SMOOTHING = 0.0
CONFIG.CLASSIFICATION.DROPOUT_RATE = 0.0

CONFIG.TCN = edict()
CONFIG.TCN.POSITIVE_WINDOW = 5
CONFIG.TCN.REG_LAMBDA = 0.002

CONFIG.OPTIMIZER = edict()

CONFIG.OPTIMIZER.TYPE = 'AdamOptimizer'

CONFIG.OPTIMIZER.LR = edict()

CONFIG.OPTIMIZER.LR.INITIAL_LR = 0.0001

CONFIG.OPTIMIZER.LR.DECAY_TYPE = 'fixed'
CONFIG.OPTIMIZER.LR.EXP_DECAY_RATE = 0.97
CONFIG.OPTIMIZER.LR.EXP_DECAY_STEPS = 1000
CONFIG.OPTIMIZER.LR.MANUAL_LR_STEP_BOUNDARIES = [5000, 10000]
CONFIG.OPTIMIZER.LR.MANUAL_LR_DECAY_RATE = 0.1
CONFIG.OPTIMIZER.LR.NUM_WARMUP_STEPS = 0


CONFIG.DATA = edict()
CONFIG.DATA.SHUFFLE_QUEUE_SIZE = 0
CONFIG.DATA.NUM_PREFETCH_BATCHES = 1
CONFIG.DATA.RANDOM_OFFSET = 1
CONFIG.DATA.STRIDE = 16
CONFIG.DATA.SAMPLING_STRATEGY = 'offset_uniform'
CONFIG.DATA.NUM_STEPS = 2  
CONFIG.DATA.FRAME_STRIDE = 15  

CONFIG.DATA.FRAME_LABELS = True
CONFIG.DATA.PER_DATASET_FRACTION = 1.0
CONFIG.DATA.PER_CLASS = False

CONFIG.DATA.SAMPLE_ALL_STRIDE = 1

CONFIG.AUGMENTATION = edict()
CONFIG.AUGMENTATION.RANDOM_FLIP = True
CONFIG.AUGMENTATION.RANDOM_CROP = False
CONFIG.AUGMENTATION.BRIGHTNESS = True
CONFIG.AUGMENTATION.BRIGHTNESS_MAX_DELTA = 32.0 / 255
CONFIG.AUGMENTATION.CONTRAST = True
CONFIG.AUGMENTATION.CONTRAST_LOWER = 0.5
CONFIG.AUGMENTATION.CONTRAST_UPPER = 1.5
CONFIG.AUGMENTATION.HUE = False
CONFIG.AUGMENTATION.HUE_MAX_DELTA = 0.2
CONFIG.AUGMENTATION.SATURATION = False
CONFIG.AUGMENTATION.SATURATION_LOWER = 0.5
CONFIG.AUGMENTATION.SATURATION_UPPER = 1.5


CONFIG.LOGGING = edict()
CONFIG.LOGGING.REPORT_INTERVAL = 100


CONFIG.CHECKPOINT = edict()
CONFIG.CHECKPOINT.SAVE_INTERVAL = 1000"
Learning Depth From Videos in the Wild: Why Dilate the possibly mobile masks?,google-research/google-research,2019-10-27 00:41:29,0,,78,512903111,Why do you dilate the possible mobile masks by a dilation rate of 8? Was that to help with the problem of very small masks causing inf/nan loss? Or was there a greater importance to this? 
ROAR code - Training with estimator made no steps. Perhaps input is empty or misspecified.,google-research/google-research,2019-10-25 15:51:43,1,,76,512591526,"Hi, 
I've been trying to run the ROAR code but I get the warning:
> Training with estimator made no steps. Perhaps input is empty or misspecified.

and then the training finishes with 
> Loss for final step: None. 

I have transformed the food_101 dataset into tfrecord files and I pass the base_dir (where the tfrecord files are placed) correctly (I've checked the data_dir in the input_fn). Also since I am using the food_101 dataset I pass the dataset_name accordingly. That would be great if you could help me with this.

@sarahooker @doomie "
【evanet】How to use evanet to train on my own data for video classification ?,google-research/google-research,2019-10-25 06:11:22,1,,75,512323304,Can you show me how to do this?
evolutionary algorithm,google-research/google-research,2019-10-24 07:22:34,0,,70,511757571,"How to initialize the the population, P.
Someone can help me?"
cluster gcn,google-research/google-research,2019-10-23 01:24:09,9,,69,511015608,"Hi,I have few questions for the Amazon-2M dataset
1.Can the code of cluster_gcn run on  Amazon-2M dataset?
2.for the dataset Amazon-2M
(1) Will the stopwords be removed?
(2) What is the ratio of training test data?"
cluster_gcn: how to get the node embedding after training,google-research/google-research,2019-10-16 03:24:57,0,,66,507589388,"Is there an api or way to get the node embedding after training for `cluster_gcn`?

Not just doing a multi-class prediction task."
Can you publicly release Pouring and Penn_Action Dataset new annotations@debidatta,google-research/google-research,2019-10-15 13:37:38,2,,65,507250981,Can you publicly release Pouring and Penn_Action Dataset new annotations@debidatta
depth_from_video_in_the_wild: Ego-motion measurement unit,google-research/google-research,2019-10-15 07:21:09,0,,64,507056113,"Quick question.
What is the unit of the estimated ego-motion? Is it mm, cm, m?
If it is not real world units, how can it be converted?

Thanks a lot
"
schema_guided_dst baseline: difference between data_utils.get_num_dialog_examples() and the number of examples in dstc8_single_domain_train_examples.tf_record,google-research/google-research,2019-10-12 12:40:57,1,,62,506182201,"Hi, I doubt that data_utils.get_num_dialog_examples() returns correct number. 

In dstc8_single_domain & train, data_utils.get_num_dialog_examples() returns 82588, but the number of examples in dstc8_single_domain_train_examples.tf_record is 41294. I think these two numbers should be the same. Is it right? (The former is around as double as the latter because get_num_dialog_examples() counts USER and SYSTEM turns together. I think it's wrong.)"
schema_guided_dst baseline code causes error when running on Cloud TPU,google-research/google-research,2019-10-11 07:03:14,3,,60,505676551,"I use Tensorflow 1.14.

Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/dhlee347_estsoft_com/google-research/schema_guided_dst/baseline/train_and_predict.py"", line 908, in <module>
    tf.compat.v1.app.run(main)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/dhlee347_estsoft_com/google-research/schema_guided_dst/baseline/train_and_predict.py"", line 854, in main
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2876, in train
    rendezvous.raise_errors()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 131, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2871, in train
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1158, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1192, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1484, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1252, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1353, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1338, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1411, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1158, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 487, in __init__
    self._assert_fetchable(graph, fetch.op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 500, in _assert_fetchable
    'Operation %r has been marked as not fetchable.' % op.name)
**ValueError: Operation u'truediv' has been marked as not fetchable.**
ERROR:tensorflow:Closing session due to error Step was cancelled by an explicit call to `Session::Close()`.
"
The color of the restored image by run processing.py is not right on DND dataset.,google-research/google-research,2019-10-11 03:03:04,0,,59,505610196,"hi,can you give the unprocessing_srgb_loss code,I run the process.py to process bayer RGGB into sRGB imag on DND dataset.But the color of the restored image is not right."
Code snippet for simple prediction,google-research/google-research,2019-08-31 17:48:19,2,,53,487794640,"Hello, Im trying to get prediction results (depth and camera parameters) given Image1, Image2 and Masks. But Im having hard time how to, assuming I have these images as numpy matrixes, can i make this prediction by calling a single method? I tried doing this myself but code is overwhelmingly complicated and I pretty much got lost

Im running the train code with single step and its executing without any errors, but cant find prediction results anywhere, tensorboard Image folder is empty.

Essentially what i am trying to create is a method with following signature
`depth_image, camera_calibration = model.predict(previous_image, current_image, mask)`

I have found a method that returns self.est_depth which i presume is depth map, but I couldnt find out how to supply the inputs to that method and how do i retrieve predicted camera calibration

+Update
Setting Summary freq to 1 and training for 10 episodes on single data instance using provided code seems to generate images on Tensorboard 
`python -m depth_from_video_in_the_wild.train --checkpoint_dir=***\trained_models --data_dir=***\depth_from_video_in_the_wild\data_example --train_steps=10`"
FailedPreconditionError:2 root erros,google-research/google-research,2019-08-29 20:29:18,4,,51,487161197,"Hi, I am running into the issue related to FailedPreconditionError: 2 root error, when I am trying to run the train.py within Unprocessing Images for Learned Raw Denoising.  Do you have any idea why this will happen and how to address it. I am using tf_gpu_1.12.0, python 3.5.8, cuda 9.0, Linux. Thanks

![image](https://user-images.githubusercontent.com/34078389/63974085-f950bd00-ca60-11e9-8a6b-bd4cb5e18c3d.png)"
Negative distance function in tcc.visualize_aligment.py,google-research/google-research,2019-08-26 21:31:01,1,,49,485462791,"Hello,

regarding the distance function in visualize_alignment:

`
def dist_fn(x, y):
  dist = -1.0  * np.matmul(x, y.T)
  return dist
`

that is passed as the argument for the dist parameter in the align function, 

By using the negative of the matmul call, I believe the dynamic time warping to be finding the worst possible path. Experimentally I have verified that the reconstruction error is 0 when removing the negation."
Code readability and potential confusion in tcc/visualize_alignment.py,google-research/google-research,2019-08-23 18:38:02,1,,48,484661113,"Hello,
a section of the tcc code in visualize_alignment.py has high potential for confusion and misuse. The align function is defined as follows:

```
def align(candidate_feats, query_feats, use_dtw):
  """"""Align videos based on nearest neighbor in embedding space.""""""
  if use_dtw:
    _, _, _, path = dtw(candidate_feats, query_feats, dist=dist_fn)
    _, uix = np.unique(path[0], return_index=True)
    nns = path[1][uix]
  else:
    nns = []
    for i in range(len(candidate_feats)):
      nn_frame_id, _ = get_nn(query_feats, candidate_feats[i])
      nns.append(nn_frame_id)
  return nns
```

The function call is: 
```nns.append(align(embs[query], embs[candidate], use_dtw))```

The positional arguments for the query and candidate features are reversed. Clearly, we do not want to iterate over the candidate frame matching it to the reference. There is no logical error as the arguments are passed in to the function in reverse order but it may lead to issues downstream if these functions are built upon.

The function definition should read:

```
def align(query_feats,candidate_feats, use_dtw):
  """"""Align videos based on nearest neighbor in embedding space.""""""
  if use_dtw:
    _, _, _, path = dtw(query_feats,candidate_feats, dist=dist_fn)
    _, uix = np.unique(path[0], return_index=True)
    nns = path[1][uix]
  else:
    nns = []
    for i in range(len(query_feats)):
      nn_frame_id, _ = get_nn(query_feats[i], candidate_feats)
      nns.append(nn_frame_id)
  return nns
```

"
Trouble training temporal cycle consistency model on a new dataset,google-research/google-research,2019-08-15 16:29:10,9,,45,481232219,"I'm getting this error while trying to train the model on a new dataset:

`tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __inference_<lambda>_59670}} Name: , Feature list 'frame_labels' is required but could not be found.  Did you mean to include it in feature_list_dense_missing_assumed_empty or feature_list_dense_defaults?
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext]] [Op:__inference_<lambda>_59670]
`"
Trouble in running /mol_dqn/experimental,google-research/google-research,2019-08-08 11:44:27,0,,43,478419181,"each .py files in /mol_dqn/experimental contains such codes:

`from mol_dqn.chemgraph.mcts import deep_q_networks
from mol_dqn.chemgraph.mcts import molecules as molecules_mdp
from mol_dqn.chemgraph.mcts import run_dqn`

But where is the module mol_dqn.chemgraph.mcts?"
Unprocessing : code and pretrained model has mismatch,google-research/google-research,2019-08-02 14:05:24,2,,41,476197050,"While checking the unprocessing, I could not make model similar to the one uploaded in google drive.
When I tried to train the model in our environment, graph does not have short noise and read noise.
I got below error when I tried to denoise the model which is created using the code committed in github.
The name 'stddev/shot_noise:0' refers to a Tensor which does not exist. The operation, 'stddev/shot_noise', does not exist in the graph. The name 'stddev/shot_noise:0' refers to a Tensor which does not exist. The operation, 'stddev/shot_noise', does not exist in the graph. 

Could you check training code shared is latest one."
how to run multiple references with best score?,google-research/google-research,2019-07-31 12:23:19,0,,40,475111893,
demogen: loading resnet models fails on GPU,google-research/google-research,2019-07-31 10:38:38,8,,39,475067769,"The problem described in [previous issue](https://github.com/google-research/google-research/issues/35) is resolved when working with tensorflow with enabled GPU support, but then there is a zoo of behaviors:

- 5 of the saved resnet models loads correctly
- Most fail with `Not found: Key resnet/group_norm/beta not found in checkpoint`
- Many fail with `Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]` 
- Few fail with `ValueError: Trying to share variable resnet/conv2d/kernel, but specified shape (3, 3, 3, 32) and found shape (3, 3, 3, 16).`

Correctly loaded:
```
resnet cifar10 resnet_wide_1.0x_batchnorm_aug_decay_0.0_1
resnet cifar10 resnet_wide_1.0x_batchnorm_aug_decay_0.0_lr_0.001_1

resnet cifar100 resnet_wide_1.0x_batchnorm_aug_decay_0.0_1
resnet cifar100 resnet_wide_1.0x_batchnorm_aug_decay_0.0_lr_0.001_1
resnet cifar100 resnet_wide_1.0x_batchnorm_aug_decay_0.0_lr_0.1_1
```

Not found:
```
resnet cifar10 resnet_wide_1.0x_batchnorm_aug_decay_0.0_2
2019-07-31 13:22:08.859328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:22:08.859377: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:22:08.859386: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:22:08.859393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:22:08.859405: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:22:08.859413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:22:08.859420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:22:08.859428: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:22:08.859802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:22:08.859822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:22:08.859826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:22:08.859829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:22:08.860214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
W0731 13:22:08.969132 139832240281408 deprecation.py:323] From ~/google-research/demogen/models/resnet.py:47: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
2019-07-31 13:22:10.279506: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key resnet/group_norm/beta not found in checkpoint
Traceback (most recent call last):
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 262, in load_parameters
    saver.restore(tf_session, model_dir)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1302, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

2 root error(s) found.
  (0) Not found: Key resnet/group_norm/beta not found in checkpoint
         [[node save_2/RestoreV2 (defined at ~/google-research/demogen/model_config.py:261) ]]
  (1) Not found: Key resnet/group_norm/beta not found in checkpoint
         [[node save_2/RestoreV2 (defined at ~/google-research/demogen/model_config.py:261) ]]
         [[save_2/RestoreV2/_383]]
0 successful operations.
0 derived errors ignored.

Original stack trace for u'save_2/RestoreV2':
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 261, in load_parameters
    saver = tf.train.Saver(model_var_list)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 328, in _AddRestoreOps
    restore_sequentially)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1696, in restore_v2
    name=name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
```

Invalid argument:
```
resnet cifar10 resnet_wide_1.0x_groupnorm_aug_decay_0.0_1
W0731 13:25:40.192379 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/extract_layers_util.py:68: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-31 13:25:40.193601: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-31 13:25:40.530512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:25:40.530699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:25:40.531574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:25:40.532371: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:25:40.532577: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:25:40.533520: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:25:40.534268: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:25:40.536462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:25:40.537216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:25:40.537544: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-07-31 13:25:40.596500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b0d5471f80 executing computations on platform CUDA. Devices:
2019-07-31 13:25:40.596528: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-07-31 13:25:40.627506: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz
2019-07-31 13:25:40.628479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b0d3885e60 executing computations on platform Host. Devices:
2019-07-31 13:25:40.628495: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-31 13:25:40.628967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:25:40.629006: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:25:40.629014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:25:40.629021: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:25:40.629036: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:25:40.629043: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:25:40.629066: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:25:40.629073: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:25:40.629738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:25:40.629757: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:25:40.630519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:25:40.630526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:25:40.630529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:25:40.631262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
W0731 13:25:40.637204 140184543594304 deprecation.py:323] From ~/.local/lib64/python2.7/site-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0731 13:25:40.649481 140184543594304 deprecation_wrapper.py:119] From ~/.local/lib64/python2.7/site-packages/tensor2tensor/data_generators/image_utils.py:169: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

W0731 13:25:40.820377 140184543594304 deprecation.py:323] From ~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
W0731 13:25:40.825278 140184543594304 deprecation.py:323] From ~/google-research/demogen/data_util.py:76: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
W0731 13:25:40.837275 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/extract_layers_util.py:76: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
W0731 13:25:40.838011 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/models/resnet.py:383: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0731 13:25:40.838236 140184543594304 deprecation.py:323] From ~/google-research/demogen/models/resnet.py:136: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0731 13:25:41.450165 140184543594304 deprecation.py:323] From ~/google-research/demogen/models/resnet.py:430: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
W0731 13:25:41.451108 140184543594304 deprecation.py:506] From ~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0731 13:25:42.371058 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/model_config.py:261: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W0731 13:25:42.421227 140184543594304 deprecation.py:323] From ~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Traceback (most recent call last):
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 262, in load_parameters
    saver.restore(tf_session, model_dir)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1322, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

2 root error(s) found.
  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]
         [[node save/Assign_50 (defined at ~/google-research/demogen/model_config.py:261) ]]
         [[save/RestoreV2/_120]]
  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]
         [[node save/Assign_50 (defined at ~/google-research/demogen/model_config.py:261) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/Assign_50:
 resnet/group_norm_15/beta (defined at ~/google-research/demogen/models/resnet.py:66)

Input Source operations connected to node save/Assign_50:
 resnet/group_norm_15/beta (defined at ~/google-research/demogen/models/resnet.py:66)

Original stack trace for u'save/Assign_50':
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 261, in load_parameters
    saver = tf.train.Saver(model_var_list)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 72, in restore
    self.op.get_shape().is_fully_defined())
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/state_ops.py"", line 227, in assign
    validate_shape=validate_shape)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 66, in assign
    use_locking=use_locking, name=name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
```

ValueError logs:
```
resnet cifar10 resnet_wide_1.0x_groupnorm__decay_0.002_lr_0.001_3
2019-07-31 13:19:29.317723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:19:29.317779: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:19:29.317789: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:19:29.317803: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:19:29.317811: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:19:29.317819: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:19:29.317826: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:19:29.317834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:19:29.318213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:19:29.318235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:19:29.318239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:19:29.318243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:19:29.318637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
2019-07-31 13:19:36.919128: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key resnet/batch_normalization/beta not found in checkpoint
resnet cifar10 resnet_wide_2.0x_batchnorm_aug_decay_0.0_1
2019-07-31 13:19:37.275569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:19:37.275624: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:19:37.275643: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:19:37.275652: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:19:37.275659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:19:37.275667: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:19:37.275675: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:19:37.275691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:19:37.276063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:19:37.276086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:19:37.276090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:19:37.276094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:19:37.276490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
Traceback (most recent call last):
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 89, in extract_layers
    end_points_collection=end_points_collection)
  File ""~/google-research/demogen/models/resnet.py"", line 391, in __call__
    strides=self.conv_stride, data_format=self.data_format)
  File ""~/google-research/demogen/models/resnet.py"", line 136, in conv2d_fixed_padding
    data_format=data_format)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/convolutional.py"", line 424, in conv2d
    return layer.apply(inputs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1479, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/base.py"", line 537, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 591, in __call__
    self._maybe_build(inputs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1881, in _maybe_build
    self.build(input_shapes)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/base.py"", line 450, in add_weight
    **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 384, in add_weight
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/tracking/base.py"", line 663, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1496, in get_variable
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1239, in get_variable
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 562, in get_variable
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 514, in _true_getter
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 869, in _get_single_variable
    (name, shape, found_var.get_shape()))
ValueError: Trying to share variable resnet/conv2d/kernel, but specified shape (3, 3, 3, 32) and found shape (3, 3, 3, 16).
```"
problems when running bam/run_classifier.py ,google-research/google-research,2019-07-19 04:18:11,2,,36,470129626,"Hi there, a problem occured when running the bam/run_classifier.py.

Tensorflow seems to lock the `events.out.tfevents` file until the whole program end,  when execute the command `utils.rmkdir(config.checkpoints_dir)` at run_classfier.py line 271, `tf.gfile.DeleteRecursively` appears to can't be done and raise the error given below

```
Traceback (most recent call last):
  File ""D:/bam/run_classifier.py"", line 281, in <module>
    tf.app.run()
  File ""C:\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:/bam/run_classifier.py"", line 276, in main
    utils.rmkdir(config.checkpoints_dir)
  File ""D:\bam\helpers\utils.py"", line 71, in rmkdir
    rmrf(path)
  File ""D:\bam\helpers\utils.py"", line 60, in rmrf
    tf.gfile.DeleteRecursively(path)
  File ""C:\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 563, in delete_recursively
    delete_recursively_v2(dirname)
  File ""C:\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 577, in delete_recursively_v2
    pywrap_tensorflow.DeleteRecursively(compat.as_bytes(path), status)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to remove a directory: bam_dir\models\debug-model\checkpoints; Directory not empty

```
The enviroment currently use : tensorflow-gpu 1.13.1 windows10.
However, when run this code on centos, the problem doesn't exist.
Really hope to give me some advice
"
error running example.py,google-research/google-research,2019-07-13 04:25:59,5,,33,467668984,"I am trying to run the demogen example.py in a conda  environment, and in either 2.7 or 3.7

get the following error:

```
File "".../google-research/demogen/data_util.py"", line 73, in input_data
    dataset = prob.dataset(mode)
  File "".../anaconda3/envs/demogen/lib/python3.7/site-packages/tensor2tensor/data_generators/problem.py"", line 631, in dataset
    assert data_dir

```
I am running TF versions

```
tensor2tensor            1.13.4   
tensorboard              1.14.0   
tensorflow               1.14.0   
tensorflow-datasets      1.0.2    
tensorflow-estimator     1.14.0   
tensorflow-metadata      0.13.0   
tensorflow-probability   0.7.0 
```

"
Load node2vec graph into persona graph_embedding,google-research/google-research,2019-07-01 15:48:54,0,,30,462788252,"Is it possibile to load a node2vec graph format like

```
node1_id_int node2_id_int <weight_float, optional>
```

Into the persona graph embedding?
Also node2vec provides several graph samples (like FB Egonet, etc.) in https://snap.stanford.edu/node2vec/, shall I first convert those graph to the NetworkX format before running in 

```
python3 -m graph_embedding.persona.persona --input_graph=${graph} \
   --output_clustering=${clustering_output}
```

Thank you."
frechet_video_distance,google-research/google-research,2019-06-04 06:52:11,1,,27,451826358,"Hello. Thanks for sharing remarkable work.

I want to use the ""frechet_video_distance.py"" for my research.
But I can't find how to give my video as input.

May I get some help for it?
"
Trouble in running rouge package,google-research/google-research,2019-05-16 23:25:03,0,,25,445199791,"I was trying to use the rouge package in a multi reference scenario? Is it possible to provide an example in the rouge/readme on how one can do that? 
In the current example in ""How to run"" where do you define the directory of the files?

Also, I am wondering how do you compute rouge for multi-reference scenario. Looking into the code you do some sort of bootstrap aggregation while in the original paper here (https://www.aclweb.org/anthology/W04-1013) in section 2 looks like they do simple micro-averaging while in section 2.1 it seems they do maximization over pairewise computation in rouge!

Thanks,
MP

"
In enas_lm model,google-research/google-research,2019-05-08 07:39:19,2,,23,441594935,"In the paper, they said 
""For the output, we simply average all the loose ends, i.e. the nodes that are not selected as inputs to any other nodes""

however in the code below 

```
next_s = tf.add_n(layers[1:]) / tf.cast(num_layers, dtype=tf.float32)
all_s = all_s.write(step, next_s)
```
it seems like averaging all the outputs of the nodes..
Is this the right implementation?
"
"Have you tried ""state_of_sparsity"" on MobileNets?",google-research/google-research,2019-04-30 09:30:14,0,,22,438682470,Do you think it will be good on MobileNets?
Recruiter Contacts,google-research/google-research,2019-04-06 09:05:43,0,,19,430014323,"Can you post:
1. recruiter contacts
2. if posted opening does not march what you can contribute best in contact information where you can discuss or send in a proposal or speculative application"
Setup.py,google-research/google-research,2019-03-11 18:49:58,1,,16,419628634,"Hi, what do you think about creating `setup.py` file to enable `pip install git+...`
Don't mind contributing if you find this helpful.

Thanks."
Rouge - Handling empty last line of prediction file,google-research/google-research,2019-01-19 23:19:29,2,,12,401044705,"Hi,
What's the best way to handle mismatched lines in source and target. Currently, the code fails on line 111:

    # Check whether num_targets < num_predictions
    if next(pred_gen, None) is not None or next(pred_gen, None) is not None:
      raise ValueError(""Must have equal number of lines across target and ""
                       ""prediction files. Mismatch between files: %s, %s."" %
                       (target_filename, prediction_filename))

My predictions files have more sentences than the target and the pyrouge package which is built on the PERL rouge handles this without breaking. Should I be pre-processing my target files to have same no: of lines as the prediction? 

Can this be handled the rouge package level instead similar to the perl one?
Commenting out this check leads to 10 points lower in all metrics as compares to the pyrouge outputs. 

Let me know if any additional code/output files are required to better understand this issue.
"
"in ./graph_embedding/watch_your_step Reason for scaling up positive loss by the number of nodes, significance of attention and why no validation split?",google-research/google-research,2018-12-31 11:54:58,1,,9,394970040,"[Line no 226, graph_attention_learning.py, Watch Your Step] return tf.transpose(d_sum) * **GetNumNodes()** * 80, feed_dict
Why is an arbitrary scaling by the number of nodes done? I am not sure if it is reported in the Watch Your Step paper.
When I remove the scaling, there is some decrease in the results for most of the datasets
Results and Ablation Study:
PPI(without scaling,learn attention) - 90.84
PPI(with scaling,learn attention) - 91.8
PPI(uniform attention,scaling) - 91.7

ca-HepTh(without scaling,learn attention) - 93.14
ca-HepTh(with scaling,learn attention) - 93.8
ca-HepTh(uniform attention,scaling) - 93.9

Wiki-Vote(without scaling,learn attention) - 94.3
Wiki-Vote(with scaling,learn attention) - 93.7
Wiki-Vote(uniform attention,scaling) - 94.0

Configs:
Embedding dimension:128
Share Embeddings: False
Transition_powers: 5
Loss: nlgl
context_regularizer: 0.1
learnable attention - softmax over 5 hops
uniform attention - Equal attention of earch of 5 hops(0.2 for each hop)

A couple of more questions:
1) Why is a validation set(validation positive edges, negative edges) not chosen for stopping? I know that Learning Edge Representations via Low-Rank Asymmetric Projections, other baselines and many of the graph embedding literature doesn't use it for link prediction, but I feel that stopping based on validation from ROC-AUC scores is more appropriate than stopping by best train ROC-AUC.

2.a) I see that attention makes only little contribution to an increase in the performance. Uniform attention works well. For example, in PPI, the paper reports that the attention learned favors the first hop. But not learning any attention(or in other words, uniform attention) also performs equally well. This is true even for Soc-Facebook, Wiki-Vote, ca-HepTh, ca-AstroPh. 

2.b)
Why is the stopping criteria in line 441
""if i - 100 > eval_metrics['i at best train']:
      LogMsg('Reached peak a while ago. Terminating...')
      break""

based on training error? Shouldn't stopping criteria be always based on validation error?

2.c) in line 340 ""eval_metrics['test auc at best train'] = float(test_auc)""

Why log and report test AUC at best train? We always report metrics and save model that gives the best performance on the validation set. If we modify the code to report the AUC / precision scores at least validation error then there is little to no difference between having weights and trainable attention.

"
About Time-varying Convex Optimization,google-research/google-research,2018-12-20 15:33:33,0,,8,393096676,It's amazing and so magical! Where is the source paper and code?
Undefined name: 'embed_translation' in ./qanet/squad_helper.py,google-research/google-research,2018-11-10 06:36:38,0,,5,379396452,"$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./qanet/squad_helper.py:87:1: F822 undefined name 'embed_translation' in __all__
__all__ = ['preprocess_inputs', 'build_a_layer',
^
```"
Undefined name: ‘transfer’ in ./qanet/squad_helper.py,google-research/google-research,2018-11-10 06:21:06,0,,4,379395657,"$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./qanet/squad_helper.py:166:22: F821 undefined name 'transfer'
    encoder_states = transfer.elmo_tensor_from_chars(
                     ^
./qanet/squad_helper.py:181:22: F821 undefined name 'transfer'
    encoder_states = transfer.elmo_tensor_from_sentences(
                     ^
```

Should these be [__embed_elmo_chars()__](https://github.com/google-research/google-research/blob/master/qanet/squad_helper.py#L162) and [__embed_elmo_sentences()__](https://github.com/google-research/google-research/blob/master/qanet/squad_helper.py#L177)?"
Undefined name: NltkAndPunctTokenizer() does not exist in qanet/util/tokenizer_util.py,google-research/google-research,2018-11-10 06:16:13,2,,3,379395290,https://github.com/google-research/google-research/search?q=NltkAndPunctTokenizer&unscoped_q=NltkAndPunctTokenizer
infer_visdrone.py AssertionError     assert filters.is_contiguous(),ChenhongyiYang/QueryDet-PyTorch,2022-10-31 08:31:42,5,,56,1429460499,"   self.act_type)
  File ""/root/anaconda3/envs/querydet/lib/python3.7/site-packages/torch/cuda/amp/autocast_mode.py"", line 209, in decorate_fwd
    return fwd(*args, **kwargs)
  File ""/root/anaconda3/envs/querydet/lib/python3.7/site-packages/spconv/pytorch/functional.py"", line 224, in forward
    raise e
  File ""/root/anaconda3/envs/querydet/lib/python3.7/site-packages/spconv/pytorch/functional.py"", line 214, in forward
    act_type)
  File ""/root/anaconda3/envs/querydet/lib/python3.7/site-packages/spconv/pytorch/ops.py"", line 1467, in implicit_gemm
    assert filters.is_contiguous()
AssertionError

有没有在visdrone上测试的，我运行下面这个直接报错了

python infer_visdrone.py --config-file models/querydet/configs/visdrone/test.yaml --num-gpu 1 --eval-only MODEL.WEIGHTS / /out/model_final.pth MODEL.QUERY.QUERY_INFER True
"
Performance on MSCOCO test-dev?,ChenhongyiYang/QueryDet-PyTorch,2022-10-26 02:44:00,0,,55,1423351081,Could you report the performance of QueryDet on MSCOCO test-dev?
train_visdrone.py ImportError,ChenhongyiYang/QueryDet-PyTorch,2022-10-06 02:49:41,0,,54,1398679953,"from models.querydet.detector import RetinaNetQueryDets

RetinaNetQueryDet多加了一个s"
Can a single GPU complete the training of visdrone?,ChenhongyiYang/QueryDet-PyTorch,2022-09-14 10:03:56,3,,51,1372708774,"I only have a 3070 on my computer. When I started training, it kept showing ""CUDA out of Memory"". I would like to ask whether a single GPU can complete the training, thanks"
Code issues,ChenhongyiYang/QueryDet-PyTorch,2022-09-08 13:02:49,0,,50,1366284799,"Hello author, your job is very good. Can you answer the meaning of the following code for me?


        for i in range(-1*self.context, self.context+1):
            for j in range(-1*self.context, self.context+1):
                sparse_ys.append(ys+i)
                sparse_xs.append(xs+j)

        sparse_ys = torch.cat(sparse_ys, dim=0)
        sparse_xs = torch.cat(sparse_xs, dim=0)


        good_idx = (sparse_ys >= 0) & (sparse_ys < fh) & (sparse_xs >= 0)  & (sparse_xs < fw)"
Code issues,ChenhongyiYang/QueryDet-PyTorch,2022-09-08 13:02:43,0,,49,1366284597,"Hello author, your job is very good. Can you answer the meaning of the following code for me?


        for i in range(-1*self.context, self.context+1):
            for j in range(-1*self.context, self.context+1):
                sparse_ys.append(ys+i)
                sparse_xs.append(xs+j)

        sparse_ys = torch.cat(sparse_ys, dim=0)
        sparse_xs = torch.cat(sparse_xs, dim=0)


        good_idx = (sparse_ys >= 0) & (sparse_ys < fh) & (sparse_xs >= 0)  & (sparse_xs < fw)"
Exception has occurred: KeyError 'res4',ChenhongyiYang/QueryDet-PyTorch,2022-08-22 06:54:15,0,,47,1345910547,"File ""/home/xxx/project/QueryDet-PyTorch-main/apex_tools/apex_trainer.py"", line 278, in build_model
    model = build_model(cfg)
  File ""/home/xxx/project/QueryDet-PyTorch-main/train_tools/coco_train.py"", line 146, in start_train
    model = Trainer.build_model(cfg)
  File ""/home/xxx/project/QueryDet-PyTorch-main/train_coco.py"", line 21, in <module>
    args=(args,),
This bug is encountered during debugging. Does anyone know that it has been solved?"
Exception has occurred: ScannerError mapping values are not allowed here,ChenhongyiYang/QueryDet-PyTorch,2022-08-18 10:51:50,0,,46,1342923124,"Exception has occurred: ScannerError
mapping values are not allowed here
  in ""/home/xxx/project/QueryDet-PyTorch-main/models/config.py"", line 5, column 29
  File ""/home/xxx/project/QueryDet-PyTorch-main/train_tools/coco_train.py"", line 135, in setup
    cfg.merge_from_file(args.config_file)
  File ""/home/xxx/project/QueryDet-PyTorch-main/train_tools/coco_train.py"", line 143, in start_train
    cfg = setup(args)
  File ""/home/xxx/project/QueryDet-PyTorch-main/train_coco.py"", line 21, in <module>
    args=(args,),"
The categories problem during training the VisDrone dataset,ChenhongyiYang/QueryDet-PyTorch,2022-07-28 08:05:01,0,,41,1320552839,"While training the RetinaNet baseline using your splited VisDrone dataset, I find the processed dataset has only 9 categories. In the [visdrone/data_prepare.py](https://github.com/ChenhongyiYang/QueryDetPyTorch/blob/main/visdrone/data_prepare.py), I find these codes 

```def make_json(images, annotations, new_label_json):
    ann_dict = {}
    ann_dict['categories'] = [
        {'supercategory': 'things', 'id': 1, 'name': 'people'},
        {'supercategory': 'things', 'id': 2, 'name': 'bicycle'},
        {'supercategory': 'things', 'id': 3, 'name': 'car'},
        {'supercategory': 'things', 'id': 4, 'name': 'van'},
        {'supercategory': 'things', 'id': 5, 'name': 'truck'},
        {'supercategory': 'things', 'id': 6, 'name': 'tricycle'},
        {'supercategory': 'things', 'id': 7, 'name': 'awning-tricycle'},
        {'supercategory': 'things', 'id': 8, 'name': 'bus'},
        {'supercategory': 'things', 'id': 9, 'name': 'motor'}
    ]
    ann_dict['images'] = images
    ann_dict['annotations'] = annotations
    with open(new_label_json, 'w') as outfile:
        json.dump(ann_dict, outfile)
```

Does that mean the people and pedestrian categories are merged? 
However, I can't find the merge part, the label's category_id seems to be exactly the same as the category_id from annotations‘ txt file. Maybe there exist some mistakes?"
loss nan!,ChenhongyiYang/QueryDet-PyTorch,2022-07-26 17:37:58,1,,40,1318560800,"when i train in visdrone, i just modify the batchsize and the max iter ,but there is a promblem in 768iter
![image](https://user-images.githubusercontent.com/96114577/181073931-9234a746-af7b-4701-bcd7-34d115762d2d.png)

"
Pretrained Weights Release,ChenhongyiYang/QueryDet-PyTorch,2022-07-25 04:27:11,0,,39,1316250799,"Hi, thanks for this great work. Do you have a plan for releasing the pretrained weights on COCO? E.g., the model weights in Table 1, QueryDet with CSQ, AP 38.36."
"Can anyone successfully run this project, please tell me your versions, thx",ChenhongyiYang/QueryDet-PyTorch,2022-07-20 19:18:11,4,,38,1311743333,"Can anyone successfully run this project, please tell me your versions, thx

"
can you provide the configure file about fcos？,ChenhongyiYang/QueryDet-PyTorch,2022-07-11 12:28:33,0,,36,1300641589,I‘m very interested with your method，can you provide something detail about fcos?
"Hello,when i run the train_coco.py,i came across a bug:'AttributeError: 'Trainer' object has no attribute '_detect_anomaly',could you help me?",ChenhongyiYang/QueryDet-PyTorch,2022-07-04 11:15:41,5,,33,1293023104,"Traceback (most recent call last):
  File ""train_coco.py"", line 15, in <module>
    launch(
  File ""/home/lei/anaconda3/envs/py38/lib/python3.8/site-packages/detectron2/engine/launch.py"", line 82, in launch
    main_func(*args)
  File ""/home/lei/Project/QueryDet-PyTorch-main*/train_tools/coco_train.py"", line 177, in start_train
    return trainer.train()
  File ""/home/lei/Project/QueryDet-PyTorch-main*/apex_tools/apex_trainer.py"", line 227, in train
    self.run_step()
  File ""/home/lei/Project/QueryDet-PyTorch-main*/apex_tools/apex_trainer.py"", line 251, in run_step
    self._detect_anomaly(losses, loss_dict)
AttributeError: 'Trainer' object has no attribute '_detect_anomaly'"
Cannot achieve good accuracy after training on COCO dataset,ChenhongyiYang/QueryDet-PyTorch,2022-06-30 07:14:14,1,,32,1289701553,"I am trying to retrain your network. I set up a training environment following issue #25 . My computer has two GPU GeForce RTX 3090, 24G. I reduce batch size from your set up (16) to 8 in [train_mp.yaml](https://github.com/ChenhongyiYang/QueryDet-PyTorch/blob/main/models/querydet/configs/coco/train_mp.yaml) as follows.

```
SOLVER:
  # 3x
  # STEPS: (210000, 250000)
  # MAX_ITER: 270000

  # 1x
  BASE_LR: 0.005
  STEPS: (120000, 160000)
  MAX_ITER: 180000
  IMS_PER_BATCH: 8
```
After training, I load model_final.pth (change in [test.yaml](https://github.com/ChenhongyiYang/QueryDet-PyTorch/blob/main/models/querydet/configs/coco/test.yaml)). [The weight is uploaded here [GoogleDriver](https://drive.google.com/file/d/15LPaHmxa4YlmsuepuAa40HTsjfgSmAal/view?usp=sharing). Training log is [here](https://drive.google.com/file/d/1O94k6BWAGSpoXBDR-B4zHhvVDd0XP0OX/view?usp=sharing).]

```
MODEL:
  META_ARCHITECTURE: ""RetinaNetQueryDet""
  WEIGHTS: ""../default_dir/model_final.pth""
```

I then run [infer_coco.py.](https://github.com/ChenhongyiYang/QueryDet-PyTorch/blob/main/infer_coco.py), and use an image in COCO dataset (`datasets/coco/test2017/000000000001.jpg`). It gives me a weird result.

Do you know what went wrong?

![image](https://user-images.githubusercontent.com/15703029/176614984-f77c912c-3e54-45ab-99bc-54293ee78c20.png)
"
"How to get gt_query in yolo?Specifically,how to get gt_instances in yolo?",ChenhongyiYang/QueryDet-PyTorch,2022-06-20 02:26:12,3,,31,1276274297,
visdrone training is pulling and doing coco dataset training,ChenhongyiYang/QueryDet-PyTorch,2022-06-11 06:17:29,1,,30,1268177702,I am trying to train on visdrone and the code is pulling coco_train.py from the train_tools instead of visdrone_train.py Can you please look into this ?
No test/detect_face function for custom image/data ,damo-cv/mogface,2022-06-13 08:22:52,0,,2,1269082628,There is no test/detect_face function for detecting faces in images other than the widerface dataset.
A Misspelling in the USAGE,damo-cv/mogface,2022-05-12 12:03:15,0,,1,1233879557,In the USAGE it may be “MogFace_E” but not “MogFace-E”
The issue about dataset,kreshuklab/spoco,2022-10-28 07:48:34,0,,13,1426839644,"Hi, I am a master course student tudying computer vision. I opened the issue as I found a probelm while trying to run the code `spoco_train.py` for spoco.

I debugged the code as CUDA memory error kept occurring in the loss calculation part. And I found that there was a problem with the part at `datasets/cvppp.py`.

When performing the  `self.train_label_transform`, the mask is not converted into a form suitable for learning due to `transforms.RandomResizedCrop(448, scale=(0.7, 1.), interpolation=0)`. The version of torchvision was 0.13.1 in my case.

For example, even if there are five instances in the mask returned by dataloader, It was found that applying `torch.unique()` did not properly count the number of instances as below. When I replaced all `transforms.RandomResizedCrop()` to `transforms.Resize`, the train works well.

![image](https://user-images.githubusercontent.com/44194558/198533947-01930402-657f-475d-a5bc-b54e2e440acd.png)


I would appreciate it if you could consider these points if you modify the code later. Also, I recommend that those who use this code take this into consideration.

Finally, hank you very much for your research and hard work."
Where is the model structure folder?,qliu24/render-3d-segmentation,2022-11-05 14:21:53,0,,5,1437049933,"Sry, I don't find the model_structure.py.  Has this file been uploaded yet?"
Release of Part Annotation on Synthetic Images,qliu24/render-3d-segmentation,2021-12-14 02:23:38,0,,4,1079233742,Thanks a lot for your awesome work! Your dataset is quite important for part segmentation tasks! When will you release the dataset for Part Annotation on Synthetic Images? I'm very looking forward to that :D. Will you also release the blender tool for mesh annotation and the pretrained models in your paper?
when the complete code will be released ?,jinx-USTC/GI-ReID,2022-09-19 04:04:25,0,,3,1377363621,
大佬，项目还会开源吗,jinx-USTC/GI-ReID,2022-01-28 07:47:58,0,,2,1117097080,
Question about imagenet (100epoch) vanilla performance,shashankvkt/alignmixup_cvpr22,2022-10-23 16:00:45,0,,6,1419840532,"Hello. Thank you for your great work.

Currently, I'm trying to reproduce vanilla performance with your imagenet_fast protocol. However, my model 2-3% behind with your configuration. Can you share tricks (data augmentation or optimized configuration) or training log?"
training code about auto-encoder,shashankvkt/alignmixup_cvpr22,2022-09-05 05:25:35,0,,5,1361422160,"hello! It's an excellent work, and could you provide the code about training the auto-encoder please? Thank you very much!"
Would you like to provide the training logs for ImageNet,shashankvkt/alignmixup_cvpr22,2022-06-06 09:00:34,1,,3,1261560601,"Hi, thank you so much for this nice work, would you like to provide the training logs for ImageNet ?"
FID not matching for COCO dataset,wtliao/text2image,2022-09-05 19:16:26,0,,19,1362399812,"Hi,

Thanks for providing the code for the paper!

I tried to reproduce the FID score on the COCO dataset by generating images for the validation dataset as reported in the paper using the generator for 120th epoch (netG_120.pth) and text encoder for 595th epoch. I used the Pytorch implementation of the FID score and it gives me around 121 FID score as opposed to 19 reported.

I had resized the original COCO images to 256x256 resolution to have a consistent image size but the score is still high. 

Even the generation is weird for sample sentences.

For eg.

The image attached has caption ""A close up of a boat on a field with a cloudy sky"". This caption has been taken from the paper but the generation using the final generator model and text encoder model is nowhere near presented in the paper.

Any suggestions from your side as to what has to be done?
![img_0](https://user-images.githubusercontent.com/107555279/188504263-53baeafd-69b1-4aa1-857f-0ac10280e73c.png)

Also can you please mention the difference between main.py and main_finetune.py? There is not much of a difference in both these scripts.

"
FID score in COCO ,wtliao/text2image,2022-05-17 13:57:59,0,,18,1238687112,"Hello，I test the  trained models from your onedrive in COCO，the FID score  is 28.1929. It has a big gap with the paper data (19.37) . My FID code is from  https://github.com/bioinf-jku/TTUR  and my GPU is 4 3090  . I'm confused  about this and want to know the  parameters that your FID code sets about COCO, such as  the image size of groundtruth，whether or not  you split the coco data and so on.Thanks"
How do I generate new images after the training process?,wtliao/text2image,2022-04-06 15:43:33,0,,17,1194793815,What function/script shall be the one used for inference task?
How to use multiple GPUs for training？,wtliao/text2image,2022-04-05 11:56:58,4,,16,1193045859,
pretrain model,lyndonzheng/TFill,2022-10-14 09:54:19,0,,10,1409086871,
About the training of models applied to 256*256 resolution images,lyndonzheng/TFill,2022-07-08 10:44:17,3,,7,1298839508,"Thank you for sharing your wonderful work. We would like to use your model as a baseline, but due to the difference in training settings, the direct application of the model parameters you provided to 256*256 images would not yield particularly good results.  So we would like to train a TFill for a 256*256 image version, can you give us some advice on the training settings regarding the model?"
Access to the celeba test images ,lyndonzheng/TFill,2022-05-18 20:15:12,0,,5,1240466684,"Hi, which celeba images did you use for test the model and comparison?
In this[ file ](https://github.com/lyndonzheng/TFill/blob/main/scripts/test.sh) you mentioned (--img_file ./examples/celeba/img/ \ --mask_file ./examples/celeba/mask/ \  ). How we can access or create test image for comparison?
@lyndonzheng "
Issue on BaseModel,lyndonzheng/TFill,2022-01-19 06:04:28,1,,4,1107679184,"Thanks for releasing the code. 
During installing the code, I met a small problem with the base model. The notification is ""In model.base_model.py, there should be a subclass of BaseModel with class name that matches base in lowercase."". Could you kindly tell me how to fix it? I really appreciate it."
More information about training,lyndonzheng/TFill,2021-11-24 20:51:55,1,,3,1062883974,"Hello, thank you for releasing the code. I really appreciate your work. I am trying to train this model on a custom dataset. But I don't know how the data should be structured. Should I have two folders, one for the image and the other for the masks? What should be the input resolution for the images? What should be changed in ""train.sh"" to allow for training on a custom dataset?"
Results for VisDA-C dadaset,tim-learn/dine,2022-05-06 07:40:16,1,,2,1227525771,"While Fine-tuning the distilled model, the performance on the  VisDA-C dataset drops, the results are as follows:

```
seed=2019: 74.98 --> 74.80
seed=2020: 76.17 --> 75.17
seed=2021: 79.8 --> 78.6.
```
The learning rate setting for VisDA-C dataset may matter."
Getting inverse Depth Map as output,aliyun/NeWCRFs,2022-10-28 19:16:01,0,,28,1427711066,Even after changing post_process = False in test.py I am getting inverse depth map as an output while visualization
test on kitti,aliyun/NeWCRFs,2022-10-11 12:35:36,0,,27,1404565954,"RuntimeError: shape '[1, 7, 7, 23, 7, 256]' is invalid for input of size 2060800

When I test on the kitti dataset, I can't get the output depth map, and the run reports an error as above.Thank you!!!"
In what units is the result returned?,aliyun/NeWCRFs,2022-09-13 12:02:00,1,,26,1371339622,"Sorry for the stupid question, but I can't figure out what units the result is in.

```
    with torch.no_grad():
        for _, sample in enumerate(tqdm(dataloader.data)):
            image = Variable(sample['image'].cuda())
            # Predict
            depth_est = model(image)
```
[[3.6550407 3.6550407 3.461664  ... 3.2897868 3.4753633 3.4753633],...

I think it's the distance meters for each pixel from the camera, but I'm not sure."
AttributeError: module 'glm' has no attribute 'vec3',aliyun/NeWCRFs,2022-09-09 08:31:57,5,,25,1367480014,"AttributeError: module 'glm' has no attribute 'vec3'
AttributeError: module 'glm' has no attribute 'mat4'
i run demo but i cant continue with frame , error is glm has no attribute mat4 and vec3 ;
i test all version by glm ,my os is win11 python 3.9.12 ;
hope u can help me,thank u"
Please provide complete and correct requirements,aliyun/NeWCRFs,2022-08-29 14:18:12,0,,23,1354385950,"Hello, 

I am trying to reproduce the results of your paper and I wanted first to fiddle with demo.py. 
However, I am stuck at the requirements. First of all, the commands you provide do not install pytorch and torchvision in their GPU versions and seem to cause a segfault later on in the process when importing torchvision.
Moreover, demo.py requires scipy, pyside2, opengl, and scikit-image, which are not listed in the requirements.

If I manage to fix my error, I can open a PR with the necessary changes, but I would be extremely grateful if you provided a complete and correct requirements.txt anyway 😄 "
About GPU Memory requirements for training,aliyun/NeWCRFs,2022-08-23 05:10:45,1,,21,1347346345,"
Hello, I configured the environment with reference to the information in the readme.md file. But it keeps getting the error ""CUDA out of memory""

_RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.92 GiB already allocated; 0 bytes free; 5.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF_

_RuntimeError: HIP out of memory. Tried to allocate 54.00 MiB (GPU 0; 15.98 GiB total capacity; 14.39 GiB already allocated; 17179869183.78 GiB free; 14.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_HIP_ALLOC_CONF_



"
Config file to train kitti full dataset,aliyun/NeWCRFs,2022-08-20 16:04:22,0,,20,1345210654,"Hi,

Can you please provide the config file to train official kitti dataset split?

Regards,
Ashutosh"
Eval result,aliyun/NeWCRFs,2022-08-17 15:51:58,1,,19,1341974238,"那个我跟着装了这边的库，跟着BTS那边首页的readme下载了NYU和KITTI，KITTI好像只有GT所以没办法用你这边的eval，然而我NYU直接跑那个eval出来的结果和你这边readme给的数据以及论文数据差距很大，我跑""python newcrfs/eval.py configs/arguments_eval_nyu.txt""的结果大概如下：
![image](https://user-images.githubusercontent.com/71368398/185185291-7eea8b1b-67e4-4f36-849b-8a79d684b2b6.png)
想问一下是什么原因，我检查了一下数据集应该是匹配的

Well, I downloaded NYU and Kitti follow the readme on the front page of BTS. It seems that Kitti only has GT, so I can't use your eval code to test. However, the results of NYU dataset running the eval code differ greatly from the data given by your readme and the paper data. My result when running ""python newcrfs/eval.py configs/arguments_eval_nyu.txt"" is like following:
![image](https://user-images.githubusercontent.com/71368398/185185291-7eea8b1b-67e4-4f36-849b-8a79d684b2b6.png)
I wonder why cause I checked that the data set is matched."
Test on custom images?,aliyun/NeWCRFs,2022-08-09 08:11:15,1,,18,1332882762,Could you provide an interface for testing on custom images? your test.py seems be prepared for nyu and kitti.
Does this model predict the depth of video in the same way as pictures?,aliyun/NeWCRFs,2022-08-08 10:17:34,1,,17,1331648475,Does this model predict the depth of video in the same way as pictures?
pretrained swin-L model and loaded state dict do not match exactly,aliyun/NeWCRFs,2022-07-29 08:35:46,1,,16,1321932547,"Here is the output:
===============
Load encoder backbone from: model_zoo/swin_transformer/swin_large_patch4_window7_224_22k.pth
The model and loaded state dict do not match exactly

unexpected key in source state_dict: norm.weight, norm.bias, head.weight, head.bias, layers.0.blocks.1.attn_mask, layers.1.blocks.1.attn_mask, layers.2.blocks.1.attn_mask, layers.2.blocks.3.attn_mask, layers.2.blocks.5.attn_mask, layers.2.blocks.7.attn_mask, layers.2.blocks.9.attn_mask, layers.2.blocks.11.attn_mask, layers.2.blocks.13.attn_mask, layers.2.blocks.15.attn_mask, layers.2.blocks.17.attn_mask

missing keys in source state_dict: norm0.weight, norm0.bias, norm1.weight, norm1.bias, norm2.weight, norm2.bias, norm3.weight, norm3.bias
================
It seems that the pretrained swin-L model and the source state dict do not match."
About Computation resources,aliyun/NeWCRFs,2022-07-28 08:41:31,2,,14,1320594641,"Hi, 
It seems you use multiple GPUs for the training.
Can you answer how many GPUs(""Nvidia  GTX  2080  Ti  GPUs."" mentioned in your paper) did you use for training?

Thanks.
"
about test based on kitti,aliyun/NeWCRFs,2022-07-20 13:14:05,1,,13,1311077414,"Hello, I would like to ask you about what is the command when using KITTI dataset for testing. And what is the folder structure of the KITTI dataset? "
如何导出ply格式的点云文件,aliyun/NeWCRFs,2022-05-24 07:25:49,0,,11,1246103216,
Model weights on panoramic image training,aliyun/NeWCRFs,2022-05-13 09:58:50,1,,9,1235019449,"Hi!

Congratulations on your work!

Will you release the model weights for the panoramic images/training code for it?

Besides, in the paper you mention that you also tried to train the model with 50k images and then fine tune it with the Matterport 3D dataset. Did you obtain those 50k real world images using the Matterport camera?

Thanks in advance!"
验证过程中显存会不断增加,aliyun/NeWCRFs,2022-05-12 14:13:03,14,,8,1234049048,"您好，我在使用您的代码训练时发现，每次验证，显存都会增加，最后导致显存out of memory中断了训练，请问下这个问题怎么解决呢？
"
KITTI Crops,aliyun/NeWCRFs,2022-05-09 10:17:15,2,,7,1229487980,"Thanks for releasing the code of your work!

I've realized that when you evaluate on the KITTI dataset you first perform the kb_crop and then the garg_crop. I don't think that's the expected behavior.

When performing the evaluation, on the dataloader you do:
```python
if self.args.do_kb_crop is True:
  height = image.shape[0]
  width = image.shape[1]
  top_margin = int(height - 352)
  left_margin = int((width - 1216) / 2)
  image = image[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]
  if self.mode == 'online_eval' and has_valid_depth:
      depth_gt = depth_gt[top_margin:top_margin + 352, left_margin:left_margin + 1216, :]
```

So you are cropping both the image and the ground truth. Then, on `eval.py`, you:
```python
if args.do_kb_crop:
    height, width = gt_depth.shape
    top_margin = int(height - 352)
    left_margin = int((width - 1216) / 2)
    pred_depth_uncropped = np.zeros((height, width), dtype=np.float32)
    pred_depth_uncropped[top_margin:top_margin + 352, left_margin:left_margin + 1216] = pred_depth
    pred_depth = pred_depth_uncropped
```

But this actually does nothing, as the ground truth depth has already been cropped.

Your evaluation code is based on the work of BTS, but in their code they do not kb_crop the ground truth when evaluating. They crop the input image with both kb_crop and garg_crop, but the ground truth only with garg_crop. That means that, as I understood the code, your evaluation code and the BTS perform a different cropping on the ground truth. 

Evaluating on the ground truth only with the garg_crop (and inputs with both crops as in BTS) worsens the results.

Am I missing something?

thanks a lot!"
Input resolution seems to have a big impact on performance,aliyun/NeWCRFs,2022-04-22 11:19:50,4,,6,1212233447,"Hi, I find that the input resolution seems to have a very big impact on the performance. 

When I keep the input shape unchanged, e.g. 640 x 480,  everything seems fine, I can reproduce the metrics claimed in the paper on NYU test set.
However, when I change the input resolution, e.g. resize every image to 384 x 288, the RMSE and other metric show a notable drop。 The RMSE drops from 0.33 to 0.95. Why does this happen? Requiring the same resolution as training in inference stage 
seems unreasonable.

Thanks"
About the learning rate,aliyun/NeWCRFs,2022-04-19 17:04:23,1,,5,1208628396,"Hi, it seems that the learing rate mentioned in your paper is a constant, but in your training code, the lr :
![image](https://user-images.githubusercontent.com/53251573/164056979-54217f5b-eb82-49dd-bc2d-14f4691388d2.png)
can you explain that ? Looking forward to your reply."
"RuntimeError: shape '[1, 20, 7, 35, 7, 256]' is invalid for input of size 8843520",aliyun/NeWCRFs,2022-04-16 08:24:01,7,,4,1206053711,"Hi,I'm trying to test on my own test set, but the following issues arise and I would like to ask you how to fix them
The environment used is as follows：
python 3.8
cuda 10.1
pytorch 1.8.1
mmcv 1.4.8
![image](https://user-images.githubusercontent.com/30140950/163667933-c8d87150-3269-4fe7-8eae-6c06989e277f.png)
"
Docker container is not pulling with given command: sudo docker pull 8feef0e83aed,Shahad24/AICITY2022_Track3_Team95,2022-06-22 19:30:17,1,,1,1280769930,"Error response from daemon: pull access denied for 8feef0e83aed, repository does not exist or may require 'docker login': denied: requested access to the resource is denied"
How to train on CIFAR,flamiezhu/bcl,2022-11-02 11:34:17,1,,4,1432989250,"Thanks for your work!
I wonder the part of CIFAR, will you release this part?"
Could the v1/v2/v3 input data be same?,flamiezhu/bcl,2022-10-11 11:17:48,2,,3,1404460143,"Hi @FlamieZhu 

Could the v1/v2/v3 input data be the same for the proposed BCL algorithm? Or must the v2/v3 be with same input different from v1 data augmentation? What's the influence of the same v1/v2/v3 for the proposed BCL loss? Thanks.

Xiaobing"
"How to understand feat_mlp, logits, centers_logits in 2D image network?",flamiezhu/bcl,2022-10-04 08:57:41,3,,1,1395903118,"Hi @FlamieZhu 

I‘m trying to apply the loss you proposed to 3D point cloud. I have one question. How to understand feat_mlp, logits, centers_logits in 2D image network? If I use this loss for 3D semantic segmentation, how to correspond them? Thanks a lot.

I have seen that you used CE+SCL in training, but in testing just used CE. Could I still use CE+SCL in testing?

Xiaobing"
How to input with ImageDataGenerator ? ,clementchadebec/benchmark_VAE,2022-11-06 16:17:18,0,,63,1437458273,"Hi, I saw that the examples use tf.datasets but I would like to use ImageDataGenerator instead. Do you know how to do it ? Thank you very much. "
Loss value history,clementchadebec/benchmark_VAE,2022-11-01 15:05:05,3,question,62,1431586256,"**Is your feature request related to a problem? Please describe.**
I can't seem to find documentation or an object that holds the training loss history.

**Describe the solution you'd like**
Training history is saved in a logger, but not in a structured python object from which one can make basic loss curve plots. I would like to have an output from the `train` method of a trainer, or from a pipeline that allows me to plot basic loss curves.

**Describe alternatives you've considered**
In the BaseTrainer one could have (I think, I'm not sure how to do it) a callback that saves this basic information in an object.
**Additional context**
My code is taken from the examples:
```python
# Set up the training configuration
my_training_config = BaseTrainerConfig(
    output_dir='my_model',
    num_epochs=100,
    learning_rate=1e-3,
    batch_size=256,
    steps_saving=None
)
# Set up the model configuration
my_vae_config = model_config = VAEConfig(
    input_dim=(data.shape[1],),
    latent_dim=10
)
# Build the model
my_vae_model = VAE(
    model_config=my_vae_config,
    decoder=DecoderVAE(args=my_vae_config))
# Build the Pipeline
pipeline = TrainingPipeline(
    training_config=my_training_config,
    model=my_vae_model
)
# Launch the Pipeline
pipeline(
    train_data=data, # must be torch.Tensor or np.array
    eval_data=data[:1024] # must be torch.Tensor or np.array
)
```
As far as I know, there's no way of passing a callback to the training pipeline."
[MODEL REQUEST] TopoAE,clementchadebec/benchmark_VAE,2022-10-26 07:12:55,1,new model,61,1423545908,"Hello and thank you for a great repo!

I was wondering if it would make sense to add the topological autoencoder from : https://arxiv.org/abs/1906.00722

The authors' implementation lives [here](https://github.com/BorgwardtLab/topological-autoencoders) and used for dataviz by e.g. [UMATO](https://github.com/hyungkwonko/umato/tree/master/src/evaluation/models/topoae) but is no longer being maintained.
"
Scheduling within models,clementchadebec/benchmark_VAE,2022-08-05 13:15:25,1,feature request,41,1329927546,Some models make use of some sort of scheduling or annealing internally (e.g. KL warmup or temperature annealing) based on the current step index - what's the correct way to implement this within `pythae`?
Model request: SQ-VAE,clementchadebec/benchmark_VAE,2022-07-06 08:38:43,0,new model,30,1295464674,"**Feature request**
It would be nice to see this variant (https://arxiv.org/abs/2205.07547) of the SQ-VAE implemented in the library."
Release results VTM and AV1 version?,InterDigitalInc/CompressAI,2022-10-17 01:55:58,0,,166,1410754670,"Thank you for publishing 128f003 results, but I'd like to know what av1 and vtm version you used for testing?  Is it the latest VTM reference software  https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM and AOM AV1 reference software https://aomedia.googlesource.com/aom?"
Help: VTM Usage Issue,InterDigitalInc/CompressAI,2022-09-21 21:34:37,1,,163,1381520787,"I am trying to use VTM codec to encode an JPEG image and decode it. 
I am not very sure about the ""-b"" and ""-c"" arguments so I got errors. 
I use Colab to test the codes.
[https://colab.research.google.com/drive/1lxPfasLZnMwfd8GaO4y5eiC6aeAYvxca?usp=sharing](url)
I created one image folder under the compressai folder and uploaded one JPEG image into the image folder.
I also created a VTM_image folder and I need to save the compressed file and the decoded image. 

I have a big difficulty to figure out how to solve the errors. Can you see the codes and help me with the issues? Thank you!
------------------------------------------------------------------------------------
I got to know I need to download VTM software and then use the compressai codes. I will try to test it. Thank you!


"
Problem when I download compressai with pip ,InterDigitalInc/CompressAI,2022-09-10 09:32:00,5,,160,1368564984,"## Bug

Hi, first of all thanks for your precious work, amazing. 
Recently I tried to download your repository via the command ""pip install compressai"", and it failed. It seemed that it is not a problem of pip: it is possible that it is a problem of setup requirements, 

OS: WIndows 11
PC: Inspiron 14 5410 
Processor: 11th Gen Intel(R) Core(TM) i7-1195G7 @ 2.90GHz   1.80 GHz 
Python: Python 3.8.8
Gcc compiler: g++ (GCC) 11.2.0

In the following screen I report the bug.


<img width=""941"" alt=""Immagine 2022-09-10 112533"" src=""https://user-images.githubusercontent.com/43781044/189477476-f92c56a0-c629-4919-bb07-7e2eb8f454e8.png"">

Steps to reproduce the behavior:

1. open terminal
2. pip install compressai 



## Environment

Please copy and paste the output from `python3 -m torch.utils.collect_env`


Collecting environment information...
PyTorch version: 1.12.1+cpu
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: Microsoft Windows 11 Home
GCC version: (GCC) 11.2.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: N/A

Python version: 3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)] (64-bit runtime)
Python platform: Windows-10-10.0.22000-SP0
Is CUDA available: False
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.2
[pip3] torch==1.12.1
[conda] Could not collect
"
Bug when I try to evaluate a model on my own dataset,InterDigitalInc/CompressAI,2022-06-09 12:09:47,2,,145,1266035915,"Hi, 

I think I found a bug when I run the following command (suggested by you):

python3 -m compressai.utils.eval_model checkpoint /path/to/images/folder/ -a $ARCH -p $MODEL_CHECKPOINT...

The bug is the following: 

Traceback (most recent call last):
  File ""/opt/conda/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/lib/python3.7/site-packages/compressai/utils/eval_model/__main__.py"", line 310, in <module>
    main(sys.argv[1:])
  File ""/opt/conda/lib/python3.7/site-packages/compressai/utils/eval_model/__main__.py"", line 286, in main
    model = load_func(*opts, run)
  File ""/opt/conda/lib/python3.7/site-packages/compressai/utils/eval_model/__main__.py"", line 150, in load_checkpoint
    return architectures[arch].from_state_dict(state_dict).eval()
  File ""/opt/conda/lib/python3.7/site-packages/compressai/models/google.py"", line 157, in from_state_dict
    N = state_dict[""g_a.0.weight""].size(0)
KeyError: 'g_a.0.weight'


I think also that the error comes from the fact that you should pass the actual state_dict of the net, which is state_dict[""state_dict""], not only state_dict; in my opinion, we should have something like:

N = state_dict[""state_dict""][""g_a.0.weight""].size(0)


Maybe I miss something in my previous command.

Alberto 
"
Distributed Data Parallel support given that DataParallel may be deprecated in the next release of PyTorch,InterDigitalInc/CompressAI,2021-11-16 02:48:09,0,,104,1054373161,"Hi @jbegaint @fracape 
I'm still waiting for CompressAI's DDP support mentioned [here](https://github.com/InterDigitalInc/CompressAI/issues/30#issuecomment-885786036). Could you please reconsider this option again?

I think it's a great timing to consider it given that PyTorch team is thinking of DataParallel deprecation with their upcoming v1.11 release of PyTorch (See the following issue) and many projects 

https://github.com/pytorch/pytorch/issues/65936

## Feature
Distributed Data Parallel support for faster model training

## Motivation

- PyTorch team is planning to deprecate DataParallel with their upcoming v1.11 release of PyTorch
https://github.com/pytorch/pytorch/issues/65936
- Many projects depend on this great framework as shown [here](https://github.com/InterDigitalInc/CompressAI/network/dependents?package_id=UGFja2FnZS0xODkxNzI4ODI0), and faster model training with your DDP support would be very appreciated in the communities

Thank you!
"
Wrong ms-ssim q4 pretrained model for cheng2020-anchor,InterDigitalInc/CompressAI,2021-10-23 11:27:54,1,,100,1034145339,"Thanks for your great work!

When I download the ms-ssim q4 pretrained model for cheng2020-anchor

https://github.com/InterDigitalInc/CompressAI/blob/278f45e6f444448d3c44d973e51f9723768afa18/compressai/zoo/image.py#L149

, it seems that the channel dimension of this checkpoint is 128, which should be 192 instead.

https://github.com/InterDigitalInc/CompressAI/blob/278f45e6f444448d3c44d973e51f9723768afa18/compressai/zoo/image.py#L215-L222

Could you help fix this pretrained model? Thank you very much.

"
Support for ONNX export,InterDigitalInc/CompressAI,2021-08-27 14:22:32,1,,87,981301558,"## Feature
Enable CompressAI models to be exportable to the ONNX format.

## Motivation
I would like to use some of the CompressAI models in a third-party inference framework which allows models to be imported from ONNX files. However, the models currently do not support ONNX export in my tests.

Therefore, I'd like to ask: Is it generally possible to rewrite the CompressAI models to support ONNX export? I just started reading into the ONNX standard so my understanding might be incomplete. Possible issues that I came up with so far are:
- Some ops within the models are not supported by the ONNX opset (as indicated by the error message below)
- The way a model returns output cannot be handled by ONNX
- My code has a bug

Any help/feedback is appreciated!

## Additional context
What I tried so far:

```
import torch
from compressai.zoo import models

net = models[""bmshj2018-factorized""](quality=1, metric=""mse"", pretrained=True)
# Some dummy input
x = torch.randn(1, 3, 224, 224, requires_grad=True)

# Export the model
torch.onnx.export(net,                       # model being run
                  x,                         # model input (or a tuple for multiple inputs)
                  ""model.onnx"",              # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=11,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = ['input'],   # the model's input names
                  output_names = ['output'], # the model's output names
                  dynamic_axes={'input': {0 : 'batch_size'},    # variable length axes
                                'output': {0 : 'batch_size'}}
                 )
```

The above code fails with
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_15137/1585788810.py in <module>
      1 # Export the model
----> 2 torch.onnx.export(net,                  # model being run
      3                   x,                         # model input (or a tuple for multiple inputs)
      4                   ""bmshj2018-factorized.onnx"",           # where to save the model (can be a file or file-like object)
      5                   export_params=True,        # store the trained parameter weights inside the model file

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)
    273 
    274     from torch.onnx import utils
--> 275     return utils.export(model, args, f, export_params, verbose, training,
    276                         input_names, output_names, aten, export_raw_ir,
    277                         operator_export_type, opset_version, _retain_param_name,

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)
     86         else:
     87             operator_export_type = OperatorExportTypes.ONNX
---> 88     _export(model, args, f, export_params, verbose, training, input_names, output_names,
     89             operator_export_type=operator_export_type, opset_version=opset_version,
     90             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference)
    687 
    688             graph, params_dict, torch_out = \
--> 689                 _model_to_graph(model, args, verbose, input_names,
    690                                 output_names, operator_export_type,
    691                                 example_outputs, _retain_param_name,

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)
    461     params_dict = _get_named_param_dict(graph, params)
    462 
--> 463     graph = _optimize_graph(graph, operator_export_type,
    464                             _disable_torch_constant_prop=_disable_torch_constant_prop,
    465                             fixed_batch_size=fixed_batch_size, params_dict=params_dict,

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)
    198             dynamic_axes = {} if dynamic_axes is None else dynamic_axes
    199             torch._C._jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)
--> 200         graph = torch._C._jit_pass_onnx(graph, operator_export_type)
    201         torch._C._jit_pass_lint(graph)
    202 

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/__init__.py in _run_symbolic_function(*args, **kwargs)
    311 def _run_symbolic_function(*args, **kwargs):
    312     from torch.onnx import utils
--> 313     return utils._run_symbolic_function(*args, **kwargs)
    314 
    315 

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/utils.py in _run_symbolic_function(g, block, n, inputs, env, operator_export_type)
    992                     return None
    993                 attrs = {k: n[k] for k in n.attributeNames()}
--> 994                 return symbolic_fn(g, *inputs, **attrs)
    995 
    996         elif ns == ""prim"":

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/symbolic_helper.py in wrapper(g, *args, **kwargs)
    170             if len(kwargs) == 1:
    171                 assert '_outputs' in kwargs
--> 172             return fn(g, *args, **kwargs)
    173 
    174         return wrapper

~/Projects/tensorrt_test/venv/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py in _convolution(g, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups, benchmark, deterministic, cudnn_enabled, allow_tf32)
   1271 
   1272     if kernel_shape is None or any([i is None for i in kernel_shape]):
-> 1273         raise RuntimeError('Unsupported: ONNX export of convolution for kernel '
   1274                            'of unknown shape.')
   1275 

RuntimeError: Unsupported: ONNX export of convolution for kernel of unknown shape.
```"
JIT not working for (de)compress,InterDigitalInc/CompressAI,2021-07-06 09:10:02,3,,72,937683444,"## Bug

I see that you are testing whether JIT of the EntropyBottleneck works (and it does), but JITing the compress/decompress method of an entropy bottleneck does not work. which means that as a whole the ENtropy Bottleneck cannot really be used with JIT.

## To Reproduce

```python
from compressai.entropy_models import EntropyBottleneck
import torch

torch.jit.script(EntropyBottleneck(4).compress)
torch.jit.script(EntropyBottleneck(4).decompress)
```

![Screen Shot 2021-07-06 at 11 07 07](https://user-images.githubusercontent.com/24327668/124574125-7661d000-de4a-11eb-9d53-508bbffb7b58.png)
![Screen Shot 2021-07-06 at 11 10 46](https://user-images.githubusercontent.com/24327668/124574536-db1d2a80-de4a-11eb-98fb-6c9cac9756ce.png)

## Expected behavior

Successful JIT conversion.

## Environment

```
- compressai==1.1.5
- torch==1.7.1
```
"
How to use MNIST dataset?,emtiyaz/vmp-for-svae,2019-09-05 16:31:42,0,,8,489872869,"I use the same setting as auto dataset and displayed problem:
```
Crashed for config:
 {'dataset': 'mnist', 'method': 'svae-cvi', 'lr': 0.0003, 'lrcvi': 0.2, 'decay_rate': 0.
95, 'K': 20, 'L': 6, 'U': 50, 'seed': 0}
list index out of range
```

if there is a example of how to config MNIST dataset would be nice !

"
"'c' argument has 20 elements, which is not acceptable for use with 'x' with size 10, 'y' with size 10.`",emtiyaz/vmp-for-svae,2019-06-12 10:14:17,0,,7,455128025,"`2019-06-12 08:37:57.382950: W tensorflow/core/kernels/queue_base.cc:277] _0_data_prep/shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
2019-06-12 08:37:57.383017: W tensorflow/core/kernels/queue_base.cc:277] _0_data_prep/shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed
Crashed for config: 
{'dataset': 'auto', 'method': 'svae-cvi', 'lr': 0.0003, 'lrcvi': 0.2, 'decay_rate': 0.95, 'K': 10, 'L': 6, 'U': 50, 'seed': 0}
'c' argument has 20 elements, which is not acceptable for use with 'x' with size 10, 'y' with size 10.`

Nothing change to codes. env: tf 1.11.0 GPU

"
module 'tensorflow.contrib' has no attribute 'linalg',emtiyaz/vmp-for-svae,2018-11-13 18:52:01,3,,5,380372320,"Hello, I am trying to run the experiments.py for the first time. and got the following error:

module 'tensorflow.contrib' has no attribute 'linalg'

which prompted me to think which version of tensorflow does the code use? I have tf.__version__ == '1.12.0'

Thanks,
Machuck"
Hilbert path code is wrong,Bojian/Hilbert-CNN,2020-03-16 21:05:54,0,,6,582598743,"Hi-

I tried out a few different Hilbert path libraries and only one of them did it right. I verified the libraries by drawing a path through different 2^N size grids.

The HilbertCurve package by G. Altay was the one that worked."
Can you please add train.py ? ,Bojian/Hilbert-CNN,2019-09-01 19:41:30,1,,5,487923445,
train.py is empty ?,Bojian/Hilbert-CNN,2019-04-27 17:34:48,0,,4,437966422,Can you please upload the train.py ? 
train.py,Bojian/Hilbert-CNN,2018-11-08 22:54:22,0,,2,378951487,"thanks for this awesome repository! Could you provide the train.py as well?
"
visualize data,Bojian/Hilbert-CNN,2018-07-08 05:28:55,4,,1,339198453,@Doulrs How can visualize the dna sequence after apply Hilbert curve?. I want to see images of each dna sequence and save in image format.
Some errors in loading dataset,jramapuram/LifelongVAE_pytorch,2022-09-11 08:43:28,0,,5,1368876366,"Your paper is excellent, but there are some unresolved errors when running the code. Google told me that I need to change the torch version to 0.4.1, but I don't want to do so. Is there any other way

    raise ValueError('{} attribute should not be set after {} is '
ValueError: dataset attribute should not be set after DataLoader is initialized"
Type error: get_updates() got an unexpected keyword argument 'learning_rate_multipliers',YerevaNN/DIIN-in-Keras,2018-12-11 20:47:06,0,,6,389946638,"tried to run the model but got the error:
Type error: get_updates() got an unexpected keyword argument 'learning_rate_multipliers'"
How to load and test the pretrained model?,YerevaNN/DIIN-in-Keras,2018-04-29 14:29:50,0,,5,318721549,I'm a DL beginer and may I ask how to load and test the pretrained model?
Questions about logp calculation and GNN input in kmeans.py,alexnowakvila/DiCoNet,2020-11-17 03:24:37,0,,1,744376844,"Hi! 

Thank you for your kindness to share your code! Divide and conquer strategy is really awesome. 

I have read your paper and Kmeans code carefully with several unclear questions remained. Looking forward to your replay:

1. `logp` calculation

As shown below, when `last ==false`, variable `Lgp` will be reassigned to a zero tensor at every iteration of `k`, thus only the last `Lgp` is calculated in the backward process.  Why not take all split steps into account?
https://github.com/alexnowakvila/DiCoNet/blob/15e48e33f77ab918c726a9291666bbe17ee5072e/src/Kmeans/kmeans.py#L279-L296

2. `OP` `x` `Y` 

I find it really hard to understand each variable's functionality, maybe it's because I am not familiar with GNN. Could you simply explain or refer me to some related links? For example, what does `W` `D` `U` in `OP` means?

https://github.com/alexnowakvila/DiCoNet/blob/15e48e33f77ab918c726a9291666bbe17ee5072e/src/Kmeans/kmeans.py#L147-L163

Thanks a lot:) "
Index out of range error while training the model,Microsoft/PixelDefend,2020-08-03 11:21:03,0,,3,672004028,"![image](https://user-images.githubusercontent.com/39803284/89177590-75c11980-d5a9-11ea-8827-1264108d473f.png)
"
FileNotFoundError,Microsoft/PixelDefend,2020-04-10 01:44:36,0,,1,597646598,"Dear sir，
  Hi,when I train the PixelCNN++ with train_pxpp_softmax.py,Error as follows:
FileNotFoundError: results/data/f mnist clean/test batch Not found!
  Could you please tell me how to deal with ?
  My environment is tf=1.2,python=3.6,numpy=1.13.1."
What are the simplest methods for the label noise problem?,khetan2/MBEM,2020-06-03 02:29:42,0,,3,629643438,"If I have enough low quality data from unsupervised methods or rule-based methods.

I read from https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise ,but these methods are a little complex for me.

In detail, I deal with a multi-label classification task. First I crawl web page such as wiki and use regex-based rule to mark the label. The model input is the wiki title and the model output is the rule-matched labels from wiki content. My task is to predict the labels for the wiki title.

Do you think **removing the wrong data predicted by trained model** is a simple but effective method?

@khetan2  Thank you very much!"
Main function starts MBEM with trained resnet prediction?,khetan2/MBEM,2019-03-07 21:24:29,0,,2,418519656,"It is tripping to see the code actually starts the MBEM algorithm after training the resnet with weighted majority vote labels twice. Is this how the results are obtained?

`print ""Algorithm: MBEM:\t\t\t"",    
    # running the proposed algorithm ""MBEM: model bootstrapped expectation maximization"" 
    # computing posterior probabilities of the true labels given the noisy labels and the worker identities.
    # post_prob_DS function takes the noisy labels given by the workers ""resp_org"", model prediction obtained 
    # by running ""weighted majority vote"" algorithm, and the worker identities.
    probs_est_labels = post_prob_DS(resp_org[valid],naive_pred[valid],workers_this_example[valid])      
    algo_agg = np.zeros((n,k))    
    algo_agg[valid] = probs_est_labels
    # calling the ""call_train"" function with aggregated labels being the posterior probability distribution of the 
    # examples given the model prediction obtained using the ""weighted majority vote"" algorithm.
    _, val_acc = call_train(n,samples,k,algo_agg[valid],workers_val_label,fname,epochs,depth,gpus)`"
