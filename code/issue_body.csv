"Pip fails with the following error:

```
Pip subprocess error:
  Running command git clone -q https://github.com/LPMP/LPMP.git /tmp/pip-req-build-btoiy_7y
  Running command git submodule update --init --recursive -q
  fatal: remote error: upload-pack: not our ref 6df61c7d478016053cffa2f998e0ef4fdf57be8c
  fatal: The remote end hung up unexpectedly
  Fetched in submodule path 'external/DD_ILP', but it did not contain 6df61c7d478016053cffa2f998e0ef4fdf57be8c. Direct fetching of that commit failed.

```

The commit `6df61c7d478016053cffa2f998e0ef4fdf57be8c` does not exist in DD_ILP repo, but is pointed to in `.gitmodules`. Please fix."
"I am trying to install the precise version of graph matching as used in ""Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers."". I used the following command to install it, which you gave in README.md.
 `python3 -m pip install git+https://github.com/lpmp/LPMP.git@keypiont_submission` 

However, I meet a problem as follow:
```
Collecting git+https://github.com/lpmp/LPMP.git@keypiont_submission
  Cloning https://github.com/lpmp/LPMP.git (to revision keypiont_submission) to /tmp/pip-req-build-8vg9x9xr
  Running command git clone -q https://github.com/lpmp/LPMP.git /tmp/pip-req-build-8vg9x9xr
  WARNING: Did not find branch or tag 'keypiont_submission', assuming revision or ref.
  Running command git checkout -q keypiont_submission
  error: pathspec 'keypiont_submission' did not match any file(s) known to git.
WARNING: Discarding git+https://github.com/lpmp/LPMP.git@keypiont_submission. Command errored out with exit status 1: git checkout -q keypiont_submission Check the logs for full command output.
ERROR: Command errored out with exit status 1: git checkout -q keypiont_submission Check the logs for full command output.
```

Did the ""keypiont_submission"" branch removed ? How can I get this graph matching solver ?

"
"Hi, when I run the command
`PACKAGES=""gm"" python3 -m pip install git+https://github.com/AndreaHor/LPMP.git`
I get an error during installing collected packages: lpmp-py. I try to install LPMP on Ubuntu 18.04 with gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~18.04). The full error message is below. Thank you for your time and attention.

```
  Cloning https://github.com/AndreaHor/LPMP.git to /tmp/pip-aw_xn3gm-build
Installing collected packages: lpmp-py
  Running setup.py install for lpmp-py ... error
    Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-aw_xn3gm-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-vi1j_bdf-record/install-record.txt --single-version-externally-managed --compile --user --prefix=:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/lpmp_py
    copying lpmp_py/raw_solvers.py -> build/lib.linux-x86_64-3.6/lpmp_py
    copying lpmp_py/__init__.py -> build/lib.linux-x86_64-3.6/lpmp_py
    running build_ext
    extension name: graph_matching_py
    Testing gcc...
    ...has version 9.4.0
    Found suitable gcc/g++ version gcc g++
    -- The C compiler identification is GNU 9.4.0
    -- The CXX compiler identification is GNU 9.4.0
    -- Check for working C compiler: /usr/bin/gcc
    -- Check for working C compiler: /usr/bin/gcc -- works
    -- Detecting C compiler ABI info
    -- Detecting C compiler ABI info - done
    -- Detecting C compile features
    -- Detecting C compile features - done
    -- Check for working CXX compiler: /usr/bin/g++
    -- Check for working CXX compiler: /usr/bin/g++ -- works
    -- Detecting CXX compiler ABI info
    -- Detecting CXX compiler ABI info - done
    -- Detecting CXX compile features
    -- Detecting CXX compile features - done
    -- Found OpenMP_C: -fopenmp (found version ""4.5"")
    -- Found OpenMP_CXX: -fopenmp (found version ""4.5"")
    -- Found OpenMP: TRUE (found version ""4.5"")
    -- HDF5: Using hdf5 compiler wrapper to determine CXX configuration
    -- Found HDF5: /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_cpp.so;/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so;/usr/lib/x86_64-linux-gnu/libpthread.so;/usr/lib/x86_64-linux-gnu/libsz.so;/usr/lib/x86_64-linux-gnu/libz.so;/usr/lib/x86_64-linux-gnu/libdl.so;/usr/lib/x86_64-linux-gnu/libm.so (found version ""1.10.0.1"") found components:  CXX
    -- /usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5_cpp.so/usr/lib/x86_64-linux-gnu/hdf5/serial/libhdf5.so/usr/lib/x86_64-linux-gnu/libpthread.so/usr/lib/x86_64-linux-gnu/libsz.so/usr/lib/x86_64-linux-gnu/libz.so/usr/lib/x86_64-linux-gnu/libdl.so/usr/lib/x86_64-linux-gnu/libm.so
    -- /usr/include/hdf5/serial
    --
    -- pybind11 v2.6.2 dev1
    CMake Warning at external/pybind11/tools/pybind11Common.cmake:174 (message):
      USE -DCMAKE_CXX_STANDARD=17 instead of PYBIND11_CPP_STANDARD
    Call Stack (most recent call first):
      external/pybind11/CMakeLists.txt:169 (include)
    
    
    -- Found PythonInterp: /usr/bin/python3 (found version ""3.6.9"")
    -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.6m.so
    -- Performing Test HAS_FLTO
    -- Performing Test HAS_FLTO - Success
    -- Found OpenMP_C: -fopenmp (found version ""4.5"")
    -- Found OpenMP_CXX: -fopenmp (found version ""4.5"")
    -- Configuring done
    CMake Warning (dev) at src/asymmetric_multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""asymmetric_multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    CMake Warning (dev) at src/multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    CMake Warning (dev) at src/asymmetric_multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""asymmetric_multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    CMake Warning (dev) at src/multiway_cut/CMakeLists.txt:7 (add_library):
      Policy CMP0038 is not set: Targets may not link directly to themselves.
      Run ""cmake --help-policy CMP0038"" for policy details.  Use the cmake_policy
      command to set the policy and suppress this warning.
    
      Target ""multiway_cut_gaec"" links to itself.
    This warning is for project developers.  Use -Wno-dev to suppress it.
    
    -- Generating done
    -- Build files have been written to: /tmp/pip-aw_xn3gm-build/build/temp.linux-x86_64-3.6
    Scanning dependencies of target lglmain
    Scanning dependencies of target lgl
    [ 25%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lglmain.dir/lglmain.c.o
    [ 25%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lglib.c.o
    [ 25%] Linking C static library liblglmain.a
    [ 25%] Built target lglmain
    [ 25%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lglopts.c.o
    [ 50%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lgldimacs.c.o
    /tmp/pip-aw_xn3gm-build/external/DD_ILP/external/lingeling/lglib.c: In function ‘lglstampall’:
    /tmp/pip-aw_xn3gm-build/external/DD_ILP/external/lingeling/lglib.c:19233:2: warning: this ‘if’ clause does not guard... [-Wmisleading-indentation]
    19233 |  if (rootsonly) noimpls++; goto CONTINUE;
          |  ^~
    /tmp/pip-aw_xn3gm-build/external/DD_ILP/external/lingeling/lglib.c:19233:28: note: ...this statement, but the latter is misleadingly indented as if it were guarded by the ‘if’
    19233 |  if (rootsonly) noimpls++; goto CONTINUE;
          |                            ^~~~
    [ 50%] Building C object external/DD_ILP/external/lingeling/CMakeFiles/lgl.dir/lglbnr.c.o
    [ 50%] Linking C static library liblgl.a
    [ 50%] Built target lgl
    Scanning dependencies of target graph_matching_frank_wolfe
    Scanning dependencies of target MRF_factors
    [ 50%] Building CXX object src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/graph_matching_frank_wolfe.cpp.o
    [ 75%] Building CXX object src/mrf/CMakeFiles/MRF_factors.dir/pairwise_simplex_factor.cpp.o
    g++: fatal error: Killed signal terminated program cc1plus
    compilation terminated.
    src/mrf/CMakeFiles/MRF_factors.dir/build.make:62: recipe for target 'src/mrf/CMakeFiles/MRF_factors.dir/pairwise_simplex_factor.cpp.o' failed
    make[3]: *** [src/mrf/CMakeFiles/MRF_factors.dir/pairwise_simplex_factor.cpp.o] Error 1
    CMakeFiles/Makefile2:1181: recipe for target 'src/mrf/CMakeFiles/MRF_factors.dir/all' failed
    make[2]: *** [src/mrf/CMakeFiles/MRF_factors.dir/all] Error 2
    make[2]: *** Waiting for unfinished jobs....
    g++: fatal error: Killed signal terminated program cc1plus
    compilation terminated.
    src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/build.make:62: recipe for target 'src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/graph_matching_frank_wolfe.cpp.o' failed
    make[3]: *** [src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/graph_matching_frank_wolfe.cpp.o] Error 1
    CMakeFiles/Makefile2:2199: recipe for target 'src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/all' failed
    make[2]: *** [src/graph_matching/CMakeFiles/graph_matching_frank_wolfe.dir/all] Error 2
    CMakeFiles/Makefile2:2294: recipe for target 'src/graph_matching/CMakeFiles/graph_matching_py.dir/rule' failed
    make[1]: *** [src/graph_matching/CMakeFiles/graph_matching_py.dir/rule] Error 2
    Makefile:786: recipe for target 'graph_matching_py' failed
    make: *** [graph_matching_py] Error 2
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-aw_xn3gm-build/setup.py"", line 137, in <module>
        zip_safe=False,
      File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 129, in setup
        return distutils.core.setup(**attrs)
      File ""/usr/lib/python3.6/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/usr/lib/python3.6/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/lib/python3/dist-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/usr/lib/python3.6/distutils/command/install.py"", line 589, in run
        self.run_command('build')
      File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/usr/lib/python3.6/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/usr/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/usr/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/pip-aw_xn3gm-build/setup.py"", line 35, in run
        self.build_extension(ext)
      File ""/tmp/pip-aw_xn3gm-build/setup.py"", line 112, in build_extension
        subprocess.check_call(['cmake', '--build', '.', '--target', ext.name] + build_args, cwd=self.build_temp)
      File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call
        raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'graph_matching_py', '--config', 'Release', '--', '-j2']' returned non-zero exit status 2.
    
    ----------------------------------------
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-aw_xn3gm-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-vi1j_bdf-record/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/pip-aw_xn3gm-build/
```"
Hi there! I found that currently both of the two pip install commands are down. Do you have any plans to fix this?
"Thanks for sharing this repository, which is quite complete !

I am particularly interested in the multicut problem. If I am not wrong, there are 2 LP-based solvers:
- [Iterative Cycle Packing (ICP)](http://proceedings.mlr.press/v80/lange18a/lange18a.pdf). The corresponding files are `src/multicut/multicut_<CONSTRAINTS>_packing*`, e.g. [multicut_cycle_packing](https://github.com/LPMP/LPMP/blob/master/src/multicut/multicut_cycle_packing.cpp).
- [Message Passing (MP)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Swoboda_A_Message_Passing_CVPR_2017_paper.pdf). The corresponding files are `src/multicut/multicut_message_passing*`, e.g. [multicut_message_passing_text_input_parallel](https://github.com/LPMP/LPMP/blob/master/src/multicut/multicut_message_passing_text_input_parallel.cpp).

On top of them, I wonder if there is another solver for multicut, whose the corresponding files are `multicut_<CONSTRAINTS>_text_input.*`, e.g. [multicut_cycle_text_input](https://github.com/LPMP/LPMP/blob/master/src/multicut/multicut_cycle_text_input.cpp). Is this right? If so, could you point me to its reference article ?

Moreover, the third solver has many parameters. I wonder how to call this method for a good performance. I suppose that you have already explored this aspect in some way. I saw the following call in the `mulitcut/eval` directory: `./multicut_odd_bicycle_wheel_text_input -i INPUT_FILE.txt -o OUTPUT_FILE.txt --standardReparametrization anisotropic --roundingReparametrization uniform:0.5 --tightenReparametrization uniform:0.5 --tightenInterval 50 --tightenIteration 1 --tightenConstraintsPercentage 0.05  --primalComputationStart 1 --primalComputationInterval 10 -v 2 --lowerBoundComputationInterval 10 --tighten`. Can we rely on this ?

By the way, is there any multicut ILP solver in this repository (based on the cutting plane approach)?

Thanks in advance.
"
"I have pip installed the forked version by  mrolinek which is the only one that really builds. But I get the following error when running the train_eval.py from blackbox-deep-graph-matching. Within the python package lpmp_py/raw_solvers.py tries to import the **bindings.graph_matching_py**  but there is no graph_matching_py in the bindings package.

ModuleNotFoundError: No module named 'bindings.graph_matching_py'.


Is this really working? I have tried everything."
"Hi, when I run the installation instruction 

    python3 -m pip install git+https://github.com/lpmp/LPMP.git


I get an error during building lpmp-py wheel causing the installation to fail. I am running Mac 10.14 and Clang 11. The full error message is below. Thanks for your attention. 
```
~ simon$ python3 -m pip install git+https://github.com/lpmp/LPMP.git
Collecting git+https://github.com/lpmp/LPMP.git
  Cloning https://github.com/lpmp/LPMP.git to /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g
  Running command git clone -q https://github.com/lpmp/LPMP.git /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g
  Running command git submodule update --init --recursive -q
Building wheels for collected packages: lpmp-py
  Building wheel for lpmp-py (setup.py) ... error
  ERROR: Complete output from command /Users/Simon/miniconda3/bin/python3 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-wheel-nhuiet1t --python-tag cp37:
  ERROR: running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.macosx-10.7-x86_64-3.7
  creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py
  copying lpmp_py/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
  copying lpmp_py/raw_solvers.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
  creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/graph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/multigraph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/utils.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
  running build_ext
  xargs: illegal option -- d
  usage: xargs [-0opt] [-E eofstr] [-I replstr [-R replacements]] [-J replstr]
               [-L number] [-n number [-x]] [-P maxprocs] [-s size]
               [utility [argument ...]]
  Traceback (most recent call last):
    File ""<string>"", line 1, in <module>
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 108, in <module>
      zip_safe=False,
    File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 161, in setup
      return distutils.core.setup(**attrs)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/core.py"", line 148, in setup
      dist.run_commands()
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
      self.run_command(cmd)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py"", line 192, in run
      self.run_command('build')
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
      cmd_obj.run()
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 32, in run
      self.build_extension(ext)
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 87, in build_extension
      self._prepare_environment()
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 62, in _prepare_environment
      gcc, gpp = self._find_suitable_gcc_gpp()
    File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 50, in _find_suitable_gcc_gpp
      all_gccs = subprocess.check_output(cmd_for_all_gccs, shell=True).decode(""utf-8"").rstrip().split(""\n"")
    File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 395, in check_output
      **kwargs).stdout
    File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 487, in run
      output=stdout, stderr=stderr)
  subprocess.CalledProcessError: Command 'echo -n $PATH | xargs -d : -I {} find -H {} -maxdepth 1 -perm -o=x -type l,f -printf '%P
  ' | grep '^gcc-[0-9].\?.\?.\?'' returned non-zero exit status 1.
  ----------------------------------------
  ERROR: Failed building wheel for lpmp-py
  Running setup.py clean for lpmp-py
Failed to build lpmp-py
Installing collected packages: lpmp-py
  Running setup.py install for lpmp-py ... error
    ERROR: Complete output from command /Users/Simon/miniconda3/bin/python3 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-record-0gw1stpo/install-record.txt --single-version-externally-managed --compile:
    ERROR: running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.7-x86_64-3.7
    creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py
    copying lpmp_py/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
    copying lpmp_py/raw_solvers.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py
    creating build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/graph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/multigraph_matching.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/utils.py -> build/lib.macosx-10.7-x86_64-3.7/lpmp_py/torch_wrappers
    running build_ext
    xargs: illegal option -- d
    usage: xargs [-0opt] [-E eofstr] [-I replstr [-R replacements]] [-J replstr]
                 [-L number] [-n number [-x]] [-P maxprocs] [-s size]
                 [utility [argument ...]]
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 108, in <module>
        zip_safe=False,
      File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/setuptools/__init__.py"", line 161, in setup
        return distutils.core.setup(**attrs)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 966, in run_commands
        self.run_command(cmd)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/Users/Simon/miniconda3/lib/python3.7/site-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/command/install.py"", line 545, in run
        self.run_command('build')
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/Users/Simon/miniconda3/lib/python3.7/distutils/dist.py"", line 985, in run_command
        cmd_obj.run()
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 32, in run
        self.build_extension(ext)
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 87, in build_extension
        self._prepare_environment()
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 62, in _prepare_environment
        gcc, gpp = self._find_suitable_gcc_gpp()
      File ""/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py"", line 50, in _find_suitable_gcc_gpp
        all_gccs = subprocess.check_output(cmd_for_all_gccs, shell=True).decode(""utf-8"").rstrip().split(""\n"")
      File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 395, in check_output
        **kwargs).stdout
      File ""/Users/Simon/miniconda3/lib/python3.7/subprocess.py"", line 487, in run
        output=stdout, stderr=stderr)
    subprocess.CalledProcessError: Command 'echo -n $PATH | xargs -d : -I {} find -H {} -maxdepth 1 -perm -o=x -type l,f -printf '%P
    ' | grep '^gcc-[0-9].\?.\?.\?'' returned non-zero exit status 1.
    ----------------------------------------
ERROR: Command ""/Users/Simon/miniconda3/bin/python3 -u -c 'import setuptools, tokenize;__file__='""'""'/private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-record-0gw1stpo/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/c6/hhr8nmbn38j1xqp4s4cxxj2m0000gn/T/pip-req-build-re65t60g/
```"
"When compiling, I get the following error:

```
In file included from /net/hciserver03/storage/jschnell/software/gccbin/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/immintrin.h:41,
                 from /export/home/jschnell/masterarbeit/code/LPMP/external/libsimdpp/simdpp/setup_arch.h:281,
                 from /export/home/jschnell/masterarbeit/code/LPMP/external/libsimdpp/simdpp/simd.h:14,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/config.hxx:15,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/memory_allocator.hxx:9,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/vector.hxx:4,
                 from /export/home/jschnell/masterarbeit/code/LPMP/include/mrf/pairwise_simplex_factor.h:4,
                 from /export/home/jschnell/masterarbeit/code/LPMP/src/mrf/pairwise_simplex_factor.cpp:1:
/net/hciserver03/storage/jschnell/software/gccbin/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/avxintrin.h: In function ‘void simdpp::arch_avx2::detail::insn::i_load(simdpp::arch_avx2::float64x4&, const char*)’:
/net/hciserver03/storage/jschnell/software/gccbin/lib/gcc/x86_64-pc-linux-gnu/8.2.0/include/avxintrin.h:859:1: error: inlining failed in call to always_inline ‘__m256d _mm256_load_pd(const double*)’: target specific option mismatch
 _mm256_load_pd (double const *__P)
```

This can be fixed by adding
```
add_definitions(-mavx)
```
in the CMakeLists.txt

However, when trying to execute one of the resulting binaries, for example `multigraph_matching_tightening_mp`, I get an `Illegal instruction (core dumped)`.
I have no clue on how to debug this and am glad about any help I can get :))"
"I tried to install the package with 

```
python3 -m pip install git+https://github.com/lpmp/LPMP.git
```

I have raised the problem in https://github.com/martius-lab/blackbox-deep-graph-matching/issues/2 

The full error is as follows:

```
(Blackbox) gabby-suwichaya@gabby-suwichaya:/mnt/HDD4TB3/blackbox-deep-graph-matching$ python3 -m pip install --user git+https://github.com/lpmp/LPMP.git
Collecting git+https://github.com/lpmp/LPMP.git
  Cloning https://github.com/lpmp/LPMP.git to /tmp/pip-req-build-ldhlw0wy
Building wheels for collected packages: lpmp-py
  Building wheel for lpmp-py (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /home/gabby-suwichaya/anaconda3/envs/Blackbox/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-ldhlw0wy/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-ldhlw0wy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-_fyayuif
       cwd: /tmp/pip-req-build-ldhlw0wy/
  Complete output (59 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.6
  creating build/lib.linux-x86_64-3.6/lpmp_py
  copying lpmp_py/__init__.py -> build/lib.linux-x86_64-3.6/lpmp_py
  copying lpmp_py/raw_solvers.py -> build/lib.linux-x86_64-3.6/lpmp_py
  creating build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/utils.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/__init__.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/multigraph_matching.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
  copying lpmp_py/torch_wrappers/graph_matching.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
  running build_ext
  Traceback (most recent call last):
    File ""/usr/local/bin/cmake"", line 11, in <module>
      load_entry_point('cmake==3.18.4.post1', 'console_scripts', 'cmake')()
    File ""/usr/local/lib/python3.8/dist-packages/cmake-3.18.4.post1-py3.8-linux-x86_64.egg/cmake/__init__.py"", line 46, in cmake
      raise SystemExit(_program('cmake', sys.argv[1:]))
    File ""/usr/local/lib/python3.8/dist-packages/cmake-3.18.4.post1-py3.8-linux-x86_64.egg/cmake/__init__.py"", line 42, in _program
      return subprocess.call([os.path.join(CMAKE_BIN_DIR, name)] + args)
    File ""/usr/lib/python3.8/subprocess.py"", line 340, in call
      with Popen(*popenargs, **kwargs) as p:
    File ""/usr/lib/python3.8/subprocess.py"", line 854, in __init__
      self._execute_child(args, executable, preexec_fn, close_fds,
    File ""/usr/lib/python3.8/subprocess.py"", line 1702, in _execute_child
      raise child_exception_type(errno_num, err_msg, err_filename)
  PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.8/dist-packages/cmake-3.18.4.post1-py3.8-linux-x86_64.egg/cmake/data/bin/cmake'
  Traceback (most recent call last):
    File ""<string>"", line 1, in <module>
    File ""/tmp/pip-req-build-ldhlw0wy/setup.py"", line 123, in <module>
      zip_safe=False,
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/site-packages/setuptools/__init__.py"", line 153, in setup
      return distutils.core.setup(**attrs)
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/core.py"", line 148, in setup
      dist.run_commands()
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 955, in run_commands
      self.run_command(cmd)
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/site-packages/wheel/bdist_wheel.py"", line 299, in run
      self.run_command('build')
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/command/build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""/tmp/pip-req-build-ldhlw0wy/setup.py"", line 22, in run
      out = subprocess.check_output(['cmake', '--version'])
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/subprocess.py"", line 356, in check_output
      **kwargs).stdout
    File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/subprocess.py"", line 438, in run
      output=stdout, stderr=stderr)
  subprocess.CalledProcessError: Command '['cmake', '--version']' returned non-zero exit status 1.
  ----------------------------------------
  ERROR: Failed building wheel for lpmp-py
  Running setup.py clean for lpmp-py
Failed to build lpmp-py
Installing collected packages: lpmp-py
    Running setup.py install for lpmp-py ... error
    ERROR: Command errored out with exit status 1:
     command: /home/gabby-suwichaya/anaconda3/envs/Blackbox/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-ldhlw0wy/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-ldhlw0wy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-vn0b4ztd/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/gabby-suwichaya/.local/include/python3.6m/lpmp-py
         cwd: /tmp/pip-req-build-ldhlw0wy/
    Complete output (61 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/lpmp_py
    copying lpmp_py/__init__.py -> build/lib.linux-x86_64-3.6/lpmp_py
    copying lpmp_py/raw_solvers.py -> build/lib.linux-x86_64-3.6/lpmp_py
    creating build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/utils.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/__init__.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/multigraph_matching.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
    copying lpmp_py/torch_wrappers/graph_matching.py -> build/lib.linux-x86_64-3.6/lpmp_py/torch_wrappers
    running build_ext
    Traceback (most recent call last):
      File ""/usr/local/bin/cmake"", line 11, in <module>
        load_entry_point('cmake==3.18.4.post1', 'console_scripts', 'cmake')()
      File ""/usr/local/lib/python3.8/dist-packages/cmake-3.18.4.post1-py3.8-linux-x86_64.egg/cmake/__init__.py"", line 46, in cmake
        raise SystemExit(_program('cmake', sys.argv[1:]))
      File ""/usr/local/lib/python3.8/dist-packages/cmake-3.18.4.post1-py3.8-linux-x86_64.egg/cmake/__init__.py"", line 42, in _program
        return subprocess.call([os.path.join(CMAKE_BIN_DIR, name)] + args)
      File ""/usr/lib/python3.8/subprocess.py"", line 340, in call
        with Popen(*popenargs, **kwargs) as p:
      File ""/usr/lib/python3.8/subprocess.py"", line 854, in __init__
        self._execute_child(args, executable, preexec_fn, close_fds,
      File ""/usr/lib/python3.8/subprocess.py"", line 1702, in _execute_child
        raise child_exception_type(errno_num, err_msg, err_filename)
    PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.8/dist-packages/cmake-3.18.4.post1-py3.8-linux-x86_64.egg/cmake/data/bin/cmake'
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-req-build-ldhlw0wy/setup.py"", line 123, in <module>
        zip_safe=False,
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/site-packages/setuptools/__init__.py"", line 153, in setup
        return distutils.core.setup(**attrs)
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/core.py"", line 148, in setup
        dist.run_commands()
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/site-packages/setuptools/command/install.py"", line 61, in run
        return orig.install.run(self)
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/command/install.py"", line 545, in run
        self.run_command('build')
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/command/build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/distutils/dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""/tmp/pip-req-build-ldhlw0wy/setup.py"", line 22, in run
        out = subprocess.check_output(['cmake', '--version'])
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/subprocess.py"", line 356, in check_output
        **kwargs).stdout
      File ""/home/gabby-suwichaya/anaconda3/envs/Blackbox/lib/python3.6/subprocess.py"", line 438, in run
        output=stdout, stderr=stderr)
    subprocess.CalledProcessError: Command '['cmake', '--version']' returned non-zero exit status 1.
    ----------------------------------------
ERROR: Command errored out with exit status 1: /home/gabby-suwichaya/anaconda3/envs/Blackbox/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-req-build-ldhlw0wy/setup.py'""'""'; __file__='""'""'/tmp/pip-req-build-ldhlw0wy/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-vn0b4ztd/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /home/gabby-suwichaya/.local/include/python3.6m/lpmp-py Check the logs for full command output.
``` 
"
""
"Hi  ,  ;-)
when i run ""python3 -m pip install git+https://github.com/lpmp/LPMP.git"", i got a error as follows.
Here is my envs:

`
Ubuntu 5.4.0-6ubuntu1~16.04.9
gcc versioon: 9.3.0
cmake version: 3.19.0-rc2
`

How could i fix this error?
Many thanks ! 

![image](https://user-images.githubusercontent.com/23171037/97801525-55702c00-1c78-11eb-8e45-7573a23b0307.png)

![image](https://user-images.githubusercontent.com/23171037/97801512-45584c80-1c78-11eb-8bb0-bb94d999808d.png)
"
"Is it possible to use LPMP to solve a linear assignment problem, instead of the full graph matching?"
"Hi,

When I use `make` after `cmake .`
I got the following errors:
![image](https://user-images.githubusercontent.com/33629224/79048708-8b54b100-7c51-11ea-9c48-fbf5220cf44c.png)

Could anybody tell me what to do?"
The identifier '3_instance_list' in https://github.com/LPMP/LPMP/blob/9259497f35b8059b5c29fef9b9acec0d20fd7d36/src/graph_matching/eval/worms_mm_evaluation_data.py.in#L5 is invalid - as are all the others beginning with numbers.
"When building, I run into the following error:

```
Scanning dependencies of target multicut_odd_wheel_andres_input
[ 64%] Building CXX object src/multicut/CMakeFiles/multicut_odd_wheel_andres_input.dir/multicut_odd_wheel_andres_input.cpp.o
In file included from LPMP/src/multicut/multicut_odd_wheel_andres_input.cpp:2:
LPMP/include/multicut/multicut.h:14:10: fatal error: cut_base/cut_base_lifted_constructor.hxx: No such file or directory
 #include ""cut_base/cut_base_lifted_constructor.hxx""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```"
"In the latest commit in MCF-SSP, functions are added that are used in this code.
However, the code still depends on the second-to-last commit.

An example of this is [mcf_->reset_costs()](https://github.com/LPMP/LPMP/blob/c7586822aa26737bc9ef056a860c8a44a974c161/include/graph_matching/graph_matching_constructor.hxx#L419)"
"Hi, i have a confusion when running this paper’s experiments.
when i set the transformation to flip or rotation, the gradient of the adversarial examples is none. 
What should i do? Thank you so much!"
"In table1 of this paper, FGSM has success rate of 64.6% on inc-v3 with maximum perturbation 15. However, in table2 of another paper MITIGATING ADVERSARIAL EFFECTS THROUGH RANDOMIZATION, success rates of FGSM on inc-v3 with different perturbations are 66.8%/68.9%/67.0%(perturbation 2/5/10 respectively). If I read you right, both these two work use 5000 random images of imagenet valset and use inc-v3 as source and target model ,so why FGSM with the largest perturbation has the lowest success rate?"
"Hello, how to use this code? I've just been in the research field. I don't know how to use your code very well. What should I pay attention to when using this code? For example, what format should I initialize the parameters in attack.py, or can I have complete source code for reference? I'm in a hurry to study and use it. I hope you can help me solve this problem and reply to me as soon as possible. Sorry to disturb you, thank you."
"Hi, I'm very intersted in your paper, but I have some problems about the img shape. Could you help me?
In your code, the method `input_diversity()` will return `padded` or `input_tensor`.
For `padded`, the code is
```python
padded = tf.pad(rescaled, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], constant_values=0.)
padded.set_shape((input_tensor.shape[0], FLAGS.image_resize, FLAGS.image_resize, 3))
```
and from the code, we can see that the shape of `padded` is ""batch_sizex330x330x3""

But for `input_tensor`
```python
  batch_shape = [FLAGS.batch_size, FLAGS.image_height, FLAGS.image_width, 3]

  with tf.Graph().as_default():
    # Prepare graph
    x_input = tf.placeholder(tf.float32, shape=batch_shape)
```
and from the code, we can see that the shape of `input_tensor` is ""batch_sizex229x229x3""

Why are they different? Is it ok?"
"Hi, thx for your sharing. I notice that in your CVPR-19 paper ""Improving transferbility of Adversarial examples with input diversity"", your work contains attacking an ensemble of networks, but in your released code, I haven't found the corresponding code, can you release it?"
"Hi, thanks for your great work. I am so interested in your work and i have a confusion about the algorithm.  The output of the transformation is resized to [330,330,3], while the original image is [299,299,3]. 
Are the input images(clean image) of the experiments all resized to [330,330,3]?
If resized, what is the resize method applied to clean image?
Thank you so much for answering!!!"
"The return value of input_diversity function may be 'input_tensor' or 'padded'，but the shape of ‘input_tensor’ is [batch_size,299,299,3] and the shape of 'padded' is [batch_size, 331, 331, 3]. The code sometimes gets wrong, why not reshape 'padded' to [batch_size, 299, 299, 3]?"
"In 
https://github.com/cihangxie/DI-2-FGSM/blob/dfa64f2842561518f99515ea9572b42f9bee0c62/attack.py#L179

the variable `grad` is initialized to zero. Presumably, `grad` stores the accumulated gradient for momentum updates. However, it seems that `grad` is not updated anywhere else in the code. Am I missing something? 

Thanks!"
"Can you give me the complete code, the current code can't be verified.Thank you!"
"Hello, I want to ask the function of optical flow data.
In the literature, after input the pointcloud data into flownet3d, it can output the sceneflow. Therefore if we only need the pointcloud data to use the network, I don't understand why need to input optical flow data and gt data to get the sceneflow in training and evaluation? I think maybe it turns optical flow and gt into the correct sceneflow and makes the sceneflow as the label to train and test. Am I correct? If it is incorrect, what is the funtion of input optical flow and gt?

Thank you."
"Hi,
Thank your for sharing your codes!
I'm finetuning the model with KITTI data with ground points. I use the data you provide in [meteornet](https://github.com/xingyul/meteornet). But I cannnot get the result presented in the paper. Could you please share the details about finetuing? For example, are there any layers frozen?

Best regards,"
"Hi,

I'd like to use FlowNet3D for motion segmentation. It was described in Section F of [supplemental](https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Liu_FlowNet3D_Learning_Scene_CVPR_2019_supplemental.pdf), however, I couldn't find the implementation of this in the repo. Could you point out where the relevant code section is?

Thanks in advance!"
"@xingyul 

Hello, thanks for your awesome work!

However, i find that the implement of upconv. part is different between repo. and paper.

In the paper, 
1. use different radius to group neighbor points
2. layer's unit setting
3. four upconv. layers

In the repo., 
1. use knn
2. same as above
3. three upconv.layers and one 3d interpolation layer

Look forward for your response, thanks!"
"Hi,

In your paper, you mention that a lambda scaling factor was used to scale the scene flow output when concatenating it with the original xyz values. What was the value used for lambda? How was it determined?"
"Hi,

I have a query regarding the finetuning on KITTI. Was it done with 2048 points as in your FlyingThings3D training script or with 16384 points as with your KITTI evaluation script? I tried finetuning with 2048 points on KITTI and then evaluating with your KITTI evaluation script using 8192 points and the results become worse compared to what they were before finetuning. I'd be grateful if you could provide any inputs regarding the same. I'd be also grateful if you could share the script for converting point cloud into chunks for KITTI finetuning/evaluation.

Yours sincerely,"
"Hi,

For KITTI evaluation, you've stated: ""Note that the model used for evaluation is in `model_concat_upsa_eval_kitti.py` instead of the model used for training. The average 3D EPE result is approximately 0.175m, better than what was reported in the paper"". However, this is true only for the dataset with ground points. However, your preprocessed dataset here is without ground points, for which the EPE mentioned in the paper is 0.122, much lower (better) than the EPE you're getting with the KITTI evaluation script. Please help me resolve this discrepancy. Is it that the dataset is with ground points or is there an issue with the KITTI evaluation script?

Best Regards"
"Hi,

It seems the using the preprocessed data and KITTI evaluation scripts you included a few days ago, I am not getting the same values as in the paper. Am I doing something wrong? I've tried with models both trained from scratch as well as the pre-trained model given by you and I've used both cases when RGB values are used and when (R,G,B) = (0,0,0). Please let me know what might be the cause of this discrepancy. 

Best Regards"
"As title.

Many thanks."
Can you share the origin data with ground LiDAR info?
"evaluation for flying_things_dataset by using the pre-trained weight you release.
======================

```
batch EPE 3D: 0.195748  ACC 3D: 0.225382        ACC 3D 2: 0.564744
(8, 4096, 6)
batch loss: 0.009314
batch EPE 3D: 0.113960  ACC 3D: 0.287980        ACC 3D 2: 0.619208
(8, 4096, 6)
batch loss: 0.104013
batch EPE 3D: 0.249060  ACC 3D: 0.311374        ACC 3D 2: 0.582282
(8, 4096, 6)
batch loss: 0.028099
batch EPE 3D: 0.173225  ACC 3D: 0.286175        ACC 3D 2: 0.540984
040/251
(8, 4096, 6)
batch loss: 0.063560
batch EPE 3D: 0.211014  ACC 3D: 0.177892        ACC 3D 2: 0.631412
(8, 4096, 6)
batch loss: 0.014414
batch EPE 3D: 0.144333  ACC 3D: 0.245626        ACC 3D 2: 0.512642
(8, 4096, 6)
batch loss: 0.009335
batch EPE 3D: 0.111291  ACC 3D: 0.261149        ACC 3D 2: 0.664810
(8, 4096, 6)
batch loss: 0.019046
batch EPE 3D: 0.153138  ACC 3D: 0.161448        ACC 3D 2: 0.492261
(8, 4096, 6)
batch loss: 0.025903
batch EPE 3D: 0.153123  ACC 3D: 0.220680        ACC 3D 2: 0.569799
(8, 4096, 6)
batch loss: 0.028518
batch EPE 3D: 0.153472  ACC 3D: 0.293120        ACC 3D 2: 0.579715
```
"
"Thank you for your great work and code release! 
I found that the disparity of  KITTI scene flow dataset is based on frame one, so that the disparity of pixels for the second frame can not be gotten directly. How dou you get them, from the raw lidar data or some method else? 

If you use the raw lidar data, how could you get so dense depth values?
If you did not use the raw lidar data, why do you use 150 frames rather than 200?"
"Hi, @xingyul , thanks for your sharing codes and it is great. But i got a little trouble while testing it, could you or anyone help me?
I downloaded the processed data of flyingtings3d which is around 11GB, i extracted it and put the folder under data_processing, and then i run 'sh command_train.sh"" as you suggested.There is the error :


2019-09-30 13:38:13.146499: W tensorflow/core/framework/allocator.cc:108] Allocation of 33554432 exceeds 10% of system memory.
 -- 010 / 032 --
mean loss: 1.026778
Traceback (most recent call last):
  File ""train.py"", line 283, in <module>
    train()
  File ""train.py"", line 155, in train
    train_one_epoch(sess, ops, train_writer)
  File ""train.py"", line 201, in train_one_epoch
    batch_data, batch_label, batch_mask = get_batch(TRAIN_DATASET, train_idxs, start_idx, end_idx)
  File ""train.py"", line 173, in get_batch
    pc1, pc2, color1, color2, flow, mask1 = dataset[idxs[i+start_idx]]
  File ""/home/py001/cyz/flownet3d/flying_things_dataset.py"", line 36, in __getitem__
    data = np.load(fp)
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/site-packages/numpy/lib/npyio.py"", line 426, in load
    pickle_kwargs=pickle_kwargs)
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/site-packages/numpy/lib/npyio.py"", line 182, in __init__
    _zip = zipfile_factory(fid)
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/site-packages/numpy/lib/npyio.py"", line 112, in zipfile_factory
    return zipfile.ZipFile(file, *args, **kwargs)
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/zipfile.py"", line 1026, in __init__
    self._RealGetContents()
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/zipfile.py"", line 1093, in _RealGetContents
    raise BadZipFile(""File is not a zip file"")
zipfile.BadZipFile: File is not a zip file
Exception ignored in: <bound method NpzFile.__del__ of <numpy.lib.npyio.NpzFile object at 0x7f3f12e80630>>
Traceback (most recent call last):
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/site-packages/numpy/lib/npyio.py"", line 219, in __del__
    self.close()
  File ""/home/py001/anaconda3/envs/cyzpy35/lib/python3.5/site-packages/numpy/lib/npyio.py"", line 210, in close
    if self.zip is not None:
AttributeError: 'NpzFile' object has no attribute 'zip


Is there anything wrong with the data i downloaded ?"
""
"Hi Xingyu,

Thank you for releasing code here. 

I noticed that the loss function used in the code is different from the one in the Paper. In the paper, you used a huber loss together with a cycle-consistency regularization.  

Are there particular reasons that you removed it from your code?

Thank you very much.
Best,
Dylan"
" In the Supplementary of your paper, E part describes the details on the Scan Registration Application saying that "" we iteratively regress twice for the scene flow (i.e. predict a flow from point cloud 1 to point cloud 2, and then predict a second residual flow from point cloud 1 + first flow to point cloud 2) "" . I want to know what is the ""residual flow"" . Did you just estimate scene flow twice? And I want to know the detail of how to translate the scene flow into the rigid motion estimation, how you "" fit a rigid transformation from the point cloud 1 to the point cloud 2 + scene flow, as they have one-to-one correspondences "" ?  Is the code of this part available ?"
"@xingyul ,
In which section of the code, motion segmentation can be made to visualize larger motion and smaller motion"
"Hi, Xingyu. I have run your code and the performance is excellent. Thank you for sharing. I am interested in the 3d scan registration application in your paper, but I have trouble generating the dataset (partial scanned modelnet40 ). Can you provide some detail about generating the dataset?

Best regards,
Xuting"
"Hi, thanks for the great work!

I have a short question for the evaluation.
Do you evaluate the scene flow for all 3D points? or only non-occluded points?

It seems like it only evaluates the scene flow for non-occluded points by using the **mask**:

https://github.com/xingyul/flownet3d/blob/ca2a2cb8e1e747949111fc4aa9d3bc010cc0e09b/evaluate.py#L105-L121

It will be appreciated if you can clarify this :) "
"root@f9adda5586c3:/mnt/txf/codes/FlowNet3D# python train.py
pid: 22198
WARNING:tensorflow:From /mnt/txf/codes/FlowNet3D/model_concat_upsa.py:16: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From train.py:83: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From train.py:102: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

--- Get model and loss
WARNING:tensorflow:From /mnt/txf/codes/FlowNet3D/model_concat_upsa.py:39: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

Segmentation fault (core dumped)

My running environment configuration as follow:
GPU: GTX 1080
CUDA: CUDA10.0
Tensorflow: Tensorflow-gpu-1.14.0

I get a segmentation fault when running the train.py gets to the function https://github.com/xingyul/flownet3d/blob/master/tf_ops/sampling/tf_sampling.py#L56. I am not sure what could be causing the issue as I got to compile with no problems the code previously. "
"My experiment setup is as follows：

1. 50 epoch
2. 10 batchsize（cause my gpu just has 10GB memory）
3. the train mean loss decrease to around 0.02, and the evaluation loss decrease to around 0.03
4. the final exp. result
                  eval mean EPE 3D: 0.220860
                  eval mean ACC 3D: 0.016343
                  eval mean ACC 3D 2: 0.151503

So, Is my setting up has problems?

Looking for forward your help and thanks very much~"
"Hi,

Following the Kitti experiment I get a segmentation fault when the evaluate.py gets to the function https://github.com/xingyul/flownet3d/blob/master/tf_ops/sampling/tf_sampling.py#L29 . I am not sure what could be causing the issue as I got to compile with no problems the code previously. I am running on ubuntu 18.04, Cuda 9.1 and Tensorflow 1.14.

Thanks.
"
"Is the scene flow output saved as ""test_results.pkl""? If so, how do I convert it from a binary format, to a readable format for visualization?"
"Hi,

After installing the requirements described in the README (tensorflow-gpu 1.9.0, g++ 5.4.0, CUDA 9.0 and Python 3.5 on Ubuntu 16.04), I was able to compile successfully. However, when I execute `sh command_train.sh`, I encounter the following error:

    Traceback (most recent call last):
      File ""train.py"", line 49, in <module>
        MODEL = importlib.import_module(FLAGS.model) # import network module
      File ""/home/workstation/.pyenv/versions/anaconda3- 
    5.0.0/envs/flownet3d/lib/python3.5/importlib/__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""<frozen importlib._bootstrap>"", line 985, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 968, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 957, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
      File ""<frozen importlib._bootstrap_external>"", line 697, in exec_module
      File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
      File ""/home/workstation/Documents/flownet3d/model_concat_upsa.py"", line 13, in <module>
        from pointnet_util import *
      File ""/home/workstation/Documents/flownet3d/utils/pointnet_util.py"", line 16, in <module>
        from tf_sampling import farthest_point_sample, gather_point
      File ""/home/workstation/Documents/flownet3d/tf_ops/sampling/tf_sampling.py"", line 12, in 
    <module>
        sampling_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_sampling_so.so'))
      File ""/home/workstation/.pyenv/versions/anaconda3-5.0.0/envs/flownet3d/lib/python3.5/site-        
    packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
        lib_handle = py_tf.TF_LoadLibrary(library_filename)
    tensorflow.python.framework.errors_impl.NotFoundError:     
    /home/workstation/Documents/flownet3d/tf_ops/sampling/tf_sampling_so.so: undefined symbol:     
    _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv

I am not sure what went wrong. Any explanation as to why this is happening and how to solve the issue would be much appreciated."
"Hi,

I installed TensorFlow and tried executing `sh command_train.sh` but I ended up getting the following error:

    Traceback (most recent call last):
      File ""train.py"", line 49, in <module>
        MODEL = importlib.import_module(FLAGS.model) # import network module
      File ""/home/workstation/.pyenv/versions/anaconda3- 
    5.0.0/envs/rangenet++/lib/python3.6/importlib/__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
      File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
      File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
      File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
      File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
      File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
      File ""/home/workstation/Documents/flownet3d/model_concat_upsa.py"", line 13, in <module>
        from pointnet_util import *
      File ""/home/workstation/Documents/flownet3d/utils/pointnet_util.py"", line 16, in <module>
        from tf_sampling import farthest_point_sample, gather_point
      File ""/home/workstation/Documents/flownet3d/tf_ops/sampling/tf_sampling.py"", line 12, in 
    <module>
        sampling_module=tf.load_op_library(os.path.join(BASE_DIR, 'tf_sampling_so.so'))
      File ""/home/workstation/.pyenv/versions/anaconda3-5.0.0/envs/rangenet++/lib/python3.6/site- 
    packages/tensorflow/python/framework/load_library.py"", line 64, in load_op_library
        None, None, error_msg, error_code)
    tensorflow.python.framework.errors_impl.NotFoundError: 
    /home/workstation/Documents/flownet3d/tf_ops/sampling/tf_sampling_so.so: cannot open 
    shared object file: No such file or directory

How do I fix this error?"
"Hi,
Line 119 in train.py
`train_op = optimizer.minimize(loss, global_step=batch)`
Should this loss be loss[0]?  Since the loss is a tuple object from the get_loss() function.

Btw, was the pretrained model trained by a huber loss or L2 loss?"
"Hi, thanks a lot for the previous reply. Just another doubt, in the function sample_and_group, if the flag use_xyz is set to true, we have : new_points = tf.concat([grouped_xyz, grouped_points], axis=-1) # (batch_size, npoint, nample, 3+channel). To use XYZ values only, instead of setting (R,G,B) = (0,0,0), I was doing new_points = grouped_xyz which gives me significantly different results. I can't figure out why this creates that much of a difference as I was grouping X,Y,Z coordinates without the concatenated (R,G,B) = (0,0,0) values. Can you please help me with this?"
"Hi, I download the pretrained model and the processed data.
Then I evaluate the model using the given scripts.  
I keep getting very huge loss like this:

> loss value 
>  1019623.6
> pred_value 
>  [[[-1183.0692   -451.19238  -942.2431 ]
>   [-1617.0183   -613.1068  -1288.0482 ]
>   [-1646.4847   -628.52234 -1312.6896 ]
>   ...
>   [-1316.52     -497.82248 -1044.1621 ]
>   [ -975.26605  -370.09656  -777.7711 ]
>   [-1316.516    -497.82562 -1044.1562 ]]]
> 
> loss value 
>  5635.0854
> pred_value 
>  [[[-181.02733     78.97927     54.753517 ]
>   [ -60.915703    37.60071      5.6472397]
>   [-143.16997     68.90349     30.370804 ]
>   ...
>   [-141.26614     68.30685     29.489553 ]
>   [-143.37962     68.98323     30.424229 ]
>   [ -94.74317     50.511147    22.388628 ]]]

Do you have any idea about this? Thx."
"Hello, 

is there a way to download the trained model and use it without a GPU. 
If yes, how can I do that?
Thanks in advance. "
"Dear Sir,

In the paper, it is mentioned that ""Our work
focuses on learning scene flow directly from point clouds, without any dependence on RGB images or assumptions on
rigidity and camera motions"". However, in the code for training using flying things 3D, you are clearly using RGB color information of point clouds to construct feature maps. Isn't that the same as using RGB images? Please correct me if I'm wrong.

Best Regards,"
"sh command_evaluate.sh

report OOM issue.

"
"Hi,
Thanks for your work.

I just want to do inference for one frame to get scene flow. How could I do that?"
"Hello, liu，can you give me your email? I have some questions to ask you."
"I found that your evaluation code did not include the part about ""On large KITTI scenes,
we split the scene into multiple chunks."" in your paper.  And the points are always the 2048 points first. 

If you use the method of your paper for evaluation on KITTI, how can you make sure that you get all points for evaluation in frame one?
"
"Hi,
I downloaded your preprocessed Flyingthings3d dataset and when I run the evaluate command I get these unreasonable numbers for amost every batch:
```
loss_val nan
pred_val [[[ 7.80575696e+36  3.97343718e+36  2.06021729e+36]
  [ 5.75067476e+36  1.49935495e+36  1.30909517e+36]
  [ 1.14683589e+37  7.62623228e+36  2.77971755e+36]
  ...
  [ 1.69982565e+37 -2.35347431e+36 -7.84758246e+36]
  [ 1.00178770e+37  7.96418159e+36  3.90681105e+36]
  [ 1.12353090e+37  1.07022519e+36  7.07184686e+34]]

 [[ 4.45941417e+35 -2.48141516e+35  9.25996419e+33]
  [ 3.36659517e+35 -4.07464617e+35 -1.70080173e+34]
  [-6.47037686e+34 -1.89484747e+35 -7.36334955e+34]
  ...
  [ 3.55201482e+35 -4.03794649e+35 -1.92975676e+34]
  [ 4.68557136e+35  5.35420753e+34  1.08498641e+35]
  [ 2.90908917e+35 -8.76533249e+34  3.15325586e+34]]

 [[-8.20998982e+35 -7.27648726e+34  2.14235447e+35]
  [-1.01996078e+36 -2.44069148e+35 -5.02296449e+35]
  [-2.28599505e+36  5.09481255e+35 -3.73197565e+35]
  ...
  [-1.46082059e+36 -2.59530524e+35 -8.53456066e+35]
  [-1.59142710e+36  2.68669968e+35 -2.88856551e+35]
  [-1.02024886e+36 -2.46612155e+35 -5.03460509e+35]]
```

Is there anything I'm doing wrong?"
"In the function __getitem__ in flying_things_dataset.py, the point clouds, stored in the cache,  are not randomly sampled with 2048 points and are returned with 8192 points. In this case, in the function get_batch in train.py, only first 2048 points of these point clouds will be used.

So I guess, there might exist an indentation problem between line47 and line65 in flying_things_dataset.py."
"Hi xingyul,

Thanks for the code and the pre-trained model. 
I run the `evaluation.py` over the pre-trained model you provided but I got performance output below:
```
eval mean loss: 0.404094
eval mean EPE 3D: 0.790930
eval mean ACC 3D: 0.000854
eval mean ACC 3D 2: 0.006859
```

maybe it's an untrained model? or did I do it in the wrong way?

Best,
Zirui"
"Hi xingyul,

Thanks for your code and all preprocessing dataset. Can I ask is your KITTI result in your paper is evaluated with XYZ feature only? or is it evaluated use 6 channels (XYZ-RGB) but with rgb filled with some constants? or is it evaluated with full XYZ-RGB features?

Best,
Zirui"
"Thanks for your open code.
When I use your pre-trained model and processed kitti dataset for a evaluation, with setting rgb zeros, valid mask ones, the evaluation performance is much lower than which reported in your paper.

eval mean loss: 0.024921
eval mean EPE 3D: 0.170297    (0.122)
eval mean ACC 3D: 0.101357   (0.0561)
eval mean ACC 3D 2: 0.372272
"
"Hi,

I found mlp and mlp2 in the modules like pointnet_sa_module, set_upconv_module. Sometime mlp is [] while in other cases mlp2 is []. Is there any rule or do you have any suggestions about when to use them or not?"
"Hi Xingyul,

Thank you for providing your code here. I was trying to replicate the results from your paper and I am facing an issue doing that. When I run the code with the processed data you have provided I get my prediction scores as 'nan' (after 0.5 epoch only). As a result the loss value is 'nan' too. Please let me know what am I missing here? 

Looking forward to your reply."
Can you please share the KITTI evaluation script?
"Could you help me solve the problem?

My e-mail:cc755253@sina.com"
"Hi

For the flow embedding layer, in the paper, the method of ball point query and radius have been mentioned. Then, why does this layer in the code has the parameter knn as True? 

Moreover, when I try to run the code with ball point query and radius = 5, it throws the error of CUDA_ERROR_ILLEGAL_ADDRESS. Can you advise on resolving this issue?

Thanks"
"Hi Xingyu,

I noticed that the preprocessed KITTI scene flow dataset that you had provided only contains the xyz coordinates for the pc1, pc2 and flow-gt. However, to run the evaluate.py, the model also needs the color features to run. Can you please also provide these features as part of the dataset?

Thanks"
"I use 
python3.5 cuda9.2 tensorflow1.10.0  GCC  4.8.5

tensorflow.python.framework.errors_impl.NotFoundError: /disk2/wjn/flownet3d/tf_ops/sampling/tf_sampling_so.so: undefined symbol:_ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv


I  try to  remove -D_GLIBCXX_USE_CXX11_ABI=0,  But it doesn't work. 


"
"Thanks for your work!

With your processed data(11G) and the pre-trained model, I ran the code in two PCs.
Environments are the same: ubuntu 16.04, python3.5, TF1.9.0, Cuda 9.0

PC1: 1060(6g), i7 8700, 16g

PC2: 1080ti(11g), Xeon-E5, 32g


**Results of PC1**
---- EVALUATION ----
000/126
020/126
040/126
060/126
080/126
100/126
120/126
eval mean loss: 0.037443
eval mean EPE 3D: 0.188115
eval mean ACC 3D: 0.175726
eval mean ACC 3D 2: 0.487984

**Results of PC2**
---- EVALUATION ----
000/126
020/126
040/126
060/126
080/126
100/126
120/126
eval mean loss: 0.024307
eval mean EPE 3D: 0.147914
eval mean ACC 3D: 0.286565
eval mean ACC 3D 2: 0.616701

We can find that the results of PC2 are much better than those of PC1.
Everything is the same except for the computer hardware. Also, I have never trained yet, just use the provided pre-trained model.

What caused the difference in results? The graphics card? The memory?
"
"@xingyul 

I got your evaluation code , but how to visualize the flow in the paper you have shown. Kindly give some hints"
"Hi everyone,

I am very interested in this network and I have modified the scripts `command_evaluate.sh`, `evaluate.py` and `model_concat_upsa.py` in order to be able to do inference on point clouds of objects from `YCB` dataset.

Specifically, I changed the code in a way such that the point clouds of the two frames can have a different number of points as indicated in the paper. I tried again this code with the evaluation set from `FlyingThings3D`, that you provided, to verify that performance are unchanged.

However, when I try to feed the network with point clouds (xyz + colors in [0, 1]) from `YCB` I am getting very weird results.

As an example consider the following two frames involving the `Cracker Box` object

![Screenshot from 2019-06-16 12-21-14](https://user-images.githubusercontent.com/6014499/59562810-71e2d880-9031-11e9-816f-774a540bc469.png)

Basically, the point clouds represent one of the faces of the box after it undergoes a small rotation. The rotation is such that the maximum change between points of the two frames is in the order of `~ 4cm` as shown in the figure.

When doing inference, the translation vectors from the frame on the left to the frame on the right are very strange and not compatible with that motion, something like
```
[[[-0.25203004 -0.04336617 -0.24234553]
  [-0.25667757 -0.05793389 -0.23309565]
  [-0.25017104 -0.03977326 -0.2452542 ]
  ...
  [-0.2619986  -0.07882687 -0.21964237]
  [-0.26357266 -0.0843826  -0.21680456]
  [-0.26159576 -0.07794474 -0.21985136]]]
```

Do you think the point clouds I am feeding to the network are too different from the data from the `FlyingThings3D` dataset such that the network is not able to generalize with a model trained on that dataset?

Also, I saw that, within the model `model_concat_upsa.py`, several `radiuses` are defined, i.e.

```
RADIUS1 = 0.5
RADIUS2 = 1.0
RADIUS3 = 2.0
RADIUS4 = 4.0
```

plus some other hardcoded radius like `radius = 5.0`,  `radius = 2.4`, `radius = 1.2` and `radius = 0.6` in the definitions of the layers.

In your experience, are these values critical for the network? In case I need to the re-train the network with point clouds, optical flow and colors from scenes involving small objects, e.g. from `YCB` dataset, should I care in selecting them carefully or they are general enough?

Please find attached the point clouds (xyz + colors) of the two frames (files `frame0.txt` and `frame1.txt`) and the results of the inference (file `predicted_val.txt`) obtained using the pre-trained model available in this repository.

Thank you for your availability.

[frame0.txt](https://github.com/xingyul/flownet3d/files/3294039/frame0.txt)
[frame1.txt](https://github.com/xingyul/flownet3d/files/3294040/frame1.txt)
[predicted_val.txt](https://github.com/xingyul/flownet3d/files/3294041/predicted_val.txt)




"
"Hi,
Is there is any plan to relesae code relevaant to KITTI dataset?"
"Hi,

I am interested in this work and I would like to try it. It seems that on the repository an already trained model is not available. Are you planning to add it?

Thank you."
"Hi,
Excellent work! I have trained the model and obtained close results to the paper on FlyingThings3D dataset. I have some problems. 1). How to visualize the scene flow like your paper? 2).  When will the training and test on the KITTI dataset be released? And, can you provide the pre-trained model? Thank you very much!
Best,
Zhai"
"Hi, thank you for sharing your code. I generate my own dataset with points1 xyz and points2 xyz and gt flow, but without color1 and color2.  I tried to use my own dataset to test trained model, but there are always some format errors. Do you have the code which is compatible with my dataset?

Thank you !

Best regards,
Xuting"
"Hi, Xingyu, thank you for providing the code. I run your code and got a bug, I guess it might because of the configuration. 
My configuration is:
CUDA:9.0
Tensorflow-gpu1.9.0
gcc:4.8.5
Ubuntu:16.04
Is there any problem in my config ? and did you face similar problem?

Yours,
Xuting"
Q1：If i want to evaluate on awa2，should i still train on imagenet？
"Thank you very much for your contribution!
I have tested your codes with default settings(with finetuning), and the best preformance on imageNet(2-hops) is 25.49%/25.82%(SGCN/DGP).
It seems there is a little degradation with your paper(26.2%/26.6% for SGCN/DGP).Do I miss something or you have some other tricks?
I find that one tenth of the pictures were randomly picked in 'evaluate_imagenet.py'. Maybe that's the reason?
By the way, I'll be very appriciate it if you can provide the 'best.pred'.
Thank you!
 "
How can I find the file fall2011.tar
"Hi! Thanks for your code! 
ImageNet dataset is unable to download all 22k images, could you please share your dataset to us?
Thanks!"
"How much GPU memory do you use when training the dense gcn?

I am using the GTX2080TI  whose memory is 11G. But it always raises an error: RuntimeError: CUDA out of memory."
"How long does finetuning (20 epochs) take? I am at epoch 11 and it took about a day, so I expect the total tuning time will be roughly two days. Does that sound right? "
"in your paper, your model is trained for 3000 epochs, how much time do you spend in it"
In 6.4 you made a comparison between 1-hidden-layer GCN with attention mechanism and 1-hidden-layer GCN. Why the results of GAT-1 are so close to GCN-1 ? The attention mechanism really works?
"Hi, thanks for sharing codes. I tried to get ""2-hops"", ""3-hops"" and ""all-hops"" dataset (I followed this repo https://github.com/JudyYe/zero-shot-gcn/blob/master/DATASET.md), but most of URLs in ImageNet are not valid right now. I only can download ImageNet 2012 as whole (120 GB). Any tips for that? Thanks"
"root@aca4066e21cd:/tmp/pycharm_project_546/materials# python make_induced_graph.py
Traceback (most recent call last):
  File ""make_induced_graph.py"", line 4, in <module>
    from nltk.corpus import wordnet as wn
ImportError: No module named nltk.corpus
"
"Thanks for your contributions on this work, but I have some questions about SGCN.

In the paper, it mentions that ""As DGP, our SGCN model makes use of the proposed two-stage finetuning approach"", but did it use the descendant and ancestor patterns and weighting schema, like DGP as well? 

From the code, it seems that SGCN is very similar to GCNZ. Their differences mainly are the number of hidden layers, training epochs and implementation details, is it corret?

Thanks very much!"
"Hi,

Thanks for the release of your code, which gives me a lot of references in zero-shot learning literature.

I found that the dataset split in the awa2-split.json file in this repo is different from the split shown on the awa2 dataset website: https://cvml.ist.ac.at/AwA2/.

Can I ask if you use different splits and if so why you use different splits?
"
"Hi,

I noticed that there is a ""imagenet-xml-wnids.json"" file used in `make_induced_graph.py`.
Would you mind to let me where does it come from and what is it used for?

Best Regards,"
Thanks for your awesome work! Can you provide the download link for ImageNet and AWA2 datasets?
"I am a newbie..............when I train graph networks,I doubt about these error and I don not know how to resolve them:
Traceback (most recent call last):
  File ""train_gcn_basic.py"", line 61, in <module>
    gcn = GCN(n, edges, word_vectors.shape[1], fc_vectors.shape[1], hidden_layers).cuda()
  File ""E:\实验\DGP-master\models\gcn.py"", line 51, in __init__
    adj = spm_to_tensor(adj)
  File ""E:\实验\DGP-master\utils.py"", line 68, in spm_to_tensor
    (sparse_mx.row, sparse_mx.col))).long()
TypeError: can't convert np.ndarray of type numpy.int32. The only supported types are: double, float, float16, int64, int32, and uint8."
"hi,
I get val_loss=0.0000 when run ""python train_gcn_*.py"". Why is it ?
Thank you."
"Hi,

It's a great work, but i have some confusions about GPM and GCNZ.

From the reported result in paper, GPM has already received a huge improvement compared to the SOTA method GCNZ, however the paper just said GPM only use single layer compared to GNCZ.

However in GNCZ, author's experiment indicate that addition layers bring the improvement results. So i'm very confused.

Can i ask more clear difference between GPM and GCNZ?

Thanks very much!"
"Hi, 

I noticed that some words in the test set don't have valid word embedding. How to deal with them? Are these categories ignored during evluation?

Thanks!
"
Did I make awa2_induced_graph by replace input and output arguments? And what the meaning of induce_parents function? Also I notice that you are construct graph with all (train + test) categories together and Would that obtain test information during training?
"Where do you get this prior information?

sorry for my simple question, I'm a newbie for this topic."
" python3 train.py /media/disk1/xgl/cc-pil/formatted --dispnet DispResNet6 --posenet PoseNetB6   --masknet MaskNet6 --flownet Back2Future --pretrained-disp /media/disk1/xgl/cc-pil/geometry/dispnet_k.pth.tar   --pretrained-pose /media/disk1/xgl/cc-pil/geometry/posenet.pth.tar --pretrained-flow /media/disk1/xgl/cc-pil/geometry/back2future.pth.tar   --pretrained-mask /media/disk1/xgl/cc-pil/geometry/masknet.pth.tar -b4 -m0.1 -pf 0.5 -pc 1.0 -s0.1 -c0.3   --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997 --with-flow-gt   --with-depth-gt --epochs 100 --smoothness-type edgeaware  --fix-masknet --fix-flownet   --log-terminal --name EXPERIMENT_NAME
=> will save everything to checkpoints/EXPERIMENT_NAME
=> fetching scenes in '/media/disk1/xgl/cc-pil/formatted'
588 samples found in 5 train scenes
154 samples found in 1 valid scenes
=> creating model
=> using pre-trained weights for explainabilty and pose net
=> using pre-trained weights for explainabilty and pose net
=> using pre-trained weights from /media/disk1/xgl/cc-pil/geometry/dispnet_k.pth.tar
=> using pre-trained weights for FlowNet
=> setting adam solver


N/A% (0 of 100) |                                                                                                                                                     | Elapsed Time: 0:00:00 ETA:  --:--:--


N/A% (0 of 147) |                                                                                                                                                     | Elapsed Time: 0:00:00 ETA:  --:--:--


N/A% (0 of 38) |                                                                                                                                                      | Elapsed Time: 0:00:00 ETA:  --:--:--

/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:2941: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn(""nn.functional.upsample is deprecated. Use nn.functional.interpolate instead."")
/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  ""See the documentation of nn.Upsample for details."".format(mode))
/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn(""nn.functional.sigmoid is deprecated. Use torch.sigmoid instead."")
/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/functional.py:3384: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(""Default grid_sample and affine_grid behavior has changed ""
Traceback (most recent call last):
  File ""train.py"", line 784, in <module>
    main()
  File ""train.py"", line 353, in main
    train_loss = train(train_loader, disp_net, pose_net, mask_net, flow_net, optimizer, args.epoch_size, logger, training_writer)
  File ""train.py"", line 463, in train
    flow_fwd, flow_bwd, _ = flow_net(tgt_img_var, ref_imgs_var[1:3])
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py"", line 85, in parallel_apply
    output.reraise()
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/_utils.py"", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 1 on device 1.
Original Traceback (most recent call last):
100% (100 of 100) |###################################################################################################################################################| Elapsed Time: 0:00:04 Time:  0:00:04
    output = module(*input, **kwargs)
  File ""/home/xgl/anaconda3/envs/cuda10/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
100% (147 of 147) |###################################################################################################################################################| Elapsed Time: 0:00:04 Time:  0:00:04
  File ""/media/disk1/xgl/cc-pil/models/back2future.py"", line 174, in forward
    corr6_fwd = corr6_fwd.index_select(1,self.idx_fwd)
100% (38 of 38) |#####################################################################################################################################################| Elapsed Time: 0:00:04 Time:  0:00:04




Excuse me, can you help me solve this problem? thank you very much."
"When I set --log-terminal=True, the following error is occur

self.epoch_bar = progressbar.ProgressBar(maxval=n_epochs, fd=Writer(self.t, (0, h-s+e)))

TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'

and I find that in logger.py line16,   h = self.t.height,  then the h is None

Could you help me with this? Thank you very much"
"Hey Anurag, thanks for your wonderful work!

I got the following results when running test_flow.py:

| | epe_total        | epe_sp           | epe_mv  | Fl  | epe_total_gt_mask | epe_sp_gt_mask | epe_mv_gt_mask | Fl_gt_mask |
| ------------- |:-------------:|:-----:|:-------------:|:-------------:|:-----:|:-----:|:-----:|:-----:|
|Errors | 81.6802 |  6.4460 | 174.5231 | 0.6040 | 36.2934 | 6.3183| 183.9449 | 0.4385 |

This seems pretty large, especially the EPE. 

I run the evaluation in the same way as mentioned in the readme, with the following script:
```
model_dir=/path/to/cc_models/geometry
cd /path/to/cc/
python test_flow.py \
    --pretrained-disp=""$model_dir/dispnet_cs_k.pth.tar"" \
    --pretrained-pose=""$model_dir/posenet.pth.tar"" \
    --pretrained-mask=""$model_dir/masknet.pth.tar"" \
    --pretrained-flow=""$model_dir/back2future.pth.tar"" \
    --kitti-dir=/path/to/kitti_data/
```
All other settings are set as default. My PyTorch and cuda version are 1.4.0 and 10.0.

Is there something wrong? Or I miss something?

Thanks in advance!
Best,

"
"Hi,

Is there any demo script for producing optical flow on a test video? Specifically, I am interested in running test_flow.py on a video but the current code reads data (tgt_img, ref_imgs, intrinsics, intrinsics_inv) from a validation dataset loader. How do I extract these information from a test video?

Thanks"
"Hello, there seems something wrong with ""vstack"". The codes reports error as follows:

viz3_im = Image.fromarray(viz3.astype('uint8'))
TypeError: Cannot handle this data type

Can you help me with this, thanks!"
"Hi anuragranj,

The requirements.txt of your code denotes the necessary packages for your code's implementation. However I encoutered problems when installing the spatial-correlation-sampler, which may be a result of version conflict of spatial-correlation-sampler v0.2.1 and torch v0.4.1.

So I wonder your package versions of your spatial-correlation-sampler, torch and torchvision. I want to specify them to successfully install them. 

Thanks!"
"I have an indoor stereo image dataset with a total of 8,000 images. There is a salient mobile robot in the dataset scene that has been moving within the camera field. I have resized my dataset image to the size required by your code. I am training my model with the parameters in your open source code based on your pre-training model. I want to get a model that can perform good indoor monocular depth estimation and can segment the moving robot. But the model obtained is very poor and I can't see anything visually. Do you have any suggestions?"
"imresize is no longer provided by scipy. If the intention is to support 1.3.0 and above, I suggest to switch to Pillow or something else."
"Tensors in pytorch are formatted in CHW(BCHW) by default, so if you wanna output the results of depth,flow and mask, you should change them into HWC format.
such as:
 test_flow.py line 180
>row1_viz_im = Image.fromarray((255*row1_viz).astype('uint8'))
row2_viz_im = Image.fromarray((row2_viz).astype('uint8'))

this will raise TypeError(""Cannot handle this data type"")
you should transpose/permute the format into HWC like this below:

>row1_viz_im = Image.fromarray((255*row1_viz).astype('uint8').transpose((1,2,0)))
row2_viz_im = Image.fromarray((row2_viz).astype('uint8').transpose((1,2,0)))
"
"https://github.com/anuragranj/cc/blob/2b4e36292c18f8ee68ad5d210a4190f9adf881dc/models/back2future.py#L174

Thanks for your sharing。Can you explain why use index_select function here？
The index_select remap the costvolume according to the idx_fwd 。This remap process is better for optical flow learning？

Looking forward to your reply。"
"Hi, thanks for publishing the codes!

I have a question, when you train the network, it seems that you use the small image path? Like 256*832. But in the testing, do you use the original image with larger size (around 320*1240)?

If not, how do you handle the image size? Thanks! "
"Hi @anuragranj !

It is not clear from the train.py that how you were able to train like the training algorithm mentioned in the paper. How did you do that?

Thankyou!"
"Hi @anuragranj !

The cityscapes dataset is about 324GB, which means, in total I would need 700GB of disk space. Is there a efficient way to download the dataset?

Thankyou!"
"Hi @anuragranj !

After freshly checking out your code, I was trying to train dispnet and posenet with the followng command:
`python3 train.py /cdtemp/ezorfa/gateway30/datasets/kitti/kitti_cc --
dispnet DispResNet6 --posenet PoseNetB6   --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 1.0 -pf 0.0 -c 0.0 -s 0.1  --log-output -f 50 --nlevels 6 --lr 1e-4 -wssim 0.997    --epochs 100 --smoothness-type edgeaware  --fix-masknet --fix-flownet   --log-terminal --name cc_depth  --epoch-size 1000`

But even after 30K iterations there seem to be no hint of training: 

<img width=""515"" alt=""Screenshot 2019-12-09 at 08 19 06"" src=""https://user-images.githubusercontent.com/50787308/70415304-9c4fa480-1a5c-11ea-8a5b-b0541b3b9915.png"">


<img width=""515"" alt=""Screenshot 2019-12-09 at 08 18 50"" src=""https://user-images.githubusercontent.com/50787308/70415305-9c4fa480-1a5c-11ea-8060-8460b38dc5f3.png"">

Previously, I also trained for over 200K, but I dint see any result or any hint of training. It just stays white image  all the time. **Could you suggest me or hint at what I might be doing wrong??**


Appreciate your help,
Thankyou!"
"Hi @anuragranj !

I am training the model using all the pre-trained models that you published online. However, I do not get proper rigidity masks. Do you know why this is happening?  As I understand, Rigidity masks are the subtraction of rigid flow with flow generated by the flownet with some threshold(I havent chaned the code at all). Below is the picture:

<img width=""1086"" alt=""Screenshot 2019-11-26 at 07 21 07"" src=""https://user-images.githubusercontent.com/50787308/69604463-d745e700-101d-11ea-82f4-495a724bf2e5.png"">


Also, Could you share the optimizer checkpoint at the time of your training, so that I could visualize all the results at once?"
"Hi, thank you for sharing the code. I downloaded your pretrained models. It seems there's no pretrained model for flownetC6. Could you please share it?"
"Hi,  I have two questions about the training details.

1. Which weights do you use when starting a new cc training phase: the ""best"" weights selected through validation during the training of last phase    **or**    the weights saved in the end of last phase.

2.  When training depth&pose in competition step,  weights λ is set to (1.0, 0.5, 0.05, 0). Since flownet and  masknet are fixed, why set the  λ flowloss to 0.5 and λ exploss 0.05? (It doesn't make much difference anyway.)

Thank you.
"
"Could I get to know that if you use the `--spatial-normalize `? How should I use it and set the hyperparameters such as `-s`? I added the `--spatial-normalize` to train the depth and pose networks, but the results are very poor. 
However, I found that other paper used it and had a performance improvement.
I found the method `--spatial-normalize` comes from this paper. Thank you.

> C. Wang, J. M. Buenaposada, R. Zhu and S. Lucey, “Learning Depth from Monocular Videos using Direct Methods,” in Conference on Computer Vision and Pattern Recognition, 2018, pp. 2022-2030.
"
"When I want to run cc on my own dataset using run_inference.py I got following error.

python run_inference.py  --output-disp --pretrained ./downloads/geometry/dispnet_k.pth.tar --dataset-list ./output/test_files.txt --dataset-dir ./output/test_data --output-dir ./output/output --img-exts png
Traceback (most recent call last):
  File ""run_inference.py"", line 77, in <module>
    main()
  File ""run_inference.py"", line 37, in main
    disp_net.load_state_dict(weights['state_dict'])
  File ""/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py"", line 769, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for DispNetS:
        Missing key(s) in state_dict: ""conv2.0.weight"", ""conv2.0.bias"", “….


—————
where --pretrained “pretrained DispNet” weights are downloaded from:	
•	DispNet, PoseNet, MaskNet and FlowNet in joint unsupervised learning of depth, camera motion, optical flow and motion segmentation."
"Hey, I got This error when I want to run code.
 import spatial_correlation_sampler_backend as correlation

lib/python3.6/site-packages/spatial_correlation_sampler_backend.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZTIN3c1021AutogradMetaInterfaceE

Cuda 10, gcc 7, pytorch-1.0"
"hey, I traind all nets accroding to the readme file.When I tested posenet, I found that the camera pose in output was in 6DoF format and then converted to a 3*4 transformation matrix.

In a 3*4 transformation matrix, we usually think that the last column describes the translational motion of the camera.But when the test datas are pictures at the corner of the street,I feel that the network is not performing well when describing the translation.

For example,I chose five images as input .These images are all from the KITTI dataset.They described a street corner. In the transformation matrix of the output, three variables describing the translation, I think at least two variables should have a large change, however, there is only one.  This obviously does not describe the positional change of the camera during the turning of the vehicle. Detailed data are attached.

Where did I go wrong?

Thanks.
[test_data.zip](https://github.com/anuragranj/cc/files/3067860/test_data.zip)


"
"Hi, is single 1080Ti GPU (11GB) enough to train the whole model? "
"Thanks for your nice work and code release. 
- I wonder if there is any script for the training procedure in Alg.1 of your paper? I followed your paper and wrote my own version (without ground truth) as follows. Is it correct?

```
python3 train.py $data_root --dispnet DispResNet6 --posenet PoseNetB6 \
  --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 1.0 -pf 0.0 -m 0.0 -c 0.0 -s 0.1 \
  --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997  \
  --epochs $epochs --smoothness-type edgeaware  --fix-masknet --fix-flownet \
  --log-terminal --name $EXPERIMENT_NAME 

## Train flownet
python3 train.py $data_root --dispnet DispResNet6 --posenet PoseNetB6 \
  --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 0.0 -pf 1.0 -m 0.0 -c 0.0 -s 0.1 \
  --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997  \
  --epochs $epochs --smoothness-type edgeaware  --fix-dispnet --fix-posenet  --fix-masknet \
  --log-terminal --name $EXPERIMENT_NAME  --resume

## Train masknet
python3 train.py $data_root --dispnet DispResNet6 --posenet PoseNetB6 \
  --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 1.0 -pf 0.5 -m 0.0 -c 0.0 -s 0.1 \
  --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997  \
  --epochs $epochs --smoothness-type edgeaware  --fix-dispnet --fix-posenet --fix-flownet \
  --log-terminal --name $EXPERIMENT_NAME  --resume

while true
do

    ## Train dispnet & posenet
    python3 train.py $data_root --dispnet DispResNet6 --posenet PoseNetB6 \
      --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 1.0 -pf 0.5 -m 0.05 -c 0.0 -s 0.1 \
      --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997  \
      --epochs $epochs --smoothness-type edgeaware  --fix-masknet --fix-flownet \
      --log-terminal --name $EXPERIMENT_NAME  --resume

    ## Train flownet
    python3 train.py $data_root --dispnet DispResNet6 --posenet PoseNetB6 \
      --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 0.0 -pf 1.0 -m 0.005 -c 0.0 -s 0.1 \
      --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997  \
      --epochs $epochs --smoothness-type edgeaware  --fix-dispnet --fix-posenet  --fix-masknet \
      --log-terminal --name $EXPERIMENT_NAME  --resume

    ## Train masknet
    python3 train.py $data_root --dispnet DispResNet6 --posenet PoseNetB6 \
      --masknet MaskNet6 --flownet Back2Future  -b 4 -pc 1.0 -pf 0.5 -m 0.005 -c 0.3 -s 0.1 \
      --epoch-size 1000 --log-output -f 0 --nlevels 6 --lr 1e-4 -wssim 0.997  \
      --epochs $epochs --smoothness-type edgeaware  --fix-dispnet --fix-posenet --fix-flownet \
      --log-terminal --name $EXPERIMENT_NAME  --resume

done
```

- Besides, I found that if `--fix-flownet` is not given in the script, then there will be an error. I wonder if it is caused by the flownet model.
```
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
```"
"Hi @anuragranj, I should say that your work is impressive!. I would like to implement your methods but I am confused with some details, and I will appreciate if you can give me some insights.

About the training algorithm, Are the first three initializations of pose/depth, flow, and motion are just one iteration, or several iterations of gradient descent?. On each iteration of the main loop, does each step (collaboration and competition) run 100K iterations? (I mean: 100K compet, 100K collab, 100K compet and so on.)

Thanks in advance,"
"Such a great work I think, but I still need real code to understand it better. Hope you guys can make it public soon. 😭 "
"Hello @anuragranj ,

Thank you a lot for your work. Is the code related to Unsupervised Learning of Multi-Frame Optical Flow with Occlusions paper available ?

Thank you
"
Very interesting work. Eagerly waiting
"panoptic_semseg_train2017/000000000247.png
![000000000247](https://user-images.githubusercontent.com/101508488/164885924-19228ef2-0296-43b7-9153-455b25f139bb.png)


I want to know why the greyscale of the sky is 119, the sky-other in the [labels](https://github.com/nightrome/cocostuff/blob/master/labels.md) may 157 or 146(157-11)(some classes have been removed)?

I am so confused that how to build the mapping relationships between classes and greyscale in the .png"
"I got `ZeroDivisionError: division by zero` from `panopticapi/evaluation.py` for empty prediction.
With `tp + fp + fn == 0`, self increament of `n` is skipped. 
When all of categories.items() are skipped because empty prediction, `n` becomes zero and an error is reported.
Shouldn't `n` increase whenever category `isthing` (right after the first continue)? 
``` python
def pq_average(self, categories, isthing):
    pq, sq, rq, n = 0, 0, 0, 0
    per_class_results = {}
    for label, label_info in categories.items():
        if isthing is not None:
            cat_isthing = label_info['isthing'] == 1
            if isthing != cat_isthing:
                continue
        iou = self.pq_per_cat[label].iou
        tp = self.pq_per_cat[label].tp
        fp = self.pq_per_cat[label].fp
        fn = self.pq_per_cat[label].fn
>>        if tp + fp + fn == 0:
>>            per_class_results[label] = {'pq': 0.0, 'sq': 0.0, 'rq': 0.0}
>>            continue
>>        n += 1
        pq_class = iou / (tp + 0.5 * fp + 0.5 * fn)
        sq_class = iou / tp if tp != 0 else 0
        rq_class = tp / (tp + 0.5 * fp + 0.5 * fn)
        per_class_results[label] = {'pq': pq_class, 'sq': sq_class, 'rq': rq_class}
        pq += pq_class
        sq += sq_class
        rq += rq_class

    return {'pq': pq / n, 'sq': sq / n, 'rq': rq / n, 'n': n}, per_class_results
```"
"In the code, every `ann['segmentation']`, both for the instance annotations and the semantic annotations, is read through the functions in pycocotools.mask:

`from pycocotools import mask as COCOmask`

but these functions work only with the RLE format, so my guess is that the code fails for segments saved as polygons, instead of RLE.

Am I correct?"
"Running detection2panoptic_coco_format.py first fails on untouched fresh unzipped coco-things 

```
ubuntu@XXXXX:~/GitRepos/panopticapi$ python3 converters/detection2panoptic_coco_format.py --input_json_file /home/ubuntu/coco-things/annotations/instances_val2017.json --output_json_file /home/ubuntu/coco-things/panoptic_things.json
Creating folder /home/ubuntu/coco-things/panoptic_things for panoptic segmentation PNGs
CONVERTING...
COCO detection format:
        JSON file: /home/ubuntu/coco-things/annotations/instances_val2017.json
TO
COCO panoptic format
        Segmentation folder: /home/ubuntu/coco-things/panoptic_things
        JSON file: /home/ubuntu/coco-things/panoptic_things.json


loading annotations into memory...
Done (t=0.47s)
creating index...
index created!
Number of cores: 6, images per core: 834
Core: 0, 0 from 834 images processed
Caught exception in worker thread:
Traceback (most recent call last):
  File ""/home/ubuntu/miniconda3/lib/python3.9/site-packages/panopticapi/utils.py"", line 16, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/GitRepos/panopticapi/converters/detection2panoptic_coco_format.py"", line 69, in convert_detection_to_panoptic_coco_format_single_core
    raise Exception(""Segments for image {} overlap each other."".format(img_id))
Exception: Segments for image 397133 overlap each other.
multiprocessing.pool.RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""/home/ubuntu/miniconda3/lib/python3.9/multiprocessing/pool.py"", line 125, in worker
    result = (True, func(*args, **kwds))
  File ""/home/ubuntu/miniconda3/lib/python3.9/site-packages/panopticapi/utils.py"", line 20, in wrapper
    raise e
  File ""/home/ubuntu/miniconda3/lib/python3.9/site-packages/panopticapi/utils.py"", line 16, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/GitRepos/panopticapi/converters/detection2panoptic_coco_format.py"", line 69, in convert_detection_to_panoptic_coco_format_single_core
    raise Exception(""Segments for image {} overlap each other."".format(img_id))
Exception: Segments for image 397133 overlap each other.
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/ubuntu/GitRepos/panopticapi/converters/detection2panoptic_coco_format.py"", line 148, in <module>
    convert_detection_to_panoptic_coco_format(args.input_json_file,
  File ""/home/ubuntu/GitRepos/panopticapi/converters/detection2panoptic_coco_format.py"", line 118, in convert_detection_to_panoptic_coco_format
    annotations_coco_panoptic.extend(p.get())
  File ""/home/ubuntu/miniconda3/lib/python3.9/multiprocessing/pool.py"", line 771, in get
    raise self._value
Exception: Segments for image 397133 overlap each other.
Core: 1, 0 from 834 images processed

```
I've modified to code to skip over anything with an overlap and then the png maps load but the JSON conversion fails at saving 

```
# modification to skip overlaps
        if np.sum(overlaps_map > 1) != 0:
            bad_img_ids.append(img_id) #init this list as global
            continue
```
JSON Serialization Error. Something about d_coco['annotations'] fails even when using python ints 

```
Traceback (most recent call last):
  File ""/home/ubuntu/GitRepos/panopticapi/converters/detection2panoptic_coco_format.py"", line 151, in <module>
    convert_detection_to_panoptic_coco_format(args.input_json_file,
  File ""/home/ubuntu/GitRepos/panopticapi/converters/detection2panoptic_coco_format.py"", line 127, in convert_detection_to_panoptic_coco_format
    save_json(d_coco, output_json_file)
  File ""/home/ubuntu/miniconda3/lib/python3.9/site-packages/panopticapi/utils.py"", line 99, in save_json
    json.dump(d, f)
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/__init__.py"", line 179, in dump
    for chunk in iterable:
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/encoder.py"", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/encoder.py"", line 405, in _iterencode_dict
    yield from chunks
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/encoder.py"", line 405, in _iterencode_dict
    yield from chunks
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/encoder.py"", line 438, in _iterencode
    o = _default(o)
  File ""/home/ubuntu/miniconda3/lib/python3.9/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
```
```
    with open(input_json_file, 'r') as f:
        d_coco = json.load(f)
    d_coco['annotations'] = annotations_coco_panoptic
    d_coco['categories'] = categories_list
# converting bounding boxes & area to python integers
    for item in d_coco['annotations']:
        for item2 in item.get('segments_info'):
            if item2.get('bbox'):
                temp_bboxes = [int(num) for num in item2.get('bbox')]
                item2['bbox'] = temp_bboxes
                item2['area'] = int(item2['area'])
    save_json(d_coco, output_json_file) 
```



Even when I convert both the bounding boxes and area to python integers I still get the error. If I try to copy a sample to a jupyter notebook it has no problem saving to JSON 

```
test1 = {'image_id': 403385, 'file_name': '000000403385.png', 'segments_info': [{'area': 16555, 'iscrowd': 0, 'bbox': [411, 237, 93, 242], 'category_id': 70, 'id': 7906560}, {'area': 7561, 'iscrowd': 0, 'bbox': [9, 313, 142, 78], 'category_id': 81, 'id': 6538619}]}
save_json(test1,output_json_file) # success 
```"
"
I believe that Cityscapes is using coco format for input generation using scripts.

after running the panoptic deep lab model with Coco format input on Cityscapes, I am getting one JSON file in output with some result matrix. How can I use that JSON file with input test files to generate a predicted mask?

Is there any script available?"
"Since I found the evaluation caculation need the annotation json file and panoptic segmentation PNG file as the inputs, currently I just get the result of panoptic FPN model. 
So can I use the result of panoptic segmentation to caculate the evaluation result of PQ metric data? 
If you know how to do this or having some code to do this, can you share to me? 
Thank you so much!"
"Hey!
I saw that using detection2panoptic is only possible if there are ne overlappings in the annotations. 
Since I have some overlappings in my annoations, how to get rid of them?
Any suggestion? :) 

I'm struggling with this problem already for a while.."
"Following [these indications](https://github.com/cocodataset/panopticapi/blob/master/CONVERTERS.md), I am trying to obtain the json file for semantic segmentation, starting from the json file for panoptic segmentation.

The command that I used:
`python converters/panoptic2semantic_segmentation.py --input_json_file /path/to/json/panoptic_<train, val>2017.json --output_json_file /path/to/json/semantic_<train, val>2017.json`

The error:
 ```Traceback (most recent call last):
  File ""converters/panoptic2semantic_segmentation.py"", line 210, in <module>
    extract_semantic(args.input_json_file,
  File ""converters/panoptic2semantic_segmentation.py"", line 174, in extract_semantic
    json.dump(d_coco, out)
  File ""/usr/lib/python3.8/json/__init__.py"", line 179, in dump
    for chunk in iterable:
  File ""/usr/lib/python3.8/json/encoder.py"", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/usr/lib/python3.8/json/encoder.py"", line 405, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.8/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/usr/lib/python3.8/json/encoder.py"", line 405, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.8/json/encoder.py"", line 405, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.8/json/encoder.py"", line 438, in _iterencode
    o = _default(o)
  File ""/usr/lib/python3.8/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type bytes is not JSON serializable
```

Other information: I successfully used the converter for the images (from panoptic json to folder with semantic segmentation png) with both the training and validation set. 

Python version: 3.8.5"
"Hi. Thank you for sharing this work and dataset :)

When trying to run the conversion script from panoptic to segmentation, I ran into a `KeyError` because of [this line](https://github.com/cocodataset/panopticapi/blob/118ffaba478b5d45925e88d504625b4123f3160a/converters/panoptic2detection_coco_format.py#L120). I'm afraid I didn't save away the full stack trace.

Anyways, commenting that line out made the conversion go smoothly. I believe the intended reference was to the [`categories` variable](https://github.com/cocodataset/panopticapi/blob/118ffaba478b5d45925e88d504625b4123f3160a/converters/panoptic2detection_coco_format.py#L97) which does have a `color` key"
"Here is a coco panoptic segmentation annatation format from [COCO website](http://cocodataset.org/#format-data):
![image](https://user-images.githubusercontent.com/29373801/84879442-cd6cf980-b0bd-11ea-9c64-59d69cfd7404.png)

Since the `segment_info` has only `category_id` related to instance `color` mask in annotation mask image, how do you tell instances have the same `category_id` one from another?

Also, I noticed panoptic annotation mask images in [this project](https://github.com/cocodataset/panopticapi): `sample_data/panoptic_examples/000000142238.png`
![000000142238](https://user-images.githubusercontent.com/29373801/84880074-a95de800-b0be-11ea-94d4-e400872cf809.png)
the color of these `person` category are a little different from each others.

Thanks!
"
"In an attempt to convert COCO detection annotations to Panoptic Segmentation, I am getting the following output:
- The Generated PNG images are nothing but black images, no segments are visible
- The COCO Panoptic Annotation Generated JSON contains `""segments_info"": []` (Empty List) for all images. 

Command used to convert:
~~~
python converters/detection2panoptic_coco_format.py \
  --input_json_file /media/Data/Documents/Python-Codes/data/reading_order_dataset/complex/textline/via_export_coco.json \
  --output_json_file /media/Data/Documents/Python-Codes/data/reading_order_dataset/complex/textline/coco_panoptic.json
~~~

I have attached a sample of the files below:

- Input COCO Detection JSON with BBox and Segmentation Coordinates:
![image](https://user-images.githubusercontent.com/16799596/84217038-bbe98780-aae8-11ea-8e32-a0f7990f8587.png)
- Output Panoptic Segmentation JSON with empty `""segments_info""`:
![image](https://user-images.githubusercontent.com/16799596/84217150-0408aa00-aae9-11ea-94fa-3ca37761c018.png)
- Output generated Sample PNG which is completely blank black image:
![001](https://user-images.githubusercontent.com/16799596/84217219-2dc1d100-aae9-11ea-92d7-30327e9d1a04.png)

Please let me know what I am doing wrong and how to fix this.
"
"Hi, question:
[Here](https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py#L30) I read:

> The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.

However, in the [code](https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py#L77), a different encoding is used:
```
return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]
```

Which is correct? Thanks!"
"Hello

I am trying to run the script `panoptic2detection_coco_format.py`

My full command is 
```
python converters/panoptic2detection_coco_format.py   --input_json_file ~/data/coco_panoptic/annotations/panoptic_val2017.json --categories_json_file ./panoptic_coco_categories.json  --segmentations_folder ~/data/coco_panoptic/annotations/panoptic_val2017  --output_json_file ~/data/coco_panoptic/annotations/converted/panoptic2det_val2017.json
```
However, after a few seconds of processing the script errors out with
```python
Traceback (most recent call last):
  File ""converters/panoptic2detection_coco_format.py"", line 153, in <module>
    args.things_only)
  File ""converters/panoptic2detection_coco_format.py"", line 120, in convert_panoptic_to_detection_coco_format
    category.pop('color')
KeyError: 'color'
```

I was wondering is this was a bug or a mistake in how I am invoking the converter. Any help would be appreciated.

Thanks
Vighnesh"
"Hi Team,
Thanks for the great work.
I was wondering if it is possible to evaluate panoptic without predections in COCO panoptic json format. just with ground truth image and prediction image and panoptic coco format ground truth. 
thanks, 
best"
""
"Greetings,

I have a question regarding ID generator, lets say I create panoptic labels for a dataset with 5 different colours for cars viz. [0 , 0 , 255], [0, 0,  200], [0, 0, 150], [0, 0 , 100], [0, 0, 50] and i assign differnt category ids to them and use 5 different class ids for the same class ""car"".
can I still use IDGenerator for all the car colours I have assigned? will it try to generate more shades of blue or use all the ones I have mentioned? 
"
"Hello,

I am using python 3 and i have some issues storing json files from panoptic to detection and panoptic to semantic segmentation conversion. 

In panoptictosemantic conversion, the data from RLE is not gettiing decoded to utf 8 and it throws an error when it tries to save the data from 'count' on from RLE in the json as its bytes. 

interestingly same code works in the panoptictodetection conversion but it converts the byte as it is to a string as in the image attached. 

has anyone seen the same issue?

thanks 
![utf problem](https://user-images.githubusercontent.com/55992570/69324015-050ae480-0c48-11ea-927c-5ecaa8af868f.JPG)
"
"I think 
```
workers.close()
workers.join()
```
should be added right before the `return xxx` in method `combine_to_panoptic_multi_core` and `pq_compute_multi_core` for closing multiprocessing safely, as my collegue found it might not release the memory properly. I agreed and adopted it, but I didn't test it."
"[The RGB encoding used is ID](https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py#L30) seems not correct based on the implementation.

The comments should be **The RGB encoding used is ID = R + 256 * G + 256 * 256 * B**."
I am using cityscapes_panoptic_converter.py with test images. i only change the original_format_folder to the test images but I am getting blank black images. it works fine for train and val. how to fix this
how to test an image
"Hi,

I noticed that there is a folder called 'panoptic_train2017' in the annotation zip file, and this folder contains color png images corresponding to the training samples.
I wonder if there is any way to convert these png images to the panoptic annotations?
Or I have to incorporate other information from the json file?
Thank you."
"Hi, I was reading evaluation.py and found some lines confusing:
https://github.com/cocodataset/panopticapi/blob/8a32c19cde88dccb7336dc011e8a4a91f9d68c01/evaluation.py#L141-L147
For 'is_crowd' segments, category_id and segment id are stored in a dictionary, so it implies that
for each category, coco annotates at most one segment with is_crowd as true. In other words, suppose that there are two distant crowds of people, they are annotated with the same segment id. However, I can't find any support of this implication in the description of the panoptic dataset. Is this a feature of coco dataset?"
"According to  the code, it gives the combination method of predictions of instance and semantic segmentation as the paper -- Panoptic Segmentation. 

However, how do you generate the ground truth panoptic dataset? How do you deal with the conflicts of semantic and instance segmentation and the overlap of different instances when there are no confidence scores anymore?"
"I get a TypeError when trying to run the cityscapes converter.

The problem seems to be that something is using a numpy type which can't be serialized by the json module.

```
Traceback (most recent call last):
  File ""cityscapes_gt_converter/cityscapes_panoptic_converter.py"", line 119, in <module>
    panoptic_converter(original_format_folder, out_folder, out_file)
  File ""cityscapes_gt_converter/cityscapes_panoptic_converter.py"", line 116, in panoptic_converter
    json.dump(d, f)
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/__init__.py"", line 179, in dump
    for chunk in iterable:
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 430, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
    yield from chunks
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
    yield from chunks
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 325, in _iterencode_list
    yield from chunks
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 404, in _iterencode_dict
    yield from chunks
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 437, in _iterencode
    o = _default(o)
  File ""/export/home/yuyanli/.local/opt/miniconda3/envs/t4/lib/python3.6/json/encoder.py"", line 180, in default
    o.__class__.__name__)
TypeError: Object of type 'int64' is not JSON serializable
```

P.S. I also had to change `from utils import IdGenerator` to `from panopticapi.utils import IdGenerator`"
"In section 6 of ""Panoptic Quality"" paper, under ""Human annotations"", it is mentioned that PQ is symmetric, i.e. the order of ground truth and predictions is unimportant. In practice this does not seem to be the case, for example:
```
python panopticapi/evaluation.py --gt_json_file gt.json --pred_json_file pred.json
...
          |    PQ     SQ     RQ     N
--------------------------------------
All       |  30.1   45.2   33.3     2
Things    |  30.1   45.2   33.3     2
Stuff     |   0.0    0.0    0.0     0
...
```
```
python panopticapi/evaluation.py --gt_json_file pred.json --pred_json_file gt.json
...
          |    PQ     SQ     RQ     N
--------------------------------------
All       |  81.3   81.3  100.0     1
Things    |  81.3   81.3  100.0     1
Stuff     |   0.0    0.0    0.0     0
...
```
One of the reasons seems to be the special treatment of VOID class, for example predictions are not counted as false positives when their overlap with ground truth VOID is bigger than 0.5 (section 4.2 in the paper, under ""Void labels""). I think it enforces one interpretation of VOID, meaning ""not labeled"", when ignoring such areas would make sense. Alternative would be ""not known class"", in which case marking it with known class would count as false positive. The second interpretation is more common IMHO and would retain the symmetric property of the metric.

I suggest to change the interpretation of VOID to make the metric symmetric, or add it as a command-line option."
"- recently i am study Panoptic Segmentation.  so learn the panoptic_annotations_trainval2017 datasets.
- i have understand some thing from the datasets , i know from the json file can get lots information.
- we can get category like this ` ""categories"": [{""supercategory"": ""person"", ""color"": [220, 20, 60], ""isthing"": 1, ""id"": 1, ""name"": ""person""}...}` the person is thing , the color is [220, 20, 60]; so we can get the ids thought `ids=R+G*256+B*256^2.` ; the ids is in ` ""annotations"": [{""segments_info"": [{""area"": 3528, ""category_id"": 1, ""iscrowd"": 0, ""id"": 3937500, ""bbox"": [282, 207, 48, 149]},...` ; so i think the thing ids is unique; 
- but in the annotations : 
```
{""segments_info"": [{""area"": 3528, ""category_id"": 1, ""iscrowd"": 0, ""id"": 3937500, ""bbox"": [282, 207, 48, 149]}, {""area"": 3751, ""category_id"": 1, ""iscrowd"": 0, ""id"": 4260062, ""bbox"": [420, 183, 49, 173]}, {""area"": 2301, ""category_id"": 1, ""iscrowd"": 0, ""id"": 2035955, ""bbox"": [271, 118, 53, 141]}, {""area"": 4076, ""category_id"": 1, ""iscrowd"": 0, ""id"": 4325578, ""bbox"": [337, 208, 54, 149]}, {""area"": 2931, ""category_id"": 1, ""iscrowd"": 0, ""id"": 2628072, ""bbox"": [47, 221, 36, 138]}...
```
the category_id :1 has more than one id:(3937500,2035955,2628072...); then i found when i get_color the thing may has random chagne base on original color:
```
    def get_color(self, cat_id):
        def random_color(base, max_dist=30):
            new_color = base + np.random.randint(low=-max_dist,
                                                 high=max_dist+1,
                                                 size=3)
            return tuple(np.maximum(0, np.minimum(255, new_color)))

        category = self.categories[cat_id]
        if category['isthing'] == 0:
            return category['color']
        base_color_array = category['color']
        base_color = tuple(base_color_array)
        if base_color not in self.taken_colors:
            self.taken_colors.add(base_color)
            return base_color
        else:
            while True:
                color = random_color(base_color_array)
                if color not in self.taken_colors:
                     self.taken_colors.add(color)
                     return color
```

- so different thing instance has litte different color, but is not fixed; 
- so question is when i get my own predict panoptic result png, the thing instance ids is different with groudtruth because of the random ?
- How do you make sure you get the right match ? when use the evaluation.py `matched_annotations_list`
- I don't know if I understand it correctly ? so please help me ."
"When averaging SQ over classes, even those classes are taken into account where TP=0, i.e. which were not even recognized correctly. For those classes SQ=0, so they reduce average SQ considerably. At least for me this breaks the intuition of SQ being a metric that measures how well the segmentation matches ground truth. Consider this example:

class|IOU|TP|FP|FN|PQ|SQ|RQ
-|-|-|-|-|-|-|-
class 1|0.88|1|0|1|0.59|0.88|0.67
class 2|0.69|1|0|0|0.69|0.69|1
class 3|0.63|1|0|0|0.63|0.63|1
class 4|0|0|1|1|0|0|0
class 5|0|0|1|0|0|0|0
**average**|||||**0.38**|**0.44**|**0.53**

Even though per-class segmentation results were decent for first three classes, when averaged over all classes 0.44 seems unfair. Following the SQ calculation algorithm one could wonder how SQ can ever be smaller than 0.5. Simple solution would be to count only non-zero SQ results when averaging, i.e. in this case the average SQ would be 0.73.

At first I thought that this would break the nice PQ=SQ*RQ formula. But then I realized that averaging breaks it anyway. I understand that this brings up the question how to average PQ as well and it would be nice if the rules would be consistent. That's why I'm posting this as an issue to discuss rather than pull request."
"Hi!

I am new to image segmentation and wanted to know if it's possible to extract the semantic and instance segmentation images using `visualization.py` script. I understand that we have **converters** scripts but they require **pycocopi** which requires downloading huge images dataset (more than 1 GB). Is it possible to modify the visualization script somehow to get the two segmentation results for the sample images?

Thanks in advance"
"Hi, @alexander-kirillov , I have a custom dataset with annotations including instance segmentation masks and semantic segmentation masks. I generate this dataset in 2channels format, but I found that 2channels2panoptic_coco_format.py doesn't give 'bbox' and 'area' in 'segments_info', which makes the panoptic segmentation model unable to train.  So, I convert it to instance segmentation format, and then try to convert it to panoptic segmentation format.But I got the following error, 

line 68, in convert_detection_to_panoptic_coco_format_single_core
raise Exception(""Segments for image {} overlap each other."".format(img_id))

Exception: Segments for image 1 overlap each other.

This problem has been bothering me for a long time. And How can I get the correct data format for panoptic segmentation?"
"So I'm trying to prepare some data in the coco panoptic format for Detectron2's PanopticFPN. I'm missing the stuff annotation pngs. Apparently you can use the panoptic2semantic_segmentation Converter. But there is a default file path:

parser.add_argument('--categories_json_file', type=str,
                        help=""JSON file with Panoptic COCO categories information"",
                        default='./panoptic_coco_categories.json')

I have no idea where to get this json file from... I have the png annotations and the overall .json file with all the instances etc. Someone already did this?
"
"Hi,

I'm a bit confused how get these pixel wise class-agnostic image segmentation PNGs as mentioned on the coco website.
Can you point me in the right direction?

Thanks for you help,
Bjarne"
"I want to run a script combine_semantic_and_instance_predictions.py. It requires one of the input as JSON file with corresponding image set information. From where I can get it for COCO2017 datasets. 

Thanks"
"Great initiative and awesome work. Thanks.

I have some data in the form of rgb images and panoptic labels also as images. 
I sadly do not have the labels in any other format.
Is there a way to get the labels from images to coco panoptic format? 

thanks"
""
"Hi, I'm trying to run evaluation on cityscapes dataset. However, I can't find proper coco format of the gt files. I found a convertor in repo https://github.com/facebookresearch/Detectron/blob/master/tools/convert_cityscapes_to_coco.py.
But it can't work for panoptic evaluation. Could anyone please tell me there can I find a gt files for cityscapes dataset?

Thank you"
"From http://cocodataset.org/#panoptic-2018: 

""The panoptic task uses all the annotated COCO images and includes the 80 thing categories from the detection task and a subset of the 91 stuff categories from the stuff task, with any overlaps manually resolved.""

Should the panoptic categories not contain 171 classes then? When I run the following code it prints out 133.

```
pan_cats = ""panoptic_coco_categories.json""
with open(pan_cats, 'r') as COCO_Pan:
    pan = json.loads(COCO_Pan.read())
    print(len(pan))

```"
"When running the script with the following arguments:

> --input_json_file ""panoptic_train2017.json""
> --segmentations_folder ""stuff_train2017_pixelmaps""
> --output_json_file ""my_folder""
> --categories_json_file ""~/panopticapi/panoptic_coco_categories.json""
> --things_only

I get the following error:

> CONVERTING...
COCO panoptic format:
	Segmentation folder: stuff_train2017_pixelmaps
	JSON file: panoptic_train2017.json
TO
COCO detection format
	JSON file: train_semantic_seg
Saving only segments of things classes.


>Reading annotation information from panoptic_train2017.json
Number of cores: 8, images per core: 14786
Core: 0, 0 from 14786 images processed
Caught exception in worker thread:
Traceback (most recent call last):
  File ""/home/nicolas/panopticapi/converters/utils.py"", line 14, in wrapper
    return f(*args, **kwargs)
  File ""/home/nicolas/panopticapi/converters/panoptic2detection_coco_format.py"", line 61, in convert_panoptic_to_detection_coco_format_single_core
    segm_info['segmentation'] = COCOmask.encode(np.asfortranarray(mask))[0]
KeyError: 0
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""/home/nicolas/.pyenv/versions/3.6.0/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/home/nicolas/panopticapi/converters/utils.py"", line 18, in wrapper
    raise e
  File ""/home/nicolas/panopticapi/converters/utils.py"", line 14, in wrapper
    return f(*args, **kwargs)
  File ""/home/nicolas/panopticapi/converters/panoptic2detection_coco_format.py"", line 61, in convert_panoptic_to_detection_coco_format_single_core
    segm_info['segmentation'] = COCOmask.encode(np.asfortranarray(mask))[0]
KeyError: 0
""""""

>The above exception was the direct cause of the following exception:

>Traceback (most recent call last):
  File ""/home/nicolas/panopticapi/converters/panoptic2detection_coco_format.py"", line 153, in <module>
    args.things_only)
  File ""/home/nicolas/panopticapi/converters/panoptic2detection_coco_format.py"", line 109, in convert_panoptic_to_detection_coco_format
    annotations_coco_detection.extend(p.get())
  File ""/home/nicolas/.pyenv/versions/3.6.0/lib/python3.6/multiprocessing/pool.py"", line 608, in get
    raise self._value
KeyError: 0

I tried to remove the [0] in the line `segm_info['segmentation'] = COCOmask.encode(np.asfortranarray(mask))[0]` which resulted in 

> KeyError: 'color'

later on. Any help would be much appreciated."
"HI, 

The semantic data.py is described as extraction of semantic annotation for stuffs and things from ground truths and saving them as a single channel png .

Loading gt using opencv as greyscale : 
```
>>> o = cv2.imread(""../panoptic_val2017/000000000139.png"", 0 ) 
>>> o.shape
(426, 640)
>>> o
array([[119, 119, 119, ..., 119, 119, 119],
       [119, 119, 119, ..., 119, 119, 119],
       [119, 119, 119, ..., 119, 119, 119],
       ..., 
       [ 78,  78,  78, ..., 129, 129, 129],
       [ 78,  78,  78, ..., 129, 129, 129],
       [ 78,  78,  78, ..., 129, 129, 129]], dtype=uint8)
```
 After running the `semantic_data.py` and generating the pngs : 
```
>>> i = cv2.imread(""000000000139.png"", 0 ) 
>>> i.shape
(426, 640)
>>> i
array([[119, 119, 119, ..., 119, 119, 119],
       [119, 119, 119, ..., 119, 119, 119],
       [119, 119, 119, ..., 119, 119, 119],
       ..., 
       [ 78,  78,  78, ..., 129, 129, 129],
       [ 78,  78,  78, ..., 129, 129, 129],
       [ 78,  78,  78, ..., 129, 129, 129]], dtype=uint8)
```
Comparing the two : 
```
>>> a = i == o
>>> a
array([[ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       ..., 
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True],
       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)
>>> True in a
True
>>> False in a
False
```

Is there any difference between the two that I am missing ? Can I simply use the imread with loading as greyscale, instead of an extra pre-processing step ? "
"I am trying to visualize some annotations output from instance_data.py. In the following are three images of class `PERSON` (in the validation set), each shows the mask of `ONE` particular instance of class `PERSON` (the foreground are `WHITE`). Are they supposed to be like this?

000000364884.jpg
![000000364884_person](https://user-images.githubusercontent.com/4015328/43178913-bbdd39ac-8f84-11e8-8bb4-18ab3e525242.png)
000000005586.jpg
![000000005586_person](https://user-images.githubusercontent.com/4015328/43178897-a3cf958a-8f84-11e8-8f02-05b1058257f4.png)
000000191845.jpg
![000000191845_person](https://user-images.githubusercontent.com/4015328/43178939-dd7e4ae2-8f84-11e8-816e-36b0ec974fdd.png)
"
"I'm trying to run visualization.py as is. This folder 

https://github.com/cocodataset/panopticapi/blob/d9c2c1fbaa69cb78c86f1a3a8d6cb11aa8698120/visualization.py#L15

doesn't exist in the repo."
"Hi, thanks for the work!
I have some questions about the ""instance_data.py"":
the line:
`pan = pan_format[:,:,0] + 256 * pan_format[:,:,1] + 256*256*pan_format[:,:,2]`
Could you give some details about this line?
When I print the results after converting to the COCO instance format, it seems that there are all the binary encoding for the ""segmentation"" rather than the decimal code as the COCO instance format.
Thanks very much!"
"Hi,
It seems that that COCO Panoptic annotation does not have 'color' attribute
http://images.cocodataset.org/annotations/panoptic_annotations_trainval2017.zip

eg.
...{""supercategory"": ""building"", ""isthing"": 0, ""id"": 197, ""name"": ""building-other-merged""}, {""supercategory"": ""solid"", ""isthing"": 0, ""id"": 198, ""name"": ""rock-merged""}, {""supercategory"": ""wall"", ""isthing"": 0, ""id"": 199, ""name"": ""wall-other-merged""}, {""supercategory"": ""textile"", ""isthing"": 0, ""id"": 200, ""name"": ""rug-merged""}]}

When I use format_converter.py I get ""KeyError: 'color'""

Thanks!"
"in line:
`print('no prediction for the image with id: {}'.format(img_id))`
I think the variable `img_id` should be `image_id`"
""
I read your paper and I think this is really inspiring work. I really hope to try the model and test some ideas. So I'm wondering whether/when will the code or model described in paper be released?
"Hi, I am building the depth estimation and object detection networks using your mtan framework. I used equal weighting for each task and adam as optimiser. I found that the detection tasks perform quite bad during the training. But it performed reasonably when I trained it as single task attention network. Do you have any idea of why this happens. 

Many Thanks"
"Hi! Really impressive work! I am trying to build the attention framework on the mobilenet. I have some problems about the structure of the encoder_block_att. I noticed the implementation of ""encoder_block_att"" which is a shared feature extractor described in your paper on SegNet and the Resnet is quite different. In the SegNet, it is just a 3*3Conv with batchnormalisation and pooling which is said on your paper. But in the Resnet, it is a more complicated structure with 3 convolution operations. I wander if the block should be different on different network. If so, how should design block on my network.

Thanks!
Resnet:
  (encoder_block_att_1): Bottleneck(
    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )

SegNet:
    (0): Sequential(
      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )"
"您好  
我将您的DWA-loss策略  使用在停车位多任务中  ，这个任务是既有分割 又有检测的任务，
我希望解决任务中多任务负转移的问题   在我的实验中  第一个第二个epoch会正常收敛  但是在第三个epoch 梯度loss全部变为nan，我不明白其中的原因   希望可以得到您的帮助"
"Thanks for releasing the code and dataset!

I have a question about the image in your pre-processed cityscapes. 
May I ask which pre-processed image corresponds to which image in the original cityscapes?
For example, Does ""0.npy"" in your training set match with ""aachen_000000_000019_leftImg8bit.png"" in the original training set?

Do you have a mapping list between your image names and the original image names? (e.g., ""0.npy"" - ""aachen_000000_000019_leftImg8bit.png"")

Thank you!"
"Hi,
Many thanks for the paper and for the most readable code!
I tried to reproduce two lines out of table 3 in the paper using your code and following the instructions in the readme file.
The results I got are not consistent with those in the paper (not sure how to attach - attaching as spreadsheet and as image):
![Table_3](https://user-images.githubusercontent.com/10104853/148905209-e380d6f0-fbd0-4fda-a8a8-168fda675327.png)
[mtan_res.ods](https://github.com/lorenmt/mtan/files/7844776/mtan_res.ods)

All number are average of validation results for the last 10 epochs, as printed by the code.
How can such discrepancy be explained?
"
"您好，请问您论文中的Attention Module为什么要这样设计呢？是基于什么原理这样设计的呢？还是参考了现有文献？
谢谢~~"
"It seems dwa only considers the rate of change of loss for each task. without the common scale of gradient, which introduced by GradNorm. So can dwa get comapareble results to GradNorm?"
"Hi,

If I understand your code correctly, the dynamic weights are calculated using the unweighted loss of each epoch `(train_loss)` , which is accumulated in the `avg_cost `variable. Shouldn't the weighted loss `(loss)` be accumulated instead? Or would we lose the sense of the training pace because the weights had already been applied?

Thank you for your time."
"Hi,  thanks for releazing the codes.

I'm wonder how to visulize the predicted result of surface normal estimation and segmentation map on nyu-v2 dataset.
Can you share these code?

Thank you!"
"你好，当我使用您代码中的uncertainty loss进行训练的时候
loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))
整个loss在经过几个epoch之后会一直是负的，请问您训练时是这样的情况吗？
此外，想问下您为什么logsigma全部初始化为-0.5。
谢谢！"
I read the image “0.npy” and times 255. but after that I can't see the real picture.
您好，我想问一下联合多任务损失函数中DWA的Temperature的取值范围在多少合适呢，依据什么来选择得到文章中的2呢？（loss值的大小？），先提前感谢您的回答
您好，我想问一下您的多任务损失函数中DWA是在哪定义的和调用呢？另外，在多任务网络中可以存在多个优化器嘛？
"How to use the generated multi-tasking network model to complete the semantic segmentation and depth estimation of a single image, as shown in Figure 4 of the paper?"
"Hi,

I'm trying to run your `resnet_mtan.py` to do two semantic label tasks. I used your `utils.py` and `create_dataset.py. `

From `resnet_mtan.py` I modified the `# Task specific decoders` part as follows:

```
        for i, t in enumerate(self.tasks):
            #out[i] = F.interpolate(self.decoders[i](a_4[i]), size=out_size, mode='bilinear', align_corners=True)
            if t == 'segmentation1':
                out[i] = F.log_softmax(out[i], dim=1)
            if t == 'segmentation2':
                out[i] = F.log_softmax(out[i], dim=1)
            #if t == 'normal':
            #    out[i] = out[i] / torch.norm(out[i], p=2, dim=1, keepdim=True)
        return out
```

But I get this error (`AttributeError: 'int' object has no attribute 'log_softmax'`):

```
Parameter Space: ABS: 72062479.0, REL: 2.8847
LOSS FORMAT: SEMANTIC-1_LOSS MEAN_IOU PIX_ACC | SEMANTIC-2_LOSS MEAN_IOU PIX_ACC
Standard training strategy without data augmentation.
torch.Size([512, 512])
Traceback (most recent call last):
  File ""resnet_mtan.py"", line 177, in <module>
    two_task_trainer(nyuv2_train_loader,
  File ""/home/models/mtan/im2im_pred/utils.py"", line 359, in two_task_trainer
    train_pred = two_task_model(train_data)
  File ""/home/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py"", line 153, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/home/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""resnet_mtan.py"", line 112, in forward
    out[i] = F.log_softmax(out[i], dim=1)
  File ""/home/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py"", line 1535, in log_softmax
    ret = input.log_softmax(dim)
AttributeError: 'int' object has no attribute 'log_softmax'
```
Could you help me to solve the error?

Also, I think this line (`out[i] = F.interpolate(self.decoders[i](a_4[i]), size=out_size, mode='bilinear', align_corners=True)
`) aime to do depth prediction, but I'm not sure what the out-size variable is for. So, I remove it based on my tasks, is that right? 

Your help is greatly appreciated."
"Hello, I would like to ask where is the definition of the multitasking loss function? Does the code lack the loss function call？"
"
Hi lorenmt!

Thank you for sharing your code. I would like to train the `model_segnet_mtan.py` with my own dataset. I have prepared the data to have the same size (500x500) and to be in the same format as in `create_dataset.py`. However, when I've started the training I got the following error: 

```
Parameter Space: ABS: 44229076.0, REL: 1.7705
LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30
Standard training strategy without data augmentation.
image: torch.Size([3, 500, 500])
semantic: torch.Size([500, 500])
depth: torch.Size([500, 500])
normal: torch.Size([500, 500])
Traceback (most recent call last):
  File ""model_segnet_mtan.py"", line 222, in <module>
    200)
  File ""/home/models/mtan/im2im_pred/utils.py"", line 158, in multi_task_trainer
    train_pred, logsigma = multi_task_model(train_data)
  File ""/home/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""model_segnet_mtan.py"", line 143, in forward
    g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])
  File ""/home/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/miniconda3/lib/python3.7/site-packages/torch/nn/modules/pooling.py"", line 356, in forward
    self.padding, output_size)
  File ""/home/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py"", line 598, in max_unpool2d
    return torch._C._nn.max_unpool2d(input, indices, output_size)
RuntimeError: Shape of input must match shape of indices
```
It seems the error because of the input shape. Do I need to resize the images or can the model accept images with size 500x500?, how can I modify the indices shape? Could you help me to solve the error, I'm new to PyTorch?."
"Hi there,
I downloaded the [256x512, 2.42GB] version of the Cityscapes dataset and found that there are only 490 files in _val/label_19_ but 500  in other corresponding folders. I checked the Dropbox folder roughly and found out that some files like _390.npy_ is missing. Could you please help me with the missing files? 
Thank you very much!"
"Hi,

I downloaded the precomputed NYUv2 surface normals ground truth as mentioned in the readme (nyu link). I see that there is a grey border surrounding the surface normal ground truth image as in one example below:

![0000](https://user-images.githubusercontent.com/26181088/114562423-c75c9080-9c6e-11eb-8910-5dfc26b752a4.png)

could you let me know whether you also observed this ?
"
"Hi,

In your preprocessed cityscape dataet, there exit a segment label -1 . 
What mean of that?
byw, do you know the max depth of cityscape?

thanks so much!"
"Hi,
Thanks for your code!

Could you please provide a **pre-processing CityScapes dataset google dive link**? 
I cannot download from dropbox.

Thanks!"
"Hi there, I have tried multiple versions of python to unpickle the `dict_mean_std` but I keep getting a `KeyError: 10` due to trying to unpickle in a different python version. Would you be so kind to tell me either the python version you used to pickle this object or the actual mean and std values. Thanks in advance!"
"Hello,
I wonder if you provide a subset of the Nyu dataset to enable people to run the code without downloading 8.5 GB.


Thanks

"
"Hello ! 
Can you explain please why did you initialize the avg cost function with  24?
eg :      avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)
 "
"Hello, I read your paper recently and thank you for sharing your codes.
I tried to read the architecture mentioned in the paper, but I can't find the attention module.
Your code' format is different with other common Pytorch code. 
In my understanding, you implement attention by 1x1 depth-wise conv, and then merged the knowledge. I is different with attention in NLP, for example, the Multi-head Attention, have a k\q\v vector.
Would you please help me, thank you very much."
"Thank you for your excellent code!

In README, there is the option of 'uncert' in 'weight', but there are no details about 'uncert' in the code. Can you provide the relevant code? Thank you very much!"
"Hi, I ran your official MTAN-DeepLabv3 and find that miou decrease when training while picAcc stay steady and loss on validate set are increasing consistently.

I also modify it to become `resnet_split.py`, `resnet_single.py`, `resnet_cs.py`. Same trends can be found. 

When training MTL model, benchmarks in depth and surface normal estimation are increasing/decreasing normally.

I also run `model_segnet_*.py`. Decreasing on miou when training also exists but is much smaller and imperceptible. I think it is because of low miou and accuracy with SegNet.

I'm a new hand in NYUv2 and segmentation, I am not sure whether it is because of overfitting.

Some implement details:

- I use the same code in `model_segnet_*.py` for compute miou and pixAcc except a slight modification to accelerate training.

- optimizer and lr_scheduler are the same with `model_segnet_*.py`

- for more details, my code can be find in [resnet_mtan.py](https://paste.ubuntu.com/p/XHjTrG7vvM/) and [train_utils.py](https://paste.ubuntu.com/p/vYvr9MY2cw/)

Here are some records(mtan, split, single, and mixed respectively):

![image](https://user-images.githubusercontent.com/21152393/87113912-e7a38d00-c2a1-11ea-94ae-8ad2939b0269.png)
![image](https://user-images.githubusercontent.com/21152393/87113929-f12cf500-c2a1-11ea-880b-3df455ef01cd.png)
![image](https://user-images.githubusercontent.com/21152393/87113944-f7bb6c80-c2a1-11ea-9fcb-a4c11f1d147b.png)
![image](https://user-images.githubusercontent.com/21152393/87114025-233e5700-c2a2-11ea-99bd-b2d961d92765.png)
"
"First, Thanks for your amazing work!
When I run the program in the im2im_pred folder, I don't know how to save and evaluate the model. And I don't know whether I've missed some of the highlights abou this aspect. I would appreciate it if you could answer me.
If there are some grammatical errors, please forgive me."
"thank you for sharing your cod impressive work,
is there any implementation in keras?"
"Hello and sorry to bother.
I am currently trying to train a multitask segnet model on NYUv2 dataset. I just have a few questions about it and hope you could help me.
1) After running the code and training for long enough, where the trained model will save? The wide resnet codes have torch.save at the bottom lines of the script. but the segnet models don't include the same lines and basically after the system finishes running the script I can't find any model as the result. Of course results of the evaluation on all 3 tasks will be given to me but I just can't find the saved model.

2) I also wanted to ask for your help about the prediction code. Is there a way to give the trained model and one desired image to some prediction script and get 3 different image outputs for semantic masks and depth plus the surface normalization?

Best regards.


"
"`def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


print('Parameter Space: ABS: {:.1f}, REL: {:.4f}\n'.format(count_parameters(SegNet),
                                                           count_parameters(SegNet)/24981069))`

what is the meaning of 24981069"", why is the total parameters devided by 24981069 ?  What is the unit of  parameters?"
"Hello, sorry to bother you.
I ran **the same code several times** but got unstable evaluate results. Here is the numbers:

|                    | MEAN_IOU | PIX_ACC | ABS_ERR | REL_ERR | MEAN    | MED     | <11.25 | <22.5  | <30    |
| ------------------ | -------- | ------- | ------- | ------- | ------- | ------- | ------ | ------ | ------ |
| semantic (2 times) | 0.1693   | 0.548   |         |         |         |         |        |        |        |
|                    | 0.178    | 0.5685  |         |         |         |         |        |        |        |
| depth (2 times)    |          |         | 0.6336  | 0.2739  |         |         |        |        |        |
|                    |          |         | 0.6398  | 0.2585  |         |         |        |        |        |
| normal (3 times)   |          |         |         |         | 31.8452 | 26.0211 | 0.2161 | 0.4419 | 0.5633 |
|                    |          |         |         |         | 30.3995 | 23.9483 | 0.2406 | 0.4779 | 0.5975 |
|                    |          |         |         |         | 29.788  | 23.3448 | 0.2453 | 0.4861 | 0.6058 |
| split (2 times)    | 0.1858   | 0.5571  | 0.6323  | 0.288   | 32.4637 | 27.4159 | 0.2028 | 0.4184 | 0.5412 |
|                    | 0.164    | 0.5255  | 0.6479  | 0.3159  | 33.8992 | 28.8828 | 0.1806 | 0.395  | 0.5174 |

Details: 

1. results above are **the average of last 10 epoches**.
2. I used most of default options, which means I ran:

```
python model_segnet_single.py --task [task] --dataroot nyuv2
```

and

```
python model_segnet_split.py --dataroot nyuv2
```

3. I made some modification on the code. There are mainly two: 
   - move `scheduler.step()` according to README
   - change batch size from 2 to 8

UPDATE

Here is the results of the code **without any modification** (except the location of `scheduler.step()`) to make my claim more convincing:

|                 | ABS_ERR | REL_ERR |
| --------------- | ------- | ------- |
| depth (2 times) | 0.6615  | 0.2859  |
|                 | 0.7053  | 0.2996  |

"
"Hello, sorry to bother. But I found that using different `batch_size` settings may cause different Evaluation Results. For Example, the follows are Metrics in SGNet-MTAN, and the `batch_size` is used as test_batch_size.(`RMSE` is modified according to the equation)

Batch_size | RMSE | Abs_Rel | M-IOU | Pixel_ACC | MEAN | MED | <11.25 |Times
---|---|---|---|---|---|---|---|---
1 | **0.7974** | 0.2550 | 0.1993 | **0.5536** | 30.9889 | 27.0419 | 0.1986 | 49s
2 | 0.8138 | 0.2545 | 0.1993 | 0.5536 | 30.9630 | 26.9051 | 0.1988 | 47s
4 | 0.8241 | 0.2536 | 0.1993 | 0.5532 | 30.9538 |  26.8244 | 0.1988 | 47s
8 | 0.8321 | **0.2526** | 0.1993 | 0.5531 | 30.9388 | 26.7398 | 0.1989 | 46s
 "
"Hello, I have a question about the architecture of the model. In Fig.2, is the `pool` in Attention Module for Encoder the same as the same stage in VGG? And so as the `samp` in Decoder part.
Thanks for your time!"
"Hi, @lorenmt,

I read your code and found that on NYUv2, you use VGG as the backbone, but
1. it is not implemented by vgg in torchvision, which exact vgg structure do you use?
2. it is not initialized by the model pretrained on ImageNet. Why is it the case?

Thanks."
"Hi, @lorenmt. I downloaded your processed Cityscapes dataset and found that the values in those numpy arrays are >= 0 (probably most of them are <0.5). And when I load the official Cityscapes disparity, the values are also >=0, but are much larger (maybe ~30000). Would you tell me how do you pre-process the original disparity data to get those numpy arrays? Thanks in advance."
"hi @lorenmt Thanks for sharing your high-quality code .
 
1.  I  download the dataset and devkit at the official Visual Decathlon Challenge website, and run python model_wrn_mtan.py for training , the result seems much lower than yours . Is there someting wrong with my code? and  I find visual_decathlon did not use DWA module 


EPOCH: 0398 | DATASET: aircraft || TRAIN: 4.4714 0.0295 || TEST: 4.5622 0.0113
EPOCH: 0398 | DATASET: cifar100 || TRAIN: 3.9217 0.1044 || TEST: 4.4817 0.0232
EPOCH: 0398 | DATASET: daimlerpedcls || TRAIN: 0.5747 0.7077 || TEST: 0.8359 0.5173
EPOCH: 0398 | DATASET: dtd || TRAIN: 3.4813 0.1200 || TEST: 3.5718 0.0908
EPOCH: 0398 | DATASET: gtsrb || TRAIN: 2.4235 0.2809 || TEST: 3.5151 0.0662
EPOCH: 0398 | DATASET: omniglot || TRAIN: 6.6661 0.0077 || TEST: 8.3716 0.0012
EPOCH: 0398 | DATASET: svhn || TRAIN: 2.0999 0.2669 || TEST: 2.3221 0.0960
EPOCH: 0398 | DATASET: ucf101 || TRAIN: 4.2835 0.0416 || TEST: 4.2502 0.0405
EPOCH: 0398 | DATASET: vgg-flowers || TRAIN: 3.8070 0.1078 || TEST: 3.5612 0.1459

EPOCH: 0399 | DATASET: aircraft || TRAIN: 4.4398 0.0367 || TEST: 4.5639 0.0136
EPOCH: 0399 | DATASET: cifar100 || TRAIN: 3.7655 0.1223 || TEST: 4.4952 0.0246
EPOCH: 0399 | DATASET: daimlerpedcls || TRAIN: 0.5523 0.7139 || TEST: 0.8194 0.6353
EPOCH: 0399 | DATASET: dtd || TRAIN: 3.5056 0.1171 || TEST: 3.5246 0.1028
EPOCH: 0399 | DATASET: gtsrb || TRAIN: 2.3447 0.2975 || TEST: 3.5864 0.0551
EPOCH: 0399 | DATASET: omniglot || TRAIN: 6.9420 0.0035 || TEST: 7.3892 0.0014
EPOCH: 0399 | DATASET: svhn || TRAIN: 2.1091 0.2652 || TEST: 2.3105 0.1013
EPOCH: 0399 | DATASET: ucf101 || TRAIN: 4.3354 0.0334 || TEST: 4.2975 0.0337
EPOCH: 0399 | DATASET: vgg-flowers || TRAIN: 4.2414 0.0676 || TEST: 3.7590 0.0885

2. when I run python coco_results.py, there is a error:
    Traceback (most recent call last):
  File ""coco_results.py"", line 7, in <module>
    pickle_in = open(""imagenet.pickle"",""rb"")
FileNotFoundError: [Errno 2] No such file or directory: 'imagenet.pickle'

I can't find  this file 'imagenet.pickle' 
"
"Dear Shikun Liu,

First of all, thanks for sharing your code.

I was checking it out and noticed that it has this line for your final loss used by both the dwa and equal weighting schemes:

`loss = torch.mean(sum(lambda_weight[i, index] * train_loss[i] for i in range(3)))`

In this line of code, I believe the torch.mean is not doing anything and might be confusing, as the input vector will be 1x1d, i.e., the sum of lambda * train_loss is 1x1-d.

Honestly, I am not sure if dividing the total loss by 3 would make any difference, but just to clarify, your intent was to have something like:

`torch.mean(torch.Tensor([lambda_weight[i, index] * train_loss[i] for i in range(3)]))`

or 
`torch.sum(torch.Tensor([lambda_weight[i, index] * train_loss[i] for i in range(3)]))`


Thanks,

Joao

https://github.com/lorenmt/mtan/blob/cb017f0ad19b9913cab0a128200d34483e816391/im2im_pred/model_segnet_mtan.py#L321
"
"Good evening,

I would like to use your repo with my own dataset. Images are of size 256x256 pixels and I have 5 different ground truth sets of the same size. Some are binary images and some other tasks contain multiple classes. 

Could you please guide me on what I need to change to train your proposed model?

Thank you very much for your time."
"Hello - thank you so much for uploading high-quality code along with your paper.

I'm interested in reproducing Table 3, starting with ""One Task"". I ran 'model_segnet_single.py' with task=semantic, and got mIOU scores which looked a lot higher than the paper (17.82 vs 15.10). Is this expected? Did something change in the codebase since when the paper was submitted?

Thanks!

Here are the first 30 epochs of logs:
Epoch: 0000 | TRAIN: 1.9690 0.0748 0.3519 TEST: 1.7898 0.0909 0.3818
Epoch: 0001 | TRAIN: 1.7170 0.1067 0.4119 TEST: 1.7185 0.1073 0.4083
Epoch: 0002 | TRAIN: 1.6698 0.1123 0.4205 TEST: 1.6828 0.1334 0.4168
Epoch: 0003 | TRAIN: 1.6397 0.1254 0.4343 TEST: 1.6710 0.1163 0.4221
Epoch: 0004 | TRAIN: 1.6036 0.1329 0.4447 TEST: 1.6538 0.1332 0.4249
Epoch: 0005 | TRAIN: 1.5880 0.1350 0.4478 TEST: 1.6328 0.1306 0.4315
Epoch: 0006 | TRAIN: 1.5570 0.1429 0.4593 TEST: 1.6215 0.1559 0.4438
Epoch: 0007 | TRAIN: 1.5254 0.1507 0.4703 TEST: 1.6558 0.1374 0.4345
Epoch: 0008 | TRAIN: 1.5026 0.1545 0.4773 TEST: 1.5411 0.1455 0.4665
Epoch: 0009 | TRAIN: 1.4802 0.1562 0.4838 TEST: 1.5541 0.1525 0.4637
Epoch: 0010 | TRAIN: 1.4455 0.1616 0.4960 TEST: 1.5362 0.1441 0.4690
Epoch: 0011 | TRAIN: 1.4185 0.1616 0.5024 TEST: 1.5183 0.1539 0.4725
Epoch: 0012 | TRAIN: 1.3854 0.1654 0.5138 TEST: 1.5059 0.1523 0.4774
Epoch: 0013 | TRAIN: 1.3608 0.1668 0.5205 TEST: 1.4475 0.1524 0.4972
Epoch: 0014 | TRAIN: 1.3269 0.1698 0.5317 TEST: 1.4648 0.1649 0.5012
Epoch: 0015 | TRAIN: 1.2967 0.1731 0.5422 TEST: 1.4877 0.1551 0.4949
Epoch: 0016 | TRAIN: 1.2693 0.1778 0.5522 TEST: 1.4616 0.1596 0.4787
Epoch: 0017 | TRAIN: 1.2284 0.1843 0.5667 TEST: 1.4990 0.1772 0.5040
Epoch: 0018 | TRAIN: 1.1794 0.1926 0.5853 TEST: 1.4896 0.1568 0.4949
Epoch: 0019 | TRAIN: 1.1606 0.1974 0.5917 TEST: 1.4416 0.1698 0.5186
Epoch: 0020 | TRAIN: 1.0995 0.2084 0.6145 TEST: 1.4340 0.1686 0.5167
Epoch: 0021 | TRAIN: 1.0474 0.2168 0.6341 TEST: 1.4271 0.1649 0.5252
Epoch: 0022 | TRAIN: 1.0047 0.2268 0.6466 TEST: 1.4728 0.1714 0.5108
Epoch: 0023 | TRAIN: 0.9398 0.2376 0.6708 TEST: 1.4875 0.1721 0.5255
Epoch: 0024 | TRAIN: 0.8777 0.2489 0.6930 TEST: 1.5140 0.1708 0.5050
Epoch: 0025 | TRAIN: 0.8138 0.2587 0.7159 TEST: 1.5678 0.1716 0.5222
Epoch: 0026 | TRAIN: 0.7687 0.2687 0.7326 TEST: 1.5570 0.1745 0.5309
Epoch: 0027 | TRAIN: 0.6881 0.2884 0.7633 TEST: 1.5163 0.1762 0.5360
Epoch: 0028 | TRAIN: 0.6309 0.2993 0.7832 TEST: 1.7214 0.1782 0.5246
Epoch: 0029 | TRAIN: 0.5857 0.3097 0.8008 TEST: 1.7705 0.1710 0.5105
Epoch: 0030 | TRAIN: 0.5333 0.3222 0.8184 TEST: 1.7390 0.1679 0.5236

"
"Hi,
thanks for the code! It seems that the initialization for Conv is never used in model_wrn_mtan.py:
https://github.com/lorenmt/mtan/blob/6c8e6a4c1f27633d93a8010ac734cd5d4c4d4dfe/visual_decathlon/model_wrn_mtan.py#L17
Are MTAN modules sensitive to initializations in your experience? Was there a reason you chose Xavier uniform specifically?
Best"
"I'm sorry but I can't understand the npy file, like this.
![image](https://user-images.githubusercontent.com/50009373/70965768-d3146300-20cb-11ea-8168-a5b0f051cbe9.png)
where can I get this npy file?Thank you
"
"Hello, I ran the model_segnet_single.py using the provided cityscape dataset (batch size is set to 8). The relative error is around 24 which is much better than that reported in your paper (which is around 34) . Is there any other modification about the code should be made when using cityscape rather than nyud?"
Wish you show me your file(data) structure.I can't run this program since I don't konw how to prepare the data
""
"I noticed that the number of invalid depth pixels in a cityscapes depth image you provided is more than three-quarters of the total number of pixels in the image. So that's the original number of invalid depth or did you do the extra processing? In addition, during the training, the rel-depth-error is too large. Can you explain your preprocessing about the raw depth data in cityscapes?  Is it the same as the following:

disparity = (p-1)/256,   for each pixel p with p > 0
depth = baseline * fx / disparity,  where baseline=0.20, fx=2262"
"Hi @lorenmt , thanks for releasing your code!

I am running the single task script `model_segnet_single.py` and the final result seems to be not the same as reported in your paper. Here is my training log [semantic-1022070532.log](https://github.com/lorenmt/mtan/files/3756387/semantic-1022070532.log).

I just ran the script without modifying any params setting in the code. The best is about 12%, while in the paper the best is 15%. Could you please check the params setting?

Thanks."
"Hello. Did you mask out pixels where the ground truth is missing for surface normal estimation and depth prediction as <Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture>?"
"I noticed that your used depth is range in [-1,1], and you set invalid depth value as 0 so that a normal and depth ignored mask is used in your code [](https://cs.nyu.edu/~deigen/dnl/) . But from [NYUv2 dataset homepage](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2), I can't find any mask.
I want to know why you not use the original depth ranged in [0,10], which is used by many other depth estimation methods  and how you get the depth invalid mask. "
"Hi.  
For NYUv2 dataset,  the label I read from the original dataset is more than 13 categories, maybe dozens of categories.  The pre-processed label you provides is range from -1 to 12.  I want to know how you transform the original  labels to final labels with only 13 classes or  whether you have discarded some classes?  Also, do you know the numbers of final labels, e.g. -1 ,0, 1 , represent which class respectively?
I will appreciate it if you can offer more helpful information about the NYUv2 dataset."
"Hi, @lorenmt . Thanks for your code! 
I'm following your work and have some questions. Considering you only show us your pre-processed label,  .npy  file , without any explanation, could you please provide your preprocessing code about original datasets? I want to know more details, such as data argumentation, calculation methods about normal  and  so on through your preprocessing code. "
"Hi， Thank you for sharing the good idea and code.

In  **model_wrn_mtan.py**，Error while running code：IsADirectoryError: [Errno 21] Is a directory: '**model_weights/wrn_final**'，is the usual format for saving **state_dict** not a "".pt"" or'.pth' file?"
Thank you for sharing code. I'm puzzled about training  `python model_wrn_eval.py --dataset 'notimagenet'`. I can't find the flags about this setting.
"Do you use any pretrained weights (e.g. VGG-16 on ImageNet) or you just start from scratch? The performance gap even for the baseline Cross Stitch on NYU v2 using VGG 16 is very interesting, and I am trying to find out the reason. Thanks!"
"Hi,
@lorenmt 
Thanks for releasing cityscape dataset. Now NYUV2 dataset is not compatible to cityscapes dataset since normals  data hasn't been uploaded. In this image-to-image methods, should i need to major changes in architecture(CNN N/w) since Surface Normal dataset is not available for cityscapes. 
Kindly suggest how to go ahead to solve this problem. Please help"
"Hi,

I found that there is no 'relu' activation in the One Task architecture except the last prediction layers. 
https://github.com/lorenmt/mtan/blob/master/im2im_pred/model_segnet_single.py#L22

Is it the same as the original SegNet?"
"Why is the  relative descending rate a quotient of two epoch losses, not a gradient?"
"Hi,

is there is any code update on Multi-task learning wrt cityscape dataset(Semantic segmentation + Depth/disparity) maps. Kindly help"
Have you tried to use tensorflow to complete the DWA module?
"hello,could you tell me model_segnet_dense.py corresponds which paper?"
"Hi, thank you for the nice work here. 

Can I understand how you obtained `person_keypoints_train2017_pose2seg.json` and `person_keypoints_val2017_pose2seg.json`? Was it obtained by filtering out the ""person"" class from the COCO dataset and also removing ""Small"" persons? 

Thanks!"
"dear author, i have a question ,use your model how to get  17 pose points.I found your model output no pose point parameter ,can you explain ?thank you "
"Hello, I am using alphapose to estimate key points and perform pose alignment without segmentation. I don’t know how to modify it.Do I need to change the size of my picture?"
"First of all, Impressive work @liruilong940607 .

Secondly, This repo needs to have an extra file explaining how should input look, how to feed inputs and get outputs (and maybe explaining pre-processing steps as well). I am posting this because there are many ""issues"" posted in this repo discussing and wanting to know the same (including myself)."
"Hello @liruilong940607, thank you very much for this repo. I wanted to know if it is possible to train on a smaller COCO dataset than what is originally provided in the readme.

I tried looking into the keypoints JSON file and train2017 folder of images. But I'm not sure which data to modify.

During training, the process line (snippet below) indicates there are 14150 images in dataloader. 
https://github.com/liruilong940607/Pose2Seg/blob/64fcc5e0ee7b85c32f4be2771ce810a41b9fcb38/train.py#L75-L85

The train2017 directory contains 118288 images. (I found out by using `ls -1 | wc -l` in the train2017 directory)
The person_keypoints_train2017_pose2seg.json has the 149813 items in the ""images"" field. (Using pythons json module)
The person_keypoints_train2017_pose2seg.json has the 56599 items in the ""images"" field. (Using pythons json module)

I suppose that if I am trying to reduce the number of images the training process uses, I need to reduce the 14150 indicated by the dataloader, but I'm not sure how. 

Thank you 
"
"I follow all steps but when I run ```python train.py``` I find this error ```RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 4.00 GiB total capacity; 2.57 GiB already allocated; 74.77 MiB free; 2.85 GiB reserved in total by PyTorch) (malloc at ..\c10\cuda\CUDACachingAllocator.cpp:289)
(no backtrace available)``` "
"when i dealed my private image as title I get some problem:
warnings.warn(""nn.functional.upsample is deprecated. Use nn.functional.interpolate instead."")
so, i ask can it influence pose2seg work normal?"
作者您好，在多人遮挡的图片上，用模型跑的效果很👍，但是多人的情况下，时间效率有些低（3-4人的图片150多ms），可以给点提升速度的建议吗？  用更小的backbone？ 简化segmodule ？
"In training, I notice that all InputMatirxs obtained by get_aug_matrix are all identical. It means that there is no translation/rotation/scale/etc. for any input image and pose pair. 

Actually, there is little rotation angle as the default input argument for get_aug_matrix. And it's interesting to see these little changes are ALL suppressed when it's called. Could you please kindly explain the pros and cons of doing so? As stated in other issue, the released model works not that good when we pass poor human pose estimation results. Can we improve the issue with some active data augmentation during training? Many thanks! "
"@liruilong940607 Thank you for sharing such a great work.
But I wonder why didnt you combine this network with human pose detection in order to make an end-to-end instance segmentation network ?"
"Thank you for releasing your code.

I have generated keypoints using OpenPose and converted them to Coco format (OpenPose has an extra neck keypoint that I removed). Each keypoint is a 17x3 array. The first two columns are X and Y coordinates and the last column is the confidence score ranging from 0.0 to 1.0.

Is this keypoint format suitable for running inference on the pretrained pose2seg model?"
"I read the paper and I know that the network takes input as the pose and the image.
But I was just wondering "
"I seted up the dataset as the repo refered, but when I run `python train.py`, I got the error like this.


![image](https://user-images.githubusercontent.com/33691506/65846799-97da8180-e371-11e9-9643-94ee73d66cf5.png)

what should I do to solve this?"
"Hi,!Dear Researcher，
Thank you for your excellent work!
I can't access the dataset address provided by the dataset repo. Could you download it for me or give me a link to download?

Or anyone can help me? Thanks a lot!  :)"
""
"Pose2Seg model provided by author was used to generate instance segmentation on Crowdhuman dataset with the input of keypoints generated by AlphaPose model.  According to the visualization result, the instance segmentation is not so good compared to mask rcnn model because the performance of instance segmentation of Pose2Seg  depended heavily on keypoints performance. 

The keypoints visualization:
![273275,8192f000acfb8e7b](https://user-images.githubusercontent.com/3873891/63092834-a45b7400-bf95-11e9-884b-96eca0a94044.jpg)

The instance segmentation visualization:
![273275,8192f000acfb8e7b](https://user-images.githubusercontent.com/3873891/63092851-b1786300-bf95-11e9-8d84-d3ad0f750ba7.jpg)

I was amazed when I saw the visualization of instance segmentation in COCO or COHuman dataset. But when using another model to generate keypoints, the result is not good."
"I want to run model to process my images. My data has not grand truth annotations and keypoints.
How can i process my images ? 

```
`import argparse
import numpy as np
from tqdm import tqdm

from modeling.build_model import Pose2Seg
from datasets.CocoDatasetInfo import CocoDatasetInfo, annToMask
from pycocotools import mask as maskUtils
import cv2, os
import matplotlib
import matplotlib.pyplot as plt

from skimage import data, io, filters

import os
import cv2
import json
model = Pose2Seg().cuda()
model.init('pose2seg_release.pkl')
ImageRoot = './data/coco2017/val2017'
AnnoFile = './data/coco2017/annotations/person_keypoints_val2017_pose2seg.json'
datainfos = CocoDatasetInfo(ImageRoot, AnnoFile, onlyperson=True, loadimg=True)
model.eval()
results_segm = []
imgIds = []
rawdata = datainfos[1]
img = rawdata['data']
print(rawdata['image'])
image_id = rawdata['id']
height, width = img.shape[0:2]
gt_kpts = np.float32(rawdata['gt_keypoints']).transpose(0, 2, 1)  # (N, 17, 3)
gt_segms = rawdata['segms']
gt_masks = np.array([annToMask(segm, height, width) for segm in gt_segms])
output = model([img], [gt_kpts], [gt_masks])
for mask in output[0]:
    maskencode = maskUtils.encode(np.asfortranarray(mask))
    maskencode['counts'] = maskencode['counts'].decode('ascii')
    results_segm.append({
        ""image_id"": image_id,
        ""category_id"": 1,
        ""score"": 1.0,
        ""segmentation"": maskencode
    })
imgIds.append(image_id)` 

```
model need inputs as keypoints and masks. 

output = model([img], [gt_kpts], [gt_masks])

I need to create mask and keypoints from my image and visualize them
Great work , thanks."
"Hi,
I am getting the segmentation output after running the test.py file on my in the wild images.
output -
{""image_id"": 6, ""category_id"": 1, ""score"": 1.0, ""segmentation"": {""size"": [720, 1280], ""counts"": ""obh`01^f02N2OLiYO0\\f0000kUW;""}

How are we supposed to visualize this ?"
"In the paper you said you ""use the author’s released code and configurations
from [11]"" which is **Detecton**. I am current training the same dataset on **maskrcnn-benchmark**, but I cannot get anywhere near the result in your paper (0.532 | 0.433 | 0.648)

I was wondering whether you can share some lights on me, that would be so great! 

By the way, I've visualized your results on coco and it's amazing. Great work you guys have done ! "
"Hi, 
I love the work you have done.
But I wanted to run it on my own set of images or video ?
How should I do that?"
"Hi @liruilong940607, we are trying to reproduce the results of Pose2Seg. 

For the test results without using GT keypoints (2nd row of Table 2a and 2b on your paper), i.e., using another pose estimator to predict the keypoints instead, I understand that you use Associative Embedding: https://github.com/princeton-vl/pose-ae-train to generate the predicted keypoints first. 

Are you able to share the same json file that contains the predicted keypoints from AE so we can reproduce the results? 

Thank you! "
"Your work is fascinating.

Could you share some information on the environment you used, like OS (& version), python version, and libraries' versions (especially PyTorch)?

Also, when it comes to inference, How much GPU memory is required for a single image (let's say 512x512, same as your experiments)?

Thanks"
"Hi, I just cloned the code and installed cocoAPI from GitHub. But I got wrong test results about AP(area=medium) when I ran test.py as your introductions in OCHuman dataset. The model is 'pose2seg_release.pkl' and the test results are as follows.

 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.573
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.945
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.637
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = **0.073**
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.580
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.422
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.682
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.682
[POSE2SEG]          AP|.5|.75| S| M| L|    AR|.5|.75| S| M| L|
[segm_score] OCHumanVal 0.573 0.945 0.637 -1.000 0.073 0.580 0.422 0.682 0.682 -1.000 0.550 0.682 


 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.547
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.937
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.582
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = **0.064**
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.549
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.379
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.649
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.649
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.300
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.650
[POSE2SEG]          AP|.5|.75| S| M| L|    AR|.5|.75| S| M| L|
[segm_score] OCHumanTest 0.547 0.937 0.582 -1.000 0.064 0.549 0.379 0.649 0.649 -1.000 0.300 0.650 


Why the AP(area=medium)  are extremely low?  

And I also have another question about the OCHuman dataset. I updated the dataset to the latest version, but I found the validation contains 4291 instances and the test contains 3819 instances, which is still not consistent with the description in the paper. Is there a problem with my dataset？ 


"
"How to download """" *images [667MB] & annotations"""" that belongs to ochuman.. The link you presented there is not redirecting to download, Help to download"
""
"I used pos-ae get 17 keypoints of one person. But how can I generate the coco format json file using the 17 keypoints . Like this , {""keypoints"": [176.7578125, 111.5390625, 0.9444317817687988, 182.6171875, 105.6796875, 0.9576615691184998, 170.8984375, 105.6796875, 0.9330981969833374, 192.3828125, 111.5390625, 0.9314539432525635, 166.9921875, 111.5390625, 0.6315260529518127, 0, 0, 0, 155.2734375, 140.8359375, 0.8885959982872009, 217.7734375, 177.9453125, 0.9087645411491394, 151.3671875, 179.8984375, 0.8826849460601807, 219.7265625, 207.2421875, 0.8257762789726257, 145.5078125, 215.0546875, 0.6683874130249023, 194.3359375, 218.9609375, 0.7143456935882568, 161.1328125, 217.0078125, 0.7429389357566833, 192.3828125, 285.3671875, 0.7802388072013855, 159.1796875, 279.5078125, 0.7996304035186768, 190.4296875, 330.2890625, 0.831089973449707, 159.1796875, 336.1484375, 0.8201078176498413], ""score"": 0.7800430655479431, ""image_path"": ""test1.png""}"
"      您好，非常感谢帮忙解决了我在上一个issues提出的问题，我现在还有一个疑问，在您提供的代码中怎样通过前向输出coco annotation的格式json文件，通过您提供的ochuman API 进行掩膜和骨架的显示。
     还是输出coco annotation格式的json，在另外的代码中，能方便提供吗？
     谢谢！"
"您好 ，我按下面这种方法生成json文件 然后可视化时出现错误。
代码如下：
imgIds.append(image_id)
filename = ""./seg_images/annotations/"" + ""instances_val2017.json""
f_obj = open(filename, 'w')
json.dump(results_segm, f_obj)
这样生成json文件后，json部分显示如下：
[{""category_id"": 1, ""segmentation"": {""counts"": ""hX]52V=3N1O1O1O100O2N1O1O1O1O1iNC^E?_:I[E9d:IVE<i:GQE=n:EmD?S;l001N3N3N3L2N2OO10O1001O1l0UO<D7H3MO0O2N2N1O101N3M4L5K7I5K5J7J4L4K7VOgDeN86W;f0S1DQmV2"", ""size"": [426, 640]}, ""image_id"": 139, ""score"": 1.0}, {""category_id"": 1, ""segmentation"": {""counts"": ""cQf31U=5K5L3N2N2O1N2O0O2OO10O10000O010000O010O010O010O01O00100O0010O010O10O10O010O0100O010O10OO2O001O1O010O1O1N1O2O1O001M3M2O2O1L3K6L4M3N1N3iNSNoFP2n8[NgFg1W9bNaF`1Z9Z1L4N101O1eIdLh2\3SMmLh2V3SMQMi2P3TMUMi2l2VMVMh2k2VMYMh2g2VM]Mg2d2VMbMg2^2XMfMe2[2XMiMe2X2ZMlMc2U2

但是我看您生成的 ochuman.json文件内容显示 不是这样的 ，我不知道 哪里出了问题。
我现在想生成json文件，然后可视化。
期待您的回复，谢谢！"
""
"您好
我运行了你的模型在ochuamn上  给了一些数  
请问 怎么将结果可视化出来呢 好像在你们的论文里图片那样"
"HI man, 
I installed pose2seg and it runs perfectly on your OCHuman, 
How can I test it on a single image using your trained model?

BTW, I cannot open the link of visualize_cluster.ipynb .
Thanks."
""
"你好，读完论文有几个问题，望解答
1） 4.1节说image + pose作为input，那么，
1.1）pose是怎么表达的？是heatmap吗，还是如Figure4,一样，画在原始图像上？
1.2)  “+” 的操作是什么意思？是concat，将heatmap（若是）和image按通道组合起来吗？

2）pose获取的问题：我在另外的issue里读到，你们用Associative embedding: End-to-end learning for joint detection and grouping方法提取pose，那么你们是否有用过MSRA的Simple Baselines for Human Pose Estimation and Tracking呢？bottom-up与top-down的结果对于你们这里的做segment的效果有影响吗？

3）既然skeleton feature是joint heatmap 与 PAF 叠在一起，那为什么不直接在原图上跑openpose的方法，然后可以直接affine-align operation把skeleton feature map给转过来，这样就省去了一次预测pose的过程。因为现在的pipeline实际上预测了2次pose（input一次，skeleton feature一次），如果我理解的没错，那么原文的做法是否有点冗余？

4）整个系统（Figure4所描述的）是end-to-end训练的吗？


"
"Hi, thanks for your contribution!
I have a question that why should we align all the keypoints? For ROI align, the features must be input to fully connected layer so the size of them should be the same. But in the segmentation task, there is no fc layer and I cannot figure out the function of affine-align operation.
"
最近在follow 你的工作，你们的输入是单张图片然后估计skeleton 在进行affine，还是输入是两个，一个是图片另外一个是已经存在的skeleton？ 我对论文又些许不明白
Is there a way to visualize masks (as seen in the paper?) once the training is done?
"hollo, thanks for your job, is there a available pkl model to test the myself picture? 
"
"hii, thank for your sharing, 
Many of the strategies for optimizing result  of the transformation estimation in the image problem.
the algorithm suitable for find SIM3 matrix from two group 3D points pair in coordinate transforamton problem?
"
""
"Hello,everyone,Why the project I downloaded is missing some header files????????????  Can someone post me? like this:

-- The CXX compiler identification is GNU 7.5.0
-- Check for working CXX compiler: /usr/local/bin/c++
-- Check for working CXX compiler: /usr/local/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
CMake Warning (dev) at CMakeLists.txt:9 (option):
  Policy CMP0077 is not set: option() honors normal variables.  Run ""cmake
  --help-policy CMP0077"" for policy details.  Use the cmake_policy command to
  set the policy and suppress this warning.

  For compatibility with older versions of CMake, option is clearing the
  normal variable 'CREATE_SAMPLE_PROJECT'.
This warning is for project developers.  Use -Wno-dev to suppress it.

-- Found OpenCV: /opt/ros/kinetic (found version ""3.3.1"") 
Found Gflags 2.2.2
-- > GFLAGS_INCLUDE_DIR:   /usr/local/include
-- > GFLAGS_LIBRARIES:   gflags_shared
Glog library found.
-- Found OpenMP_CXX: -fopenmp (found version ""4.5"") 
-- Found OpenMP: TRUE (found version ""4.5"")  
-- The C compiler identification is GNU 7.5.0
-- Check for working C compiler: /usr/local/bin/gcc
-- Check for working C compiler: /usr/local/bin/gcc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Found PythonInterp: /usr/bin/python3.5 (found version ""3.5.1"") 
-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.5m.so
-- pybind11 v2.1.1
-- Performing Test HAS_CPP14_FLAG
-- Performing Test HAS_CPP14_FLAG - Success
-- Performing Test HAS_CPP11_FLAG
-- Performing Test HAS_CPP11_FLAG - Success
-- Performing Test HAS_FLTO
-- Performing Test HAS_FLTO - Success
-- LTO enabled
-- Configuring done
-- Generating done
-- Build files have been written to: /home/jjb/envirement/magsac/build
jjb@jjb:~/envirement/magsac/build$ make
Scanning dependencies of target GraphCutRANSAC
[  5%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/GCoptimization.cpp.o
[ 10%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/LinkedBlockList.cpp.o
[ 15%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/gamma_values.cpp.o
/home/jjb/envirement/magsac/graph-cut-ransac/src/pygcransac/include/gamma_values.cpp:1:9: 警告：#pragma once 出现在主文件中
 #pragma once
         ^~~~
[ 21%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/graph.cpp.o
[ 26%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/maxflow.cpp.o
[ 31%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/relative_pose/bundle.cpp.o
[ 36%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/relative_pose/colmap_models.cpp.o
[ 42%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/relative_pose/essential.cpp.o
[ 47%] Linking CXX static library libGraphCutRANSAC.a
[ 47%] Built target GraphCutRANSAC
Scanning dependencies of target pymagsac
[ 52%] Building CXX object CMakeFiles/pymagsac.dir/src/pymagsac/src/bindings.cpp.o
[ 57%] Building CXX object CMakeFiles/pymagsac.dir/src/pymagsac/src/magsac_python.cpp.o
**/home/jjb/envirement/magsac/src/pymagsac/src/magsac_python.cpp:3:10: 致命错误：fundamental_estimator.h：没有那个文件或目录
 #include ""fundamental_estimator.h""**
"
"I tried building it for windows10 using VC-2015. But ran in to issues because of gflags.
It would be great if author can release windows package or even package installable via pip .
"
"Hi,

When I'm running the example code in example_essential_matrix.ipynb i receive the following error 

![image](https://user-images.githubusercontent.com/42467164/116602743-69959d00-a92c-11eb-8102-3137e6217d92.png)

I have experimented a bit with trying to find the correct arguments, but with no luck so I am posting it here. 

Thanks in advance for any help!"
"Hello. Whenever I run `pymagsac.findHomography' with `use_magsac_plus_plus=True` I always get really verbose prints:

```
Setting the core number for MAGSAC++ is deprecated.
Setting the partition number for MAGSAC++ is deprecated.
```

Is there a way to suppress this? I believe the prints are coming from [here](https://github.com/danini/magsac/blob/master/src/pymagsac/include/magsac.h#L100), but it's been hard-coded [here](https://github.com/danini/magsac/blob/master/src/pymagsac/src/magsac_python.cpp#L33).

Perhaps the following lines (L33-34) can be deleted if `use_magsac_plus_plus` is false?

```
    magsac->setCoreNumber(1); // The number of cores used to speed up sigma-consensus
    magsac->setPartitionNumber(partition_num); // The number partitions used for speeding up sigma consensus. As the value grows, the algorithm become slower and, usually, more accurate.
```

I'd be happy to create a PR if you think it'd be useful."
"Hi, @danini and @ducha-aiki , 

Thanks for sharing your great work! 

Unfortunatelly, issues raised when I compile it on linux machine with gcc 5.2.0. I am stuck and any advice would be appreciated!  

Thanks! 

Appended is the error: 

`$ make -j4`

[ 10%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/GCoptimization.cpp.o
[ 20%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/graph.cpp.o
[ 30%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/LinkedBlockList.cpp.o
[ 40%] Building CXX object CMakeFiles/MAGSAC.dir/include/gamma_values.cpp.o
[ 50%] Building CXX object CMakeFiles/MAGSAC.dir/include/magsac.cpp.o
[ 60%] Building CXX object CMakeFiles/MAGSAC.dir/include/model_score.cpp.o
[ 70%] Building CXX object CMakeFiles/GraphCutRANSAC.dir/graph-cut-ransac/src/pygcransac/include/maxflow.cpp.o
[ 80%] Building CXX object CMakeFiles/MAGSAC.dir/src/main.cpp.o
[ 90%] Linking CXX static library libGraphCutRANSAC.a
[ 90%] Built target GraphCutRANSAC
In file included from /home/Code/magsac/include/magsac.cpp:1:0:
/home/Code/magsac/include/magsac.h: In member function ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensusPlusPlus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&)’:
/home/Code/magsac/include/magsac.h:743:20: error: expected unqualified-id before ‘[’ token
   for (const auto &[residual, idx] : residuals)
                    ^
/home/Code/magsac/include/magsac.h:743:20: error: expected ‘;’ before ‘[’ token
/home/Code/magsac/include/magsac.h:743:21: error: ‘residual’ was not declared in this scope
   for (const auto &[residual, idx] : residuals)
                     ^
/home/Code/magsac/include/magsac.h:743:31: error: ‘idx’ was not declared in this scope
   for (const auto &[residual, idx] : residuals)
                               ^
/home/Code/magsac/include/magsac.h: In lambda function:
/home/Code/magsac/include/magsac.h:743:36: error: expected ‘{’ before ‘:’ token
   for (const auto &[residual, idx] : residuals)
                                    ^
/home/Code/magsac/include/magsac.h: In member function ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensusPlusPlus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&)’:
/home/Code/magsac/include/magsac.h:743:36: error: expected ‘;’ before ‘:’ token
/home/Code/magsac/include/magsac.h:743:36: error: expected primary-expression before ‘:’ token
/home/Code/magsac/include/magsac.h:743:36: error: expected ‘)’ before ‘:’ token
/home/Code/magsac/include/magsac.h:743:36: error: expected primary-expression before ‘:’ token
make[2]: *** [CMakeFiles/MAGSAC.dir/include/magsac.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/Code/magsac/graph-cut-ransac/src/pygcransac/include/fundamental_estimator.h:52:0,
                 from /home/Code/magsac/graph-cut-ransac/src/pygcransac/include/types.h:39,
                 from /home/Code/magsac/graph-cut-ransac/src/pygcransac/include/utils.h:48,
                 from /home/Code/magsac/src/main.cpp:16:
/home/Code/magsac/graph-cut-ransac/src/pygcransac/include/GCRANSAC.h: In member function ‘void gcransac::GCRANSAC<_ModelEstimator, _NeighborhoodGraph, _ScoringFunction, _PreemptiveModelVerification>::run(const cv::Mat&, const _ModelEstimator&, gcransac::sampler::Sampler<cv::Mat, long unsigned int>*, gcransac::sampler::Sampler<cv::Mat, long unsigned int>*, const _NeighborhoodGraph*, gcransac::Model&, _PreemptiveModelVerification&)’:
/home/Code/magsac/graph-cut-ransac/src/pygcransac/include/GCRANSAC.h:305:8: error: expected ‘(’ before ‘constexpr’
     if constexpr (!std::is_same<preemption::EmptyPreemptiveVerfication<_ModelEstimator>, _PreemptiveModelVerification>())
        ^
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
 }
 ^
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
/home/Code/magsac/src/main.cpp: At global scope:
/home/Code/magsac/src/main.cpp:1181:1: error: expected ‘}’ at end of input
make[2]: *** [CMakeFiles/MAGSAC.dir/src/main.cpp.o] Error 1
make[1]: *** [CMakeFiles/MAGSAC.dir/all] Error 2
make: *** [all] Error 2"
"Hi,

I have been studying your paper, thanks for uploading your code with it !

I was wondering, when you apply SigmaConsensus, in the paper you use the formula (6) to compute weights, but in the code you only use the exponential part of this formula. Why is the factor \sigma^{-\rho} D(\theta, \sigma)^{\rho - 1} ignored in the computation of the weights ?

Best regards,
Clément Riu."
"Thanks a lot for your great work and code, do you have a plan to add support for rigid transformation estimation? 

Best,
Xuyang"
"Is there a way to estimate Essential Matrix using DefaultEssentialMatrixEstimator() for cameras which don't have camera intrinsic matrices (for eg, 360 degree cameras)"
"Hi,
your paper is very interesting, thanks for sharing the code!

Just out of curiosity. In some of my old experiments (see ""[noRANSAC for fundamental matrix estimation](http://www.bmva.org/bmvc/2011/proceedings/paper98/paper98.pdf)"", Table 5), I found that the geometric epipolar error (i.e. sqrt(e12^2+e21^2)) is usually more robust than the symmetric epipolar error. Did you also try some kind of similar error with MAGSAC?

Thanks again,
Fabio "
"Firstly, thanks for your contribution!

When I run your demo example, I met such an issue, could you please help me fix it?

![image](https://user-images.githubusercontent.com/22257640/123928674-b5160700-d9c0-11eb-904d-3bda0f43ba5e.png)

My opencv version is 3.4.6"
"Hi, @danini and @ducha-aiki ,

Thanks for sharing your great work!

Unfortunatelly, issues raised when I run the example_homography.I never modify the content of this file.

Appended is the error:
TypeError: findHomography(): incompatible function arguments. The following argument types are supported:
    1. (x1y1: numpy.ndarray[float64], x2y2: numpy.ndarray[float64], w1: float, h1: float, w2: float, h2: float, use_magsac_plus_plus: bool=True, sigma_th: float=1.0, conf: float=0.99, max_iters: int=1000, partition_num: int=5) -> tuple"
"Hello Daniel, 

I saw your talk at EECVC and I tried to clone your C++ MAGSAC project and get it to run on Ubuntu 18.04.  I was unable to do so w/o some alterations, some notes:

1- it is not obvious from the README that one has to clone and then copy the `src` folder from your `graph-cut-ransac` repo to the folder in `magsac/graph-cut-ransac` (which is empty when cloning magsac).  By reading through the CMakeLists.txt file I figured it out and was able to configure/generate with Cmake and then build.

2- I set the CREATE_SAMPLE_PROJECT var as follows `set(CREATE_SAMPLE_PROJECT` ON)` and Cmake generation failed:

````
CMake Error at CMakeLists.txt:105 (target_link_libraries):
  Target ""MAGSAC"" of type EXECUTABLE may not be linked into another target.
  One may link only to INTERFACE, OBJECT, STATIC or SHARED libraries, or to
  executables with the ENABLE_EXPORTS property set.
````

I changed the `target_link_libraries()` section in `CREATE_SAMPLE_PROJECT` and it generated, built, and I can run it now:

````
target_link_libraries(SampleProject 
		${OpenCV_LIBS}
		Eigen3::Eigen
	)
````

A question, I see that you are linking with OpenMP, but I didn't see anywhere in the paper where you mention parallelization or implementation details (but I only did a high-level read).  Is the OpenMP for speeding up the OpenCV parts of the code, or do you use parallelization currently in the MAGSAC++ algo? "
"Hi @danini,
I am trying to aplicate the magsac algorithm with my work, in a part of my work I need to estimate the fundamental matrix, but in my case I need points in 3d, because I am working with 360 images in spherical domain(I project the image points to a sphere) and my z-point is not one, I notice that in the magsac the algorithm for fundamental matrix load only 2d points. It would be great to extend to 3d points. I am trying to do that.

Or Do you have a version with 3d points for the fundamental matrix?

Greetings and thank you,
Jeffri"
"hi @danini 

CMAKE( it look fine) 
-- The CXX compiler identification is GNU 9.3.0
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found OpenCV: /home/artcs/anaconda3/envs/pymagsac (found version ""3.4.2"") 
-- Found OpenMP_CXX: -fopenmp (found version ""4.5"") 
-- Found OpenMP: TRUE (found version ""4.5"")  
-- Configuring done
-- Generating done
-- Build files have been written to: /home/artcs/Downloads/magsac-master/build

but when I run make it shows:

[ 10%] Building CXX object CMakeFiles/MAGSAC.dir/include/gamma_values.cpp.o
[ 20%] Building CXX object CMakeFiles/MAGSAC.dir/include/magsac.cpp.o
In file included from /home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/GCoptimization.h:109,
                 from /home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/estimator.h:37,
                 from /home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/model.h:39,
                 from /home/artcs/Downloads/magsac-master/include/magsac.h:6,
                 from /home/artcs/Downloads/magsac-master/include/magsac.cpp:1:
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h: In member function ‘void Energy<captype, tcaptype, flowtype>::add_term3(Energy<captype, tcaptype, flowtype>::Var, Energy<captype, tcaptype, flowtype>::Var, Energy<captype, tcaptype, flowtype>::Var, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value)’:
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h:262:17: warning: ISO C++17 does not allow ‘register’ storage class specifier [-Wregister]
  262 |  register Value pi = (E000 + E011 + E101 + E110) - (E100 + E010 + E001 + E111);
      |                 ^~
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h:263:17: warning: ISO C++17 does not allow ‘register’ storage class specifier [-Wregister]
  263 |  register Value delta;
      |                 ^~~~~
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h:264:15: warning: ISO C++17 does not allow ‘register’ storage class specifier [-Wregister]
  264 |  register Var u;
      |               ^
[ 30%] Building CXX object CMakeFiles/MAGSAC.dir/include/model_score.cpp.o
[ 40%] Building CXX object CMakeFiles/MAGSAC.dir/src/main.cpp.o
In file included from /home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/GCoptimization.h:109,
                 from /home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/estimator.h:37,
                 from /home/artcs/Downloads/magsac-master/include/magsac_utils.h:8,
                 from /home/artcs/Downloads/magsac-master/src/main.cpp:15:
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h: In member function ‘void Energy<captype, tcaptype, flowtype>::add_term3(Energy<captype, tcaptype, flowtype>::Var, Energy<captype, tcaptype, flowtype>::Var, Energy<captype, tcaptype, flowtype>::Var, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value, Energy<captype, tcaptype, flowtype>::Value)’:
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h:262:17: warning: ISO C++17 does not allow ‘register’ storage class specifier [-Wregister]
  262 |  register Value pi = (E000 + E011 + E101 + E110) - (E100 + E010 + E001 + E111);
      |                 ^~
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h:263:17: warning: ISO C++17 does not allow ‘register’ storage class specifier [-Wregister]
  263 |  register Value delta;
      |                 ^~~~~
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/energy.h:264:15: warning: ISO C++17 does not allow ‘register’ storage class specifier [-Wregister]
  264 |  register Var u;
      |               ^
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:25:
/home/artcs/Downloads/magsac-master/include/estimators.h: At global scope:
/home/artcs/Downloads/magsac-master/include/estimators.h:83:22: error: ‘Model’ has not been declared
   83 |    bool isValidModel(Model& model_,
      |                      ^~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:136:30: error: ‘Model’ has not been declared
  136 |    inline bool applyDegensac(Model& model_, // The input model to be tested
      |                              ^~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h: In member function ‘double magsac::estimator::FundamentalMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::residualForScoring(const cv::Mat&, const gcransac::Model&) const’:
/home/artcs/Downloads/magsac-master/include/estimators.h:38:22: warning: there are no arguments to ‘squaredSymmetricEpipolarDistance’ that depend on a template parameter, so a declaration of ‘squaredSymmetricEpipolarDistance’ must be available [-fpermissive]
   38 |     return std::sqrt(squaredSymmetricEpipolarDistance(point_, model_.descriptor));
      |                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h: In member function ‘bool magsac::estimator::FundamentalMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::isValidModel(int&, const cv::Mat&, const std::vector<long unsigned int>&, const size_t*, double, bool&) const’:
/home/artcs/Downloads/magsac-master/include/estimators.h:96:48: error: request for member ‘descriptor’ in ‘model_’, which is of non-class type ‘int’
   96 |     const Eigen::Matrix3d &descriptor = model_.descriptor; // The decriptor of the current model
      |                                                ^~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:97:36: warning: there are no arguments to ‘sampleSize’ that depend on a template parameter, so a declaration of ‘sampleSize’ must be available [-fpermissive]
   97 |     constexpr size_t sample_size = sampleSize(); // Size of a minimal sample
      |                                    ^~~~~~~~~~
In file included from /home/artcs/anaconda3/envs/pymagsac/include/opencv2/core.hpp:52,
                 from /home/artcs/Downloads/magsac-master/src/main.cpp:9:
/home/artcs/Downloads/magsac-master/include/estimators.h:100:41: error: ‘minimum_inlier_ratio_in_validity_check’ was not declared in this scope
  100 |      MAX(sample_size, inliers_.size() * minimum_inlier_ratio_in_validity_check);
      |                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:25:
/home/artcs/Downloads/magsac-master/include/estimators.h:108:10: warning: there are no arguments to ‘squaredSymmetricEpipolarDistance’ that depend on a template parameter, so a declaration of ‘squaredSymmetricEpipolarDistance’ must be available [-fpermissive]
  108 |      if (squaredSymmetricEpipolarDistance(data_.row(idx), descriptor) < squared_threshold)
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:122:9: error: ‘use_degensac’ was not declared in this scope
  122 |     if (use_degensac)
      |         ^~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h: In member function ‘bool magsac::estimator::FundamentalMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::applyDegensac(int&, const cv::Mat&, const std::vector<long unsigned int>&, const size_t*, double, bool&) const’:
/home/artcs/Downloads/magsac-master/include/estimators.h:158:13: error: request for member ‘descriptor’ in ‘model_’, which is of non-class type ‘int’
  158 |      model_.descriptor.block<3, 3>(0, 0);
      |             ^~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:176:30: error: request for member ‘descriptor’ in ‘model_’, which is of non-class type ‘int’
  176 |      epipolar_cross * model_.descriptor;
      |                              ^~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:235:29: warning: there are no arguments to ‘sampleSize’ that depend on a template parameter, so a declaration of ‘sampleSize’ must be available [-fpermissive]
  235 |      for (size_t i = 0; i < sampleSize(); ++i)
      |                             ^~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:269:30: error: ‘squared_homography_threshold’ was not declared in this scope
  269 |       if (squared_residual < squared_homography_threshold)
      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:289:38: error: ‘HomographyEstimator’ in namespace ‘magsac::estimator’ does not name a template type
  289 |      static const magsac::estimator::HomographyEstimator<
      |                                      ^~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:301:11: error: ‘homography_estimator’ was not declared in this scope
  301 |       if (homography_estimator.squaredResidual(data_.row(inlier_idx), best_homography) < squared_homography_threshold)
      |           ^~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:301:90: error: ‘squared_homography_threshold’ was not declared in this scope
  301 |  (homography_estimator.squaredResidual(data_.row(inlier_idx), best_homography) < squared_homography_threshold)
      |                                                                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~

/home/artcs/Downloads/magsac-master/include/estimators.h:305:38: error: ‘homography_estimator’ was not declared in this scope
  305 |      if (homography_inliers.size() < homography_estimator.nonMinimalSampleSize())
      |                                      ^~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:310:18: error: ‘Model’ was not declared in this scope; did you mean ‘gcransac::Model’?
  310 |      std::vector<Model> homographies;
      |                  ^~~~~
      |                  gcransac::Model
In file included from /home/artcs/Downloads/magsac-master/include/magsac_utils.h:9,
                 from /home/artcs/Downloads/magsac-master/src/main.cpp:15:
/home/artcs/Downloads/magsac-master/graph-cut-ransac/src/pygcransac/include/model.h:43:8: note: ‘gcransac::Model’ declared here
   43 |  class Model
      |        ^~~~~
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:25:
/home/artcs/Downloads/magsac-master/include/estimators.h:310:23: error: template argument 1 is invalid
  310 |      std::vector<Model> homographies;
      |                       ^
/home/artcs/Downloads/magsac-master/include/estimators.h:310:23: error: template argument 2 is invalid
/home/artcs/Downloads/magsac-master/include/estimators.h:313:6: error: ‘homography_estimator’ was not declared in this scope
  313 |      homography_estimator.estimateModelNonminimal(data_, // All data points
      |      ^~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:320:23: error: request for member ‘size’ in ‘homographies’, which is of non-class type ‘int’
  320 |      if (homographies.size() != 1)
      |                       ^~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:325:21: error: invalid types ‘int[int]’ for array subscript
  325 |       homographies[0].descriptor;
      |                     ^
/home/artcs/Downloads/magsac-master/include/estimators.h:335:11: error: expected ‘;’ before ‘model’
  335 |      Model model;
      |           ^~~~~~
      |           ;
/home/artcs/Downloads/magsac-master/include/estimators.h:352:7: error: ‘model’ was not declared in this scope; did you mean ‘modfl’?
  352 |       model, // The estimated model
      |       ^~~~~
      |       modfl
/home/artcs/Downloads/magsac-master/include/estimators.h: In member function ‘double magsac::estimator::EssentialMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::residualForScoring(const cv::Mat&, const gcransac::Model&) const’:
/home/artcs/Downloads/magsac-master/include/estimators.h:396:12: warning: there are no arguments to ‘squaredSymmetricEpipolarDistance’ that depend on a template parameter, so a declaration of ‘squaredSymmetricEpipolarDistance’ must be available [-fpermissive]
  396 |     return squaredSymmetricEpipolarDistance(point_, model_.descriptor);
      |            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h: In member function ‘double magsac::estimator::HomographyEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::residualForScoring(const cv::Mat&, const gcransac::Model&) const’:
/home/artcs/Downloads/magsac-master/include/estimators.h:454:12: warning: there are no arguments to ‘residual’ that depend on a template parameter, so a declaration of ‘residual’ must be available [-fpermissive]
  454 |     return residual(point_, model_.descriptor);
      |            ^~~~~~~~
/home/artcs/Downloads/magsac-master/src/main.cpp: In function ‘void testEssentialMatrixFitting(double, double, const string&, bool, double)’:
/home/artcs/Downloads/magsac-master/src/main.cpp:430:48: warning: format ‘%d’ expects argument of type ‘int’, but argument 3 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]
  430 |  printf(""\tNumber of points closer than %f is %d\n"",
      |                                               ~^
      |                                                |
      |                                                int
      |                                               %ld
  431 |   drawing_threshold_, inlier_number);
      |                       ~~~~~~~~~~~~~             
      |                       |
      |                       size_t {aka long unsigned int}
/home/artcs/Downloads/magsac-master/src/main.cpp: In function ‘void opencvEssentialMatrixFitting(double, double, const string&, bool)’:
/home/artcs/Downloads/magsac-master/src/main.cpp:1160:47: warning: format ‘%d’ expects argument of type ‘int’, but argument 3 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]
 1160 |  printf(""\tNumber of points closer than %f = %d\n"",
      |                                              ~^
      |                                               |
      |                                               int
      |                                              %ld
 1161 |   threshold_, inlier_number);
      |               ~~~~~~~~~~~~~                    
      |               |
      |               size_t {aka long unsigned int}
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:17:
/home/artcs/Downloads/magsac-master/include/magsac.h: In instantiation of ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::FundamentalMatrixEstimator<gcransac::estimator::solver::FundamentalMatrixSevenPointSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’:
/home/artcs/Downloads/magsac-master/include/magsac.h:266:15:   required from ‘bool MAGSAC<DatumType, ModelEstimator>::run(const cv::Mat&, double, ModelEstimator&, gcransac::sampler::Sampler<cv::Mat, long unsigned int>&, gcransac::Model&, int&, ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::FundamentalMatrixEstimator<gcransac::estimator::solver::FundamentalMatrixSevenPointSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’
/home/artcs/Downloads/magsac-master/src/main.cpp:539:8:   required from here
/home/artcs/Downloads/magsac-master/include/magsac.h:537:31: error: cannot convert ‘__gnu_cxx::__alloc_traits<std::allocator<gcransac::Model>, gcransac::Model>::value_type’ {aka ‘gcransac::Model’} to ‘int&’
  537 |  if (sigma_models.size() == 1 && // If only a single model is estimated
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:25:
/home/artcs/Downloads/magsac-master/include/estimators.h:83:29: note:   initializing argument 1 of ‘bool magsac::estimator::FundamentalMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::isValidModel(int&, const cv::Mat&, const std::vector<long unsigned int>&, const size_t*, double, bool&) const [with _MinimalSolverEngine = gcransac::estimator::solver::FundamentalMatrixSevenPointSolver; _NonMinimalSolverEngine = gcransac::estimator::solver::FundamentalMatrixEightPointSolver; size_t = long unsigned int]’
   83 |    bool isValidModel(Model& model_,
      |                      ~~~~~~~^~~~~~
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:17:
/home/artcs/Downloads/magsac-master/include/magsac.h: In instantiation of ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensusPlusPlus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::FundamentalMatrixEstimator<gcransac::estimator::solver::FundamentalMatrixSevenPointSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’:
/home/artcs/Downloads/magsac-master/include/magsac.h:273:15:   required from ‘bool MAGSAC<DatumType, ModelEstimator>::run(const cv::Mat&, double, ModelEstimator&, gcransac::sampler::Sampler<cv::Mat, long unsigned int>&, gcransac::Model&, int&, ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::FundamentalMatrixEstimator<gcransac::estimator::solver::FundamentalMatrixSevenPointSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’
/home/artcs/Downloads/magsac-master/src/main.cpp:539:8:   required from here
/home/artcs/Downloads/magsac-master/include/magsac.h:804:27: error: cannot convert ‘gcransac::Model’ to ‘int&’
  804 |   estimator_.isValidModel(polished_model,
      |                           ^~~~~~~~~~~~~~
      |                           |
      |                           gcransac::Model
In file included from /home/artcs/Downloads/magsac-master/src/main.cpp:25:
/home/artcs/Downloads/magsac-master/include/estimators.h:83:29: note:   initializing argument 1 of ‘bool magsac::estimator::FundamentalMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::isValidModel(int&, const cv::Mat&, const std::vector<long unsigned int>&, const size_t*, double, bool&) const [with _MinimalSolverEngine = gcransac::estimator::solver::FundamentalMatrixSevenPointSolver; _NonMinimalSolverEngine = gcransac::estimator::solver::FundamentalMatrixEightPointSolver; size_t = long unsigned int]’
   83 |    bool isValidModel(Model& model_,
      |                      ~~~~~~~^~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h: In instantiation of ‘double magsac::estimator::EssentialMatrixEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::residualForScoring(const cv::Mat&, const gcransac::Model&) const [with _MinimalSolverEngine = gcransac::estimator::solver::EssentialMatrixFivePointSteweniusSolver; _NonMinimalSolverEngine = gcransac::estimator::solver::FundamentalMatrixEightPointSolver]’:
/home/artcs/Downloads/magsac-master/include/magsac.h:951:16:   required from ‘void MAGSAC<DatumType, ModelEstimator>::getModelQuality(const cv::Mat&, const gcransac::Model&, const ModelEstimator&, double&, double&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::EssentialMatrixEstimator<gcransac::estimator::solver::EssentialMatrixFivePointSteweniusSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’
/home/artcs/Downloads/magsac-master/include/magsac.h:550:3:   required from ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::EssentialMatrixEstimator<gcransac::estimator::solver::EssentialMatrixFivePointSteweniusSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’
/home/artcs/Downloads/magsac-master/include/magsac.h:266:15:   required from ‘bool MAGSAC<DatumType, ModelEstimator>::run(const cv::Mat&, double, ModelEstimator&, gcransac::sampler::Sampler<cv::Mat, long unsigned int>&, gcransac::Model&, int&, ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::EssentialMatrixEstimator<gcransac::estimator::solver::EssentialMatrixFivePointSteweniusSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>]’
/home/artcs/Downloads/magsac-master/src/main.cpp:401:8:   required from here
/home/artcs/Downloads/magsac-master/include/estimators.h:396:44: warning: ‘squaredSymmetricEpipolarDistance’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
  396 |     return squaredSymmetricEpipolarDistance(point_, model_.descriptor);
      |            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:396:44: note: declarations in dependent base ‘gcransac::estimator::EssentialMatrixEstimator<gcransac::estimator::solver::EssentialMatrixFivePointSteweniusSolver, gcransac::estimator::solver::FundamentalMatrixEightPointSolver>’ are not found by unqualified lookup
/home/artcs/Downloads/magsac-master/include/estimators.h:396:44: note: use ‘this->squaredSymmetricEpipolarDistance’ instead
/home/artcs/Downloads/magsac-master/include/estimators.h: In instantiation of ‘double magsac::estimator::HomographyEstimator<_MinimalSolverEngine, _NonMinimalSolverEngine>::residualForScoring(const cv::Mat&, const gcransac::Model&) const [with _MinimalSolverEngine = gcransac::estimator::solver::HomographyFourPointSolver; _NonMinimalSolverEngine = gcransac::estimator::solver::HomographyFourPointSolver]’:
/home/artcs/Downloads/magsac-master/include/magsac.h:951:16:   required from ‘void MAGSAC<DatumType, ModelEstimator>::getModelQuality(const cv::Mat&, const gcransac::Model&, const ModelEstimator&, double&, double&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::HomographyEstimator<gcransac::estimator::solver::HomographyFourPointSolver, gcransac::estimator::solver::HomographyFourPointSolver>]’
/home/artcs/Downloads/magsac-master/include/magsac.h:550:3:   required from ‘bool MAGSAC<DatumType, ModelEstimator>::sigmaConsensus(const cv::Mat&, const gcransac::Model&, gcransac::Model&, ModelScore&, const ModelEstimator&, const ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::HomographyEstimator<gcransac::estimator::solver::HomographyFourPointSolver, gcransac::estimator::solver::HomographyFourPointSolver>]’
/home/artcs/Downloads/magsac-master/include/magsac.h:266:15:   required from ‘bool MAGSAC<DatumType, ModelEstimator>::run(const cv::Mat&, double, ModelEstimator&, gcransac::sampler::Sampler<cv::Mat, long unsigned int>&, gcransac::Model&, int&, ModelScore&) [with DatumType = cv::Mat; ModelEstimator = magsac::estimator::HomographyEstimator<gcransac::estimator::solver::HomographyFourPointSolver, gcransac::estimator::solver::HomographyFourPointSolver>]’
/home/artcs/Downloads/magsac-master/src/main.cpp:686:8:   required from here
/home/artcs/Downloads/magsac-master/include/estimators.h:454:20: warning: ‘residual’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
  454 |     return residual(point_, model_.descriptor);
      |            ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/artcs/Downloads/magsac-master/include/estimators.h:454:20: note: declarations in dependent base ‘gcransac::estimator::RobustHomographyEstimator<gcransac::estimator::solver::HomographyFourPointSolver, gcransac::estimator::solver::HomographyFourPointSolver>’ are not found by unqualified lookup
/home/artcs/Downloads/magsac-master/include/estimators.h:454:20: note: use ‘this->residual’ instead
CMakeFiles/MAGSAC.dir/build.make:134: recipe for target 'CMakeFiles/MAGSAC.dir/src/main.cpp.o' failed
make[2]: *** [CMakeFiles/MAGSAC.dir/src/main.cpp.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/MAGSAC.dir/all' failed
make[1]: *** [CMakeFiles/MAGSAC.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2


Can you give me some advice?

Thanks for your help !"
"Hi @danini - thank you for making the code to your research available! I'm curious if you'd be willing to add an explicit license to this repository (MIT, BSD, Public Domain, etc)."
"Interesting paper, Dani, but I'm very surprised you do not compare your method to existing parameter-free RANSAC methods. For example it would be very useful to compare MAGSAC against the very successful method developed by @pmoulon:
P. Moulon, P. Monasse, R. Marlet. Adaptive Structure from Motion with a contrario mode estimation. ACCV 2012"
"https://github.com/danini/magsac/blob/baede2ba343c3b1b9b26867359acd263a3054586/src/main.cpp#L817

I guess this is a bug? I think the score here should be squared."
"Hello authors,
Thanks much for sharing the code.
Actually, I am looking for the code for estimating the essential matrix but fail to figure out. Do you release the code for essential matrix as described in the paper?"
"Hi,

I think, the 3.4.6 is too strict. I haven`t checked if it would work with 3.0, but 3.4.2 is OK for sure"
"Excuse me，It's error when I use this module.The shape I entered is torch.Size([2, 64, 144, 192])。"
"Hi, thank you for your implementation. In your paper, you use a higher learning rate (like 100x) for your NDDR layers but I could not see this being done in your implementation. Is there any particular reason this change exists?"
""
"I've been playing around with the [causal attention performer implementation](https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py) and believe I've found a much simpler representation that leads to more straight-forward implementation (no explicit loops/scans or custom gradients). Performance is comparable, though compilation/tracing time is orders of magnitude faster. I'd be happy to put a PR in with changes, but I realize these types of improvements aren't necessarily a high priority so thought I'd check here first to see if there's demand for it here first.

TL;DR: the implementations below, compared to the tensorflow implementation in this repo:
- is way less code (~5 lines of logic for numerator/denominator, compared to ~50)
- jit-compiles faster (thanks to no explicit python loops)
- gives the same results in roughly the same time using basic dimensions

Implementations/benchmarks/tests are available [here](https://github.com/jackd/simple-fast-attention). Relevant parts included below and a notebook is [here](https://colab.research.google.com/drive/1Fk7LWy87LzZ1usDDVhllIYxUNpoZelTh?usp=sharing).

## Implementations

```python
def causal_numerator(qs: tf.Tensor, ks: tf.Tensor, vs: tf.Tensor):
    """"""Computes not-normalized FAVOR causal attention A_{masked}V.

    Args:
      qs: query_prime tensor of the shape [L,B,H,M].
      ks: key_prime tensor of the shape [L,B,H,M].
      vs: value tensor of the shape [L,B,H,D].

    Returns:
      Not-normalized FAVOR causal attention A_{masked}V.
    """"""
    # rhs = tf.einsum('lbhm,lbhd->lbhdm', ks, vs)
    rhs = tf.expand_dims(ks, axis=-2) * tf.expand_dims(vs, axis=-1)  # [L,B,H,D,M]
    rhs = tf.cumsum(rhs, axis=0)
    # return tf.einsum('lbhm,lbhdm->lbhd', qs, rhs)
    return tf.linalg.matvec(rhs, qs)
```

```python
def causal_denominator(qs, ks):
    """"""Computes FAVOR normalizer in causal attention.

    Args:
      qs: query_prime tensor of the shape [L,B,H,M].
      ks: key_prime tensor of the shape [L,B,H,M].

    Returns:
      FAVOR normalizer in causal attention.
    """"""
    rhs = tf.cumsum(ks, axis=0)
    return tf.einsum(""lbhm,lbhm->lbh"", qs, rhs)
```

## Theory

The task we consider is to compute the noncausal numerator $N$, where

$N = \left[(Q K^T) \circ L\right] V$

where $Q$, $K$ and $V$ are the query, key and value matrices used in fast attention, $L$ is a lower triangular matrix with values of $1$ on and below the diagonal and $\circ$ is the _Hadamard product_ (elementwise product). Noting that $Q$ and $K$ are low-rank (that's the whole point of performers/FAVOR), we can use the following handy dandy property of Hadamard products ([Property 1](http://pi.math.cornell.edu/~ajt/presentations/HadamardProduct.pdf)):

$\left[A \circ \sum_j \mathbf{u}_j \mathbf{Pv}_j^T\right]x = \sum_j D(\mathbf{u}_j) A D(\mathbf{v}_j) \mathbf{x}$

where $D(\mathbf{z})$ is the diagonal matrix with diagonal values $\mathbf{z}$. This means we can express our fast causal attention output as

$N = \sum_m D(\mathbf{q}_m) L D(\mathbf{k}_m) V$

where $\mathbf{q}_m$ and $\mathbf{k}_m$ are the $m^\text{th}$ columns of Q and K respectively.

Note it is neither efficient nor necessary to compute any of the new matrices above. $D(\mathbf{k}_m) Z$ is just the scaling of rows of $Z$ by $\mathbf{k}_m$, while $L Z$ is the cumulative sum of $Z$ on the leading dimension. This results in a significantly simpler tensorflow implementation without the need to implement custom gradients or use python loops.

## Benchmarks

Results using google-benchmark are given below. `v0` is the original, `v1` is the one discussed above. `warmup_time` is the time for the first run, which I'm using as a proxy for compilation time. Results were generated on a fairly old laptop with an Nvidia 1050Ti. Script to generate available [here](https://github.com/jackd/simple-fast-attention/blob/main/gbenchmark.py).

```txt
--------------------------------------------------------------
Benchmark                    Time             CPU   Iterations
--------------------------------------------------------------
v0_forward-cpu         5403096 ns       364764 ns         1000
v1_forward-cpu         5419832 ns       365650 ns         1000
v0_backward-cpu         268558 ns       238634 ns         2896
v1_backward-cpu         267089 ns       235842 ns         2937
v0_forward-gpu          288531 ns       241580 ns         2874
v1_forward-gpu          285695 ns       238078 ns         2908
v0_backward-gpu         268220 ns       237309 ns         2869
v1_backward-gpu         268324 ns       240429 ns         2751
v0_forward-cpu-jit      299143 ns       271613 ns         2516
v1_forward-cpu-jit      291873 ns       269618 ns         2538
v0_backward-cpu-jit     303150 ns       275359 ns         2483
v1_backward-cpu-jit     303948 ns       276806 ns         2482
v0_forward-gpu-jit      278147 ns       277842 ns         2450
v1_forward-gpu-jit      276128 ns       275956 ns         2523
v0_backward-gpu-jit     256809 ns       256798 ns         2706
v1_backward-gpu-jit     252543 ns       252537 ns         2769

Warmup time for v0_forward-cpu: 6.56445574760437
Warmup time for v1_forward-cpu: 0.1015627384185791
Warmup time for v0_backward-cpu: 22.0670325756073
Warmup time for v1_backward-cpu: 0.08140373229980469
Warmup time for v0_forward-gpu: 6.233572244644165
Warmup time for v1_forward-gpu: 0.028412342071533203
Warmup time for v0_backward-gpu: 22.226712226867676
Warmup time for v1_backward-gpu: 0.051419734954833984
Warmup time for v0_forward-cpu-jit: 6.481787443161011
Warmup time for v1_forward-cpu-jit: 0.05790424346923828
Warmup time for v0_backward-cpu-jit: 24.72081184387207
Warmup time for v1_backward-cpu-jit: 0.09151363372802734
Warmup time for v0_forward-gpu-jit: 8.328083515167236
Warmup time for v1_forward-gpu-jit: 0.08592033386230469
Warmup time for v0_backward-gpu-jit: 24.7033634185791
Warmup time for v1_backward-gpu-jit: 0.12377095222473145
```"
"When I click the ""UPLOAD..."" button in the user interface at the bottom of the Colab demo, I am given the choice to upload an image.  I did that, but I am just presented with a grey image, no processing occurs like with the other 3 images."
"I'm trying to reproduce the d3pm-uniform model on my environment. When I ran 150,000 steps, I got this:
![steps150000](https://user-images.githubusercontent.com/60313002/204120144-5ea3a80e-289a-4c56-bbbd-fc08cbd7f31c.png)

Then when I ran up to 200,000 steps, I got this:
![steps200000](https://user-images.githubusercontent.com/60313002/204120166-9856d4bb-e8cc-41e0-8f0f-1e0fc14925ab.png)

When I ran 300,000 steps, I got:
![steps300000](https://user-images.githubusercontent.com/60313002/204120181-677f78bc-cda0-40dc-aabc-1699154b1ba1.png)

This is the sampled images after 850,000 steps:
![steps850000](https://user-images.githubusercontent.com/60313002/205454842-deeb99a0-4dad-4339-be69-08acc3732017.png)


Seems the training is so difficult and  not stable. So I wonder is this normal? how many steps it need to train d3pm uniform on CIFAR-10? @jacobaustin123 @danieldjohnson Thank you so much!"
"Hi. Thanks for the released code.
There are some problems about model MACs.
I try to compute the flops of lightweight vct in one training step, with batch size 1, frame dimension (1920*1080) and the num_frame I set 3 here. Then I calculate 1.19b flops in just one training step.
Is that correct? As I increase the num_frames, the flops won't increase a lot. So I'm not sure whether the flops of this released lightweight model is actually correct.
Finally, how can I accurately compute the MACs on every module?"
"while trying to run https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb, i get :
```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
[<ipython-input-3-e286e8d7df2d>](https://localhost:8080/#) in <module>
      1 # TF streaming
----> 2 from kws_streaming.models import models
      3 from kws_streaming.models import utils
      4 from kws_streaming.models import model_utils
      5 from kws_streaming.layers.modes import Modes

3 frames
[/content/google-research/kws_streaming/layers/spectrogram_augment.py](https://localhost:8080/#) in <module>
     17 from typing import Any, Dict
     18 
---> 19 import tensorflow_model_optimization as tfmot
     20 
     21 from kws_streaming.layers.compat import tf

ModuleNotFoundError: No module named 'tensorflow_model_optimization'

---------------------------------------------------------------------------
```
"
"At the 1122 line of internal/datasets.py
```python
    # Generate random poses.
    random_poses = []
    cam2world = poses_avg(poses)
    up = poses[:, :3, 1].mean(0)
    for _ in range(n_poses):
      t = radii * np.concatenate([2 * np.random.rand(3) - 1., [1,]])
      position = cam2world @ t
      lookat = cam2world @ [0, 0, -focal, 1.]
      z_axis = position - lookat
    random_poses.append(viewmatrix(z_axis, up, position))
    self.random_poses = np.stack(random_poses, axis=0)
```
Its obvious that  `random_poses.append(viewmatrix(z_axis, up, position))` should in the for-loop, but it lies outside, which leads to only 1 random pose is genertated."
Can you please provide pretrained model for the Publaynet dataset?
"why the clip range is (-6, 60) in mobilebert quantization?

https://github.com/google-research/google-research/blob/ba74f16e2e193f62133faf73a06e7f0792d42681/mobilebert/modeling.py#L1135

The comments tells e^-6 = 0.002, Is this value related to the tensor value range of the network?  How about using -60?

How about the max value 60？Is this value related to the tensor value range of the network or a **general configuration**?"
"Hi authors,
I have read your excellent paper [PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS](https://arxiv.org/abs/2202.00512), and I want to reproduce the result by distilling the pretrained model by myself.
I found the source code and demo IPython notebook at [diffusion_distillation](https://github.com/google-research/google-research/blob/3b61da1d73543c374fc0564943e237db151af452/diffusion_distillation/README.md), but I cannot find the original training script.
Can you provide an example training script for real datasets, e.g. CIFAR-10?
Thanks!"
"There are the following codes on lines 270 and 239  in dselect_k_moe.py and tf version>=1.10 in requirements.txt，but the shape of self._binary_codes and smooth_step_activations is different, version1.10 doesn't has broadcast function。If using higher than version 2.0 or tf.where_v2 in version 1.5, that's right.
`selector_outputs = tf.math.reduce_prod(
        tf.where(self._binary_codes, smooth_step_activations,
                 1 - smooth_step_activations),
        axis=2)`"
"For module d3pm (https://github.com/google-research/google-research/tree/master/d3pm/text), the version of packages are not specified in 'requirement.txt' . Could you please provide the versions of packages that are required? They seem not compatible when running the code. Thanks!"
"Dear Authors
hey, I have a question about the flare loss 
Does it really makes a difference ？
If the forecast is already very good, combined scene substrating prediction is the flare?
I think they're close but not equal。The intensity will be much lower
I try to only use scene loss。 the results is much better
Can you explain it？"
"Hi,

I am trying to reproduce DreamField metrics result, would you like to share checkpoints for that? 

Best,
Shukai"
i use command 'pip install dmvr' and 'conda install dmvr' to isnatll dmvr package.But it is fail.
"Hello, 

I ran the predictions with the KONIQ checkpoint and it works ok.
But if I use imagenet_pretrain.npz checkpoint ( downloaded from https://console.cloud.google.com/storage/browser/gresearch/musiq;tab=objects?prefix=&forceOnObjectsSortingFiltering=false )

python -m musiq.run_predict_image \
  --ckpt_path=/tmp/spaq_ckpt.npz \
  --image_path=/tmp/image.jpg

then i get the following error : 

Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/Musiq_project/musiq/run_predict_image.py"", line 178, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/content/Musiq_project/musiq/run_predict_image.py"", line 173, in main
    params, FLAGS.image_path)
  File ""/content/Musiq_project/musiq/run_predict_image.py"", line 145, in run_model_single_image
    logits = model.call(params, image)
  File ""/usr/local/lib/python3.7/dist-packages/flax/nn/base.py"", line 238, in wrapper
    return super_fn(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/flax/nn/base.py"", line 238, in wrapper
    return super_fn(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/flax/nn/base.py"", line 562, in call
    y = instance.apply(*args, **kwargs)
  File ""/content/Musiq_project/musiq/model/multiscale_transformer.py"", line 125, in apply
    x = nn.Dense(x, num_classes, name=""head"", kernel_init=nn.initializers.zeros)
  File ""/usr/local/lib/python3.7/dist-packages/flax/nn/base.py"", line 301, in __new__
    y = instance.apply(*args, **apply_kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/flax/nn/linear.py"", line 164, in apply
    kernel = self.param('kernel', (inputs.shape[-1], features), kernel_init)
  File ""/usr/local/lib/python3.7/dist-packages/flax/nn/base.py"", line 591, in param
    param.shape, shape))

ValueError: Existing shape (384, 1000) differs from requested shape (384, 1)

Thanks in advance !"
"The AVX512 flag is disabled by default. However if we enable the AVX512, the recall becomes very small and the distance seems incorrect."
"Compiled scann but got error when import it
**_scann_ops.so: undefined symbol: _ZN10tensorflow12OpDefBuilderC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEE**

Any idea about it?"
"Hello, I'm searching with abstract Nas and get some interesting results, after searching I receive directory with data for every generation and `checkpoint.npz` and config files. How can I reuse this data to get tensorflow model back?"
"could you help explain following code?
https://github.com/google-research/google-research/blob/9b84ad0a60e4fdff0b52b7e3581f9208478597eb/jaxnerf/nerf/datasets.py#L45"
""
"Hi! Thanks a lot for releasing the code and Clay dataset to denoise the Rico dataset.
However, I don't know how to run the project in the terminal of my laptop (MacBook). Could you please provide an example of shell command and the python environment (pip packages versions) required for the project?
Thank you again!"
"I try to reproduce the result of PRL paper using the colab [demo](https://colab.research.google.com/github/google-research/google-research/blob/master/jax_dft/examples/training_neural_xc_functional.ipynb), but at certain import code block, the jaxlib and cuda version can't match.

Just re-run the colab script from the beginning to reproduce the error."
""
"Thanks for releasing codes and the interesting paper a lot! 
I follow the paper to keep the max element number as 25 in Rico, and 22 in Publaynet. While there is no split rate given in paper, so I split the Rico with 0.85:0.05:0.10 for train/val/test.  I re-train the BLT model with this data, but the metrics results calculated by the code in utils/metrics.py seems a little strange, especially for Overlap. So I'm here to ask for help. Thanks for your attention~
![image](https://user-images.githubusercontent.com/44337029/198168495-1277c861-9fab-4463-bb00-d5ab2fbc473d.png)
![image](https://user-images.githubusercontent.com/44337029/198168791-cab44fb6-4b83-44a7-b2ec-f6a8b0c83436.png)

"
"Hey, I am a student do some researches on VD. I noticed that you've proposed a great dataset called PhotoChat for multi-model  dialog generative system.
However, it seems that both the url provided and id are invalid for me now. Could you give me some tips how can I solve it without download the whole image challenge dataset with is 500G+(too expensive for me).
```
train/0a5e922c15ffb08d https://c6.staticflickr.com/1/156/400565484_fdfaa551e7_o.jpg
train/048fe12fa832143f https://c8.staticflickr.com/1/29/96515447_4ac8050343_o.jpg
train/080a8a8692a66687 https://farm8.staticflickr.com/6082/6042280582_c7a5d88b51_o.jpg
train/07f372b1aba8b165 https://farm6.staticflickr.com/7416/9782494465_1f1a8a315f_o.jpg
train/a593754ce22e87da https://c3.staticflickr.com/4/3488/3697088360_5b402b5c66_o.jpg
train/0617ddfd18b1343f https://c8.staticflickr.com/6/5565/15132241305_540a42cf78_o.jpg
```
@XiaoxueZang 
Best regards,
Zeyu Li"
Is it possible to get quantized embedding from scann library after calibration? 
"Hello,
Is it possible to get the model checkpoints from dedicated models that were trained for each visibility pattern with synthetic occlusions using different keypoint occlusion augmentation training strategies (as stated in section 5.5 of the paper)?"
"```
input_string = ""[NLU] 吾輩は猫である <extra_id_0>""                                               
inputs = tokenizer(input_string, return_tensors=""pt"", add_special_tokens=False).input_ids.to(""cuda"")
outputs = model.generate(inputs, max_length=200)
print(tokenizer.decode(outputs[0]))
```
↓
<pad><extra_id_0> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>. <unk> <unk>..<unk>..<unk>..<unk>..<unk>..<unk>..<extra_id_1>..</s>
"
"I have built my searcher, but i have no idea why it can not run when using scann.scann_ops_pybind.load_searcher(self.searcher_savedir)

![image](https://user-images.githubusercontent.com/101795593/196019259-d65ce558-90c6-4721-bef5-32162fa0fa8c.png)


what mean is **'parse_npy_header: failed to find header keyword: 'fortran_order''**"
"I experimented with the DMoN (path: graph_embedding/dmon) with several datasets, which are the focus of the DMoN's publication. Especially with Amazon PC and Amazon Photo datasets, I get far worse results than the mentioned results, while other state-of-the-art models show satisfiable performance. Does anyone get similar results with me? Or can anyone manage to get similar results as the reference paper? Thanks in advance!"
"Thanks for the very interesting paper.

I try to train script, but got an error.

```
main.py --config configs/bert_layout_publaynet_config.py --workdir exp

Traceback (most recent call last):
  File ""main.py"", line 87, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 77, in main
    trainer.train()
  File ""*/layout-blt/trainers/base_trainer.py"", line 255, in train
    train_ds, eval_ds, _, vocab_size, pos_info = input_pipeline.get_all_dataset(
  File ""*/layout-blt/input_pipeline.py"", line 149, in get_all_dataset
    train_ds, vocab_size, pos_info = get_dataset(batch_size, dataset_folder,
  File ""*/layout-blt/input_pipeline.py"", line 113, in get_dataset
    dataset = LayoutDataset(dataset_name, ds_path, add_bos, shuffle)
  File ""*/layout-blt/input_pipeline.py"", line 192, in __init__
    self.data = _normalize_entries(data, shuffle)
  File ""*/layout-blt/input_pipeline.py"", line 65, in _normalize_entries
    children = document[""children""]
TypeError: string indices must be integers

```

There is no key named children in publaynet. Are there any plans to provide code to generate train/val/test.json appropriate for each data?
"
"Hi 
I'm trying to run the [colab](https://colab.research.google.com/drive/1TjCWS2_Q0HJKdi9wA2OSY7avmFUQYGje?usp=sharing) notebook mentioned in porject page, for dreamfields.
But I am getting the below error, while running training step. What can be done to overcome it?

![image](https://user-images.githubusercontent.com/110157037/194856242-729cbf85-2055-45ad-81d8-e5f6f0262ab7.png)
"
"Thank you for your amazing work and I successfully reproduced the results of RegNeRF.
However, I kept trying to reproduce the LPIPS results but could not made the same or similar results of the paper.
Could you share the code for LPIPS evaluation?

Thank you in advance !"
"scann.scann_ops_pybind.builder(db, num_neighbors, **distance_measure**)

There are dot_product and squared_l2 options for distance_measure, and I ask how quantization, clustering, ah, search and etc are performed according to each option.

I can't find any related documents..."
"It seems like the 'heavy augmentation' settings have not been shared. 

The following probabilities for augmentations are set to zero:
probability_relative_scale = 0.,
probability_rotation = 0.0,
probability_relative_rotation = 0.0,
probability_crop_offset = 0.0,

Does this mean that these augmentations were not used for the results mentioned in the final paper?"
"https://colab.research.google.com/drive/1208hU7pVYW3iG8nPKDoZU2aSssaJfWp1?authuser=4#scrollTo=6uQJRMw-NLKQ

hello, I get this error when executing, I did the previous steps correctly, I'm a newbie in this, I attach a screenshot

[https://ibb.co/dQF956g](url)"
""
"亲爱的作者，您好，不知道您能不能把数据集放在中国一份，目前我不知道可以用什么方法打开Google数据集。
"
""
"I  couldn't find the code for generating the dataset format used in this paper. Would you mind share an example?
Thanks in advance and congrats for the excellent work!

Flavio."
"After installing the requirements.txt, I tried to run the code , but failed with the error ""AttributeError: module 'jaxlib.pocketfft' has no attribute 'pocketfft'"". How can I install pocketfft ?"
"I calculated the optical flow of the billiard_clip.mp4 as described in the run.sh file, but when I'm running my own video, can I just change the code inside convert_video_to_dataset_test.py to generate a TFRecord file? How can I modify the running code to calculate the optical flow in my own video?"
"system: ubuntu 20.04
tensorflow:  2.9.1

Based on the [experiments/](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments) , I have success in train & export saved model of non_stream .
Tf2.9.1 is the only version I tried can train the models with some little fixed , I can't find  tf_nightly-2.3.0.dev20200515 in the [experiments/](https://github.com/google-research/google-research/tree/master/kws_streaming/experiments)  。
But when i tried to export saved model of stream_state_internal.
I found it stuck at [reconstruct_from_config](https://github.com/tensorflow/tensorflow/blob/v2.9.1/tensorflow/python/keras/engine/functional.py#L1114) , the node_index_map is  always empty ,so node_index_map return always None. It keeps add node into unprocessed_nodes.
I print some variables .
The unprocessed_nodes is 
```
{<kws_streaming.layers.speech_features.SpeechFeatures object at 0x7ff1804fb730>:[[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056d940>]], 
<kws_streaming.layers.lstm.LSTM object at 0x7ff1804fbbb0>: 
[[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056dc40>]], 
<kws_streaming.layers.stream.Stream object at 0x7ff1804fbb20>:
 [[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056dc10>]], 
<keras.layers.regularization.dropout.Dropout object at 0x7ff180229e50>:
 [[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056d400>]], 
<keras.layers.core.dense.Dense object at 0x7ff18069bf40>: 
[[<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x7ff18056d430>]]}
```

And my export script is:
```
python -m kws_streaming.train.model_train_eval --data_url '' \
--data_dir kws_streaming/data2 \
--train_dir  kws_streaming/lstm_peep/ \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 20 \
--resample 0.15 \
--alsologtostderr \
--train 0 \
--lr_schedule 'exp' \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 10 \
--frequency_masks_number 2 \
--frequency_mask_max_size 5 \
lstm \
--lstm_units 500 \
--return_sequences 0 \
--use_peepholes 1 \
--num_proj 200 \
--dropout1 0.3 \
--units1 '' \
--act1 '' \
--stateful 0
```

How to solve it ?
Thanks!

"
"Hello,
I have question about the description in the sectioni 4.3 of the paper
""In all the following experiments in this section, we compute our Pr-VIPE embeddings on single video frames and use the negative logarithm of the matching probability (7) as the distance between two frames. Then we apply temporal averaging within an atrous kernel of size 7 and rate 3 around the two center frames and use this averaged distance as the frame matching distance. Given the matching distance, we use standard dynamic time warping (DTW) algorithm to align two action sequences by minimizing the sum of frame matching distances.""


Why we need matching distance as input for DTW to align two video sequences? Do you mean the distance function used in DTW is different from the default( L2 distance), and we should use matching distance function you describe above?
 


And also in section 4.3.2 
""We measure the alignment quality of our embeddings quantitatively using Kendall’s Tau [12], which reflects how well an embedding model can be applied to align unseen sequences if we use nearest neighbor in the embedding space to match frames for video pairs.""

As far as I understand, you use KNN to align two video sequences, which is somehow different from 4.3 description.


Thank you!


"
"Hi everyone, 
I have been reading the code of Felix (Flexible text-editing model), but I cannot understand how the non-autoregressivity is implemented. 

Where in the code is it implemented? Does it have to do with the classes BertEncoder and BertPretrainerV2? "
"When will the code be released, thank you!"
"Hi, @yilei , thank you for open-soucing Reg-NeRF!

I am interested in the effect of applying geometry regularization in rendering patches of unobserved viewpoint. May I ask have you tried to apply this regularization term in rendering patches from training images, and how about the effect?

Hope for the reply,
Thanks!"
May i ask about how to inference some image ?
"I'd love to use UL2, however the 20B model is far too big for my application. Section 4 of the paper mentions that multiple smaller models were trained. Could we please get access? Just the best performing model that was used as a base design for the 20B model would be fine. Additionally, a model around the size of T5-small would be great."
Hi. How i can compile scann 1.2.6 version from sources. Were i can get old versions of scann sources?
"In the F-Net implementation the padding tokens are not considered an issue https://github.com/google-research/google-research/blob/4d0e1709b347e33ac95fb557a180271bf057ee9e/f_net/layers.py#L107. 

However, the logic of the operation is to do an FFT on the hidden dimension, and then the time dimension. padding/zeroes influences the output of the FFT so it feels like this is an oversight and may influence the resultant values? I.e. the same example, in two different batches will have a different FFT outcome, if those batches have a different max sequence length (which everything gets padded to). "
"https://github.com/google-research/google-research/blob/4d0e1709b347e33ac95fb557a180271bf057ee9e/dreamfields/README.md?plain=1#L17

in Dockerfile, it is not `nvcr.io/nvidia/tensorflow:21.11-tf2-py3`, but `nvcr.io/nvidia/tensorflow:21.12-tf2-py3`."
"Why does the image after the sampler that is the next step, namely the color step 2 and 3, the resulting image is a blank."
""
""
"In ""Light Field Neural Rendering"",
Could you provide the code for the Two-phere parameterization of the light field and the experiments in the 360° scene?"
"Hello,

[MSU Graphics & Media Lab Video Group](https://videoprocessing.ai/) has recently launched two new Super-Resolution Benchmarks.
* [Video Upscalers Benchmark: Quality Enhancement](https://videoprocessing.ai/benchmarks/video-upscalers.html) determines the best upscaling methods for increasing video resolution and improving visual quality.
* [Super-Resolution for Video Compression benchmark](https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html) aims to test Super-Resolution methods on compressed videos and select the best model for each video codec standard.

Your method achieved 13th place in [Video Upscalers Benchmark: Quality Enhancement](https://videoprocessing.ai/benchmarks/video-upscalers.html) in 'Animation 2x' category and 1st place in [Super-Resolution for Video Compression Benchmark](https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html) in 'VVC compression' category. We congratulate you on your result and look forward to your future work!

We would be grateful for your feedback on our work."
Looking for a way to view the baked model online. I couldn't find any document to use the viewer code. Could you help me on this?
"Hi,

It seems like only the 200M parameter model checkpoint is available.

```
tf.io.gfile.listdir(""gs://rl-infra-public/multi_game_dt"")
# ['checkpoint_38274228.pkl']
```

Can you release a checkpoint for the 40M parameter model? (10M parameter model ckpt would be nice as well)"
"The code below cannot be resolved under Tensorflow 2.8.2
import tensorflow.compat.v1 as tf

"
"I want to used 3D object.
Anyone have an idea about ""How to export the model as Mesh ply or obj format ?""

Thankful for response in advance.
"
"I tried to train a searcher based on the code from [here](https://github.com/CompVis/latent-diffusion/blob/a506df5756472e2ebaf9078affdde2c4f1502cd4/scripts/train_searcher.py#L117-L119)

But I got ：
```
Finished loading of retrieval database of length 2616145.
(2616145, 768)
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
Initializing scaNN searcher with the following values:
k: 20
metric: dot_product
reorder_k: 40
anisotropic_quantization_threshold: 0.2
dims_per_block: 2
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
Start training searcher....
N samples in pool is 2616145
Using using partioning, asymmetric hashing search and reordering.
Partitioning params:
num_leaves: 1617
num_leaves_to_search: 80
[libprotobuf WARNING external/com_google_protobuf/src/google/protobuf/text_format.cc:339] Warning parsing text-format research_scann.ScannConfig: 43:11: text format contains deprecated field ""min_cluster_size""
Killed
```
When I reduce the dataszie to, say `(616145, 768)`, it works.
How can I train searcher using all the embeddings?"
Currently I'm using the att_rnn model for training the customized speech command models. Is there any approaches to  retrain the model with the new data from a checkpoint that can improve the overall accuracy?  
"Hi,

I was trying to play with the multigame decision transformer notebook on colab (i.e. https://github.com/google-research/google-research/blob/master/multi_game_dt/Multi_game_decision_transformers_public_colab.ipynb) but the runtime crashes whenever the line of code `init_params, init_state = model_fn.init(rng, dummy_datapoint)` is executed. The log did not report anything informational at the time of the crash:

Aug 19, 2022, 11:48:19 AM | WARNING | WARNING:root:kernel 402f5699-0e26-4cef-a58c-dfd68b89e706 restarted
-- | -- | --
Aug 19, 2022, 11:48:19 AM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports

The model parameters seem to be loaded successfully by displaying
> loading checkpoint from: gs://rl-infra-public/multi_game_dt/checkpoint_38274228.pkl
Number of model parameters: 1.98e+08

However, there was a warning that said:

Aug 19, 2022, 11:46:29 AM | WARNING | 2022-08-19 18:46:29.161081: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""NOT_FOUND: Could not locate the credentials file."". Retrieving token from GCE failed with ""NOT_FOUND: Error executing an HTTP request: HTTP response code 404"".
-- | -- | --

To provide more information for debugging, I'm a Colab Pro user and I used TPU runtime with standard memory allocation. The TPU runtime with high RAM allocation crashed too. The notebook is unmodified and I ran it cell by cell. I also tried restarting the runtime after installing all packages but it did not work as well.

I'd really appreciate if someone could look into the error and help fix it. Thank you!"
"In the checkpoint section of the following page:
https://github.com/google-research/google-research/tree/master/ul2 T5 config links lead to the page https://storage.googleapis.com/scenic-bucket/ul2/ul220b/config.gin that returns a ""NoSuchKey"" error."
"I have tried following the instructions to run FNet here: https://github.com/google-research/google-research/tree/master/f_net

But I run into error: 
```
ImportError: cannot import name 'optim' from 'flax'
```
when running 
```
python3 -m unittest discover -s f_net -p '*_test.py'
```

and can't seem to fix it by trying to roll back to a version of flax where optim is not deprecated"
"I am trying to reproduce the results of the ""Predicting the Generalization Gap in Deep Networks with Margin Distributions"" paper using the Demogen project. The code seems to be developed using TF1, and I can not run it on TF2. I have changed some parts of the code, but it still has a problem with tensor2tensor library, and the error is: 
    _kwargs = spec_.kwargs.copy()
AttributeError: 'NoneType' object has no attribute 'copy'

Could you please tell me how I can solve this problem and make this code compatible with TF2?

"
"I wonder if SCANN supports large dataset in general. For example, to train the sift1B (1 billion x 128 dim) dataset on a small machine with 64 GB of memory, I need to memmap the dataset and pass it to the scann's builder. However, seems SCANN only supports to load the dataset fully in memory before it can start training. 

Is there any way to train large dataset given constraint memory (like faiss)? Or is SCANN designed for rather small datasets only?  

```
Traceback (most recent call last):
  File ""/data/scann_test/scann_sift1B.py"", line 68, in <module>
    searcher = scann.scann_ops_pybind.builder(xb, topK, ""squared_l2"").tree(
  File ""/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_builder.py"", line 243, in build
    return self.builder_lambda(self.db, config, self.training_threads, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 78, in builder_lambda
    return create_searcher(db, config, training_threads, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 86, in create_searcher
    scann_pybind.ScannNumpy(db, scann_config, training_threads))
TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:
    1. scann_pybind.ScannNumpy(arg0: str, arg1: str)
    2. scann_pybind.ScannNumpy(arg0: numpy.ndarray[numpy.float32], arg1: str, arg2: int)

Invoked with: memmap([[2.9440201e-20, 1.8216880e-44, 2.8302559e+26, ..., 3.4590261e-11,
```"
""
"In the [paper](https://arxiv.org/abs/2007.12140), Figure 1 shows a predicted disparity of an image of [plants](https://vision.middlebury.edu/stereo/data/scenes2014/):
![disparity-in-paper](https://user-images.githubusercontent.com/523148/184538505-8afccb8c-a728-469e-92d4-161cad3c735a.png)

In the [pre-trained model](https://github.com/google-research/google-research/tree/master/hitnet),  the script `predict_middlebury.sh` produces a different image:
<img src=""https://user-images.githubusercontent.com/523148/184538977-d4298ef5-0781-4968-8e3f-c11be9909c39.jpg"" width=""337"" height=""251"">
False-colored image is created by replacing the [function `encode_image_as_16bit_png`](https://github.com/google-research/google-research/blob/master/hitnet/predict.py#L268) with

```python
def save(data, filename):
  image = data
  import numpy as np
  image = (image-image.min())/(image.max()-image.min()) * 255
  image = image.astype(np.uint8)
  import cv2
  image = cv2.applyColorMap(image, cv2.COLORMAP_JET)
  image = image[...,::-1]
  from PIL import Image
  image = Image.fromarray(image)
  image.save(filename)
```
TensorFlow version is 2.6.2.
Tried with CPU and GPU backends.
Tried with all middlebury models, which have max disparities 160, 288 and 400.
"
"I am trying to apply protoattend on adult census income data as mentioned in the paper. But I am having issues in building a LSTM encoder for that.

can anyone help me on that"
"pip install -r rouge/requirements.txt didn't seem to work for me.  I get:
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'

I then tried pip install -r rouge-score/requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge-score/requirements.txt'

And tried pip install -r rouge_score/requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge_score/requirements.txt'

Please advise or update the instructions on https://pypi.org/project/rouge-score/ and https://github.com/google-research/google-research/tree/master/rouge

Thanks!"
"I noticed that the [MBPP dataset on the Hugging Face hub](https://huggingface.co/datasets/mbpp) only contains a single ""test"" split encompassing the whole dataset, instead of a train-test split. While writing an [issue](https://github.com/huggingface/datasets/issues/4795) to correct this, it became apparent that the splits used for the experiments of the [""Program Synthesis with Large Language Models"" paper](https://arxiv.org/abs/2108.07732) were not completely clear, at least to me.

The paper mentions a four-way split of the full variant of MBPP in subsection 2.1:
> In the experiments described later in the paper, we hold out 10 problems for **few-shot prompting**, another 500 as our **test** dataset (which is used to evaluate both few-shot inference and fine-tuned models), 374 problems for **fine-tuning**, and the rest for **validation**.

The paper doesn't explicitly state the task ID ranges of the splits, but the [README.md from this repo](https://github.com/google-research/google-research/blob/master/mbpp/README.md), which is referenced in the paper, does:
> We specify a train and test split to use for evaluation. Specifically:
> 
> * Task IDs 11-510 are used for evaluation.
> * Task IDs 1-10 and 511-1000 are used for training and/or prompting. We typically used 1-10 for few-shot prompting, although you can feel free to use any of the training examples.

I.e. compared to the paper, the few-shot, train and validation splits are combined into one split, with a soft suggestion of using the first ten for few-shot prompting.

It is not explicitly stated whether the 374 fine-tuning samples mentioned in the paper have Task ID 511 to 784 or 601 to 974 or whether they were randomly sampled from Task ID 511 to 974. The total number of samples is misstated as 1000 instead of 974.

Regarding the edited variant of the dataset, the paper states the following:
> For evaluations involving the edited dataset, we perform comparisons with 100 problems that appear in both the original and edited dataset, using the same held out 10 problems for few-shot prompting and 374 problems for fine-tuning. 

The language here doesn't appear to be very precise, as among the 10 few-shot problems, those with Task ID 1, 5 and 10 are not part of the edited variant, and many from the task_id range from 511 to 974 are missing (e.g. Task ID 511 to 553).
I suppose the idea the Task ID ranges for each split remain the same, even if some of the task_ids are not present, but I think this should be stated explicitly (if it is the case).

"
"Hello, I'm researching about long-range stereo vision systems Cause I'm planning to estimate the depth of an object that is 1000m away. I have a question, Do you think this task is possible if we use the far-field lens in our research? What would be the limitations? even if we use an extra lens with a large focal length.
"
"Receiving the following backtrace when I try to `import scann` in a Python project on Ubuntu 20.04:

```
#0  0x00007fff9f1e9209 in google::protobuf::UninterpretedOption::UninterpretedOption() ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#1  0x00007fff9f1c482b in InitDefaultsscc_info_UninterpretedOption_google_2fprotobuf_2fdescriptor_2eproto() ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#2  0x00007fff9f277d43 in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.9425925494154413262] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#3  0x00007fff9f277d2f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.9425925494154413262] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
#4  0x00007fff9f277d2f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) [clone .llvm.9425925494154413262] ()
   from /home/toddwildey/Projects/ontology/library/hivemind/.venv/lib/python3.8/site-packages/scann/scann_ops/cc/_scann_ops.so
```

It looks like the error comes from this file effectively, when defining TensorFlow operations: https://github.com/google-research/google-research/blob/master/scann/scann/scann_ops/cc/ops/scann_ops.cc

Here's a current view of the `requirements.txt` file:

```
numpy
protobuf<3.20,>=3.9.2
pyarrow
scann>=1.2.6
```

Here's the result of `pip freeze` for the virtual environment:

```
absl-py==1.2.0
astunparse==1.6.3
cachetools==5.2.0
certifi==2022.6.15
charset-normalizer==2.1.0
flatbuffers==2.0
gast==0.5.3
google-auth==2.9.1
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
grpcio==1.47.0
h5py==3.7.0
idna==3.3
importlib-metadata==4.12.0
keras==2.8.0
Keras-Preprocessing==1.1.2
libclang==14.0.6
Markdown==3.4.1
MarkupSafe==2.1.1
numpy==1.23.1
oauthlib==3.2.0
opt-einsum==3.3.0
protobuf==3.19.4
pyarrow==8.0.0
pyasn1==0.4.8
pyasn1-modules==0.2.8
requests==2.28.1
requests-oauthlib==1.3.1
rsa==4.9
scann==1.2.6
six==1.16.0
tensorboard==2.8.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.1
tensorflow==2.8.2
tensorflow-estimator==2.8.0
tensorflow-io-gcs-filesystem==0.26.0
termcolor==1.1.0
typing-extensions==4.3.0
urllib3==1.26.11
Werkzeug==2.2.1
wrapt==1.14.1
zipp==3.8.1
```"
"I have followed the [steps](https://github.com/google-research/google-research/issues/355) to save the `ScannSearcher` using numpy `serialize` method, but am getting the following error while loading back the searcher. Any solutions?
```
F ./scann/utils/reordering_helper.h:90] Cannot enable exact reordering when the original dataset is empty.
Aborted (core dumped)
```
"
"### The idea is based on optimizing the interaction between users and sites' support. This is necessary for users to work for the platforms and for the platforms to work for the users.

At the moment, the most important thing that worries me is the way to optimize the user's communication with the sites. Yelp/Google/Glassdoor/etc have a huge audience and it's impossible to cover all this flow of users and their opinions or reports only by internal instruments.

**Profit for the user**: to pay money and not waste time on solving your specific (or not so) problem.

**Profit for sites**: the effectiveness of solving external issues + templating requests according to the criteria of the site itself.

_So, if someone likes the idea, please share your fascinating experience communicating with the support of one or another site._

"
"How can we obtain the **ground truth** of target policy.
How can we compute the **RMSE error** as in Figure 1, 2, 3, 4, 5 in the paper https://arxiv.org/abs/1906.04733

As described in README: ""The print-out of the Target (oracle) rewards is an estimate! In a perfect world this would be calculated with num_trajectories=\infty and max_trajectory_length=\infty. In practice, to get a better estimate, just set these to a large value (e.g., 1000).""

So, did you treat the **estimated Target (oracle) rewards** as ground truth?
I think this is not reasonable.

Can you give some suggestions?
Thanks a lot!

"
"I am able to run the demo code mentioned in the repo (graph_embedding/dmon) but after I compute pairwise_accuracy, it's only about 11%. I use the accuracy computing code in the repo."
"Thank you for this work.

I don't find the code for generating Kitti and EuROC dataset format used in this paper : 
Depth from Video in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras."
"Hi,

In https://github.com/google-research/google-research/blob/master/automl_zero/compute_cost.cc#L32
> ""To add a new op compute cost here, first run **ops_benchmark.cc**""

Where can I find this benchmark code you used?
Thanks!"
"File ""/root/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f/rouge.py"", line 21, in <module>
    from rouge_score import rouge_scorer, scoring
ImportError: cannot import name 'rouge_scorer' from 'rouge_score' (unknown location)


Version 0.0.4 works perfectly fine."
"https://github.com/google-research/google-research/tree/master/dual_pixels#android-app-to-capture-dual-pixel-data

I'm trying to get this example to run on a Pixel 6 Pro. After updating some constants (see below) I kept getting the following error:

`W/CameraDevice-JV-0: Stream configuration failed due to: createSurfaceFromGbp:390: Camera 0: No supported stream configurations with format 0x20 defined, failed to create output stream`

Modifications:
```
  private static final int DP_WIDTH = 2040;
  private static final int DP_HEIGHT = 768;
```

Plus I updated build script to work with more recent version of Gradle.

Any ideas?"
"Thank you for this work

I don't find the camera intrinsics evaluation code (Table5)."
"@andrewluchen hi，I'm interested in your project https://github.com/google-research/google-research/tree/master/multiple_user_representations. However, I'm confused about the method Iterative density weighting. Would you please share the related paper. Thx a lot."
"@rybakov  I need a help from you.
When I set the preprocess = 'mfcc', I could not convert the model to streaming mode correctly.  tf version=2.7.0.  Do you know the reason for this?
Thanks!

The error message is like:

W0720 23:15:13.229021 22708 stream.py:258] There is no need to use Stream on time dim with size 1
W0720 23:15:13.579084 22708 stream.py:258] There is no need to use Stream on time dim with size 1
W0720 23:15:13.660865 22708 test.py:646] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 2 from 1 for '{{node streaming/average_pooling2d/AvgPool}} = AvgPool[T=DT_FLOAT, data_format=""NHWC"", ksize=[1, 2, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/dropout_1/Identity)' with input shapes: [1,1,1,64].
2022-07-20 23:15:13.662452: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0720 23:15:13.663857 22708 test.py:449] tflite stream model state external with reset_state 1
I0720 23:15:33.224785 22708 model_train_eval.py:291] FAILED to run TFLite streaming: Model provided has model identifier '}☻"
"Hello, thank you for this work
Where I can find the EuROC  training database?"
"@junjiek 

I get the following error when I run inference with only some images.  Some images are fine.  My guess is that the image dimensions are somehow not compatible.  I noticed that the issue comes on images that are tall and narrow but not always.

Does the images have to be a specific dimension to have them do inference correctly?.

`
Image size: 728x2048
Image ratio: 0.35546875

Traceback (most recent call last):

  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 267, in <module>
    main()
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 251, in main
    pred = run_model_single_image(model_config, num_classes, pp_config, params, image_data)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 160, in run_model_single_image
    image = prepare_image(image_path, pp_config)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 137, in prepare_image
    data = pp_fn(data)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 301, in _preprocess_fn
    image = get_multiscale_patches(image, **preprocessing_kwargs)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 210, in get_multiscale_patches
    out = _extract_patches_and_positions_from_image(resized_image, patch_size, patch_stride, hse_grid_size,
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 164, in _extract_patches_and_positions_from_image
    out = tf.concat([p, spatial_p, scale_p, mask_p], axis=2)
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [1,28,3072] vs. shape[1] = [1,21,1] [Op:ConcatV2] name: concat
`

I get another error with a different image shape.
`
Image size: 2480x3509
Image ratio: 0.7067540609860359

Traceback (most recent call last):

  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 267, in <module>
    main()
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 243, in main
    pred_mos = run_model_single_image(model_config, num_classes, pp_config, params, image_path)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 160, in run_model_single_image
    image = prepare_image(image_path, pp_config)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 137, in prepare_image
    data = pp_fn(data)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 301, in _preprocess_fn
    image = get_multiscale_patches(image, **preprocessing_kwargs)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 210, in get_multiscale_patches
    out = _extract_patches_and_positions_from_image(resized_image, patch_size, patch_stride, hse_grid_size,
  File ""/Users/timothyhunt/PycharmProjects/image_quality/model/preprocessing.py"", line 144, in _extract_patches_and_positions_from_image
    p = tf.reshape(p, [n_crops, -1, patch_size * patch_size * c])
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 35840 values, but the requested shape requires a multiple of 3072 [Op:Reshape]
`

These image dimensions I get a successful inference.

`
Image size: 2550x3301
Image ratio: 0.7724931838836716

36.122936 36.12_2de0e5a9-67b8-4011-bb1e-163a85863314-page-1.jpeg
`

Here is another one that processed successfully.
`
Image size: 2550x3301
Image ratio: 0.7724931838836716

35.829983 35.83_2550590.jpeg
`

Can you help?
"
"Hi, I have one question how we can give our known 2D keypoints to the pr-vipe model.

_Originally posted by @B-rkh-verma in https://github.com/google-research/google-research/issues/611#issuecomment-1186964327_"
"After the inference model outputs the result vector, is it necessary to normalize the feature vector before distance calculation"
"Since there hasn't been a release of `rouge_score` in a while (#1199) I tried to install the package from source with:

```bash
pip install git+https://github.com/google-research/google-research.git#subdirectory=rouge
```

Which returns:
```bash
(env)  ~/git/tmp  pip install git+https://github.com/google-research/google-research.git#subdirectory=rouge
Collecting git+https://github.com/google-research/google-research.git#subdirectory=rouge
  Cloning https://github.com/google-research/google-research.git to /private/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/pip-req-build-4q0gbc1a
  Running command git clone --filter=blob:none --quiet https://github.com/google-research/google-research.git /private/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/pip-req-build-4q0gbc1a
  Resolved https://github.com/google-research/google-research.git to commit f3033edb77b3f4549858482c91db6131a7a7af11
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [42 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 14, in <module>
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/setuptools/__init__.py"", line 16, in <module>
          import setuptools.version
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/setuptools/version.py"", line 1, in <module>
          import pkg_resources
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 84, in <module>
          __import__('pkg_resources.extern.packaging.requirements')
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/packaging/requirements.py"", line 10, in <module>
          from pkg_resources.extern.pyparsing import (  # noqa
        File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
        File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
        File ""<frozen importlib._bootstrap>"", line 565, in module_from_spec
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/extern/__init__.py"", line 52, in create_module
          return self.load_module(spec.name)
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/extern/__init__.py"", line 37, in load_module
          __import__(extant)
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/__init__.py"", line 140, in <module>
          from .core import __diag__, __compat__
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 5663, in <module>
          _escapedPunc = Word(_bslash, r""\[]-*.$+^?()~ "", exact=2).set_parse_action(
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 677, in set_parse_action
          self.parseAction = [_trim_arity(fn) for fn in fns]
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 677, in <listcomp>
          self.parseAction = [_trim_arity(fn) for fn in fns]
        File ""/Users/leandro/git/tmp/env/lib/python3.9/site-packages/pkg_resources/_vendor/pyparsing/core.py"", line 286, in _trim_arity
          _trim_arity_call_line = (_trim_arity_call_line or traceback.extract_stack(limit=2)[-1])
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/traceback.py"", line 211, in extract_stack
          stack = StackSummary.extract(walk_stack(f), limit=limit)
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/traceback.py"", line 366, in extract
          f.line
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/traceback.py"", line 288, in line
          self._line = linecache.getline(self.filename, self.lineno).strip()
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py"", line 30, in getline
          lines = getlines(filename, module_globals)
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py"", line 46, in getlines
          return updatecache(filename, module_globals)
        File ""/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/linecache.py"", line 136, in updatecache
          with tokenize.open(fullname) as fp:
      AttributeError: module 'tokenize' has no attribute 'open'
      [end of output]
```

The issue is that there is a file `tokenize.py` in the root directory which interferes with Python's `tokenize` module. Are there any workarounds for this besides renaming the file before installation?"
"Running run.sh errors with the following output:
```
+ source ./bin/activate
+++ '[' ./bin/activate = ./run.sh ']'
+++ deactivate nondestructive
+++ unset -f pydoc
+++ '[' -z '' ']'
+++ '[' -z '' ']'
+++ hash -r
+++ '[' -z '' ']'
+++ unset VIRTUAL_ENV
+++ '[' '!' nondestructive = nondestructive ']'
+++ VIRTUAL_ENV=/home/<user>/source/google-research/muzero
+++ '[' linux-gnu = cygwin ']'
+++ '[' linux-gnu = msys ']'
+++ export VIRTUAL_ENV
+++ _OLD_VIRTUAL_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/home/<user>/source/google-research/muzero/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ export PATH
+++ '[' -z '' ']'
+++ '[' -z '' ']'
+++ _OLD_VIRTUAL_PS1=
+++ '[' x '!=' x ']'
++++ basename /home/<user>/source/google-research/muzero
+++ PS1='(muzero) '
+++ export PS1
+++ alias pydoc
+++ true
+++ hash -r
++ pip install -r muzero/requirements.txt
ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'muzero/requirements.txt'
```
Given the contents of the script, it looks like it was designed to be run from the parent folder:
```set -e
set -x

virtualenv -p python3 .
source ./bin/activate

pip install -r muzero/requirements.txt
python -m muzero.core_test
```

Doing so, however, reveals an issue with requirements.txt:
```ERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)
ERROR: No matching distribution found for tensorflow==2.4.1
```
It sounds like you're assuming the virtualenv will be created from an old version of python?
"
"First of all, thank you for sharing the idea.  The way of making data mentioned in the paper is very good, but I wonder if there is more theoretical explanation for this way?  This paper is only mentioned in Chapter 4: 'The additive nature of light implies we can model flare
as an additive artifact on top of the “ideal” image' . After subtracting F from the If I shot, there will be a purple artifact around the light source.  I am very confused. Could you please tell me the reason why you made the data like this?"
"Hello,

I followed the instructions from the **caltrain** README to download the data, but I am unable to download the data due to an access denied error.

**Data download instructions followed:**
```
DATA_DIR='./caltrain/data' # This is the default value if omitted below
python -m caltrain.download_data --data_dir=${DATA_DIR}
```

**Access denied error:** 
```
<Error>
<Code>AccessDenied</Code>
<Message>Access denied.</Message>
<Details>Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object.</Details>
</Error>
```
I would appreciate any help on this issue. Thanks! :) 
"
"`python3 -m smurf.multiframe_training.main -- --input_dir=<path **multiframe records**> --output_dir=<path to output directory>`

Which way(eg .py files) can generate **multiframe records**？I can't find answear in the README.md or code.

Thanks~"
"Hello,
I need to minimize docker image size, I don't use tf_serving, I only use like this:
```python
searcher = scann.scann_ops_pybind.builder(search_index, 2, ""dot_product"").score_brute_force().build()
neighbours, similarities = searcher.search_batched([person_vector])
```
So is it possible to build ScaNN without tensorflow by removing part of the code or is ScaNN completely dependent on tensorflow?
Thank you in advance."
"Hi there,

I am reproducing the muNet on 8 A100 GPUs. Compared to running it on Colab TPUv2 8 cores,  it takes too long to compile each child model. XLA also reminds me that it takes too long and suggests that there may be a bug. So, an [issue](https://github.com/google/jax/issues/11271) is also posted on the JAX repository, but so far no one respond. Maybe it's better to post it here. To be clear, I will add more main info here.

- The script I am using is [munet](https://colab.research.google.com/github/google-research/google-research/blob/master/muNet/muNet.ipynb), the experiment running is the smallest one, 'ViT tiny 3 layers / characters benchmark’. 
- Packages are installed in the way specified by munet script.
- The Cuda version is 11.4, then cudnn version is 8.2, python version is 3.9
- it only happens when running on 8 gpus, running it on one gpu is fine
- I am running it with Slurm, so maybe slurm does not give enough process to Python? The command is srun --partition=xxx --gres=gpu:8 -N1 -n1 --ntasks-per-node=1 --cpus-per-task=32 python munet_30June.py. I give 32 cores to this task. It should be fine.

Any hint is welcome. Thanks in advance."
"I am dealing with a low-resource langauge - the bert model is ok but it is too slow; there is also hardware limitation in the field that makes me to consider with mobile BERT. I wonder how I can train this mobile BERT from scrach ? any resources ?
Thank you for your good work."
"hello, can youplease give me imagen pls pls pls :C I really want to :(
depressedintokyoo@gmail.com

I want to create a unique and world's first nft collection of cats, they will have uniqueness and all that! I'll make a revolution with imagen
I know you haven't allowed commercial use yet, but I think you will in the future
I've been your fan since the 1st version that was on github
(I generated about 10k images in it)
I will not sell them, I really want to start developing the collection"
"I can create a RougeScorer called scorer, and even call scorer.score().

However, I want to use scorer.score_multi(), which is shown in the RougeScorer class... but then I get an error saying that RougeScorer object has no attribute 'score_multi'. What could be the case here?


For context, I am using google collab."
"Hi, I think i found a bug in regnerf datasets.py row 1131, the code should be in the for-loops?
https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/regnerf/internal/datasets.py#L1131"
Pr-vipe checkpoint file.
Please add a proper python-build or setuptools support for https://github.com/google-research/google-research/tree/master/model_pruning. For an ArchLinux user it is simply a shame to install pip packages!
"Hello, I'm interested in your single_view_mpi code. And when I see it, there's no train code in single_view_mpi directory. Could you provide me train code of single_view_mpi?? "
"@junjiek or @andrewluchen

- I have downloaded the code for MUSIQ IQA and also the checkpoint models. 
- I have updated the libraries to overcome the deprecated version of TensorFlow by upgrading to version 2.9.1
- I ran the prediction with the KONIQ checkpoint and it processes the image initially.

I run into the following error when the application starts to do the prediction in line 143 in original code line 141 in my revised code.

```
Traceback (most recent call last):
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 174, in <module>
    app.run(main)
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/Users/timothyhunt/opt/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 169, in main
    pred_mos = run_model_single_image(model_config, FLAGS.num_classes, pp_config, params, FLAGS.image_path)
  File ""/Users/timothyhunt/PycharmProjects/image_quality/run_predict_image.py"", line 141, in run_model_single_image
    model = model_mod.Model.partial(num_classes=num_classes, train=False, **model_config)
AttributeError: type object 'Model' has no attribute 'partial'
```

The model_mod.Model class is referenced from the multiscale_transformer.py script in the model directory.  When I go into that file there is no Class method for the Model Class called partial.  

Is this app posted by Google Research missing some code to run the prediction or am I misunderstaning something here and not setting it up right?

Thanks."
"Hello!
I read a paper: [meta back-translation](https://arxiv.org/pdf/2102.07847v1.pdf), but I could not find the code.
Link code from paper does not exit.
https://github.com/google-research/google-research/tree/master/meta_back_translation
So, Where can I find it?
Thank so!"
"@rybakov 
Hi,
I am preparing to use BC-ResNet to implement streaming inference. When command below used(padding is same), it seems unable to stream inference. 
If padding is set to causal, dimension inconsistency will occur in streaming inference。
Whether `kws_streaming/models/bc_resnet.py` needs to be modified to enable streaming inference?

Thank you

```
$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/bc_resnet_1/ \
--mel_upper_edge_hertz 7500 \
--mel_lower_edge_hertz 125 \
--how_many_training_steps 100,100,100,100,30000,30000,20000,10000,5000,5000 \
--learning_rate 0.001,0.002,0.003,0.004,0.005,0.002,0.0005,1e-5,1e-6,1e-7 \
--window_size_ms 30.0 \
--window_stride_ms 10.0 \
--mel_num_bins 40 \
--dct_num_features 0 \
--resample 0.1 \
--alsologtostderr \
--train 1 \
--use_spec_augment 0 \
--time_masks_number 2 \
--time_mask_max_size 25 \
--frequency_masks_number 2 \
--frequency_mask_max_size 7 \
--pick_deterministically 1 \
bc_resnet \
--sub_groups 5 \
--last_filters 32 \
--first_filters 16 \
--paddings 'same' \
--dilations '(1,1),(2,1),(4,1),(8,1)' \
--strides '(1,1),(1,2),(1,2),(1,1)' \
--blocks_n '2, 2, 4, 4' \
--filters '8, 12, 16, 20' \
--dropouts '0.1, 0.1, 0.1, 0.1' \
--pools '1, 1, 1, 1' \
--max_pool 0
```

> tflite Final test accuracy, non stream model = 96.50% (N=4890)
> tf stream model state external with reset_state 1
> TF Final test accuracy of stream model state external = 0.00% (N=10)
> tf stream model state external with reset_state 0
> TF Final test accuracy of stream model state external = 0.00% (N=10)"
Please wrap `delta_r_warmup` at line 202 of darc.agent.py with `int()`: `int(delta_r_warmup)`.
How to train the SMURF model on custom datasets? 
"When I install easy_install, I got the following error on windows 10:
        >(tf) D:\Download\ez_setup-0.9\ez_setup-0.9>python ez_setup.py
        Extracting in C:\Users\ADMINI~1\AppData\Local\Temp\tmp4yuz2578
        Traceback (most recent call last):
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 485, in <module>
            main(sys.argv[1:])
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 481, in main
            _install(tarball)
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 74, in _install
            _extractall(tar)
          File ""D:\Download\ez_setup-0.9\ez_setup-0.9\ez_setup.py"", line 467, in _extractall
            self.chown(tarinfo, dirpath)
        TypeError: TarFile.chown() missing 1 required positional argument: 'numeric_owner'

Anybody can help me?

Thanks,"
Wondering if the fine-tuned checkpoints for RealFormer are available anywhere? Particular interested in the model fine-tuned on HotpotQA
"I was trying to run 02_inference.ipynb, in the section with respect to run non streaming inference with TFLite, and i got the error Failed to import metagrap as shown in the image, will you be able to help correct this problem? Thank's for your time.
![erro](https://user-images.githubusercontent.com/64547325/171291490-38c33d4c-f1fe-4b30-b429-7c40aee0ae17.png)

"
"
Hello all, we are trying to export our complete 3d models in order to open it in Blender or similar softwares. Do you know how to export obj, fbx or any other mesh format, including textures or vertex colors?
Thank you
Mattia
"
"I keep getting the following problem when trying to use the train function. I've tried everything I can think of.

INFO:tensorflow:Saving dict for global step 0: absolute_difference = 0.0, bass_spectrogram = 0.0, drums_spectrogram = 0.0, global_step = 0, loss = 0.0, other_spectrogram = 0.0, vocals_spectrogram = 0.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: musdb_model\model.ckpt-0
WARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.
INFO:tensorflow:Loss for final step: None.
INFO:spleeter:Model training done

I am running on Windows 10. I get the same issue with whatever version of Python/Tensorflow I use. This is my command line:

python -m spleeter train -p configs/musdb_config.json -d C:\Python\spleeter\configs --verbose

Is there anything anyone can think of for me to try?

I added extra print statements to try to track down. Here is the output:

We are in:  __main__
Entered entrypoint:  __main__
Someone called spleeter_callback:   __main__
Let us train:  __main__
INFO:spleeter:Just imported TF
Did this instantiate_pre?
We are in class InstrumentDatasetBuilder spleeter.dataset
We are in class class DatasetBuilder spleeter.dataset
Did this instantiate?
INFO:tensorflow:Using config: {'_model_dir': 'musdb_model', '_tf_random_seed': 3, '_save_summary_steps': 5, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.45
}
, '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Ready to jump?......
Just jumped?......
INFO:spleeter:Start model training
estimator........:  <tensorflow_estimator.python.estimator.estimator.EstimatorV2 object at 0x0000028688343E50>
train_spec.......:  TrainSpec(input_fn=functools.partial(<function get_training_dataset at 0x00000286ABE050D0>, {'train_csv': 'configs/musdb_train.csv', 'validation_csv': 'configs/musdb_validation.csv', 'model_dir': 'musdb_model', 'mix_name': 'mix', 'instrument_list': ['vocals', 'drums', 'bass', 'other'], 'sample_rate': 44100, 'frame_length': 4096, 'frame_step': 1024, 'T': 512, 'F': 1024, 'n_channels': 2, 'n_chunks_per_song': 40, 'separation_exponent': 2, 'mask_extension': 'zeros', 'learning_rate': 0.0001, 'batch_size': 4, 'training_cache': 'cache/training', 'validation_cache': 'cache/validation', 'train_max_steps': 200000, 'throttle_secs': 1800, 'random_seed': 3, 'save_checkpoints_steps': 1000, 'save_summary_steps': 5, 'model': {'type': 'unet.unet', 'params': {'conv_activation': 'ELU', 'deconv_activation': 'ELU'}}}, <spleeter.audio.ffmpeg.FFMPEGProcessAudioAdapter object at 0x0000028688343DC0>, 'C:\\Python\\spleeter\\configs'), max_steps=200000, hooks=(), saving_listeners=())
evaluation_spec..:  EvalSpec(input_fn=functools.partial(<function get_validation_dataset at 0x00000286AD9AA9D0>, {'train_csv': 'configs/musdb_train.csv', 'validation_csv': 'configs/musdb_validation.csv', 'model_dir': 'musdb_model', 'mix_name': 'mix', 'instrument_list': ['vocals', 'drums', 'bass', 'other'], 'sample_rate': 44100, 'frame_length': 4096, 'frame_step': 1024, 'T': 512, 'F': 1024, 'n_channels': 2, 'n_chunks_per_song': 40, 'separation_exponent': 2, 'mask_extension': 'zeros', 'learning_rate': 0.0001, 'batch_size': 4, 'training_cache': 'cache/training', 'validation_cache': 'cache/validation', 'train_max_steps': 200000, 'throttle_secs': 1800, 'random_seed': 3, 'save_checkpoints_steps': 1000, 'save_summary_steps': 5, 'model': {'type': 'unet.unet', 'params': {'conv_activation': 'ELU', 'deconv_activation': 'ELU'}}}, <spleeter.audio.ffmpeg.FFMPEGProcessAudioAdapter object at 0x0000028688343DC0>, 'C:\\Python\\spleeter\\configs'), steps=None, name=None, hooks=(), exporters=(), start_delay_secs=120, throttle_secs=1800)

INFO:tensorflow:Not using Distribute Coordinator.
INFO:tensorflow:Running training and evaluation locally (non-distributed).
INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. 
        Checkpoint frequency is determined based on RunConfig arguments: 
            save_checkpoints_steps 1000 or save_checkpoints_secs None.
WARNING:tensorflow:From C:\Users\windo\anaconda3\envs\g397\lib\site-packages\tensorflow\python\training\training_util.py:
                235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be 
                    removed in a future version.
Instructions for updating:
Use Variable.read_value. 
Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.

We are in get_training_dataset:  spleeter.dataset
Loaded from:  C:\Python\spleeter\spleeter\dataset.py
Here is df:  
                                              mix_path  ...    duration
0    train\A Classic Education - NightOwl\mixture.wav  ...  171.247166
1                 train\ANiMAL - Clinic A\mixture.wav  ...  237.865215
2               train\ANiMAL - Easy Tiger\mixture.wav  ...  205.473379
3           train\Actions - Devil's Words\mixture.wav  ...  196.626576
4      train\Actions - South Of The Water\mixture.wav  ...  176.610975
..                                                ...  ...         ...
81                train\Triviul - Dorothy\mixture.wav  ...  187.361814
82  train\Voelund - Comfort Lives In Belief\mixtur...  ...  209.908390
83            train\Wall Of Death - Femme\mixture.wav  ...  238.933333
84     train\Young Griffo - Blood To Bone\mixture.wav  ...  254.397823
85            train\Young Griffo - Facade\mixture.wav  ...  167.857052

[86 rows x 6 columns]
dataset....................... <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
Loaded from:  C:\Python\spleeter\spleeter\utils\tensor.py
dataset:   <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
dataset before:  <ShuffleDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: (), start: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64, start: tf.float64}>
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
INFO:tensorflow:Calling model_fn.
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for vocals_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
WARNING:tensorflow:From C:\Users\windo\anaconda3\envs\g397\lib\site-packages\tensorflow\python\keras\layers\normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for drums_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for bass_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for other_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in:  spleeter.model
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from musdb_model\model.ckpt-0
WARNING:tensorflow:From C:\Users\windo\anaconda3\envs\g397\lib\site-packages\tensorflow\python\training\saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...
INFO:tensorflow:Saving checkpoints for 0 into musdb_model\model.ckpt.
INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...
We are in get_validation_dataset:  spleeter.dataset
Loaded from:  C:\Python\spleeter\spleeter\dataset.py
Here is df:  
                                              mix_path  ...    duration
0                 train\ANiMAL - Rockshow\mixture.wav  ...  165.511837
1        train\Actions - One Minute Smile\mixture.wav  ...  163.375601
2   train\Alexander Ross - Goodbye Bolero\mixture.wav  ...  418.632562
3   train\Clara Berry And Wooldog - Waltz For My V...  ...  175.240998
4        train\Fergessen - Nos Palpitants\mixture.wav  ...  198.228753
5           train\James May - On The Line\mixture.wav  ...  256.092880
6    train\Johnny Lokke - Promises & Lies\mixture.wav  ...  285.814422
7                train\Leaf - Summerghost\mixture.wav  ...  231.804807
8              train\Meaxic - Take A Step\mixture.wav  ...  282.517188
9   train\Patrick Talbot - A Reason To Leave\mixtu...  ...  259.552653
10        train\Skelpolu - Human Mistakes\mixture.wav  ...  324.498866
11      train\Traffic Experiment - Sirens\mixture.wav  ...  421.279637
12             train\Triviul - Angelsaint\mixture.wav  ...  236.704218
13           train\Young Griffo - Pennies\mixture.wav  ...  277.803537

[14 rows x 6 columns]
dataset....................... <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
Loaded from:  C:\Python\spleeter\spleeter\utils\tensor.py
dataset:   <TensorSliceDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64}>
dataset before:  <MapDataset shapes: {mix_path: (), vocals_path: (), drums_path: (), bass_path: (), other_path: (), duration: (), start: ()}, types: {mix_path: tf.string, vocals_path: tf.string, drums_path: tf.string, bass_path: tf.string, other_path: tf.string, duration: tf.float64, start: tf.float64}>
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
We are in load_waveform:  spleeter.dataset
We are in compute_spectrogram:  spleeter.dataset
We are in compute_spectrogram_tf in:  spleeter.audio.spectrogram
here is the tensor:   Tensor(""transpose_1:0"", shape=(None, None, None), dtype=complex64)
INFO:tensorflow:Calling model_fn.
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model
We are in:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for vocals_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for drums_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for bass_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
We are in apply_unet:  spleeter.model.functions.unet
INFO:tensorflow:Apply unet for other_spectrogram
We are in _get_conv_activation_layer spleeter.model.functions.unet
We are in _get_deconv_activation_layer:  spleeter.model.functions.unet
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2022-05-29T18:24:50
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from musdb_model\model.ckpt-0
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Inference Time : 4.72633s
INFO:tensorflow:Finished evaluation at 2022-05-29-18:24:54
INFO:tensorflow:Saving dict for global step 0: absolute_difference = 0.0, bass_spectrogram = 0.0, drums_spectrogram = 0.0, global_step = 0, loss = 0.0, other_spectrogram = 0.0, vocals_spectrogram = 0.0
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 0: musdb_model\model.ckpt-0
WARNING:tensorflow:Training with estimator made no steps. Perhaps input is empty or misspecified.
INFO:tensorflow:Loss for final step: None.
INFO:spleeter:Model training done"
"In the function format_outputs(prediction):

about **line 1240** in the tft/libs/tft_model.py:

flat_prediction = pd.DataFrame(
          prediction[:, :, 0],
          columns=[
              **'t+{}'.format(i)**
              for i in range(self.time_steps - self.num_encoder_steps)
          ])

should be:

flat_prediction = pd.DataFrame(
          prediction[:, :, 0],
          columns=[
              **'t+{}'.format(i+1)**
              for i in range(self.time_steps - self.num_encoder_steps)
          ])

or the **line 1244**:

flat_prediction['forecast_time'] = time[:, **self.num_encoder_steps - 1**, 0]

should be:

flat_prediction['forecast_time'] = time[:, **self.num_encoder_steps**, 0]?


Thanks and welcome anybody to review this issue!

"
"Hello. I'm trying to pre-train BERT model following the CuBERT paper to create CuBERT model for C++ language.
On the CuBERT GitHub page, it explains only that they modify run_pretraining.py from BERT. But how much did you change run_pretraining.py to work with CuBERT tokenizer? "
"This is not an issue but a question rather. I had been evaluating Scann few months back and came across this project: https://big-ann-benchmarks.com/ . I spent some time in coming up with the framework code for Scann and the PR can be found here https://github.com/vamossagar12/big-ann-benchmarks/tree/t1/scann. 
I couldn't get the time to continue on this further but before I did that, I realized I should ask it here.
I am not a google employee so I am not quite sure if I should be proceeding with submitting for these evaluations without the consent of researchers at google. There are some deadlines for submission etc so wanted to understand if anybody from google research would be interested in participating in this ? If yes, then would it be a collaborative effort where I can also chime in or somebody from your side would pick this up? And if it's a no, then well I can also pause this effort :) "
"In The YouTube Android App When a User Visit The App And Click **Any Video** to Watching And **After Watching Any Video** Then go to Watch **Short Video** There **Show Recently video Inside Every Short Video** There is an Issue. Please See and Fix The Issue.
![Youtube_Issue_17 19 34](https://user-images.githubusercontent.com/101797831/170092107-fbb3dae1-92ec-4005-a6fa-a5f4ae77f091.png)
"
"I cannot install the dependencies using requirements.txt. I have tried both conda and pip.

Running ```pip install -r requirements.txt``` produces the following error: 

```
ERROR: Invalid requirement: '_libgcc_mutex=0.1=main' (from line 4 of izmailov/requirements.txt)
Hint: = is not a valid operator. Did you mean == ?
```

Running ```conda create --name <env> --file <this file>``` produces the following error:

```
Collecting package metadata (current_repodata.json): done
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - future==0.18.2=pypi_0
  - tabulate==0.8.9=pypi_0
  - wrapt==1.12.1=pypi_0
  - markdown==3.3.4=pypi_0
  - opt-einsum==3.3.0=pypi_0
  - pyasn1-modules==0.2.8=pypi_0
  - google-auth-oauthlib==0.4.4=pypi_0
  - certifi==2020.12.5=py38h06a4308_0
  - tk==8.6.10=hbc83047_0
  - promise==2.3=pypi_0
  - typing-extensions==3.7.4.3=pypi_0
  - keras-preprocessing==1.1.2=pypi_0
  - sqlite==3.35.4=hdfb4753_0
  - google-pasta==0.2.0=pypi_0
  - attrs==20.3.0=pypi_0
  - libstdcxx-ng==9.1.0=hdf63c60_0
  - jmp==0.0.2=pypi_0
  - libgcc-ng==9.1.0=hdf63c60_0
  - ld_impl_linux-64==2.33.1=h53a641e_7
  - dm-tree==0.1.6=pypi_0
  - absl-py==0.12.0=pypi_0
  - grpcio==1.32.0=pypi_0
  - idna==2.10=pypi_0
  - ncurses==6.2=he6710b0_1
  - importlib-resources==5.1.2=pypi_0
  - readline==8.1=h27cfd23_0
  - tensorboard-data-server==0.6.0=pypi_0
  - tensorflow-metadata==0.30.0=pypi_0
  - python==3.8.8=hdb3f193_5
  - chex==0.0.6=pypi_0
  - jax==0.2.12=pypi_0
  - h5py==2.10.0=pypi_0
  - toolz==0.11.1=pypi_0
  - xz==5.2.5=h7b6447c_0
  - numpy==1.19.5=pypi_0
  - zlib==1.2.11=h7b6447c_3
  - pyasn1==0.4.8=pypi_0
  - requests-oauthlib==1.3.0=pypi_0
  - oauthlib==3.1.0=pypi_0
  - protobuf==3.15.8=pypi_0
  - flatbuffers==1.12=pypi_0
  - tqdm==4.60.0=pypi_0
  - pip==21.1=pypi_0
  - astunparse==1.6.3=pypi_0
  - chardet==4.0.0=pypi_0
  - gast==0.3.3=pypi_0
  - google-auth==1.30.0=pypi_0
  - optax==0.0.6=pypi_0
  - six==1.15.0=pypi_0
  - urllib3==1.26.4=pypi_0
  - tensorflow-estimator==2.4.0=pypi_0
  - tensorboard-plugin-wit==1.8.0=pypi_0
  - jaxlib==0.1.65+cuda112=pypi_0
  - requests==2.25.1=pypi_0
  - rsa==4.7.2=pypi_0
  - scipy==1.6.3=pypi_0
  - setuptools==52.0.0=py38h06a4308_0
  - tensorflow==2.4.1=pypi_0
  - tensorboard==2.5.0=pypi_0
  - dill==0.3.3=pypi_0
  - ca-certificates==2021.4.13=h06a4308_1
  - werkzeug==1.0.1=pypi_0
  - cachetools==4.2.2=pypi_0
  - tensorflow-datasets==4.2.0=pypi_0
  - googleapis-common-protos==1.53.0=pypi_0
  - termcolor==1.1.0=pypi_0
  - dm-haiku==0.0.5.dev0=pypi_0
  - libffi==3.3=he6710b0_2
  - openssl==1.1.1k=h27cfd23_0

Current channels:

  - https://repo.anaconda.com/pkgs/main/win-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/win-64
  - https://repo.anaconda.com/pkgs/r/noarch
  - https://repo.anaconda.com/pkgs/msys2/win-64
  - https://repo.anaconda.com/pkgs/msys2/noarch
  - https://conda.anaconda.org/conda-forge/win-64
  - https://conda.anaconda.org/conda-forge/noarch
  - https://conda.anaconda.org/anaconda/win-64
  - https://conda.anaconda.org/anaconda/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
```

Any help is appreciated, thanks!"
"Hello!

I was very impressed by the results achieved in the ""Entity Linking in 100 Languages"" paper. I wanted to replicate the results you have achieved on Mewsli-9 with Model F+ and use this in my research. However, I could not find implementation of Model F+ and its training process anywhere. Is it possible for you to publish the source code, please?

Thanks!"
"Hi @hassanhub , first of all great work on VATT and thanks a lot for open sourcing its code. I'm currently trying to pretrain VATT on my own dataset. So far, I have converted my raw videos and text to TFRecords and have also made changes to `vatt/data/datasets/toy_dataset.py` as described [here](https://github.com/google-research/google-research/tree/master/vatt#data).

Can you please help me figure out what more do I need to do to get the pretraining running on my dataset? Would really be grateful!"
"Hi! I am wondering what gin files are needed to fine-tune UL2. I tried the following gin file, adapted from https://github.com/google-research/t5x/blob/main/t5x/examples/t5/t5_1_1/examples/small_wmt_finetune.gin. However, I met the problem of `NameError: 't5_architecture' was not provided by an import statement.`. I guess we will also need some gin files from FlaxFormer, right? Could you provide the complete gin file list for fine-tuning UL2? Thanks!

```
from __gin__ import dynamic_registration

import __main__ as train_script
from t5.data import mixtures
from t5x import models
from t5x import partitioning
from t5x import utils

include ""ul2.gin""
include ""t5x/configs/runs/finetune.gin""

USE_CACHED_TASKS = False
MIXTURE_OR_TASK_NAME = ""wmt_t2t_ende_v003""
TASK_FEATURE_LENGTHS = {""inputs"": 256, ""targets"": 256}
TRAIN_STEPS = 2651000
DROPOUT_RATE = 0.0
INITIAL_CHECKPOINT_PATH = ""gs://scenic-bucket/ul2/ul220b/checkpoint_2650000/""
# `LOSS_NORMALIZING_FACTOR`: When fine-tuning a model that was pre-trained
# using Mesh Tensorflow (e.g. the public T5 / mT5 / ByT5 models), this should be
# set to `pretraining batch_size` * `target_token_length`. For T5 and T5.1.1:
# `2048 * 114`. For mT5: `1024 * 229`. For ByT5: `1024 * 189`.
LOSS_NORMALIZING_FACTOR = 233472
```"
271 images not found in multimodalchat
"in README.md in 
https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/trillsson

The Models are in TensorFlow hub [here](https://github.com/google-research/google-research/blob/master/non_semantic_speech_benchmark/trillsson/?).

The above suppose to link to TensorFlow hub, but it linked back to the repo instead"
"I followed the code in the prediction part, I have given this parameter (label_map_file), but it shows as None.
This is my predict_run.sh code:
![2028fdf385b81f61b23ad70b69fcf20](https://user-images.githubusercontent.com/80745053/169202666-5aa85f1c-18c3-4b91-9c0b-ea1d7a45aad5.png)

This is the error:

![d8d399d69ce3696cb33603419c71be1](https://user-images.githubusercontent.com/80745053/169202104-4627ad35-06c1-4fb8-8122-75b0a13940f0.png)

Hope you can help me，Thanks!


"
"Hi, I read the code of regnerf, and I found there is no NLL loss, which is different from the [released paper](https://drive.google.com/file/d/1S_NnmhypZjyMfwqcHg-YbWSSYNWdqqlo/view). Is this term not important? As shown the ablation study (Table 3), the appearance regularizer slightly improves the performance. Is it the reason that the code excludes the flow model? Thank you!"
""
"Hi,
is there a method to decrease the training GPU memory consumption?

Thanks"
"![image](https://user-images.githubusercontent.com/72790108/168244466-729de0b4-4d37-4bf4-816d-b375a192559a.png)
question: Cannot load from D:\Anaconda\project\nn\Time_forest\lstm_transformer\google-research-master\tft\expt_settings\..\outputs\saved_models\volatility\main\tmp, skipping ...
In script_hyperparam_opt.py, training has finished but cannot load tmp, resulted in recycle training ,I do not know about reason. Thanks "
"Running on Windows 10 Version 20H2 (OS Build 19042.1645)

Installed Bazel, cloned git, ran:

`./run_demo.sh`

and get the following output:

> Starting local Bazel server and connecting to it...
> INFO: Repository rules_cc instantiated at:
>   C:/users/jack/documents/google-research/automl_zero/WORKSPACE:26:13: in <toplevel>
> Repository rule http_archive defined at:
>   C:/users/jack/_bazel_jack/n5bafl7x/external/bazel_tools/tools/build_defs/repo/http.bzl:353:31: in <toplevel>
> ERROR: An error occurred during the fetch of repository 'rules_cc':
>    Traceback (most recent call last):
>         File ""C:/users/jack/_bazel_jack/n5bafl7x/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 100, column 45, in _http_archive_impl
>                 download_info = ctx.download_and_extract(
> Error in download_and_extract: java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> ERROR: C:/users/jack/documents/google-research/automl_zero/WORKSPACE:26:13: fetching http_archive rule //external:rules_cc: Traceback (most recent call last):
>         File ""C:/users/jack/_bazel_jack/n5bafl7x/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 100, column 45, in _http_archive_impl
>                 download_info = ctx.download_and_extract(
> Error in download_and_extract: java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> ERROR: Skipping ':run_search_experiment': no such package '@rules_cc//cc': java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> WARNING: Target pattern parsing failed.
> ERROR: no such package '@rules_cc//cc': java.io.IOException: Error extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip to C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767: Prefix ""rules_cc-master"" was given, but not found in the archive. Here are possible prefixes for this archive: ""rules_cc-main"".
> INFO: Elapsed time: 23.970s
> INFO: 0 processes.
> FAILED: Build did NOT complete successfully (0 packages loaded)
> FAILED: Build did NOT complete successfully (0 packages loaded)
>     currently loading:
>     Fetching C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc; Extracting C:/users/jack/_bazel_jack/n5bafl7x/external/rules_cc/temp1749116481683058767/master.zip"
"Hi,
thanks for sharing your implementation. I have two questions about it:

1.	Does it also work on tabular data?
2.	Is it possible to identify the noisy instances (return the noisy IDs or the clean set)?

Thanks!"
"Hello 👋

I run a security community that finds and fixes vulnerabilities in OSS. A researcher (@dirac231) has found a potential issue, which I would be eager to share with you.

Could you add a `SECURITY.md` file with an e-mail address for me to send further details to? GitHub [recommends](https://docs.github.com/en/code-security/getting-started/adding-a-security-policy-to-your-repository) a security policy to ensure issues are responsibly disclosed, and it would help direct researchers in the future.

Looking forward to hearing from you 👍

(cc @huntr-helper)"
"Hey @ajayjain, are there utilities available in dreamfields project to train model on custom dataset of text-mesh pairs.

For example, I want to train on Amazon products Dataset: https://amazon-berkeley-objects.s3.amazonaws.com/index.html"
@joshuahm The data and code have been in the release process for 2 years and have not yet been released
"Building Scann from source on MacOs M1 fails with 
```
Original error was: dlopen(/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/_multiarray_umath.cpython-38-darwin.so' (no such file), '/usr/lib/_multiarray_umath.cpython-38-darwin.so' (no such file)
Is numpy installed?
ERROR: /Users/sagarrao/gitprojects/google-research/scann/scann/scann_ops/cc/python/BUILD.bazel:8:17: //scann/scann_ops/cc/python:scann_pybind.so depends on @local_config_python//:python_headers in repository @local_config_python which failed to fetch. no such package '@local_config_python//': Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/__init__.py"", line 23, in <module>
    from . import multiarray
  File ""/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/multiarray.py"", line 10, in <module>
    from . import overrides
  File ""/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/overrides.py"", line 6, in <module>
    from numpy.core._multiarray_umath import (
ImportError: dlopen(/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so, 0x0002): tried: '/Users/sagarrao/gitprojects/google-research/scann/venv/lib/python3.8/site-packages/numpy/core/_multiarray_umath.cpython-38-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/_multiarray_umath.cpython-38-darwin.so' (no such file), '/usr/lib/_multiarray_umath.cpython-38-darwin.so' (no such file)
```
after a lot of other steps which needed to be done. Anybody knows any workaround for this?"
"I try to run ""infinite nature"" on colab but I have several errors:
1-  ""ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.
albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.""
3- ""---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
[<ipython-input-3-497b3f26180e>](https://localhost:8080/#) in <module>()
     13 config.set_training(False)
     14 model_path = ""ckpt/model.ckpt-6935893""
---> 15 render_refine, style_encoding = infinite_nature_lib.load_model(model_path)
     16 initial_rgbds = [
     17     pickle.load(open(""autocruise_input1.pkl"", ""rb""))['input_rgbd'],

9 frames
<__array_function__ internals> in prod(*args, **kwargs)

[/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py](https://localhost:8080/#) in __array__(self)
    747   def __array__(self):
    748     raise NotImplementedError(""Cannot convert a symbolic Tensor ({}) to a numpy""
--> 749                               "" array."".format(self.name))
    750 
    751   def __len__(self):

NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array."""
"Hi, I tried to run run_demo.sh. First it complains about no prefix ""rules-cc-master"" and suggests ""rules-cc-main"". The same for ""googletest-master"", so I changed them in WORKSPACE. Then I get the following error. Could someone help me on this? Thanks.

ERROR: /home/xiaoan/.cache/bazel/_bazel_xiao/a739b7a7b94483f081eeb0f6b7bbd028/external/com_google_absl/absl/BUILD.bazel:100:1: unexpected keyword 'visibility' in call to _config_setting_group(name, match_any = [], match_all = [])
ERROR: Analysis of target '//:run_search_experiment' failed; build aborted: Analysis failed
"
"* After doing data distillation on one dataset, can update function preserve the data distillation capability?"
""
"Hi,

Can we leverage advancements made in DALL-E 2 training strategies and diffusion models in this project somehow?

Best,
Rakesh"
"I try to run this code : 
![image](https://user-images.githubusercontent.com/54568460/163562947-c3fae365-5104-4a8b-927f-b5da3e2cb708.png)

But it's take more than 10 hours and just train 36.000 / 1 mil .


![10h](https://user-images.githubusercontent.com/54568460/163563088-f3c75f10-1a1b-47aa-bef8-811168b52404.PNG)


"
I want to adapt TFT for my time series classification use case but there is no loss function available for such use case. Only quantile loss is available. Is there any way I can use TFT for binary classification?
Could you tell me the version of python and the package used? Maybe this is causing the error
"Hello! I am trying to replicate VATT.

I am trying to generate DMVR records as required for the two datasets you used for text-to-video retreival. I understand how to generate the YouCook2 csv files for this because the original annotations provide a start and end time and caption. 

However, I am struggling to understand how you generated the CSV file required for MSRVTT. 

In order to keep the replication accurate to your implementation, I was hoping you could provide such a file or reference on how to generate such a file for the MSRVTT dataset. 

Thanks!"
"@cll27 After reading the article ""On the Generalization of Representations in Reinforcement Learning"" I wonder what is the best way to represent  observation  which is 3D on-hot sparse np.array with size about (300,200,1000) and values 0 or 1.
The array represents solution of school timetable -  with 300 rooms, 200 groups and 1000 timeslots.
I thought to use 3D keras  convolution layers to prepare dense representation but I will be grateful for any sugestion.

Peter"
"  File ""/home/google-research/dreamfields/dream/lib/python3.6/site-packages/jax/_src/random.py"", line 599, in choice
    raise ValueError(""a must be an integer or 1-dimensional"")
jax._src.traceback_util.UnfilteredStackTrace: ValueError: a must be an integer or 1-dimensional
"
"Hi,

Can you please share the training command (with hyper-parameters) for reproducing numbers in Table 1 of the main paper?

I'm unable to reproduce reported results for implicit_pdf on the SYMSOL1 dataset. The paper (arxiv version) specifies the following hyper-parameters for reproducing results (Section S8):

- ""One network was trained for all five shapes of SYMSOL I"" (`--symsol_shapes symsol1`)
- ""... with a batch size of 128 images""
- ""... for 100,000 steps""
- ""three positional encoding terms were used for the query""
- ""four fully connected layers of 256 units with ReLU activation for the MLP""
- Note that the paper also says that it renders 100k images for each shape. However, the `symmetric_solids` dataset in tfds only contains 50k images each, and that's what I train on.

The corresponding training run command is: 
```bash
python -m implicit_pdf.train --symsol_shapes symsol1 \
 --number_fourier_components 3 \
 --batch_size 128 \
 --number_training_iterations 100000 \
 --head_network_specs 256 --head_network_specs 256 --head_network_specs 256 --head_network_specs 256 
```

However, the trained model seems to be overfitting as `gt_log_likelihood` starts reducing for `cyl` and `cone` after ~3k iterations. Please see the [uploaded tensorboard logs](https://tensorboard.dev/experiment/PLJkBENPR6aqG7VwJqzWAQ/). Reducing the depth of the MLP network to the default 2 layers didn't help either. 

Thanks and Regards,
Shubham"
"Hello, 
Thank you very much for this great contribution. 
I have just implemented your example.py file in the below link and I wonder how to implement my own similarity metric in this code. 

https://github.com/google-research/google-research/blob/master/scann/docs/example.ipynb
Thank you in advance."
Just wanted to ask whether FNet is stable when using mixed precision training?
"Hi!
How can I use GPU to get the ddgk graph embdding?
I have tried the following code:
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"".
The GPU memory usage is very high but the GPU-Util is close to 0.
I have also tried to replace multiprocessing.pool with 'for' loop, but it doesn't work either.
My version of tensorflow is 2.1.0.
Thanks for your reply!"
"Dear @maniatis 

I fine-tuned Cubert in my own dataset, and I would like to get the embedding of source code (function)?"
""
"Hi，
When I run file ‘run_predict_image.py’, the error is reported as follows

File ""/home/xj/Dataset/WXQ/labelingcocodataset/musiq/run_predict_image.py"", line 144, in run_model_single_image
    model = model_mod.Model.patial(num_classes=num_classes, train=False, **model_config)
AttributeError: type object 'Model' has no attribute 'patial'"
"Thank you for the good work of SMURF. I am trying to replicate the reported performance on the training set of the Sintel dataset, namely 1.71 EPE on Clean and 2.58 EPE on Final.

I used apply_smurf.py on the sintel sequences but cannot get the same performance. Could you please provide some instructions on what commands to run?

Thank you."
"When I use this file, it prompts an error 
AttributeError: type object 'Model' has no attribute 'partial'

"
"This question is for the [paper](https://arxiv.org/abs/2008.04637) corresponding to this: https://github.com/google-research/google-research/tree/master/sign_language_detection part.
1] Would it be right to conclude from Table-1 (of the paper) that the inference times in Table-1 (0.35 ms in the worst case) can be ignored since in actual applications, the bottleneck latency will be due to the pose-estimation algorithm (> 300 ms)?

2] The paper mentions that the OpenPose's latency per frame is 300 ms. How does the demo then support real-time processing at 25 fps (i.e. < 40 ms delay per frame)? Is the 300 ms latency for heavy ResNet based versions?"
"In ""Large-Scale Generative Data-Free Distillation"", the resnet34 pre-trained on imagenet32 achieves 60% top-1 auuuracy on val set. We attempt to train the model on imagenet32, however, only get about 35% top 1 accuracy. Could you provide the pre-trained weight of resnet34, or some suggests. thx!"
"`example` is a dict of numpy arrays, neither of which are hashable
https://github.com/google-research/google-research/blob/master/ipagnn/adapters/common_adapters.py#L108
"
"Hi, appreciate of your great work! But i have some confuse about loss function.

if we have a minibath data: A， A1，A2，A3，B，C，D，E. Ai belongs to one class, and B, C, D, E belongs other 4 class.

the paper said eq2 let more positive data pair contribute to loss function, and positive pair (such as A•A1, A•A2, A•A3)  becomes lager, negtive pair  (such as: A•B, A•C, A•D) becomes smaller.

The eq2 indicate that the numerator inside log still only have one positive data pair and the denominator have all positive and negtive pair each calculate.

If A is anchor, A1，A2，A3 are positive data, and B，C，D，E are negtive data. the part of eq2 is： 
（log（A•A1 / ( A•A1 + A•A2 + A•A3 + A•B + A•C + A•D + A•E）+ 
    log（A•A2 / ( A•A1 + A•A2 + A•A3 + A•B + A•C + A•D + A•E）+ 
    log（A•A3 / ( A•A1 + A•A2 + A•A3 + A•B + A•C + A•D + A•E））/ 3 


then, when loss is going down, the numerator (such as A•A1) in log is going to get bigger and the denominator is going to get samller. Is that correct? It seems like every elements in denominator is going to get smaller expect A•A1. So that means A•A2, A•A3 in denominator are going to get smaller？Logically, the values of A•A2, A•A3 should be larger.

I'm confused about this.
looking forward to your replies! 
"
"hello 

I have a question while studying the assembleNet.

I wonder where the loss function for learning is on the code

If there is no loss function, I wonder why there is no loss function.

thank you"
"Thanks for providing the codes. I'd like to know how to calculate the fid between the colorization and ground truth.
In your paper, you state that generate 5000 colorizations and there is no overlap between colorization and ground truths. So how the two datasets (colorization and ground truth) are splitted."
"Hello, I obtained ""attention_weights"" with the following command: 
weights = libs.tft_model.TemporalFusionTransformer(dictMerged).get_attention(raw_data)

I now have three problems:
(1) tft_ model. py explains that ""decoder_self_attn"" in ""attention_weights"" is the time attention weight. What is the time attention weight?
(2) Are ""static_flags"", ""historical_flags"" and ""future_flags"" in ""attention_weights"" the result of variable selection network output?
(I don't know why I can't upload pictures. These attributes are mentioned in line 1020 of tft_model. py)
(3) According to the above question (2), how to obtain the weight of each variable corresponding to P10, P50 and P90?


"
"When I run the function `Splitter` of `splitter.py`, `unsupported operand type(s) for +: 'int' and 'str'` occurred in line 267 where the code is `iter=iterations` , how can I solve it?
Thanks!"
"Hello, im getting the following error after these lines in the import cell of the google collab:

""import tensorflow as tf
tf.config.experimental.set_visible_devices([], ""GPU"")""



![2022-03-06_20h38_04](https://user-images.githubusercontent.com/58479885/156939295-5bd14151-7fe6-4f5a-b89a-f989e34d44f5.png)

Any help would be much appreciated, 
Thanks 
"
"Hello,

I also have a problem while getting the attention weights from the model with code:
model.get_attention(train)
after I load the model generated by:
!python3 -m script_train_fixed_params $EXPT $OUTPUT_FOLDER $USE_GPU
from model_folder

I got the following errors:
![image](https://user-images.githubusercontent.com/68455198/156923199-9b15570d-0ecd-4dd5-8311-880a8d88802f.png)

May I know how to solve this problem?

Thanks,
Erica"
"Hi there,

Refering to this issue [https://github.com/snap-stanford/ogb/issues/303](https://github.com/snap-stanford/ogb/issues/303). I was wondering if you have the **original text data** for all the 47 labels in Amazon2M dataset?

Thanks."
"Hi, 

Im looking to produce a tflite model with internal streaming state. However, it looks like the code only produces external state streaming tflite models. Is there a limitation with doing this, or is this just something that has not been implemented yet? 

Thanks,
Brett Young 

@rybakov"
"https://arxiv.org/abs/2202.10447

Excuse me， When will the code and pre training model of this paper be submitted?
thanks！
"
"Dear machine learning Aces, 

What happens when I set the epochs setting to 1 ? 
I have been trying to execute the electricity model with the script_hyperparam_opt. 

I know I am not using a GPU, but it takes forever... However, my PC parameters are not too bad too...
![image](https://user-images.githubusercontent.com/100216305/156133314-5ce735a5-1cc0-41d4-81d4-c412bbdcaa2e.png)

I really dont get what is happening in the background, when it says (see below) and runs the same multiple times:

None
*** Fitting TemporalFusionTransformer ***
Getting batched_data
Using cached training data
Using cached validation data
Using keras standard fit
Train on 450000 samples, validate on 50000 samples
273152/450000 [=================>............] - ETA: 3:30:42 - loss: 0.4841
"
I've looked for it but I've not found any documentation about scann rather than just initializing and searching on it. Is it possible to add new data and create new clusters?
"When I try to train tft model with kaggle retail dataset  favorita, I got the following error running script_download_data.py 
process_favorita function
![image](https://user-images.githubusercontent.com/22293352/154432588-a6b02c64-eb99-4043-88ea-f7d9c23aa202.png)
"
""
"Hi, @hassanhub. I encountered this problem and  failed to solve it after searching for several solutions to similar(just look like similar) issues.

As you mentioned in #962, I modified the `objectives.py` for conducting a multi-gpu pre-training experiment and encountered this problem. Then I tried to inplement a single-gpu version and also encountered this problem.

The error information is as:
```
2022-02-11 15:07:20.266179: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.266326: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.266722: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.268130: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.268490: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
Traceback (most recent call last):
  File ""main.py"", line 97, in <module>
    app.run(main)
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 93, in main
    return executor.run(mode=params.mode)
  File ""experiments/base.py"", line 439, in run
    self.train()
  File ""experiments/base.py"", line 280, in train
    metrics = train_step(data_iterator, num_iterations)
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""anaconda3/envs/vatt/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
         [[{{node strided_slice_3}}]]
         [[while/body/_1/while/IteratorGetNext]]
         [[while/body/_1/while/model/tx_mlp_fac/backbone_stack/audio_module/wat_base/RandomShuffle/_72]]
  (1) INVALID_ARGUMENT:  slice index 0 of dimension 0 out of bounds.
         [[{{node strided_slice_3}}]]
         [[while/body/_1/while/IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_step_97988]

Function call stack:
train_step -> train_step

2022-02-11 15:07:20.269632: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.269706: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.271021: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.271261: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.271735: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.274001: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.274614: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.275279: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.275625: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.
2022-02-11 15:07:20.276116: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at strided_slice_op.cc:108 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds

I mainly modified the `base.py` and `objectives.py`. Since I'm not familiar with Tensorflow framework, the code base_and_objectives.tar.gz may exists a lot of  ridiculous problems. Do you have some idea about this problem?

Thanks."
" Following the link, https://github.com/google-research/google-research/tree/master/dual_pixels#android-app-to-capture-dual-pixel-data
 the app to capture dual pixel imagery is installed. While I was capturing dual pixel data using Google pixel 4a , I could see the message "" xxxxxxx is saved"" coming . As no google pixel camera is with SD card, the captured image(s) are not being directed to the path ""/sdcard/Android/data/com.google.reseach.pdcapture/files""

The issues Image(s) are not getting saved in the specified directory."
"Hey @vanzytay,

Thanks a lot for uploading all of your newest T5 checkpoints to GCP. 
It's really amazing that you guys are taking the time to make the weights available to the community. Especially, the smaller versions `{small, mini, tiny}` are extremely useful IMO.

I've started an automatic conversion of these checkpoints to Transformers (see: https://huggingface.co/NewT5/) to make them even more available. The idea is to convert them to all frameworks we offer: PyTorch, Tensorflow, and JAX and give them a bit more visibility - hope this is ok for you! Before releasing them we would move them to the Google org on the Hub.

The conversion is going well so far - however I've noticed the that the following checkpoints seem to be incomplete or missing:
1. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_lg_h8_l16_law_03-21-23-05 has no checkpoints
2. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_lg_h8_l32_law_03-21-23-06
3. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_3B_l28_law_04-04-11-05 

For 1. there are no checkpoints at all.
For 2. & 3. it seems like only the checkpoints up to step 306000 are available - should I just take those for the conversion or are the 524288 ones available somewhere?

"
"The author of hitnet used dilated convolution in the last three propagations of the paper, but the model you provided did not use dilated convolution. Why? Is it because this works better ?

"
"Hi,

I have a question about the scann source code.

In the scann.cc, the `ScannInterface::Serialize(std::string path)`, there is a [circle](https://github.com/google-research/google-research/blob/master/scann/scann/scann_ops/cc/scann.cc#:~:text=for%20(const%20auto,datapoint_to_token.npy%22%2C%20datapoint_to_token)) to serialize `datapoint_to_token` into file `datapoint_to_token.npy`:

The method `VectorToNumpy()` is defined in the scann/utils/io_npy.h, it uses the `OpenSourceableFileWriter` class to write data into file datapoint_to_token.npy.
The `OpenSourceableFileWriter` constructor is:
```
OpenSourceableFileWriter::OpenSourceableFileWriter(absl::string_view filename)
    : fout_(std::string(filename), std::ofstream::binary) {}
```
The `fout_` is a std::ofstream, it is not appended mode. The `datapoint_to_token.npy` will be overwritten by the circle for multiple times, is it expected?"
It seems the link to the coltran colab notebook (https://tinyurl.com/coltranfewclicks) has become invalid. Could you fix it?
"Hi, I couldn't find pipeline to convert *.las file to trainable dataset format. How are we supposed bro train on custom datasets without this feature?"
"Hi, Thanks for sharing your VATT work! Will you provide PyTorch version of the VATT code and checkpoints? That will be more helpful, thanks!"
"in non_decomp/ for the paper ""Teaching overparametrised models...' by harikrishna et al, in the execution of CSL loss, there seems to be an issue with the loss function, becomes NaN after a few epochs.
"
"@ajayjain

It's working regardless but I noticed that this warning is shown:

```
/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3662: 
UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32.
To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable.
See https://github.com/google/jax#current-gotchas for more.
  lax._check_user_dtype_supported(dtype, ""zeros"")
```"
"@ajayjain 

https://colab.research.google.com/drive/17GtPqdUCbG5CsmTnQFecPpoq_zpNKX7A?usp=sharing#scrollTo=5NN3LHPaYORI

In the Colab notebook Cell 3 does not fix the matlibplot version problem. Moving the cell to the top, running it, and restarting restarting the runtime fixes the issue though.

```
# Upgrade packages to deal with version mismatch. Restart after running this cell.
!pip install --upgrade matplotlib numpy"
"Hi @hassanhub, sorry for the interruption.
I have followed the instruction with the command 'python -m vatt.main --task=pretrain --mode=train --model_dir=PATH/TO/RUN --model_arch=tx_fac --strategy_type=mirrored' on the GPU environement.
The TPU configuration is set to None.
However it got the error as following, which seems related to the TPU.
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'AllToAll' used by {{node StatefulPartitionedCall/AllToAllGather}} with these attrs: [split_count=4, concat_dimension=1, split_dimension=0, T=DT_FLOAT]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>
Do you  the idea for the error?"
"if you get this error,   look in wit.py and wit_kaggle.py in the lib/site-packages/tensorflow_datasets/vision_language

wit.py and wit_kaggle.py need this fix

import ctypes as ct
csv.field_size_limit(int(ct.c_ulong(-1).value // 2))
#csv.field_size_limit(sys.maxsize)                                                                                                                              
limit1 = csv.field_size_limit()
""0x{0:016X}"".format(limit1)


fixes it.   Wrong ""Long"" type apparently.
"
"I tried reading the raw data and converting it to RGGB Bayer domain, like dnd_denoise.py does, and feeding the data to process.py to get an RGB color image, but I get weird colors. However, if I swap the flip direction of the two bayer_pattern cases, I get the correct color.

I'm just wondering if it's because we do the transpose on the image matrix but not on the bayer_pattern or I do something wrong?
Below is part of the code I used to convert the raw data to an RGB image.

```
img = h5py.File(filename, 'r')
Inoisy = np.float32(np.array(img['Inoisy']).T)
bayer_pattern = np.asarray(info[info['camera'][0][i]]['pattern']).tolist()

if (bayer_pattern == [[1, 2], [2, 3]]):
    pass
elif (bayer_pattern == [[2, 1], [3, 2]]):
    Inoisy = np.flipud(Inoisy)                <==== Here I change the flip direction
elif (bayer_pattern == [[2, 3], [1, 2]]):
    Inoisy = np.fliplr(Inoisy)                <==== Here I change the flip direction
else:
    print('Warning: assuming unknown Bayer pattern is RGGB.')

height, width = Inoisy.shape
channels = []
for yy in range(2):
    for xx in range(2):
        noisy_crop_c = Inoisy[yy:height:2, xx:width:2].copy()
        channels.append(noisy_crop_c)
channels = np.stack(channels, axis=-1)

Inoisy = process.process(channels, red_gains, blue_gains, cam2rgbs)

```
"
"Hi there,

I am trying to use ScANN (with both backends, including TF with GPU) on my data. The data consists of 200M vectors of 256 Dims. Even if I use big EC2 instance of 744G RAM I still get Out of memory exception, even if I set all hyperparameters to extremely small values:

```
    searcher = scann\
        .scann_ops.builder(all_embeddings, 50, ""dot_product"")\
        .tree(num_leaves=60, 
              num_leaves_to_search=1, 
              training_sample_size=300)\
        .score_ah(2, anisotropic_quantization_threshold=0.2)\
        .reorder(100)
```

The embeddings are already loadded into memory, but after build() starts to work I see that it just starts taking more and more RAM using 1 core only (so I guess actual training haven't started yet). 

The same situation happens if I use GPU, but it first loads something in GPU, but then fails.

Does ScANN actually supports data of this size? If yes, what can I do to successfully run it?"
"if not, hint/pointer to how to extend is also greatly appreciated!"
"**Behaviour observed with**:
* Python 3.7 Tensorflow 2.6, Py 3.7 TF 2.7, Py 3.8 TF 2.7, Py 3.9.9 TF 2.7.0, 
* GPUs: 
    * GeForce GTX 1660 SUPER, compute capability: 7.5, 5944MiB, CUDA 11.2
    * Tesla M60, compute capability: 5.2, 8129MiB, CUDA 11.5

**Error Message**:
```
InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
	 [[map/while/loop_body_control/_21/_51]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_adapt_step_1288]

Function call stack:
adapt_step -> adapt_step
```
**ReprEx**:
```
import os, pathlib
import tensorflow as tf
from tensorflow import keras

base_dir = pathlib.Path(""RepRex"")
os.makedirs(base_dir / ""neg"", exist_ok = True)
os.makedirs(base_dir / ""pos"", exist_ok = True)

with open(base_dir / ""pos"" / ""sample1.txt"", ""w"") as f:
    f.write(""The food was excellent.\n"")

with open(base_dir / ""neg"" / ""sample1.txt"", ""w"") as f:
    f.write(""The wine was awful, we will never go to that place again.\n"")


batch_size = 2
ds = keras.utils.text_dataset_from_directory(""RepRex"", batch_size=batch_size)
text_only_ds = ds.map(lambda x, y: x)   # Prepare a dataset that only yields raw text inputs (no labels).

text_vectorization = keras.layers.TextVectorization(max_tokens=20000, output_mode=""tf_idf"")
print(""Running adapt() with 'tf_idf' on CPU"") # ok
with tf.device('/cpu:0'):
    text_vectorization.adapt(text_only_ds)  
    
text_vectorization2 = keras.layers.TextVectorization(max_tokens=20000, output_mode=""count"")
print(""Running adapt() with 'count' on GPU"") # ok
with tf.device('/gpu:0'):
    text_vectorization2.adapt(text_only_ds)  

text_vectorization3 = keras.layers.TextVectorization(max_tokens=20000, output_mode= ""multi_hot"")
print(""Running adapt() with 'multi_hot' on GPU"") # ok
with tf.device('/gpu:0'):
    text_vectorization3.adapt(text_only_ds)  

text_vectorization4 = keras.layers.TextVectorization(max_tokens=20000, output_mode=""tf_idf"")
print(""Running adapt() with 'tf_idf' on GPU"") # error
with tf.device('/gpu:0'):
    text_vectorization4.adapt(text_only_ds)  
 
```

** ReprEx output**:
```
Found 2 files belonging to 2 classes.
Running adapt() with 'tf_idf' on CPU
Running adapt() with 'count' on GPU
WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe5d80cec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Running adapt() with 'multi_hot' on GPU
WARNING:tensorflow:6 out of the last 6 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fe5d80ceaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Running adapt() with 'tf_idf' on GPU
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/tmp/ipykernel_610484/3457753437.py in <module>
     37 print(""Running adapt() with 'tf_idf' on GPU"")
     38 with tf.device('/gpu:0'):
---> 39     text_vectorization4.adapt(text_only_ds)
     40 

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/keras/engine/base_preprocessing_layer.py in adapt(self, data, batch_size, steps)
    242       with data_handler.catch_stop_iteration():
    243         for _ in data_handler.steps():
--> 244           self._adapt_function(iterator)
    245           if data_handler.should_sync:
    246             context.async_wait()

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

~/.local/share/virtualenvs/jbodart_argen-x.com-ve3sV5aU/lib/python3.9/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     56   try:
     57     ctx.ensure_initialized()
---> 58     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     59                                         inputs, attrs, num_outputs)
     60   except core._NotOkStatusException as e:

InvalidArgumentError: 2 root error(s) found.
  (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
	 [[map/while/loop_body_control/_21/_51]]
  (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
	 [[{{node map/TensorArrayUnstack/TensorListFromTensor/_42}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_adapt_step_1288]

Function call stack:
adapt_step -> adapt_step
```"
"Hi!
I've started working with the gldv2 dataset for FL using the split provided [here](https://github.com/google-research/google-research/tree/master/federated_vision_datasets).
In particular, I'm using the [dataset loader](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/gldv2/load_data) from tensorflow federated.

I've noticed that roughly a 10% of images are missing: [this](https://github.com/tensorflow/federated/blob/da2ca219e75e3317c43e10a83d07b635aad9cfd2/tensorflow_federated/python/simulation/datasets/gldv2.py#L108) exception is raised when creating the clients tfrecords.
It looks that some filenames in the federated splits are not matching filenames from the original dataset. 

I've collected the missing files in a [json file](https://drive.google.com/file/d/1HbMKi8LrWCwldhl7H4DuZmEJcLBSavgq/view?usp=sharing) that you can check (list of dict - one element per client - with fields `user_id`, `total`, `found`, `missing`, `missing_files`)

@hang-qi I'm keeping bugging you sorry :)"
"flare_removal requires keras, what is the version of keras"
"Thanks for your amazing work. I was trying to generate the right disparity image from the given demo files. The code throws an error 
KeyError: ""The name 'graph/secondary_output_disparity:0' refers to a Tensor which does not exist. The operation, 'graph/secondary_output_disparity', does not exist in the graph.""
To double-check, I printed the names of all the tensors in the graph and was not able to find a key that matches the word ""secondary"". 
Can you please let me know how I can get the right disparity image?
Thanks in advance."
"Trying to call `distributed_shampoo` (as the version in the JAX folder doesn't support Optax) without setting any parameter other than `learning_rate` and `block_size` results in this error:
```py
    974     else:
--> 975       to_pad = -num_statistics % num_devices_for_pjit
    976       padded_statistics = [pad_matrix(stat, max_size) for stat in statistics]
    977       padded_statistics.extend([

TypeError: unsupported operand type(s) for %: 'int' and 'NoneType'
```

Forcing me to set a value for `num_devices_for_pjit`, while in the source these comments are written at [line 580](https://github.com/google-research/google-research/blob/03cb03de1583d48940ff087987fabffe85710f98/scalable_shampoo/optax/distributed_shampoo.py#L580):
```py
    ### Only set following 3 params in pjit/spmd mode.
    ### WARNING: Experimental
    mesh_axis_names=None,
    num_devices_for_pjit=None,
    shard_optimizer_states=False,
```
Passing `num_devices_for_pjit = 1` also brings about another error, now caused by what I think is an error in the code at [line 1012](https://github.com/google-research/google-research/blob/03cb03de1583d48940ff087987fabffe85710f98/scalable_shampoo/optax/distributed_shampoo.py#L1012), `if not batch_axis_name:` then enters a branch that uses the **PJIT** mode for matrix inverse pth root, while I think that logically if `batch_axis_name` isn't set pjit mode shouldn't be enabled (As commented on [line 578](https://github.com/google-research/google-research/blob/03cb03de1583d48940ff087987fabffe85710f98/scalable_shampoo/optax/distributed_shampoo.py#L578)).

Going ahead and trying to patch this doesn't seem to make some progress and this is where I ended up, I hope something can be done in being able to run the Optax optimizer **WITHOUT** needing to configure everything in terms of batches.

If this isn't possible I hope there's a chance for the non distributed optimizer to be adapted to Optax."
"https://github.com/google-research/google-research/blob/742c8e68a411247264ef338a430a95be9e700faf/depth_and_motion_learning/consistency_losses.py#L95

I just can't understand why the occlusion mask is generated by comparing between `frame1transformed_depth.depth` and `frame2depth_resampled`. Since `frame1transformed_depth` is at frame2's location, `frame2depth_resampled` is at frame1's location, what's the meaning of compare the depth at the same pixel when they are at different location? Does not compare `frame1depth` and `frame2depth_resampled` at the same pixel more reasonable?"
"Hi, @chihkuanyeh  
When I click the link ""https://console.cloud.google.com/storage/browser/concept_discovery""
I got the following error:
  _Additional permissions required to list objects in this bucket. Ask a bucket owner to grant you 'storage.objects.list' permission._ 
How can I fix it.

Thank you in advance."
"Hi,

I have been looking into the Meta Pseudo Labels paper https://arxiv.org/abs/2003.10580 and checking out the corresponding code herein. First of all, thanks for the great work and sharing the code. I have a couple of questions regarding the batch size used in the paper.

1. The paper mentions (Section 3.3 Implementation Details) about using a batch size of 4096 images for labelled data and 32768 for unlabelled data in a single iteration step. I was thinking about the impact if the Student is updated using pseudo labels on unlabelled data with a batch size of 4096 and repeat the process 8 times (effective batch size of 32768) and then update the Teacher with the feedback of Student's performance on 4096 labelled images. Could you please share your thoughts on this, or if you tried any experiments with this strategy?
2. How would the method perform if trained with a smaller batch size, e.g. 256, 512 etc? Would its performance be close to the paper results or drop down significantly?

It would be really great if you could provide some insights into the above queries on batch size or if you have some results with these that you could please share.

Looking forward to your reply.

Thank You !!

Anuj"
""
Are there any yocto recipes for this framework?
"Hi,

There are 35 labels audio data in speech_commands_v0.02.tar.gz but I can't see the setting which label will be used for training and test.
When we want to do [12 labels experiment](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md), should we set which labels are used? or Should they be set automatically correctly?
(I've seen [12 labels experiment](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md) have been done because the final layer has 12 output)
And if we want to 12 labels experiment, do dataset actually need 35 labels data?

Thank you"
"https://github.com/google-research/google-research/blob/bbee4fa1d4459c2505052d689cb8ecca4f1360a7/poem/run.sh#L57

TEMPORL_PR_VIPE_TRAIN_DIR->TEMPORAL_PR_VIPE_TRAIN_DIR"
"I've noticed when using a custom data split with this repo, theres no silence or unknown audio added to the dataset, similar to how it is added in when the default subfolder organization is used. It's simple to add an unknown class by just adding a 'unknown' folder in the dataset with a bunch of random audio files, however, this solution doesn't add silence clips, and it's tedious to make a directory full of silence clips. I've added a simple way to add silence in my own local copy of this repo. Was wondering if the authors of this repo would be interested in me turning this option into something that can be controlled with command line flags, and making a pull request for this feature? 
@rybakov

Thanks,
Brett"
"Hi, i could not find the source code of SNIP, where is metioned in the paper ""Pruning Redundant Mappings in Transformer Models
via Spectral-Normalized Identity Prior"".

Could you release the code about this paper(SNIP), Thanks!"
"Hi, @hassanhub. I have some problem with DMVR.

I followed the [dmvr example](https://github.com/deepmind/dmvr/tree/master/examples) to generate the dataset.
And changed the corresponding code so the code can read the correct path of data.

But when I run the vatt evaluation process, it would raise following error:

> tensorflow.python.framework.errors_impl.InvalidArgumentError: 3 root error(s) found.
>   (0) INVALID_ARGUMENT:  Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
>          [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
>          [[MultiDeviceIteratorGetNextFromShard]]
>          [[RemoteCall]]
>          [[IteratorGetNext_3]]
>          [[model/video_module/vit_base/spatio_temporal_embeddings/strided_slice_5/_1]]
>   (1) INVALID_ARGUMENT:  Name: <unknown>, Key: image/encoded, Index: 0.  Number of values != expected.  values size: 91 but output shape: []
>          [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
>          [[MultiDeviceIteratorGetNextFromShard]]
>          [[RemoteCall]]
>          [[IteratorGetNext_3]]
>   (2) CANCELLED:  Function was cancelled before it was started
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_evaluation_step_21087]
> 
> Function call stack:
> evaluation_step -> evaluation_step -> evaluation_step

I located the error at line150 in file ./vatt/data/loading.py where the object generate by command `factory.make_dataset()` cannot be read. The minimize pseudocode is as follow:

```
dataset =  factory.make_dataset(
              shuffle=self.shuffle,
              num_epochs=self.num_epochs,
              batch_size=per_replica_batch_size,
              padded_batch=False,
              drop_remainder=True,
              keep_key=False,
              override_preprocess_fn=None,
              )
iter_data = iter(dataset)
next(iter_data) // this would trigger the error
```

-------------------------------------------------------

Here is the pipeline I used to generate the data file. I write some info in the README.txt.
[dmvr_example.tar.gz](https://github.com/google-research/google-research/files/7769292/dmvr_example.tar.gz)

I cannot identify whether I miss used the dmvr, or I miss code the vatt data reader.

What should I do?

Thanks
"
"
![image](https://user-images.githubusercontent.com/9492778/147052569-67ec9e3c-4e23-4fc9-8fd1-5a0de32b71ba.png)
"
"Hi, I am a beginner in UI understanding and found your amazing work, but I have some trouble reproducing your work.
I tried to run your code on the recent environment (`python 3, tensorflow==2.7.0, apache_beam==2.34.0`), but I got an error:
```
Traceback (most recent call last):
  File ""E:/github/google-research/widget_caption/create_tf_example_main.py"", line 364, in <module>
    app.run(main)
  File ""D:\Anaconda3\envs\tf25\lib\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""D:\Anaconda3\envs\tf25\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""E:/github/google-research/widget_caption/create_tf_example_main.py"", line 360, in main
    runners.DataflowRunner().run_pipeline(pipeline)
TypeError: run_pipeline() missing 1 required positional argument: 'options'
```

It seems that `run_pipeline` in the recent `apache_beam` (`2.34.0`) takes two input parameters: `pipeline` and `options`. 
I checked the `apache_beam` API documentation and found that until version 2.9.0, `run_pipeline` only needs the parameter `pipeline`, but version 2.9.0 and previous versions only support python 2.7 but not python 3, which makes me confused. 
Could you please provide the requirements for running?"
"I use google colab-tpu to run the official MPL code, but i got some questions.
Question 1: i can not find the parameter 'num_cores_per_replica' in main.py(line 71). And the paremeter is not defined in flag_utils.py.
Question 2: i donot know which tpu_lib is imported in main.py(line 66).
thank you very much."
"Hi @AustinCStone 
When I click the link at: https://console.cloud.google.com/storage/browser/gresearch/smurf/kitti
I got nothing but a blank page like this:
![image](https://user-images.githubusercontent.com/58680686/146515861-0e8cf497-5b4f-4f21-b61c-74d66bd791b9.png)

Then I use: gsutil cp -r gs://gresearch/smurf* /tmp/smurf/
I got an error:
root@mygpu:~/smurf# gsutil cp -r gs://gresearch/smurf* /tmp/smurf/
Can't locate LWP/UserAgent.pm in @INC (you may need to install the LWP::UserAgent module) (@INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.22.1 /usr/local/share/perl/5.22.1 /usr/lib/x86_64-linux-gnu/perl5/5.22 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.22 /usr/share/perl/5.22 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base) at /usr/bin/gsutil line 26.
BEGIN failed--compilation aborted at /usr/bin/gsutil line 26.

Is there any other way to get the pre-trained model?
Thank you in advance."
Is there an equivalent of faiss index.add() in ScaNN?
"How can I run ETC (Extended Transformer Construction) model using the graphic card (GPU) on my own system (windows 10). I mean, I am not going to use google Cloud.
I just use the WikiHop fine-tuning section of the model. The section employs **apache beam** and I think it does not work well. Because, I got an error which is shown in the following highlighted line ([data_utils.py](https://github.com/google-research/google-research/blob/master/etcmodel/models/wikihop/data_utils.py))


![image](https://user-images.githubusercontent.com/42575196/145620192-1c54acea-114c-4a83-88fd-f97dafd24f8f.png)


error:
```
raise JSONDecodeError(""Expecting value"", s, err.value) from None
RuntimeError: json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1) [while running 'DevParseExample']
```

I think, I got the error because I did not use google cloud. I appreciate to inform me if you have any idea.
"
"@junjiekCopybara-Service
@junjiek

I am impressed with the approach and work done with MUSIQ for image quality assessment.  I would like to use it to expand the capabilities and even train with additional data for specific IQA instances.

At the moment it appears that this code is for doing a prediction using one of the model checkpoints created from the competitive datasets.  I would like to train the approach on some unique datasets and train it further.

Although I think I may be able to come up with the code to train a model based on the paper, I wanted to see if you would be willing to release your training scripts as well so I could train new datasets quicker.  Thanks."
"We are trying to reproduce the results presented in the paper ""![What Are Bayesian Neural Network Posteriors Really Like?](https://arxiv.org/abs/2104.14421)"". However, when we tried to run the script, we found that it throws an error. After examining the code, we found that the problem is in https://github.com/google-research/google-research/blob/730deec43de1980d70eeb5d0a2d93e723d334fc7/bnn_hmc/utils/train_utils.py#L204
![图片](https://user-images.githubusercontent.com/34415979/145117704-17245884-98e1-4c16-8654-2bfe4a66acd9.png)
The order of arguments of this function is not consistent with how it is called in https://github.com/google-research/google-research/blob/730deec43de1980d70eeb5d0a2d93e723d334fc7/bnn_hmc/run_hmc.py#L122
And it also uses the wrong order of arguments when it calls `_perdevice_log_prob_and_grad` https://github.com/google-research/google-research/blob/730deec43de1980d70eeb5d0a2d93e723d334fc7/bnn_hmc/utils/train_utils.py#L147
"
"I noticed the conclusion in your paper - ""In contrast, Single -> Multi knowledge distillation improves or matches the performance of the other methods on all tasks except STS, the only regression task in GLUE. **We believe distillation does not work well for regression tasks because there is no distribution over classes passed on by the teacher to aid learning**"". 


Did you test the knowledge distillation technique on other regression tasks?  E.g., noise reduction.

Looking forward to your reply.

B.R.
"
"Hi,
I wonder whether tf3d will become available in tensorflow 2.7.0 or future versions.

Sparse conv is often used for point cloud recognition and is very important. I think many researchers will want to use it with latest tensorflow. However, currently, tf3d is only supported for tensorflow 2.3.

I read its build codes(WORKSPACE), and found comments for future works. For example,
[https://github.com/google-research/google-research/blob/4a383cf4eef1144b5c711802393fd91407f38323/tf3d/ops/WORKSPACE#L77](https://github.com/google-research/google-research/blob/4a383cf4eef1144b5c711802393fd91407f38323/tf3d/ops/WORKSPACE#L77)

So, I expect tf3d will be updated. Do you have such a plan?

Thank you."
"I'm experimenting with the 4 dataset in the original paper. One thing that I noticed is that val losses usually gets it's best value in first couple of epochs and don't improve or stays the same. Is this normal? What is the point of training more than 2-3 epochs than? Also predefined eary calls usually stops the network between 10-20 epochs. The paper says that the model is trained for 6 hours on a V100 for electiricty dataset. Is there a reason why I am not seeing val loss improvements, or this is normal and that 6 hours training time is with hparam search included. I'd appreciate if you help me understand, thanks."
"I tried to make a model by using MobileBERT.
And I'm struggling to train IB-BERT first.
But There is no any information about IB-BERT.
So I'm asking how to train IB-BERT,
If only way to train IB-BERT is fixing BERT, could you explain me how to reduce dimension and add layers."
"Hi,

My input is a single .png format image, whenever I finish running stage1, stage2 (color upsampler) produces this error:

Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/google-research/coltran/custom_colorize.py"", line 244, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/content/google-research/coltran/custom_colorize.py"", line 223, in main
    out = model.sample(bit_cond=prev_gen, gray_cond=gray_64)
  File ""/content/google-research/coltran/models/upsampler.py"", line 115, in sample
    logits = self.upsampler(bit_cond, gray_cond, training=False)
  File ""/content/google-research/coltran/models/upsampler.py"", line 101, in upsampler
    channel = tf.concat((channel, gray_embed), axis=-1)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [5,64,64,512] vs. shape[1] = [1,64,64,512] [Op:ConcatV2] name: concat



What could be causing this?
Thank you

"
"Getting this error when trying to install `scann` from `pip`:
```
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```

Tried with both Python 3.9.6 and 3.8.12.

`pip freeze`:
```
absl-py==0.15.0
astunparse==1.6.3
atomicwrites==1.4.0
attrs==21.2.0
autopep8==1.6.0
cachetools==4.2.4
certifi==2021.10.8
charset-normalizer==2.0.7
clang==5.0
cycler==0.11.0
dython==0.6.8
flatbuffers==1.12
gast==0.4.0
gensim==4.1.2
google-auth==2.3.3
google-auth-oauthlib==0.4.6
google-pasta==0.2.0
greenlet==1.1.2
grpcio==1.42.0
h5py==3.1.0
idna==3.3
importlib-metadata==4.8.2
joblib==1.1.0
keras==2.7.0
Keras-Preprocessing==1.1.2
kiwisolver==1.3.2
Markdown==3.3.6
matplotlib==3.4.3
numpy==1.19.5
oauthlib==3.1.1
opt-einsum==3.3.0
pandas==1.3.2
pathlib-mate==1.0.1
Pillow==8.4.0
protobuf==3.19.1
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycodestyle==2.8.0
pyparsing==3.0.6
python-dateutil==2.8.2
pytz==2021.3
requests==2.26.0
requests-oauthlib==1.3.0
rsa==4.7.2
scikit-learn==1.0.1
scikit-plot==0.3.7
scipy==1.7.2
seaborn==0.11.2
six==1.15.0
smart-open==5.2.1
SQLAlchemy==1.4.27
tensorboard==2.7.0
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.0
tensorflow==2.6.0
tensorflow-estimator==2.7.0
tensorflow-recommenders==0.6.0
termcolor==1.1.0
threadpoolctl==3.0.0
toml==0.10.2
typing-extensions==3.7.4.3
urllib3==1.26.7
uszipcode==0.2.6
Werkzeug==2.0.2
wrapt==1.12.1
zipp==3.6.0
```"
Which exact tensorflow version should I use? Minor errors appear due to the version issue. There's no info in _google-research/tft/requirements.txt_
"Hi Team,

I followed your instructions listed [here](https://github.com/google-research/google-research/tree/master/goemotions) and was able to train a model on an uncased BERT-base. After the training process, I exported the model which resulted in a new folder containing:

```
variables/
saved_model.pb
```

I then tried several loading mechanisms, e.g. with Keras, but in every case I observed errors like:

```
WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'bert/encoder/layer_11/output/LayerNorm/gamma:0' shape=(768,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().
```

It seems that there are additional ressources that have to be loaded to actually use the exported model. 

Can you briefly explain how to do so?

Kind regards
Julian"
"- ### How long do I need to train on a custom dataset? 
- ### How do I know if the training is complete?
- ### How to get the result/output for a custom dataset?
- ### How to calculate the FID?

I am using this Notebook, for training on the cityscapes dataset.
[Link to Colab Notebook](https://colab.research.google.com/drive/1hG5t2djdrToWGyQdvGJ3Sf_DkDfWi5jE?usp=sharing)

I trained the model on colab (till GPU resource got exhausted), got around 23 checkpoints.
```
I1115 14:34:04.833563 140478266419072 run.py:286] Saved checkpoint to /content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt/model-20
I1115 14:37:47.372999 140478266419072 run.py:282] Loss: 0.549 bits/dim, Speed: 0.449 steps/second
I1115 14:41:29.732624 140478266419072 run.py:282] Loss: 0.507 bits/dim, Speed: 0.450 steps/second
I1115 14:45:12.382166 140478266419072 run.py:282] Loss: 0.507 bits/dim, Speed: 0.449 steps/second
I1115 14:48:54.532032 140478266419072 run.py:282] Loss: 0.497 bits/dim, Speed: 0.450 steps/second
I1115 14:52:36.791089 140478266419072 run.py:282] Loss: 0.529 bits/dim, Speed: 0.450 steps/second
I1115 14:52:40.168719 140478266419072 run.py:286] Saved checkpoint to /content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt/model-21
I1115 14:56:22.615087 140478266419072 run.py:282] Loss: 0.530 bits/dim, Speed: 0.450 steps/second
I1115 15:00:05.021047 140478266419072 run.py:282] Loss: 0.522 bits/dim, Speed: 0.450 steps/second
I1115 15:03:47.351305 140478266419072 run.py:282] Loss: 0.489 bits/dim, Speed: 0.450 steps/second
I1115 15:07:29.647610 140478266419072 run.py:282] Loss: 0.528 bits/dim, Speed: 0.450 steps/second
I1115 15:11:11.850597 140478266419072 run.py:282] Loss: 0.550 bits/dim, Speed: 0.450 steps/second
I1115 15:11:15.348102 140478266419072 run.py:286] Saved checkpoint to /content/drive/MyDrive/Colab_Work/HONORS/ColTran-v2/google-research/coltran/logs/cityscapes_ckpt/model-22
I1115 15:14:58.002671 140478266419072 run.py:282] Loss: 0.542 bits/dim, Speed: 0.449 steps/second
I1115 15:18:40.301140 140478266419072 run.py:282] Loss: 0.501 bits/dim, Speed: 0.450 steps/second
I1115 15:22:22.593688 140478266419072 run.py:282] Loss: 0.528 bits/dim, Speed: 0.450 steps/second
I1115 15:26:05.144472 140478266419072 run.py:282] Loss: 0.499 bits/dim, Speed: 0.449 steps/second
```
- How long / which step / loss value should I continue to train?
- Any parameters which need to be changed? (I did change the batch size as mentioned here #838)

Next, after training on a custom dataset, 
how to evaluate the model or obtain the results of colorized/recolorized images?

I used this cmd, but I guess it works only for imagenet dataset.

```
python -m coltran.run --config=coltran/configs/colorizer.py --mode=eval_valid --logdir=$LOGDIR --dataset=custom --data_dir=$EVAL_DATA_DIR 
```

Now, I am trying to use this (see the notebook for the next 2 steps)

```
python -m coltran.custom_colorize --config=coltran/configs/colorizer.py --logdir=$LOGDIR --img_dir=$IMG_DIR --store_dir=$STORE_DIR --mode=$MODE
```

Can someone please tell me if I am following the correct commands for getting the output?
A step-by-step guide would be appreciated. I am getting confused about which flow to follow.

- Is there any recommended way or package to calculate the FID scores to compare the results with the paper?

Also, the paper mentions 3 different coloured outputs for one input bnw image. 
![image](https://user-images.githubusercontent.com/42700922/141965832-818ac5a5-ee6e-4649-9794-e85d9434a555.png)
How to get such results?


"
"Thanks for the great work, i m wondering where is the cosine distance implementation you mention in appendix c.3? Also, it seem like you use a Taylor approximation way to calculate the dot product, is there a none Taylor approximation way to calculate the dot product? where one has higher acc?"
"surely ""rater-agreement""?"
"Hi all

I'm looking to [package](https://github.com/conda-forge/staged-recipes/pull/16944) scann for conda-forge, which would make installation a lot easier (ex. #779 #782). However, there are no sources available on PyPI, which makes this process much harder.

In fact, the best (from the POV of a packager) would be to split off scann into its own repository, but I'd be happy with the sources already. :)"
""
"@rybakov 
I have a dataset _folder1_ which contains `_background_noise_ ` folder and several other folders corresponding to `wanted_words` and some folders which doesn't belong to `wanted_words`
I have a CRNN model which was trained on the dataset _folder1_ with split_data = 1 (so code automatically splits data in test ,valid, train sets and creates two extra labels `_unknown_` and `_silence_`  + `wanted_words`).

Now, I have dataset _folder2_  with similar structure as _folder1_  which contains other wave files which I intent to test the model. 
I want to run some tests from [test.py](https://github.com/google-research/google-research/blob/master/kws_streaming/train/test.py) to check the accuracy of dataset _folder2_ 
How should I use the functions 
```
test.tf_non_stream_model_accuracy(flags, ...)                # (the flags was used from the previously trained model)
test.tf_stream_state_internal_model_accuracy(flags, ..)
```

to check the accuracy on the dataset _folder2_. What changes in flags do I need to make?
"
"I tried to run the TFT model using the electricity dataset, but I got this error
module 'tensorFlow has no attribute 'variable_scope.'
I updated tf.variable_scope   ----> tf.compat.v1.variable_scope
 Also, I used the automatic update mechanism, but without luck.
Any help."
"I am so interested in google pathways and already have some work on a similar net.
I can't get the registered SMS for Twitter, looking forward to your reply.

https://github.com/FFiot/HWlayer_V2
https://github.com/FFiot/HWnet"
"Hi, 
    Thanks for sharing the repo. as mentioned in the paper that FisherBRC is the same as BRAC. Both need behavior cloning to be run first. However, in the README documentation, it only mentioned running `train_offline.py`.  And it is different from BRAC, in BRAC, if BC is not run first BRAC cannot be run. Nevertheless, I could directly run `fbrc` without bc run before. 
    Can you clarify the usage?"
"![image](https://user-images.githubusercontent.com/47161914/140963451-4fbd894a-08f8-4d49-8718-c65894cf47e2.png)

I ran DVRL with fashion mnist, with 10000 flipped labels I introduced. I would expect some fraction of these to crop up as negative data values (harm the predicting model) but all are positive. Can negative values be output by this set-up?

![image](https://user-images.githubusercontent.com/47161914/140963874-2bc74902-2edc-4a56-95f9-2b5fc75818bf.png)

"
"Hi @simonster , when using CKA across models, which axis corresponds to which model on the generated heatmap, based on the function signature `update_state_across_models(self, activations1, activations2)`?

In case its relevant, i'm plotting with
```
plt.imshow(heatmap)
plt.gca().invert_yaxis()
```

Thanks!"
"Hey, 
I am trying to use the temporal fusion transformer for time series prediction. 
I would like to predict exact futurevalues, not only the P10, P50 and P90-values. Till now, i haven't found a possibility to do so. 
Is there any opportunity to predict the series? 

My idea so far was, to switch the loss to ""mse"" and change the data preparation. But I could not find a answer to the following question: Where and why are the target values processed to the shape ""3"" in z-direction? 

Thanks for your help!
Hinnerk8"
"Hi, 
I have a problem with training the model using GPU, I got this error (module 'tensorflow' has no attribute 'ConfigProto') even If I select the (tf_device=""CPU"") it gives me the same error.
I replaced tf.ConfigProto by tf.compat.v1.ConfigProto
Also, I tried--->  import tf.compat.v1 as tf instead of ---> import tensorflow as tf

Any suggestion?"
"In Table 1 of your paper, how can you directly take results from other papers? How do you know the set of random seeds is the same as theirs? DRL is notorious for its non-reproducibility, slight change on the set of random seeds could lead to a great difference in performance. This problem is addressed in several papers, could you justify this?"
"Hi!

thanks for releasing your implementation of MuZero. I'm trying to run MuZero on my laptop. According to your instructions, it should work with SEED RL however the instructions are not clear enough. It's not obvious where to put each file and which scripts to run.

I managed to run MuZero with SEED RL in some way, but it's not working. All libraries load, variables are initialized, the training loop starts, but the actor freezes on GRPC call of initial_inference function. Here are logs from [learner](https://pastebin.com/aGQNNSdi) and [actor](https://pastebin.com/vgpm2KhQ)

Could you provide more detailed instructions on how to run MuZero with SEED RL?

I found out that requirements.txt of MuZero is missing three libraries (inflection, tf-agents, ale-py). This occurs when I try to run MuZero on Atari. I ran `pip install -r muzero/requirements.txt` inside [this](https://github.com/google-research/seed_rl/blob/master/docker/Dockerfile.atari) docker image and the code for MuZero did not load correctly unless I installed those 3 libraries.

Could you provide an output of `pip freeze` of a python env in which MuZero runs correctly? I'm providing a `pip freeze` of my env [here](https://pastebin.com/2hXitfAZ)."
"I cloned the entire repository (both on my system and in Gitpod) and executed:  
`gcc -std=c++14 dynamic_submodular_maximization_main.cc`  
from its directory.  

gcc version:
```
gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
```
I am always getting the following error. Please help.
```
/usr/bin/ld: /tmp/ccAA286T.o: in function `insertInOrderThenDeleteLargeToSmall(SubmodularFunction&, Algorithm&)':
dynamic_submodular_main.cc:(.text+0x316): undefined reference to `SubmodularFunction::DeltaAndIncreaseOracleCall(int) const'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x340): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x34b): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: /tmp/ccAA286T.o: in function `main':
dynamic_submodular_main.cc:(.text+0x6d6): undefined reference to `OurSimpleAlgorithm::OurSimpleAlgorithm(double)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x6f1): undefined reference to `OurSimpleAlgorithm::OurSimpleAlgorithm(double)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x700): undefined reference to `std::allocator<char>::allocator()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x71d): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, std::allocator<char> const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x736): undefined reference to `GraphUtility::GraphUtility(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x745): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x754): undefined reference to `std::allocator<char>::~allocator()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x90f): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x914): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9ae): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9b3): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9c6): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9d0): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0x9db): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0xb7b): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0xb93): undefined reference to `std::allocator<char>::~allocator()'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__static_initialization_and_destruction_0(int, int)':
dynamic_submodular_main.cc:(.text+0xcd8): undefined reference to `std::ios_base::Init::Init()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text+0xced): undefined reference to `std::ios_base::Init::~Init()'
/usr/bin/ld: /tmp/ccAA286T.o: in function `RandomHandler::CheckRandomNumberGenerator()':
dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x1f): undefined reference to `RandomHandler::generator_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x31): undefined reference to `RandomHandler::generator_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x53): undefined reference to `std::cerr'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x58): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN13RandomHandler26CheckRandomNumberGeneratorEv[_ZN13RandomHandler26CheckRandomNumberGeneratorEv]+0x67): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SubmodularFunction::~SubmodularFunction()':
dynamic_submodular_main.cc:(.text._ZN18SubmodularFunctionD0Ev[_ZN18SubmodularFunctionD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Algorithm::~Algorithm()':
dynamic_submodular_main.cc:(.text._ZN9AlgorithmD0Ev[_ZN9AlgorithmD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SieveStreaming::SieveStreaming()':
dynamic_submodular_main.cc:(.text._ZN14SieveStreamingC2Ev[_ZN14SieveStreamingC5Ev]+0x1f): undefined reference to `vtable for SieveStreaming'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SimpleGreedy::SimpleGreedy()':
dynamic_submodular_main.cc:(.text._ZN12SimpleGreedyC2Ev[_ZN12SimpleGreedyC5Ev]+0x1f): undefined reference to `vtable for SimpleGreedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Greedy::Greedy()':
dynamic_submodular_main.cc:(.text._ZN6GreedyC2Ev[_ZN6GreedyC5Ev]+0x1f): undefined reference to `vtable for Greedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void runExperimentForAlgorithms<double (SubmodularFunction&, Algorithm&, int), int>(double ( const&)(SubmodularFunction&, Algorithm&, int), SubmodularFunction&, std::vector<std::reference_wrapper<Algorithm>, std::allocator<std::reference_wrapper<Algorithm> > > const&, std::vector<int, std::allocator<int> > const&, int)':
dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4c): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x51): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x87): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <char, std::char_traits<char>, std::allocator<char> >(std::basic_ostream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x96): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0xa2): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x129): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x12e): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x164): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <char, std::char_traits<char>, std::allocator<char> >(std::basic_ostream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x173): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x17f): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x217): undefined reference to `std::cerr'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x21c): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x22f): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x239): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x244): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x250): undefined reference to `RandomHandler::generator_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x286): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x2da): undefined reference to `SubmodularFunction::oracle_calls_'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x34a): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x34f): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x39f): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3a4): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3b3): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3de): undefined reference to `std::ostream::operator<<(double)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x3ed): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x400): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x40a): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x40f): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x41d): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x422): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x46e): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x473): undefined reference to `std::ostream::operator<<(int)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x482): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4ab): undefined reference to `std::ostream::operator<<(long)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4ba): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4cd): undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::endl<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&)'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4d7): undefined reference to `std::cout'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x4dc): undefined reference to `std::ostream::operator<<(std::ostream& (*)(std::ostream&))'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x51b): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_[_Z26runExperimentForAlgorithmsIFdR18SubmodularFunctionR9AlgorithmiEJiEEvRKT_S1_RKSt6vectorISt17reference_wrapperIS2_ESaISA_EERKS8_IiSaIiEEDpT0_]+0x539): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<std::pair<double, int>, std::allocator<std::pair<double, int> > >::_M_realloc_insert<double, int&>(__gnu_cxx::__normal_iterator<std::pair<double, int>*, std::vector<std::pair<double, int>, std::allocator<std::pair<double, int> > > >, double&&, int&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x279): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x2e1): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_[_ZNSt6vectorISt4pairIdiESaIS1_EE17_M_realloc_insertIJdRiEEEvN9__gnu_cxx17__normal_iteratorIPS1_S3_EEDpOT_]+0x2ed): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<double, std::allocator<double> >::_M_realloc_insert<double const&>(__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, double const&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJRKdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<double, std::allocator<double> >::_M_realloc_insert<double>(__gnu_cxx::__normal_iterator<double*, std::vector<double, std::allocator<double> > >, double&&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_[_ZNSt6vectorIdSaIdEE17_M_realloc_insertIJdEEEvN9__gnu_cxx17__normal_iteratorIPdS1_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<std::pair<double, int>, std::allocator<std::pair<double, int> > >::_M_check_len(unsigned long, char const*) const':
dynamic_submodular_main.cc:(.text._ZNKSt6vectorISt4pairIdiESaIS1_EE12_M_check_lenEmPKc[_ZNKSt6vectorISt4pairIdiESaIS1_EE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<double, std::allocator<double> >::_M_check_len(unsigned long, char const*) const':
dynamic_submodular_main.cc:(.text._ZNKSt6vectorIdSaIdEE12_M_check_lenEmPKc[_ZNKSt6vectorIdSaIdEE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<int, std::allocator<int> >::_S_check_init_len(unsigned long, std::allocator<int> const&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIiSaIiEE17_S_check_init_lenEmRKS0_[_ZNSt6vectorIiSaIiEE17_S_check_init_lenEmRKS0_]+0x62): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<std::reference_wrapper<Algorithm>, std::allocator<std::reference_wrapper<Algorithm> > >::_S_check_init_len(unsigned long, std::allocator<std::reference_wrapper<Algorithm> > const&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorISt17reference_wrapperI9AlgorithmESaIS2_EE17_S_check_init_lenEmRKS3_[_ZNSt6vectorISt17reference_wrapperI9AlgorithmESaIS2_EE17_S_check_init_lenEmRKS3_]+0x62): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<double>::deallocate(double*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIdE10deallocateEPdm[_ZN9__gnu_cxx13new_allocatorIdE10deallocateEPdm]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::pair<double, int> >::deallocate(std::pair<double, int>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt4pairIdiEE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorISt4pairIdiEE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<SieveStreaming::SingleThresholdSieve>::deallocate(SieveStreaming::SingleThresholdSieve*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIN14SieveStreaming20SingleThresholdSieveEE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorIN14SieveStreaming20SingleThresholdSieveEE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<int>::deallocate(int*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim[_ZN9__gnu_cxx13new_allocatorIiE10deallocateEPim]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::unique_ptr<SubmodularFunction, std::default_delete<SubmodularFunction> > >::deallocate(std::unique_ptr<SubmodularFunction, std::default_delete<SubmodularFunction> >*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt10unique_ptrI18SubmodularFunctionSt14default_deleteIS2_EEE10deallocateEPS5_m[_ZN9__gnu_cxx13new_allocatorISt10unique_ptrI18SubmodularFunctionSt14default_deleteIS2_EEE10deallocateEPS5_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `void std::vector<long, std::allocator<long> >::_M_realloc_insert<long>(__gnu_cxx::__normal_iterator<long*, std::vector<long, std::allocator<long> > >, long&&)':
dynamic_submodular_main.cc:(.text._ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_[_ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_]+0x267): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_[_ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_]+0x2d0): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_[_ZNSt6vectorIlSaIlEE17_M_realloc_insertIJlEEEvN9__gnu_cxx17__normal_iteratorIPlS1_EEDpOT_]+0x2dc): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::pair<double, int> >::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt4pairIdiEE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<double>::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIdE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<int>::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIiE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::reference_wrapper<Algorithm> >::deallocate(std::reference_wrapper<Algorithm>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::reference_wrapper<Algorithm> >::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorISt17reference_wrapperI9AlgorithmEE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<long>::deallocate(long*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIlE10deallocateEPlm[_ZN9__gnu_cxx13new_allocatorIlE10deallocateEPlm]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::vector<long, std::allocator<long> >::_M_check_len(unsigned long, char const*) const':
dynamic_submodular_main.cc:(.text._ZNKSt6vectorIlSaIlEE12_M_check_lenEmPKc[_ZNKSt6vectorIlSaIlEE12_M_check_lenEmPKc]+0x5f): undefined reference to `std::__throw_length_error(char const*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node_base*>::deallocate(std::__detail::_Hash_node_base**, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorIPNSt8__detail15_Hash_node_baseEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `std::pair<double, int>* std::__uninitialized_copy<false>::__uninit_copy<std::move_iterator<std::pair<double, int>*>, std::pair<double, int>*>(std::move_iterator<std::pair<double, int>*>, std::move_iterator<std::pair<double, int>*>, std::pair<double, int>*)':
dynamic_submodular_main.cc:(.text._ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_[_ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_]+0x7f): undefined reference to `__cxa_begin_catch'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_[_ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_]+0x97): undefined reference to `__cxa_rethrow'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_[_ZNSt20__uninitialized_copyILb0EE13__uninit_copyISt13move_iteratorIPSt4pairIdiEES5_EET0_T_S8_S7_]+0xa3): undefined reference to `__cxa_end_catch'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node<std::pair<int const, int>, false> >::deallocate(std::__detail::_Hash_node<std::pair<int const, int>, false>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKiiELb0EEEE10deallocateEPS6_m[_ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeISt4pairIKiiELb0EEEE10deallocateEPS6_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::__detail::_Hash_node<int, false> >::deallocate(std::__detail::_Hash_node<int, false>*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeIiLb0EEEE10deallocateEPS3_m[_ZN9__gnu_cxx13new_allocatorINSt8__detail10_Hash_nodeIiLb0EEEE10deallocateEPS3_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<long>::allocate(unsigned long, void const*)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv]+0x30): undefined reference to `std::__throw_bad_alloc()'
/usr/bin/ld: dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv[_ZN9__gnu_cxx13new_allocatorIlE8allocateEmPKv]+0x40): undefined reference to `operator new(unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x10): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x18): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x20): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x28): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x30): undefined reference to `__cxa_pure_virtual'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTV9Algorithm[_ZTV9Algorithm]+0x38): more undefined references to `__cxa_pure_virtual' follow
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTI9Algorithm[_ZTI9Algorithm]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.ro._ZTI18SubmodularFunction[_ZTI18SubmodularFunction]+0x0): undefined reference to `vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SieveStreaming::~SieveStreaming()':
dynamic_submodular_main.cc:(.text._ZN14SieveStreamingD2Ev[_ZN14SieveStreamingD5Ev]+0x13): undefined reference to `vtable for SieveStreaming'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SieveStreaming::~SieveStreaming()':
dynamic_submodular_main.cc:(.text._ZN14SieveStreamingD0Ev[_ZN14SieveStreamingD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SimpleGreedy::~SimpleGreedy()':
dynamic_submodular_main.cc:(.text._ZN12SimpleGreedyD2Ev[_ZN12SimpleGreedyD5Ev]+0x13): undefined reference to `vtable for SimpleGreedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `SimpleGreedy::~SimpleGreedy()':
dynamic_submodular_main.cc:(.text._ZN12SimpleGreedyD0Ev[_ZN12SimpleGreedyD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Greedy::~Greedy()':
dynamic_submodular_main.cc:(.text._ZN6GreedyD2Ev[_ZN6GreedyD5Ev]+0x13): undefined reference to `vtable for Greedy'
/usr/bin/ld: /tmp/ccAA286T.o: in function `Greedy::~Greedy()':
dynamic_submodular_main.cc:(.text._ZN6GreedyD0Ev[_ZN6GreedyD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `OurSimpleAlgorithm::~OurSimpleAlgorithm()':
dynamic_submodular_main.cc:(.text._ZN18OurSimpleAlgorithmD2Ev[_ZN18OurSimpleAlgorithmD5Ev]+0x13): undefined reference to `vtable for OurSimpleAlgorithm'
/usr/bin/ld: /tmp/ccAA286T.o: in function `OurSimpleAlgorithm::~OurSimpleAlgorithm()':
dynamic_submodular_main.cc:(.text._ZN18OurSimpleAlgorithmD0Ev[_ZN18OurSimpleAlgorithmD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `GraphUtility::~GraphUtility()':
dynamic_submodular_main.cc:(.text._ZN12GraphUtilityD2Ev[_ZN12GraphUtilityD5Ev]+0x13): undefined reference to `vtable for GraphUtility'
/usr/bin/ld: /tmp/ccAA286T.o: in function `GraphUtility::~GraphUtility()':
dynamic_submodular_main.cc:(.text._ZN12GraphUtilityD0Ev[_ZN12GraphUtilityD5Ev]+0x29): undefined reference to `operator delete(void*, unsigned long)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<OurSimpleAlgorithm::OurSimpleAlgorithmSingleThreshold>::deallocate(OurSimpleAlgorithm::OurSimpleAlgorithmSingleThreshold*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorIN18OurSimpleAlgorithm33OurSimpleAlgorithmSingleThresholdEE10deallocateEPS2_m[_ZN9__gnu_cxx13new_allocatorIN18OurSimpleAlgorithm33OurSimpleAlgorithmSingleThresholdEE10deallocateEPS2_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<unsigned long>::deallocate(unsigned long*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorImE10deallocateEPmm[_ZN9__gnu_cxx13new_allocatorImE10deallocateEPmm]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o: in function `__gnu_cxx::new_allocator<std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > >::deallocate(std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> >*, unsigned long)':
dynamic_submodular_main.cc:(.text._ZN9__gnu_cxx13new_allocatorISt13unordered_setIiSt4hashIiESt8equal_toIiESaIiEEE10deallocateEPS7_m[_ZN9__gnu_cxx13new_allocatorISt13unordered_setIiSt4hashIiESt8equal_toIiESaIiEEE10deallocateEPS7_m]+0x20): undefined reference to `operator delete(void*)'
/usr/bin/ld: /tmp/ccAA286T.o:(.data.rel.local.DW.ref.__gxx_personality_v0[DW.ref.__gxx_personality_v0]+0x0): undefined reference to `__gxx_personality_v0'
collect2: error: ld returned 1 exit status
```"
"Hi,
Thank you very much about all the sharings! I have some problems with launching ieg. I have downloaded the code and run as required.

And I got the error:`AttributeError: 'NoneType' object has no attribute 'shape'`

when trying to execute the line in datasets.py:
`self.probe_size = x_probe.shape[0]`

from this:
```
出

    if not self.split_probe and x_probe is not None:
      # Usually used for supervised comparison.
      tf.logging.info('Merge train and probe')
      x_train = np.concatenate([x_train, x_probe], axis=0)
      y_train = np.concatenate([y_train, y_probe], axis=0)
      y_gold = np.concatenate([y_gold, y_probe], axis=0)

    conf_mat = sklearn_metrics.confusion_matrix(y_gold, y_train)
    conf_mat = conf_mat / np.sum(conf_mat, axis=1, keepdims=True)
    print('Corrupted confusion matirx\n {}'.format(conf_mat))
    x_test, y_test = shuffle_dataset(x_test, y_test)
    self.train_dataset_size = x_train.shape[0]
    self.val_dataset_size = x_test.shape[0]
    if self.split_probe:
      self.probe_size = x_probe.shape[0]
```
The reason for the error is that `x_probe = None`, then `self.probe_size = x_probe.shape[0]`, where the value of `self.split_probe` is True.

So I tried to assign the value of `self.split_probe` to False, but another error occurred：
`AttributeError: 'CIFAR' object has no attribute 'probe_dataflow'`

when trying to execute the line in model.py:
```
probe_ds = self.dataset.probe_dataflow.repeat().batch(
        self.batch_size,
        drop_remainder=True).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
```

I kindly ask for your further help in solving the problem.
Best Regards."
"I download the quantize-aware training int8 model(saved_model.pb) from repo `google-research/google-research`.And I use run_squad.py which offered in the repo with tensorflow 1.15. `https://github.com/google-research/google-research/tree/master/mobilebert/run_squad.py`.I also try to convert this model with tensorflow 2.2/2.3/2.5/2.6/2.8-nightly.But it didn't work anymore.
Could you please provide some information to help convert the model?"
"Hi,

I would like to report that, the recent commits to FELIX (https://github.com/google-research/google-research/commit/22119bcefacaf7ae696d3c9cb57f9760e135a622), and to `tensorflow/models` (https://github.com/tensorflow/models/commit/9b23daf9daaa1b44aee32e9f2e3b7ee5c9462b11#diff-163581806ba7ae5f9a9dc1f68f99befae3e361d4e702f5a31d0fceb4ae9a45bf) result in a breaking change in the import of `layers` modules.

FELIX asks for `tf-models-official==2.4.0` and imports `official.nlp.modeling.layers.position_embedding import PositionEmbedding`, but such class is not present in versions `2.4.0`, nor `2.5.0` and `2.6.0` (latest release in Aug.).
It is only present in the current master version of the repository (https://github.com/tensorflow/models/blob/ac7f9e7f2d0508913947242bad3e23ef7cae5a43/official/nlp/modeling/layers/__init__.py#L37) which is not released yet.

So for the FELIX codes to work, one should either use the old version of the codes or manually clone the master version.

"
"
The image ID of split part dont match id of mini-imagenet-annotations

So is not possible use this dataset. 

Someone can help me?

Thx"
"Dividing by the forbenius norm has broken [basic sanity checks](https://github.com/brando90/ultimate-anatome/issues/3) I expected for CCA. 

Does it also break things for CKA? What is the right normalization for CKA? Is dividing by the standard devation better?"
"hi, i want  convert realformer tf checkpoint to torch model, but get a error 
```
torch.nn.modules.module.ModuleAttributeError: 'LayerNorm' object has no attribute 'beta'
```
I think this is caused by changing the bert structure. 
i change convert code，but got a another error
```
AssertionError:
(torch.Size([21128, 768]), (30522, 768))
```

convert code:
```
for name, array in zip(names, arrays):
        name = name.split('/')
        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
        # which are not required for using pretrained model
        if name[-1] in [""adam_v"", ""adam_m""]:
            print(""Skipping {}"".format(""/"".join(name)))
            continue
        pointer = model
        for m_name in name:
            if re.fullmatch(r'[A-Za-z]+_\d+', m_name):
                l = re.split(r'_(\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'kernel':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'output_bias' or l[0] == 'beta':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'output_weights' or l[0] == 'gamma':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        if m_name[-11:] == '_embeddings':
            pointer = getattr(pointer, 'weight')
        elif m_name == 'kernel':
            array = np.transpose(array)
        try:
            assert pointer.shape == array.shape # error
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        print(""Initialize PyTorch weight {}"".format(name))
        pointer.data = torch.from_numpy(array)
```
Whether the vocabulary size is inconsistent？
"
I want to know how to transfer Gwikimatch which only include urls into tfrecord ?
"Some questions in the paper COMISR: Compression-Informed Video Super-Resolution.
The results are incredible. But I don't think the results of other models are containing so many artifacts like your exhibition in your paper if you retrain these models with compressed videos. I used to retrain EDVR and RSDN with compressed videos on CRF37, and the results are just over-smoothed instead of full of artifacts. Is it because something is wrong with my knowledge about that or you just use the trained model provided by their authors?"
"i'm stuck at step 6 sparse_convo_py_test in ops compilation folders 
here is the log please tell me what to do
2021-10-19 17:43:59.277603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-10-19 17:43:59.277846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-10-19 17:43:59.278272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
INFO:tensorflow:time(__main__.SparseConvOpTest.test_spar_conv_op): 0.48s
I1019 17:43:59.404408 140462161450752 test_util.py:1973] time(__main__.SparseConvOpTest.test_spar_conv_op): 0.48s
[  FAILED  ] SparseConvOpTest.test_spar_conv_op
======================================================================
ERROR: test_spar_conv_op (__main__.SparseConvOpTest)
SparseConvOpTest.test_spar_conv_op
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/39e26583fdab04cdafc9b2ab1c2b6d71/execroot/__main__/bazel-out/k8-opt/bin/sparse_conv_ops_py_test.runfiles/__main__/tensorflow_sparse_conv_ops/sparse_conv_ops_test.py"", line 32, in test_spar_conv_op
    dtype=tf.float32)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 264, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 275, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 300, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 97, in convert_to_eager_tensor
    ctx.ensure_initialized()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 539, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid

----------------------------------------------------------------------
Ran 2 tests in 0.487s

FAILED (errors=1, skipped=1)
"
"Thanks for your awesome work! 

I am confused about the order of training, which is described in the first paragraph in Part 4 of your paper as this ""In order to approximate regret, we use the difference between the payoffs of two agents acting under the same environment conditions. Assume we are given a fixed environment with parameters fixed policy for the protagonist agent, and we then train a second antagonist agent, πA, to optimality in this environment. Then, the difference between the reward obtained by the antagonist, and the protagonist.""

So, the env_adversary will first generate an env based on the fixed policy of the Protagonist, then the Antagonist will be trained to optimality on this env. After training the Antagonist, we will compute the reward obtained by the Antagonist (already been trained) and Protagonist (not yet been trained) on this env. However, in your implementation, I could not find the order of training follows this way. What I find is that in the run() function you run the three agents in order, and the Antagonist is not first being trained (https://github.com/google-research/google-research/blob/5a4d95be1ca8ca4335d42f5a0326d79262c6992e/social_rl/adversarial_env/adversarial_driver.py#L118). 

Could you please explain this a bit to me? Really appreciate it.


"
"Hi, 

Thanks for open-sourcing your MuZero implementation. I was wondering if you've benchmarked the implementation, and whether it reproduces some of the original Atari results. Also, are there any plans to implement ReAnalyse? cc @galmacky @mochi1219 @sgirgin "
"Hi, I have a question about the `dual_pixels` dir.
It is the official imprementations of the paper [""Learning Single Camera Depth Estimation using Dual-Pixels""](https://arxiv.org/abs/1904.05822).

Maybe, I could see only the following functions.

- Evaluation for the results predicted previously.
- Android app to capture dual-pixel photos using some Google smartphone.

I could NOT see the following functions.

- Model/DNN-design definition.
- Prediction for other dual-pixel photos.
- Training/finetuning using other datasets.

Why they have not been published?
or
If they have been published at another repository, would you teach me it?

Thanks for your great works!"
"I'd like to know which version of tokenizers you use, couldn't find it in requirements.txt

The following import fails:

`from tokenizers import BertWordPieceTokenizer`

ModuleNotFoundError: No module named 'tokenizers'"
您好，我在跑kws_streaming这个项目时，　依赖包tf_nightly==2.3.0.dev20200515无法安装，请问怎么解决呢
"[https://github.com/google-research/google-research/blob/25b69734731eee63b1da062562ddfb6a77171a26/tf3d/instance_segmentation/scripts/scannet_scene/run_train_locally.sh#L40](https://github.com/google-research/google-research/blob/25b69734731eee63b1da062562ddfb6a77171a26/tf3d/instance_segmentation/scripts/scannet_scene/run_train_locally.sh#L40)

Hello,

Any chance that you know what happened here. Under the task of instance segmentation, it's using the config/model of semantic segmentation. 

I am trying to understand the code here. And this line is very confusing to me. Further clarification would be appreciated!

All the best,
Weiwei.
 "
"Good morning, I'm trying to run the example code in BLUR project called Learning_Boolean_Functions_Using_BLUR.ipynb, but when I try to run Gradient descent on genome section, I get this error: AttributeError: module 'blur.genome_util' has no attribute 'GRADIENT_GENOME'. I read the genome_util.py file and there's NO a Gradient_genome method or process or whatever there, and I can't continue with the execution. ¿What can I do? ¿Where did that attribute go? I'll really appreciate help because is for my homework. Thanks."
"Hi, ive gone through the repo and it seems that only the pose invariant embedding algorithm is implemented. Is there any implementation of video alignment and action recognition algo as well? Thanks..."
"This references https://github.com/google-research/google-research/commit/e4154f628b12c819c8b1383d0535774f69383e12

I followed the instructions on how to set up TF3D and TF3D ops as described in https://github.com/google-research/google-research/blob/master/tf3d/doc/setup.md and  https://github.com/google-research/google-research/tree/master/tf3d/ops/setup.md and got as far as step 6 in setup.md and try to run ""configure.sh"" when it gives this error message.  

![image](https://user-images.githubusercontent.com/4442406/134961287-989d3e9b-b011-49cd-91cd-1aee3e3c0996.png)

Please note that the existing version of numpy provided in the tensorflow/tensorflow:2.3.0-custom-op-gpu-ubuntu16 docker image is  numpy-1.18.5, and tensorflow 2.3.0 requires numpy version <1.19.0.  If I try to force a higher version of numpy, it still fails.  "
"Hi, 
The CKA described [here](https://github.com/google-research/google-research/blob/master/representation_similarity/Demo.ipynb) works on n x f matrices. I am not sure how to calculate it for neural network layers. For example, if we take all elements of activation tensors as features, then the dimension would be N x (C*H*W) which is huge for working matrices to fit into memory. Other ways can be to take spatial mean(N x C) or channel mean(N x H*W). Does someone know what preprocessing is done to calculate CKA on neural network like resnets"
" Hello, I am trying to use DVRL and apply it to the data evaluation for text classification.

Besides the blog and adult dataset, I add another two public datasets: sst-2, AG News for experiments.
For simplicity and consistency, I use pre-trained sentence encoder to project each sample of text into embedding, which is of 1024 dimension.(I have validate the effectiveness of the projector and if I use a simple one-layer network on top of it, the accuracy is ok, about 0.8+).

Other settings follow the script of https://github.com/google-research/google-research/blob/master/dvrl/main_data_valuation.py

I use AUC to measure the performance of DVRL, because clean samples are expected to be ranked higher than corrupted ones, in terms of the scores from DVE network. AUC measures the ranking ability. (the marks of clean samples are 1 while 0 for corrupted samples, whose labels are distorted intentionally)

This is my implementation: https://github.com/yananchen1989/topic_classification_augmentation/blob/main/dvrl_repo/main_data_valuation.py  

I run each dataset multiple times, with the exactly same settings.

```
for i in {1..12}
do
	#seed=$RANDOM

    python -u main_data_valuation.py --inner_iterations 100 --batch_size 256 \
        --batch_size_predictor 256 --iterations ${1} \
        --learning_rate 0.01  --perf_metric accuracy \
       --train_no 512 --valid_no 64 --dsn ${2} > dvrl.${2}.${1}.${i}.log 
done
```

However, I find that the auc is not stable each time.
For the AG and SST-2 datasets, here is the metrics of all trials.

![image](https://user-images.githubusercontent.com/26405281/134842551-5dc96346-b0e1-494a-a19c-645401df9c5e.png)

The blog and adult datasets bear the same outcomes.

My question is how to avoid this unstability and what is the cause for that ?

Thanks. 
"
"Hi @hyhieu, thanks for sharing this neat idea with academia.

Currently, I am applying the idea of MPL for my own project, and having trouble with computing the gradients for the 
teacher network.

In the original paper, the gradient for updating the teacher network is defined as below. 
![image](https://user-images.githubusercontent.com/20310517/134666637-93dd867f-a101-4873-9024-755cea27dc9d.png)

From the fruitful [discussion from other researchers,](https://github.com/google-research/google-research/issues?q=mpl), I found that `dot_product` term ('h' in the paper) is related to the Taylor series approximation. 

However, unlike the paper [implementation](https://github.com/google-research/google-research/blob/7c88de21b15ceb0161ac3b3a604a437c873aefce/meta_pseudo_labels/training_utils.py#L424) , the term is 
subtracted in the Taylor is computed via 'labeled' samples and labels, while the original paper claims that term should be computed on the unlabeled samples with pseudo labels.

I assume the code implementation is correct, but the paper still makes me suspicious about the term.
I wonder if the paper has typos in explaining the term 'h'.

Thanks in advance."
How do I find this module? Is there an implementation code for PARENT？
""
"Hi, 
Thank you very much about all the sharings! I am currently trying to work on TCC - Temporal Cycle-Consistency Learning - but there is a technical question I can't figure out.
What exactly does the network sees? At first I thought it was pairs of videos of the same class, but the loss takes in account 1 batch of samples.

It seems that the loss is computed over all pairs of samples in the batch, but what is the condition on the samples of the batch? In particular, in the paper it is said that ""The core contribution of this work is a self-supervised
approach to learn an embedding space where two similar
video sequences can be aligned temporally."". So I would find it weird to align 2 videos that aren't similar (like pouring/tennis serve).

Do you know where in the code can I find information about such stuff? I am not really familiar with tensorflow :("
"I have been trying to implement the sparse ops for a 3D object detection but get stuck in step 5, I kindly ask for your further help in solving the problem.
Best Regards. 
![20210912_045039](https://user-images.githubusercontent.com/72381301/133275944-61b74305-905d-48d7-983c-9c3dfbc5c86d.jpg)
![Uploading 20210911_033838.jpg…]()
"
"flax.nn is deprecated but ptopk_patch_selection still use it, so it is better upgrade the code to falx.linen"
@rybakov Can you please provide me the trained model files of CRNN model of KWS_Streaming. I couldn't train it due to colab and PC limitations. I need it urgently please.
"When I use felix model training Discofuse dataset, the model's train loss will rise in the last epoch no matter which size of epoch I chosen.  My training parameters : num_training_examples = 450,000 ,num_eval_examples = 20,000, batch_size = 32, max_seq_length=128, lr = 0.00003"
"Hi @rybakov , 

I am trying to follow the steps for svdf_resnet [here for quantization](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_30k_12_labels.md) with the below command on custom data. 

Below is the command used:
```
$CMD_TRAIN \
--wanted_words 'srewai' \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/svdf_resnet/ \
--mel_upper_edge_hertz 7600 \
--how_many_training_steps 20000,20000,20000,20000 \
--learning_rate 0.001,0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 80 \
--dct_num_features 40 \
--resample 0.15 \
--time_shift_ms 100 \
--feature_type 'mfcc_op' \
--fft_magnitude_squared 1 \
--preprocess 'raw' \
--train 1 \
--lr_schedule 'exp' \
svdf_resnet \
--block1_memory_size '7' \
--block2_memory_size '7' \
--block3_memory_size '11,11' \
--block1_units1 '32' \
--block2_units1 '50' \
--block3_units1 '50,128' \
--blocks_pool '2,2,1' \
--use_batch_norm 1 \
--bn_scale 1 \
--activation 'relu' \
--svdf_dropout 0.0 \
--svdf_pad 1 \
--svdf_use_bias 0 \
--dropout1 0.0 \
--units2 '64' \
--flatten 0
```
The model summary looks like this.
```
Instructions for updating:
Colocations handled automatically by placer.
W0903 18:00:45.072718 140373190989632 deprecation.py:347] From /srewai-venv/lib/python3.6/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(100, 16000)]       0           []                               
__________________________________________________________________________________________________
speech_features (SpeechFeature  (100, 49, 40)        0           ['input_1[0][0]']                
s)                                                                                                
__________________________________________________________________________________________________
dense_1 (Dense)                 (100, 49, 32)        1280        ['speech_features[0][0]']        
__________________________________________________________________________________________________
svdf_1_0 (Svdf)                 (100, 49, 32)        1632        ['speech_features[0][0]']        
__________________________________________________________________________________________________
batch_normalization_1 (BatchNo  (100, 49, 32)        128         ['dense_1[0][0]']                
rmalization)                                                                                      
__________________________________________________________________________________________________
add (Add)                       (100, 49, 32)        0           ['svdf_1_0[0][0]',               
                                                                  'batch_normalization_1[0][0]']  
__________________________________________________________________________________________________
activation (Activation)         (100, 49, 32)        0           ['add[0][0]']                    
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (100, 24, 32)        0           ['activation[0][0]']             
__________________________________________________________________________________________________
dense_3 (Dense)                 (100, 24, 50)        1600        ['max_pooling1d[0][0]']          
__________________________________________________________________________________________________
svdf_2_0 (Svdf)                 (100, 24, 50)        2150        ['max_pooling1d[0][0]']          
__________________________________________________________________________________________________
batch_normalization_3 (BatchNo  (100, 24, 50)        200         ['dense_3[0][0]']                
rmalization)                                                                                      
__________________________________________________________________________________________________
add_1 (Add)                     (100, 24, 50)        0           ['svdf_2_0[0][0]',               
                                                                  'batch_normalization_3[0][0]']  
__________________________________________________________________________________________________
activation_1 (Activation)       (100, 24, 50)        0           ['add_1[0][0]']                  
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (100, 11, 50)        0           ['activation_1[0][0]']           
__________________________________________________________________________________________________
svdf_3_0 (Svdf)                 (100, 11, 50)        3250        ['max_pooling1d_1[0][0]']        
__________________________________________________________________________________________________
dense_6 (Dense)                 (100, 11, 128)       6400        ['max_pooling1d_1[0][0]']        
__________________________________________________________________________________________________
svdf_3_1 (Svdf)                 (100, 11, 128)       8320        ['svdf_3_0[0][0]']               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNo  (100, 11, 128)       512         ['dense_6[0][0]']                
rmalization)                                                                                      
__________________________________________________________________________________________________
add_2 (Add)                     (100, 11, 128)       0           ['svdf_3_1[0][0]',               
                                                                  'batch_normalization_6[0][0]']  
__________________________________________________________________________________________________
activation_2 (Activation)       (100, 11, 128)       0           ['add_2[0][0]']                  
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (100, 9, 128)        0           ['activation_2[0][0]']           
__________________________________________________________________________________________________
global_average_pooling1d (Glob  (100, 128)           0           ['max_pooling1d_2[0][0]']        
alAveragePooling1D)                                                                               
__________________________________________________________________________________________________
dropout (Dropout)               (100, 128)           0           ['global_average_pooling1d[0][0]'
                                                                 ]                                
__________________________________________________________________________________________________
dense_7 (Dense)                 (100, 64)            8256        ['dropout[0][0]']                
__________________________________________________________________________________________________
dense_8 (Dense)                 (100, 3)             195         ['dense_7[0][0]']                
==================================================================================================
Total params: 33,923
Trainable params: 32,983
Non-trainable params: 940
__________________________________________________________________________________________________
I0903 18:00:45.455425 140373190989632 train.py:71] None
```

But it fails at the time of converting into tflite stream. Here is the error:

```
2021-09-03 18:22:54.631382: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2021-09-03 18:22:54.634525: E tensorflow/core/grappler/grappler_item_builder.cc:669] Init node svdf_1_0/dense/kernel/Assign doesn't exist in graph
I0903 18:22:54.659212 140426826090304 lite.py:1723] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
2021-09-03 18:22:54.663914: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.
2021-09-03 18:22:54.663933: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.
2021-09-03 18:22:54.698313: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1855] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):
Flex ops: FlexAudioSpectrogram, FlexMfcc
Details:
	tf.AudioSpectrogram(tensor<16000x1xf32>) -> (tensor<1x49x513xf32>) : {device = """", magnitude_squared = true, stride = 320 : i64, window_size = 640 : i64}
	tf.Mfcc(tensor<1x49x513xf32>, tensor<i32>) -> (tensor<1x49x40xf32>) : {dct_coefficient_count = 40 : i64, device = """", filterbank_channel_count = 80 : i64, lower_frequency_limit = 2.000000e+01 : f32, upper_frequency_limit = 7.600000e+03 : f32}

******snipped****

I0903 18:01:06.509099 140373190989632 test.py:510] tflite test accuracy, non stream model = 74.63% 200 out of 221
I0903 18:01:06.552580 140373190989632 test.py:514] tflite Final test accuracy, non stream model = 75.11% (N=221)
I0903 18:01:06.553761 140373190989632 model_train_eval.py:257] run TFlite streaming model accuracy evaluation
W0903 18:01:07.091468 140373190989632 test.py:560] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: Negative dimension size caused by subtracting 3 from 1 for '{{node streaming/max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=""NHWC"", explicit_paddings=[], ksize=[1, 3, 1, 1], padding=""VALID"", strides=[1, 2, 1, 1]](streaming/max_pooling1d/ExpandDims)' with input shapes: [1,1,1,32].
I0903 18:01:07.091856 140373190989632 test.py:364] tflite stream model state external with reset_state 1
I0903 18:01:07.342433 140373190989632 model_train_eval.py:282] FAILED to run TFLite streaming: Mmap of '7' at offset '0' failed with error '22'.

```
Below is my environment :

```
Package                       Version
----------------------------- -------------------
absl-py                       0.13.0
argon2-cffi                   21.1.0
astunparse                    1.6.3
async-generator               1.10
attrs                         21.2.0
backcall                      0.2.0
bleach                        4.1.0
cached-property               1.5.2
cachetools                    4.2.2
certifi                       2021.5.30
cffi                          1.14.6
charset-normalizer            2.0.4
cycler                        0.10.0
dataclasses                   0.8
decorator                     5.0.9
defusedxml                    0.7.1
dm-tree                       0.1.6
entrypoints                   0.3
flatbuffers                   1.12
gast                          0.4.0
google-auth                   1.35.0
google-auth-oauthlib          0.4.5
google-pasta                  0.2.0
graphviz                      0.17
grpcio                        1.39.0
h5py                          3.1.0
idna                          3.2
importlib-metadata            4.8.1
ipykernel                     5.5.5
ipython                       7.16.1
ipython-genutils              0.2.0
ipywidgets                    7.6.4
jedi                          0.18.0
Jinja2                        3.0.1
jsonschema                    3.2.0
jupyter                       1.0.0
jupyter-client                7.0.2
jupyter-console               6.4.0
jupyter-core                  4.7.1
jupyterlab-pygments           0.1.2
jupyterlab-widgets            1.0.1
keras-nightly                 2.7.0.dev2021083107
Keras-Preprocessing           1.1.2
kiwisolver                    1.3.1
libclang                      11.1.0
Markdown                      3.3.4
MarkupSafe                    2.0.1
matplotlib                    3.3.4
mistune                       0.8.4
nbclient                      0.5.4
nbconvert                     6.0.7
nbformat                      5.1.3
nest-asyncio                  1.5.1
notebook                      6.4.3
numpy                         1.19.5
oauthlib                      3.1.1
opt-einsum                    3.3.0
packaging                     21.0
pandocfilters                 1.4.3
parso                         0.8.2
pexpect                       4.8.0
pickleshare                   0.7.5
Pillow                        8.3.1
pip                           21.2.4
prometheus-client             0.11.0
prompt-toolkit                3.0.20
protobuf                      3.17.3
ptyprocess                    0.7.0
pyasn1                        0.4.8
pyasn1-modules                0.2.8
pycparser                     2.20
pydot                         1.4.2
Pygments                      2.10.0
pyparsing                     2.4.7
pyrsistent                    0.18.0
python-dateutil               2.8.2
pyzmq                         22.2.1
qtconsole                     5.1.1
QtPy                          1.10.0
requests                      2.26.0
requests-oauthlib             1.3.0
rsa                           4.7.2
scipy                         1.5.4
Send2Trash                    1.8.0
setuptools                    57.4.0
six                           1.15.0
tb-nightly                    2.6.0a20210806
tensorboard-data-server       0.6.1
tensorboard-plugin-wit        1.8.0
tensorflow-addons             0.14.0
tensorflow-model-optimization 0.6.0
termcolor                     1.1.0
terminado                     0.11.1
testpath                      0.5.0
tf-estimator-nightly          2.7.0.dev2021083108
tf-nightly                    2.7.0.dev20210806
tornado                       6.1
traitlets                     4.3.3
typeguard                     2.12.1
typing-extensions             3.7.4.3
urllib3                       1.26.6
wcwidth                       0.2.5
webencodings                  0.5.1
Werkzeug                      2.0.1
wheel                         0.37.0
widgetsnbextension            3.5.1
wrapt                         1.12.1
zipp                          3.5.0
```
The data folder has 1-sec recordings as below:

```
data folder:
_background_noise_  bed  bird  cat  dog  happy  srewai  house  marvin   sheila  tree  wow
 ```

Please note that svdf works fine for me.


"
"Hi,

In the paper of ""Depth_from_video_in_the_wild"", you mentioned the calibration results from the EUROC dataset where you use a pinhole+radial distortion model to model the fisheye camera in the dataset. 

However, in the [code](https://github.com/google-research/google-research/blob/45eca1118642cc1257824856ac6e1ab0aa7bf299/depth_from_video_in_the_wild/motion_prediction_net.py#L23) itself, there seems to be a standard pinhole model predicted.

It doesn't seem trivial to me how to add the extra two distortion parameters into the model, because the unprojection operation does not have a closed-form solution and involves finding the root of a high-order polynomial. I appreciate if you could enlighten how do you implement that.

Thanks,
JD"
When will the musiq network code be open source
"I am trying https://github.com/google-research/google-research/blob/master/kws_streaming/colab/00_check_data.ipynb in colab.
I am getting error as :
ModuleNotFoundError                       Traceback (most recent call last)

<ipython-input-16-40e192295e39> in <module>()
      1 
----> 2 from kws_streaming.models import models
      3 from kws_streaming.models import utils
      4 from kws_streaming.layers.modes import Modes
      5 import tensorflow as tf

ModuleNotFoundError: No module named 'kws_streaming'
"
"I am trying to add tf_trees to my existing installation of tensorflow, but it is not working. when I try to import tf_trees, it says it is not installed.

I have installed tensorflow from pip and here are the details. so i tried to install it binary way. And downloaded tf_trees folder from repository through DownGit

bash-4.2$ python3
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux


bash-4.2$ uname -a
Linux inlpc9 5.10.9-1.el7.elrepo.x86_64 #1 SMP Mon Jan 18 17:47:08 EST 2021 x86_64 x86_64 x86_64 GNU/Linux


bash-4.2$ pip show tensorflow
Name: tensorflow
Version: 2.6.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/jshah/.local/lib/python3.8/site-packages
Requires: absl-py, google-pasta, wrapt, numpy, keras, wheel, tensorboard, gast, opt-einsum, flatbuffers, termcolor, grpcio, astunparse, h5py, keras-preprocessing, clang, typing-extensions, six, protobuf, tensorflow-estimator
Required-by: "
"the requirement of felix code confuse me and is not compatible with my environment.
Can we get origin tensorflow1.x code? Thanks a lot."
"Can't load TF2 TRILL model (non semantic speech benchmanrk) with MirroredStrategy.

```
hub.load(tfhub_model_path)
*** tensorflow.python.framework.errors_impl.InvalidArgumentError:  assertion failed: [Trying to access a placeholder that is not supposed to be executed. This means you are executing a graph generated from the cross-replica context in an in-replica context.]
	 [[node Assert/Assert (defined at /ec2-user-home/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_hub/module_v2.py:106) ]] [Op:__inference_restored_function_body_6478]

Function call stack:
restored_function_body
```

Works fine outside of the MirroredStrategy. Any solution?"
"Hi, I'm trying to install scann on OSX 11.5.2 (20G95) in the context of Anaconda jupyter notebooks 6.1.4

```
❯ pip install -q scann
ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann
```

Here's all my current tensorflow related versions:

```
tensorboard                        2.4.1
tensorboard-plugin-wit             1.8.0
tensorflow                         2.4.1
tensorflow-datasets                4.4.0
tensorflow-estimator               2.4.0
tensorflow-hub                     0.12.0
tensorflow-metadata                0.29.0
tensorflow-recommenders            0.5.1
```

has anyone had any luck installing on OSX?  Any hints about what I might be able to fix this?"
"Hi, I have a question about layer_utils.py which in google-research/state_of_sparsity/layers/utils.
I'm getting an error when I run this file,
![1629788769(1)](https://user-images.githubusercontent.com/72150671/130572235-62ed33f5-2a9d-40b1-b047-01895ecb825d.png)
So, I looked at the code in this file and found that line 21 contained commands in the TF2.0 environment, and line 22 ('tensorflow.contrib') contained in TF1.0. 


21     import tensorflow.compat.v1 as tf 
22     from tensorflow.contrib.eager.python import tfe as contrib_eager
23
24         from tensorflow.contrib.layers.python.layers import utils as layer_utils

here are my packages,

absl-py==0.12.0
numpy==1.19.5
six==1.15.0
tensorflow==2.6.0
tensor2tensor==1.15.7
Is there a problem with the version of my packages? What version do you use?
"
"Hello,

Can you please clarify your variable selection method, that is used in Temporal Fusion Transformer?

It seems that it has an enormously large number of trainable parameters for electricity dataset:
number of entities is 370, and hidden_layer_size is 160.
So in input we will have tensor of size (batch_size, n_timestaps, 370), and after ```get_tft_embeddings``` method tensor will be converted to (batch_size, n_timestaps, 160, 370). When the tensor is flattened it will have shape (batch_size, n_timestaps, 59200), and GRN layer has to process about 30 million of trainable parameters (59200 * 160 * 4). If we add sizes of GRN layers from each individual feature (160 * 160 * 4), the total number of parameters will be close to 70 million, which seems quite big in comparison with LSTM and MultiHeadAttention sizes (200k and 100k respectively).
"
"@eladeban,
      Recently I try to reproduce dual_dice algorithm on Reacher, but I can't get the results as you presented in the paper. The phenomena is quite strange: the estimate average step reward is quite close to the behavior policy's step reward rather than the target policy's. Since there is no source code for continuous action setting, could you help to provide more details? 
     i) when calculating \nu(s',a'),  I use (1/N)*\sum_{i=1}^{N}nu(s',\pi(s')) and try N=1,10,100 respectively, and the results are the same. How you calculate it?
    ii) For this setting, do you choose Fenchel conjugate trick or not?
   iii) Is any other tips I need to know?

   Look forward to your reply.
   Best wishes.
                       
"
"I want to try DSelect-k on my case, but I only see the implement layer with keras. 
Can you provide examples? Thanks
"
I have data from bert.How do I cluster my data with scann?
"The manylinux wheel appears to have an incompatibility with the tensorflow we run in the cluster. I've created a python virtual environment with TF 2.5 and numpy 1.19.2. Then downloaded the manylinux wheel 3.7 and pip installed it, with the following error on import:

<pre>
import scann
2021-08-16 16:27:37.804869: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jshleap/building/3.7/lib/python3.7/site-packages/scann/__init__.py"", line 2, in <module>
    from scann.scann_ops.py import scann_ops
  File ""/home/jshleap/building/3.7/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops.py"", line 26, in <module>
    ""cc/_scann_ops.so""))
  File ""/home/jshleap/building/3.7/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/jshleap/building/3.7/lib/python3.7/site-packages/scann/scann_ops/cc/_scann_ops.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb
</pre>"
"@saberkun @renjie-liu 
I would like to convert mobilebert to tflite format.
I use the quantized weight you offered in the repo with tensorflow 1.15. https://github.com/google-research/google-research/tree/master/mobilebert#pre-trained-checkpoints 

But the error occurs when I try to convert the model to int8 tflite model. 
I use the flag: `--use_post_quantization  --activation_quantization`

Log:
```
2021-08-13 09:03:07.437436: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2021-08-13 09:03:09.818900: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2021-08-13 09:03:09.818948: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 7077 nodes (-6520), 7317 edges (-6524), time = 1308.49194ms.
2021-08-13 09:03:09.818955: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 7077 nodes (0), 7317 edges (0), time = 426.431ms.
Traceback (most recent call last):
  File ""run_squad_ptq.py"", line 653, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""run_squad_ptq.py"", line 642, in main
    tflite_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0
```

I also try to convert the model in Tensorflow 2.3/2.4/2.6. In those versions, the model can convert successfully to tflite.
However, a runtime error occurs during inference.
```
RuntimeError: tensorflow/lite/kernels/dequantize.cc:75 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 2 (DEQUANTIZE) failed to prepare.
 ```
I've referred to #325. It still does not work.
I can convert model weights to integers, but whenever I want to convert activation error happened.

Another question: why the model which I converts to fp32 tflite model cannot show in Netron? Your models in tfhub can do. Are there any differences between tfhub models and https://github.com/google-research/google-research/tree/master/mobilebert#pre-trained-checkpoints? "
"I try to install scann using _`pip install scann`_, it give me the following error:

_ERROR: Could not find a version that satisfies the requirement scann (from versions: none)
ERROR: No matching distribution found for scann_

Here is package version in my env:

**tensorflow=2.6.0
tensorflow recommender=v0.5.2
tensorflow dataset=4.4.0**

It can install the scann on Colab and I used the same env in my local computer, BUT it did not work. Any suggestions are appreciated. "
"This is nothing serious, just raise this so you know. Fixing it would help to debug."
"I am trying to feed my point cloud data into the semantic segmentation sparseconvunet by modifying run_train_local.py and adding new siemens_spec.py, siemens_dataset_frame.py and siemens_train.gin. Waymo semantic segmentation can train smoothly but the siemens one is not working. 

I get stuck on this error: 

```
I0809 09:53:35.545849 140474592175936 train.py:117] Model fit starting for 100 epochs, 10                                                                    0 step per epoch, total batch size:2
I0809 09:53:36.200485 140474592175936 callback_utils.py:370] Saving ckpt for epoch: 0 at                                                                     /tmp/tf3d_experiment/seg_siemens_001/model
Epoch 1/100
Traceback (most recent call last):
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/runpy.py"", line 193, in _run_module_                                                                    as_main
    ""__main__"", mod_spec)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/sdl/assem/users/yvu2cv/tf3d/train.py"", line 222, in <module>
    app.run(main)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/absl/app.py"", line 303                                                                    , in run
    _run_main(main, args)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/absl/app.py"", line 251                                                                    , in _run_main
    sys.exit(main(argv))
  File ""/sdl/assem/users/yvu2cv/tf3d/train.py"", line 177, in main
    train(strategy=strategy, write_path=write_path)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/gin/config.py"", line 1                                                                    069, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/gin/utils.py"", line 41                                                                    , in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/gin/config.py"", line 1                                                                    046, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/sdl/assem/users/yvu2cv/tf3d/train.py"", line 124, in train
    verbose=1 if FLAGS.run_functions_eagerly else 2)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 806, in train_function
    return step_function(self, iterator)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/kera                                                                    s/engine/training.py"", line 795, in step_function
    data = next(iterator)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/dist                                                                    ribute/input_lib.py"", line 649, in __next__
    return self.get_next()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/dist                                                                    ribute/input_lib.py"", line 694, in get_next
    self._iterators[i].get_next_as_list_static_shapes(new_name))
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/dist                                                                    ribute/input_lib.py"", line 1474, in get_next_as_list_static_shapes
    return self._iterator.get_next()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/data                                                                    /ops/multi_device_iterator_ops.py"", line 581, in get_next
    result.append(self._device_iterators[i].get_next())
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/data                                                                    /ops/iterator_ops.py"", line 825, in get_next
    return self._next_internal()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/data                                                                    /ops/iterator_ops.py"", line 764, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/eage                                                                    r/context.py"", line 2105, in execution_mode
    executor_new.wait()
  File ""/u/yvu2cv/miniconda3/envs/tf3d/lib/python3.6/site-packages/tensorflow/python/eage                                                                    r/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Need minval < maxval, got 0                                                                     >= 0
         [[{{node random_uniform}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
  In call to configurable 'train' (<function train at 0x7fc2c2df4048>)

```

Does anyone have clue on what caused this? Any help will be appreciated! "
Hdhdh
"I am splitting my data into train, test, valid csvs and looking to load them to use with the DVRL package, but an extra column is being added at the front which leads to almost perfect accuracy (around 99.5%). Does anyone know what might be causing this?

![Snip20210728_9](https://user-images.githubusercontent.com/58536665/127398668-a16084be-dffe-4626-b518-5368f1868ab2.png)
"
"Hello, I have used adaptive loss implementation on a neural network, however after training a model long enough, I am getting negative loss values. Any help/suggestion would be highly appreciated! Please let me know if you need additional info

*Model definition -* 

####### define model
best_hyperparameter_space = {""gru_up"": 64, 
                             ""up_dropout"": 0.2,
                             ""learning_rate"": 0.004,
                             ""batch_size"": 1024}

def many_to_one_model(params):
  input_1 = tf.keras.Input(shape =(1, 53), name='input_1')

  input_2 = tf.keras.Input(shape=(1, 19), name='input_2') # this one is 
  
  input_3 = tf.keras.Input(shape=(1, 130), 
                                   name='input_3')
  input_3_flatten = Flatten()(input_3)
  input_3_flatten = RepeatVector(1)(input_3_flatten)
  
  concat_outputs = Concatenate()([input_1, 
                                  input_2, 
                                  input_3_flatten])

  output_1 = GRU(units=int(params['gru_up']), 
                              kernel_initializer=tf.keras.initializers.he_uniform(),
                              activation='relu')(concat_outputs)
  output_1 = Dropout(rate=float(params['up_dropout']))(output_1)
  output_1 = Dense(units=1, 
                               activation='linear', 
                               name='output_1')(output_1)

  model = tf.keras.models.Model(inputs=[input_1, 
                                        input_2, 
                                        input_3], 
                                outputs=[output_1],
                                name='many_to_one_model')

  return model


*Model summary -*

Model: ""many_to_one_model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 1, 130)]     0                                            
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 130)          0           input_3[0][0]                    
__________________________________________________________________________________________________
input_1 (InputLayer)            [(None, 1, 53)]      0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 1, 19)]      0                                            
__________________________________________________________________________________________________
repeat_vector_5 (RepeatVector)  (None, 1, 130)       0           flatten_5[0][0]                  
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1, 202)       0           input_1[0][0]                    
                                                                 input_2[0][0]                    
                                                                 repeat_vector_5[0][0]            
__________________________________________________________________________________________________
gru_5 (GRU)                     (None, 64)           51456       concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 64)           0           gru_5[0][0]                      
__________________________________________________________________________________________________
output_1 (Dense)                (None, 1)            65          dropout_5[0][0]                  
==================================================================================================
Total params: 51,521
Trainable params: 51,521
Non-trainable params: 0
__________________________________________________________________________________________________


*Adaptive loss implementation -*

####### Create the initial model object
model = many_to_one_model(best_hyperparameter_space)

####### Define robust loss function
adaptive_lossfun = robust_loss.adaptive.AdaptiveLossFunction(num_channels=1, float_dtype=np.float32)

variables = (list(model.trainable_variables) + list(adaptive_lossfun.trainable_variables))

####### Get dynamic param for learning rate
optimizer_call = getattr(tf.keras.optimizers, ""Adam"")         # Can update to meet needs
optimizer = optimizer_call(learning_rate=best_hyperparameter_space[""learning_rate""], amsgrad=True)

mlflow_callback = LambdaCallback()
for epoch in range(750):
  def lossfun():
    ####### Stealthily unsqueeze to an (n,1) matrix, and then compute the loss.
    ####### A matrix with this shape corresponds to a loss where there's one shape
    ####### and scale parameter per dimension (and there's only one dimension for
    ####### this data).
    aa = y_train_up - model([train_cat_ip, train_num_ip, ex_train_num_ip])
    mean_calc = tf.reduce_mean(adaptive_lossfun(aa))
    return mean_calc

  optimizer.minimize(lossfun, variables)

  loss = lossfun()
  alpha = adaptive_lossfun.alpha()[0, 0]
  scale = adaptive_lossfun.scale()[0, 0]
  print('{:<4}: loss={:+0.5f}  alpha={:0.5f}  scale={:0.5f}'.format(epoch, loss, alpha, scale))
  mlflow_callback.on_batch_end(epoch, mlflow.log_metrics({""loss"":loss.numpy(), 
                                                          ""alpha"":alpha.numpy(), 
                                                          ""scale"":scale.numpy()}, epoch))


*Loss, alpha and scale vs epochs graph -*

![image](https://user-images.githubusercontent.com/34693127/127228166-a490381b-1d3c-4603-b0cd-50488976f26c.png)
"
"Hello,

I have successfully completed the installation of Tensorflow 3D but I am unable to use the libraries through my depth camera and Lidar. Has anyone been able to use them and if so, how?
Can someone also share a demo that explains how to use datasets and for example object detection?

Thank you
"
"I tried to run the code on Google Colab and this is the output that I got -
```
2021-07-26 11:24:31.361832: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-26 11:24:32.907777: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-07-26 11:24:32.970575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:32.971227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-07-26 11:24:32.971284: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-26 11:24:33.154572: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-07-26 11:24:33.154694: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-07-26 11:24:33.259957: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-07-26 11:24:33.312886: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-07-26 11:24:33.559715: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10
2021-07-26 11:24:33.617689: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-07-26 11:24:33.623997: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-07-26 11:24:33.624181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.624909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.627938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-07-26 11:24:33.628872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.629559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5
coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s
2021-07-26 11:24:33.629649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.630282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:33.630836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-07-26 11:24:33.632529: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-26 11:24:38.774730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-07-26 11:24:38.774779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-07-26 11:24:38.774795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-07-26 11:24:38.774971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:38.775604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:38.776196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-26 11:24:38.776739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2021-07-26 11:24:38.776822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
```

The code literally stops after the last execution. There are no errors thrown at all. What can be the solution to this?

On a side note, when will the pre-trained model for the paper be released? My work just requires inference results for images on pre-trained models so it would be great if the authors can release the pre-trained implementation anytime soon."
"Hello,
I am trying to apply FAD but facing the Fatal Python error: Segmentation fault here.
Currently I am using tf 2.5.0 and apache-beam 2.24.0 with python 3.7.

I am stuck on the step > **Compute embeddings and eastimate multivariate Gaussians**
`python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats`

I found the problem **Fatal Python error** happened in `def create_pipeline()` of `create_embeddings_beam.py` :

> if files_input_list:
    examples = (
        pipeline
        | 'Read File List' >> ReadFromText(files_input_list)
        | 'Read Files' >> beam.ParDo(ReadWavFiles()))

Cause I am really not an expert using apache so couldn't figure it out what's going on. 

Also by googling it, it seems something wrong with stack memory which may cause the **Segmentation fault** problem, 
so I have tried to increase the stack that my operating system allocates for the python process ended by `ulimit -s 262140`.
But it's still not working.

I will be appreciate if someone has any ideas.
Thank you!!
"
Can 4 GPU Titan RTX train the model Coltran(Colorization Transformer)？Thank you for answer!
"Hello!
I have some problems with launching CuBERT. I've loaded the dataset and model for the variable misuse task using methods from bert (run_classifier.py).

And I got the error: `""ValueError: The initializer passed is not valid. It should be a callable with no arguments and the shape should not be provided or an instance of 'tf.keras.initializers.*' and 'shape' should be fully defined."" `

when trying to execute the line: 
`result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)`

from this:
```
from cubert_processors import VarmisuseProcessor
from bert.run_classifier import convert_single_example, file_based_convert_examples_to_features, file_based_input_fn_builder
from bert.run_classifier import PaddingInputExample

processor = VarmisuseProcessor()
eval_batch_size = batch_size
eval_examples = processor.get_test_examples(data_dir)
num_actual_eval_examples = len(eval_examples)

label_list = processor.get_labels()
convert_single_example(0, eval_examples[1], label_list, max_seq_length, full_tokenizer)

if is_use_tpu:
    while len(eval_examples) % eval_batch_size != 0:
        eval_examples.append(PaddingInputExample())

eval_file = os.path.join(output_dir, ""eval.tf_record"")
file_based_convert_examples_to_features(
    eval_examples, label_list, max_seq_length, full_tokenizer, eval_file)

tf.logging.info(""***** Running evaluation *****"")
tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                len(eval_examples), num_actual_eval_examples,
                len(eval_examples) - num_actual_eval_examples)
tf.logging.info(""  Batch size = %d"", eval_batch_size)

# This tells the estimator to run through the entire set.
eval_steps = None
# However, if running eval on the TPU, you will need to specify the
# number of steps.
if is_use_tpu:
    assert len(eval_examples) % eval_batch_size == 0
    eval_steps = int(len(eval_examples) // eval_batch_size)

eval_drop_remainder = True if is_use_tpu else False
eval_input_fn = file_based_input_fn_builder(
    input_file=eval_file,
    seq_length=max_seq_length,
    is_training=False,
    drop_remainder=eval_drop_remainder)

# evaluate all checkpoints; you can use the checkpoint with the best dev accuarcy
steps_and_files = []
model_dir = saved_model_path
filenames = tf.gfile.ListDirectory(model_dir)
for filename in filenames:
    if filename.endswith("".index""):
        ckpt_name = filename[:-6]
        cur_filename = os.path.join(model_dir, ckpt_name)
        global_step = int(cur_filename.split(""-"")[-1])
        tf.logging.info(""Add {} to eval list."".format(cur_filename))
        steps_and_files.append([global_step, cur_filename])
steps_and_files = sorted(steps_and_files, key=lambda x: x[0])
output_eval_file = os.path.join(output_dir, ""eval_results_albert_zh.txt"")
print(""output_eval_file:"", output_eval_file)
tf.logging.info(""output_eval_file:"" + output_eval_file)
with tf.gfile.GFile(output_eval_file, ""w"") as writer:
    for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):
        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)

        tf.logging.info(""***** Eval results %s *****"" % (filename))
        writer.write(""***** Eval results %s *****\n"" % (filename))
        for key in sorted(result.keys()):
            tf.logging.info(""  %s = %s"", key, str(result[key]))
            writer.write(""%s = %s\n"" % (key, str(result[key])))
```

Could someone tell me what I'm doing wrong?
"
"SCANN is not compiled because of error
proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:cc_toolchain' does not have mandatory provider 'ProtoInfo'"
"Would it be possible to create a simple custom training workflow for smurf and uflow?

Both these modules run into some or the other issues for a custom training.."
"Hi there, when I use TFT for custom datasets, I'm facing two questions, hope can find it out here, many thanks.
Q1: how to set the parameters ""total_time_steps"" and ""num_encoder_steps"" for custom datasets?
Q2: If I'm using some sales data, and different item has different time length, eg item1 sales 30days, item2 sales 20days, can TFT handle this?
Thanks"
"Install sosumi and replace common and find Jan Sloot code
https://files.fm/u/82zpbg9yc
https://uptobox.com/f9ugkg0vim8e
https://mega.nz/folder/3TgkXBLS#b9brcWFuyVaqhjx47FVZUQ
![Screenshot at 2021-07-02 18-16-46.png](https://user-images.githubusercontent.com/75278757/124348444-3f8d7f00-dbea-11eb-911e-f6b66fce20c3.png)![20210703_070714.jpg](https://user-images.githubusercontent.com/75278757/124348453-44eac980-dbea-11eb-82c4-1e2f69100397.jpg)![Screenshot at 2021-07-02 18-16-32.png](https://user-images.githubusercontent.com/75278757/124348460-4c11d780-dbea-11eb-9b72-2b78b98cff42.png)"
"I can't understand how intra-block positional embeddings can save position of each character after block operations.
Can someone explain idea about it considering discussion in lucidrains repo [embeddings](https://github.com/lucidrains/charformer-pytorch/issues/1#issue-934034665)
"
"Hi! Thanks for the amazing works!
I have one question in  `nonnegative_softmax_kernel_feature_creator`.
I don't understand why `jnp.max` terms are used in `nonnegative_softmax_kernel_feature_creator` as below. I cannot find the corresponding equations in the original paper. Is it just for normalization? Is there a mathematical background? 
Thank you!

https://github.com/google-research/google-research/blob/c249ee982c9ca3bb0cca4788758435c87c71fc7d/performer/fast_attention/jax/fast_attention.py#L101-L109

"
"Hi, I didn't get the reported accuracy according to your sh script and model. Where might be the problems? thanks for your help!
On half resolution. The printed metrics are:
Images processed:
15
psm_epe bad_0.1 bad_0.5 bad_1.0 bad_2.0 bad_3.0
[ 2.07099784 85.09916146 42.11792107 23.01151438 12.87441467  9.49672834]"
"I have encountered an issue like this: 

tensorflow.python.framework.errors_impl.FailedPreconditionError: /data/TF_Sintel/test; Is a directory
[[{{node MultiDeviceIteratorGetNextFromShard}}]]
[[RemoteCall]] [Op:IteratorGetNext]

in smurf/smurf_trainer.py(321)train_eval()
distributed_inputs = train_it.next()

Could you provide me some suggestions on this issue?"
"To replicate:
```console
conda create -n ptopk python=3.8
pip install -r requirements.txt
python image_classification.py --config <some config> --workdir /tmp
```

Getting incompatible function arguments error when image_classification.py tries to import flax.nn:

```console
TypeError: jit(): incompatible function arguments. The following argument types are supported:
    1. (fun: function, cache_miss: function, get_device: function, static_argnums: List[int], static_argnames: List[str] = [], donate_argnums: List[int] = [], cache: jaxlib.xla_extension.jax_jit.CompiledFunctionCache = None) -> object

Invoked with: <function _rfft_transpose at 0x7f22f751fdc0>, <function _cpp_jit.<locals>.cache_miss at 0x7f22f751fe50>, <function _cpp_jit.<locals>.get_device_info at 0x7f22f751fee0>, <function _cpp_jit.<locals>.get_jax_enable_x64 at 0x7f22f751ff70>, <function _cpp_jit.<locals>.get_jax_disable_jit_flag at 0x7f22f7526040>, (0, 2)
```

OS Info:
Ubuntu 20.04.1 LTS"
""
https://github.com/google-research/google-research/blob/d3e4296414efe111427d71a082ff454edb7d8ed5/direction_net/pano_utils/geometry.py#L123
""
"Followed the instructions in the readme, which was not successful, seems like training on custom data isn't supported "
"
When run the automl-zero , I meet the following error:

execute command: `bash ./run_demo.sh`

The errror is : 
```bash 
ERROR: /home/gitlab-runner/.cache/bazel/_bazel_gitlab-runner/80373f2a2f278aa1c4548a331a0e2355/external/com_google_protobuf/BUILD:873:21: in blacklisted_protos attribute of proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:_internal_wkt_protos_genrule' does not have mandatory providers: 'ProtoInfo'
ERROR: Analysis of target '//:run_search_experiment' failed; build aborted: Analysis of target '@com_google_protobuf//:cc_toolchain' failed
INFO: Elapsed time: 0.109s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
    Fetching @eigen_archive; fetching
```"
Is there exits GPU version code? 
I was running the script using provided Case Bert Base model and train and test data for the GoEmotion model (Bert_classifier.py). After I run the calculate_metric.py script it does not generate the results described in the paper. Is there any change that need to be made to recreate that result?  
"Hi,

I'm trying to run the [cifar10 example](https://github.com/google-research/google-research/tree/master/graph_compression/compression_lib/examples/cifar10) in _graph_compression_ directory related to the [Matrix Compression Library](https://github.com/google-research/google-research/tree/master/graph_compression)
Based on the [doc](https://drive.google.com/file/d/1843aNpKx_rznpuh9AmEshgAKmISVdpJY/view) in Low-Rank Approximation we divide an original weight matrix (A) into two low rank matrix B and C. 
According to the results by applying Low-Rank Approximation on cifar10 and determining rank=200 we can reduce parameters from **1,067,584** to **680512**.
But when I execute the example and print the summary of layers and number of parameters I have this result:
**1,639,690**
It means we still have the original weight matrix (A) in our graph.
How can we fix this problem?

Thanks.
Miladdona
 "
"Hi there,

Thank you for the amazing work and for making the fine-tuning datasets online.  I was wondering if it would be possible to make cubert's fine-tuning parameters available to us?  Looking at the paper, it said,

> We pre-train CuBERT with the default configuration of the BERT Large model, one model per example length (128, 256, 512, and 1,024 subword tokens) with batch sizes of 8,192, 4,096, 2,048, and 1,024 respectively, and the default BERT learning rate of 1 × 10−4. Fine-tuned models also used the same batch sizes as for pre-training, and BERT’s default learning rate (5 × 10−5). For both, we gradually warm up the learning rate for the first 10 % of examples, which is BERT’s default value.

What do you mean by with batch sizes 8,192, 4,096, 2,048, and 1,024?  This seems not to match what I found on this page https://github.com/google-research/bert/blob/master/README.md.  Also, the paper said the evaluation is done on either V100 or P100.  Could you also specify the memory limit on those GPUs?

For the exception dataset, I can only achieve an accuracy of 75%, which is 4% away from what was reported, so I would greatly appreciate any help, as we find this work very interesting!  Thank you in advance!"
Why did you choose to have a folder structure with all repos in one folder?  When you could have everything modular in distinct repos? 
"Hi, I want to refer to the implementation of AutoDropout, but it seems that I cannot find it here. The paper said that code is published in this website, so how can I touch it ? "
I am amazed by the amazing results of the model. I think it would be great if there is an option of recording the animation video.
"I'm new to TensorFlow. Can I run the model on GPU, your code can be run successfully on the CPU. However, when I change the device to GPU, It crashes. "
"In Appendices to the Paper, you have a figure that visualises the attention scores. How is it possible to recover those? "
"I am implementing this paper myself. Without fine-tuning, the student trained on pseudo-label (CIFAR-10-4000) only achieves an accuracy of about 50%. Is this the true status or this is my implementation issue? What is the official accuracy without fine-tuning?
Thank you so much!"
""
"In the file datasets.py, line 287, you scale the pose matrix to a default boundary matrix:

        # Rescale according to a default bd factor.
        scale = 1. / (bds.min() * .75)
        poses[:, :3, 3] *= scale
        bds *= scale

why?"
"I want to use trill-distilled model v3 as a feature extractor for an input for an other classification model.

When I make some test with the model I found a strange behavior about the output size. 

If the input vector is in shape in in between:         (1,) - (18320,) model gives TensorShape([1, 2048])
If the input vector is in shape in in between: (18320,) - (21040,) model gives TensorShape([2, 2048])
If the input vector is in shape in in between: (21040,) - (23760,) model gives TensorShape([3, 2048])
If the input vector is in shape in in between: (23760,) - (26480,) model gives TensorShape([4, 2048])
If the input vector is in shape in in between: (26480,) - (29200,) model gives TensorShape([5, 2048])

Its adds up one 2048 element long row for increasing input vector size by 2720. Each increment adds an other feature row as output.

My question is why is the first output range is different from the others?

Thank you..."
"Hi

For me there are two items on [https://github.com/google-research/google-research/blob/master/tabnet/tabnet_model.py](url)  that are different from what was reported in the paper [https://arxiv.org/pdf/1908.07442.pdf](url) 

On **interpretability** section the author @soarik  says that:
<img src=""https://render.githubusercontent.com/render/math?math=\sum_{c=1}^{N_d} ReLU(d_{b,c}[i])"">
But the given code in github is dividing this sum by the number of steps:
(line 177)
`# Aggregated masks are used for visualization of the`
` # feature importance attributes.`
 `         scale_agg = tf.reduce_sum(
              decision_out, axis=1, keep_dims=True) / (
                  self.num_decision_steps - 1)`
Also, for the aggregate feature importance mask, it looks like the normalization is missing. The paper states:
<img src=""https://render.githubusercontent.com/render/math?math=\frac{ \sum_{i=1}^{N_{steps}} \eta_b[i]M_{b,j}[i]} {\sum_{j=1}^{D}\sum_{i=1}^{N_{steps}} \eta_b[i]M_{b,j}[i]}"">
But the give code is:
(line 182)
`aggregated_mask_values += mask_values * scale_agg`
Inside a for looping over each step, which will give only the top part of the equation.

Am I missing something or the paper and code are really different?
"
"Hi!

I'm interested in the Pr-VIPE technology, and trying to adapt it to action recognition.
I already made sample ""input.csv"" and confirmed that infer.py works well.

But, although I changed some scores of keypoint to zero assuming the keypoint is not detected, results (unnormalized_embeddings.csv) did not change.
Of course, I set  the min_input_keypoint_score_2d to greater than 0 (actually set to 0.1).

Would you show me how to treat missing keypoint?

Best."
""
""
"According to the documentation, there is a possibility for summary-level evaluation of RougeL (using `rougeLsum` mode). But is a similar thing possible for Rouge-N variations?
Right now looking at the code, I can't help but conclude that it is not. So I was wondering, was that a conscious decision to not make it an option? Would something like that make no sense to compute? 

Ultimately, I am interested in computing Rouge-2 and Rouge-3 with 1 candidate sentence and multiple references. Right now I'm looking at calculating pairwise scores between the candidate and all references and then average out the result. 
I was wondering if the way `rougeLsum` does it would be a better choice and if it's possible using the current state of the code. "
"When I tested the model, raised an error:
File ""/home/sy/nerf/code/jaxnerf/eval.py"", line 73, in main 
    lpips_model = tf_hub.load(LPIPS_TFHUB_PATH) 

I can not solve this problem. I hope you can give me some advice， thank you."
"Hello to everyone, thanks for sharing of your work. I'm using BAM to solve new type of tasks. It'seems that the training takes place on CPU in absence of TPU. It's possible to train using GPU?"
"I clone repository at https://github.com/google-research/google-research/tree/master/ieg , set 'use_imagenet_as_eval' to False
then I run command:
CUDA_VISIBLE_DEVICES=0 python -m ieg.main --dataset=cifar10_uniform_0.2 --network_name=resnet29 --probe_dataset_hold_ratio=0.002 --checkpoint_path=${SAVEPATH}/ieg

That's all step, but in paper the results is ~0.92, when I test it is only 0.8x.

I use 1 GPU cuda 10.1 cudnn 7.6.5 with all required package in conda env.
"
"How can I run the APE task?

There seems to be no example, so please contact us.
"
"Hi @debidatta

Once training is completed, I usually see that I get the training loss of around 0.005 with 60000 iterations for my settings.
But for evaluation when I run `python -m tcc.evaluate --alsologtostderr --logdir $MODEL_DIR`, I get a very high training loss like ~1.000. And I got a very small value for the kendalls tau task like 0.02. Though alignment looks working quite well. 

I am wondering what could cause this discrepancy in the training loss between training and evaluation. Any feedback would be appreciated. Thank you!"
"Thanks for uploading the PG-19 checkpoints and the 'routing_transformer' scripts. 

'routing_transformer_test.py' works but I'm unclear how to run inference from the scripts as per the published paper.

I would appreciate an example to run. Using ubuntu 18.04 and tf 1.15 

Cheers"
working with posenet but not working with the c++ api
"In the paper, it mentions that it separate data into clean and noisy using T, and only keep clean ones to calculate unsupervised loss. However, I don't see this step in the code. Do anyone know where this step is?"
"Hello,
I am planning to work on 3D object detection using tf3d.
I am curious to know is it possible to use my own custom dataset to train a object detection model? As on repository it is mentioned that currently it supports only 3 datasets (waymo, rio and scannet).
If Yes, than please can any one help me in preparing .tfrecord with which i can train tf3d object detection model? As i don't have much experience in this filed.

Right now i am able to train the tf3d object detection model with only following waymo dataset source:(gs://waymo_open_dataset_tf_example_tf3d/original_tfrecords/train-00000-of-01212.tfrecords .)

Thank you"
"Hi, 
I would like to try out the sparce conv 3d layer for one of my work project 
However I am running a win10/wsl setup and it seems I wont be able to complete the tf3d/ops instructions with the nvidia docker image with GPU support unless in get windows insider build crap and I am not even sure it would work then.

Any plan for pip/conda package ?    "
"Hi,

In ""Rethinking Attention with Performers"", in the appendix, section D.5, Performers seem to be faster than Linear Transformers. I'm not sure I understand how. 

Let's assume we're in a base case  (nb_heads, nb_layers, d_ff, d), and we have a sequence of size n:
 - Linear transformer simply applie `elu(x) + 1`, thus converts out input from `(batch_size, nb_heads, sequence_length, d // nb_heads)` to `(batch_size, nb_heads, sequence_length, d // nb_heads)`
 - Performer converts input from `(batch_size, nb_heads, sequence_length, d // nb_heads)` to `(batch_size, nb_heads, sequence_length, m * l)` (m being the number of random vector we use, and l the number of kernel functions).
 
 Performers approximation should be slower, unless `m*l < d // nb_heads` right? "
"As performer need to generate <img src=""https://render.githubusercontent.com/render/math?math=\omega""> inside the attention calculation, is the generation way the same for training and inference?"
"Shouldn't it be inside the for loop (1-indentation in lines 581 and 583 )?

https://github.com/google-research/google-research/blob/8c62c43029efc0c49d9dfc019bd9af11578cc1ae/norml/maml_rl.py#L581

Thank you for making the repository available. "
"Dear @sarahooker , @doomie 
thanks a lot for the nice work and code. 
I have a question regarding the ranking algorithm. E.g. in the `compute feature ranking()` in ""data_input.py"", the saliency map is regarded as a 3-channel map and the ranking is for each channel separately. 
It means that for one pixel ( (x,y) coordinate) in an input image,  two of the channels could be removed but one channel left. Is that correct? 
Thank you very much!

Best,"
"![I7}R~$2_O`SJ}TD6O3WI8D9](https://user-images.githubusercontent.com/38975880/116345697-3e6f5880-a81b-11eb-8bc9-4cee9d9824c9.png)
I got an error like this when I tried to get the dataset that the aurther mentioned in the Readme. Did anyone solve this pronlem?"
"Dear authors, 

 I am trying to evaluate another unsupervised depth estimation method on Waymo dataset, and I would like to use similar if not the same setting you used for your work. I would be grateful if you could share more details about the training/evaluation setting like the following: 
 * I found that data records have also radial and tangential distortion parameters, Have you processed Waymo's data considering those distortions?
 * What is the input resolution of the model for Waymo's data? (480 * 196?)
 * Did you crop the input images so only the part of the image that has lidar ground truth is processed? (0.4 * height crop?)
 
Thank you for sharing the code of your work,
"
"Hi @gariel-google,
first of all thanks for your great work, also with [depth_and_motion_learning]. Upon porting this model to tf2, I've stumbled upon the following issue and it would be very kind if you could help me with that.

In [train.py line 64](https://github.com/google-research/google-research/blob/b12b5753dc0870af2e3c30a0ac1d0adfb3da0754/depth_from_video_in_the_wild/train.py#L64) the default weight regularization is set to 1e-2, so it seems to be active by default. Also in the CVF [paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.pdf) it is stated
![image](https://user-images.githubusercontent.com/826149/115832258-fc0cdc80-a412-11eb-9aa9-1e131fb2a20e.png)
that a l2 regularization term is used. Digging deeper in the code, special care is taken where the l2-weights are applied:
- In the Resnet encoder, the parameter is ignored.
- For the depth decoder, it is applied to each Conv2D and Conv2DTranspose layer.
- For the motion network, we apply it in the enocder, decoder, and refinements in the v2-variants, but only to the encoder in the v1-variants.

If I load the [Cityscapes + KITTI checkpoint](https://www.googleapis.com/download/storage/v1/b/gresearch/o/depth_from_video_in_the_wild%2Fcheckpoints%2Fcityscapes_kitti_learned_intrinsics.zip?generation=1566493762028542&alt=media) in my new implementation and train with the same l2-weight, I get a quite high l2-loss. Additionally, in your reference training code I didn't find a call to `tf.GraphKeys.REGULARIZATION_LOSSES`, so I'm now wondering if this l2-regularization is really active.

Thank you already in advance
xerxesr"
"### I have followed the instruction mentioned in (https://github.com/tensorflow/custom-op), but I face error with both bazel and makefile builders:
**bazel error msg:**
**_""bled by setting --experimental_repo_remote_exec
ERROR: error loading package '': in C:/users/windows10-desktop/_bazel_windows10-desktop/gbkcodrx/external/org_tensorflow/tensorflow/workspace.bzl: Extension file 'third_party/py/python_configure.bzl' has errors
INFO: Elapsed time: 0.164s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)""_**

**makefile error msg:
""make: *** No rule to make target 'zero_out_pip_pkg'.  Stop.""**

please find below screenshots for tf3d/ops directory:
![ops](https://user-images.githubusercontent.com/72650269/115450035-79c2c380-a22c-11eb-9833-d249aa3894f6.jpg)
"
"Dear @simonster 



Hello, I'm a graduate student who is new in deep learning.

First, please take my gratitude for your your great work.

I want to ask a question about your CKA similarity index.

Does CKA works properly on Fully Connected Layer?

I tested it on FC Layers with 2 Hidden Layers and 1 Output Layer, but the result was so bad even it cannot passed the sanity check.

I wonder did I do wrong on the test or does CKA not work properly on FC Layer.

Please give me any advise.

It will be so much helpful to me.

Thank you in advance."
"Dear authors @AustinCStone @eladeban, 

I would like to benchmark your model, however I cannot find the trained model checkpoints.
Would you be willing to share the files?

Kind Regards,
Stefano"
"$ bash ./benchmark.sh ../../sgk_models/transformer/sparse sparse
   ........
   No OpKernel was registered to support Op '**CsrSoftmax**' used by {{node sparse_transformer/body/decoder/layer_0/self_attention/multihead_attention/CsrSoftmax}} with these attrs: []
Registered devices: [CPU]
Registered kernels:
  device='GPU

$  $ bash ./benchmark.sh ../../sgk_models/transformer/sparse dense
 .........
 No OpKernel was registered to support Op '**FusedSoftmax**' used by {{node sparse_transformer/body/decoder/layer_0/self_attention/multihead_attention/FusedSoftmax}} with these attrs: []
Registered devices: [CPU]
Registered kernels:
  device='GPU'"
"Hi @agarwl ,

Thanks for uploading the PSE code. I have a question about running PSE on dm-control. The following command ran fine, i.e. when contrastive_loss_weight was set to zero.

`python -m pse.dm_control.run_train_eval
        --trial_id=1
        --seed 0
        --env_name=cartpole-swingup
        --root_dir=/tmp/drq
        --num_train_steps=10000
        --eval_interval=25
        --policy_save_interval=50
        --checkpoint_interval=100
        --gin_bindings=""load_dm_env_for_eval.action_repeat=8""
        --contrastive_loss_weight=0
        --gin_bindings=""drq_agent.train_eval.initial_collect_steps=300""
        --gin_bindings=""drq_agent.train_eval.eval_episodes_per_run=1""
        --alsologtostderr`

Then I followed the instructions to download and configure distracting_control and the dataset for cartpole-swingup and set `contrastive_loss_weight` to 1.0. The training script failed because when it isn't 0, it needs `data_dir` to load the episode data from the dataset I downloaded from the GCP bucket. It's a simple fix. No problem. But the training still failed with the following exception:

`tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:GPU:0 vs /job:localhost/replica:0/task:0/device:CPU:0. The edge src node is Func/StatefulPartitionedCall/Losses_1/contrastive_loss/write_summary/summary_cond/StatefulPartitionedCall/input/_1059 , and the dst node is StatefulPartitionedCall/Losses_1/contrastive_loss/write_summary/summary_cond/StatefulPartitionedCall/Losses_1/contrastive_loss/write_summary/summary_cond [Op:__inference__train_27434]
`

Any idea on how to fix this? Thanks!


"
""
May you share the code about `CV-MIM: Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization`?
"Referring to the paper, it says ""With product quantization we encode each datapoint into an M dimensional codeword, each with k possible states"".
Here, k directly relates to the compression rate of the dataset so I wanted to know how ScaNN's behavior differs as k varies. However, I could not find a parameter to set k and it seems like k is fixed to 16 in the code.

Is there any reason why ScaNN only supports k=16? If it is, is it because to maximize the benefit of SIMD instruction?
Or is there other way to adjust k?"
"I'm interested in a C++ version of the 3D Non Max Suppression algorithm. The implementation in tf3d is already super nice but I'm looking for a high performance one like the 2D Non Max Suppression that already exists in TensorFlow (see [this thread](https://github.com/tensorflow/addons/issues/2434) for more precise discussion).

Have you considered creating it?"
Could you provide the checkpoints of Meta Pseudo Label in efficientnet l2 and b7?
"@zhouxin913 

hi，I can’t find the gbash.sh file in the seq2act project. Is this file not submitted?
![image](https://user-images.githubusercontent.com/19492817/112750429-2da0ac80-8ffb-11eb-8f01-ad3990e3620d.png)

In fact, I still have a lot of questions about the seq2act project. I would like to consult you in detail. Thank you very much.

"
"As this seems like another issue, I open a new one.
I am currently at step 6 according to https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md
at line ""bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec --verbose_failures""

i get this error:
```
WARNING: Download from https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
DEBUG: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /root/.cache/bazel/_bazel_root/ec891c5b3b8ae1c73a1e1d73216b2747/external/local_config_tf/BUILD:5674:1: target 'libtensorflow_framework.so' is both a rule and a file; please choose another name for the rule
```

Is there any regional restriction for this url ```https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/fcc4b966f1265f466e82617020af93670141b009.tar.gz``` ?
"
"Traceback (most recent call last):
  File ""meta_pseudo_labels/main.py"", line 463, in main
    train_tpu(params, should_eval=should_eval)
  File ""meta_pseudo_labels/main.py"", line 237, in train_tpu
    set_tpu_info(params)
  File ""meta_pseudo_labels/main.py"", line 68, in set_tpu_info
    topology_proto = sess.run(tpu_init)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by node ConfigureDistributedTPU (defined at /usr/local/lib/p
ython3.7/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [tpu_embedding_config="""", is_global_init=false, embedding_config=""""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>
         [[ConfigureDistributedTPU]]
I0322 13:53:47.967103 140539649898304 main.py:470] Failed 50 times. Retry!"
"Hello,

Request to upload the pretrained tuple phrase extraction model for seq2act repository.
Only pretrained grounding model is provided. 

thanks."
"Can you publish the code of the training process on github?
I need to know more details of the training process.
Thank you a lot!"
"This is not an issue I'm just wondering that the reasoning is behind https://github.com/google-research/google-research/blob/master/performer/fast_attention/jax/fast_attention_test.py#L46

1. Why are Q and K the same (byproduct of random setting random seed to 0). Interestingly even though K uses the same seed the result is different (because different shape?). 
2. Why is `nb_random_features` set to 10000. Is this just to verify the approximation works at all? Obviously the whole point of the FAVOR mechanism allows `nb_random_features` << sequence length.

Context: I think this is wonderful and I'm working on notebook(s) for https://plutojl.org/plutocon2021 with the aim explaining FAVOR in an accessible way."
"I used a fresh ubuntu system and tried to run Automl-zero research (https://github.com/google-research/google-research/tree/master/automl_zero) done by google.

Here is the order of codes I used in a notebook (Instruction were given in the same auto-ml repository):
1. cd
2. ! sudo apt install apt-transport-https curl gnupg
3. ! sudo apt install g++
4. ! g++ --version # output: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
5. ! curl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor > bazel.gpg
6. ! sudo mv bazel.gpg /etc/apt/trusted.gpg.d/
7. ! echo ""deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8"" | sudo tee /etc/apt/sources.list.d/bazel.list
8. ! sudo apt update && sudo apt install bazel-3.4.1
9. ! sudo apt update && sudo apt full-upgrade
10. ! sudo ln -s /usr/bin/bazel-3.4.1 /usr/bin/bazel
11. ! bazel --version
12. ! git clone https://github.com/google-research/google-research.git
13. cd google-research/automl_zero
14. ! ./run_demo.sh

And after running run_demo.sh file, I am getting an error.

[211 / 443] 2 actions running
    Compiling com_google_absl/absl/flags/usage.cc; 0s processwrapper-sandbox
[212 / 443] 2 actions running
    Compiling com_google_absl/absl/flags/usage.cc; 0s processwrapper-sandbox
[212 / 443] 2 actions running
    Compiling com_google_absl/absl/flags/usage.cc; 1s processwrapper-sandbox
**ERROR**: /root/google-research/automl_zero/BUILD:30:14: **Generating C++ proto_library** //:task_proto failed (Exit 1): protoc failed: error executing command bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/k8-opt/bin' '-Itask.proto=task.proto' --direct_dependencies task.proto ... (remaining 2 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox protoc failed: error executing command bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/k8-opt/bin' '-Itask.proto=task.proto' --direct_dependencies task.proto ... (remaining 2 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
devtools/staticanalysis/pipeline/analyzers/proto_best_practices/proto/optouts.proto: File not found.
task.proto:20:1: Import ""devtools/staticanalysis/pipeline/analyzers/proto_best_practices/proto/optouts.proto"" was not found or had errors.
Target //:run_search_experiment failed to build
Use --verbose_failures to see the command lines of failed build steps.
**INFO**: Elapsed time: 231.997s, Critical Path: 14.04s
**INFO**: 216 processes: 21 internal, 195 processwrapper-sandbox.
**FAILED**: Build did NOT complete successfully
**FAILED**: Build did NOT complete successfully

Could someone help me with this?
@crazydonkey200 
@claytonkanderson"
"Hi, I open this issue to ask you if it is possible to have a sort of readme where you explain from the beginning how to train a network using the modules that you provide. For example, my ideal tutorial for semantic segmentation would be.

1 - How to prepare you dataset for the network. Which data are needed? In which format? etc..
2 - Build a simple model from scratch using sparse convolutions etc, or loading existing ones, how to feed data inside the model and how tu run a training. 
3 - Load trained weights and test the result

I know that these information can be extracted from the repository but I think that some examples with detailed documentation will be very useful for every user. I let you an example of a well designed repository that explain from the beginning how to use it (https://github.com/matterport/Mask_RCNN).

Thank you for your patience!

Francesco. "
"Hi everyone,

I am running the standard script `run_train_locally.sh` for the waymo dataset with the standard configuration of 240 Epochs and 100 Steps. However after a while (always different number of steps), the box classification value becomes NaN (see picture).

![image](https://user-images.githubusercontent.com/17144445/111959683-117aa800-8aef-11eb-908f-bb667fb3b56b.png)


However there is no error message whatsoever and the training continuous, expect that the total loss also turns into a NaN respectively. I'm not sure what causes this. Does anyone have experienced something similar ?

Thanks a lot!

Max"
"The instructions in [README.md](https://github.com/google-research/google-research/blob/master/seq2act/data_generation/README.md)  have listed all the steps about how to generate the ```AndroidHowTo```  Dataset, but the ```CommonCrawl``` Dataset is very large and difficult for me the deal with. So if it is convenient for you guys to supply the final ```crawled_instructions.json``` file? That will help a lot of people who have situations like me."
"I just followed the instruction and installed metis=5.1.0 and networkx=1.11(python=3.6.2).
However,when I test :
`import networkx as nx
`
 `import metis` 
`G = metis.example_networkx()
`
 `(edgecuts, parts) = metis.part_graph(G, 3)`

Error occured:
`Traceback (most recent call last):
`
 ` File ""<string>"", line 1, in <module>
`
`  File ""/root/work_dir/anaconda3/envs/cluster-gcn/lib/python3.6/site-packages/metis.py"", line 765, in part_graph
`
 `   graph = networkx_to_metis(graph)
`
  `File ""/root/work_dir/anaconda3/envs/cluster-gcn/lib/python3.6/site-packages/metis.py"", line 574, in networkx_to_metis`
   ` for i in H.nodes:
`
`TypeError: 'method' object is not iterable`
could someone help?"
"Attempting to run `frechet_video_distance/example.py` I get the error:

`NotImplementedError: Cannot convert a symbolic Tensor (covariance/Size_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported`

Ubuntu 20.04.2 LTS
tensorflow version: 2.4.1

complete traceback:

```
  File ""/home/ian/projects/frame-prediction-pytorch/fid/example.py"", line 51, in <module>
    tf.app.run(main)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/ian/projects/frame-prediction-pytorch/fid/example.py"", line 40, in main
    result = fvd.calculate_fvd(
  File ""/home/ian/projects/frame-prediction-pytorch/fid/frechet_video_distance.py"", line 140, in calculate_fvd
    return tfgan.eval.frechet_classifier_distance_from_activations(real_activations, generated_activations)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow_gan/python/eval/classifier_metrics.py"", line 792, in frechet_classifier_distance_from_activations
    return _frechet_classifier_distance_from_activations_helper(
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow_gan/python/eval/classifier_metrics.py"", line 716, in _frechet_classifier_distance_from_activations_helper
    tfp.stats.covariance(activations1),)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow_probability/python/stats/sample_stats.py"", line 438, in covariance
    tf.ones([sample_ndims], tf.int32)),
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 3120, in ones
    output = _constant_if_small(one, shape, dtype, name)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 2804, in _constant_if_small
    if np.prod(shape) < 1000:
  File ""<__array_function__ internals>"", line 5, in prod
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 3030, in prod
    return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 87, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
  File ""/home/ian/miniconda3/envs/torch-tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 852, in __array__
    raise NotImplementedError(
NotImplementedError: Cannot convert a symbolic Tensor (covariance/Size_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```

Conda environment:
```
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                      1_llvm    conda-forge
_tflow_select             2.1.0                       gpu  
absl-py                   0.12.0             pyhd8ed1ab_0    conda-forge
aiohttp                   3.7.4            py38h497a2fe_0    conda-forge
astunparse                1.6.3              pyhd8ed1ab_0    conda-forge
async-timeout             3.0.1                   py_1000    conda-forge
attrs                     20.3.0             pyhd3deb0d_0    conda-forge
blas                      1.0                         mkl    conda-forge
blinker                   1.4                        py_1    conda-forge
brotlipy                  0.7.0           py38h497a2fe_1001    conda-forge
bzip2                     1.0.8                h7f98852_4    conda-forge
c-ares                    1.17.1               h7f98852_1    conda-forge
ca-certificates           2021.1.19            h06a4308_1  
cachetools                4.2.1              pyhd8ed1ab_0    conda-forge
cairo                     1.16.0            h7979940_1007    conda-forge
certifi                   2020.12.5        py38h578d9bd_1    conda-forge
cffi                      1.14.5           py38ha65f79e_0    conda-forge
chardet                   4.0.0            py38h578d9bd_1    conda-forge
click                     7.1.2              pyh9f0ad1d_0    conda-forge
cloudpickle               1.6.0                      py_0  
comet_ml                  3.5.0                      py38    comet_ml
configobj                 5.0.6                      py_0    conda-forge
cryptography              3.4.6            py38ha5dfef3_0    conda-forge
cudatoolkit               10.1.243             h036e899_8    conda-forge
cudnn                     7.6.5.32             hc0a50b0_1    conda-forge
cupti                     10.1.168                      0  
cycler                    0.10.0                     py_2    conda-forge
dbus                      1.13.6               hfdff14a_1    conda-forge
decorator                 4.4.2              pyhd3eb1b0_0  
dill                      0.3.3              pyhd3eb1b0_0  
dulwich                   0.20.20          py38h497a2fe_0    conda-forge
everett                   1.0.2                      py_1    comet_ml
expat                     2.2.10               h9c3ff4c_0    conda-forge
ffmpeg                    4.3.1                hca11adc_2    conda-forge
fontconfig                2.13.1            hba837de_1004    conda-forge
freetype                  2.10.4               h0708190_1    conda-forge
future                    0.18.2                   py38_1  
gast                      0.4.0              pyh9f0ad1d_0    conda-forge
gettext                   0.19.8.1          h0b5b191_1005    conda-forge
glib                      2.66.7               h9c3ff4c_1    conda-forge
glib-tools                2.66.7               h9c3ff4c_1    conda-forge
gmp                       6.2.1                h58526e2_0    conda-forge
gnutls                    3.6.13               h85f3911_1    conda-forge
google-auth               1.24.0             pyhd3deb0d_0    conda-forge
google-auth-oauthlib      0.4.1                      py_2    conda-forge
google-pasta              0.2.0              pyh8c360ce_0    conda-forge
googleapis-common-protos  1.52.0           py38h06a4308_0  
graphite2                 1.3.13            h58526e2_1001    conda-forge
grpcio                    1.36.1           py38hdd6454d_0    conda-forge
gst-plugins-base          1.18.4               h29181c9_0    conda-forge
gstreamer                 1.18.4               h76c114f_0    conda-forge
h5py                      2.10.0          nompi_py38h7442b35_105    conda-forge
harfbuzz                  2.7.4                h5cf4720_0    conda-forge
hdf5                      1.10.6          nompi_h7c3c948_1111    conda-forge
icu                       68.1                 h58526e2_0    conda-forge
idna                      2.10               pyh9f0ad1d_0    conda-forge
importlib-metadata        3.7.3            py38h578d9bd_0    conda-forge
jasper                    1.900.1           h07fcdf6_1006    conda-forge
jpeg                      9d                   h36c2ea0_0    conda-forge
jsonschema                3.2.0              pyhd8ed1ab_3    conda-forge
keras-preprocessing       1.1.2              pyhd8ed1ab_0    conda-forge
kiwisolver                1.3.1            py38h1fd1430_1    conda-forge
krb5                      1.17.2               h926e7f8_0    conda-forge
lame                      3.100             h7f98852_1001    conda-forge
lcms2                     2.12                 hddcbb42_0    conda-forge
ld_impl_linux-64          2.35.1               hea4e1c9_2    conda-forge
libblas                   3.9.0                     8_mkl    conda-forge
libcblas                  3.9.0                     8_mkl    conda-forge
libclang                  11.1.0          default_ha53f305_0    conda-forge
libcurl                   7.75.0               hc4aaa36_0    conda-forge
libedit                   3.1.20191231         he28a2e2_2    conda-forge
libev                     4.33                 h516909a_1    conda-forge
libevent                  2.1.10               hcdb4288_3    conda-forge
libffi                    3.3                  h58526e2_2    conda-forge
libgcc-ng                 9.3.0               h2828fa1_18    conda-forge
libgfortran-ng            7.5.0               h14aa051_18    conda-forge
libgfortran4              7.5.0               h14aa051_18    conda-forge
libglib                   2.66.7               h3e27bee_1    conda-forge
libiconv                  1.16                 h516909a_0    conda-forge
liblapack                 3.9.0                     8_mkl    conda-forge
liblapacke                3.9.0                     8_mkl    conda-forge
libllvm11                 11.1.0               hf817b99_0    conda-forge
libnghttp2                1.43.0               h812cca2_0    conda-forge
libopencv                 4.5.1            py38h703c3c0_0    conda-forge
libpng                    1.6.37               h21135ba_2    conda-forge
libpq                     13.1                 hfd2b0eb_2    conda-forge
libprotobuf               3.15.6               h780b84a_0    conda-forge
libssh2                   1.9.0                ha56f1ee_6    conda-forge
libstdcxx-ng              9.3.0               h6de172a_18    conda-forge
libtiff                   4.2.0                hdc55705_0    conda-forge
libuuid                   2.32.1            h7f98852_1000    conda-forge
libuv                     1.41.0               h7f98852_0    conda-forge
libwebp-base              1.2.0                h7f98852_1    conda-forge
libxcb                    1.13              h7f98852_1003    conda-forge
libxkbcommon              1.0.3                he3ba5ed_0    conda-forge
libxml2                   2.9.10               h72842e0_3    conda-forge
llvm-openmp               11.0.1               h4bd325d_0    conda-forge
lz4-c                     1.9.3                h9c3ff4c_0    conda-forge
markdown                  3.3.4              pyhd8ed1ab_0    conda-forge
matplotlib                3.3.4            py38h578d9bd_0    conda-forge
matplotlib-base           3.3.4            py38h0efea84_0    conda-forge
mkl                       2020.4             h726a3e6_304    conda-forge
mkl-service               2.3.0            py38h1e0a361_2    conda-forge
multidict                 5.1.0            py38h497a2fe_1    conda-forge
mysql-common              8.0.23               ha770c72_1    conda-forge
mysql-libs                8.0.23               h935591d_1    conda-forge
ncurses                   6.2                  h58526e2_4    conda-forge
nettle                    3.6                  he412f7d_0    conda-forge
ninja                     1.10.2               h4bd325d_0    conda-forge
nspr                      4.29                 h9c3ff4c_1    conda-forge
nss                       3.62                 hb5efdd6_0    conda-forge
numpy                     1.20.1           py38h18fd61f_0    conda-forge
nvidia-ml                 7.352.0                    py_0    conda-forge
oauthlib                  3.0.1                      py_0    conda-forge
olefile                   0.46               pyh9f0ad1d_1    conda-forge
opencv                    4.5.1            py38h578d9bd_0    conda-forge
openh264                  2.1.1                h780b84a_0    conda-forge
openssl                   1.1.1j               h27cfd23_0  
opt_einsum                3.3.0                      py_0    conda-forge
pcre                      8.44                 he1b5a44_0    conda-forge
pillow                    8.1.2            py38ha0e1e83_0    conda-forge
pip                       21.0.1             pyhd8ed1ab_0    conda-forge
pixman                    0.40.0               h36c2ea0_0    conda-forge
promise                   2.2.1                      py_0    powerai
protobuf                  3.15.6           py38h709712a_0    conda-forge
psutil                    5.8.0            py38h497a2fe_1    conda-forge
pthread-stubs             0.4               h36c2ea0_1001    conda-forge
py-opencv                 4.5.1            py38h81c977d_0    conda-forge
pyasn1                    0.4.8                      py_0    conda-forge
pyasn1-modules            0.2.7                      py_0    conda-forge
pycparser                 2.20               pyh9f0ad1d_2    conda-forge
pyjwt                     2.0.1              pyhd8ed1ab_0    conda-forge
pyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge
pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge
pyqt                      5.12.3           py38h578d9bd_7    conda-forge
pyqt-impl                 5.12.3           py38h7400c14_7    conda-forge
pyqt5-sip                 4.19.18          py38h709712a_7    conda-forge
pyqtchart                 5.12             py38h7400c14_7    conda-forge
pyqtwebengine             5.12.1           py38h7400c14_7    conda-forge
pyrsistent                0.17.3           py38h497a2fe_2    conda-forge
pysocks                   1.7.1            py38h578d9bd_3    conda-forge
python                    3.8.8           hffdb5ce_0_cpython    conda-forge
python-dateutil           2.8.1                      py_0    conda-forge
python-flatbuffers        1.12               pyhd8ed1ab_0    conda-forge
python_abi                3.8                      1_cp38    conda-forge
pytorch                   1.7.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch
qt                        5.12.9               hda022c4_4    conda-forge
readline                  8.0                  he28a2e2_2    conda-forge
requests                  2.25.1             pyhd3deb0d_0    conda-forge
requests-oauthlib         1.3.0              pyh9f0ad1d_0    conda-forge
requests-toolbelt         0.9.1                      py_0    conda-forge
rsa                       4.7.2              pyh44b312d_0    conda-forge
scipy                     1.6.1            py38h91f5cce_0  
setuptools                49.6.0           py38h578d9bd_3    conda-forge
six                       1.15.0             pyh9f0ad1d_0    conda-forge
sqlite                    3.34.0               h74cdb3f_0    conda-forge
tensorboard               2.4.1              pyhd8ed1ab_0    conda-forge
tensorboard-plugin-wit    1.8.0              pyh44b312d_0    conda-forge
tensorflow                2.4.1           gpu_py38h8a7d6ce_0  
tensorflow-base           2.4.1           gpu_py38h29c2da4_0  
tensorflow-datasets       3.1.0                      py_0    powerai
tensorflow-estimator      2.4.1              pyheb71bc4_0  
tensorflow-gan            2.0.0                      py_0    powerai
tensorflow-gpu            2.4.1                h30adc30_0  
tensorflow-hub            0.8.0              pyh1d8a796_0    powerai
tensorflow-metadata       0.21.0             pyh1d8a796_0    powerai
tensorflow-probability    0.7                        py_2  
termcolor                 1.1.0                      py_2    conda-forge
tk                        8.6.10               h21135ba_1    conda-forge
torchvision               0.8.1                py38_cu101    pytorch
tornado                   6.1              py38h497a2fe_1    conda-forge
tqdm                      4.59.0             pyhd3eb1b0_1  
typing                    3.7.4.3          py38h06a4308_0  
typing-extensions         3.7.4.3                       0    conda-forge
typing_extensions         3.7.4.3                    py_0    conda-forge
urllib3                   1.26.4             pyhd8ed1ab_0    conda-forge
websocket-client          0.57.0           py38h578d9bd_4    conda-forge
werkzeug                  1.0.1              pyh9f0ad1d_0    conda-forge
wheel                     0.36.2             pyhd3deb0d_0    conda-forge
wrapt                     1.12.1           py38h497a2fe_3    conda-forge
wurlitzer                 1.0.3                      py_2    comet_ml
x264                      1!161.3030           h7f98852_0    conda-forge
xorg-kbproto              1.0.7             h7f98852_1002    conda-forge
xorg-libice               1.0.10               h7f98852_0    conda-forge
xorg-libsm                1.2.3             hd9c2040_1000    conda-forge
xorg-libx11               1.7.0                h7f98852_0    conda-forge
xorg-libxau               1.0.9                h7f98852_0    conda-forge
xorg-libxdmcp             1.1.3                h7f98852_0    conda-forge
xorg-libxext              1.3.4                h7f98852_1    conda-forge
xorg-libxrender           0.9.10            h7f98852_1003    conda-forge
xorg-renderproto          0.11.1            h7f98852_1002    conda-forge
xorg-xextproto            7.3.0             h7f98852_1002    conda-forge
xorg-xproto               7.0.31            h7f98852_1007    conda-forge
xz                        5.2.5                h516909a_1    conda-forge
yarl                      1.6.3            py38h497a2fe_1    conda-forge
zipp                      3.4.1              pyhd8ed1ab_0    conda-forge
zlib                      1.2.11            h516909a_1010    conda-forge
zstd                      1.4.9                ha95c52a_0    conda-forge
```"
"I am unable to find a link to the demos mentioned in the tf3d setup doc:

https://github.com/google-research/google-research/blob/master/tf3d/doc/setup.md

""Now you are ready to start training or evaluation or try one of our **demos**""

Can you add a link, please?"
"In my practice, I found that the package version in requirements.txt is written in xxx >= yyy format, such as 'tensorflow-probability >= 0.11.1', but it seems that pip3 installs the latest version of the package by default according to requirements.txt, which resulting in an error:  'ERROR: The version of TensorFlow requires Tensorflow version >=2.4; Detected an Installation of Version 2.3.0.'.
When I manually changed >= to == and installed the specified version of the software package, for example, tensorflow-probability==0.11.1, the error was fixed. So it is recommended to specify the minimum version in requirements.txt to avoid conflicts with the tensorflow version."
"Help, i am unable to execute the file due to this error code: ModuleNotFoundError: No module named 'meta_pseudo_labels'"
"I'm reading (https://arxiv.org/pdf/2004.01170.pdf) and I would like to understand in detail how do you apply Voxelization to the input point cloud, I took a look at the code but it's not easy to understand it without many comments. Then I would like to understand how to re-obtain the point cloud in output from voxels"
"I am trying to run the script from  https://github.com/google/flax/tree/master/examples/mnist   but with the shampoo optimizer from    https://github.com/google-research/google-research/blob/master/scalable_shampoo/jax/shampoo.py

The training is not working properly as it is  stuck.  I did some debugging and  found out that the line 
` return jaxpr, out_avals, consts `  in  /jax/interpreters/partial_eval.py(1204)trace_to_subjaxpr_dynamic() is not excuted and that is why training is stuck.

Any chance some one can help?


Thanks a lot
"
"I see that the student is trained on the augmented unlabeled data. Is the student able to produce reliable labels for these augmented images that it was trained on, or is it only able to produce reliable labels for the labeled and original (non-augmented) unlabeled images? Has this been evaluated?"
"Hi there!

How should one go about selecting `training_sample_size` for the `tree()` and `score_ah()` methods of the `ScannBuilder` class? The hyperparameter is not mentioned in the [algorithms](https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md) section. Should one leave it as default (e.g. `100000`) or stick to a value similar to the one from the example notebook (e.g. `250000`)? Does it depend on the dataset? In my case, I would like to build a ScaNN index on word embeddings with ~4M rows and 300 features.

Thanks in advance."
"Hi, I wonder if the teacher model in paper ""meta pseudo labels"" is initialized randomly?"
"Hello, I found that some blogs said that Metis can be installed on windows, but they failed to use cmake to configure.
Is there a Metis that has been compiled and installed directly like a python library?"
"Hi Admin and authors,

Is there any way to visualize feature map like feature map of images?

Any help will be helpful.

Thanks in advance"
"Hi @AustinCStone, 

Thank you for your great work. I just have a question about uflow's training loss and training memory requirement. 
1. Training loss: in your final SOTA result, the losses you used are self-sup + smoothness + occ masking + photo loss as census loss, right? But I just have one question: all these losses are applied only on model output level, i,e, level 2 in your implementation or the losses are applied on all levels?
2.  Since self-sup loss will engage after 50% of total iteration, how much memory will be required if using batch size of 1?

I am looking forward to your reply. Thank you very much!

Best,"
"Hi, I'm trying to install [tf3d](https://github.com/google-research/google-research/tree/master/tf3d) but encounter a problem of step 1 [Preparing and Compiling the Sparse Conv Op](https://github.com/google-research/google-research/tree/master/tf3d/ops). <br> My virtual environment (managed by using miniconda) is ```tensorflow==2.3.0, Python=3.7``` running on RTX-3090.<br> I tried ```
 pip3 install tf3d/ops/packages/tensorflow_sparse_conv_ops-0.0.1-cp37-cp37m-linux_x86_64.whl  ```.  <br> Then when I ``` import tf3d.ops.tensorflow_sparse_conv_ops as sparse_conv_ops``` in python shell, I got error message as bellow:

```bash
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/media/xxx/357B56513FD6C042/dockers/tf3d/google-research/tf3d/ops/tensorflow_sparse_conv_ops/__init__.py"", line 19, in <module>
    from tensorflow_sparse_conv_ops import sparse_conv_ops
  File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/__init__.py"", line 18, in <module>
    from tensorflow_sparse_conv_ops.sparse_conv_ops import submanifold_sparse_conv3d, submanifold_sparse_conv2d
  File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/sparse_conv_ops.py"", line 27, in <module>
    resource_loader.get_path_to_datafile('_sparse_conv_ops.so'))
  File ""/home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow/python/framework/load_library.py"", line 58, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/xxx/anaconda3/envs/tf3d/lib/python3.7/site-packages/tensorflow_sparse_conv_ops/_sparse_conv_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb
>>> 
```
"
"Hi,

Thanks for publishing this amazing work.
I have downloaded the FlyingThings model using the provided script. When I feed the model with  FlyingThings data I obtain correct and sharp disparity predictions. Nevertheless, if I feed it KITTI data, I obtain incredibly wrong outputs, not even close to the expected predictions.

Is there any reason your model does not work with images from a different dataset? Do I have to apply some sort of preprocessing before feeding the data to the network?

Thanks in advance,
Sergio"
"Hi, I install the BiGG in my new machine.

This is my Environment:
* RTX3090, cuda 11.1
* CentOS7
* Python3.6
* PyTorch1.8

I can run the test code normally on cpu:
`python -m bigg.unit_test.lib_test`

But I get error on gpu:
`python -m bigg.unit_test.lib_test -gpu 0`
This is Error message:
```
$ python -m bigg.unit_test.lib_test -gpu 0
Namespace(accum_grad=1, batch_exec=False, batch_size=10, bfs_permute=False, bits_compress=256, blksize=-1, data_dir='.', dev_ratio=0.2, directed=False, display=False, dist_backend='gloo', embed_dim=256, epoch_load=None, epoch_save=100, eval_folder=None, g_type=None, gpu=0, grad_clip=5, greedy_frac=0, learning_rate=0.001, max_num_nodes=-1, model_dump=None, node_order='default', num_epochs=100000, num_proc=1, num_test_gen=-1, old_model=False, param_layers=1, phase='train', pos_base=10000, pos_enc=True, rnn_layers=2, save_dir='.', seed=34, self_loop=False, share_param=True, train_method='full', train_ratio=0.8, tree_pos_enc=False)
use gpu indexed: 0
====== begin of tree_clib configuration ======
| bfs_permute = 0
| max_num_nodes = 1000000
| bits_compress = 256
| dim_embed = 256
| gpu = 0
| seed = 34
======   end of tree_clib configuration ======
venv_of_bigg/lib/python3.6/site-packages/torch/cuda/__init__.py:104: UserWarning: 
GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.
If you want to use the GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join(arch_list), device_name))
Traceback (most recent call last):
  File ""venv_of_bigg/bigg/bigg/unit_test/lib_test.py"", line 47, in <module>
    ll, _ = model.forward_train([0])
  File ""venv_of_bigg/bigg/bigg/model/tree_model.py"", line 470, in forward_train
    fn_hc_bot, h_buf_list, c_buf_list = self.forward_row_trees(graph_ids, list_node_starts, num_nodes, list_col_ranges)
  File ""venv_of_bigg/bigg/bigg/model/tree_model.py"", line 445, in forward_row_trees
    binary_embeds, base_feat = TreeLib.PrepareBinary()
  File ""venv_of_bigg/bigg/bigg/model/tree_clib/tree_lib.py"", line 176, in PrepareBinary
    feat = torch.cuda.FloatTensor(num_nodes + 2, self.embed_dim).fill_(0)
RuntimeError: CUDA error: no kernel image is available for execution on the device
```

Could you please help fix this error or tell me how to avoid this problem?
Thank you for your excellent work.
I am looking forward to your reply."
"Thanks for sharing of your work .
In the paper(View-Invariant Probabilistic Embedding forHuman Pose),the embeddings for action recogni-
tion is evaluated by using nearest neighbor search with the sequence distance . 
Can I understand in this way that some pose have already been recognized, and other pose use nearest neighbor search with the sequence distance to find the nearest already recognized pose?Does the model need to additional training?"
"After I run configure.py and try to build blaze, I get the following error message:

```
 CC=clang-8 bazel build -c opt --features=thin_lto --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
DEBUG: Rule 'com_google_protobuf' indicated that a canonical reproducible form can be obtained by modifying arguments commit = ""52b2447247f535663ac1c292e088b4b27d2910ef"", shallow_since = ""1569016252 -0700"" and dropping [""tag""]
DEBUG: Repository com_google_protobuf instantiated at:
  /mnt/hd1/gabrielcrds/github/google-research/scann/WORKSPACE:39:15: in <toplevel>
Repository rule git_repository defined at:
  /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/com_google_protobuf/BUILD:979:21: in proto_lang_toolchain rule @com_google_protobuf//:cc_toolchain: '@com_google_protobuf//:cc_toolchain' does not have mandatory provider 'ProtoInfo'.
WARNING: /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/local_config_tf/BUILD:5785:8: target 'libtensorflow_framework.so.2' is both a rule and a file; please choose another name for the rule
DEBUG: Rule 'pybind11_bazel' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1586412825 -0700""
DEBUG: Repository pybind11_bazel instantiated at:
  /mnt/hd1/gabrielcrds/github/google-research/scann/WORKSPACE:12:15: in <toplevel>
Repository rule git_repository defined at:
  /home/gabrielcrds/.cache/bazel/_bazel_gabrielcrds/24d73921da891953adeceaa85dca9e91/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis of target '@com_google_protobuf//:cc_toolchain' failed
INFO: Elapsed time: 0.275s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
```

Any idea how to fix the problem?"
![image](https://user-images.githubusercontent.com/80189847/110230933-22d38a00-7f1d-11eb-9c28-67276b96d899.jpeg)
"Hi, Can anyone please help me running the AutoML-Zero In Google Colab Notebook."
"Hello, I am new here and studying the TFT project. 
I encounter a problem in _batch_sampled_data() which convert time-series data input model inputs and outputs. 

Let me copy and code in here first.

```
    id_col = self._get_single_col_by_type(InputTypes.ID)
    time_col = self._get_single_col_by_type(InputTypes.TIME)
    target_col = self._get_single_col_by_type(InputTypes.TARGET)
    input_cols = [
        tup[0]
        for tup in self.column_definition
        if tup[2] not in {InputTypes.ID, InputTypes.TIME}
    ]

    for i, tup in enumerate(ranges):
      if (i + 1 % 1000) == 0:
        print(i + 1, 'of', max_samples, 'samples done...')
      identifier, start_idx = tup
      sliced = split_data_map[identifier].iloc[start_idx -
                                               self.time_steps:start_idx]
      inputs[i, :, :] = sliced[input_cols]
      outputs[i, :, :] = sliced[[target_col]]
      time[i, :, 0] = sliced[time_col]
      identifiers[i, :, 0] = sliced[id_col]

    sampled_data = {
        'inputs': inputs,
        'outputs': outputs[:, self.num_encoder_steps:, :],
        'active_entries': np.ones_like(outputs[:, self.num_encoder_steps:, :]),
        'time': time,
        'identifier': identifiers
    }
```

In the Volatility dateset, the target_col is 'log_vol' while input_cols are ['log_vol', 'open_to_close', 'days_from_start', 'day_of_week', 'day_of_month', 'week_of_year', 'month', 'Region']

It is reasonable to have 'log_vol' in input data as it is the observation of past timestep. 
However, it is not removed in the period of target. 
As you can see, the outputs is cropped but not inputs in the line of sample_data.

So, I am wonder if the label is removed somewhere else? If not, then it seems strange to include the label in inputs? 

Thank you for your attentions. "
"Hello,

I read your recent article on ""Meta BACK-Translation"" but I could not find the code as stated in your article. The link provided is not working.   

_**https://github.com/google-research/google-research/tree/master/meta_back_translation**_

Is it possible to have a link to the original code ? "
"Hello, thanking for your amazing work. I have read your paper and code, then I have some questions about the dot_product in code. I think the dot_prodoct should be s_loss_old - s_loss_new but s_loss_new - s_loss_old for the reason here.
<a href=""https://ibb.co/xgbvSzc""><img src=""https://i.ibb.co/kDTngqr/image.png"" alt=""image"" border=""0""></a>
Am I wrong?
"
I have a model for the T5 CBQA task and I wish to extract the loglikelihood scores along with predictions from an exported model. How would I go about doing that?
"Hi,
I have been trying to implement the code in Google Colab but not able to run it successfully. I am receiving the following error while running ./run_demo.sh command.

![image](https://user-images.githubusercontent.com/79981317/109807982-9723da00-7c4c-11eb-8126-aa520d199733.png)
"
"Hello, 

In [kws_streaming] part I follow the tutorial and I input
wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz
it's work. but when i input
wget https://storage.googleapis.com/kws_models/models1.zip
it returns 403 forbidden. 
Does my network wrong? Or it just close the access to the model.

Thank you"
"@tkipf Could you please upload the file lists for CLEVR6 / CLEVR10 train and test splits? It would make repro much easier!

From what I understood that the first 70K frames were used for training. How was the test split obtained? What was its size? Was filtering <= 6 objects done before or after getting the train/test splits?

Do you train only on CLEVR6 with crops?

Are crops done both in training and in testing?

Thank you!"
"CODE:
https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py#L28

BUG:
TypeError: Expected int for argument 'seed2' not <tf.Tensor 'oos/while/while/attention_6/random_normal/mod:0' shape=() dtype=int32>.

FIX:
tf.random.normal takes seed as python int. To take seed as tensorflow tensor, please use tf.random.stateless_normal instead.

BTW:
various static Tensor.shape[x] had better be its dynamic counterpart tf.shape(tensor)[x]"
"Hi, 
my case is image recognition (classifier for custom images). I adapted my custom datasets to .npz format and use notebook (https://github.com/google-research/google-research/blob/master/dvrl/main_dvrl_image_transfer_learning.ipynb). 
I have tried to use DVRL for 2 cases:
1. choosing the best described images by model,
2. choosing the best synthetic data

 
1. In this problem I have many described by model photos and some data described by human. My assumption was that data described by human has better quality than desribed by model and dvrl model should can pick up the best  photos from train set. Valid and test is from the same distribution, splited to 2 sets. Datsets:
train = 191443 - data described by model, I assume this set has lower quality than valid and train
valid = 3639 - data described by human, I assume this set has high quality
test = 8268 - data described by human, I assume this set has high quality

First of all, is it possible for dvrl to choose data which are wrong described by model? What do you mean by low quality, is it only jitter on photos, or it could be my case (wrong described photos by model, for instance wrong class)?


2. In this problem I have synthetic data generated by GAN in train dataset. Valid and test has same distribution splited to 2 sets. Valid and test are real photos, not synthetic.
 
train = 1132 - generated by gan
valid = 35 - real data
test = 40 - real data
Number of classes: 4

Does dvrl could choose high quality synthetic data, based on real photos included in valid and test set? It is possible that my valid and test set are to small?"
"Hi @HRLTY and @afathi3, could you please also provide wheel for python 3.8 for sparse convolution op? Python 3.8 is the default for Ubuntu 20.04 LTS.

Regards,"
The README for depth_and_motion says that you should supply a checkpoint for the depth model pretrained on imagenet.  Can you release such a model checkpoint?
"Hi,
It might be a dumb question, but how do I merge subtokenized tokens if I used the **_[gs://cubert/20201018_Java_Deduplicated/github_java_vocabulary.txt]_**  from Cubert project to subtokenize the java code in the first place?

Thank you,
Peter

"
"Hi, 

The dependencies you detail in the `requirements.txt` are not working.

Long story short: 

1. Update the `readme.md` by saying that to run the code, `python==3.6` is needed (I actually tested in on `python3.6.9`). (This step can be made via `virtualenv --python=/usr/bin/python3.6 ~/env`)
2. Update the `requirements.txt` as follow:

```
tensorflow==2.2.0
tensorflow-probability=0.10.1
cloudpickle==1.3.0
```

Here, I detail the reason for this update.


I followed all the steps described, using virtual env and installing the prerequisites.
Doing that, I get the following error from `pip`:

```
Collecting absl-py>=0.5.0
  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)
Collecting numpy>=1.13.3
  Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)
ERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0
ERROR: No matching distribution found for tensorflow==1.14.0
```

I found out that this issue is given because I was using python 3.8.
I, therefore, recreated the virtual environment using python 3.6.
Doing that, the installation of the requirements proceed fine. However, once running 

```
python -m train_online \
 --sub_dir=0 \
 --env_name=HalfCheetah-v2 \
 --eval_target=4000 \
 --agent_name=sac \
 --total_train_steps=500000 \
 --gin_bindings=""train_eval_online.model_params=(((300, 300), (200, 200),), 2)"" \
 --gin_bindings=""train_eval_online.batch_size=256"" \
 --gin_bindings=""train_eval_online.optimizers=(('adam', 0.0005),)""
```

I get a long list of warnings 

```
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/samuele/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
```

 followed by an error

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_online.py"", line 35, in <module>
    tf.compat.v1.enable_v2_behavior()
AttributeError: module 'tensorflow._api.v1.compat.v1.compat' has no attribute 'v1'
```

After seeing this error, I installed the newest `tensorflow==2.4.1`, which, instead, raises the issue

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_online.py"", line 32, in <module>
    from behavior_regularized_offline_rl.brac import train_eval_online
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_eval_online.py"", line 32, in <module>
    from behavior_regularized_offline_rl.brac import train_eval_utils
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_eval_utils.py"", line 25, in <module>
    from tf_agents.environments import suite_mujoco
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/environments/__init__.py"", line 26, in <module>
    from tf_agents.environments import utils
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/environments/utils.py"", line 25, in <module>
    from tf_agents.policies import random_py_policy
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/policies/__init__.py"", line 18, in <module>
    from tf_agents.policies import actor_policy
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/policies/actor_policy.py"", line 29, in <module>
    from tf_agents.networks import network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/__init__.py"", line 18, in <module>
    from tf_agents.networks import actor_distribution_network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/actor_distribution_network.py"", line 26, in <module>
    from tf_agents.networks import categorical_projection_network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/categorical_projection_network.py"", line 26, in <module>
    from tf_agents.networks import network
  File ""/home/samuele/env/lib/python3.6/site-packages/tf_agents/networks/network.py"", line 32, in <module>
    from tensorflow.python.keras.engine import network as keras_network  # TF internal
ImportError: cannot import name 'network'
```
To solve this, I found out that one needs to use `tensorflow-2.2.0`.

Doing that, however, I had a problem with cloudpickle:

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/train_online.py"", line 31, in <module>
    from behavior_regularized_offline_rl.brac import agents
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/agents.py"", line 21, in <module>
    from behavior_regularized_offline_rl.brac import bc_agent
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/bc_agent.py"", line 25, in <module>
    from behavior_regularized_offline_rl.brac import networks
  File ""/home/samuele/Projects/google_research/behavior_regularized_offline_rl/brac/networks.py"", line 24, in <module>
    import tensorflow_probability as tfp
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/__init__.py"", line 76, in <module>
    from tensorflow_probability.python import *  # pylint: disable=wildcard-import
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py"", line 23, in <module>
    from tensorflow_probability.python import distributions
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/distributions/__init__.py"", line 88, in <module>
    from tensorflow_probability.python.distributions.pixel_cnn import PixelCNN
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/distributions/pixel_cnn.py"", line 37, in <module>
    from tensorflow_probability.python.layers import weight_norm
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/layers/__init__.py"", line 31, in <module>
    from tensorflow_probability.python.layers.distribution_layer import CategoricalMixtureOfOneHotCategorical
  File ""/home/samuele/env/lib/python3.6/site-packages/tensorflow_probability/python/layers/distribution_layer.py"", line 28, in <module>
    from cloudpickle.cloudpickle import CloudPickler
ImportError: cannot import name 'CloudPickler'
```

I found out that there is an incompatibility between `tensorflow-probability` and `cloudpickle`.
I've therefore switched to `tensorflow-probability-0.10.1` that aims to overcome this issue and requires `cloudpickle==1.3.0`.

This solved the problem.
"
"Firstly, a big thanks for open sourcing this work.

I am following [01_train.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/01_train.ipynb) and [02_inference.ipynb](https://github.com/google-research/google-research/blob/master/kws_streaming/colab/02_inference.ipynb) for training a model for keyword detection on a dataset which has only 1 class (+ unknown + silence).
I have 2 questions:
1. What size of the dataset will I require to train a model that is robust enough to be relied upon in real life? (are 3000 instances of keyword class of 1-sec audio clips good enough?) (size of unknown words?)

2. Tips, suggestions for training?"
"Hi @uniqueness, 

thank you very much for releasing the code from the paper. Can you please also release the pre-trained models? I would be interested in using the estimated depths for the Waymo dataset.

Thank you very much in advance."
"File ""/home/google-research/meta_pseudo_labels/augment.py"", line 63, in <module>
    'AutoContrast': autocontrast,
NameError: name 'autocontrast' is not defined

How can I solve it?"
Am I missing something?
"Hi @HRLTY and @afathi3,
Thanks for releasing tf3d repo. 
Got some questions regarding current model architecture for 3d object detection:
* The graph layer is missing compared with paper. Why? It seems like interesting concept
* Can we expect implementation of LSTM enhancement from the second paper?

Regards,"
"Any instructions to know how to load the provided pre-trained models? I understand that these are probably just BERT models trained on Source code using different tokenizer but I can't figure out how to incorporate the Cubert tokenizer with original BERT files to load / use the pretrained models. Any help is appreciated. Also please correct me if I am wrong about my assumption above.
Thanks!"
"Hi @eladeban,

For mpi_extrapolation, you have shown how to run the testing code. As an argument, you're passing a file (`mpi_extrapolation/examples/0.npz`), but I can't find this file in the repository. Can you kindly add this file or update the readme on the structure of this file?"
"@tkipf I'm trying slot attention with higher level features.

Hard K-means variant trains very fast, while the full attention variant with `slots = updates` is prone to gradient blow-up (probably because of softmax and maybe), even with warmup. 

It seems that the `slots = updates` variant does not do residual update in contrast to GRU (which in return requires learned parameters). Does it make sense to first try some residual / momentum `slots = alpha * slots_prev + (1 - alpha) * updates`?

Also it turns out that temperature `sqrt(slot_size)` is too high, I've had to decrease it for softmax not converging to uniform assignment.

Could you recommend what gradients / distributions should I monitor? Gradients wrt projection weights? Gradients wrt inputs?

Have you tried using more than 3 iterations in training? E.g. 10 iterations? Was it still stable?"
"Hi!
Thanks a lot for the great work! I am considering using ROAR in my own research but would like to clarify something first. In the paper and also in the README you mention that you use the estimated feature importance, and you write 
> A feature importance estimate is a ranking of the contribution of each input pixel to the model prediction for that image.

From my own experiments with some of the methods, e.g. Integrated Gradients, I know that the attributions can also be negative. I couldn't find any hint how negative attributions are treated. I could imagine a few possible solutions, therefore I was wondering if you could clarify how it is done. Are the contributions ranked by their absolute value, are negative values ignored, are the contributions sorted from large positive to large negative values?

Thanks a lot for your time, I would really appreciate an answer!
Best regards
Verena  Haunschmid

tagging: @sarahooker @doomie"
"Hi, thanks for your sharing of MPL paper and code and I have two question about this paper.
Firstly, during the updating of student on pseudo labeled data, is the student trained for many epochs until convergence or only sample a batch of unlabeled data to update student for one time ?
Secondly, what is the component of teacher’s update gradients in the two moon dataset experiment? Does it contains student feedback, supervised gradient and UDA gradient all together? I tried to test MPL using only supervised loss and students feed back on this toy dataset, but did not get result as good as your paper."
"Hi, 
First, I appreciate that your researches and shared codes.

I studied the TFT (Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting, Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister) to solve a time-series forecasting problem. 

The paper is well-written to understand and the codes can be found on this github.

However, (as noticed in this title) I'm very confused because the output (labels, targets) is included in input dict.
there can be static covariates, history of target, etc., as inputs to predict the future target. 
But, I found that the target (output) is included in input data. 

How can it be possible? does it make sense?

Sincerely, "
"Hi, good implementation in directory ""google-research/ged_tts""! 
Following usage in README, I got points like 6.158, 4.535 in audio generated by WaveRNN, and Parallel WaveGAN respectively, which are a lot higher than those in the paper ""A Spectral Energy Distance for Parallel Speech Synthesis"", do you know why? Thanks!"
"Hi,

I have looked at the code, but it is not exactly clear on how to obtain representation. Is this something to obtain from layers.get_masked_lm_output, after the NN transformation?

In the paper you suggest that you take the first block representation, but it is not exactly clear how you extract the CLS token from the code at the document level 

Many thanks"
"I tried to run the command given in the readme file for training testing split in the dataset. But I am getting the following error. Can you help me with this?

![issue_new](https://user-images.githubusercontent.com/35806906/106694185-3f209600-6602-11eb-97f0-1db20ccb4e4a.png)
"
"Hi @uniqueness and @gariel-google ,

Thank you for providing the code for this wonderful work! I have some questions and hopefully you will get back to me.

1. What is the general purpose and significance of this [Auto_mask](https://github.com/google-research/google-research/blob/88d2083a36cc0566585ed4e431534f9e3db0b312/depth_and_motion_learning/object_motion_nets.py#L212)? Why do you compute the mean_sq_residual_translation over all residual translations in the batches instead of independent mean_sq_residual_translation for each residual translation in the batch? If I recall correctly then the 0th dimension should have batch, and this [line](https://github.com/google-research/google-research/blob/88d2083a36cc0566585ed4e431534f9e3db0b312/depth_and_motion_learning/object_motion_nets.py#L215) will take the mean over the entire batch. 

2. I tried training your object motion model on the Kitti data set but the residual translations in some images were not sparse which affected the entire performance of the model. How can I change alpha and beta in the motion regularization to make them more sparse and constant throughout a moving object? 

3. Did you try predicting object motion without using depth information? If so, how were the results? Do you think its possible to extract motion maps without using depth since this method requires us to predict depth for 2 frames?

4. Is your method extendable to an ""n"" frame sequence like in other methods? Did you try 3 frames?

Thanks !"
"I am using the following command:
`python -m uflow.uflow_main --train_on=""sintel:/mnt/d/Courses/CTU/Diploma/data/sintel-test-tf/test/final/"" --plot_dir=/mnt/d/Courses/CTU/Diploma/data/results/sintel-plots/ --checkpoint_dir=/mnt/d/Courses/CTU/Diploma/data/results/sintel-checkpoints/`

This gave me the following traceback with the `NonImplementedError`. This was solved by downgrading the `numpy` to 1.19.5. I guess, either the code should be updated to fix the error, or the `requirements.txt` should be updated to include only compatible numpy versions instead of `numpy>=1.18.3`, which installs the incompatible newer version.

```
.Traceback (most recent call last):
  File ""/home/iegorval/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/iegorval/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/iegorval/Diploma/google-research/uflow/uflow_main.py"", line 379, in <module>
    app.run(main)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/iegorval/Diploma/google-research/uflow/uflow_main.py"", line 345, in main
    occ_active=occ_active)
  File ""/home/iegorval/Diploma/google-research/uflow/uflow_net.py"", line 549, in train
    occ_active=occ_active)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 871, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 726, in _initialize
    *args, **kwds))
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2969, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3361, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3206, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 990, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 634, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 3887, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 977, in wrapper
    raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:

    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:482 train_step  *
        weights,
    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:453 train_step_no_tf_function  *
        losses, gradients, variables = self._loss_and_grad(
    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:625 _loss_and_grad  *
        losses = self.compute_loss(
    /home/iegorval/Diploma/google-research/uflow/uflow_net.py:668 compute_loss  *
        flows, selfsup_transform_fns = uflow_utils.compute_features_and_flow(
    /home/iegorval/Diploma/google-research/uflow/uflow_utils.py:982 compute_features_and_flow  *
        flow = teacher_flow_model(
    /home/iegorval/Diploma/google-research/uflow/uflow_model.py:209 call  *
        warp_up = uflow_utils.flow_to_warp(flow_up)
    /home/iegorval/Diploma/google-research/uflow/uflow_utils.py:40 flow_to_warp  *
        i_grid, j_grid = tf.meshgrid(
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3552 meshgrid
        mult_fact = ones(shapes, output_dtype)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:3120 ones
        output = _constant_if_small(one, shape, dtype, name)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:2804 _constant_if_small
        if np.prod(shape) < 1000:
    <__array_function__ internals>:6 prod
        
    /home/iegorval/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3031 prod
        keepdims=keepdims, initial=initial, where=where)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    /home/iegorval/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:855 __array__
        "" a NumPy call, which is not supported"".format(self.name))

    NotImplementedError: Cannot convert a symbolic Tensor (pwc_flow/meshgrid/Size:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported

```"
the link is https://github.com/google-research/google-research/tree/master/wt5
"For the dual_pixels project. I wonder whether I can use Google Pixel 5 for data capturing. Is there a best model for such data capturing, or any model works equally? Thanks a lot."
"I'm following the code flow by running example.py and defining search as below,
`searcher = scann.ScannBuilder(normalized_dataset, 10, ""dot_product"").tree(
    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(2, anisotropic_quantization_threshold=0.2).reorder(100).create_pybind()`

I was expecting to see the part where it trains codewords with the loss function that was proposed in the paper, but I only see training codewords with GenericKmeans (original kmeans-clustering algorithm). 

Does the example.py uses loss function from the paper? If not, how can I change the searcher definition to make it use the loss function from the paper?"
""
"hi, scann compile from source, the tensorflow must be gpu version, or cpu version is ok?"
"Hi everyone.
Are there any plans to add C or C++ API to SCANN? Or maybe would you consider an external PR implementing it?"
"Hi there,

I really enjoyed the recent release of **T5X**.
While having no problems running it on Colab (standard notebook with one GPU), I am facing some difficulties in running the code on one of my machines. Specifically, my configuration is Ubuntu 18.04, CUDA 11, CuDNN 7, with **two GPUs**  (TITAN Xp, 12GB each).
The GPUs are recognized by JAX, and the code runs correctly up to the `pmap` related to `p_train_epoch`.

The error is the following:
`RuntimeError: Unimplemented: Requested AllReduce not implemented on GPU; replica_count: 2; operand_count: 131; IsCrossReplicaAllReduce: 1; NCCL support: 1; first operand array element-type: BF16`

By working with float32 the training is performed successfully. It appears an error related to using **jnp.float16** on **multi-GPUs**. Can you help me?

Here the complete Traceback:
```
Traceback (most recent call last):
  File ""ft_t5_small_super_glue.py"", line 84, in <module>
    train(model_dir='t5x_data', config=fine_tuning_cfg)
  File ""/workspace/linear_t5x/src/t5x_utils/training.py"", line 525, in train
    jnp.array(0, dtype=jnp.int32), 1)
  File ""/opt/conda/lib/python3.6/site-packages/jax/api.py"", line 1564, in f_pmapped
    global_arg_shapes=tuple(global_arg_shapes_flat))
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 1262, in bind
    return call_bind(self, fun, *args, **params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 1226, in call_bind
    outs = primitive.process(top_trace, fun, tracers, params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 1265, in process
    return trace.process_map(self, fun, tracers, params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/core.py"", line 598, in process_call
    return primitive.impl(f, *tracers, **params)
  File ""/opt/conda/lib/python3.6/site-packages/jax/interpreters/pxla.py"", line 635, in xla_pmap_impl
    *abstract_args)
  File ""/opt/conda/lib/python3.6/site-packages/jax/linear_util.py"", line 251, in memoized_fun
    ans = call(fun, *args)
  File ""/opt/conda/lib/python3.6/site-packages/jax/interpreters/pxla.py"", line 892, in parallel_callable
    compiled = xla.backend_compile(backend, built, compile_options)
  File ""/opt/conda/lib/python3.6/site-packages/jax/interpreters/xla.py"", line 349, in backend_compile
    return backend.compile(built_c, compile_options=options)
```
"
"Hi there,

I really enjoyed reading the paper by @hyhieu et al.! I don't have the resources to train the models, or even run the code, but I do have some questions about the code. Especially about the loss of the teacher model. If I'm understanding correctly, the final loss is defined here: 

https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L494-L496

I have a question about the MPL term. `cross_entropy['mpl'] * dot_product`.

`dot_product` seems to be a scalar (like mentioned in the paper), without gradient computation:

https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L483

`cross_entropy['mpl']` seems to be defined a few lines above:
https://github.com/google-research/google-research/blob/ec13eb6661a7b9500016cc6d7e3ab940c2dbf184/meta_pseudo_labels/training_utils.py#L484-L487

This seems to be a cross-entropy where the logits and targets are the same (since `softmax_cross_entropy` also applies a softmax to the logits). This loss can be non-zero when the targets are not 'hard' (soft targets), however, I don't think there is any signal in that case? Since the logits and targets seem to be equal. 

My question is — if I'm understanding the code correctly — if there is no signal and it's scaled by the `dot_product`, I do not know what the value is of this term in the optimization?

Thanks again for your work!
Hans

P.S. small side question: in the paper, it is mentioned the method uses hard-targets, but I cannot find this in the code.

"
"Hello developers, 
I would like to know which is the space partitioning algorithm used in ScaNN to reduce the number of instances to evaluate. I read in the docs that is a tree-based algorithm. Is it possible to have some details about it? Maybe a reference to a paper. 
"
"There are only simplified vertex CAD files in keypose.
Is there a way to download complete cad models of keypose?"
"Hi,

I tried to train a kws_streaming ""**ds_tc_resnet**"" model on my own data, but when the training is finished, I got the Error,



    W0114 14:36:01.360302 140229770733312 test.py:675] FAILED to convert to mode STREAM_EXTERNAL_STATE_INFERENCE, tflite: in user code:
    /homecode/2021/google-research/kws_streaming/layers/speech_features.py:237 call  *
        outputs = self._mfcc_op(outputs)
    /home/code/2021/google-research/kws_streaming/layers/speech_features.py:195 _mfcc_op  *
        outputs = self.data_frame(inputs)
    /home/code/2021/google-research/kws_streaming/layers/data_frame.py:114 call  *
        output, self.output_state = self._streaming_external_state(
    /home/code/2021/google-research/kws_streaming/layers/data_frame.py:201 _streaming_external_state  *
        raise ValueError('inputs.shape[1]:%d must be = self.frame_step:%d' %

    ValueError: inputs.shape[1]:320 must be = self.frame_step:160

Here is my training cmd,

CUDA_VISIBLE_DEVICES=6 python -m kws_streaming.train.model_train_eval --data_url '' --data_dir /data/audio_data/HiCar_data/RecordedSLU4TCResNet/ --batch_size 100 --train_dir ./models/RecordedSLU4StreamingDSTCResNetHiCarNoise/ --wanted_words my_wanted_words --clip_duration_ms 3000  --split_data 0  --mel_upper_edge_hertz 7600  --how_many_training_steps 40000,40000,20000,20000   --learning_rate 0.001,0.0005,0.0002,0.0001 --window_size_ms 30.0  --window_stride_ms 10.0  --mel_num_bins 60  --dct_num_features 40  --resample 0.15 --alsologtostderr  --train 1 --feature_type 'mfcc_op'  --fft_magnitude_squared 1  --use_spec_augment 1 --time_masks_number 2 --time_mask_max_size 25 --frequency_masks_number 2 --frequency_mask_max_size 7 --pick_deterministically 1   ds_tc_resnet --activation 'relu'  --dropout 0.0  --ds_filters '128,64,64,64,128,128'  --ds_repeat '1,1,1,1,1,1'  --ds_residual '0,1,1,1,0,0' --ds_kernel_size '11,13,15,17,29,1' --ds_stride '1,1,1,1,1,1' --ds_dilation '1,1,1,1,2,1'

Did I set something wrong?
Thank you."
"Hi,
I tried to train the model and notice that ""model.save_weights"" in keras does not have the parameter max_to_keep parameter.
Should I delete some checkpoints by myself or could you please give some tips?
Thank you."
Thank you
""
"Hello @ddemszky  @dattias and @eladeban,
I'm trying to apply your model to some text that is not part of the GoEmotions dataset.
To do so I need to mask the input text in the same way you do, that is replacing religions and names with the [RELIGION] and [NAME] tokens.
In the paper I read that the list of religion terms is included with the dataset, but I can't find it, maybe I'm looking in the wrong places?
Also, I'm looking for the BERT-based NER model you used for name tagging, do you have a link to the model by any chance?
Thank you very much!"
""
"Hi, thank you for your great work. I'm trying to fine-tune TRILL (https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark) but got the bug bellow:

```
LookupError: No gradient defined for operation 'map/while' (op type: StatelessWhile)
```

Here is a code to reproduce this bug:

```
import tensorflow as tf
import tensorflow_hub as hub


model = tf.keras.models.Sequential()
model.add(tf.keras.Input((128000,)))  # Input is [bs, input_length]
trill_layer = hub.KerasLayer(
    handle='https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/3',
    trainable=True,
    arguments={'sample_rate': tf.constant(16000, tf.int32)},
    output_key='embedding',
    output_shape=[None, 2048]
)
model.add(trill_layer)

opt = tf.keras.optimizers.Adam(lr=0.001)

model.summary()

@tf.function
def train_step():
    with tf.GradientTape() as tape:
        logits = model(
            tf.random.uniform(shape=[1, 128000], dtype=tf.float32), 
            training=True
        )
        loss = tf.reduce_mean(logits)
    grads = tape.gradient(loss, model.trainable_variables)
    opt.apply_gradients(zip(grads, model.trainable_variables))
    tf.print(loss)


for _ in range(1000):
    train_step()
```

@joel-shor can you take a look? "
"Hi, I've used the code to train model on sintel dataset. The training is not completed and I try to do evaluation on the mediate models. I use the script: 
```
python3 -m uflow.uflow_evaluator --eval_on=""sintel:uflow/data_tfrecords/sintel/test/clean"" --plot_dir=uflow/plot/sinte    l --checkpoint_dir=uflow/checkpoints/sintel
```
then I get the following error:
```
I0107 17:50:51.010394 140387742058304 uflow_evaluator.py:61] New checkpoint found: uflow/checkpoints/sintel/ckpt-526
2021-01-07 17:50:51.090070: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at example_parsing_ops.cc:476 : Invalid argument: I
nconsistent number of elements for feature flow_uv: 0 vs 1
2021-01-07 17:50:51.090382: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at iterator_ops.cc:941 : Invalid argument: Inconsis
tent number of elements for feature flow_uv: 0 vs 1
         [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]
2021-01-07 17:50:51.092987: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at example_parsing_ops.cc:476 : Invalid argument: I
nconsistent number of elements for feature flow_uv: 0 vs 1
Traceback (most recent call last):
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py"", line 1897, in execution_mo
de
    yield
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 659, in _next
_internal
    output_shapes=self._flat_output_shapes)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py"", line 2479, in iterat
or_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 6606, in raise_from_n
ot_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent number of elements for feature flow_uv: 0 vs 1
         [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/google-research-master/uflow/uflow_evaluator.py"", line 98, in <module>
    app.run(main)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/google-research-master/uflow/uflow_evaluator.py"", line 92, in main
    evaluate()
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/google-research-master/uflow/uflow_evaluator.py"", line 66, in evaluate
    eval_results = evaluate_fn(uflow)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_data.py"", line 277, in eval_function
    width, progress_bar, plot_dir, num_plots)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/data/data_utils.py"", line 222, in evaluate
    for test_batch in it:
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 630, in __nex
t__
    return self.next()
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py"", line 665, in _next
_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mo
de
    executor_new.wait()
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent number of elements for feature flow_uv: 0 vs 1
         [[{{node ParseSingleSequenceExample/ParseSequenceExample/ParseSequenceExampleV2}}]]

If you suspect this is an IPython 7.12.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with ""%tb"", or use ""%debug""
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

```
I think it's strange, because when i do evaluation on Chairs, it works well.  Could you help to fix this problem?
"
"Hi, I found some small mistakes in `data_conversion_scripts/convert_KITTI_flow_to_tfrecords.py`, for example:

> line 71: flow_path = os.path.join(data_dir, FLAGS.subdir, 'flow_' + version, 

After fixing generating the KITTI15 tfrecords data, I tried to do training. In training,  I got the following error:
```
Restoring model from checkpoint uflow/checkpoints/kitti15.
Making eval datasets and eval functions.
Making training iterator.
Starting training loop.

Traceback (most recent call last):
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_main.py"", line 379, in <module>
    app.run(main)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/scratch/workspace/wenjingk/anaconda-3.6/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_main.py"", line 357, in main
    uflow_plotting.print_log(log, epoch)
  File ""/proj/xcdhdstaff1/wenjingk/SLAM/uflow/uflow_plotting.py"", line 46, in print_log
    np.mean(log['total-loss'][-mean_over_num_steps:]))
KeyError: 'total-loss'

```
The train data loader **train_it** is empty and the training loop wasn't executed. Could you help to fix?"
"Thank you for this awesome work. 

When trying to train the UFlow network on the billiard dataset, I had the following error message : Unknown data format ""custom"". 
After looking at the uflow_data.py file I found no support for 'custom' dataset format on the make_train_iterator function. 

Is there a way to avoid this error ?

Finally, kindly share any tips for training Uflow on an unsupervised dataset other than 'Flying chairs', 'Sintel', 'Kitti'.

Best regards,
Mouad Lazrak.
"
"1. I use the transformer model in models/official/nlp/transformer/transformer.py to train a seq2seq model, replacing the built-in keras attention with performer fast-attention(tensorflow version),  for example, the decoder stack is as follows
![image](https://user-images.githubusercontent.com/7539692/103272848-d76e9e00-49f8-11eb-9594-b0f57114e67a.png)
> It seams images can't be displayed: 
```
    # DecoderStack
    def build(self, input_shape):
        """"""Builds the decoder stack.""""""
        params = self.params
        for _ in range(params[""num_hidden_layers""]):
            if not params[""use_fast_attn""]:  # ==========set to True to use fast_attention=============
                self_attention_layer = attention_layer.SelfAttention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""])
                enc_dec_attention_layer = attention_layer.Attention(
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""])
            else:
                self_attention_layer = fast_attention.SelfAttention(  # ==========use fast_attention=============
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""],
                    kernel_transformation=fast_attention.softmax_kernel_transformation,
                    causal=True,
                    projection_matrix_type=1,
                    nb_random_features=nb_random_features,  # =============100(model hidden size is 512)============
                )
                enc_dec_attention_layer = fast_attention.Attention(  # ==========use fast_attention=============
                    params[""hidden_size""], params[""num_heads""],
                    params[""attention_dropout""],
                    kernel_transformation=fast_attention.softmax_kernel_transformation,
                    causal=False,
                    projection_matrix_type=1,
                    nb_random_features=nb_random_features,
                )
            feed_forward_network = ffn_layer.FeedForwardNetwork(
                params[""hidden_size""], params[""filter_size""], params[""relu_dropout""])

            self.layers.append([
                PrePostProcessingWrapper(self_attention_layer, params),  # x->LNorm(x)->{layer(x)=y}->Dropout(y)->y+x
                PrePostProcessingWrapper(enc_dec_attention_layer, params),
                PrePostProcessingWrapper(feed_forward_network, params)
            ])
        self.output_normalization = tf.keras.layers.LayerNormalization(
            epsilon=1e-6, dtype=""float32"")
        super(DecoderStack, self).build(input_shape)
```

2. The training stage is perfect, with good decoder outputs almost the same as the targets. An example is, if the target is **""He is playing basktball in the basketball gym""**, the output could be the same.

3. But when doing inference, the decoder outputs are very bad. The output for the above example now is **""He is playing is playing is playing is playing is playing </s>""**, even more worse with **""He He He He He He He He He He He He </s>""**.

4. I just change the mode from ""train"" to ""predict"", and use beam search. Whether setting beam_size=1 or not, the output is the same and bad.

5. When doing inference, I check the **decoder attention output(layer0)** in ""fast_attention.py>Attention>call()"", the states between beams of an sample are almost the same in step 2. 
![image](https://user-images.githubusercontent.com/7539692/103276792-7b107c00-4a02-11eb-8604-f9f72ae47bee.png)
![image](https://user-images.githubusercontent.com/7539692/103276588-ec9bfa80-4a01-11eb-9c57-c4edb4565d63.png)

> It seams images can't be displayed: 
```      
        if cache is not None:
            # Combine cached keys and values with new keys and values.
            if decode_loop_step is not None:
                cache_k_shape = cache[""k""].shape.as_list()
                indices = tf.reshape(
                    tf.one_hot(decode_loop_step, cache_k_shape[1], dtype=key.dtype),
                    [1, cache_k_shape[1], 1, 1])
                key = cache[""k""] + key * indices
                cache_v_shape = cache[""v""].shape.as_list()
                indices = tf.reshape(
                    tf.one_hot(decode_loop_step, cache_v_shape[1], dtype=value.dtype),
                    [1, cache_v_shape[1], 1, 1])
                value = cache[""v""] + value * indices
        else:
            key = tf.concat([tf.cast(cache[""k""], key.dtype), key], axis=1)
            value = tf.concat([tf.cast(cache[""v""], value.dtype), value], axis=1)
    
            # Update cache
            cache[""k""] = key
            cache[""v""] = value

        attention_output = favor_attention(query, key, value,
                                           self.kernel_transformation, self.causal,
                                           bias, projection_matrix)
        
        # the attention_output is: 
        attention_output = tf.Tensor(
        [[-9.661879   2.9591107  1.3627272 ... -5.7556434 -4.247857   5.5499997]
         [-9.661878   2.9591098  1.3627269 ... -5.7556424 -4.247855   5.55     ]
         [-9.661879   2.9591098  1.3627272 ... -5.755643  -4.247857   5.5499997]
         [-9.661878   2.9591103  1.3627266 ... -5.755642  -4.2478557  5.549999 ]
         [-9.661879   2.9591098  1.3627272 ... -5.7556434 -4.247856   5.5499997]], shape=(5, 512), dtype=float32)
```

6. Using the raw code and the built-in keras attention, everying is good."
"After running `configure.py` I try to build with Bazel, but I'm getting the following error:

    $ CC=clang bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
    INFO: Build option --action_env has changed, discarding analysis cache.
    WARNING: /private/var/tmp/_bazel_thdy/2bcb61de71a5e80e9c149676ae8deec4/external/local_config_tf/BUILD:5430:8: target 'ensorflow_framework.2' is both a rule and a file; please choose another name for the rule
    ERROR: /private/var/tmp/_bazel_thdy/2bcb61de71a5e80e9c149676ae8deec4/external/local_config_tf/BUILD:11:11: in srcs attribute of cc_library rule @local_config_tf//:libtensorflow_framework: '@local_config_tf//:ensorflow_framework.2' does not produce any cc_library srcs files (expected .cc, .cpp, .cxx, .c++, .C, .cu, .cl, .c, .h, .hh, .hpp, .ipp, .hxx, .h++, .inc, .inl, .tlh, .tli, .H, .tcc, .S, .s, .asm, .a, .lib, .pic.a, .lo, .lo.lib, .pic.lo, .so, .dylib, .dll, .o, .obj or .pic.o)
    ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis of target '@local_config_tf//:libtensorflow_framework' failed
    INFO: Elapsed time: 0,692s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (5 packages loaded, 319 targets configured)
        Fetching ...ogle_protobuf; Cloning tags/v3.9.2 of https://github.com/protocolbuffers/protobuf.git

This is on

    $ uname -a
    Darwin mac611211 19.6.0 Darwin Kernel Version 19.6.0: Mon Aug 31 22:12:52 PDT 2020; root:xnu-6153.141.2~1/RELEASE_X86_64 x86_64"
"https://github.com/google-research/google-research/tree/master/bertseq2seq
In the above repo, can you tell on how many WMT2014 en-> de sentences the bertseq2seq/bert24_en_de was trained?"
"When executing `test_softmax_noncausal_attention_block_output` in `performer/fast_attention/tensorflow/fast_attention_test.py`
it produces a maximum error of 0.20 which is clearly below the max allowed error `max_error = 2.0`.
However, when changing `length` to a different value. The maximum relative error jumps up significantly.
The commit I'm running on is: 6f29c8099fbb1474769046276c8744521470cab9
I run on TF 1.15.
Here is a table of length vs error and unit test state:

length | relative error | unit test status
------|-----|------
2 | 0.20 | Pass
4 | 1.24 | Pass
8 | 106 | FAIL
10 | 9.7 | FAIL

Ping @xingyousong
"
"Hello, firstly thanks for sharing code and model. I notice ""classifier_activation"" is set to false in model config, which means the output logit of NSP won't be normalized using ""tanh"" and value will be quite large, like [3e6, 3e6]. Since I think MLM logit is normalized through the final layer normalization layer and won't be very large, is it correct to combine the small MLM loss with a huge NSP loss? or I miss some processing steps in the code?
Hope you can explain that for me, and please tell me the final loss of MLM and NSP. Thank you!"
"I got absl.flags._exceptions.IllegalFlagValueError: flag --output_dir=None: Flag --output_dir must have a value other than None.
so i put dir address.

```
`2020-12-22 18:51:53.991739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --data_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --vocab_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --bert_config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:975: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 550, in _assert_validators
    validator.verify(self)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py"", line 82, in verify
    raise _exceptions.ValidationError(self.message)
absl.flags._exceptions.ValidationError: Flag --output_dir must have a value other than None.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\google-research\goemotions\bert_classifier.py"", line 975, in <module>
    tf.app.run()
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 297, in run
    flags_parser,
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 366, in _run_init
    flags_parser=flags_parser,
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 213, in _register_and_parse_flags_with_usage
    args_to_main = flags_parser(original_argv)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\app.py"", line 31, in _parse_flags_tolerate_undef
    return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 658, in __call__
    self.validate_all_flags()
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 532, in validate_all_flags
    self._assert_validators(all_validators)
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_flagvalues.py"", line 553, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --output_dir=None: Flag --output_dir must have a value other than None.`
```



AND  after that, I got this error below... what should i do.

```
`2020-12-22 19:00:07.287395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --data_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --vocab_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --bert_config_file has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\flags\_validators.py:356: UserWarning: Flag --output_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:975: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

28 labels
Multilabel: False
Getting distance matrix...
[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W1222 19:00:09.434083  5656 module_wrapper.py:139] From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W1222 19:00:09.453032  5656 module_wrapper.py:139] From D:\google-research\goemotions\bert_classifier.py:765: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From D:\google-research\goemotions\bert\modeling.py:113: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

W1222 19:00:09.467995  5656 module_wrapper.py:139] From D:\google-research\goemotions\bert\modeling.py:113: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

Windows fatal exception: access violation

Current thread 0x00001618 (most recent call first):
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 84 in _preread_check
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 122 in read
  File ""D:\google-research\goemotions\bert\modeling.py"", line 114 in from_json_file
  File ""D:\google-research\goemotions\bert_classifier.py"", line 773 in main
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\absl\app.py"", line 303 in run
  File ""C:\Users\User\anaconda3\envs\research\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40 in run
  File ""D:\google-research\goemotions\bert_classifier.py"", line 975 in <module>
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 85 in _run_code
  File ""C:\Users\User\anaconda3\envs\research\lib\runpy.py"", line 193 in _run_module_as_main`
```"
"Dear @xingyousong 


I tried to run your ES-MAML code, but I failed.

In declaration phase, `blackbox_maml_objects.py` does import like below.

```
from es_maml.first_order import first_order_pb2
from es_maml.first_order import first_order_pb2_grpc

from es_maml.zero_order import zero_order_pb2
from es_maml.zero_order import zero_order_pb2_grpc
```

However, there is no `first_order_pb2`, `first_order_pb2_grpc`, `zero_order_pb2` and `zero_order_pb2_grpc` in `es_maml/first_order` and `es_maml/zero_order` directories.

As result, an error occurred like below.
`ImportError: cannot import name 'first_order_pb2' from 'es_maml.first_order' (unknown location)`

Could you upload those things to provide workable ES_MAML code?

If I misunderstood the way to run your code, please give me any advise.

Thank you in advance."
"Hello,
When I run MolDQN at about 3500episode, I found that the memory of the server has been exhausted and the program can not run. What is the reason for this? Please tell me. Thank you very much. Best wishes.

Anny
2020.12.21"
"Thanks again for releasing all the Closed Boook QA T5 checkpoints. The models are now fully ported to hugging face and the < xl/3b models can also be used on the inference API: https://huggingface.co/google/t5-large-ssm-nq?text=how+many+episodes+in+season+2+breaking+bad%3F

At the moment we use simple greedy search as the decoding algorithm for those models, but we could switch to beam search if it gives better performance. What is your opinion here? In case you found that beam search works better, could you post your suggested hyperparameters (max_length/min_length/num_beams/length_penalty). Thanks a lot!

Hope it's ok to tag you here @adarob :-) 
"
"Hi, thanks for this wonderful project. Are there any pretrained weights for IB-BERT(large) available for download, or planned to be?"
""
"Hi,
Thanks for this nice project!
Are there any pretrained weights available for download, or planned to be?
"
"I've followed all the instructions mentioned in this code repository, but when I want to run this algorithm on my dataset, I get the following error: 

IllegalFlagValueError: flag --input_graph=None: Flag --input_graph must have a value other than None.

Can someone help?

Many thanks in advance. "
"Hello!
In the ""gen_train_tfrecords.py""， the h5 files be note as
 ""
The instructions
for downloading the H5 files are located in the Github page:
https://github.com/una-dinosauria/3d-pose-baseline.
""

But I can't find the structure of  the h5 files in this page. Can you give me some examples of the h5 files?
Looking forward to your advice."
Hello. I’m very interested in the paper “Widget Captioning: Generating Natural Language Description for MobileUser Interface Elements” published in EMNLP 2020. I went to the url provided in the paper for dataset but got nothing. Any plan to release widget captioning dataset? Thanks!
"Is anisotropic product quantization (section 4.1 in [paper](https://arxiv.org/pdf/1908.10396.pdf)) implemented in scann library?
If so, it would be great if you show how to do it in code.

Thanks!"
"Can you provide pretrained model on waymo dataset and some guides for  running the inference code? Although there is README file for using the sparse conv, i still stuck in runing the inference code. I want to reproduce the  reported 12ms inference time @HRLTY "
"Hi all,

 I'm checking out the new unsupervised depth model. I can see that there is no separate inference script. Which is the proposed way to run prediction?
 - Should the depth_motion_field_model.py (function infer_depth(rgb_image, params) ) be applied and somehow  the input batch passed to it? Normalized?
 - Like at the depth_from_videos_in_the_wild,  one should use and slightly modify the inference script from struct2depth project? (signiture of infer_depth is now quite different as it was earlier with inference_depth(im_batch, sess) ).

BEst Regards
 György "
"Hi, thanks for sharing your code for EuRoC depth map groundtruth generation.

I tested it and it seems that the image is not exactly aligned with generated groundtruth.

It can even be seen in the paper at fig. A1, here is visualization of image + depthmap :
![problem_depth](https://user-images.githubusercontent.com/4380424/100263180-75c89900-2f4d-11eb-96bc-5c0606641704.png)
![problem_depth_zoom](https://user-images.githubusercontent.com/4380424/100263205-7e20d400-2f4d-11eb-9ca2-dd0aa0d39ba8.png)

You can see on the zoom that background pixels are actually thought to be foreground.

You could say that this is very subtle, but the de-alignment change from video to video, when trying it on first video, V1_01_easy, the difference is actually huge :

![1403715283262142976mg](https://user-images.githubusercontent.com/4380424/100272803-07d79e00-2f5c-11eb-8d83-2735c99fbf67.png)

You can see how the cupboard is much more on the left on the depth map than on the image. Also, notice the tatami on the far left which is very dephased.

Is this an implementation problem ?
I tried to check the code and it seems good to me. Does it mean that the Odometry of EuRoC, at least for these two scenes is not perfect ? It seems to me that the position is good, but not the orientation.

To check if it is only a problem of camera extrinsics, which would explain the good position but bad orientation, I converted the EuRoC to Colmap : https://gist.github.com/ClementPinard/662c6642c4be37c5030fedaf5ad5414c

And then I used the point triangulator : https://colmap.github.io/faq.html#reconstruct-sparse-dense-model-from-known-camera-poses to know if there was a rigid transformation offset.

It turns out that the transformation is not constant, otherwise, we wouldn't get this reconstruction : 

![colma_problem2](https://user-images.githubusercontent.com/4380424/100285619-7a06ad80-2f71-11eb-8f6d-4f940211e352.png)


Here is one reconstruction when we map everything from scratch without using groundtruth pose, and you can see that the model is much sharper :

![colma_no_problem](https://user-images.githubusercontent.com/4380424/100285435-2f853100-2f71-11eb-84a7-344e9cd36e32.png)


What are your thoughts on this problem ? Thanks is advance for your help

I do concede that the subject is very broad, but criticizing EuRoC odometry is a very strong claim so I prefer a second opinion :)"
""
"Hello. Thank you very much for this project and your awesome work on it!

I've been reading the implementation of the M-DQN and M-IQN and found that the log-policy clipping is applied differently.
In the original paper authors clip this member between [lo, 0], where lo is hyperparameter.
In M-IQN it [looks correct](https://github.com/google-research/google-research/blob/master/munchausen_rl/agents/m_iqn.py#L310), but in the M-DQN the value is [clipped in [lo, 1]](https://github.com/google-research/google-research/blob/master/munchausen_rl/agents/m_dqn.py#L156).
Probably I'm missing something, so if this is not a bug, could you please give some details about why you use 1 as a max clipping value?

Thanks in advance!"
"For NNGP-guided Neural Architecture Search at https://github.com/google-research/google-research/tree/master/nngp_nas, I get an error when I try to run your Google Colab notebook for it at https://colab.research.google.com/github/google-research/google-research/blob/master/nngp_nas/NNGP_on_NASBench101.ipynb 👍 
---->  import tensorflow_datasets as tfds
ImportError: This version of TensorFlow Datasets requires TensorFlow version >= 2.1.0; Detected an installation of version 1.15.2. Please upgrade TensorFlow to proceed.
"
"In the paper of performer, we know ORFs require m <= d, but the description of fig.4 is ""L=4096 and d=16"" and x axis (m) varies from 0 to 200. Is there any typo?"
"I could not tell from the paper or repo, if and how ScaNN can handle larger-than-core datasets.  Any comment?"
"Dears,
In ENAS_LM, may I ask which part exactly in the code the controller trains all the children (explicitly), and how to know how many children do you train in one epoch?"
"Missing [FEELVOS](https://arxiv.org/pdf/1902.09513.pdf) repo, Kindly add the repo for this paper and open source it's code."
"I want to apply TFT model to my own dataset, but there are missing values at some time steps. My question is whether the model can handle missing values? Do I need to preprocess my data to keep the time series continuous?"
"After reading through your paper and applying TFT to model COVID-19 trends across US states, I am still unclear about how effectively TFT deals with nonstationarity. My trained model appears to be doing as well as can be expected, given how aberrant the pandemic trends have been in the US; however, I'm wondering if TFT necessitates stationarity transformations in this case or if the sequential attention is able to learn to deal with this on its own fairly effectively (given that obviously the more feature engineering that is done can help improve model performance but then again part of the appeal of TFT is that one can avoid much of that work through the model).

Thanks for your thoughts. Much appreciated!"
"Hi team,

Thanks a lot for the providing it publicly.

It would be great if you can provide code for evaluating the trained model and it's different applications as mentioned in the paper.

Thanks in advance for your continuous support .

Harish Naidu Chinnam "
"Hello all Dears,

How can we access free tpu via TFRC for pre-training BERT language Model on a specific language?

the link below explains that we have to sign up here: https://services.google.com/fb/forms/tpusignup/ , but it seems that it's a dead URL.
https://ai.googleblog.com/2017/05/introducing-tensorflow-research-cloud.html

Nothing happened By Apply now at https://www.tensorflow.org/tfrc"
"https://github.com/google-research/google-research/blob/fa685638bd1d873a2f58d485655ae4edadca1426/performer/fast_self_attention/fast_self_attention.py#L84

What is the reasoning behind adding zeros here? It doesn't make much sense to me, but then again I haven't really looked much into jax. Also the data_mod_shape doesn't match the projection_matrix shape, so how would that work?

Thx in advance :) "
"As somebody with more of a physics background than ML, the implementation [here](https://github.com/google-research/google-research/tree/master/simulation_research/ising_model) for Ising MCMC simulation on TPU is too cumbersome/lengthy with Tensorflow 1.x. Will be a lot more helpful for the physics community if somebody can rewrite with 2.x..."
"Thanks for your interesting works!

It is too difficult to download the whole dataset from every single image url. Can you share me the image file?

thanks a lot!  "
"Hi, does SCANN support sparse matrix? If yes, where can I find instruction for it? Thank you."
""
"Hi, I'm just wondering if there are plans to release the COLA model pre-trained on AudioSet. Seems like it could be useful for a lot of downstream tasks and fine-tuning. Thanks!"
"if  I use clang-8,it works,but I have to use gcc10 to build scann and there is a error. Anyone knows how to solve. it likes the different rule between clang and gcc.

command：
CC=gcc bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg

--->
ERROR: /home/xuyao/src/google-research/scann/scann/distance_measures/one_to_one/BUILD.bazel:146:11: C++ compilation of rule '//scann/distance_measures/one_to_one:dot_product_sse4' failed (Exit 1): gcc failed: error executing command /home/xuyao/gcc9.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 58 argument(s) skipped)
scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
scann/distance_measures/one_to_one/dot_product_sse4.cc:30:62: error: expected '{' before '->' token
   30 |   auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
      |                                                              ^~
scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductByteImpl(const Byte*, const Byte*, size_t)':
scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected primary-expression before '{' token
   30 |   auto as_m128i = [](const Byte* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
      |                                                                          ^
scann/distance_measures/one_to_one/dot_product_sse4.cc:30:74: error: expected ',' or ';' before '{' token
scann/distance_measures/one_to_one/dot_product_sse4.cc: In lambda function:
scann/distance_measures/one_to_one/dot_product_sse4.cc:169:64: error: expected '{' before '->' token
  169 |   auto as_m128i = [](const int8_t* x) SCANN_SSE4_INLINE_LAMBDA -> __m128i* {
      |                                                                ^~
scann/distance_measures/one_to_one/dot_product_sse4.cc: In function 'double tensorflow::scann_ops::dp_internal::DenseDotProductSse4(const tensorflow::scann_ops::DatapointPtr<signed char>&, const tensorflow::scann_ops::DatapointPtr<float>&)':

"
"Can we share the pre-train encoder model so that we don't need to re-train it from scratch? 

Thanks."
"I am having difficulty in understanding that why does the `tft` code uses previous observations which are also targets? For example using electricity example, and if `total_time_steps` are 192 and `num_encoder_steps` are 168, I understand that the model uses 168 previous values to predict next 24 values (or their quantiles). But the input data i.e. `data` in line [1145](https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py#L1145) has the shape `(450000, 192, 5)` while the `labels` have shape `(450000, 24, 5)`. However the last 24 values in `data` are exactly what the 24 values of `labels` are. For validation I ran following lines of code during debugging after line [1145](https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py#L1145)

```python
for i,j in zip(data[0, -24:, 0], labels[0]):
    print(np.abs(np.subtract(i,j[0])))
```
and it only prints 0s. 

My question is if we are required to feed the target values as inputs, then what is the point of having a model? What am I missing? "
"Hi Very nice work on the BERT Seq2Seq model. One question: In order for it to fit in the Seq2Seq model, you need to create attention mechansim between encoder-decoder. Bert/GPT2 is pretty much using self-attention. I cannot find any source code on this since you guys release the trained models. Can you share that piece of code/design ideas? 

For Summary task: 
I saw on the shared models that it directly outputs the summary. Can I know if there is a way to finetune the model base on our data? https://github.com/google-research/google-research/tree/master/bertseq2seq"
"Hi @debidatta 
Can RepNet be used for counting multiple actions in a video? Also would it be possible to process a video as long as 2-3 hours with sporadically repeated actions? How long would the processing take?"
"Hi. Thanks for the repo. Following the steps in the repo, I am trying to run command `python -m bam.run_classifier debug-model $BAM_DIR '{""debug"": true}'` as a minimal test. However, below line of code, raise `Extra data: line 1 column 5 (char 4)` error. Does anyone have any idea what should be the root cause? I tried to run other commands, still, this line raises an issue with the JSON. 

Line raining the error:
`\bam\run_classifier.py"", line 258, in main
    config = configure.Config(topdir, model_name, **json.loads(hparams))`

Error: 
`raise JSONDecodeError(""Extra data"", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 5 (char 4)`"
"once python import scann, python crash & core dump with error 'Illigal instruction'. is there any problem with compablity?

system info
```
[root@k8s-new-node-2-37nir tmp]# cat /etc/os-release 
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

[root@k8s-new-node-2-37nir tmp]# uname -a
Linux k8s-new-node-2-37nir.vclound.com 4.20.7-1.el7.elrepo.x86_64 #1 SMP Wed Feb 6 13:17:46 EST 2019 x86_64 x86_64 x86_64 GNU/Linux
[root@k8s-new-node-2-37nir tmp]# python3 --version
Python 3.6.0
[root@k8s-new-node-2-37nir tmp]# pip3 --version
pip 20.2.4 from /usr/local/python36/lib/python3.6/site-packages/pip (python 3.6)
```

gdb info
```
Core was generated by `python3'.
Program terminated with signal 4, Illegal instruction.
#0  0x00007fbaaf0b1c54 in InitDefaultsscc_info_FixedPoint_scann_2fproto_2fexact_5freordering_2eproto() () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
Missing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.6-13.el7.x86_64 glibc-2.17-157.el7_3.1.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.14.1-27.el7_3.x86_64 libcom_err-1.42.9-9.el7.x86_64 libgcc-4.8.5-11.el7.x86_64 libselinux-2.5-6.el7.x86_64 libstdc++-4.8.5-11.el7.x86_64 libuuid-2.23.2-33.el7.x86_64 ncurses-libs-5.9-14.20130511.el7_4.x86_64 openssl-libs-1.0.2k-19.el7.x86_64 pcre-8.32-15.el7_2.1.x86_64 readline-6.2-11.el7.x86_64 zlib-1.2.7-17.el7.x86_64
(gdb) bt
#0  0x00007fbaaf0b1c54 in InitDefaultsscc_info_FixedPoint_scann_2fproto_2fexact_5freordering_2eproto() () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#1  0x00007fbaaf19cf23 in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#2  0x00007fbaaf19cf0f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#3  0x00007fbaaf19cf0f in google::protobuf::internal::(anonymous namespace)::InitSCC_DFS(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#4  0x00007fbaaf19cdc0 in google::protobuf::internal::InitSCCImpl(google::protobuf::internal::SCCInfoBase*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#5  0x00007fbaaf0e2545 in google::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable const*) () from /usr/local/python36/lib/python3.6/site-packages/scann/scann_ops/cc/_scann_ops.so
#6  0x00007fbaffd5b1e3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2
#7  0x00007fbaffd5f8f6 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2
#8  0x00007fbaffd5aff4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#9  0x00007fbaffd5efeb in _dl_open () from /lib64/ld-linux-x86-64.so.2
#10 0x00007fbaff92cfbb in dlopen_doit () from /lib64/libdl.so.2
#11 0x00007fbaffd5aff4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#12 0x00007fbaff92d5bd in _dlerror_run () from /lib64/libdl.so.2
#13 0x00007fbaff92d051 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2
#14 0x00007fbabd53d88a in tensorflow::internal::LoadLibrary(char const*, void**) () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#15 0x00007fbabd523af7 in tensorflow::(anonymous namespace)::PosixEnv::LoadLibrary(char const*, void**) () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#16 0x00007fbabd1e30b4 in tensorflow::LoadLibrary(char const*, void**, void const**, unsigned long*) () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.2
#17 0x00007fbac1b811b7 in TF_LoadLibrary () from /usr/local/python36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```"
Thanks so much for making the code for Performer publicly available! Do you think you could also open source the weights of the pre-trained models shown in the paper?
"The input data was ~10M vectors of 2048 dim each, the searcher was successfully built after 14 minutes, and there were hundreds of GB to spare (several multiples of the space used by the searcher itself), but it was not possible to to save it

```
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
<ipython-input-33-3451ef45ab69> in <module>
      1 os.makedirs('.data/scann_artifacts/test', exist_ok=True)
----> 2 searcher.serialize('./data/scann_artifacts/test')

~/.conda/envs/vsms/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops_pybind.py in serialize(self, artifacts_dir)
     70 
     71   def serialize(self, artifacts_dir):
---> 72     self.searcher.serialize(artifacts_dir)
     73 
     74 

MemoryError: std::bad_alloc
```

The problem goes away if I build the searcher with the first 2M vectors."
"Keypose sample code and pretrained models work very well.
I want to train a Keypose network with a custom dataset.
So I want to know when the training code will be released.
@kkonolige "
"In pre-trained weight file, there are 'r1', 'r2', ..., 'r4' folders. What does mean by those numbers?"
"The task is formalized as 
<img width=""308"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643397-83cb7300-0ae1-11eb-88d2-a235716b843f.png"">
<img width=""415"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643446-ce4cef80-0ae1-11eb-8396-82b3b8f1a0ef.png"">
the past targets y is part of the inputs.

But in the figure, it seems that past targets are not in this model.
<img width=""612"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643428-ae1d3080-0ae1-11eb-8d86-8aeebe7143ff.png"">
where
<img width=""128"" alt=""image"" src=""https://user-images.githubusercontent.com/56542320/95643442-c0976a00-0ae1-11eb-83a9-222f79374256.png"">
So where are the past targets?"
"Extracting 10 samples...
Cached data ""valid"" updated
*** Fitting TemporalFusionTransformer ***
Getting batched_data
Using cached training data
Using cached validation data
Using keras standard fit
Train on 100 samples, validate on 10 samples
2020-10-09 07:11:36.762729: W tensorflow/c/c_api.cc:326] Operation '{name:'TemporalFusionTransformer/lstm/while' id:3156 op device:{} def:{{{node TemporalFusionTransformer/lstm/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=45, _read_only_resource_inputs=[8, 9, 10], body=TemporalFusionTransformer_lstm_while_body_3416[], cond=TemporalFusionTransformer_lstm_while_cond_3415[], output_shapes=[[], [], [], [], [?,5], ..., [], [], [], [], []], parallel_iterations=32](TemporalFusionTransformer/lstm/while/loop_counter, TemporalFusionTransformer/lstm/while/maximum_iterations, TemporalFusionTransformer/lstm/time, TemporalFusionTransformer/lstm/TensorArrayV2_1, TemporalFusionTransformer/layer_normalization_4/add, TemporalFusionTransformer/layer_normalization_5/add, TemporalFusionTransformer/lstm/strided_slice, TemporalFusionTransformer/lstm/TensorArrayUnstack/TensorListFromTensor, TemporalFusionTransformer/lstm/lstm_cell/kernel, TemporalFusionTransformer/lstm/lstm_cell/recurrent_kernel, TemporalFusionTransformer/lstm/lstm_cell/bias, TemporalFusionTransformer/lstm/while/EmptyTensorList, TemporalFusionTransformer/lstm/while/EmptyTensorList_1, TemporalFusionTransformer/lstm/while/EmptyTensorList_2, TemporalFusionTransformer/lstm/while/EmptyTensorList_3, TemporalFusionTransformer/lstm/while/EmptyTensorList_4, TemporalFusionTransformer/lstm/while/EmptyTensorList_5, TemporalFusionTransformer/lstm/while/EmptyTensorList_6, TemporalFusionTransformer/lstm/while/EmptyTensorList_7, TemporalFusionTransformer/lstm/while/EmptyTensorList_8, TemporalFusionTransformer/lstm/while/EmptyTensorList_9, TemporalFusionTransformer/lstm/while/EmptyTensorList_10, TemporalFusionTransformer/lstm/while/EmptyTensorList_11, TemporalFusionTransformer/lstm/while/EmptyTensorList_12, TemporalFusionTransformer/lstm/while/EmptyTensorList_13, TemporalFusionTransformer/lstm/while/EmptyTensorList_14, TemporalFusionTransformer/lstm/while/EmptyTensorList_15, TemporalFusionTransformer/lstm/while/EmptyTensorList_16, TemporalFusionTransformer/lstm/while/EmptyTensorList_17, TemporalFusionTransformer/lstm/while/EmptyTensorList_18, TemporalFusionTransformer/lstm/while/EmptyTensorList_19, TemporalFusionTransformer/lstm/while/EmptyTensorList_20, TemporalFusionTransformer/lstm/while/EmptyTensorList_21, TemporalFusionTransformer/lstm/while/EmptyTensorList_22, TemporalFusionTransformer/lstm/while/EmptyTensorList_23, TemporalFusionTransformer/lstm/while/EmptyTensorList_24, TemporalFusionTransformer/lstm/while/EmptyTensorList_25, TemporalFusionTransformer/lstm/while/EmptyTensorList_26, TemporalFusionTransformer/lstm/while/EmptyTensorList_27, TemporalFusionTransformer/lstm/while/EmptyTensorList_28, TemporalFusionTransformer/lstm/while/EmptyTensorList_29, TemporalFusionTransformer/lstm/while/EmptyTensorList_30, TemporalFusionTransformer/lstm/while/EmptyTensorList_31, TemporalFusionTransformer/lstm/while/EmptyTensorList_32, TemporalFusionTransformer/lstm/while/EmptyTensorList_33)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
100/100 [==============================] - ETA: 0s - loss: 1.9711WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
100/100 [==============================] - 3s 33ms/sample - loss: 1.9711 - val_loss: 1.5490
Cannot load from data/saved_models/electricity/fixed/tmp, skipping ...
Using cached validation data
Optimal model found, updating
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 238, in <module>
    use_testing_mode=True)  # Change to false to use original default params
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 136, in main
    opt_manager.update_score(params, val_loss, model)
  File ""/content/google-research/tft/libs/hyperparam_opt.py"", line 227, in update_score
    model.save(self.hyperparam_folder)
  File ""/content/google-research/tft/libs/tft_model.py"", line 1307, in save
    tf.keras.backend.get_session(),
AttributeError: module 'tensorflow.keras.backend' has no attribute 'get_session'"
"Selecting GPU ID=0
*** Training from defined parameters for electricity ***
Loading & splitting data...
/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  mask |= (ar1 == a)
Formatting train-valid-test splits.
Setting scalers with training data...
*** Loading hyperparm manager ***
*** Running calibration ***
Params Selected:
dropout_rate: 0.1
hidden_layer_size: 5
learning_rate: 0.001
minibatch_size: 64
max_gradient_norm: 0.01
num_heads: 4
stack_size: 1
model_folder: data/saved_models/electricity/fixed
WARNING:tensorflow:From /content/google-research/tft/script_train_fixed_params.py:121: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.

Resetting temp folder...
*** TemporalFusionTransformer params ***
# dropout_rate = 0.1
# hidden_layer_size = 5
# learning_rate = 0.001
# max_gradient_norm = 0.01
# minibatch_size = 64
# model_folder = data/saved_models/electricity/fixed
# num_heads = 4
# stack_size = 1
# total_time_steps = 192
# num_encoder_steps = 168
# num_epochs = 1
# early_stopping_patience = 5
# multiprocessing_workers = 5
# column_definition = [('id', <DataTypes.REAL_VALUED: 0>, <InputTypes.ID: 4>), ('hours_from_start', <DataTypes.REAL_VALUED: 0>, <InputTypes.TIME: 5>), ('power_usage', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>), ('hour', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('day_of_week', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('hours_from_start', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('categorical_id', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>)]
# input_size = 5
# output_size = 1
# category_counts = [369]
# input_obs_loc = [0]
# static_input_loc = [4]
# known_regular_inputs = [1, 2, 3]
# known_categorical_inputs = [0]
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 238, in <module>
    use_testing_mode=True)  # Change to false to use original default params
  File ""/content/google-research/tft/script_train_fixed_params.py"", line 124, in main
    model = ModelClass(params, use_cudnn=use_gpu)
  File ""/content/google-research/tft/libs/tft_model.py"", line 456, in __init__
    self.model = self.build_model()
  File ""/content/google-research/tft/libs/tft_model.py"", line 1006, in build_model
    = self._build_base_graph()
  File ""/content/google-research/tft/libs/tft_model.py"", line 929, in _build_base_graph
    = get_lstm(return_state=True)(historical_features,
  File ""/content/google-research/tft/libs/tft_model.py"", line 907, in get_lstm
    lstm = tf.keras.layers.CuDNNLSTM(
AttributeError: module 'tensorflow.keras.layers' has no attribute 'CuDNNLSTM"
"I guess we are taking 90 days history to forecast for next 30 days. So shouldn't the num_encoder_steps be 90 instead ? 
It seems for other datasets num_encoder_steps are defined correctly, but wrong for retail dataset (favorita)

tft/data_formatters/favorita.py

# Default params
  def get_fixed_params(self):
    """"""Returns fixed model parameters for experiments.""""""

    fixed_params = {
        'total_time_steps': 120,
        'num_encoder_steps': 30,
        'num_epochs': 100,
        'early_stopping_patience': 5,
        'multiprocessing_workers': 5
    }

    return fixed_params"
"I ran 'end_to_end_test.py' file, and get the results of dropout, svi, vanilla. 
I also want to get the ensemble's result, but it seems that ensemble method is not included in the 'end_to_end_test.py' file.
How can I test ensemble model?"
"Do you have example of using TabNet for self-supervised pretraining? Especially the ones you mentioned in the paper:
`We study self-supervised learning on Higgs and Forest Cover Type datasets. `

Thanks!"
"When trying to serialize a searcher for a Dataset with 2,000,000 items and each item with a vetor length of 4,096. It will throw an error.

```
Traceback (most recent call last):
  File ""scann_run.py"", line 141, in <module>
    train()
  File ""scann_run.py"", line 116, in train
    searcher.serialize(save_target)
  File ""~/.local/lib/python3.6/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 55, in serialize
    self.searcher.serialize(artifacts_dir)
ValueError: cannot create std::vector larger than max_size()
```"
"Traceback (most recent call last):
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/tungdinh/CSProjects/UW/Capstone/Research/google-research/goemotions/bert_classifier.py"", line 1037, in <module>
    tf.compat.v1.app.run()
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/app.py"", line 293, in run
    flags_parser,
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/app.py"", line 362, in _run_init
    flags_parser=flags_parser,
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/app.py"", line 212, in _register_and_parse_flags_with_usage
    args_to_main = flags_parser(original_argv)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 31, in _parse_flags_tolerate_undef
    return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow_core/python/platform/flags.py"", line 112, in __call__
    return self.__dict__['__wrapped'].__call__(*args, **kwargs)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 636, in __call__
    self._assert_all_validators()
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 510, in _assert_all_validators
    self._assert_validators(all_validators)
  File ""/Users/tungdinh/.pyenv/versions/3.6.8/lib/python3.6/site-packages/absl/flags/_flagvalues.py"", line 531, in _assert_validators
    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))
absl.flags._exceptions.IllegalFlagValueError: flag --vocab_file=None: Flag --vocab_file must have a value other than None.

I'm running this program via this repo: https://github.com/tungxd96/google-research.git
First resolved error so far is to replace tensorflow v2 to tensorflow v1. Next, I'm receiving this error while running ""python -m goemotions.bert_classifier"". I found this vocab_file is assigned to None instead of a directory pointing to the vocabulary file as shown in line 75 of bert_classifier.py. I can't find any vocabulary file in google-research/goemotions/data. Am I missing any important files so far besides downloading 3 csv files in full_datasets? Has someone faced this issue before? Please show me how to solve it or if anyone has trained this model successfully, could you provide reproducing steps? Thank you"
"Hello, 

Could you please let me know where I can download the GoEmotion Dataset? 

I loaded the GoEmotion dataset from TensorFlow dataset but I am not able to decode the column ""comment_text"". 

Therefore, could you please let me know where can I download the dataset maybe in csv or tsv file format? 

Thank you 

"
"Hi,

I was going through the repo but couldn't find any resource on how to increase the database size once the tree is built. If I have trained the tree on 1 million data points, is it possible to add 1 million more data points??."
"the GED paper specifies the use of an 8x overcomplete basis when computing spectrograms in appendix C.1. But it appears that a basis of fixed dimension 256 is used in every case, and that it is not possible for the bin count to vary with window length as implemented.

`ged` calls `calc_spectrograms` without changing the default value of 256:
https://github.com/google-research/google-research/blob/1c49e6ffd1dd009c21c4ff7596341c9ee4c90603/ged_tts/distance_function/spectral_ops.py#L95-L98
this same value is passed for every window length to `get_spectral_matrix` where it determines the number of filters:
https://github.com/google-research/google-research/blob/1c49e6ffd1dd009c21c4ff7596341c9ee4c90603/ged_tts/distance_function/spectral_ops.py#L71-L78"
"Hi,

I have a large dataset with image (say ~2 million) embeddings. 

Can I add another say 100 embeddings to the scanner or I need always a new Initialization?   "
"The appendix of the paper mentions reaching 95.5% accuracy for 20news with the detailed hyperparameter settings. 
Whatever I do, I can never exceed 40% accuracy on the in-distribution data [even data]. 
Did they mean AUC? 
On all 20 classes I manage to reach 80% accuracy both with a textCNN and LSTM. 

I add in my Sacred config for comparison: 
```@ex.named_config
def ovadia_20news():
    """"""
    %maxdoclen 250; max_vocabulary 30000
    %The vanilla model uses a one-layer LSTM model of size 32 and a dense layer to predict the 10 class
    % probabilities based on word embedding of size 128. A dropout rate of 0.1 is applied to both the LSTM
    % layer and the dense layer for the Dropout model. The LL-SVI model replaces the last dense layer
    % with a Bayesian layer, the ensemble model aggregates 10 vanilla models, and stochastic methods
    % sample 5 predictions per example. The vanilla model accuracy for in-distribution test data is 0.955.
    """"""
    identifier = ""20news""
    data_folder = os.path.join(DATAROOT, identifier)
    out_folder = generate_out_folder(data_folder)
    task = ""document_classification""  # or regression
    max_vocabulary = 30000  # 20news to 30K e.g.
    composition = [""word""]

    model = ""lstm""
    embed_dim = 128
    projection_nodes = 32
    dropout = 0.1
    dropout_nonlinear = 0.1
    embedding_dropout = 0
    dropout_concrete = None
    weight_decay = 0  # .0001  # triggers AdamW optimizer
    max_document_len = 250

    epochs = 48
    optimizer = ""adam""
    clipnorm = 10
    learning_rate = 0.001
    steps_per_epoch = None  # could also be None

    posterior_sampling = 10
    use_aleatorics = False
    multilabel = False
    loss_fn = ""categorical_crossentropy"" if not use_aleatorics else ""attenuated_learned_loss""
    metrics = [""accuracy"", ""mse""] if not use_aleatorics else []
    ood = ['comp.graphics', 'comp.sys.ibm.pc.hardware', 'comp.windows.x', 'rec.autos', 'rec.sport.baseball', 'sci.crypt', 'sci.med', 'soc.religion.christian', 'talk.politics.mideast', 'talk.religion.misc']
```"
I am having trouble accessing google cloud for t5_closed_book_qa due to corporate restrictions. Any other location where I can download pre trained wieghts from ?
Can you please publicly release the raw Pouring dataset with new annotations (instead of the tfrecords one)? @debidatta
.
"Found this piece of code in [tft_model.py](https://github.com/google-research/google-research/blob/master/tft/libs/tft_model.py#L1130)

```python
print('Getting batched_data')
    if train_df is None:
      print('Using cached training data')
      train_data = TFTDataCache.get('train')
    else:
      train_data = self._batch_data(valid_df)
```

Shouldn't that be `train_df`? "
"The checkpoint is only provided for grounding model, so if I want to train my own grounding model while using the Tuple Phrase pre-trained checkpoint model, what should I do?"
"Hey, I am trying to retrain mobilebert. I was able to run the pretraing scrips with no problems. However, when fine-tuning on squad using the flag `--use_quantized_training=true`. I get the following error 

```
ERROR:tensorflow:Error recorded from training_loop: From /job:worker/replica:0/task:0:
Gradient for bert/encoder/layer_9/attention/output/dense/bias:0 is NaN : Tensor had NaN values
```
Here is the script I use for fine-tuning 

```
python3 run_squad.py \
	--bert_config_file=config/uncased_L-24_H-128_B-512_A-4_F-4_OPT_QAT.json \
	--data_dir=${DATA_DIR} \
	--do_train \
	--doc_stride=128 \
	--init_checkpoint={INIT_CKPT}/model.ckpt-1000.index \
	--learning_rate=4e-05 \
	--predict_file=dev.json \
	--do_lower_case \
  	--do_predict \
	--max_answer_length=30 \
	--max_query_length=64 \
	--max_seq_length=128\
	--n_best_size=20 \
	--num_train_epochs=1 \
	--output_dir=${OUTPUT_DIR} \
	--train_batch_size=32 \
	--train_file=train.json \
	--use_tpu \
	--tpu_name=${TPU_NAME} \
	--vocab_file=../bert/en-vocab.txt \
	--warmup_proportion=0.1 \
	--verbose_logging=True \
	--use_quantized_training=true
```"
I am trying to train mobilebert from scratch on another language. The `run_pretraining.py` file contains a flag called `init_checkpoint`. Is that flag for a pretrained BERT model ? i.e do I have to train BERT first then do the distillation ?
"@roadjiang How would you deal with imbalanced dataset in Mentormix setting? Without special handling, negative samples (say a large portion of the dataset is negative) tend to have lower loss, with MentorMix where we favor sample with lower loss, we would focus much more on only negative samples.  "
"@rybakov

Hi Oleg, I am a little confused about how you get datasets split. For instance In V2, we have 105829 samples in total, but only 36923/4445/4890 are used for train/val/test sets. 

The script [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/input_data.py) doesn't seem to handle the number of samples.

Best regards."
`ProtobufRequestMixin` has been deprecated since v0.15 and is no longer available in 1.0 and above. Would you mind updating the EEG viewer code to not use it?
"Hello,

Inspired from the scann/docs/example.ipynb, I wrote the following code to search the freebase entities:

`searcher = scann.ScannBuilder(normalized_dataset, 2000, ""dot_product"").tree(
num_leaves = 1000, num_leaves_to_search = 50, training_sample_size = 3000).score_ah(
2, anisotropic_quantization_threshold = 0.2).create_pybind()`

`# normalized_dataset.shape = 15000, 400 # freebase entity embeddings with dimension 400`

With `k = 2000`, the application crashes with **corrupted unsorted chunks error**.
For `k = 20` or `k = 50`, the application runs just fine. Any clue would be appreciated.

Complete backtrace is here
`
2020-08-26 20:01:05.869102: I scann/partitioning/partitioner_factory_base.cc:58] Size of sampled dataset for training partition: 2920
2020-08-26 20:01:06.241532: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 372.329265ms.
*** Error in '/home/uji300/.pyenv/versions/py3.7/bin/python': free(): corrupted unsorted chunks: 0x0000000074268ba0 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7c619)[0x2aaaab670619]
/home/uji300/.pyenv/versions/py3.7/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so(_ZN10tensorflow9scann_ops10ScannNumpy13SearchBatchedERKN8pybind117array_tIfLi17EEEiiib+0x780)[0x2aabbc8b7d60]
/home/uji300/.pyenv/versions/py3.7/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so(+0x5611d9)[0x2aabbc8b61d9]
/home/uji300/.pyenv/versions/py3.7/lib/python3.7/site-packages/scann/scann_ops/cc/python/scann_pybind.so(+0x556e5e)[0x2aabbc8abe5e]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyMethodDef_RawFastCallKeywords+0x364)[0x43b0b4]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyObject_FastCallKeywords+0x2e7)[0x43b3c7]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalFrameDefault+0x607e)[0x427d5e]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalCodeWithName+0xa36)[0x4ef436]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyFunction_FastCallKeywords+0xa5)[0x43ac15]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalFrameDefault+0x7137)[0x428e17]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalCodeWithName+0xa36)[0x4ef436]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyFunction_FastCallKeywords+0xa5)[0x43ac15]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalFrameDefault+0x7137)[0x428e17]
/home/uji300/.pyenv/versions/py3.7/bin/python(_PyEval_EvalCodeWithName+0xa36)[0x4ef436]
/home/uji300/.pyenv/versions/py3.7/bin/python(PyEval_EvalCode+0x23)[0x4ef533]
/home/uji300/.pyenv/versions/py3.7/bin/python(PyRun_FileExFlags+0x15e)[0x526efe]
/home/uji300/.pyenv/versions/py3.7/bin/python(PyRun_SimpleFileExFlags+0xdc)[0x5270fc]
/home/uji300/.pyenv/versions/py3.7/bin/python[0x42f71d]
/home/uji300/.pyenv/versions/py3.7/bin/python(_Py_UnixMain+0x2d)[0x42f98d]
/lib64/libc.so.6(__libc_start_main+0xf5)[0x2aaaab615c05]
/home/uji300/.pyenv/versions/py3.7/bin/python[0x42a9f4]`"
""
"```bash
ERROR: C:/home/projects/software/study/google-research/automl_zero/BUILD:533:11: C++ compilation of rule '//:randomizer' failed (Exit 2)
C:\users\seonglae\_bazel_seonglae\e4sphe5m\execroot\__main__\definitions.h(25): fatal error C1083: Cannot open include file: 'sched.h': No such file or directory
```

Output when I run `./run_demo.sh` in ` google-research/automl_zero` demo

I installed bazel with choco in windows
```bash
choco install bazel
```

Is this meaningful error? and how can I solve this problem?

"
"While running goemotions  bert_classifier.py:
```
Traceback (most recent call last):
  File ""bert_classifier.py"", line 50, in <module>
    from dataset_analysis.bert import modeling
ModuleNotFoundError: No module named 'dataset_analysis'
```

Where dataset_analysis module can be found ?
Thank you."
"Do you have the right version of tensorflow, TF_ Hub and sentencepiece to run the model 


"
"Thanks a lot for making the weights of the BertSeq2Seq models of your paper: [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) available to the public.

I'm trying to get access to the sentence piece model that was used for **bertseq2seq/roberta24_cnndm** : https://tfhub.dev/google/bertseq2seq/roberta24_cnndm/1

As far as I understand it can be found under `assets` in the unzipped folder, but I cannot find anything there. 
Is there a possibility that you can add the sentence piece models to all bertseq2seq models? 

Gently pinging @shashiongithub :-)  

Would be very thankful for any help!
"
"Thanks for your work and sharing.
I tested your pre-trained models on my own image sequence.
It seems the trajectory_inference.py only provided the relative position.
The paper said your work can learn the camera intrinsic but I can't find any part about it in this project."
"Hello,

How to download using shell\command line googleapis links such as https://www.googleapis.com/download/storage/v1/b/gresearch/o/depth_from_video_in_the_wild%2Fcheckpoints%2Fcityscapes_kitti_learned_intrinsics.zip?generation=1566493762028542&alt=media
?"
The example in `google-research/scann/docs/example.ipynb` is scann with numpy example. Is there a tensorflow example?
"Is there a way to save `scann.scann_ops.py.scann_ops_pybind.ScannSearcher`? Pickle solution failed (I tried joblib). 
Thank you."
"Hi, 
the README says to run:

cold_posterior_bnn/run_cnnlstm_experiment_small.sh

for the small version of the cnnlstm experiment, but the script seems not to exist in the repo.

"
"https://github.com/google-research/google-research/blob/39195a28fb30ecffc06f1d9d60200606dc10d1ed/grouptesting/configs/toy.gin#L105

Is it the `max_iterations` to be set instead?
```
LbpSampler.max_iterations = 100
```"
"Hi,

Thank you for your interesting work!  I have read the paper, and I want to reproduce the performance using your code. However, I have some questions about the codes.

1.   In the speech_commands.ipynb, I find that, the results are different  when I evaluate the model with different batch size. It might be casued by the batch norm, the batch norm is still in trianing mode when we evaluate the model. The same condition can be observed in the inference codes. Therefore, is this a bug？

Thanks,
Hang"
"Hi, thanks for your interesting work. I am trying to train 'Depth from Videos in the Wild' on YouTube videos. The problem that I have is that even when I set 'learn_intrinsics', the current training codes needs the camera intrinsic parameter as a text file. How can I solve it? Thanks"
"Hi,

Thank you for your interesting work! I have just started to learn BERT recently. I have some general questions regarding this topic.

1. Why BERT has 12 blocks? Not 11 or 13 etc. ? I couldn't find any explanation.

2. I want to compare the performance of BERT with different model size (transformer block number). Is it necessary to do distillation?  If I just train a BERT with 6 Layers without distillation, does the performance look bad?

3. Do you have to do pretraining every time you change the layer number of  BERT? Is it possible to just remove some layers in an existing pre-trained model and finetune on tasks? 


Thanks,
ZLK
"
"## Description

I executed the suggested Bazel build command and the error occurred as below:

```
In file included from scann/partitioning/kmeans_tree_partitioner.cc:15:
In file included from ./scann/partitioning/kmeans_tree_partitioner.h:24:
In file included from ./scann/partitioning/kmeans_tree_like_partitioner.h:22:
./scann/partitioning/partitioner_base.h:21:10: fatal error: 'hash_set' file not found
#include <hash_set>
         ^~~~~~~~~~
1 error generated.
Target //:build_pip_pkg failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

At the [reference](https://docs.microsoft.com/en-us/cpp/standard-library/hash-set-class?view=vs-2019) of this header, they said this API is obsolete and suggesting to use `<unordered_set>`.

I checked two headers are all included on `memory_logging.h`:

```cpp
#include <hash_set>
#include <type_traits>
#include <unordered_map>
#include <unordered_set>
```

## Possible Fix

We can remove `<hash_set>` and only use `<unordered_set>` if there are no concerns about breaking any dependencies. 

If not, it will be highly appreciated if you can provide the solution to use both headers when compile. (e.g. Bazel options, Configuration file change)

## Environment

My bazel version is `bazel 3.4.1-homebrew`, executed on macOS Catalina.

"
"## Description
I executed the suggested Bazel build command and the error occurred as below:

```
scann/oss_wrappers/scann_aligned_malloc.cc:24:10: error: use of undeclared identifier 'aligned_alloc'; did you mean 'aligned_malloc'?
  return aligned_alloc(minimum_alignment, size);
         ^~~~~~~~~~~~~
         aligned_malloc
scann/oss_wrappers/scann_aligned_malloc.cc:22:7: note: 'aligned_malloc' declared here
void *aligned_malloc(size_t size, size_t minimum_alignment) {
      ^
1 error generated.
```

At the [reference](https://en.cppreference.com/w/cpp/memory/c/aligned_alloc) of this function, aligned_alloc, I could find the definition inside <cstdlib> on C++17.
I also checked the header file, `scann_aligned_malloc.h`, and found the include statement for `<cstdlib>` is already there.

I checked my build option contains the option for setting standard version to 17, `--cxxopt=""-std=c++17""`, but error still remains.

## Environment
My bazel version is bazel 3.4.1-homebrew, executed on macOS Catalina."
"## Description

I executed `python configure.py` inside `virtualenv` constructed with Python 3.7 and saw generated `.bazelrc` and `.bazel_query.sh`.

I executed the suggested Bazel build command, `CC=clang-8 bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg`, and the error occurred as below:

```
...
ERROR: /private/var/tmp/_bazel_harryhong/6c67f240e5dd3cb3ad6beb460050957b/external/local_config_tf/BUILD:11:11: in srcs attribute of cc_library rule @local_config_tf//:libtensorflow_framework: '@local_config_tf//:ensorflow_framework.2' does not produce any cc_library srcs files (expected .cc, .cpp, .cxx, .c++, .C, .cu, .cl, .c, .h, .hh, .hpp, .ipp, .hxx, .h++, .inc, .inl, .tlh, .tli, .H, .tcc, .S, .s, .asm, .a, .lib, .pic.a, .lo, .lo.lib, .pic.lo, .so, .dylib, .dll, .o, .obj or .pic.o)
ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.406s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (5 packages loaded, 1355 targets configured)
```

I thought `@local_config_tf//:ensorflow_framework.2` is suspicious, so I modified `.bazelrc` as follows:
```diff
- build --action_env TF_SHARED_LIBRARY_NAME=""ensorflow_framework.2""
+ build --action_env TF_SHARED_LIBRARY_NAME=""libtensorflow_framework.2.dylib""
```
and build had proceeded with no error on that portion of code.
I also tried `libtensorflow_framework.2` on that env, but it also failed.

## Possible Fix

This error may produced by this code on `configure.py`:

```python
def generate_shared_lib_name(namespec):
  """"""Converts the linkflag namespec to the full shared library name.""""""
  # Assume Linux for now
  return namespec[1][3:]
```

This function may replace the prefix of the name of the shared library, but it fails on the macOS environment.
I can send a PR with a fix; capture the name of OS and branch off the logic.

## Environment

My bazel version is `bazel 3.4.1-homebrew`, executed on macOS Catalina."
"Hi @gariel-google ,
Your paper mentioned you leanred the distortion coeficients (two of them - quadratic and quartic), but your code has functions to apply distortion only with the quadratic coeficents, and no conv heads for it.

Did you use a 1x1 conv with 1 output, and no activation function?
Will you publish the code for the quatric distortion coeficient?

Thank you!
"
"Hi,

I have a question for TRILL model. What is the step size of feature extraction and what is the window size of each feature. 
For example, 
audio length: 2.16  feature count: 8
audio length: 3.36  feature count: 15
audio length: 6.6    feature count: 34
audio length: 1.49  feature count: 4
(Sampling rate is 16000)
What is the relation between audio length and the number of extracted feature?
Or more specifically, is there any way of knowing the timestamp of each extracted feature?

Thanks.






"
"Very impressed by the performance that mobileBERT delivers, is it possible to get a multi-lingual version please?"
"Hello, 
This is a question  for the repo about seq2act paper.
i follow the steps in the README file under seq2act/data_generation and try to Generate RicoSCA Datasets. I can successfully run seq2act/data_generation/create_rico_sca.sh and geneate tfrecord files，but those generated tfrecord files are empty. Do i need extra step except what listed in the README file? please give me your suggestion when you are free, thank you."
"In the file model_tft.py, function get_tft_embeddings(), the following piece of the code seems incorrect:

# Observed (a prioir unknown) inputs
    wired_embeddings = []
    for i in range(num_categorical_variables):
      if i not in self._known_categorical_input_idx \
        and i not in self._input_obs_loc:
        e = embeddings[i](categorical_inputs[:, :, i])
        wired_embeddings.append(e)

self._known_categorical_input_idx starts from the first categorical input of all_input, while self._input_obs_loc starts from 0 of all inputs. The condition here is incorrect.

e.g.
_column_definition = [
      ('Symbol', DataTypes.CATEGORICAL, InputTypes.ID),
      ('date', DataTypes.DATE, InputTypes.TIME),
      ('log_vol', DataTypes.REAL_VALUED, InputTypes.TARGET),
      ('open_to_close', DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),
      ('days_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),
      ('day_of_week', DataTypes.CATEGORICAL, InputTypes.OBSERVED_INPUT),
      ('day_of_month', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('week_of_year', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('month', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('Region', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),
  ]

log_vol is the target, and self._input_obs_loc = [0]
5 categorical inputs: day_of week, day_of_month, week_of_year, moth, region, and self._know_categorical_input_idx = [1, 2, 3, 4]. We made up the first categorical input 'day_of_week' to 'OBSERVED_INPUT' (unknown input).
We expected to append observed categorical input 'day_of_week' to wired_embeddings, however as the index conflict with self._input_obs_loc,  the code did not work.

Proposed fix:
# Observed (a prioir unknown) inputs
    wired_embeddings = []
    for i in range(num_categorical_variables):
      if i not in self._known_categorical_input_idx \
        and i + num_regular_variables not in self._input_obs_loc:
        e = embeddings[i](categorical_inputs[:, :, i])
        wired_embeddings.append(e)

changed 'i' to 'i + num_regular_variables'"
"Thank you very much for your code. But I had a problem loading the weight: tf.train. latest_checkpoint (checkpoint_dir) return None Type. It seems to lack the ""checkpoint"" file in the compressed package."
"I find that there is [depthwise conv 1D](https://github.com/google-research/google-research/tree/master/kws_streaming/layers) in this project. However, I don't see it being used by any model. Since I am curious about this and would try it I guess, could someone explain for me why it is not used?

In addition, imo Depthwise Separable Conv may be the most efficient model for KWS due to its much less filters and memory access. Have you tried DSC+Resnet? It is worth noting that DSC+ResNet is SOTA in terms of efficiency[[1]](https://arxiv.org/pdf/2004.12200.pdf)[[2]](https://arxiv.org/pdf/2004.08531.pdf). Just wanna know the experiments you have done so that I could have some clue on how to do my research. Much appreciated!

Sorry for bothering @rybakov. I find that you are the main contributor for this project so it would be more efficient to ping you."
"Dear Google staff

I have created the pull request on the ""conqur"" folder. My pull request is: https://github.com/google-research/google-research/pull/315.

It has been more than a week now, do you know how can my pull be accepted?"
Can mask in Scaled dot-product be set to None?
"Running on Ubuntu 18.04, I followed the brief setup instructions.

```
gcc (Ubuntu 9.3.0-11ubuntu0~18.04.1) 9.3.0
g++ (Ubuntu 9.3.0-11ubuntu0~18.04.1) 9.3.0
bazel 3.4.1
```

After running ./run_demo.sh

```
INFO: Analyzed target //:run_search_experiment (41 packages loaded, 1469 targets configured).
INFO: Found 1 target...
ERROR: /home/clayton/tree/automl/google-research/automl_zero/BUILD:272:11: C++ compilation of rule '//:experiment_util' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 63 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
experiment_util.cc:24:10: fatal error: absl/container/node_hash_map.h: No such file or directory
   24 | #include ""absl/container/node_hash_map.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
Target //:run_search_experiment failed to build
```

I confirmed that the file is present in the bazel files -

```
clayton@clayton-VirtualBox:~/tree/automl/google-research/automl_zero$ ls bazel-automl_zero/external/com_google_absl/absl/container/
btree_benchmark.cc  BUILD.bazel                           fixed_array_test.cc    inlined_vector_benchmark.cc              node_hash_map.h
btree_map.h         CMakeLists.txt                        flat_hash_map.h        inlined_vector_exception_safety_test.cc  node_hash_map_test.cc
btree_set.h         fixed_array_benchmark.cc              flat_hash_map_test.cc  inlined_vector.h                         node_hash_set.h
btree_test.cc       fixed_array_exception_safety_test.cc  flat_hash_set.h        inlined_vector_test.cc                   node_hash_set_test.cc
btree_test.h        fixed_array.h                         flat_hash_set_test.cc  internal

```

"
"Should the referenced line of code batch the train data opposed to the validation data ?

https://github.com/google-research/google-research/blob/bc0310db2874b05e08fa285697c209135d9096cc/tft/libs/tft_model.py#L1130"
"Thanks for this native python implementation of ROUGE! 
https://github.com/google-research/google-research/tree/master/rouge

While it is great that you specify in your README that you do not support all the command line arguments supported by the original perl package rouge1.5.5,  it will greatly benefit if you also say exactly which ones you support and which ones you do not and if I can turn on/off specific arguments.

thanks!"
"Hello great work @gariel-google and team!
I tried training the model without any pretrained weights on about 5300 images for about 35 epochs and also provided possibly mobile masks for the moving objects as shown
![0000001418-fseg](https://user-images.githubusercontent.com/31023599/87225782-f4f96e00-c3ac-11ea-97bf-6c7ebc794771.png)

I got the following loss graph
![lossdepth_from](https://user-images.githubusercontent.com/31023599/87225705-93d19a80-c3ac-11ea-8101-0e991c9c890f.png)
But the result of depth inference is :
![0000001419](https://user-images.githubusercontent.com/31023599/87225164-b2359700-c3a8-11ea-91b4-f47502bd5e4e.png)
The result without training and directly infering on cityscapes pretrained weights provided in the repo is little better.
![0000001418](https://user-images.githubusercontent.com/31023599/87225199-e7da8000-c3a8-11ea-8922-ac5293e6f19b.png)
I took the inference code from struct2depth and replaced their model with depth_from_video_in_the_wild's model
When I plot the egomotion with the trained model, it looks ok.
Can I know what might have gone wrong while training?
Any suggestion is greatly appreciated.
Thanks.
 "
""
"Dear scannn authors, 

I would like to bring your attention on recent work brining SIMD distance evaluation to FAISS. The work includes methods for doing SIMD lookup for larger tables thus allowing more precise distance (than with 16-values tables as done your work) while keeping the advantage of SIMD using recently-released AVX512-capable processors. I believe the method could be applied to scannn similarly to how QuickerADC extends QuickADC. 

See https://github.com/nlescoua/faiss-quickeradc for source code.

The corresponding paper is available as preprint: IEEE Transaction on Pattern Analysis and Machine Intelligence
https://ieeexplore.ieee.org/document/8896060
https://arxiv.org/abs/1812.09162

Best regards"
"Yeah I noticed that Pull Request merge too! only 2 lines seemed to have been changed in that merge.
However, for now I'm using TF 1.14 and things run fine.

_Originally posted by @SachitNayak in https://github.com/google-research/google-research/issues/284#issuecomment-636922075_"
 I have upgraded tensorflow to 2.0. why would it go wrong? module 'tensorflow' has no attribute 'compat'？
"### 1. The entire URL of the file you are using

https://github.com/google-research/exoplanet-ml/tree/master/exoplanet-ml/astronet

### 2. Describe the bug

I'm trying to create training, validation and test set files in ""Process Kepler Data"" section.

```
# Preprocess light curves into sharded TFRecord files using 5 worker processes.
bazel-bin/astronet/data/generate_input_records \
  --input_tce_csv_file=${TCE_CSV_FILE} \
  --kepler_data_dir=${KEPLER_DATA_DIR} \
  --output_dir=${TFRECORD_DIR} \
  --num_worker_processes=5
```

The process seems to start without any issues and it starts to create tfrecords, but after a while I get this error:
**""ValueError: could not broadcast input array from shape (4,43) into shape (4,39)""**

### 3. Steps to reproduce

**1. Setup the environment**
The following libraries are required:
tensorflow==1.15
tensorflow-probability==0.8
pandas
numpy
scipy
astropy
pydl
absl-py

Ensure that test and build pass

```
cd exoplanet-ml  # Bazel must run from a directory with a WORKSPACE file
bazel test astronet/... astrowavenet/... light_curve/... tf_util/... third_party/...
```

`bazel build astronet/...`

**2. Download Kepler data**

First, download the DR24 TCE Table in CSV format from the [NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu/cgi-bin/TblView/nph-tblView?app=ExoTbls&config=q1_q17_dr24_tce) and ensure the following columns are selected:

-    rowid: Integer ID of the row in the TCE table.
-    kepid: Kepler ID of the target star.
-   tce_plnt_num: TCE number within the target star.
-   tce_period: Period of the detected event, in days.
-   tce_time0bk: The time corresponding to the center of the first detected event in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days.
-  tce_duration: Duration of the detected event, in hours.
-  av_training_set: Autovetter training set label; one of PC (planet candidate), AFP (astrophysical false positive), NTP (non-transiting phenomenon), UNK (unknown).

```
# Filename containing the CSV file of TCEs in the training set.
TCE_CSV_FILE=""${HOME}/astronet/dr24_tce.csv""

# Directory to download Kepler light curves into.
KEPLER_DATA_DIR=""${HOME}/astronet/kepler/""

# Generate a bash script that downloads the Kepler light curves in the training set.
python astronet/data/generate_download_script.py \
  --kepler_csv_file=${TCE_CSV_FILE} \
  --download_dir=${KEPLER_DATA_DIR}

# Run the download script to download Kepler light curves.
./get_kepler.sh
```
### NOTE: the following script take up about 90 GB.

**3. Process Kepler data**

```
# Directory to save output TFRecord files into.
TFRECORD_DIR=""${HOME}/astronet/tfrecord""

# Preprocess light curves into sharded TFRecord files using 5 worker processes.
bazel-bin/astronet/data/generate_input_records \
  --input_tce_csv_file=${TCE_CSV_FILE} \
  --kepler_data_dir=${KEPLER_DATA_DIR} \
  --output_dir=${TFRECORD_DIR} \
  --num_worker_processes=5
```

### 4. Expected behavior

When the script finishes you will find 8 training files, 1 validation file and 1 test file in TFRECORD_DIR. The files will match the patterns train-0000?-of-00008, val-00000-of-00001 and test-00000-of-00001 respectively.

### 5. Additional context

**Traceback (most recent call last):**

File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
result = (True, func(*args, **kwds))
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 164, in _process_file_shard
example = _process_tce(tce)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 144, in _process_tce
time, flux = preprocess.process_light_curve(all_time, all_flux)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/preprocess.py"", line 72, in process_light_curve
spline = kepler_spline.fit_kepler_spline(all_time, all_flux, verbose=False)[0]
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/third_party/kepler_spline/kepler_spline.py"", line 321, in fit_kepler_spline
verbose=verbose)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/third_party/kepler_spline/kepler_spline.py"", line 216, in choose_kepler_spline
time, flux, bkspace=bkspace, maxiter=maxiter)
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/third_party/kepler_spline/kepler_spline.py"", line 104, in kepler_spline
curve = bspline.iterfit(time[mask], flux[mask], bkspace=bkspace)[0]
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/pydl/pydlutils/bspline.py"", line 639, in iterfit
x2=x2work)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/pydl/pydlutils/bspline.py"", line 189, in fit
errb = cholesky_band(alpha, mininf=min_influence)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/pydl/pydlutils/bspline.py"", line 491, in cholesky_band
L[:, 0:n] = lower
ValueError: could not broadcast input array from shape (4,43) into shape (4,39)
""""""

**The above exception was the direct cause of the following exception:**

Traceback (most recent call last):
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 256, in
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/absl/app.py"", line 299, in run
_run_main(main, args)
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
sys.exit(main(argv))
File ""/home/s.fiscale/conda/exoplanet-ml/exoplanet-ml/bazel-bin/astronet/data/generate_input_records.runfiles/main/astronet/data/generate_input_records.py"", line 248, in main
async_result.get()
File ""/home/s.fiscale/anaconda3/envs/astronet_env/lib/python3.7/multiprocessing/pool.py"", line 657, in get
raise self._value
**ValueError: could not broadcast input array from shape (4,43) into shape (4,39)**

### 6. System information

I'm testing the model in conda environment

-    OS Platform and Distribution: ('CentOS Linux', '7.6.1810', 'Core')
-    TensorFlow installed from : source (https://anaconda.org/conda-forge/tensorflow)
-   TensorFlow version : 1.15.0
-    Python version: 3.7.7
-    Bazel version : 2.1.0

Thanks.
"
Can you please add Skipgram metric in ROUGE metrics
""
"Hi, 
It seems that all links in the [directory](https://github.com/google-research/google-research/blob/master/fastconvnets) are down.
"
"I got attention_weights using model.get_attention(train) after model.fit() as below.

total_time_step = 84 + 30
num_encoder_step = 84
num_static = 2
num_known_input = 10
num_observed_input = 2

For visualizing variable importance and temporal patterns,

what part should I use?

Plus, please explain some more.

For example, if I have to use attention_weights['decoder_self_attn'][0,-1,:,:],

what is the meaning of that [114,114]?

<img width=""364"" alt=""attn_weights_example"" src=""https://user-images.githubusercontent.com/49193062/84853629-8faaa980-b09a-11ea-8202-71e83aff08b1.PNG"">
"
"when I print the results, it has the following and no 'unique_ids'
dict_keys(['start_logits', 'end_logits'])
This has been run on CPU. 


INFO:tensorflow:prediction_loop marked as finished
I0612 14:17:58.848829 139625204832064 error_handling.py:101] prediction_loop marked as finished
Traceback (most recent call last):
(text omitted here ...) 
  File ""run_squad.py"", line 1439, in main
    unique_id = int(result[""unique_ids""])
KeyError: 'unique_ids'

command to use:
python3 run_squad.py \
  --bert_config_file=uncased_L-2_H-128_A-2/bert_config.json \
  --data_dir=${DATA_DIR} \
  --do_lower_case \
  --do_predict \
  --do_train \
  --doc_stride=128 \
  --init_checkpoint=${INIT_CHECKPOINT}/bert_model.ckpt \
  --learning_rate=4e-05 \
  --max_answer_length=30 \
  --max_query_length=64 \
  --max_seq_length=384 \
  --n_best_size=20 \
  --num_train_epochs=5 \
  --output_dir=${OUTPUT_DIR} \
  --predict_file=/my_path_to/dev-v1.1.json \
  --train_batch_size=32 \
  --train_file=/my_path_to/train-v1.1.json \
  --vocab_file=${INIT_CHECKPOINT}/vocab.txt \
  --warmup_proportion=0.1\
  --export_dir=export_model
"
"Hi, I am new here. I want to ask a silly question:

This is the rough score I got:
ROUGE_1: 
AggregateScore(low=Score(precision=0.3997101114187033, recall=0.5115655746404962, fmeasure=0.43832121374077704), mid=Score(precision=0.40219078493377414, recall=0.514241835608697, fmeasure=0.4406308217147367), high=Score(precision=0.40483246876750517, recall=0.516888812840918, fmeasure=0.4430230464355599))

The BART paper report rouge_1 on cnn/daily dataset for 44.16. Does this score mean measure? What is meaning about ""low"", ""mid"" and ""high""?
"
"The weight prunning of transformer achieves high performance. But we also focus on the speed of inference. Whether the inference has also accelerated?

Looking forward to your reply.
Thanks!"
""
"In the MobileBERT paper, it's mentioned that the optimization hyperparameters (batch sizes, learning rates, number of epochs) are searched to choose the model. What are the optimal hyperparameters for the available tasks? "
"I encountered the following error when running a sample program of Temporal Fusion Transformers (tft) see [here](https://aihub.cloud.google.com/u/0/p/products%2F9f39ad8d-ad81-4fd9-8238-5186d36db2ec)

Specifically this line seems to cause it:

```
import tensorflow as tf
from libs.tft_model import TemporalFusionTransformer

tf_config = utils.get_default_tensorflow_config(tf_device=""gpu"", gpu_id=0) 
```

and the stacktrace is:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-23-9367d03d4948> in <module>()
      3 
      4 # Specify GPU usage
----> 5 tf_config = utils.get_default_tensorflow_config(tf_device=""gpu"", gpu_id=0)
      6 tf.reset_default_graph()
      7 with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:

/content/repo/tft/libs/utils.py in get_default_tensorflow_config(tf_device, gpu_id)
    151     print('Selecting GPU ID={}'.format(gpu_id))
    152 
--> 153     tf_config = tf.ConfigProto(log_device_placement=False)
    154     tf_config.gpu_options.allow_growth = True
    155 

AttributeError: module 'tensorflow' has no attribute 'ConfigProto' 

```

According to my understanding this is an issue due to incompatibility with tensorflow 2.x

I haven't yet tried the stackoverflow workaround mentioned [here](https://stackoverflow.com/questions/56127592/attributeerror-module-tensorflow-has-no-attribute-configproto)

Linking a related issue discussing migration to TF2 #260 

PS: I could work on this issue if its assigned to me - just migrating the ConfigProto part in the tft/libs/utils.py file ( not possibly migrating entire tft codebase to TF2)
"
"Thank you for sharing your wonderful OSS.

https://github.com/google-research/google-research/tree/master/seq2act/data_generation
```
sh seq2act/data_generation/create_rico_sca.sh
```

I'm trying to generate a dataset using the above steps and commands.
However, the absence of these two files in data_generation directory causes an error during the script execution.

```
--vocab_file=${PWD}""/seq2act/data_generation/lower_case_vocab"" \
--input_candiate_file=${PWD}""/seq2act/data_generation/input_candidate_words.txt"" \
```
How did you create these two files(vocab file and input_candidate_words file)?
If you can't seem to release the two files yet, I'm going to try to create my own.(if possible)
I would appreciate it if you could tell me."
"Hello All,

I am using TensorFlow-gpu 1.15.2 python3 on Ubuntu 18.04. I noted that the dql_grasping was originally made on a distributed system but I cannot find the corresponding pipeline code. Hence I decide to make one by following [this tutorial](https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md). My purpose is to run the distributed version of dql_grasping on a machine with 1 CPU and multiple GPUs (this can be extended later). However, I got the following error regarding the initialization some slim variables. Note that these TruncatedNormal variables are defined in dql_grasping/tf_modules.py

If anyone happens to have similar issue, or already have the working code of running drl_grasping in a distributed system, please let me know. Thanks!
  
File ""/home/yy/DRL/google-research-master/dql_grasping/run_train_collect_eval.py"", line 144, in <module>
    app.run(main)
  File ""/home/yy/.local/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/yy/.local/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/yy/DRL/google-research-master/dql_grasping/run_train_collect_eval_parallel.py"", line 124, in main
    worker_device=worker_device)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/yy/DRL/google-research-master/dql_grasping/train_collect_eval.py"", line 127, in train_collect_eval
    policy = policy_class()
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/yy/DRL/google-research-master/dql_grasping/policies.py"", line 178, in __init__
    self._greedy_policy = greedy_policy_class()
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1078, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/utils.py"", line 49, in augment_exception_message_and_reraise
    six.raise_from(proxy.with_traceback(exception.__traceback__), None)
  File ""<string>"", line 3, in raise_from
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/gin/config.py"", line 1055, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/yy/DRL/google-research-master/dql_grasping/policies.py"", line 237, in __init__
    q_func, state_shape, use_gpu=use_gpu, checkpoint=checkpoint)
  File ""/home/yy/DRL/google-research-master/dql_grasping/policies.py"", line 107, in __init__
    self._sess.run(init_op)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal: Could not satisfy explicit device specification '' because the node node q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal (defined at /DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) placed on device Device assignments active during op 'q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal' creation:
...
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=-1 requested_device_name_='/job:ps/task:0/device:CPU:0' assigned_device_name_='' resource_device_name_='/job:ps/task:0/device:CPU:0' supported_device_types_=[GPU, CPU, XLA_CPU, XLA_GPU] possible_devices_=[]
ReadVariableOp: GPU CPU XLA_CPU XLA_GPU 
AssignVariableOp: GPU CPU XLA_CPU XLA_GPU 
Mul: GPU CPU XLA_CPU XLA_GPU 
TruncatedNormal: GPU CPU XLA_CPU XLA_GPU 
Add: GPU CPU XLA_CPU XLA_GPU 
VarHandleOp: GPU CPU XLA_CPU XLA_GPU 
Const: GPU CPU XLA_CPU XLA_GPU 
VarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU 

Colocation members, user-requested devices, and framework assigned devices, if any:
  q_func/Conv/weights/Initializer/truncated_normal/shape (Const) 
  q_func/Conv/weights/Initializer/truncated_normal/mean (Const) 
  q_func/Conv/weights/Initializer/truncated_normal/stddev (Const) 
  q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal (TruncatedNormal) 
  q_func/Conv/weights/Initializer/truncated_normal/mul (Mul) 
  q_func/Conv/weights/Initializer/truncated_normal (Add) 
  q_func/Conv/weights (VarHandleOp) /job:ps/task:0/device:CPU:0
  q_func/Conv/weights/IsInitialized/VarIsInitializedOp (VarIsInitializedOp) /job:worker/task:0
  q_func/Conv/weights/Assign (AssignVariableOp) /job:ps/task:0/device:CPU:0
  q_func/Conv/weights/Read/ReadVariableOp (ReadVariableOp) /job:ps/task:0/device:CPU:0
  q_func/Conv/Conv2D/ReadVariableOp (ReadVariableOp) /job:ps/task:0/device:CPU:0
  save/AssignVariableOp_2 (AssignVariableOp) /job:ps/task:0/device:CPU:0

	 [[node q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal (defined at /DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]Additional information about colocations:No node-device colocations were active during op 'q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal' creation.
Device assignments active during op 'q_func/Conv/weights/Initializer/truncated_normal/TruncatedNormal' creation:
  with tf.device(None): </home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1535>
  with tf.device(): </home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/contrib/framework/python/ops/variables.py:268>
  with tf.device(_ReplicaDeviceChooser.device_function</home/yy/DRL/venv/lib/python3.6/site-packages/tensorflow_core/python/training/device_setter.py, 99>): </home/yy/DRL/google-research-master/dql_grasping/train_collect_eval.py:126>
"
"I am trying to replicate the mobileBERT paper results but the current code is missing support for QQP, RTE, and STS-B. Are there plans to update the code to include these tasks? "
"Hello, how can I solve this problem:
**The requirement.txt says I need to install tensorflow 1.15 but deterministic is an argument of TF 2.x** 

`Traceback (most recent call last):
  File ""run_pretraining.py"", line 983, in <module>
    tf.app.run()
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""run_pretraining.py"", line 863, in main
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_distill_steps)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3035, in train
    rendezvous.raise_errors()
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 136, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1188, in _train_model_default
    input_fn, ModeKeys.TRAIN))
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1025, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/home/lcy/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2987, in _call_input_fn
    return input_fn(**kwargs)
  File ""run_pretraining.py"", line 747, in input_fn
    deterministic=(not is_training))
TypeError: interleave() got an unexpected keyword argument 'deterministic'
`"
"Excuse me, I am confused about the relationship between this open source project and other open source projects under github.com/google-research organization. In other words, what kind of open source projects will be included in this large open source project? What kind of open source projects will be opened independently? Thanks."
"I compared the results of Depth from wild and PSMNet.
The size of the 3D point Cloud detected by 'Depth from video in wild' was much larger than the size of the 3D Point Cloud of PSMNet.
Like this
[PSMNet]
![LiDAR](https://user-images.githubusercontent.com/33896710/81895490-4247a200-95ed-11ea-8b67-01f2786a7429.PNG)
[Depth from video in wild]
![Google](https://user-images.githubusercontent.com/33896710/81895498-4a074680-95ed-11ea-8d41-d0c18f58a4cd.PNG)

<First Question>
Does the depth image detected from depth from 'Depth from video in wild' contain the depth value or disparity value.

<Second Question>
I know that 'Depth from video in wild' learning intrinsic, but do this still detect depth values relatively?
The performance of intrinsic learning is good, so it seems that i can get a value similar to the absolute depth value.

"
"Hi, I'm interested in using the multitask BAM network, where both the multitask model and the teacher models are ELECTRA. The question I have is since one of the tasks I need done is MLM while the other is classification, and they're done by different models (in ELECTRA the generator and discriminator respectively), can they be used as teachers for BAM? Because as I understand from the paper the teachers must have the exact same model archtecture as each other and as the student, and in this case the generator and discriminator are different models.

Thank you!"
"In the demonstration file at https://github.com/google-research/google-research/blob/master/mol_dqn/chemgraph/optimize_logp.py

Line 46 reads:
`return molecules.penalized_logp(molecule)`

However, this reward is not discounted for timesteps, unlike the same demonstration file for QED
https://github.com/google-research/google-research/blob/master/mol_dqn/chemgraph/optimize_qed.py

```
qed = QED.qed(molecule)
return qed * self.discount_factor ** (self.max_steps - self.num_steps_taken)
```
Was this an oversight, or executed on purpose? 

Thank you on advance"
"When I tried to run the code for the /data_example,the following bug showed up.Can you help me please?
-----------------------------------------------------------------------------------------------------------
2020-05-10 09:38:46.241554: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Fatal Python error: Aborted

Thread 0x00007fe4fd7fe700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe4fdfff700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe504ff9700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe5057fa700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe505ffb700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe5067fc700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe506ffd700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/threading.py"", line 551 in wait
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 311 in wait_for_stop
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 293 in _close_on_stop
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe5077fe700 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1257 in _single_operation_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 257 in _run
  File ""/usr/lib/python3.6/threading.py"", line 864 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe580f3f700 (most recent call first):
  File ""/usr/lib/python3.6/threading.py"", line 295 in wait
  File ""/usr/lib/python3.6/queue.py"", line 164 in get
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 159 in run
  File ""/usr/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/usr/lib/python3.6/threading.py"", line 884 in _bootstrap

Thread 0x00007fe619adc740 (most recent call first):
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407 in _call_tf_sessionrun
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319 in _run_fn
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334 in _do_call
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328 in _do_run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152 in _run
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929 in run
  File ""train.py"", line 195 in _train
  File ""train.py"", line 155 in main
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/absl/app.py"", line 299 in run
  File ""train.py"", line 236 in <module>
Aborted (core dumped)"
"You guys are creating a huge amount of great libraries and codebases with great explanations. But TF1 is really old and tired with it's alien syntax.

I'm mostly a pyTorch user but TF2 is really fine and dandy with how similiar it is to pyTorch and normal python syntax. Since most researchers basically use pyTorch its really hard to expect a new researchers to learn read old TF1 codes.

https://github.com/google-research/google-research/tree/master/dql_grasping
https://github.com/tensorflow/tensor2tensor

2 example of great libraries/code bases stuck with TF1 forever.

Please consider them upgrading to TF2"
"So I wanted to infer real depth from the network, and the `inference_depth()` function in `model.py` did give me some values for my test images.
1. I am unable to understand if those values are real depth or just disparity? 
2. If real depth then is the unit in meters or something else?
3. And to what range of real depth (in meters) can the n/w infer?"
"For autoML-zero, why in run_baseline.sh the search tasks and select tasks have the same held-out-pairs?"
"In the [unprocessing](https://github.com/google-research/google-research/blob/master/unprocessing/README.md) sub-repository (Section ""Evaluation on Different Real Data""), the link to the noise calibration script is broken. Could you please fix it? 

Thank you."
"@kevinkilgour 

I tried to convert the model provided at https://tfhub.dev/google/speech_embedding/1 to TFLite but unfortunately failed.

### Setup:
- tensorflow 2.2.0rc2
- tensorflow-hub 0.8.0

### Conversion
I tried the following:
1. Manually downloaded the model from https://tfhub.dev/google/speech_embedding/1?tf-hub-format=compressed
2. ```
   converter = tf.lite.TFLiteConverter.from_saved_model(
    ""speech_embedding"",
    tags=[]
   )
   tflite_model = converter.convert()
   ```
### Error
I get the following error:
```
ValueError                                Traceback (most recent call last)
<ipython-input-90-2fa2b3d63f8a> in <module>
      3     tags=[]
      4 )
----> 5 tflite_model = converter.convert()
      6 
      7 # converter.experimental_new_converter = True

~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)
    481               ""None is only supported in the 1st dimension. Tensor '{0}' has ""
    482               ""invalid shape '{1}'."".format(
--> 483                   _get_tensor_name(tensor), shape_list))
    484         elif shape_list and shape_list[0] is None:
    485           # Set the batch size to 1 if undefined.

ValueError: None is only supported in the 1st dimension. Tensor 'samples' has invalid shape '[None, None]'.
```

I tried as well to enable the new converter (`converter.experimental_new_converter = True`) but that didn't help either.

### Questions
1. Do you guys have a tflite version of the model that you would be willing to publish to tfhub?
2. Am I doing something wrong with the conversion? If so, please let me know.
3. Is there something you guys would need to change in the model to support TFLite? If so, would you be willing to perform that necessary change?

Thanks a lot for your help and the great work you have done with the speech_embedding model. Much appreciated.

"
"In the TFT branch, is there a reason why the multi-class is not activated? After looking at the code, it seems that the model itself can handle it. Thus, beside removing the lines throwing error and using target_col instead of [target_col] in few places, as the target column is now an array, I haven't seen much change to do to handle multi-class."
"For the following reference/summary pair:

```
ref = ""weald basin estimated to contain 100billion barrels of oil , new report says . \n uk oil & gas investments claims the site could supply 30 % of uk 's needs . \n if the maximum amount of oil is extracted it will be worth # 600billion . \n but some experts warn the site in sussex will be difficult to exploit .""

summ = ""trillions of pounds worth of oil - as much as the entire north sea fields - lies beneath an area of england dubbed 'britain's dallas', it was claimed yesterday. \n analysis suggests there is up to 100billion barrels of the fossil fuel under the home counties. \n the potential goldmine in the weald basin, across surrey, sussex, hampshire and kent, could meet up to a third of britain's oil demand within 15 years, according to the consortium exploring the area. \n uk oil & gas investments say they have discovered 100billion barrels worth of oil reserves in the weald basin, near gatwick airport .""

from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
scores = scorer.score(ref, summ)

# scores
# {'rouge1': Score(precision=0.2727272727272727, 
# recall=0.5192307692307693, 
# fmeasure=0.3576158940397351)}

# perl implementation results:
# 1 ROUGE-1 Average_R: 0.50000 (95%-conf.int. 0.50000 - 0.50000)
# 1 ROUGE-1 Average_P: 0.26263 (95%-conf.int. 0.26263 - 0.26263)
# 1 ROUGE-1 Average_F: 0.34437 (95%-conf.int. 0.34437 - 0.34437)
```

Looking at the unigrams extracted from each, for the PERL implementation, the stemmer will stem say -> sai and says -> say, while the nltk stemmer (nltk==3.4.5) will stem both cases to 'say' and thus have a different unigram intersection. "
"on readme.md, Customising Scripts for New Datasets section there's _my_serach_iterations_ instead of _my_search_iterations_"
"> Hi,
> 
> We just release the script/data for generating Amazon2M.
> Please download it [here](http://web.cs.ucla.edu/~chohsieh/data/Amazon2M.tar.gz), in which you can see how this data set is processed from the raw metadata.

Hello, the amazon 3M data on this [website](http://manikvarma.org/downloads/XC/XMLRepository.html) doesn't include metadata.json. It's not in both ""BoW Features"" and ""Raw text""

_Originally posted by @abcbdf in https://github.com/google-research/google-research/issues/69#issuecomment-605705200_"
"Hi google research team, I ran the file run_reddit.sh provided by you without any modification and the test accuracy is 0.96172 on Reddit dataset, which has much less than the result posted in the paper (0.996). I also notice than in the paper, you said ""For Reddit, a 4-layer GCN with 128 hidden units is used"" while in .sh file, the number of hidden units is 512 as the default value. I'm wondering if it is the reason and I'm doing experiment to verify this guess."
"Hello, In your paper, It is mentioned that you used pytorch, but this is a Tensorflow implementation. Could you please share the pytorch code, I am not familiar with Tensorflow.Many thanks!"
"When will be the code used in the paper ""Stand-Alone Self-Attention in Vision Models"" be available? The code link in the paper leads to [this page](https://github.com/google-research/google-research/tree/master/standalone_self_attention_in_vision_models) which just has a readme updated 5 months ago. "
"Hi. As is the instructions for frechet_audio_distance give errors:

```
(fad) [jon@jon-thinkpad google-research]$ python -m frechet_audio_distance.create_embeddings_main --input_files test_audio/test_files_background.cvs --stats stats/background_stats
.... Tensorflow nagging ...
Traceback (most recent call last):
  File ""/home/jon/.conda/envs/fad/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/jon/.conda/envs/fad/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/jon/work/senselab/soundquality/google-research/frechet_audio_distance/create_embeddings_main.py"", line 26, in <module>
    from frechet_audio_distance import create_embeddings_beam
  File ""/home/jon/work/senselab/soundquality/google-research/frechet_audio_distance/create_embeddings_beam.py"", line 36, in <module>
    from frechet_audio_distance.audioset_model import AudioSetModel
  File ""/home/jon/work/senselab/soundquality/google-research/frechet_audio_distance/audioset_model.py"", line 25, in <module>
    from tensorflow_models.audioset import mel_features
ImportError: cannot import name 'mel_features'
```
This seems to be due to a change in Tensorflow models, where the vggish directory was moved.
https://github.com/tensorflow/models/commit/4079c5d9693142a406f6ff392d14e2034b5f496d#diff-bdac73c3c32a5ba3834aa4ce2b8b345e

Using an earlier version seems to work fine, however then one also have to use Tensorflow 1.15
```
svn export https://github.com/tensorflow/models/tags/v1.13.0/research/audioset tensorflow_models/audioset

pip install tensorflow==1.15
```

Btw it also seems that Apache Beam supports Python3 now. Right now frechet_audio_distance is however not compatible. At least there are some trivial errors like use of `xrange`.

Would a PR which updates to Python3 and TensorFlow 2.x be welcomed? "
"ERROR: /home/fuxiai/google-research/automl_zero/BUILD:286:1: C++ compilation of rule '//:fec_hashing' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 72 argument(s) skipped)
"
"Hi big bro
When I run kws_streaming.model_train_eval.py by **CNN model** .It happend the error.
Environment:
Ubuntu 18.0.4
Tensorflow 2.1.0


Skipping registering GPU devices...
2020-03-13 15:24:52.786265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-13 15:24:52.786269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-13 15:24:52.786273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-13 15:24:53.758005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-13 15:24:53.758030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      
Traceback (most recent call last):
  File ""/home/mi/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/mi/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/mi/ASR/google-research/kws_streaming/train/model_train_eval.py"", line 548, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/mi/ASR/google-research/kws_streaming/train/model_train_eval.py"", line 185, in main
    test.tflite_non_stream_model_accuracy(flags, folder, fname)
  File ""/home/mi/ASR/google-research/kws_streaming/train/test.py"", line 453, in tflite_non_stream_model_accuracy
    interpreter.invoke()
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py"", line 493, in invoke
    self._interpreter.Invoke()
  File ""/home/mi/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 113, in Invoke
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_Invoke(self)
RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 28 (FlexBatchMatMul) failed to prepare.
"
"Throughout the repo there are a variety of executable shell scripts with shebang/interpreter lines (`#!`) that are not at the start of the relevant files (usually underneath a copyright notice). These are not parsed correctly and will be ignored as the shebang is only valid as the first two bytes of the file, see [`linux/fs/binfmt_script.c:load_script`](https://github.com/torvalds/linux/blob/f35111a946548e3b34a55abbad3e9bacce6cb10f/fs/binfmt_script.c#L42) as an example implementation.

It is likely that this was not noticed due to the majority of shebang lines specifying `/bin/bash` as the interpreter: if executing the scripts under bash this would behave mostly the same. However, a few scripts specify `-e` and `-u` which will get swallowed.

Files with shebang lines in the wrong position can be found using the following invocation:
```
find . -type f -executable -exec sh -c 'sed -n -e \'2,${/#!/q1}\' {} || echo {}' \; | sort
```
when executed from the repository root. As of 5e864806, this lists:
```
./abps/run.sh
./action_gap_rl/run.sh
./algae_dice/run.sh
./attribution/run.sh
./automl_zero/run_demo.sh
./automl_zero/run_integration_test_linear.sh
./automl_zero/run_integration_test_nonlinear.sh
./automl_zero/run_integration_tests.sh
./axial/run.sh
./bam/run.sh
./behavior_regularized_offline_rl/brac/run_bcq.sh
./behavior_regularized_offline_rl/brac/run_bc.sh
./behavior_regularized_offline_rl/brac/run_collect_data.sh
./behavior_regularized_offline_rl/brac/run_dual.sh
./behavior_regularized_offline_rl/brac/run_primal.sh
./behavior_regularized_offline_rl/brac/run_train_online.sh
./behavior_regularized_offline_rl/run.sh
./bitempered_loss/run.sh
./cfq/run_experiment.sh
./cfq/run.sh
./cluster_gcn/run_ppi.sh
./cluster_gcn/run_reddit.sh
./cnn_quantization/run.sh
./cold_posterior_bnn/run_resnet_experiment.sh
./cold_posterior_bnn/run.sh
./dac/run_evaluation_worker.sh
./dac/run_training_worker.sh
./dataset_analysis/run.sh
./dble/run.sh
./dense_representations_for_entity_retrieval/parse_wikinews.sh
./dense_representations_for_entity_retrieval/run.sh
./depth_from_video_in_the_wild/run.sh
./dql_grasping/run.sh
./dreg_estimators/run.sh
./dual_pixels/run.sh
./dvrl/run.sh
./edward2_autoreparam/run.sh
./eeg_modelling/viewer.sh
./eim/run.sh
./evanet/run.sh
./experience_replay/run.sh
./explaining_risk_increase/run.sh
./extrapolation/run.sh
./generalized_rates/run.sh
./genomics_ood/run.sh
./graph_compression/run.sh
./graph_embedding/watch_your_step/run.sh
./group_agnostic_fairness/run.sh
./hmc_swindles/run.sh
./hmc_swindles/scripts/fetch_datasets.sh
./hyperbolic_discount/run.sh
./igt_optimizer/run.sh
./interpretability_benchmark/run.sh
./large_margin/run.sh
./learnreg/run.sh
./meta_learning_without_memorization/pose_code/run.sh
./meta_reward_learning/semantic_parsing/run.sh
./meta_reward_learning/textworld/run.sh
./moew/run.sh
./moment_advice/run.sh
./m_theory/run.sh
./neutra/run.sh
./nigt_optimizer/run.sh
./nopad_inception_v3_fcn/run.sh
./norml/run.sh
./opt_list/run.sh
./playrooms/rooms/playroom/textures/download.sh
./poly_kernel_sketch/run.sh
./probabilistic_vqvae/run.sh
./pruning_identified_exemplars/run.sh
./psycholab/run.sh
./recursive_optimizer/run.sh
./rl4circopt/run.sh
./rllim/run.sh
./robust_loss/run.sh
./rouge/run.sh
./sm3/run.sh
./soft_sort/run.sh
./solver1d/run.sh
./stacked_capsule_autoencoders/eval_mnist_coupled.sh
./stacked_capsule_autoencoders/eval_mnist.sh
./stacked_capsule_autoencoders/run_constellation.sh
./stacked_capsule_autoencoders/run_mnist.sh
./stacked_capsule_autoencoders/run.sh
./stacked_capsule_autoencoders/setup_virtualenv.sh
./state_of_sparsity/run.sh
./storm_optimizer/run.sh
./sufficient_input_subsets/run.sh
./tabnet/run.sh
./tcc/dataset_preparation/download_pouring_data.sh
./tcc/run.sh
./tf_trees/run.sh
./tft/run.sh
./towards_gan_benchmarks/run.sh
./truss_decomposition/compile.sh
./truss_decomposition/run.sh
./uncertainties/scripts/train_local.sh
./uq_benchmark_2019/run.sh
./video_structure/run.sh
./wiki_split_bleu_eval/run.sh
``` "
Thanks a lot!
"Hi everyone,

I am currently reading the paper „Pairwise Fairness for Ranking and Regression“ and wanted to run the experiments. 

Unfortunately in the repository I can only find a notebook for running a toy example. The paper claims that the code for the experiments is available so I am wondering where I can find it or if it will be uploaded? 

Thank you,
Marius "
"Thankyou very much for shrae the big Dataset . However, There is No Label of each stroke (About this stroke is text or non-text) in DIdi Datset with textual context ? I want to use this dataset to do text-non-text classifaction, how can i get the label of each stroke? Thankyou ! "
How to save files in ImageFormat.RAW10 and metadata through the Java application?
"@gariel-google 
Sorry for opening a new issue about rendering the depth image from the checkpoint you provide of EuRoc MAV dataset  by the the sequence of image, since I can't find any script specific for inference the depth image for EuRoc MAV dataset.

Wish you could provide some instruction about it, thanks in advanced!"
""
"Hi,
Running the second instruction in the readme (should be python -m experiment_covertype - without the .py), I get the error: 

```
W0225 10:16:24.084449 140481427662656 ag_logging.py:145] Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fc40296fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fc40296fd90>>: AssertionError: Bad argument number for Name: 3, expecting 4
Traceback (most recent call last):
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/natalia/repos/google-research/tabnet/experiment_covertype.py"", line 200, in <module>
    app.run(main)
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/natalia/anaconda3/envs/tabnet/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/natalia/repos/google-research/tabnet/experiment_covertype.py"", line 99, in main
    feature_train_batch, reuse=False, is_training=True)
  File ""/home/natalia/repos/google-research/tabnet/tabnet_model.py"", line 201, in encoder
    mask_values = tf.contrib.sparsemax.sparsemax(mask_values)
AttributeError: module 'tensorflow.compat.v1' has no attribute 'contrib'
```"
"I have been trying to understand for how many steps (ie. gradient updates) the final Meena model (`10.2`) perplexity was trained for as it's not directly mentioned in the paper.

I think it's indirectly mentioned twice that points to around `2.5M` updates:

1) [Section: 3.3] Batch size: 4M tokens, Total tokens seen: 10T. so gradient updates comes around 2.5M

2) [Section: 3.3] Step time: 1 second, Total training time: 30 days, So number of steps: `~2.59M`.


But if above is true, there's seems to be some discrepancy or ambiguity with the following in the paper:

1) [Section: 3.2]
```
Our largest
(i.e., maximum memory usage) Evolved Transformer scored `10.2` perplexity and our largest vanilla Transformer scored perplexity 10.7 for the same number of training steps (738k).   
```
Which seems to indicate `738K` steps.


Am I interpreting the line in section 3.2 wrong? Can you please clarify this?

Thanks"
"I am trying to reproduce the TCC results on the Penn Action dataset. While scripts have been provided for the Pouring dataset, they have not been provided for the Penn Action dataset. 

To be able to train on the Penn Action dataset, I downloaded the dataset and then tried running images_to_tfrecords.py. However, I get the following error:

```
2020-02-08 23:00:23.276804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""images_to_tfrecords.py"", line 201, in <module>
    app.run(main)
  File ""/home/arjung2/.conda/envs/tcc/lib/python3.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/arjung2/.conda/envs/tcc/lib/python3.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""images_to_tfrecords.py"", line 197, in main
    FLAGS.expected_segments)
  File ""images_to_tfrecords.py"", line 188, in create_tfrecords
    FLAGS.action_label, frame_labels)
  File ""../../tcc/dataset_preparation/dataset_utils.py"", line 111, in write_seqs_to_tfrecords
    frame_labels_string=frame_labels_string)
  File ""../../tcc/dataset_preparation/dataset_utils.py"", line 80, in get_example
    context_features_dict = {'name': bytes_feature([name]),
  File ""../../tcc/dataset_preparation/dataset_utils.py"", line 39, in <lambda>
    bytes_feature = lambda v: feature(bytes_list=tf.train.BytesList(value=v))
TypeError: '1579' has type str, but expected one of: bytes
```

Interestingly, when I run the code multiple times, I get different numbers in the last line -- it is not deterministic. 

Any ideas about what might be going on? Thanks!"
Have annotations for the Penn Action dataset (for evaluation purposes) been released yet?
"Hello, I was wondering how you ends up with 1 ET, 13 ET blocks on models.

From the paper,

> The best performing Meena model is an Evolved Transformer (ET) (So et al., 2019) seq2seq model with 2.6B parameters, which includes 1 ET encoder block and 13 ET decoder blocks.

Does it means you also find Meena structure using evolutionary NAS structure? 
I would like to know how you get 1 ET and 13 ET structures since the numbers are not normal for me.

Thanks."
"I tried out the stochastic_to_deterministic research and the implementation  in the hashing.py ,but there seems to be an error in the syntax in line 55 : 
feature_sum_hex = [hashlib.md5(s).hexdigest() for s in feature_sum_str]
 The error stack trace:
Traceback (most recent call last):

  File ""<ipython-input-2-9c4701d54322>"", line 1, in <module>
    runfile('G:/s2d.py', wdir='G:')

  File ""G:\anaconda\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""G:\anaconda\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""G:/s2d.py"", line 115, in <module>
    app.run(main)

  File ""G:\anaconda\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)

  File ""G:\anaconda\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))

  File ""G:/s2d.py"", line 106, in main
    hash_val = compute_hash(features, hash_matrix, hash_vector)

  File ""G:/s2d.py"", line 63, in compute_hash
    feature_sum_hex = [hashlib.md5(s).hexdigest() for s in feature_sum_str]

  File ""G:/s2d.py"", line 63, in <listcomp>
    feature_sum_hex = [hashlib.md5(s).hexdigest() for s in feature_sum_str]

TypeError: Unicode-objects must be encoded before hashing

I have been using the required versions of absl and numpy which are specified in the requirements.txt


Resolution: The error is resolved by changing the syntax to 's.encode()' .I am submitting a PR for the same issue. "
"Hey, you did it again making us all excited about new promising research in NLP with the ""Towards a Human-like Open-Domain Chatbot"" paper and conversation transcripts. Any plans to publish the source code? "
"Take the experiment of electricity for example,   I notice that in the second window of the test set of each id in your code, it has the conditioning range with the ground truth of the values of 2014-09-01 instead of the prediction results, which equals to leak the test set information.

https://github.com/google-research/google-research/blob/1fec1abdc5a54d6f2ac8b5ef3420b36d4e6509f0/tft/libs/tft_model.py#L1220-L1229"
"Greetings google-research developers and contributors,

We’re reaching out because your project is an important part of the open source ecosystem, and we’d like to invite you to integrate with our [fuzzing](https://www.owasp.org/index.php/Fuzzing) service, [OSS-Fuzz]( https://opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html ). OSS-Fuzz is a free fuzzing infrastructure you can use to identify security vulnerabilities and stability bugs in your project. OSS-Fuzz will:

- Continuously run at scale all the fuzzers you write.
- Alert you when it finds issues.
- Automatically close issues after they’ve been fixed by a commit.

Many widely used [open source projects]( https://github.com/google/oss-fuzz/tree/master/projects ) like OpenSSL, FFmpeg, LibreOffice, and ImageMagick are fuzzing via OSS-Fuzz, which helps them find and remediate [critical issues]( https://bugs.chromium.org/p/oss-fuzz/issues/list?can=1&q=status%3AFixed%2CVerified+Type%3ABug%2CBug-Security+-component%3AInfra+ ). 

Even though typical integrations can be done in < 100 LoC, we have a [reward program]( https://www.google.com/about/appsecurity/patch-rewards/ ) in place which aims to recognize folks who are not just contributing to open source, but are also working hard to make it more secure.

We want to stress that anyone who meets the eligibility criteria and integrates a project with OSS-Fuzz is eligible for a reward.

If you're not interested in integrating with OSS-Fuzz, it would be helpful for us to understand why—lack of interest, lack of time, or something else—so we can better support projects like yours in the future.

If we’ve missed your question in our [FAQ]( https://google.github.io/oss-fuzz/faq/ ), feel free to reply or reach out to us at oss-fuzz-outreach@googlegroups.com.


Thanks!

Tommy
OSS-Fuzz Team
"
"when I run trajectory_inference.py,I got a problem:
    ValueError: Cannot feed value of shape (1, 370, 416, 3) for Tensor 'image1:0', which has shape '(1, 128, 416, 3)'.
How can I solve it?"
""
You reported results on SGD-S dev set to be 0.776 in Avg GA and 0.486 in Joint GA. I tried to reproduce your results and ran your code and it gave 0.742 Avg GA and 0.371 Joint GA. It's a large gap. I use tensorflow 1.12 so I made some necessary fixes to your code. I ran your code on one GPU. 
"I run the code of Cluster-GCN with the default parameters in run_reddit.sh, but the micro-F1 I get is 0.962. What's the optimized parameters for Reddit dataset?  "
""
"Hi, I've read and I'm really interested in your nips paper.

I wanna see the code of the experiment of artificial data that was used for checking the validity of ROAR framework.

I would like you to tell me where it is.
Thank you."
"I want to see masked_lm_loss & sentence_order_loss per iteration step(ex 1,000) with total loss during train on log.
and how plot these on tensorboard?

Are there any good ways?

"
"It appears that 
https://github.com/google-research/google-research/blob/b13a2d98f590495331e27b86268b000413ecb50d/depth_from_video_in_the_wild/transform_utils.py#L111
can produce arbitrarily wrong gradients on trans_vec1 which then back-propagate to corrupt the depth estimation network with NaN weights. 

The issue is caused by a `CUDA 10.0` bug https://github.com/tensorflow/tensorflow/issues/31166 which is fixed in `TF 2 nightly` by upgrading to `CUDA 10.1`, however this network requires `TF 1.x`. One workaround would be to recompile `TF 1.15` with `CUDA 10.1` or to use CPU for all calculations.

But in any case, the network as-is won't train on GPU for more than a few steps before going NaN with any released TF version. So I'm writing this issue mainly to document that for others.

"
"I tried to build and run the dual_pixel on my Google Pixel 3a, and the preview of dual pixel works.
but no matter how many times I push the ""capture"" button, the pgm file are not generated.

and I try to add the following codes, and I found `onCaptureFailed` occurs on my Pixel 3a.
Can anybody help me to make it works on my phone? Thank you.

```
     try {
       final CaptureRequest.Builder captureBuilder =
           cameraDevice.createCaptureRequest(CameraDevice.TEMPLATE_STILL_CAPTURE);
       captureBuilder.addTarget(imageReader.getSurface());
       captureBuilder.addTarget(new Surface(viewfinderTextureView.getSurfaceTexture()));
       captureBuilder.set(CaptureRequest.CONTROL_MODE, CameraMetadata.CONTROL_MODE_AUTO);
-      cameraCaptureSession.capture(captureBuilder.build(), null, cameraHandler);
+
+      CameraCaptureSession.CaptureCallback captureCallback = new CameraCaptureSession.CaptureCallback() {
+        @Override
+        public void onCaptureCompleted(CameraCaptureSession session, CaptureRequest request, TotalCaptureResult result) {
+          super.onCaptureCompleted(session, request, result);
+        }
+
+        @Override
+        public void onCaptureStarted(@NonNull CameraCaptureSession session, @NonNull CaptureRequest request, long timestamp, long frameNumber) {
+          super.onCaptureStarted(session, request, timestamp, frameNumber);
+          Log.d(TAG, ""onCaptureStarted"");
+        }
+
+        @Override
+        public void onCaptureFailed(@NonNull CameraCaptureSession session, @NonNull CaptureRequest request, @NonNull CaptureFailure failure) {
+          super.onCaptureFailed(session, request, failure);
+          Log.d(TAG, ""onCaptureFailed:"" + failure.getReason());
+          Log.d(TAG, ""onCaptureFailed:"" + failure.toString());
+        }
          cameraCaptureSession.capture(captureBuilder.build(), captureCallback, cameraHandler);
```

except the above code, I also get many messages in the logcat, but I do not know if that are related to this issue or not.

```
2019-11-29 23:53:14.685 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1556 SetMetadataByTag() Invalid slot; cannot set metadata tag 80210000
2019-11-29 23:53:14.685 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1720 PublishMetadataList() Invalid slot, cannot publish metadata list
2019-11-29 23:53:14.718 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1556 SetMetadataByTag() Invalid slot; cannot set metadata tag 80210000
2019-11-29 23:53:14.718 828-1099/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1720 PublishMetadataList() Invalid slot, cannot publish metadata list
```

```
2019-11-29 23:53:11.553 828-828/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1556 SetMetadataByTag() Invalid slot; cannot set metadata tag 80090003
2019-11-29 23:53:11.553 828-828/? E/CamX: [ERROR][HAL    ] camxmetadatapool.cpp:1720 PublishMetadataList() Invalid slot, cannot publish metadata list
2019-11-29 23:53:11.553 828-828/? E/CamX: [ERROR][SENSOR ] camxsensornode.cpp:2174 LoadPDlibrary() PD library CreateLib failure result=0, m_pPDLib=0x73e8735040
```

```
2019-11-29 23:53:17.664 848-1016/? I/sensors-hal: activate_physical_sensor:155, com.google.sensor.camera_vsync/7 en=0
2019-11-29 23:53:17.665 848-1016/? I/sensors-hal: activate_physical_sensor:166, com.google.sensor.camera_vsync/7 en=0 completed
2019-11-29 23:53:17.665 828-828/? I/GoogSensorSync: ~GoogSensorSync 72 Total failure/sync for camera 0: 0/58
```"
"Hi,

There is published paper and a model in TF Hub, but the training code seems to be missing from this repository."
"I am doing research in unsupervised depth estimation. And thanks for the preliminary code in 'depth from video in the wild'.

In this project, it is necessary to generate data before training, which is similar in another project of the author [https://github.com/tensorflow/models/tree/master/research/vid2depth](https://github.com/tensorflow/models/tree/master/research/vid2depth). However, this project assume that the data has not only image file and cam file, but also segment file, which is different from the vid2depth project. Without the segment file, the training cannot be conducted.

May I request the data generating code in the 'depth from video in the wild' project? Thank you very much."
What is the use of the parameter net_structure_type in the model function builder for the language model?
"I tried V2 first, it doesn't work (with TF 1.15), there is an earlier report about missing Einsum op.

I tried V1 then for Cola task, after training it shows 

eval_accuracy = 0.6912752
eval_loss = 0.6295214
global_step = 6413
loss = 0.62965536

However if I run prediction, it always predicts label ""1"", resulting test file looks really strange:

0.2420986	0.7579014
0.24209863	0.7579013
0.24209863	0.7579013
0.24209863	0.7579013
...

So did anyone tried those TFHub models? They don't look like usual bert ones (no .ckpt files), so probably some conversion/preparation is required?"
"Lines 468 - 475.

If `FLAGS.favor_shorter_ngram` is True, then pvals actually favors longer n-grams.

Line 474 should be changed to:

`if not FLAGS.favor_shorter_ngram:`"
It seems that the batchnorm weights were missing from the checkpoint. Loading from the checkpoint gives the error `Not found: Key conv1/bn/beta not found in checkpoint` @gariel-google 
"I noticed in the paper that the lambda hyperparameter is used to balance the model prediction and the true label.Also noticed that the lambda hyperparameter is linearly increased during the trainning,since the paper did not explain the linearly increasing lambda strategy,I am looking ofrward to find it in the code.However, I did not find the hyperparamter setting of lambda and the implementation of teacher annealing.All I noticed is that in the configure.py,there is a lambda=0.5,but this is used when there is no teacher annealing.
Anyone knows how the teacher annealing is implemented in the code?Or has the similiart problem with me?"
"I changed several config code position (includes `bert_dir` and `self.bert_config_file`) on `configure.Config`, so I can distill 12 layer teacher model's output to 3 layer student model by use another `bert_config_file`.

I could train teacher model successfully.
But when I train the student model of distillation, error occurs.

command for train teacher model:
`python -m bam.run_classifier news-model-teacher $BAM_DIR '{""debug"": false, ""task_names"": [""news""], ""pretrained_model_name"": ""chinese_L-12_H-768_A-12"", ""learning_rate"": 2e-5, ""num_train_epochs"": 3.0, ""distill"": false, ""max_seq_length"": 512, ""train_batch_size"": 4, ""save_checkpoints_steps"": 10000}'`

command for train student model:
`python -m bam.run_classifier news-model-student $BAM_DIR '{""debug"": false, ""task_names"": [""news""], ""pretrained_model_name"": ""chinese_L-12_H-768_A-12"", ""learning_rate"": 2e-5, ""num_train_epochs"": 3.0, ""distill"": true, ""bert_config_file"": ""/home/work/repo/google-research-master/bam/bam_dir/pretrained_models/chinese_L-12_H-768_A-12/bert_config_3_layer.json"", ""teachers"": {""news"": ""news-model-teacher""}, ""max_seq_length"": 512, ""train_batch_size"": 4, ""save_checkpoints_steps"": 10000}'`

error message:
```
Traceback (most recent call last):
  File ""/home/work/anaconda2/envs/py3-tf.1.12/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/work/anaconda2/envs/py3-tf.1.12/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/data/repo/google-research-master/bam/run_classifier.py"", line 276, in <module>
    tf.app.run()
  File ""/home/work/anaconda2/envs/py3-tf.1.12/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run 
    _sys.exit(main(argv))
  File ""/data/repo/google-research-master/bam/run_classifier.py"", line 253, in main
    model_runner = ModelRunner(config, tasks)
  File ""/data/repo/google-research-master/bam/run_classifier.py"", line 152, in __init__
    sizes) = self._preprocessor.prepare_train()
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 63, in prepare_train
    return self._serialize_dataset(self._tasks, True, ""train"")
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 108, in _serialize_dataset
    self.serialize_examples(examples, is_training, tfrecords_path)
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 127, in serialize_examples
    tf_example = self._example_to_tf_example(example, is_training)
  File ""/data/repo/google-research-master/bam/data/preprocessing.py"", line 136, in _example_to_tf_example
    example, is_training))
  File ""/data/repo/google-research-master/bam/task_specific/classification/classification_tasks.py"", line 136, in featurize
    self._distill_inputs[eid])
KeyError: 97
```"
"I am able to see INFO:tensorflow:loss = 134.62343, step = 97

but not the time.
"
"Hello, sorry, I am sorry to disturb you. I hope that you can provide the frame tag json file of Penn-Action, because the task of re-labeling the data set is really huge. Maybe you can share some tfrecord files that have been marked, I hope to get your reply. Thank you。@debidatta"
""
"After downloaded the source code, I tried to run the `run.sh`,
![Screenshot from 2019-11-04 18-58-14](https://user-images.githubusercontent.com/38068286/68112644-1f00b380-ff35-11e9-8b64-bd972d448940.png)
I always get this error shown in this picture. I am new to TF, any cues would be highly appreciated.
"
"From the paper, I understand that we can train the network by providing the only image files, but the code is asking for the camera matrix and the mask image for moving objects. Can you help to modify so that I can train using only image files?

Regards"
"I have the problem with the code below? why the input_dim becomes FLAGS_hidden1*2?the input_dim should equal to the last layer's output_dim??

for _ in range(self.num_layers - 2):
      self.layers.append(
          layers.GraphConvolution(
              input_dim=FLAGS.hidden1 * 2,
              output_dim=FLAGS.hidden1,
              placeholders=self.placeholders,
              act=tf.nn.relu,
              dropout=True,
              sparse_inputs=False,
              logging=self.logging,
              norm=self.norm,
              precalc=False))
"
"Sorry, I have a silly question. I can't run the codes, because there is always something wrong with the dataset. My directory is the following structure.
![image](https://user-images.githubusercontent.com/23647489/68083020-f0180e00-fe5e-11e9-9d62-6ced2ead0333.png)

Synthesize_MotionBlur
   -dataset
      --train
        ---1
           ----frame0.bmp
           ----frame1.bmp 
           ----blurred.bmp
     --test
   -result
   -train
     --dataset.py
     --estimator.py
     --network.py
     --train.py

And in the train.py, I set the 'train_pattern': `'../dataset/train/*'`, set the 'test_pattern': `'../dataset/test/*'`.  
![image](https://user-images.githubusercontent.com/23647489/68083055-3b322100-fe5f-11e9-83b8-0eba6b20d4ad.png)

> 2019-11-03 17:21:48.735950: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRandomAccessFile failed to Create/Open: ..\dataset\train\1\blurred.jpg/frame_0.jpg : 系统找不到指定 的路径。
> ; No such process
> 2019-11-03 17:21:48.744391: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRandomAccessFile failed to Create/Open: ..\dataset\train\1\frame_0.jpg/frame_0.jpg : 系统找不到指定 的路径。
> ; No such process
> 2019-11-03 17:21:48.756171: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRandomAccessFile failed to Create/Open: ..\dataset\train\1\frame_0.jpg/frame_1.jpg : 系统找不到指定 的路径。
> ; No such process
> 2019-11-03 17:21:48.768635: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at whole_file_read_ops.cc:114 : Not found: NewRa


I can't understand why this happended, please help me with this. How should I set up the file directory correctly?"
"  def _loss(self):
    """"""Construct the loss function.""""""
    # Weight decay loss
    if FLAGS.weight_decay > 0.0:
      for var in self.layers[0].vars.values():
        self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)"
"Hey,

Thanks for your nice work and open sources. However, It would be better if you provide a script that works on the standard KITTI odometry dataset? For example, it takes the ""dataset_dir"" as input and outputs the trajectory file in KITTI format (n x 12). 

Thanks,
Jiawang"
"I train the tcc model and evaluate 'classification'. But the result is 68.8/68.6(0.5/1.0), which is much worse than the result in paper(91.43/91.82). Is there any wrong for my training?

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from easydict import EasyDict as edict


CONFIG = edict()


CONFIG.DECODER = False

CONFIG.LOGDIR = '/tmp/alignment_logs/'

CONFIG.DATASETS = [
    # 'baseball_pitch',
    # 'baseball_swing',
    # 'bench_press',
    # 'bowling',
    # 'clean_and_jerk',
    # 'golf_swing',
    # 'jumping_jacks',
    # 'pushups',
    # 'pullups',
    # 'situp',
    # 'squats',
    # 'tennis_forehand',
    # 'tennis_serve',
    'pouring',
]


CONFIG.PATH_TO_TFRECORDS = '/home/cxu-serve/p1/zkou2/ad/'


CONFIG.TRAINING_ALGO = 'alignment'

CONFIG.IMAGE_SIZE = 224  # For ResNet50



CONFIG.TRAIN = edict()
CONFIG.TRAIN.MAX_ITERS = 150000

CONFIG.TRAIN.BATCH_SIZE = 2

CONFIG.TRAIN.NUM_FRAMES = 20
CONFIG.TRAIN.VISUALIZE_INTERVAL = 200

CONFIG.EVAL = edict()

CONFIG.EVAL.BATCH_SIZE = 2

CONFIG.EVAL.NUM_FRAMES = 20

CONFIG.EVAL.VAL_ITERS = 20
CONFIG.EVAL.TASKS = [
    # 'algo_loss',
    'classification',
    # 'kendalls_tau',
    # 'event_completion',
    # 'few_shot_classification'
]

CONFIG.EVAL.FRAMES_PER_BATCH = 25
CONFIG.EVAL.KENDALLS_TAU_STRIDE = 5  # 2 for Pouring, 5 for PennAction
CONFIG.EVAL.KENDALLS_TAU_DISTANCE = 'sqeuclidean'  # cosine, sqeuclidean
CONFIG.EVAL.CLASSIFICATION_FRACTIONS = [0.1, 0.5, 1.0]
CONFIG.EVAL.FEW_SHOT_NUM_LABELED = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
CONFIG.EVAL.FEW_SHOT_NUM_EPISODES = 50

CONFIG.MODEL = edict()

CONFIG.MODEL.EMBEDDER_TYPE = 'conv'

CONFIG.MODEL.BASE_MODEL = edict()

CONFIG.MODEL.BASE_MODEL.NETWORK = 'Resnet50_pretrained'

CONFIG.MODEL.BASE_MODEL.LAYER = 'conv4_block3_out'

CONFIG.MODEL.TRAIN_BASE = 'only_bn'
CONFIG.MODEL.TRAIN_EMBEDDING = True

CONFIG.MODEL.RESNET_PRETRAINED_WEIGHTS = '/home/cxu-serve/u1/zkou2/Code/tcc/repo/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5'


CONFIG.MODEL.VGGM = edict()
CONFIG.MODEL.VGGM.USE_BN = True

CONFIG.MODEL.CONV_EMBEDDER_MODEL = edict()

CONFIG.MODEL.CONV_EMBEDDER_MODEL.CONV_LAYERS = [
    (256, 3, True),
    (256, 3, True),
]
CONFIG.MODEL.CONV_EMBEDDER_MODEL.FLATTEN_METHOD = 'max_pool'

CONFIG.MODEL.CONV_EMBEDDER_MODEL.FC_LAYERS = [
    (256, True),
    (256, True),
]
CONFIG.MODEL.CONV_EMBEDDER_MODEL.CAPACITY_SCALAR = 2
CONFIG.MODEL.CONV_EMBEDDER_MODEL.EMBEDDING_SIZE = 128
CONFIG.MODEL.CONV_EMBEDDER_MODEL.L2_NORMALIZE = False
CONFIG.MODEL.CONV_EMBEDDER_MODEL.BASE_DROPOUT_RATE = 0.0
CONFIG.MODEL.CONV_EMBEDDER_MODEL.BASE_DROPOUT_SPATIAL = False
CONFIG.MODEL.CONV_EMBEDDER_MODEL.FC_DROPOUT_RATE = 0.1
CONFIG.MODEL.CONV_EMBEDDER_MODEL.USE_BN = True


CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL = edict()

CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.CONV_LAYERS = [(512, 3, True),
                                                   (512, 3, True)]

CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.GRU_LAYERS = [
    128,
]
CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.DROPOUT_RATE = 0.0
CONFIG.MODEL.CONVGRU_EMBEDDER_MODEL.USE_BN = True

CONFIG.MODEL.L2_REG_WEIGHT = 0.00001


CONFIG.ALIGNMENT = edict()
CONFIG.ALIGNMENT.CYCLE_LENGTH = 2
CONFIG.ALIGNMENT.LABEL_SMOOTHING = 0.1
CONFIG.ALIGNMENT.SOFTMAX_TEMPERATURE = 0.1

CONFIG.ALIGNMENT.LOSS_TYPE = 'classification'
CONFIG.ALIGNMENT.NORMALIZE_INDICES = True
CONFIG.ALIGNMENT.VARIANCE_LAMBDA = 0.001
CONFIG.ALIGNMENT.FRACTION = 1.0
CONFIG.ALIGNMENT.HUBER_DELTA = 0.1
CONFIG.ALIGNMENT.SIMILARITY_TYPE = 'l2' 

CONFIG.ALIGNMENT.STOCHASTIC_MATCHING = False


CONFIG.SAL = edict()
CONFIG.SAL.DROPOUT_RATE = 0.0

CONFIG.SAL.FC_LAYERS = [(128, True), (64, True), (2, False)]
CONFIG.SAL.SHUFFLE_FRACTION = 0.75

CONFIG.SAL.NUM_SAMPLES = 8
CONFIG.SAL.LABEL_SMOOTHING = 0.0

CONFIG.ALIGNMENT_SAL_TCN = edict()

CONFIG.ALIGNMENT_SAL_TCN.ALIGNMENT_LOSS_WEIGHT = 0.33
CONFIG.ALIGNMENT_SAL_TCN.SAL_LOSS_WEIGHT = 0.33

CONFIG.CLASSIFICATION = edict()
CONFIG.CLASSIFICATION.LABEL_SMOOTHING = 0.0
CONFIG.CLASSIFICATION.DROPOUT_RATE = 0.0

CONFIG.TCN = edict()
CONFIG.TCN.POSITIVE_WINDOW = 5
CONFIG.TCN.REG_LAMBDA = 0.002

CONFIG.OPTIMIZER = edict()

CONFIG.OPTIMIZER.TYPE = 'AdamOptimizer'

CONFIG.OPTIMIZER.LR = edict()

CONFIG.OPTIMIZER.LR.INITIAL_LR = 0.0001

CONFIG.OPTIMIZER.LR.DECAY_TYPE = 'fixed'
CONFIG.OPTIMIZER.LR.EXP_DECAY_RATE = 0.97
CONFIG.OPTIMIZER.LR.EXP_DECAY_STEPS = 1000
CONFIG.OPTIMIZER.LR.MANUAL_LR_STEP_BOUNDARIES = [5000, 10000]
CONFIG.OPTIMIZER.LR.MANUAL_LR_DECAY_RATE = 0.1
CONFIG.OPTIMIZER.LR.NUM_WARMUP_STEPS = 0


CONFIG.DATA = edict()
CONFIG.DATA.SHUFFLE_QUEUE_SIZE = 0
CONFIG.DATA.NUM_PREFETCH_BATCHES = 1
CONFIG.DATA.RANDOM_OFFSET = 1
CONFIG.DATA.STRIDE = 16
CONFIG.DATA.SAMPLING_STRATEGY = 'offset_uniform'
CONFIG.DATA.NUM_STEPS = 2  
CONFIG.DATA.FRAME_STRIDE = 15  

CONFIG.DATA.FRAME_LABELS = True
CONFIG.DATA.PER_DATASET_FRACTION = 1.0
CONFIG.DATA.PER_CLASS = False

CONFIG.DATA.SAMPLE_ALL_STRIDE = 1

CONFIG.AUGMENTATION = edict()
CONFIG.AUGMENTATION.RANDOM_FLIP = True
CONFIG.AUGMENTATION.RANDOM_CROP = False
CONFIG.AUGMENTATION.BRIGHTNESS = True
CONFIG.AUGMENTATION.BRIGHTNESS_MAX_DELTA = 32.0 / 255
CONFIG.AUGMENTATION.CONTRAST = True
CONFIG.AUGMENTATION.CONTRAST_LOWER = 0.5
CONFIG.AUGMENTATION.CONTRAST_UPPER = 1.5
CONFIG.AUGMENTATION.HUE = False
CONFIG.AUGMENTATION.HUE_MAX_DELTA = 0.2
CONFIG.AUGMENTATION.SATURATION = False
CONFIG.AUGMENTATION.SATURATION_LOWER = 0.5
CONFIG.AUGMENTATION.SATURATION_UPPER = 1.5


CONFIG.LOGGING = edict()
CONFIG.LOGGING.REPORT_INTERVAL = 100


CONFIG.CHECKPOINT = edict()
CONFIG.CHECKPOINT.SAVE_INTERVAL = 1000"
Why do you dilate the possible mobile masks by a dilation rate of 8? Was that to help with the problem of very small masks causing inf/nan loss? Or was there a greater importance to this? 
"Hi, 
I've been trying to run the ROAR code but I get the warning:
> Training with estimator made no steps. Perhaps input is empty or misspecified.

and then the training finishes with 
> Loss for final step: None. 

I have transformed the food_101 dataset into tfrecord files and I pass the base_dir (where the tfrecord files are placed) correctly (I've checked the data_dir in the input_fn). Also since I am using the food_101 dataset I pass the dataset_name accordingly. That would be great if you could help me with this.

@sarahooker @doomie "
"How to initialize the the population, P.
Someone can help me?"
"Hi,I have few questions for the Amazon-2M dataset
1.Can the code of cluster_gcn run on  Amazon-2M dataset?
2.for the dataset Amazon-2M
(1) Will the stopwords be removed?
(2) What is the ratio of training test data?"
"Is there an api or way to get the node embedding after training for `cluster_gcn`?

Not just doing a multi-class prediction task."
Can you publicly release Pouring and Penn_Action Dataset new annotations@debidatta
"Quick question.
What is the unit of the estimated ego-motion? Is it mm, cm, m?
If it is not real world units, how can it be converted?

Thanks a lot
"
"Hi, I doubt that data_utils.get_num_dialog_examples() returns correct number. 

In dstc8_single_domain & train, data_utils.get_num_dialog_examples() returns 82588, but the number of examples in dstc8_single_domain_train_examples.tf_record is 41294. I think these two numbers should be the same. Is it right? (The former is around as double as the latter because get_num_dialog_examples() counts USER and SYSTEM turns together. I think it's wrong.)"
"I use Tensorflow 1.14.

Traceback (most recent call last):
  File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/home/dhlee347_estsoft_com/google-research/schema_guided_dst/baseline/train_and_predict.py"", line 908, in <module>
    tf.compat.v1.app.run(main)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/dhlee347_estsoft_com/google-research/schema_guided_dst/baseline/train_and_predict.py"", line 854, in main
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2876, in train
    rendezvous.raise_errors()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 131, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2871, in train
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 367, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1158, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1192, in _train_model_default
    saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1484, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 754, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1252, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1353, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1338, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1411, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1169, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1158, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 487, in __init__
    self._assert_fetchable(graph, fetch.op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 500, in _assert_fetchable
    'Operation %r has been marked as not fetchable.' % op.name)
**ValueError: Operation u'truediv' has been marked as not fetchable.**
ERROR:tensorflow:Closing session due to error Step was cancelled by an explicit call to `Session::Close()`.
"
"hi,can you give the unprocessing_srgb_loss code,I run the process.py to process bayer RGGB into sRGB imag on DND dataset.But the color of the restored image is not right."
"Hello, Im trying to get prediction results (depth and camera parameters) given Image1, Image2 and Masks. But Im having hard time how to, assuming I have these images as numpy matrixes, can i make this prediction by calling a single method? I tried doing this myself but code is overwhelmingly complicated and I pretty much got lost

Im running the train code with single step and its executing without any errors, but cant find prediction results anywhere, tensorboard Image folder is empty.

Essentially what i am trying to create is a method with following signature
`depth_image, camera_calibration = model.predict(previous_image, current_image, mask)`

I have found a method that returns self.est_depth which i presume is depth map, but I couldnt find out how to supply the inputs to that method and how do i retrieve predicted camera calibration

+Update
Setting Summary freq to 1 and training for 10 episodes on single data instance using provided code seems to generate images on Tensorboard 
`python -m depth_from_video_in_the_wild.train --checkpoint_dir=***\trained_models --data_dir=***\depth_from_video_in_the_wild\data_example --train_steps=10`"
"Hi, I am running into the issue related to FailedPreconditionError: 2 root error, when I am trying to run the train.py within Unprocessing Images for Learned Raw Denoising.  Do you have any idea why this will happen and how to address it. I am using tf_gpu_1.12.0, python 3.5.8, cuda 9.0, Linux. Thanks

![image](https://user-images.githubusercontent.com/34078389/63974085-f950bd00-ca60-11e9-8a6b-bd4cb5e18c3d.png)"
"Hello,

regarding the distance function in visualize_alignment:

`
def dist_fn(x, y):
  dist = -1.0  * np.matmul(x, y.T)
  return dist
`

that is passed as the argument for the dist parameter in the align function, 

By using the negative of the matmul call, I believe the dynamic time warping to be finding the worst possible path. Experimentally I have verified that the reconstruction error is 0 when removing the negation."
"Hello,
a section of the tcc code in visualize_alignment.py has high potential for confusion and misuse. The align function is defined as follows:

```
def align(candidate_feats, query_feats, use_dtw):
  """"""Align videos based on nearest neighbor in embedding space.""""""
  if use_dtw:
    _, _, _, path = dtw(candidate_feats, query_feats, dist=dist_fn)
    _, uix = np.unique(path[0], return_index=True)
    nns = path[1][uix]
  else:
    nns = []
    for i in range(len(candidate_feats)):
      nn_frame_id, _ = get_nn(query_feats, candidate_feats[i])
      nns.append(nn_frame_id)
  return nns
```

The function call is: 
```nns.append(align(embs[query], embs[candidate], use_dtw))```

The positional arguments for the query and candidate features are reversed. Clearly, we do not want to iterate over the candidate frame matching it to the reference. There is no logical error as the arguments are passed in to the function in reverse order but it may lead to issues downstream if these functions are built upon.

The function definition should read:

```
def align(query_feats,candidate_feats, use_dtw):
  """"""Align videos based on nearest neighbor in embedding space.""""""
  if use_dtw:
    _, _, _, path = dtw(query_feats,candidate_feats, dist=dist_fn)
    _, uix = np.unique(path[0], return_index=True)
    nns = path[1][uix]
  else:
    nns = []
    for i in range(len(query_feats)):
      nn_frame_id, _ = get_nn(query_feats[i], candidate_feats)
      nns.append(nn_frame_id)
  return nns
```

"
"I'm getting this error while trying to train the model on a new dataset:

`tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __inference_<lambda>_59670}} Name: , Feature list 'frame_labels' is required but could not be found.  Did you mean to include it in feature_list_dense_missing_assumed_empty or feature_list_dense_defaults?
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext]] [Op:__inference_<lambda>_59670]
`"
"each .py files in /mol_dqn/experimental contains such codes:

`from mol_dqn.chemgraph.mcts import deep_q_networks
from mol_dqn.chemgraph.mcts import molecules as molecules_mdp
from mol_dqn.chemgraph.mcts import run_dqn`

But where is the module mol_dqn.chemgraph.mcts?"
"While checking the unprocessing, I could not make model similar to the one uploaded in google drive.
When I tried to train the model in our environment, graph does not have short noise and read noise.
I got below error when I tried to denoise the model which is created using the code committed in github.
The name 'stddev/shot_noise:0' refers to a Tensor which does not exist. The operation, 'stddev/shot_noise', does not exist in the graph. The name 'stddev/shot_noise:0' refers to a Tensor which does not exist. The operation, 'stddev/shot_noise', does not exist in the graph. 

Could you check training code shared is latest one."
""
"The problem described in [previous issue](https://github.com/google-research/google-research/issues/35) is resolved when working with tensorflow with enabled GPU support, but then there is a zoo of behaviors:

- 5 of the saved resnet models loads correctly
- Most fail with `Not found: Key resnet/group_norm/beta not found in checkpoint`
- Many fail with `Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]` 
- Few fail with `ValueError: Trying to share variable resnet/conv2d/kernel, but specified shape (3, 3, 3, 32) and found shape (3, 3, 3, 16).`

Correctly loaded:
```
resnet cifar10 resnet_wide_1.0x_batchnorm_aug_decay_0.0_1
resnet cifar10 resnet_wide_1.0x_batchnorm_aug_decay_0.0_lr_0.001_1

resnet cifar100 resnet_wide_1.0x_batchnorm_aug_decay_0.0_1
resnet cifar100 resnet_wide_1.0x_batchnorm_aug_decay_0.0_lr_0.001_1
resnet cifar100 resnet_wide_1.0x_batchnorm_aug_decay_0.0_lr_0.1_1
```

Not found:
```
resnet cifar10 resnet_wide_1.0x_batchnorm_aug_decay_0.0_2
2019-07-31 13:22:08.859328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:22:08.859377: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:22:08.859386: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:22:08.859393: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:22:08.859405: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:22:08.859413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:22:08.859420: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:22:08.859428: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:22:08.859802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:22:08.859822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:22:08.859826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:22:08.859829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:22:08.860214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
W0731 13:22:08.969132 139832240281408 deprecation.py:323] From ~/google-research/demogen/models/resnet.py:47: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).
2019-07-31 13:22:10.279506: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key resnet/group_norm/beta not found in checkpoint
Traceback (most recent call last):
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 262, in load_parameters
    saver.restore(tf_session, model_dir)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1302, in restore
    err, ""a Variable name or other graph key that is missing"")
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

2 root error(s) found.
  (0) Not found: Key resnet/group_norm/beta not found in checkpoint
         [[node save_2/RestoreV2 (defined at ~/google-research/demogen/model_config.py:261) ]]
  (1) Not found: Key resnet/group_norm/beta not found in checkpoint
         [[node save_2/RestoreV2 (defined at ~/google-research/demogen/model_config.py:261) ]]
         [[save_2/RestoreV2/_383]]
0 successful operations.
0 derived errors ignored.

Original stack trace for u'save_2/RestoreV2':
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 261, in load_parameters
    saver = tf.train.Saver(model_var_list)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 328, in _AddRestoreOps
    restore_sequentially)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 575, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1696, in restore_v2
    name=name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
```

Invalid argument:
```
resnet cifar10 resnet_wide_1.0x_groupnorm_aug_decay_0.0_1
W0731 13:25:40.192379 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/extract_layers_util.py:68: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2019-07-31 13:25:40.193601: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-31 13:25:40.530512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:25:40.530699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:25:40.531574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:25:40.532371: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:25:40.532577: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:25:40.533520: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:25:40.534268: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:25:40.536462: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:25:40.537216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:25:40.537544: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2019-07-31 13:25:40.596500: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b0d5471f80 executing computations on platform CUDA. Devices:
2019-07-31 13:25:40.596528: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-07-31 13:25:40.627506: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz
2019-07-31 13:25:40.628479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b0d3885e60 executing computations on platform Host. Devices:
2019-07-31 13:25:40.628495: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-07-31 13:25:40.628967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:25:40.629006: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:25:40.629014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:25:40.629021: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:25:40.629036: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:25:40.629043: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:25:40.629066: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:25:40.629073: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:25:40.629738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:25:40.629757: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:25:40.630519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:25:40.630526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:25:40.630529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:25:40.631262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
W0731 13:25:40.637204 140184543594304 deprecation.py:323] From ~/.local/lib64/python2.7/site-packages/tensor2tensor/data_generators/problem.py:680: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.
W0731 13:25:40.649481 140184543594304 deprecation_wrapper.py:119] From ~/.local/lib64/python2.7/site-packages/tensor2tensor/data_generators/image_utils.py:169: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

W0731 13:25:40.820377 140184543594304 deprecation.py:323] From ~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
W0731 13:25:40.825278 140184543594304 deprecation.py:323] From ~/google-research/demogen/data_util.py:76: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.
W0731 13:25:40.837275 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/extract_layers_util.py:76: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
W0731 13:25:40.838011 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/models/resnet.py:383: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0731 13:25:40.838236 140184543594304 deprecation.py:323] From ~/google-research/demogen/models/resnet.py:136: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0731 13:25:41.450165 140184543594304 deprecation.py:323] From ~/google-research/demogen/models/resnet.py:430: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
W0731 13:25:41.451108 140184543594304 deprecation.py:506] From ~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0731 13:25:42.371058 140184543594304 deprecation_wrapper.py:119] From ~/google-research/demogen/model_config.py:261: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W0731 13:25:42.421227 140184543594304 deprecation.py:323] From ~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Traceback (most recent call last):
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 262, in load_parameters
    saver.restore(tf_session, model_dir)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1322, in restore
    err, ""a mismatch between the current graph and the graph"")
tensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

2 root error(s) found.
  (0) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]
         [[node save/Assign_50 (defined at ~/google-research/demogen/model_config.py:261) ]]
         [[save/RestoreV2/_120]]
  (1) Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [1,32,1,1] rhs shape= [32]
         [[node save/Assign_50 (defined at ~/google-research/demogen/model_config.py:261) ]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/Assign_50:
 resnet/group_norm_15/beta (defined at ~/google-research/demogen/models/resnet.py:66)

Input Source operations connected to node save/Assign_50:
 resnet/group_norm_15/beta (defined at ~/google-research/demogen/models/resnet.py:66)

Original stack trace for u'save/Assign_50':
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 98, in extract_layers
    model_config.load_parameters(param_path, sess)
  File ""~/google-research/demogen/model_config.py"", line 261, in load_parameters
    saver = tf.train.Saver(model_var_list)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 825, in __init__
    self.build()
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 837, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 875, in _build
    build_restore=build_restore)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 508, in _build_internal
    restore_sequentially, reshape)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saver.py"", line 350, in _AddRestoreOps
    assign_ops.append(saveable.restore(saveable_tensors, shapes))
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/saving/saveable_object_util.py"", line 72, in restore
    self.op.get_shape().is_fully_defined())
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/state_ops.py"", line 227, in assign
    validate_shape=validate_shape)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py"", line 66, in assign
    use_locking=use_locking, name=name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
```

ValueError logs:
```
resnet cifar10 resnet_wide_1.0x_groupnorm__decay_0.002_lr_0.001_3
2019-07-31 13:19:29.317723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:19:29.317779: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:19:29.317789: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:19:29.317803: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:19:29.317811: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:19:29.317819: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:19:29.317826: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:19:29.317834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:19:29.318213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:19:29.318235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:19:29.318239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:19:29.318243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:19:29.318637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
2019-07-31 13:19:36.919128: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key resnet/batch_normalization/beta not found in checkpoint
resnet cifar10 resnet_wide_2.0x_batchnorm_aug_decay_0.0_1
2019-07-31 13:19:37.275569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6325
pciBusID: 0000:65:00.0
2019-07-31 13:19:37.275624: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-31 13:19:37.275643: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-31 13:19:37.275652: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-31 13:19:37.275659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-31 13:19:37.275667: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-31 13:19:37.275675: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-31 13:19:37.275691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-31 13:19:37.276063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-31 13:19:37.276086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-31 13:19:37.276090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2019-07-31 13:19:37.276094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2019-07-31 13:19:37.276490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:65:00.0, compute capability: 6.1)
Collecting 3072 neurons from 4 layers (5024 samples, 10 objects)
Traceback (most recent call last):
  File ""demogen/parse_tuning.py"", line 84, in <module>
    all_activations, samples_per_object, layer_names, layer_indices, layer_n_neurons = elu.extract_layers(input_fn, root_dir, model_config)
  File ""~/google-research/demogen/extract_layers_util.py"", line 89, in extract_layers
    end_points_collection=end_points_collection)
  File ""~/google-research/demogen/models/resnet.py"", line 391, in __call__
    strides=self.conv_stride, data_format=self.data_format)
  File ""~/google-research/demogen/models/resnet.py"", line 136, in conv2d_fixed_padding
    data_format=data_format)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/convolutional.py"", line 424, in conv2d
    return layer.apply(inputs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1479, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/base.py"", line 537, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 591, in __call__
    self._maybe_build(inputs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1881, in _maybe_build
    self.build(input_shapes)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 165, in build
    dtype=self.dtype)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/base.py"", line 450, in add_weight
    **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 384, in add_weight
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/training/tracking/base.py"", line 663, in _add_variable_with_custom_getter
    **kwargs_for_getter)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1496, in get_variable
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1239, in get_variable
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 562, in get_variable
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 514, in _true_getter
    aggregation=aggregation)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 869, in _get_single_variable
    (name, shape, found_var.get_shape()))
ValueError: Trying to share variable resnet/conv2d/kernel, but specified shape (3, 3, 3, 32) and found shape (3, 3, 3, 16).
```"
"Hi there, a problem occured when running the bam/run_classifier.py.

Tensorflow seems to lock the `events.out.tfevents` file until the whole program end,  when execute the command `utils.rmkdir(config.checkpoints_dir)` at run_classfier.py line 271, `tf.gfile.DeleteRecursively` appears to can't be done and raise the error given below

```
Traceback (most recent call last):
  File ""D:/bam/run_classifier.py"", line 281, in <module>
    tf.app.run()
  File ""C:\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""D:/bam/run_classifier.py"", line 276, in main
    utils.rmkdir(config.checkpoints_dir)
  File ""D:\bam\helpers\utils.py"", line 71, in rmkdir
    rmrf(path)
  File ""D:\bam\helpers\utils.py"", line 60, in rmrf
    tf.gfile.DeleteRecursively(path)
  File ""C:\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 563, in delete_recursively
    delete_recursively_v2(dirname)
  File ""C:\Python36\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 577, in delete_recursively_v2
    pywrap_tensorflow.DeleteRecursively(compat.as_bytes(path), status)
  File ""C:\Python36\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to remove a directory: bam_dir\models\debug-model\checkpoints; Directory not empty

```
The enviroment currently use : tensorflow-gpu 1.13.1 windows10.
However, when run this code on centos, the problem doesn't exist.
Really hope to give me some advice
"
"I am trying to run the demogen example.py in a conda  environment, and in either 2.7 or 3.7

get the following error:

```
File "".../google-research/demogen/data_util.py"", line 73, in input_data
    dataset = prob.dataset(mode)
  File "".../anaconda3/envs/demogen/lib/python3.7/site-packages/tensor2tensor/data_generators/problem.py"", line 631, in dataset
    assert data_dir

```
I am running TF versions

```
tensor2tensor            1.13.4   
tensorboard              1.14.0   
tensorflow               1.14.0   
tensorflow-datasets      1.0.2    
tensorflow-estimator     1.14.0   
tensorflow-metadata      0.13.0   
tensorflow-probability   0.7.0 
```

"
"Is it possibile to load a node2vec graph format like

```
node1_id_int node2_id_int <weight_float, optional>
```

Into the persona graph embedding?
Also node2vec provides several graph samples (like FB Egonet, etc.) in https://snap.stanford.edu/node2vec/, shall I first convert those graph to the NetworkX format before running in 

```
python3 -m graph_embedding.persona.persona --input_graph=${graph} \
   --output_clustering=${clustering_output}
```

Thank you."
"Hello. Thanks for sharing remarkable work.

I want to use the ""frechet_video_distance.py"" for my research.
But I can't find how to give my video as input.

May I get some help for it?
"
"I was trying to use the rouge package in a multi reference scenario? Is it possible to provide an example in the rouge/readme on how one can do that? 
In the current example in ""How to run"" where do you define the directory of the files?

Also, I am wondering how do you compute rouge for multi-reference scenario. Looking into the code you do some sort of bootstrap aggregation while in the original paper here (https://www.aclweb.org/anthology/W04-1013) in section 2 looks like they do simple micro-averaging while in section 2.1 it seems they do maximization over pairewise computation in rouge!

Thanks,
MP

"
"In the paper, they said 
""For the output, we simply average all the loose ends, i.e. the nodes that are not selected as inputs to any other nodes""

however in the code below 

```
next_s = tf.add_n(layers[1:]) / tf.cast(num_layers, dtype=tf.float32)
all_s = all_s.write(step, next_s)
```
it seems like averaging all the outputs of the nodes..
Is this the right implementation?
"
Do you think it will be good on MobileNets?
"Can you post:
1. recruiter contacts
2. if posted opening does not march what you can contribute best in contact information where you can discuss or send in a proposal or speculative application"
"Hi, what do you think about creating `setup.py` file to enable `pip install git+...`
Don't mind contributing if you find this helpful.

Thanks."
"Hi,
What's the best way to handle mismatched lines in source and target. Currently, the code fails on line 111:

    # Check whether num_targets < num_predictions
    if next(pred_gen, None) is not None or next(pred_gen, None) is not None:
      raise ValueError(""Must have equal number of lines across target and ""
                       ""prediction files. Mismatch between files: %s, %s."" %
                       (target_filename, prediction_filename))

My predictions files have more sentences than the target and the pyrouge package which is built on the PERL rouge handles this without breaking. Should I be pre-processing my target files to have same no: of lines as the prediction? 

Can this be handled the rouge package level instead similar to the perl one?
Commenting out this check leads to 10 points lower in all metrics as compares to the pyrouge outputs. 

Let me know if any additional code/output files are required to better understand this issue.
"
"[Line no 226, graph_attention_learning.py, Watch Your Step] return tf.transpose(d_sum) * **GetNumNodes()** * 80, feed_dict
Why is an arbitrary scaling by the number of nodes done? I am not sure if it is reported in the Watch Your Step paper.
When I remove the scaling, there is some decrease in the results for most of the datasets
Results and Ablation Study:
PPI(without scaling,learn attention) - 90.84
PPI(with scaling,learn attention) - 91.8
PPI(uniform attention,scaling) - 91.7

ca-HepTh(without scaling,learn attention) - 93.14
ca-HepTh(with scaling,learn attention) - 93.8
ca-HepTh(uniform attention,scaling) - 93.9

Wiki-Vote(without scaling,learn attention) - 94.3
Wiki-Vote(with scaling,learn attention) - 93.7
Wiki-Vote(uniform attention,scaling) - 94.0

Configs:
Embedding dimension:128
Share Embeddings: False
Transition_powers: 5
Loss: nlgl
context_regularizer: 0.1
learnable attention - softmax over 5 hops
uniform attention - Equal attention of earch of 5 hops(0.2 for each hop)

A couple of more questions:
1) Why is a validation set(validation positive edges, negative edges) not chosen for stopping? I know that Learning Edge Representations via Low-Rank Asymmetric Projections, other baselines and many of the graph embedding literature doesn't use it for link prediction, but I feel that stopping based on validation from ROC-AUC scores is more appropriate than stopping by best train ROC-AUC.

2.a) I see that attention makes only little contribution to an increase in the performance. Uniform attention works well. For example, in PPI, the paper reports that the attention learned favors the first hop. But not learning any attention(or in other words, uniform attention) also performs equally well. This is true even for Soc-Facebook, Wiki-Vote, ca-HepTh, ca-AstroPh. 

2.b)
Why is the stopping criteria in line 441
""if i - 100 > eval_metrics['i at best train']:
      LogMsg('Reached peak a while ago. Terminating...')
      break""

based on training error? Shouldn't stopping criteria be always based on validation error?

2.c) in line 340 ""eval_metrics['test auc at best train'] = float(test_auc)""

Why log and report test AUC at best train? We always report metrics and save model that gives the best performance on the validation set. If we modify the code to report the AUC / precision scores at least validation error then there is little to no difference between having weights and trainable attention.

"
It's amazing and so magical! Where is the source paper and code?
"$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./qanet/squad_helper.py:87:1: F822 undefined name 'embed_translation' in __all__
__all__ = ['preprocess_inputs', 'build_a_layer',
^
```"
"$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./qanet/squad_helper.py:166:22: F821 undefined name 'transfer'
    encoder_states = transfer.elmo_tensor_from_chars(
                     ^
./qanet/squad_helper.py:181:22: F821 undefined name 'transfer'
    encoder_states = transfer.elmo_tensor_from_sentences(
                     ^
```

Should these be [__embed_elmo_chars()__](https://github.com/google-research/google-research/blob/master/qanet/squad_helper.py#L162) and [__embed_elmo_sentences()__](https://github.com/google-research/google-research/blob/master/qanet/squad_helper.py#L177)?"
https://github.com/google-research/google-research/search?q=NltkAndPunctTokenizer&unscoped_q=NltkAndPunctTokenizer
"Hi @vanzytay,

Is there any information on how the prepend_prompt function is defined in the UL2 paper?

Thank you,

Enrico

```python
dataset = tfds.load('wikipedia/20220620.en', split='train', shuffle_files=True)

# def prepend_prompt(ds, output_features, prompt_mode, mode):

    
def ul2_objective(dataset: tf.data.Dataset,
    sequence_length: seqio.preprocessors.SequenceLengthType, 
    output_features: seqio.preprocessors.OutputFeaturesType, 
    use_prefix_lm_task: bool = False,
    rates: Optional[Sequence[float]] = None,
    mean_noise_span_lengths: Sequence[float] = (3.0,),
    noise_densities: Sequence[float] = (0.15,), 
    shard_ds: bool = True, 
    optional_task_prefixes: Optional[Sequence[str]] = None, 
    input_feature_key: str = ""inputs"", 
    merge_examples_to_reduce_padding: bool = True, 
    reserved_for_packing: bool = None, 
    seed: int = 7) -> tf.data.Dataset:
    
    """"""
    UL2-like pre-training objectives. This preprocessor amounts to calling the ‘span_corruption‘ function several times with different values of ‘noise_density‘ and ‘mean_noise_span_length‘. 
    We either shard or copy the dataset, then apply each function to each shard. Add S-denoising (prefixLM) using use_prefix_lm_task. 
    
    Args: 
    dataset: A tf.data.Dataset with dictionaries containing the key ‘input_feature_key‘. 
    sequence_length: dict mapping of feature key to int length for that feature. 
    output_features: mapping of keys to features. use_prefix_lm_task: <bool> If True, include PrefixLM in the task mix. 
    rates: <Optional<List<float>> List of rates per task. If None, tasks are sampled uniformly. 
    mean_noise_span_lengths: List of mean number of tokens per masked span per example. 
    noise_densities: List of what fraction of the tokens to mask. 
    shard_ds: <bool> If True, shard dataset per objective. 
    optional_task_prefixes: <Optional<list<str>> Strings to prepend for each orruption scheme. 
    NOTE: If including prefixLM task, it must be the last prefix. 
    input_feature_key: which feature to use from the dataset as the input text tokens. 
    merge_examples_to_reduce_padding: if True, combines multiple input examples to reduce padding. reserved_for_packing: if specified, reduces the desired inputs length by the specified amount to enable multiple examples to be packed together downstream. 
    seed: tf.int64 for controlling the random choice of spans. Returns: a dataset 
    """"""

    if optional_task_prefixes: # Ensure each task has a prefix. 
        num_tasks = len(noise_densities) + int(use_prefix_lm_task) 
        valid_number_of_prefixes = num_tasks == len(optional_task_prefixes) 
        if not valid_number_of_prefixes: 
            raise ValueError(""Number of task prefixes must match number of tasks."") 
    inputs_length = sequence_length[input_feature_key] 
    input_lengths, targets_lengths = [], [] 
    sequence_lengths = {x: y for x, y in sequence_length.items()} 
    if reserved_for_packing: 
        inputs_length -= reserved_for_packing 
        for x, y in sequence_length.items(): 
            sequence_lengths[x] = y - reserved_for_packing 
    hyperparams = list(zip(mean_noise_span_lengths, noise_densities)) 
    for mean_noise_span_length, noise_density in hyperparams: 
        input_length, targets_length = t5.data.preprocessors.random_spans_helper(
            extra_tokens_per_span_inputs=1, 
            extra_tokens_per_span_targets=1, 
            inputs_length=inputs_length, 
            mean_noise_span_length=mean_noise_span_length, 
            noise_density=noise_density) 
        input_lengths.append(input_length) 
        targets_lengths.append(targets_length)

        if sequence_length[""targets""] < targets_length: 
            upper_bound = max(targets_lengths) 
            raise ValueError(f""Targets length {sequence_length['targets']} is too small for the given noise_density and mean_noise_span_length. Please increase the targets length to at least {upper_bound}."")
            #raise ValueError(""f’Expected max targets length for span corruption ({upper_bound}) is ’ f’greater than configured targets length ’ f""({sequence_length[’targets’]})"")

    ds = dataset 
    ds = t5.data.preprocessors.select_random_chunk(
        ds, 
        output_features=output_features, 
        feature_key=""targets"", 
        max_length=65536) 
    if merge_examples_to_reduce_padding: 
        ds = t5.data.preprocessors.reduce_concat_tokens(
            ds, 
            feature_key=""targets"", 
            batch_size=128) 
    num_shards = len(input_lengths) + int(use_prefix_lm_task) 
    if shard_ds: 
        ds_shards = [ds.shard(num_shards, i) for i in range(num_shards)] 
    else: 
         ds_shards = [ds for _ in range(num_shards)] 
    processed_ds = [] 
    hyperparams = zip(input_lengths, hyperparams, range(num_shards)) 
    for input_length, (noise_span_length, noise_density), i in hyperparams: 
        ds = ds_shards[i] 
        ds = t5.data.preprocessors.split_tokens(
            ds,
            feature_key=""targets"", 
            min_tokens_per_segment=None, 
            max_tokens_per_segment=input_length) 
        ds = t5.data.preprocessors.denoise(
            ds, 
            output_features, 
            inputs_fn=t5.data.preprocessors.noise_span_to_unique_sentinel, 
            targets_fn=t5.data.preprocessors.nonnoise_span_to_unique_sentinel, 
            noise_density=noise_density, 
            noise_mask_fn=functools.partial(
                t5.data.preprocessors.random_spans_noise_mask, 
                mean_noise_span_length=noise_span_length), 
                input_feature_key=input_feature_key) 
        if optional_task_prefixes: 
            ds = prepend_prompt(
                ds, 
                output_features, 
                prompt_mode=optional_task_prefixes[i], 
                mode=optional_task_prefixes[i]) 
        processed_ds.append(ds) 
    if use_prefix_lm_task: 
        ds = ds_shards[-1] 
        ds = t5.data.preprocessors.prefix_lm(ds, sequence_lengths, output_features) 
        if optional_task_prefixes: 
            ds = prepend_prompt(
                ds, 
                output_features, 
                prompt_mode=optional_task_prefixes[-1], 
                mode=optional_task_prefixes[-1]) 
        processed_ds.append(ds) 
    ds = tf.data.experimental.sample_from_datasets(processed_ds, rates, seed) 
    return ds

sequence_length = {
    ""inputs"": 512,
    ""targets"": 512,
}

output_features = {
    ""inputs"":
        seqio.Feature(
            vocabulary=t5.data.get_default_vocabulary(), add_eos=False),
    ""targets"":
        seqio.Feature(
            vocabulary=t5.data.get_default_vocabulary(), add_eos=False)
}

ul2_data = ul2_objective(
    dataset,
    sequence_length, 
    output_features, 
    use_prefix_lm_task=False,
    rates=None,
    mean_noise_span_lengths=(3.0,),
    noise_densities=(0.15,), 
    shard_ds=True, 
    optional_task_prefixes=None, 
    input_feature_key=""text"", 
    merge_examples_to_reduce_padding=True, 
    reserved_for_packing=None, 
    seed=7)
```"
"Once Pr-VIPE inference has been run to produce a 16 or 32 dimension embedding, how can the embedding get translated into a particular 3D key point topology?"
"I have to patch the code to resolve some warnings and errors using `tensorflow==2.9.2`. 

```shell
diff --git a/felix/felix_models.py b/felix/felix_models.py
index 71412d51..cea1d2b2 100644
--- a/felix/felix_models.py
+++ b/felix/felix_models.py
@@ -18,7 +18,9 @@ from typing import Union
 
 import numpy as np
 from official.modeling import activations
-from official.nlp.legacy import configs
+# from official.nlp.legacy import configs
+from official.legacy.bert import configs
+# from official.nlp import configs
 from official.nlp.modeling import losses
 from official.nlp.modeling import models
 from official.nlp.modeling import networks
@@ -90,7 +92,8 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
     return mask_label_loss

   def get_config(self):
-    return self._config
+    config = super().get_config()
+    return config


 def get_insertion_model(bert_config,
@@ -129,7 +132,7 @@ def get_insertion_model(bert_config,
       activation=activations.gelu,
       dropout_rate=bert_config.hidden_dropout_prob,
       attention_dropout_rate=bert_config.attention_probs_dropout_prob,
-      sequence_length=seq_length,
+      # sequence_length=seq_length,
       max_sequence_length=bert_config.max_position_embeddings,
       type_vocab_size=bert_config.type_vocab_size,
       initializer=tf.keras.initializers.TruncatedNormal(
@@ -301,8 +304,8 @@ class FelixTagLoss(tf.keras.layers.Layer):
     return total_loss

   def get_config(self):
-    return self._config
-
+    config = super().get_config()
+    return config

 def get_tagging_model(bert_config,
                       seq_length,
@@ -339,7 +342,7 @@ def get_tagging_model(bert_config,
       activation=activations.gelu,
       dropout_rate=bert_config.hidden_dropout_prob,
       attention_dropout_rate=bert_config.attention_probs_dropout_prob,
-      sequence_length=seq_length,
+      # sequence_length=seq_length,
       max_sequence_length=bert_config.max_position_embeddings,
       type_vocab_size=bert_config.type_vocab_size,
       initializer=tf.keras.initializers.TruncatedNormal(
diff --git a/felix/felix_tagger.py b/felix/felix_tagger.py
index a3a912f1..54b5b092 100644
--- a/felix/felix_tagger.py
+++ b/felix/felix_tagger.py
@@ -86,7 +86,7 @@ class FelixTagger(tf.keras.Model):
           inner_activation=activations.gelu,
           output_dropout=self._bert_config.hidden_dropout_prob,
           attention_dropout=self._bert_config.hidden_dropout_prob,
-          output_range=seq_length,
+          # output_range=seq_length,
       )

     self._query_embeddings_layer = tf.keras.layers.Dense(
@@ -140,6 +140,7 @@ class FelixTagger(tf.keras.Model):
       The logits of the edit tags and optionally the logits of the pointer
         network.
     """"""
+    tf.print(inputs)
     if self._is_training:
       input_word_ids, input_mask, input_type_ids, edit_tags = inputs
     else:

diff --git a/felix/run_felix.py b/felix/run_felix.py
index 95569938..f3488905 100644
--- a/felix/run_felix.py
+++ b/felix/run_felix.py
@@ -126,10 +126,13 @@ def run_train(bert_config,
       num_train_steps=steps_per_mini_epoch * mini_epochs_per_epoch * epochs,
       num_warmup_steps=warmup_steps)

+  tf.config.run_functions_eagerly(True)
+  tf.data.experimental.enable_debug_mode()
+
   pretrain_model.compile(
       optimizer=optimizer,
       loss=loss_fn,
-      experimental_steps_per_execution=FLAGS.steps_per_loop)
+      steps_per_execution=FLAGS.steps_per_loop)
   train_dataset = _get_input_data_fn(
       train_file,
       seq_length,
@@ -183,7 +186,6 @@ def run_train(bert_config,
   logging.info('Starting training from iteration: %s.', checkpoint_iteration)
   summary_dir = os.path.join(model_dir, 'summaries')
   summary_cb = tf.keras.callbacks.TensorBoard(summary_dir, update_freq=1000)
-
   manager = tf.train.CheckpointManager(
       checkpoint, directory=model_dir, max_to_keep=FLAGS.keep_checkpoint_max,
       checkpoint_name=_CHECKPOINT_FILE_NAME)
```

However, I still get the error below, is there any hint on how to solve this?

```
I1009 17:18:30.512812 140272607400896 optimization.py:90] using Adamw optimizer
I1009 17:18:30.512971 140272607400896 legacy_adamw.py:56] AdamWeightDecay gradient_clip_norm=1.000000
I1009 17:18:30.710452 140272607400896 run_felix.py:169] Initializing from a BERT checkpoint...
I1009 17:18:30.976909 140272607400896 run_felix.py:186] Starting training from iteration: 0.
Epoch 1/500
Traceback (most recent call last):
  File ""/home/vimos/Data/Text/chn_nlp/TextEditing/felix/run_felix.py"", line 239, in <module>
    app.run(main)
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/home/vimos/Data/Text/chn_nlp/TextEditing/felix/run_felix.py"", line 228, in main
    return run_train(bert_config, FLAGS.max_seq_length,
  File ""/home/vimos/Data/Text/chn_nlp/TextEditing/felix/run_felix.py"", line 196, in run_train
    pretrain_model.fit(
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/vimos/anaconda3/lib/python3.9/site-packages/keras/engine/training.py"", line 1576, in fit
    raise ValueError(
ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
```"
"Hi, @agarwl. I'm been using the repo for a while, and I recently found out that I failed to fix seeds. When running the code with `--seed xx`, I can see different versions of results. I tried to install `tensorflow-determinism`, but the library is not working as expected.

I'm using TF2.5 with CUDA10.1.

Any idea? 😕 "
"I tried to sample images using the code and checkpoint from [diffusion_distillation](https://github.com/google-research/google-research/blob/3b61da1d73543c374fc0564943e237db151af452/diffusion_distillation/README.md) to reproduce their results in this paper [PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS](https://arxiv.org/abs/2202.00512). However, the FID I got is much worse than the reported numbers. For example, sampling from cifar_original checkpoint with 1024 DDIM steps only got a FID of 18. I also tried the distilled model (cifar_8) that gave me a FID of 19.8. Anyone knows how to reproduce paper's results? 

The code I used to sample is from the official [notebook demo](https://github.com/google-research/google-research/blob/3b61da1d73543c374fc0564943e237db151af452/diffusion_distillation/diffusion_distillation.ipynb). I use cleanfid to compute FID score. See my code below. 
```python
import jax
import flax
import numpy as onp
import diffusion_distillation
from PIL import Image
loaded_params = diffusion_distillation.checkpoints.restore_from_path('ckpts/cifar_8', target=None)['ema_params']

config = diffusion_distillation.config.cifar_distill.get_config()
model = diffusion_distillation.model.Model(config)
ema_params = jax.device_get(model.make_init_state()).ema_params
loaded_params = flax.core.unfreeze(loaded_params)
loaded_params = jax.tree_map(
    lambda x, y: onp.reshape(x, y.shape) if hasattr(y, 'shape') else x,
    loaded_params,
    flax.core.unfreeze(ema_params))
loaded_params = flax.core.freeze(loaded_params)
del ema_params

batch = {'image': jax.random.normal(jax.random.PRNGKey(10), shape=(250, 32, 32, 3))}
outdir = 'exp/cifar_8'
os.makedirs(outdir, exist_ok=True)

for i in range(200):
    samples = model.samples_fn(rng=jax.random.PRNGKey(i), params=loaded_params, batch=batch, num_samples=250, num_steps=8)
    samples = jax.device_get(samples).astype(onp.uint8)
    num_imgs = samples.shape[0]
    for j in range(num_imgs):
        im = Image.fromarray(samples[j])
        img_path = os.path.join(outdir, f'{i}-{j}.jpg')
        im.save(img_path)

from cleanfid import fid

score = fid.compute_fid(outdir, dataset_name='cifar10', dataset_res=32, dataset_split='train', mode='legacy_tensorflow')
print(score)
```"
Scann depencies currently set to tensorflow 2.9.
"Hello,

I have been running some teaser prompts from the Dream Fields site: https://ajayj.com/dreamfields. However, the results seems to be much worse compared to the figures in the paper. I am using the config/config_mq.py default settings. Is this because of the lower resolution (168*168) used in training or are the figures in the paper generated using the LiT-B/32 model instead of ViT-B/16?

I attached results I got for ""a sculpture of a rooster."", ""a robotic dog. a robot in the shape of a dog."" and ""matte painting of a bonsai tree; trending on artstation."".

Thanks

https://user-images.githubusercontent.com/18273944/185828051-fa7c41f4-e4d3-45a0-b2fe-a184f82e936a.mp4


https://user-images.githubusercontent.com/18273944/185828067-27a10466-9c14-4444-b1ba-be6959bbf7a6.mp4

https://user-images.githubusercontent.com/18273944/185828078-995f4b65-ec3c-4118-b8aa-d9ced958891c.mp4


!"
Disregard this was a problem with conversion to pytorch
""
"Hi @agarwl ,
I have a question on this paper [Contrastive behavioral similarity embeddings for generalization in reinforcement learning](https://agarwl.github.io/pse/). When you select the negative and positive pairs by the defined PSM and use contrasive loss function. How do you ensure the positive pair is valid? For example, as you sample pairs randomly, it is likely that all sampled pairs are not similar, but you have to select the most similar one among them as positive pair.

Best,
Jiawei"
👍
"I found an error when I ran the evaluate_model in Hypertransformer.
![图片](https://user-images.githubusercontent.com/87852654/182739950-810cc868-3655-41fd-a5be-123b88131c2c.png)
Can you tell me a more detailed configuration version?
Can you give some suggestions?
Thanks a lot!"
"Hello,
when I try to run infer.py with L2-VIPE checkpoint I got couples of exceptions and they all related to something called "" NOT_FOUND: Key SimpleModel/OutputLogits/C0/embedding_stddevs/bias not found in checkpoint"".  Anyone has any idea to fix that? Thank you.


full stack trace:

WARNING:tensorflow:From /Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
/Users/labptop/Desktop/Models/ViewInvariantModel/poem/core/models.py:85: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).
  net = tf.layers.batch_normalization(
INFO:tensorflow:Graph was finalized.
I0728 11:22:16.444565 4388341248 monitored_session.py:243] Graph was finalized.
2022-07-28 11:22:16.444939: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:tensorflow:Restoring parameters from ./poem/tmp/checkpoint2/model.ckpt-01104395
I0728 11:22:16.446062 4388341248 saver.py:1412] Restoring parameters from ./poem/tmp/checkpoint2/model.ckpt-01104395
2022-07-28 11:22:16.450572: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2022-07-28 11:22:16.479388: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:228 : NOT_FOUND: Key SimpleModel/OutputLogits/C0/embedding_stddevs/bias not found in checkpoint
Traceback (most recent call last):
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1377, in _do_call
    return fn(*args)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1360, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1453, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.NotFoundError: Key SimpleModel/OutputLogits/C0/embedding_stddevs/bias not found in checkpoint
         [[{{node save/RestoreV2}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 1417, in restore
    sess.run(self.saver_def.restore_op_name,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1396, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.NotFoundError: Graph execution error:

Detected at node 'save/RestoreV2' defined at (most recent call last):
    File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 283, in <module>
      app.run(main)
    File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 308, in run
      _run_main(main, args)
    File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 254, in _run_main
      sys.exit(main(argv))
    File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 253, in main
      saver = tf.train.Saver()
Node: 'save/RestoreV2'
Key SimpleModel/OutputLogits/C0/embedding_stddevs/bias not found in checkpoint
         [[{{node save/RestoreV2}}]]

Original stack trace for 'save/RestoreV2':
  File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 283, in <module>
    app.run(main)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 253, in main
    saver = tf.train.Saver()
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 933, in __init__
    self.build()
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 945, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 973, in _build
    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 543, in _build_internal
    restore_op = self._AddRestoreOps(filename_tensor, saveables,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 363, in _AddRestoreOps
    all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 611, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1501, in restore_v2
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py"", line 797, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 3754, in _create_op_internal
    ret = Operation(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 2133, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 66, in get_tensor
    return CheckpointReader.CheckpointReader_GetTensor(
RuntimeError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 1428, in restore
    names_to_keys = object_graph_key_mapping(save_path)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 1749, in object_graph_key_mapping
    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 71, in get_tensor
    error_translator(e)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 31, in error_translator
    raise errors_impl.NotFoundError(None, None, error_message)
tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 283, in <module>
    app.run(main)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 262, in main
    with tf.train.MonitoredSession(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 1054, in __init__
    super(MonitoredSession, self).__init__(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 757, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 1263, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 1268, in _create_session
    return self._sess_creator.create_session()
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 910, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session
    return self._get_session_manager().prepare_session(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/session_manager.py"", line 310, in prepare_session
    sess, is_loaded_from_checkpoint = self._restore_checkpoint(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/session_manager.py"", line 229, in _restore_checkpoint
    _restore_checkpoint_and_maybe_run_saved_model_initializers(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/session_manager.py"", line 67, in _restore_checkpoint_and_maybe_run_saved_model_initializers
    saver.restore(sess, path)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 1433, in restore
    raise _wrap_restore_error_with_msg(
tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Graph execution error:

Detected at node 'save/RestoreV2' defined at (most recent call last):
    File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 283, in <module>
      app.run(main)
    File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 308, in run
      _run_main(main, args)
    File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 254, in _run_main
      sys.exit(main(argv))
    File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 253, in main
      saver = tf.train.Saver()
Node: 'save/RestoreV2'
Key SimpleModel/OutputLogits/C0/embedding_stddevs/bias not found in checkpoint
         [[{{node save/RestoreV2}}]]

Original stack trace for 'save/RestoreV2':
  File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 283, in <module>
    app.run(main)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/Users/labptop/Desktop/Models/ViewInvariantModel/infer.py"", line 253, in main
    saver = tf.train.Saver()
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 933, in __init__
    self.build()
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 945, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 973, in _build
    self.saver_def = self._builder._build_internal(  # pylint: disable=protected-access
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 543, in _build_internal
    restore_op = self._AddRestoreOps(filename_tensor, saveables,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 363, in _AddRestoreOps
    all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/training/saver.py"", line 611, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1501, in restore_v2
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py"", line 797, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 3754, in _create_op_internal
    ret = Operation(
  File ""/Users/labptop/opt/anaconda3/envs/model1/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 2133, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)


"
""
"@peterjliu Hi, I see the rouge has been updated many times in the [repository](https://github.com/google-research/google-research/tree/master/rouge). So may you upgrade it on pypi for researchers' convenient use.

Thanks!"
"@agesmundo, hope to receive some reply, thanks.
- Duplicate issues: [#788](https://github.com/google/jax/issues/788) and [#4528](https://github.com/google/jax/issues/4528) are not suitable for this case.
-  How to reproduce the bug:
[1] Just run [mu2Net](https://github.com/google-research/google-research/blob/master/muNet/mu2Net.ipynb) on 8gpus A100, use BENCHMARK = 'ViT large / Chars benchmark'
[2] OOM error will occur when train_step function compiled by jax.jit is executed.
[3] The A100 have sufficent 80GiB memory per gpu, i use 8gpus. My cpu has 256g memory and 112+ cores.
[4] I can't understand why the executable needs to preallocate 114.44GiB temp allocation, though the seed ViT model is just 300M.
[5]It's useless to set any env variable about [jax memory allocation](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html  )

- Model hyperparameters:
```python
def get_exp_config_large(benchmark_string_id):
  exp_config = ConfigDict()
  exp_config.experiment_name = EXPERIMENT_NAME
  exp_config.experiments_root_dir = EXPERIMENTS_ROOT_DIR
  # Cap to 1/10th of imagenet train set size to have similar ratio of exps reported in:
  # https://arxiv.org/abs/2106.10270
  exp_config.num_train_examples_between_validations_max = 128_116
  exp_config.num_validations_per_path_training = 4
  exp_config.num_validation_examples_max = 10_000
  # Fit HBM memory: TPUv4 megacore=64, TPUv3=32.
  exp_config.batch_size = 64
  exp_config.num_task_iters = 1
  # Assuming TPUv4 32 cores * 4 generations.
  exp_config.num_samples_per_task = 32 * 4
  exp_config.mutate_adapters = False
  exp_config.force_finetune_components = ['encoder_norm']
  # Population policy params:
  exp_config.policy_class = 'PPDecay'
  exp_config.policy_kwargs = {}
  # Scorer params:
  exp_config.scorer_class = 'ScorerDecay'
  exp_config.scorer_kwargs = dict(
      base=1.0,
      num_params=303_303_682,  # Params in L/16
      )
  # Seed models params:
  exp_config.load_rand_init = False
  exp_config.load_vit_checkpoint = True
  exp_config.load_vit_checkpoint_query = 'name==""L/16"" and ds==""i21k"" and aug==""medium2"" and wd==0.03 and sd==0.1'
  exp_config.load_experiment = False
  exp_config.load_experiment_dir = ''
  set_continue_configs(exp_config)

  # Hyperparameters:
  max_num_layers = get_max_num_layers(exp_config.load_vit_checkpoint_query)
  exp_config.models_default_hparams = {
      '_mu_': 0.2,
      'num_classes': 1,
      'adapter_layers': '',
      'num_layers': max_num_layers,
      'adapter_dim': 16,
      'opt_lr': 0.01,
      'opt_lr_schedule': 'cosine',
      'opt_lr_warmup_ratio': 0.05,
      'opt_momentum': 0.9,
      'opt_nesterov': False,
      'ds_image_size': 384,
      'ds_crop': True,
      'ds_area_range_min': 0.05,
      'ds_aspect_ratio_range_min': 0.75,
      'ds_flip_left_right': True,
      'ds_brightness_delta': 0.0,
      'ds_contrast_delta': 0.0,
      'ds_saturation_delta': 0.0,
      'ds_hue_delta': 0.0,
  }
```

- Core code:
```python
@partial(jax.jit, static_argnames=['model', 'optimizer'], donate_argnums=[0, 2])
def train_step(params, fixed_params, opt_state, images, labels, model, optimizer):
  def loss_fn(params, fixed_params, images, labels):
    logits = model.apply({'params': format_params(params, fixed_params)},
                         images, train=USE_DROPOUT)
    labels = jax.nn.one_hot(labels, logits.shape[-1])
    return -jnp.mean(jnp.sum(labels * nn.log_softmax(logits), axis=-1))
  grads = jax.grad(loss_fn)(params, fixed_params, images, labels)
  updates, opt_state = optimizer.update(grads, opt_state, params=params)
  params = optax.apply_updates(params, updates)
  return params, opt_state

def train_loop(paths, ds_train, ds_validation, devices, exp_config):
  global LOOP_START
  timing = {'start_time': time.time(),
            'start_time_loop': LOOP_START}
  task = paths[0].task
  # The following values should be shared by all paths in this generation batch.
  for path in paths:
    assert task == path.task
    assert paths[0].hparams['ds_image_size'] == path.hparams['ds_image_size']

  gc.collect()

  # Compile.
  compile_train_batches_arr = jax.device_put_replicated(
      get_sample_batch(
        paths[0].hparams['ds_image_size'],
        task.train_batch_size),
      devices)
  compile_eval_batches_arr = jax.device_put_replicated(
      get_sample_batch(
          paths[0].hparams['ds_image_size'],
          task.validation_batch_size),
      devices)

  for p_id, path in enumerate(paths):
    if VERBOSE:
      print('Parent')
      print(prp(path.parent))
      print(prp(path))
    path.device_id = p_id % len(devices)
    path.device = devices[path.device_id]
    print(""path:"", p_id, ""device:"", path.device)
    path.optimizer = path.get_optimizer()
    path.optimizer_init_fn = jax.jit(path.optimizer.init, device=path.device)
    path.best_params_local = None
    path.best_opt_state_local = None
    path.best_quality = None
    path.best_score = path.parent.score() if path.task is path.parent.task else -np.inf
    path.evals = []

    # Launch parallel compilation of eval and train step functions.
    params_local = path.get_trainable_params()
    check_is_local(params_local)
    path.compile_params_device = jax.device_put(params_local, path.device)
    path.compile_fixed_params_device = jax.device_put(
        path.get_fixed_params(),
        path.device)
    path.compile_train = Thread(
        target=train_step,
        args=(path.compile_params_device,
              path.compile_fixed_params_device,
              path.optimizer_init_fn(params_local),
              compile_train_batches_arr['image'][path.device_id],
              compile_train_batches_arr['label'][path.device_id],
              path.model,
              path.optimizer))
    path.compile_eval = Thread(
        target=eval_step,
        args=(format_params(
                  path.compile_params_device,
                  path.compile_fixed_params_device),
              compile_eval_batches_arr['image'][path.device_id],
              compile_eval_batches_arr['label'][path.device_id],
              path.model))
    path.compile_eval.start()

  for path in paths:
    path.compile_eval.join()
    del path.compile_eval
    timing['end_compile_eval'] = time.time()
    path.compile_train.start()
  del compile_eval_batches_arr

  for path in paths:
    path.compile_train.join()
    del path.compile_train
    del path.compile_params_device
    del path.compile_fixed_params_device
    timing['end_compile'] = time.time()
  del compile_train_batches_arr

  gc.collect()

  # Parameter transfer.
  for path in paths:
    path.params_device = jax.device_put(
        path.get_trainable_params(),
        path.device)
    path.fixed_params_device = jax.device_put(
        path.get_fixed_params(),
        path.device)
    path.opt_state_device = path.optimizer_init_fn(path.params_device)
    # Set opt state.
    for c in path.components:
      if c.is_trainable():
        assert c.name in path.opt_state_device[1][0].trace.keys()
        if c.opt_state is not None:
          path.opt_state_device = (
              path.opt_state_device[0],
              (optax.TraceState(
                  trace=path.opt_state_device[1][0].trace.copy(
                      {c.name: jax.device_put(c.opt_state,
                                              path.device)})),
               path.opt_state_device[1][1]
               )
          )
    check_is_on_device(path.opt_state_device, path.device)

  iter_ds_validation = iter(ds_validation)
  # TRAIN
  for t_step, train_batch in zip(
      range(exp_config.num_validations_per_path_training
            * task.num_train_batches_between_validations),
      ds_train,
  ):
    train_batch_arr = jax.device_put_replicated(train_batch, devices)
    for p_id, path in enumerate(paths):
      if t_step == 0:
        timing['end_prep'] = time.time()
        t_step_0_time = time.time()
      train_step_start = time.time()
      path.params_device, path.opt_state_device = train_step(
          path.params_device,
          path.fixed_params_device,
          path.opt_state_device,
          train_batch_arr['image'][path.device_id],
          train_batch_arr['label'][path.device_id],
          path.model,
          path.optimizer)
      if t_step == 0 and time.time() - t_step_0_time > 1:
        print(f'WARNING: First train step took: {time.time()-t_step_0_time:.2f} s')
    del train_batch, train_batch_arr

    # EVAL
    # ...
```

- Full error messages/tracebacks:
```bash
Exception in thread Thread-14:
Traceback (most recent call last):
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 890, in _bootstrap
    self._bootstrap_inner()
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/api.py"", line 476, in cache_miss
    donated_invars=donated_invars, inline=inline, keep_unused=keep_unused)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/core.py"", line 1765, in bind
    return call_bind(self, fun, *args, **params)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/core.py"", line 1781, in call_bind
    outs = top_trace.process_call(primitive, fun_, tracers, params)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/core.py"", line 678, in process_call
    return primitive.impl(f, *tracers, **params)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/dispatch.py"", line 185, in _xla_call_impl
    return compiled_fun(*args)
  File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/dispatch.py"", line 615, in _execute_compiled
    out_bufs_flat = compiled.execute(input_bufs_flat)
jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 122875791936 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    1.43GiB
              constant allocation:         8B
        maybe_live_out allocation:  581.19MiB
     preallocated temp allocation:  114.44GiB
  preallocated temp fragmentation:  146.50MiB (0.13%)
                 total allocation:  115.86GiB
              total fragmentation:  146.53MiB (0.12%)
Peak buffers:
        Buffer 1:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

        Buffer 2:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

        Buffer 3:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

        Buffer 4:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

        Buffer 5:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

        Buffer 6:
                Size: 1.27GiB
                XLA Label: custom-call
                Shape: f32[64,16,577,577]
                ==========================

```

- environment:
python: 3.7.13
jax: 0.3.14/0.3.13
jaxlib: 0.3.14+cuda11.cudnn82/0.3.10+cuda11.cudnn82

- python packages:
absl-py                      1.1.0
aqtp                         0.0.7
astunparse                   1.6.3
cachetools                   5.2.0
certifi                      2022.6.15
charset-normalizer           2.0.12
chex                         0.1.3
cloudpickle                  2.1.0
clu                          0.0.3
colorama                     0.4.5
commonmark                   0.9.1
contextlib2                  21.6.0
cycler                       0.11.0
dacite                       1.6.0
decorator                    5.1.1
dill                         0.3.5.1
dm-tree                      0.1.7
einops                       0.3.0
etils                        0.6.0
flatbuffers                  1.12
flax                         0.5.2
flaxformer                   0.4.2
fonttools                    4.33.3
gast                         0.4.0
google-auth                  2.8.0
google-auth-oauthlib         0.4.6
google-pasta                 0.2.0
googleapis-common-protos     1.56.3
grpcio                       1.47.0
h5py                         3.7.0
idna                         3.3
importlib-metadata           4.12.0
importlib-resources          5.8.0
jax                          0.3.14
jaxlib                       0.3.14+cuda11.cudnn82
keras                        2.9.0
Keras-Preprocessing          1.1.2
kiwisolver                   1.4.3
libclang                     14.0.1
Markdown                     3.3.7
matplotlib                   3.5.2
ml-collections               0.1.1
msgpack                      1.0.4
numpy                        1.21.6
oauthlib                     3.2.0
opt-einsum                   3.3.0
optax                        0.1.2
packaging                    21.3
pandas                       1.3.5
Pillow                       9.1.1
pip                          21.2.2
promise                      2.3
protobuf                     3.19.4
pyasn1                       0.4.8
pyasn1-modules               0.2.8
Pygments                     2.12.0
pyparsing                    3.0.9
python-dateutil              2.8.2
pytz                         2022.1
PyYAML                       6.0
requests                     2.28.0
requests-oauthlib            1.3.1
rich                         11.2.0
rsa                          4.8
scipy                        1.7.3
setuptools                   61.2.0
six                          1.16.0
tensorboard                  2.9.1
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.1
tensorflow-addons            0.17.1
tensorflow-cpu               2.9.1
tensorflow-datasets          4.6.0
tensorflow-estimator         2.9.0
tensorflow-hub               0.12.0
tensorflow-io-gcs-filesystem 0.26.0
tensorflow-metadata          1.9.0
tensorflow-probability       0.17.0
tensorflow-text              2.9.0
termcolor                    1.1.0
toml                         0.10.2
toolz                        0.11.2
tqdm                         4.64.0
typeguard                    2.13.3
typing_extensions            4.2.0
urllib3                      1.26.9
Werkzeug                     2.1.2
wheel                        0.37.1
wrapt                        1.14.1
zipp                         3.8.0"
"Currently, installing the dependencies takes multiple hours and eventually never finishes. 
My previous attempt shortly after code release went fine, but now its not getting past the first code cell.

Here is a slice of pips progress after about 5 hours of installing dependencies:

```INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking
  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)
     |████████████████████████████████| 9.5 MB 52.4 MB/s 
Collecting pytz
  Downloading pytz-2017.2-py2.py3-none-any.whl (484 kB)
     |████████████████████████████████| 484 kB 67.5 MB/s 
Collecting pandas>=0.22.0
  Downloading pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)
     |████████████████████████████████| 9.5 MB 65.9 MB/s 
  Downloading pandas-1.1.3-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)
     |████████████████████████████████| 9.5 MB 72.3 MB/s 
  Downloading pandas-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)
     |████████████████████████████████| 10.5 MB 41.9 MB/s 
  Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)
     |████████████████████████████████| 10.5 MB 39.4 MB/s 
  Downloading pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)
     |████████████████████████████████| 10.5 MB 3.7 MB/s 
  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 75.4 MB/s 
  Downloading pandas-1.0.4-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 84.1 MB/s 
  Downloading pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)
     |████████████████████████████████| 10.0 MB 70.1 MB/s 
  Downloading pandas-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 39.6 MB/s 
  Downloading pandas-1.0.1-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 79.9 MB/s 
  Downloading pandas-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)
     |████████████████████████████████| 10.0 MB 41.4 MB/s 
  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)
     |████████████████████████████████| 10.4 MB 29.6 MB/s 
  Downloading pandas-0.25.2-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)
     |████████████████████████████████| 10.4 MB 25.4 MB/s 
  Downloading pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)
     |████████████████████████████████| 10.4 MB 47.5 MB/s 
  Downloading pandas-0.25.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)
     |████████████████████████████████| 10.4 MB 38.0 MB/s 
  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 69.1 MB/s 
Collecting pytz
  Downloading pytz-2016.10-py2.py3-none-any.whl (483 kB)
     |████████████████████████████████| 483 kB 85.3 MB/s 
  Downloading pytz-2016.7-py2.py3-none-any.whl (480 kB)
     |████████████████████████████████| 480 kB 66.2 MB/s 
  Downloading pytz-2016.6.1-py2.py3-none-any.whl (481 kB)
     |████████████████████████████████| 481 kB 86.3 MB/s 
  Downloading pytz-2016.6-py2.py3-none-any.whl (481 kB)
     |████████████████████████████████| 481 kB 64.3 MB/s 
  Downloading pytz-2016.4-py2.py3-none-any.whl (480 kB)
     |████████████████████████████████| 480 kB 93.3 MB/s 
  Downloading pytz-2016.3-py2.py3-none-any.whl (479 kB)
     |████████████████████████████████| 479 kB 84.1 MB/s 
  Downloading pytz-2016.2-py2.py3-none-any.whl (478 kB)
     |████████████████████████████████| 478 kB 88.2 MB/s 
  Downloading pytz-2016.1-py2.py3-none-any.whl (476 kB)
     |████████████████████████████████| 476 kB 87.5 MB/s 
  Downloading pytz-2015.7-py2.py3-none-any.whl (476 kB)
     |████████████████████████████████| 476 kB 100.4 MB/s 
  Downloading pytz-2015.6-py2.py3-none-any.whl (475 kB)
     |████████████████████████████████| 475 kB 96.8 MB/s 
  Downloading pytz-2015.4-py2.py3-none-any.whl (475 kB)
     |████████████████████████████████| 475 kB 89.2 MB/s 
  Downloading pytz-2015.2-py2.py3-none-any.whl (476 kB)
     |████████████████████████████████| 476 kB 97.4 MB/s 
  Downloading pytz-2014.10-py2.py3-none-any.whl (477 kB)
     |████████████████████████████████| 477 kB 90.9 MB/s 
  Downloading pytz-2014.9-py2.py3-none-any.whl (477 kB)
     |████████████████████████████████| 477 kB 85.6 MB/s 
  Downloading pytz-2014.7.zip (492 kB)
     |████████████████████████████████| 492 kB 87.3 MB/s 
  Downloading pytz-2014.4.zip (483 kB)
     |████████████████████████████████| 483 kB 76.4 MB/s 
  Downloading pytz-2014.3.zip (482 kB)
     |████████████████████████████████| 482 kB 85.3 MB/s 
  Downloading pytz-2014.2.zip (486 kB)
     |████████████████████████████████| 486 kB 77.4 MB/s 
  Downloading pytz-2014.1.1.zip (488 kB)
     |████████████████████████████████| 488 kB 72.4 MB/s 
  Downloading pytz-2014.1.zip (485 kB)
     |████████████████████████████████| 485 kB 81.5 MB/s 
  Downloading pytz-2013.9.zip (485 kB)
     |████████████████████████████████| 485 kB 75.3 MB/s 
  Downloading pytz-2013.8.zip (504 kB)
     |████████████████████████████████| 504 kB 74.5 MB/s 
  Downloading pytz-2013.7.zip (504 kB)
     |████████████████████████████████| 504 kB 66.9 MB/s 
  Downloading pytz-2013.6.zip (504 kB)
     |████████████████████████████████| 504 kB 78.2 MB/s 
  Downloading pytz-2013b.zip (535 kB)
     |████████████████████████████████| 535 kB 83.1 MB/s 
  Downloading pytz-2012c.zip (520 kB)
     |████████████████████████████████| 520 kB 5.2 MB/s 
  Downloading pytz-2012b.zip (519 kB)
     |████████████████████████████████| 519 kB 71.1 MB/s 
  Downloading pytz-2013d.zip (543 kB)
     |████████████████████████████████| 543 kB 82.0 MB/s 
  Downloading pytz-2012j.zip (535 kB)
     |████████████████████████████████| 535 kB 72.8 MB/s 
  Downloading pytz-2012h.tar.gz (255 kB)
     |████████████████████████████████| 255 kB 72.9 MB/s 
  Downloading pytz-2012g.zip (524 kB)
     |████████████████████████████████| 524 kB 77.1 MB/s 
  Downloading pytz-2012f.zip (523 kB)
     |████████████████████████████████| 523 kB 76.8 MB/s 
  Downloading pytz-2012d.zip (520 kB)
     |████████████████████████████████| 520 kB 78.4 MB/s 
  Downloading pytz-2011n.zip (519 kB)
     |████████████████████████████████| 519 kB 77.3 MB/s 
  Downloading pytz-2011k.zip (517 kB)
     |████████████████████████████████| 517 kB 80.3 MB/s 
Collecting pandas>=0.22.0
  Downloading pandas-0.24.1-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 74.8 MB/s 
  Downloading pandas-0.24.0-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)
     |████████████████████████████████| 10.1 MB 23.8 MB/s 
  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/67/a7/12261a51ac2e7be4c698ca27cbe364ca5f16d64999456ee47ea8c7b44417/pandas-0.23.4-cp37-cp37m-manylinux1_x86_64.whl
  Downloading pandas-0.23.4-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)
     |████████████████████████████████| 8.8 MB 5.2 MB/s 
  Downloading pandas-0.23.3-cp37-cp37m-manylinux1_x86_64.whl (8.9 MB)
     |████████████████████████████████| 8.9 MB 811 kB/s 
  Downloading pandas-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (8.9 MB)
     |████████████████████████████████| 8.9 MB 13.1 MB/s 
  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/27/85/f9e4f0e47a6f1410b1d737b74a1764868e9197e3197a2be843507b505636/pandas-0.23.1.tar.gz
  Downloading pandas-0.23.1.tar.gz (13.1 MB)
     |████████████████████████████████| 13.1 MB 4.8 MB/s 
  Downloading pandas-0.23.0.tar.gz (13.1 MB)
     |████████████████████████████████| 13.1 MB 5.7 MB/s 
  Downloading pandas-0.22.0.tar.gz (11.3 MB)
     |████████████████████████████████| 11.3 MB 37.2 MB/s 
  Installing build dependencies ... error
WARNING: Discarding https://files.pythonhosted.org/packages/08/01/803834bc8a4e708aedebb133095a88a4dad9f45bbaf5ad777d2bea543c7e/pandas-0.22.0.tar.gz#sha256=44a94091dd71f05922eec661638ec1a35f26d573c119aa2fad964f10a2880e6c (from https://pypi.org/simple/pandas/). Command errored out with exit status 1: /usr/bin/python3 /tmp/pip-standalone-pip-rmvyhaa6/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-3p8ugjqk/overlay --no-warn-script-location -v --no-binary :none: --only-binary :none: -i https://pypi.org/simple --extra-index-url https://us-python.pkg.dev/colab-wheels/public/simple/ -- wheel setuptools Cython 'numpy==1.9.3; python_version=='""'""'3.5'""'""'' 'numpy==1.12.1; python_version=='""'""'3.6'""'""'' 'numpy==1.13.1; python_version>='""'""'3.7'""'""'' Check the logs for full command output.
INFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.
Collecting kaggle>=1.3.9
  Downloading kaggle-1.5.12.tar.gz (58 kB)
     |████████████████████████████████| 58 kB 7.1 MB/s 
  Downloading kaggle-1.5.10.tar.gz (59 kB)
     |████████████████████████████████| 59 kB 2.5 MB/s 
```

Is there any way to update this notebook to work like it did two months ago?"
""
"Hi!

Will the checkpoints of pre-trained PaLM models be released?

[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)"
"hi @agarwl

Thanks for releasing the code. However, I have some trouble when attempting to run the jumping-task code. I've downloaded the code under this [dir](https://github.com/google-research/google-research/tree/master/pse), and I've followed the README instruction and run `python -m jumping_task.train --train_dir . --training_epochs 1` under my `/pse` dir. 

Here is what I got:
```
Traceback (most recent call last):
  File ""/home/dibbla/miniconda3/envs/pse/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/dibbla/miniconda3/envs/pse/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/dibbla/RLgen/paperCode/pse/jumping_task/train.py"", line 417, in <module>
    app.run(main)
  File ""/home/dibbla/miniconda3/envs/pse/lib/python3.8/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/home/dibbla/miniconda3/envs/pse/lib/python3.8/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/home/dibbla/RLgen/paperCode/pse/jumping_task/train.py"", line 411, in main
    train_agent(train_dir, measurements)
  File ""/home/dibbla/RLgen/paperCode/pse/jumping_task/train.py"", line 186, in train_agent
    imitation_data = data_helpers.generate_imitation_data(
  File ""/home/dibbla/RLgen/paperCode/pse/jumping_task/data_helpers.py"", line 108, in generate_imitation_data
    imitation_data[color_key] = _generate_imitation_data(
  File ""/home/dibbla/RLgen/paperCode/pse/jumping_task/data_helpers.py"", line 87, in _generate_imitation_data
    observations, actions, rewards = generate_optimal_trajectory(
  File ""/home/dibbla/RLgen/paperCode/pse/jumping_task/data_helpers.py"", line 53, in generate_optimal_trajectory
    initial_obs = env.environment._reset(  # pylint: disable=protected-access
  File ""/home/dibbla/miniconda3/envs/pse/lib/python3.8/site-packages/gym/core.py"", line 235, in __getattr__
    raise AttributeError(
AttributeError: attempted to get missing private attribute '_reset'
```

It seems like accessing a protected method in gym/core. So I downgraded the gym to 0.21. The problem stays still. Here is my environment:

- Ubuntu 20.04LTS
- RTX3090
- gym 0.21.0
- gym-jumping-task 0.0.1 (from anothor [repo](https://github.com/Maluuba/jumping-task))
- tensorflow 2.4.1

And I also downloaded all requirements in requirements.txt.

Any clue about solving the problem?"
"I am trying to train on my own dataset with alignment loss. I created the tfrecords from unlabeled videos using the videos_to_tfrecords code. However, when I run the train.py code, I get an error because the number of frames is None. I have checked the number of frames read during the tfrecord creation process and it is an integer greater than zero for each video."
"Hi. Thanks for greate repository.
I train kws on my dataset. I check logs and see program have loaded gpus.
But I noticed that sometimes, my gpus are idle (gpus usage down to 0%). I think this is because data loading is not responsive to gpus. 
how can I increase the number of cores in data loading ?"
"Hi, I can't seem to find what IDs are associated with each mode switching token in the pretrained models for UL2. I guess we use some of the `<extra_id_xx>` tokens are used for this? 

Thanks"
":wave: HI,

The links to the configs in Unifying Language Learning Paradigms are broken: https://github.com/google-research/google-research/tree/master/ul2

They point to this location https://storage.googleapis.com/scenic-bucket/ul2/ul220b/config.gin

but these are the only files located in that folder.
![image](https://user-images.githubusercontent.com/9974388/168416540-06069521-b27a-4684-aff5-8692ba4bb0c5.png)"
As above.
"Hello, I am trying to evaluate VATT on YouCook2 dataset for text-video retrieval. I am having errors trying to load a previous checkpoint among many other package issues with tensorflow v2.7, DMVR, and more (but will save that for another ticket). 

The weights I am trying to load are for ut_fac and are stored in `weights/vatt` which I am passing as the param to `override_checkpoint`:

The link to data is: https://storage.cloud.google.com/tf_model_garden/vision/vatt/pretrain/ut_fac_medium/ckpt-500000.data-00000-of-00001
The link to index is: https://storage.cloud.google.com/tf_model_garden/vision/vatt/pretrain/ut_fac_medium/ckpt-500000.index
Then I am passing the architecture as ut_fac.

What I am calling:
```
WEIGHTS_DIR=weights/vatt
python main.py --task=pretrain \
                    --mode=eval \
                    --model_dir=$WEIGHTS_DIR \
                    --model_arch=ut_fac \
                    --strategy_type=mirrored \
                     --override_checkpoint=$WEIGHTS_DIR
```

Error: 
```
022-04-28 16:53:59.807821: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/apps/anaconda3-2019.03-1/lib
2022-04-28 16:53:59.807863: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
I0428 16:53:59.813026 22875650352960 main.py:87] Model Parameters: {'checkpoint_path': 'weights/vatt',
 'eval': {'input': {'audio_mixup': False,
                    'audio_stride': 1,
                    'batch_size': 8,
                    'color_augment': True,
                    'crop_resize_style': 'VGG',
                    'frame_size': 224,
                    'has_data': True,
                    'linearize_vision': True,
                    'max_area_ratio': 1.0,
                    'max_aspect_ratio': 2.0,
                    'max_num_words': 16,
                    'mel_bins': 80,
                    'min_area_ratio': 0.08,
                    'min_aspect_ratio': 0.5,
                    'min_resize': 224,
                    'mixup_alpha': 10,
                    'mixup_beta': 1,
                    'multi_crop': False,
                    'name': ('youcook2',),
                    'num_augmentation': 1,
                    'num_examples': 4096,
                    'num_frames': 32,
                    'num_windows_test': 4,
                    'raw_audio': True,
                    'scale_jitter': True,
                    'space_to_depth': False,
                    'split': 'test',
                    'stft_length': 0.04267,
                    'stft_step': 0.02134,
                    'text_tokenizer': 'WordTokenizer',
                    'video_stride': 2,
                    'zero_centering_image': True}},
 'mode': 'eval',
 'model_config': {'backbone_config': {'name': 'unified_backbone',
                                      'unified_backbone': 'ut_medium'},
                  'head_config': {'bridge': ({'aud_to_vid_txt_kwargs': {'d_model': 512,
                                                                        'modality': 'audio',
                                                                        'name': 'audio_mlp_module'},
                                              'bn_config': {'epsilon': 1e-05,
                                                            'momentum': 0.9,
                                                            'name': 'batch_norm',
                                                            'scale': True},
                                              'name': 'mlp_fac',
                                              'txt_to_vid_aud_kwargs': {'d_model': 256,
                                                                        'modality': 'text',
                                                                        'name': 'text_mlp_module'},
                                              'use_xreplica_bn': True,
                                              'vid_to_aud_txt_kwargs': {'d_model': 512,
                                                                        'modality': 'video',
                                                                        'name': 'video_mlp_module'}},)},
                  'loss_config': {'bridge': ({'aud_txt_weight': 0.0,
                                              'loss_weight': 1.0,
                                              'name': 'asymmetric_nce',
                                              'temperature': 0.07,
                                              'vid_aud_weight': 1.0,
                                              'vid_txt_weight': 1.0},)},
                  'model_name': 'uvatt_mlp_fac'},
 'model_dir': 'weights/vatt',
 'strategy_config': {'distribution_strategy': 'mirrored', 'tpu': None},
 'task': 'Pretrain',
 'train': {'gradient_clip_norm': 0.0,
           'gradient_clip_norm_cls': None,
           'input': {'audio_mixup': False,
                     'audio_noise': 0.01,
                     'audio_stride': 1,
                     'batch_size': 8,
                     'color_augment': True,
                     'crop_resize_style': 'VGG',
                     'frame_size': 224,
                     'has_data': True,
                     'linearize_vision': True,
                     'max_area_ratio': 1.0,
                     'max_aspect_ratio': 2.0,
                     'max_context_sentences': 4,
                     'max_num_words': 16,
                     'mel_bins': 80,
                     'min_area_ratio': 0.08,
                     'min_aspect_ratio': 0.5,
                     'min_resize': 224,
                     'mixup_alpha': 10,
                     'mixup_beta': 2,
                     'name': 'howto100m+audioset',
                     'num_examples': -1,
                     'num_frames': 32,
                     'raw_audio': True,
                     'scale_jitter': True,
                     'space_to_depth': False,
                     'split': 'train',
                     'stft_length': 0.04267,
                     'stft_step': 0.02134,
                     'text_tokenizer': 'WordTokenizer',
                     'video_stride': 1,
                     'zero_centering_image': True},
           'iterations_per_loop': 50,
           'max_checkpoints': 50,
           'optimizer': {'beta_1': 0.9,
                         'beta_2': 0.999,
                         'epsilon': 1e-07,
                         'learning_rate': {'learning_rate_base': 0.0001,
                                           'learning_rate_levels': (0.0001,
                                                                    5e-05),
                                           'learning_rate_steps': (5000,
                                                                   500000),
                                           'total_steps': 500000,
                                           'warmup_learning_rate': 0.0,
                                           'warmup_steps': 5000},
                         'name': 'Adam'},
           'save_checkpoint_freq': 10000}}
2022-04-28 16:53:59.814110: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:localhost/replica:0/task:0/device:GPU:0
W0428 16:53:59.815804 22875650352960 cross_device_ops.py:1382] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:localhost/replica:0/task:0/device:GPU:0
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0428 16:53:59.829632 22875650352960 mirrored_strategy.py:376] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0428 16:54:00.367541 22875650352960 mlp_lib.py:122] Using Cross Replica BatchNorm in Relu-Dense-BN.
I0428 16:54:00.374226 22875650352960 mlp_lib.py:122] Using Cross Replica BatchNorm in Relu-Dense-BN.
I0428 16:54:00.376174 22875650352960 factory.py:123] Head stack mlp_fac created successfully.
I0428 16:54:00.501415 22875650352960 factory.py:134] Unified backbone ut_medium created successfully.
I0428 16:54:00.505504 22875650352960 factory.py:173] Entire MM model uvatt_mlp_fac created successfully.
I0428 16:54:09.682963 22875650352960 pretrain.py:99] Language embedding weights word2vec restored successfully.
I0428 16:54:09.692070 22875650352960 pretrain.py:130] Number of parameters in model: 182.653696 M.
I0428 16:54:09.701606 22875650352960 base.py:446] Override checkpoint found. Restoring the model from the checkpoint at weights/vatt.
2022-04-29 16:36:17.254240: W tensorflow/core/util/tensor_slice_reader.cc:96] Could not open weights/vatt: DATA_LOSS: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?
Traceback (most recent call last):
  File ""/home/schiappa/.conda/envs/vatt_v2/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 96, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern))
RuntimeError: Unable to open table file weights/vatt: DATA_LOSS: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 110, in <module>
    app.run(main)
  File ""conda/envs/vatt_v2/lib/python3.8/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File "".conda/envs/vatt_v2/lib/python3.8/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 106, in main
    return executor.run(mode=params.mode)
  File ""models/vatt/experiments/base.py"", line 500, in run
    self.evaluate()
  File ""models/vatt/experiments/base.py"", line 450, in evaluate
    checkpoint.restore(self.params.checkpoint_path).expect_partial().assert_existing_objects_matched()
  File "".conda/envs/vatt_v2/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 2354, in restore
    status = self.read(save_path, options=options)
  File "".conda/envs/vatt_v2/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 2229, in read
    result = self._saver.restore(save_path=save_path, options=options)
  File ""/home/schiappa/.conda/envs/vatt_v2/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py"", line 1322, in restore
    reader = py_checkpoint_reader.NewCheckpointReader(save_path)
  File ""/home/schiappa/.conda/envs/vatt_v2/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 100, in NewCheckpointReader
    error_translator(e)
  File ""schiappa/.conda/envs/vatt_v2/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py"", line 44, in error_translator
    raise errors_impl.DataLossError(None, None, error_message)
tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file weights/vatt: DATA_LOSS: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?
```

I know it says to run in python3.7 but DMVR does not work for me otherwise."
"I deployed the code following the ""Running in a virtual environment"" guideline on two different GPU servers, I found it will take very long (100 hours and 200 hours) to train even one model with low resolution. My cuda is 11.1.

![image](https://user-images.githubusercontent.com/8349580/165294767-90619200-4043-4984-9280-ccedc02044ad.png)
![image](https://user-images.githubusercontent.com/8349580/165294792-03cfba85-b476-462f-a21c-d9bccc518ee2.png)
![image](https://user-images.githubusercontent.com/8349580/165294800-2c81dae2-cb9c-4bb0-83e6-f4a2c0c218e9.png)

Thanks very much."
"Thanks for this wonderful work.

The space annealing formula (13) is different between the google drive and arxiv version.
Google drive:
<img width=""254"" alt=""image"" src=""https://user-images.githubusercontent.com/2679561/165253821-10104415-a58a-4410-bd16-8840910a929b.png"">
arixv
<img width=""213"" alt=""image"" src=""https://user-images.githubusercontent.com/2679561/165254054-f10c1a47-59c5-4aca-9cd8-9c8bff3a4563.png"">
Since `Ps` stands for ""start range"" and the sampling region should be lied in `[t_near, t_far]`, I think google drive version is correct.

"
"Hey @ajayjain. Thankyou for the amazing work with dreamfields.

Can you please have a look at the error logs for `bash run_docker.sh all ""elephant origami""`? 

```
bash run_docker.sh all ""elephant origami""
GPUS all
QUERY elephant origami

================
== TensorFlow ==
================

NVIDIA Release 21.12-tf2 (build 29871111)
TensorFlow Version 2.6.2

Container image Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
Copyright 2017-2021 The TensorFlow Authors.  All rights reserved.

NVIDIA Deep Learning Profiler (dlprof) Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

NOTE: CUDA Forward Compatibility mode ENABLED.
  Using CUDA 11.5 driver version 495.29.05 with kernel driver version 470.103.01.
  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.

NOTE: MOFED driver for multi-node communication was not detected.
      Multi-node communication performance may be reduced.

/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py:400: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.
  warnings.warn(
I0420 18:58:23.989800 140279365150528 xla_bridge.py:231] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: 
I0420 18:58:24.167545 140279365150528 xla_bridge.py:231] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available.
/usr/local/lib/python3.8/dist-packages/jax/_src/lib/xla_bridge.py:413: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.
  warnings.warn(
I0420 18:58:24.167988 140279365150528 run.py:65] JAX host: 0 / 1
I0420 18:58:24.168152 140279365150528 run.py:66] JAX devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]
I0420 18:58:24.168639 140279365150528 local.py:45] Setting task status: host_id: 0, host_count: 1
I0420 18:58:24.169077 140279365150528 local.py:50] Created artifact experiment_dir of type ArtifactType.DIRECTORY and value results.
I0420 18:58:24.169288 140279365150528 local.py:50] Created artifact work_unit_dir of type ArtifactType.DIRECTORY and value results/'elephant origami' 20220420-185824.
I0420 18:58:24.169421 140279365150528 run.py:93] experiment_dir=results work_unit_dir=results/'elephant origami' 20220420-185824
I0420 18:58:24.981649 140279365150528 run.py:108] config={
    ""acc_lam"": 0.5,
    ""acc_lam_after"": 0,
    ""acc_target0"": 0.5,
    ""acc_target1"": 0.1,
    ""acc_target_i_split"": 500,
    ""adam_eps"": 1e-05,
    ""augment_backgrounds"": true,
    ""beta_after"": 20000,
    ""bg_blur_std_range"": [
        0.0,
        10.0
    ],
    ""checker_bg_nsq"": 8,
    ""checker_bg_prob"": 0.333,
    ""checkpoint_every"": 1000,
    ""clip_width"": 224,
    ""crop_width"": 80,
    ""defragment_every"": 200,
    ""ema_scene_origin"": true,
    ""far"": 5.732050807568877,
    ""features_early"": [
        128
    ],
    ""features_late"": [
        128,
        4
    ],
    ""features_residual"": [
        [
            256,
            128
        ],
        [
            256,
            128
        ],
        [
            256,
            128
        ],
        [
            256,
            128
        ]
    ],
    ""fft_bg_prob"": 0.334,
    ""flush_every"": 500,
    ""focal_mult_range"": [
        1.2,
        1.2
    ],
    ""fourfeat"": true,
    ""hq_video_every"": 10000,
    ""iters"": 10000,
    ""jitter"": true,
    ""keep_every_n_steps"": 10000,
    ""log_scalars_every"": 50,
    ""loss_model"": ""clip_vit_b16"",
    ""lr0"": 1e-05,
    ""lr1"": 0.0001,
    ""lr2"": 0.0001,
    ""lr_cosine_decay"": false,
    ""lr_i_split"": 1500,
    ""min_aug_acc"": 0.0,
    ""mipnerf"": {
        ""decay_iters"": 1000,
        ""decay_start"": 1.0,
        ""sigma_activation"": ""softplus"",
        ""use_cov"": true
    },
    ""mlp_activation"": ""swish"",
    ""mr0"": 1.0,
    ""mr1"": 1.0,
    ""mr_i_split"": 10000,
    ""mr_norm"": ""inf"",
    ""n_local_aug"": 8,
    ""near"": 2.267949192431123,
    ""noise_bg_prob"": 0.333,
    ""num_samples"": 192,
    ""parameterization"": ""mipnerf"",
    ""phi_range"": [
        -30,
        -30
    ],
    ""posenc_deg"": 8,
    ""posenc_dirs_deg"": 4,
    ""queries_r"": [
        ""a blue bicycle parked by a metal gate."",
        ""an orange bike leaning on a pole in the snow."",
        ""there are some colored lights hanging from street lamps."",
        ""the apple symbol is show on the premacy car."",
        ""an orange motorcycle is shown at close range."",
        ""a blue motorbike has a \""minnesota\"" license plate."",
        ""the side of an american aircraft showing the door."",
        ""a floor drain is set in concrete with an advisory not to step on it."",
        ""a bus covered with assorted colorful graffiti on the side of it."",
        ""a bus covered in graffiti is stationary on the pavement."",
        ""a bike image on some double doors with windows."",
        ""a red train cart is shown at close range."",
        ""a truck is drying several items of clothing in the sun."",
        ""the rotted out bed of a truck left in the woods."",
        ""a boat on the water tied down to a stake."",
        ""an inflatable raft that has its top open."",
        ""a red light in front of a tall building."",
        ""a street sign says walk and don't walk."",
        ""a red and blue fire hydrant with flowers around it."",
        ""a red fire hydrant with an open sign on it."",
        ""a street sign with stickers on the back of it."",
        ""a red stop sign with lots of writing all over it."",
        ""a parking meter with a time expired label on it."",
        ""a blue faced machine for printing parking passes."",
        ""a bag full of trash sitting on a old park bench."",
        ""a park bench sits under a tree with the sun shining."",
        ""a picture of a flamingo scratching its neck."",
        ""a large blue bird standing next to a painting of flowers."",
        ""a cat is staring ahead as the back another cat's head is seen in front of him."",
        ""an orange cat looking upside down through glasses."",
        ""a bulldog is wearing a black pirate hat."",
        ""a dog standing at a gate wanting to get out of the fence."",
        ""a brown and white horse wearning a harness eating some hay."",
        ""a brown and white horse is wearing a blue muzzle."",
        ""a sheep standing in the grass with something on its ears."",
        ""a sheep looking through the slats of a wired fence."",
        ""a black cow looks directly at the camera."",
        ""a horned cow  standing in a green grass field."",
        ""an elephant with trimmed tusks relaxing with a covering of hay on its back."",
        ""an elephant placing some leaves in its mouth with its trunk."",
        ""the polar bear swimming briskly through the ocean current."",
        ""a large black bear is facing straight ahead."",
        ""the painting is of a zebra in a cage."",
        ""a zebra is eating grass on the ground."",
        ""a giraffe leaning it's long neck over the fence to eat leaves off a bush."",
        ""a very big giraffe that is siting on the ground."",
        ""a stuffed animal in a bag in a room."",
        ""a large umbrella open wide on a pole."",
        ""the plants can be seen through the orange mesh."",
        ""a fireplace mantle that has been faced in a light stone."",
        ""a tan table top hosts a pen and a necktie."",
        ""a gold tie is tied under a brown dress shirt with stripes."",
        ""a piece of gray luggage with travel stickers."",
        ""a cat relaxes in a suitcase next to a pile of clothes."",
        ""a yellow frisbee next to a box with nike cleats."",
        ""blue frisbee and envelope it was shipped in."",
        ""a large pair of skis rests against a wall."",
        ""appears to be some old skis propped up against a stone memorial."",
        ""a snowboard standing upright in a snow bank."",
        ""a snowboard and gloves laying in the snow."",
        ""a blue and white traffic sign on a grey brick wall."",
        ""a cat shaped kite sits in the grass."",
        ""there is a very colorful kite that is in the air."",
        ""a bat and shin guard in the closet."",
        ""a baseball bat with a batting helmet upsidedown."",
        ""a stuffed animal that is frowning is on a skateboard."",
        ""an very well used upside down skateboard on grass."",
        ""white surfboard leaning against a brown tiki wall."",
        ""a small surfboard sign that says trader vic's, los angeles."",
        ""a couple of snowmen have been built in suburban backyards after a recently fallen snow."",
        ""very large tennis racket with hello kitty on it."",
        ""a blender and jar of red liquid on a table."",
        ""a plastic jar of honey glowing in the middle of the dark."",
        ""a table with a blender and a glass on it."",
        ""a glass of wine sitting on the top of a swimming pool side."",
        ""a glass measuring cup with yellow liquid in it."",
        ""a blender full of liquid is spilling everywhere."",
        ""a dish of food topped with sour cream and a fork."",
        ""a pizza and fork on a tray on the table."",
        ""a knife sitting on top of a wooden table next to a knife."",
        ""beet tops and a chef's knife on a cutting board."",
        ""a pile of cabbage, noodles, and meat next to chopsticks."",
        ""blender half full of a slurry, next to other electrical appliances."",
        ""a bowl of a meal with an egg, \""sunny side up\"", laid on top of everything."",
        ""a pork dish with onions and peppers on a white plate."",
        ""someone wrote a message on a bunch of bananas."",
        ""fruit growing on the side of a tree in a jungle."",
        ""a bunch of apples stacked on a plate."",
        ""apples on tree ready to pick in garden area."",
        ""a sandwich with meat, vegetables, peppers, and lettuce."",
        ""a pile of crab is seasoned and well cooked."",
        ""cut up blood red oranges lay on a blue surface."",
        ""a plate of oranges sliced on top of a table."",
        ""a pile of broccoli laying on a plastic cutting board."",
        ""a plate of food has noodles and broccoli."",
        ""a tray that has meat and carrots on a table."",
        ""a stuffed grey rabbit holding a pretend carrot."",
        ""the two hotdogs have brown mustard on them."",
        ""a plate with a couple of hot dogs on it."",
        ""view of what could possibly be a pizza with colorful vegetables as toppings."",
        ""a pizza that is covered in a lot of toppings."",
        ""a picture of a glazed donut with meat in the middle."",
        ""a donut is covered with glaze and sprinkles."",
        ""fresh red strawberries on a whipped dessert."",
        ""colorful icing on a pastry in the shapes of flowers."",
        ""the cat is sleeping comfortably on the chair."",
        ""a white cat curled up on a wooden chair."",
        ""a dog is sleeping on a pile of pillows."",
        ""a girl laying on the couch while on an laptop."",
        ""a bouguet of wilted red roses on a table."",
        ""a small green vase displays some small yellow blooms."",
        ""a tear in a black, blue, white and yellow piece of material."",
        ""a bed with white comforter and two black stiletto heels."",
        ""a plate of food consisting of rice, meat and vegetables."",
        ""a plate of food is centered around a portion of rice."",
        ""a porcelian scuplture sitting on a table next to a cup."",
        ""a bucket collects drips of water, inside a metal basin."",
        ""the back of a flat screened tv connected to a ipad."",
        ""a cluster of pine trees are in a barren area."",
        ""a bobble head is placed on a laptop keyboard."",
        ""a computer mouse sitting on a keyboard on a desk."",
        ""a silver and black wired computer mouse on a wooden surface."",
        ""a slug crawling on the ground around flower petals."",
        ""two remotes sitting on a table together."",
        ""the wii controller remote is turned on and has one light showing."",
        ""a black computer keyboard with a bunch of keys on it."",
        ""a keyboard that is missing some keys in the bottom row."",
        ""a cellphone standing upright with its camera side facing forward."",
        ""there is a cell phone on a table."",
        ""a microwave with fake eyes and a beard on it."",
        ""food steaming machine on the shelf of a store."",
        ""a yellow shallow baking dish in an open oven."",
        ""some food cooking on trays in an oven."",
        ""the toaster oven is turned on, on the counter in the kitchen."",
        ""a picture of a toaster plastered on a do not enter sign."",
        ""a old kitchen with no appliances inside of it."",
        ""two pictures of a hole in a counter with a lid."",
        ""a fridge is slightly open in a room."",
        ""the poster board is a special place for remembrances."",
        ""a stuffed animal sitting in the grass, with a book in front of it."",
        ""a run down windows with a broken fence outside."",
        ""an old and rusted clock is mounted on a brick wall."",
        ""a big metal clock sitting on the wall by itself."",
        ""bouquet of flowers sitting in a clear glass vase."",
        ""a blue jug in a garden filled with mud."",
        ""a sign that is advertising a barber shop."",
        ""a pair of scallop scrapbooking scissors on top of an envelope."",
        ""a stuffed bear is wearing a shirt with personalized writing."",
        ""a pair of yellow and pink teddy bears leaning against a wall."",
        ""a large statue of a female cow with a blonde wig."",
        ""rubber shoes lying on a carpet with floral prints."",
        ""a frog with teeth is on a purple toothbrush."",
        ""a worn toothbrush sits on a windowsill near the screen.""
    ],
    ""query"": ""elephant origami"",
    ""query_template"": ""{query}"",
    ""rad_range"": [
        4.0,
        4.0
    ],
    ""render_every"": 1000,
    ""render_hq_video"": true,
    ""render_lq_video"": true,
    ""render_width"": 88,
    ""retrieve_models"": [
        ""clip_vit_b32""
    ],
    ""retrieve_widths"": [
        224
    ],
    ""seed"": 0,
    ""sn0"": 0.0,
    ""sn1"": 0.0,
    ""sn_i_split"": 10000,
    ""substeps"": 1,
    ""test"": {
        ""jitter"": false,
        ""num_samples"": 256,
        ""white_bkgd"": true
    },
    ""test_hq"": {
        ""intersect_box"": false,
        ""jitter"": false,
        ""num_samples"": 512,
        ""white_bkgd"": true
    },
    ""th_range"": [
        0,
        360
    ],
    ""transparency_loss"": ""neg_lam_transmittance_clipped"",
    ""video_every"": 5000,
    ""viewdirs"": false,
    ""white_bkgd"": false,
    ""zero_origin_lam"": 0.0
}
I0420 18:58:24.981802 140279365150528 run.py:112] Running executable: train
I0420 18:58:24.981907 140279365150528 lib.py:61] Local devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]
I0420 18:58:24.981965 140279365150528 lib.py:62] All devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0)]
100%|█████████████████████████████████████| 1.29M/1.29M [00:00<00:00, 20.6MiB/s]
I0420 18:58:26.345784 140279365150528 tokenizer.py:47] Downloaded vocabulary from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true to /root/.cache/scenic/clip
100%|█████████████████████████████████████| 1.29M/1.29M [00:00<00:00, 20.0MiB/s]
I0420 18:58:31.982127 140279365150528 tokenizer.py:47] Downloaded vocabulary from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true to /root/.cache/scenic/clip
/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py:3660: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.
  lax._check_user_dtype_supported(dtype, ""zeros"")
I0420 18:58:36.377778 140279365150528 lib.py:241] n_device 2
I0420 18:58:42.307506 140279365150528 lib.py:262] did not find checkpoint in results/'elephant origami' 20220420-185824
I0420 18:58:42.307700 140279365150528 helpers.py:67] starting defragment...
I0420 18:58:42.703572 140279365150528 helpers.py:70] finished defragment
I0420 18:58:42.724392 140279365150528 helpers.py:67] starting defragment...
I0420 18:58:43.137570 140279365150528 helpers.py:70] finished defragment
I0420 18:58:43.140103 140279365150528 lib.py:326] Experiment dir results
I0420 18:58:43.140221 140279365150528 lib.py:327] Work unit dir results/'elephant origami' 20220420-185824
training:   0%|          | 0/10001 [00:02<?, ?it/s]
====================================================
bg_sel_key
Traced<ShapedArray(uint32[2])>with<BatchTrace(level=3/1)> with
  val = Traced<ShapedArray(uint32[8,2])>with<DynamicJaxprTrace(level=0/1)>
  batch_dim = 0
====================================================
bgs
Traced<ShapedArray(float32[3,224,224,3])>with<BatchTrace(level=3/1)> with
  val = Traced<ShapedArray(float32[8,3,224,224,3])>with<DynamicJaxprTrace(level=0/1)>
  batch_dim = 0
====================================================
probs
[0.333, 0.333, 0.334]
====================================================
Traceback (most recent call last):
  File ""run.py"", line 136, in <module>
    run(train=lib.run_train, eval=lib.run_eval)
  File ""run.py"", line 132, in run
    app.run(functools.partial(main, executable_dict))
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""run.py"", line 119, in main
    executable_dict[FLAGS.executable_name](
  File ""/dreamfields/dreamfields/lib.py"", line 805, in run_train
    for _ in DreamField(config).run_train(
  File ""/dreamfields/dreamfields/lib.py"", line 425, in run_train
    new_state, augs, mean_losses, new_scene_origin = train_pstep(
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/api.py"", line 1899, in cache_miss
    out_tree, out_flat = f_pmapped_(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/api.py"", line 1778, in f_pmapped
    out = pxla.xla_pmap(
  File ""/usr/local/lib/python3.8/dist-packages/jax/core.py"", line 1705, in bind
    return call_bind(self, fun, *args, **params)
  File ""/usr/local/lib/python3.8/dist-packages/jax/core.py"", line 1630, in call_bind
    outs = primitive.process(top_trace, fun, tracers, params)
  File ""/usr/local/lib/python3.8/dist-packages/jax/core.py"", line 1708, in process
    return trace.process_map(self, fun, tracers, params)
  File ""/usr/local/lib/python3.8/dist-packages/jax/core.py"", line 631, in process_call
    return primitive.impl(f, *tracers, **params)
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py"", line 710, in xla_pmap_impl
    compiled_fun, fingerprint = parallel_callable(fun, backend, axis_name, axis_size,
  File ""/usr/local/lib/python3.8/dist-packages/jax/linear_util.py"", line 263, in memoized_fun
    ans = call(fun, *args)
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/pxla.py"", line 798, in parallel_callable
    jaxpr, out_sharded_avals, consts = pe.trace_to_jaxpr_final(
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py"", line 1529, in trace_to_jaxpr_final
    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py"", line 1507, in trace_to_subjaxpr_dynamic
    ans = fun.call_wrapped(*in_tracers)
  File ""/usr/local/lib/python3.8/dist-packages/jax/linear_util.py"", line 166, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File ""/dreamfields/dreamfields/lib.py"", line 220, in train_step
    state, aux = body_fn(state, np.squeeze(multistep_constants))
  File ""/dreamfields/dreamfields/lib.py"", line 209, in body_fn
    (_, aux), grad = grad_fn(state.target, grad_fn_key, *step_constants)
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/api.py"", line 993, in value_and_grad_f
    ans, vjp_py, aux = _vjp(
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/api.py"", line 2270, in _vjp
    out_primal, out_vjp, aux = ad.vjp(
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/ad.py"", line 118, in vjp
    out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True)
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/ad.py"", line 103, in linearize
    jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals)
  File ""/usr/local/lib/python3.8/dist-packages/jax/interpreters/partial_eval.py"", line 512, in trace_to_jaxpr
    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)
  File ""/usr/local/lib/python3.8/dist-packages/jax/linear_util.py"", line 166, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File ""/dreamfields/dreamfields/lib.py"", line 151, in loss_fn
    augs = augment.augment_rendering(config, rgb_est, acc_est, aug_key)
  File ""/dreamfields/dreamfields/augment.py"", line 157, in augment_rendering
    return jax.vmap(random_aug)(random.split(key, config.n_local_aug))
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/api.py"", line 1405, in batched_fun
    out_flat = batching.batch(
  File ""/usr/local/lib/python3.8/dist-packages/jax/linear_util.py"", line 166, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))
  File ""/dreamfields/dreamfields/augment.py"", line 152, in random_aug
    bg = random_bg(bg_key)
  File ""/dreamfields/dreamfields/augment.py"", line 109, in random_bg
    bg = random.choice(bg_sel_key, bgs, p=np.array(probs))
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/random.py"", line 459, in choice
    raise ValueError(""a must be an integer or 1-dimensional"")
jax._src.traceback_util.UnfilteredStackTrace: ValueError: a must be an integer or 1-dimensional

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""run.py"", line 136, in <module>
    run(train=lib.run_train, eval=lib.run_eval)
  File ""run.py"", line 132, in run
    app.run(functools.partial(main, executable_dict))
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""run.py"", line 119, in main
    executable_dict[FLAGS.executable_name](
  File ""/dreamfields/dreamfields/lib.py"", line 805, in run_train
    for _ in DreamField(config).run_train(
  File ""/dreamfields/dreamfields/lib.py"", line 425, in run_train
    new_state, augs, mean_losses, new_scene_origin = train_pstep(
  File ""/dreamfields/dreamfields/lib.py"", line 220, in train_step
    state, aux = body_fn(state, np.squeeze(multistep_constants))
  File ""/dreamfields/dreamfields/lib.py"", line 209, in body_fn
    (_, aux), grad = grad_fn(state.target, grad_fn_key, *step_constants)
  File ""/dreamfields/dreamfields/lib.py"", line 151, in loss_fn
    augs = augment.augment_rendering(config, rgb_est, acc_est, aug_key)
  File ""/dreamfields/dreamfields/augment.py"", line 157, in augment_rendering
    return jax.vmap(random_aug)(random.split(key, config.n_local_aug))
  File ""/dreamfields/dreamfields/augment.py"", line 152, in random_aug
    bg = random_bg(bg_key)
  File ""/dreamfields/dreamfields/augment.py"", line 109, in random_bg
    bg = random.choice(bg_sel_key, bgs, p=np.array(probs))
  File ""/usr/local/lib/python3.8/dist-packages/jax/_src/random.py"", line 459, in choice
    raise ValueError(""a must be an integer or 1-dimensional"")
ValueError: a must be an integer or 1-dimensional
```

"
"Hi,
I tried to train a custom model, and the training failed due to `InvalidArgumentError: Data too short when trying to read string
	 [[node data/DecodeWav (defined at google-research/kws_streaming/data/input_data.py:415) ]]`


When I trained a model with less number of training steps like `2,2,2,2`, the training is successful on the dataset, whereas if I increase the training steps to `40000,40000,20000,20000`, the training failed.

My dataset contains wav audio between 1 second to 3 seconds with a 16k sampling rate."
"Hi, 

I am hoping to use ScaNN in my real-time recommendation system application (developed with Tensorflow Recommenders https://www.tensorflow.org/recommenders/examples/efficient_serving), where the candidate embeddings sit in a distributed database. Do you have advices or directions on how to set this up?

My idea is to separate partitioning and the AH search phase. I can do (re-)partitioning regularly as a batch job, store the results in a n external database. At prediction time, I could quickly fetch the partition I need by partition key, and perform AH search in real-time. Does this makes sense? 

If this is a way to proceed do you have API for partitioning and for scoring separately? I only found joint function for nearest neighbours?

Thanks!

Marin"
"Should the downloaded checkpoints be stored in the model_dir directory? I got the following information:
I0329 09:37:26.503962 140321606702912 checkpoint_utils.py:140] Waiting for new checkpoint at vatt/finetune
It didn't seem to find the checkpoint I downloaded. "
"I am wondering whether we can get the pre-trained model SMURF-test for both Sintel and KITTI that are used in Table 1 in the paper?

Thank you."
""
"hi
I am assuming the model has been trained to give out only 256 dimension output right? can we get final output dimensions higher than 256? if so kindly tell were to specify the size. If not can we train it on our dataset and set a final dimension of our own on which the spatial upsampler will give desired output
"
"hi i have checked everything but it seems my model is not loading the checkpoints correctly 
using the following command

`python custom_colorize.py --config=configs/colorizer.py \ --logdir=coltran_coltran/coltran/colorizer --img_dir=imgstotest --store_dir=results \ --mode=recolorize`

my output
`TX 1650 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5
2022-03-12 17:01:42.802178: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2022-03-12 17:01:43.536067: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204
WARNING:tensorflow:From /home/haider/Documents/color/lib/python3.8/site-packages/tensorflow/python/training/moving_averages.py:457: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0312 17:01:43.937938 140209677809472 deprecation.py:339] From /home/haider/Documents/color/lib/python3.8/site-packages/tensorflow/python/training/moving_averages.py:457: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0312 17:01:44.057944 140209677809472 train_utils.py:91] Built with exponential moving average.
I0312 17:01:44.065676 140209677809472 train_utils.py:185] Restoring from /tmp/svt.
I0312 17:01:44.081349 140209677809472 custom_colorize.py:207] Producing sample after 0 training steps.
I0312 17:01:44.081509 140209677809472 custom_colorize.py:210] 1
I0312 17:02:57.903805 140209677809472 custom_colorize.py:232] ['sdasds.jpg', 'flickr_cat_000004.jpg']
I0312 17:02:57.904262 140209677809472 custom_colorize.py:236] results/stage1/sdasds.jpg
I0312 17:02:57.904421 140209677809472 custom_colorize.py:238] (64, 64, 3)
I0312 17:02:57.912902 140209677809472 custom_colorize.py:236] results/stage1/flickr_cat_000004.jpg
I0312 17:02:57.913127 140209677809472 custom_colorize.py:238] (64, 64, 3)
`"
"Hi,
Thanks for providing the code for CKA.
I just had a question regarding comparing the activations for CNNs.

So, we have 4 dimensions for a CNN activation (Batch, Width, Height, Channel).
To use CKA, we need a 2 dimensions. Should we be average pooling to create a 2D matrix (Batch, Channel)?
Or is there some other method to do this?

Thanks"
"from README:

> ScaNN supports Linux environments running Python versions 3.7-3.9.

After upgrading to Tensorflow 2.8.0 (which supports python 3.10) and scann to 1.2.5, I am able to use scann on python 3.10.
Is the README outdated? :thinking: "
"Hi, I'm wondering if there's a script following the released code base to plot the feature graphs to interpret model predictions, as shown in the paper and talk. Thanks."
"Hello, I am trying to run the run_docker.sh file from Dream Fields according to the README. However, I encounter a tensorflow error while running the container.

`Traceback (most recent call last):
  File ""run.py"", line 28, in <module>
    from dreamfields import helpers
  File ""/dreamfields/dreamfields/helpers.py"", line 29, in <module>
    from scenic.projects.baselines.clip import model as clip
  File ""/usr/local/lib/python3.8/dist-packages/scenic/projects/baselines/clip/model.py"", line 12, in <module>
    from scenic.projects.baselines.clip import download
  File ""/usr/local/lib/python3.8/dist-packages/scenic/projects/baselines/clip/download.py"", line 9, in <module>
    from tensorflow.io import gfile
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 479, in <module>
    keras._load()
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/lazy_loader.py"", line 45, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/usr/local/lib/python3.8/dist-packages/keras/__init__.py"", line 25, in <module>
    from keras import models
  File ""/usr/local/lib/python3.8/dist-packages/keras/models.py"", line 20, in <module>
    from keras import metrics as metrics_module
  File ""/usr/local/lib/python3.8/dist-packages/keras/metrics/__init__.py"", line 23, in <module>
    from keras.metrics.base_metric import Metric
  File ""/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py"", line 25, in <module>
    from keras.engine import base_layer
  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 43, in <module>
    from keras.mixed_precision import loss_scale_optimizer
  File ""/usr/local/lib/python3.8/dist-packages/keras/mixed_precision/loss_scale_optimizer.py"", line 20, in <module>
    from keras.optimizer_v2 import optimizer_v2
  File ""/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py"", line 36, in <module>
    keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py"", line 360, in __init__
    super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/monitoring.py"", line 135, in __init__
    self._metric = self._metric_methods[self._label_length].create(*args)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Another metric with the same name already exists.`
"
"SST-2 and CoLA both.  PLZ help. Thanks

**[LOG]**
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
INFO:tensorflow:**** Trainable Variables ****
I0212 13:33:31.735677 139904805205824 run_classifier.py:770] **** Trainable Variables ****
ERROR:tensorflow:Error recorded from training_loop: 'NoneType' object has no attribute 'op'
E0212 13:33:31.758976 139904805205824 error_handling.py:75] Error recorded from training_loop: 'NoneType' object has no attribute 'op'
INFO:tensorflow:training_loop marked as finished
I0212 13:33:31.759163 139904805205824 error_handling.py:101] training_loop marked as finished
WARNING:tensorflow:Reraising captured error
W0212 13:33:31.759236 139904805205824 error_handling.py:135] Reraising captured error
Traceback (most recent call last):
  File ""run_classifier.py"", line 1219, in <module>
    tf.app.run()
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""run_classifier.py"", line 1082, in main
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3035, in train
    rendezvous.raise_errors()
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 136, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/six.py"", line 719, in reraise
    raise value
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1191, in _train_model_default
    features, labels, ModeKeys.TRAIN, self.config)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2857, in _call_model_fn
    config)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3126, in _model_fn
    features, labels, is_export_mode=is_export_mode)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1663, in call_without_tpu
    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1994, in _call_model_fn
    estimator_spec = self._model_fn(features=features, **kwargs)
  File ""run_classifier.py"", line 782, in model_fn
    total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)
  File ""/data/wangyuchi/google_research/mobilebert/optimization.py"", line 96, in create_optimizer
    grads = tf.gradients(loss, tvars)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py"", line 550, in _GradientsHelper
    gradient_uid)
  File ""/root/anaconda3/envs/mobilebert/lib/python3.6/site-packages/tensorflow_core/python/ops/gradients_util.py"", line 166, in _DefaultGradYs
    with _maybe_colocate_with(y.op, gradient_uid, colocate_gradients_with_ops):
AttributeError: 'NoneType' object has no attribute 'op'

**[CMD]**
python run_classifier.py \
--bert_config_file=config/uncased_L-24_H-128_B-512_A-4_F-4_OPT_QAT.json \
--data_dir=data/CoLA \
--task_name=cola \
--do_train \
--do_lower_case \
--init_checkpoint=mobilebert/mobilebert_variables.ckpt \
--learning_rate=5e-5 \
--max_seq_length=64 \
--num_train_epochs=5 \
--output_dir=cola_output \
--train_batch_size=96 \
--vocab_file=mobilebert/vocab.txt \
--warmup_proportion=0.1 \
--export_dir=cola_export \
--verbose_logging=True \
--use_quantized_training=true"
"@rybakov  I finished training a model using kws_streaming colab script.  The log file is like this: fname=""C:\\Users\\xxxxx\\Desktop\\google-research\\kws_streaming\\models\\ds_tc_resnet\\logs\\train\\events.out.tfevents.1644273734.NA51000227"".
When I tried to use visualize_training_validation_loss.ipynb to plot the curves, it shows some errors, and I found out that the log file can not be loaded correctly (""stop parsing C:\Users\xxxxx\Desktop\google-research\kws_streaming\models\ds_tc_resnet\logs\train\events.out.tfevents.1644273734.NA51000227""). I am in Windows machine. Do you know the reason for this?
Thanks!
"
"@rybakov 
What is the reason we set desired_samples as 6400 here?  Should we change this to 16000 (1 sec of audio)?
If I run the example code of 02_inference.ipynb, it will generate an error here.

ValueError: index can't contain negative values

# pad input audio with zeros, so that audio len = flags.desired_samples
padded_wav = np.pad(wav_data, (0, flags.desired_samples-len(wav_data)), 'constant')

input_data = np.expand_dims(padded_wav, 0)
input_data.shape"
"Hello, @rybakov, I am trying to do some modifications on your colab examples to test on a single wake word detection. 
I changed flags.wanted_words='stop' as an example, but it fails while training. Do you know what else should I change to customize it for a single wake word? Thanks!
==================================================================================================
Total params: 70,139
Trainable params: 68,603
Non-trainable params: 1,536
__________________________________________________________________________________________________
INFO:absl:Training from step: 1 
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_35504/211339909.py in <module>
      1 # Model training
----> 2 train.train(flags)

~\Desktop\google-research\kws_streaming\train\train.py in train(flags)
    163 
    164     tf.keras.backend.set_value(model.optimizer.lr, learning_rate_value)
--> 165     result = model.train_on_batch(train_fingerprints, train_ground_truth)
    166 
    167     summary = tf.Summary(value=[

C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\training_v1.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1074       self._update_sample_weight_modes(sample_weights=sample_weights)
   1075       self._make_train_function()
-> 1076       outputs = self.train_function(ins)  # pylint: disable=not-callable
   1077 
   1078     if reset_metrics:

C:\ProgramData\Anaconda3\lib\site-packages\keras\backend.py in __call__(self, inputs)
   4184       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
   4185 
-> 4186     fetched = self._callable_fn(*array_vals,
   4187                                 run_metadata=self.run_metadata)
   4188     self._call_fetch_callbacks(fetched[-len(self._fetches):])

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\client\session.py in __call__(self, *args, **kwargs)
   1481       try:
   1482         run_metadata_ptr = tf_session.TF_NewBuffer() if run_metadata else None
-> 1483         ret = tf_session.TF_SessionRunCallable(self._session._session,
   1484                                                self._handle, args,
   1485                                                run_metadata_ptr)

InvalidArgumentError: var and m do not have the same shape[12] [3]
	 [[{{node training/Adam/Adam/update_dense/bias/ResourceApplyAdam}}]]
"
"@rybakov
I need some help here while running the 01_train.ipynb code, the dataset I used is from Speech Commands V2. I did not change any of your code.  While running this training part using ds_tc_resnet, the training log looks like this. Could you please help me check what is wrong on this? Thanks

==================================================================================================
Total params: 71,300
Trainable params: 69,764
Non-trainable params: 1,536
__________________________________________________________________________________________________
INFO:absl:Training from step: 1 
INFO:absl:Step #1: rate 0.001000, accuracy 0.00%, cross entropy 2.493576
INFO:absl:Step #2: rate 0.001000, accuracy 0.00%, cross entropy 2.628293
INFO:absl:Step #3: rate 0.001000, accuracy 0.00%, cross entropy 2.373230
INFO:absl:Step #4: rate 0.001000, accuracy 0.00%, cross entropy 2.624747
INFO:absl:Step #5: rate 0.001000, accuracy 0.00%, cross entropy 2.825552
INFO:absl:Step #6: rate 0.001000, accuracy 0.00%, cross entropy 2.542758
INFO:absl:Step #7: rate 0.001000, accuracy 0.00%, cross entropy 2.756225
INFO:absl:Step #8: rate 0.001000, accuracy 0.00%, cross entropy 2.711938
INFO:absl:Step #9: rate 0.001000, accuracy 0.00%, cross entropy 2.505342
INFO:absl:Step #10: rate 0.001000, accuracy 100.00%, cross entropy 2.063214
INFO:absl:Step #11: rate 0.001000, accuracy 0.00%, cross entropy 2.727900
INFO:absl:Step #12: rate 0.001000, accuracy 0.00%, cross entropy 2.802885
INFO:absl:Step #13: rate 0.001000, accuracy 0.00%, cross entropy 2.997224
INFO:absl:Step #14: rate 0.001000, accuracy 0.00%, cross entropy 2.247151
INFO:absl:Step #15: rate 0.001000, accuracy 0.00%, cross entropy 2.342353
INFO:absl:Step #16: rate 0.001000, accuracy 0.00%, cross entropy 2.332682"
"I have been trying to use tf_trees and although the compilation goes without any warning in a amazon linux (built on top of Fedora if I understood correctly) the demo.py fails with segmentation fault. Here is the specification of the machine

```
NAME=""Amazon Linux AMI""
VERSION=""2018.03""
ID=""amzn""
ID_LIKE=""rhel fedora""
VERSION_ID=""2018.03""
PRETTY_NAME=""Amazon Linux AMI 2018.03""
ANSI_COLOR=""0;33""
CPE_NAME=""cpe:/o:amazon:linux:2018.03:ga""
HOME_URL=""http://aws.amazon.com/amazon-linux-ami/""
Amazon Linux AMI release 2018.03
```

I cannot compile the same on a colab (which is an ubuntu environment). 

```
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
NAME=""Ubuntu""
VERSION=""18.04.5 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.5 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```

when I try to compile it prints the following and does not produce the `.so`  file

```
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/partial_tensor_shape.h:20:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/attr_value_util.h:23,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/node_def_util.h:23,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_util.h:24,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:24,
                 from neural_trees_ops.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.h:305:22: warning: ‘tensorflow::int64’ is deprecated: Use int64_t instead. [-Wdeprecated-declarations]
   gtl::InlinedVector<int64, 4> dim_sizes() const;
                      ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/types.h:31:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/logging.h:20,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/errors.h:24,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/errors.h:19,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.h:23,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/partial_tensor_shape.h:20,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/attr_value_util.h:23,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/node_def_util.h:23,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_util.h:24,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:24,
                 from neural_trees_ops.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/integral_types.h:29:63: note: declared here
 [[deprecated(""Use int64_t instead."")]] typedef ::std::int64_t int64;
                                                               ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:25:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/attr_value_util.h:24,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/node_def_util.h:23,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/full_type_util.h:24,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op.h:24,
                 from neural_trees_ops.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h: In member function ‘void tensorflow::internal::MaybeWith32BitIndexingImpl<Eigen::GpuDevice>::operator()(Func, Args&& ...) const’:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:176:25: error: use of ‘auto’ in lambda parameter declaration only available with -std=c++14 or -std=gnu++14
     auto all = [](const auto&... bool_vals) {
                         ^~~~
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:176:34: error: expansion pattern ‘const int&’ contains no argument packs
     auto all = [](const auto&... bool_vals) {
                                  ^~~~~~~~~
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h: In lambda function:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:177:22: error: ‘bool_vals’ was not declared in this scope
       for (bool b : {bool_vals...}) {
                      ^~~~~~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/notification.h:27:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/notification.h:21,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/cancellation.h:22,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:27,
                 from neural_trees_helpers.h:17,
                 from neural_trees_kernels.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/notification.h:61:65: warning: ‘int64’ is deprecated [-Wdeprecated-declarations]
                                              int64 timeout_in_us);
                                                                 ^
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/notification.h:62:58: warning: ‘int64’ is deprecated [-Wdeprecated-declarations]
   bool WaitForNotificationWithTimeout(int64 timeout_in_us) {
                                                          ^
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/notification.h:81:63: warning: ‘int64’ is deprecated [-Wdeprecated-declarations]
                                            int64 timeout_in_us) {
                                                               ^
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:24:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:29,
                 from neural_trees_helpers.h:17,
                 from neural_trees_kernels.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.h:305:22: warning: ‘tensorflow::int64’ is deprecated: Use int64_t instead. [-Wdeprecated-declarations]
   gtl::InlinedVector<int64, 4> dim_sizes() const;
                      ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/types.h:31:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:27,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,
                 from neural_trees_helpers.h:17,
                 from neural_trees_kernels.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/integral_types.h:29:63: note: declared here
 [[deprecated(""Use int64_t instead."")]] typedef ::std::int64_t int64;
                                                               ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:25:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:29,
                 from neural_trees_helpers.h:17,
                 from neural_trees_kernels.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h: In member function ‘void tensorflow::internal::MaybeWith32BitIndexingImpl<Eigen::GpuDevice>::operator()(Func, Args&& ...) const’:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:176:25: error: use of ‘auto’ in lambda parameter declaration only available with -std=c++14 or -std=gnu++14
     auto all = [](const auto&... bool_vals) {
                         ^~~~
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:176:34: error: expansion pattern ‘const int&’ contains no argument packs
     auto all = [](const auto&... bool_vals) {
                                  ^~~~~~~~~
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h: In lambda function:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:177:22: error: ‘bool_vals’ was not declared in this scope
       for (bool b : {bool_vals...}) {
                      ^~~~~~~~~
neural_trees_kernels.cc: In member function ‘virtual void tensorflow::NTComputeInputAndInternalParamsGradientsOp::Compute(tensorflow::OpKernelContext*)’:
neural_trees_kernels.cc:141:19: warning: ‘tensorflow::int64’ is deprecated: Use int64_t instead. [-Wdeprecated-declarations]
       const int64 cost = 10000 * std::log10(input_dim) * std::log2(num_leaves);
                   ^~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/types.h:31:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:27,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,
                 from neural_trees_helpers.h:17,
                 from neural_trees_kernels.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/integral_types.h:29:63: note: declared here
 [[deprecated(""Use int64_t instead."")]] typedef ::std::int64_t int64;
                                                               ^~~~~
neural_trees_kernels.cc: In member function ‘virtual void tensorflow::NTComputeOutputOp::Compute(tensorflow::OpKernelContext*)’:
neural_trees_kernels.cc:274:19: warning: ‘tensorflow::int64’ is deprecated: Use int64_t instead. [-Wdeprecated-declarations]
       const int64 cost = 10000 * std::log10(input_dim) * std::log2(num_leaves);
                   ^~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/types.h:31:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:27,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,
                 from neural_trees_helpers.h:17,
                 from neural_trees_kernels.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/integral_types.h:29:63: note: declared here
 [[deprecated(""Use int64_t instead."")]] typedef ::std::int64_t int64;
                                                               ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/notification.h:27:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/lib/core/notification.h:21,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/cancellation.h:22,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:27,
                 from neural_trees_helpers.h:17,
                 from neural_trees_helpers.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/notification.h:61:65: warning: ‘int64’ is deprecated [-Wdeprecated-declarations]
                                              int64 timeout_in_us);
                                                                 ^
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/notification.h:62:58: warning: ‘int64’ is deprecated [-Wdeprecated-declarations]
   bool WaitForNotificationWithTimeout(int64 timeout_in_us) {
                                                          ^
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/notification.h:81:63: warning: ‘int64’ is deprecated [-Wdeprecated-declarations]
                                            int64 timeout_in_us) {
                                                               ^
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:24:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:29,
                 from neural_trees_helpers.h:17,
                 from neural_trees_helpers.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_shape.h:305:22: warning: ‘tensorflow::int64’ is deprecated: Use int64_t instead. [-Wdeprecated-declarations]
   gtl::InlinedVector<int64, 4> dim_sizes() const;
                      ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/types.h:31:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:27,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:26,
                 from neural_trees_helpers.h:17,
                 from neural_trees_helpers.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/platform/default/integral_types.h:29:63: note: declared here
 [[deprecated(""Use int64_t instead."")]] typedef ::std::int64_t int64;
                                                               ^~~~~
In file included from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:25:0,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,
                 from /usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:29,
                 from neural_trees_helpers.h:17,
                 from neural_trees_helpers.cc:15:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h: In member function ‘void tensorflow::internal::MaybeWith32BitIndexingImpl<Eigen::GpuDevice>::operator()(Func, Args&& ...) const’:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:176:25: error: use of ‘auto’ in lambda parameter declaration only available with -std=c++14 or -std=gnu++14
     auto all = [](const auto&... bool_vals) {
                         ^~~~
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:176:34: error: expansion pattern ‘const int&’ contains no argument packs
     auto all = [](const auto&... bool_vals) {
                                  ^~~~~~~~~
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h: In lambda function:
/usr/local/lib/python3.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor_types.h:177:22: error: ‘bool_vals’ was not declared in this scope
       for (bool b : {bool_vals...}) {
                      ^~~~~~~~~
```

What can I do? I would have loved to try it, modify it, and use it in my project. 

Please help"
"When will the authors release the blender dataset support that could reproduce the results in the paper?

Best"
"I am trying to download the rogue-score package for Python, but I cannot. I have tried everything. 

When I use pip, I get this error: 

`C:\Users\onef0>pip install rogue-score
ERROR: Could not find a version that satisfies the requirement rogue-score (from versions: none)
ERROR: No matching distribution found for rogue-score`

Then, when I cloning it from github, I get this error:

`C:\Users\onef0>pip install git+https://github.com/google-research/google-research/tree/master/rouge
Collecting git+https://github.com/google-research/google-research/tree/master/rouge
  Cloning https://github.com/google-research/google-research/tree/master/rouge to c:\users\onef0\appdata\local\temp\pip-req-build-ogcfgrb9
  Running command git clone -q https://github.com/google-research/google-research/tree/master/rouge 'C:\Users\onef0\AppData\Local\Temp\pip-req-build-ogcfgrb9'
  fatal: repository 'https://github.com/google-research/google-research/tree/master/rouge/' not found
WARNING: Discarding git+https://github.com/google-research/google-research/tree/master/rouge. Command errored out with exit status 128: git clone -q https://github.com/google-research/google-research/tree/master/rouge 'C:\Users\onef0\AppData\Local\Temp\pip-req-build-ogcfgrb9' Check the logs for full command output.
ERROR: Command errored out with exit status 128: git clone -q https://github.com/google-research/google-research/tree/master/rouge 'C:\Users\onef0\AppData\Local\Temp\pip-req-build-ogcfgrb9' Check the logs for full command output.`

Finally, I have tried downloading the package files and then installing them manually and this also has not worked:

`C:\Users\onef0>cd C:\Users\onef0\Desktop\rouge_score-0.0.4`

`C:\Users\onef0\Desktop\rouge_score-0.0.4>python setup.py install
Traceback (most recent call last):
  File ""C:\Users\onef0\Desktop\rouge_score-0.0.4\setup.py"", line 1, in <module>
    import setuptools
  File ""C:\Users\onef0\anaconda3\lib\site-packages\setuptools\__init__.py"", line 16, in <module>
    import setuptools.version
  File ""C:\Users\onef0\anaconda3\lib\site-packages\setuptools\version.py"", line 1, in <module>
    import pkg_resources
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\__init__.py"", line 78, in <module>
    __import__('pkg_resources.extern.packaging.requirements')
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\_vendor\packaging\requirements.py"", line 10, in <module>
    from pkg_resources.extern.pyparsing import (  # noqa
  File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 986, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 666, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 565, in module_from_spec
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\extern\__init__.py"", line 52, in create_module
    return self.load_module(spec.name)
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\extern\__init__.py"", line 37, in load_module
    __import__(extant)
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\_vendor\pyparsing.py"", line 4756, in <module>
    _escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\_vendor\pyparsing.py"", line 1284, in setParseAction
    self.parseAction = list(map(_trim_arity, list(fns)))
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\_vendor\pyparsing.py"", line 1066, in _trim_arity
    this_line = extract_stack(limit=2)[-1]
  File ""C:\Users\onef0\anaconda3\lib\site-packages\pkg_resources\_vendor\pyparsing.py"", line 1050, in extract_stack
    frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]
  File ""C:\Users\onef0\anaconda3\lib\traceback.py"", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File ""C:\Users\onef0\anaconda3\lib\traceback.py"", line 366, in extract
    f.line
  File ""C:\Users\onef0\anaconda3\lib\traceback.py"", line 288, in line
    self._line = linecache.getline(self.filename, self.lineno).strip()
  File ""C:\Users\onef0\anaconda3\lib\linecache.py"", line 30, in getline
    lines = getlines(filename, module_globals)
  File ""C:\Users\onef0\anaconda3\lib\linecache.py"", line 46, in getlines
    return updatecache(filename, module_globals)
  File ""C:\Users\onef0\anaconda3\lib\linecache.py"", line 136, in updatecache
    with tokenize.open(fullname) as fp:
AttributeError: module 'tokenize' has no attribute 'open'`

I need this for my thesis, as I am doing text summarization using huggingface transformers and I cannot run my script without it. Any help would be greatly appreciated. Thank you!"
""
"How to get a job In google....with package of 
1 million dollar per annum"
"Hi @hedpeter ,

Thank your sharing of release code.
Btw can you share the llff config for 360 scene?
I tried to use your shared llff config for 360 captured scene, and it didn't work. 
"
"When running:

python3 '/content/drive/MyDrive/tft/script_train_fixed_params.py' $EXPT volatility $OUTPUT_FOLDER '/content/drive/MyDrive/tft/vol' $USE_GPU yes

I get: 

Computing best validation loss
7635/7635 [==============================] - 11s 1ms/sample - loss: 0.9657
Computing test loss
Traceback (most recent call last):
  File ""/content/drive/MyDrive/tft/script_train_fixed_params.py"", line 238, in <module>
    use_testing_mode=True)  # Change to false to use original default params
  File ""/content/drive/MyDrive/tft/script_train_fixed_params.py"", line 155, in main
    targets = data_formatter.format_predictions(output_map[""targets""])
  File ""/content/drive/MyDrive/tft/data_formatters/volatility.py"", line 183, in format_predictions
    output[col] = self._target_scaler.inverse_transform(predictions[col])
  File ""/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py"", line 1022, in inverse_transform
    force_all_finite=""allow-nan"",
  File ""/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py"", line 765, in check_array
    ""if it contains a single sample."".format(array)
ValueError: Expected 2D array, got 1D array instead:
array=[-1.43120418  1.58885804  0.28558148 ... -1.50945972 -0.16713021
 -0.57365613].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
"Hi!
I tried using the CIfar split released in [the repo](https://github.com/google-research/google-research/tree/master/federated_vision_datasets) from [this paper](https://arxiv.org/abs/2003.08082), but the `image_id` does not match the index from the original dataset downloaded from the Cifar website.

@hang-qi do you know how to recover the images from the original dataset using your split?


"
"Hi @hassanhub, sorry for the interruption.

I'm trying to reproduce the result of vatt, but ran into some problem:

In your code, you used the dmvr package to process the video data.
When I run the following command:
`python -m vatt.main --task=finetune --mode=eval --model_dir=/path/to/model --model_arch=vit_base --strategy_type=mirrored`
An error occured:

> ./vatt/data/processing.py"", line 579, in add_audio
> TypeError: add_audio() got an unexpected keyword argument 'preprocessor_builder'

I noticed that **modalities** is imported from **dmvr** package. So I checked the source code [here](https://github.com/deepmind/dmvr/blob/master/dmvr/modalities.py), the **add_audio** function has no argument `preprocessor_builder` indeed.

Besides, There is no argument `sample_rate`, `target_sample_rate` in **add_audio** too, but they are used in ./vatt/data/processing.py"", line 579.

The dmvr package I installed is version 0.0.1.

Is something wrong with the vatt code? Or simply I did something wrong?

Thanks :)
"
"Hi, thanks for your great repository. I have a question: How can use gpu when I train kws_streaming model? (When I do training, I check my gpu usage, and nothing happened). I don't find any param out which talk about it? Can you give me some advices?"
"The Smith (https://github.com/google-research/google-research/tree/master/smith), When i open the url https://github.com/google-research/google-research/tree/master/gwikimatch  and click  https://research.google/pubs/pub44639/ , The research.google can not be opened. How can i visit it. Thanks a lot."
"Hi,

Thanks for sharing your work !

You say in your paper that the batch size is 8 but your code only works with batch size 1. Are you going to release a version that works with batch size > 1 ?"
"Hi @walidk, congratulations for your exciting new papers about iALS! Its regularization scheme seems clever and effective.

Although I'm not sure if this is the right place ask this question, I believe that [a conjugate gradient descent based algorithm](http://rs1.sze.hu/~gtakacs/download/recsys_2011_draft.pdf) to solve iALS problem has

1. exactly the same computational complexity with iCD and iALS++ method. (However, it seems that iCD paper does not  refer to this paper either)
2. highly vectorizable implementation (like the one in [implicit](https://github.com/benfred/implicit))

Is there any reason why this paper is not referenced?"
"Hi, I'm currently trying to resolve this problem when I run the training on GPU.
I am using ubuntu 16.04.6 LTS and a conda environment with:
- python 3.7
- tensorflow 1.15
- tensorflow-graphics 1.0.0
- matplotlib 3.3.0
- CUDA version 10.2

My GPU is a
- NVIDIA GeForce GTX 1080 Ti

I set the env variable  
$LD_LIBRARY_PATH  to /usr/local/cuda-10.2/targets/x86_64-linux/lib
$PATH to /usr/local/cuda-10.2/bin

I read that there is a CUDA bug in version 10.0 in #162, but I get a similar error even with CUDA 10.2 .
The training starts ok but shortly after I get:
- Model diverged with loss = NaN.
and then
- tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training

This error as far as I know, only happens when I run the training using the GPU.


I really appreciate any help, thanks.
"
"I've been training the probabilistic model and the deterministic one, and I noticed that the training in the deterministic one is much slower than the probabilistic. Is this behavior expected?
Thanks in advance"
"I try to use [HITNET](https://github.com/google-research/google-research/tree/master/hitnet).

GPU : Nvidia GeForce RTX 2060 SUPER
Memory Size : 7981MiB

```
root@2c4b59e82551:/workspace/hitnet# cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
```

I attempted stereo matching for MiddleBury as follows:
```
bash predict_middlebury.sh
```
Then It will omit an error like the following:
```:bash
(ellipsis)

2021-10-19 05:19:40.789846: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 482344960 totalling 920.00MiB
2021-10-19 05:19:40.789866: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 1543503872 totalling 2.88GiB
2021-10-19 05:19:40.789886: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 4.37GiB
2021-10-19 05:19:40.789905: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 6694947840 memory_limit_: 6694948044 available bytes: 204 curr_region_allocation_bytes_: 8589934592
2021-10-19 05:19:40.789937: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats:
Limit:                  6694948044
InUse:                  4697149440
MaxInUse:               5784235008
NumAllocs:                    1400
MaxAllocSize:           1946157056

2021-10-19 05:19:40.790006: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *************************_*****************__*****_________************************____________*****
2021-10-19 05:19:40.790060: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at conv_grad_input_ops.cc:1063 : Resource exhausted: OOM when allocating tensor with shape[2,32,2048,2944] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn
    target_list, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[2,32,2048,2944] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node graph/fe_shared/conv_upsample_0/conv2d_transpose}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[graph/reference_output_disparity/_155]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[2,32,2048,2944] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node graph/fe_shared/conv_upsample_0/conv2d_transpose}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""predict_test.py"", line 301, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""predict_test.py"", line 255, in main
    reference_disparity = sess.run(reference, feed_dict=feed_dict)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run
    run_metadata)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[2,32,2048,2944] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node graph/fe_shared/conv_upsample_0/conv2d_transpose (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[graph/reference_output_disparity/_155]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[2,32,2048,2944] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node graph/fe_shared/conv_upsample_0/conv2d_transpose (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

Original stack trace for 'graph/fe_shared/conv_upsample_0/conv2d_transpose':
  File ""predict_test.py"", line 301, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""predict_test.py"", line 238, in main
    tf.import_graph_def(graph_def, name='graph')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 513, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def
    producer_op_list=producer_op_list)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 517, in _import_graph_def_internal
    _ProcessNewOps(graph)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 243, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3561, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3561, in <listcomp>
    for c_op in c_api_util.new_tf_operations(self)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 3451, in _create_op_from_tf_operation
    ret = Operation(c_op, self)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
```

So I changed the code as follows and tried again, but I still get the same error.

```:bash
# on predict_middlebury.sh
- MODEL_NAME=""middlebury_d400.pb""
+ MODEL_NAME=""middlebury_d160.pb""
```
and 
```:python
# on predict.py
config = tf.ConfigProto(
          gpu_options=tf.GPUOptions(
              per_process_gpu_memory_fraction=0.8,
              allow_growth=True
              )
          )
```

I think it is generally known that the `Resource exhausted` problem can be solved by lowering the batch size, but I assume that Hitnet does not have a batch size. Is this assumption correct?

I think the reason Hitnet can't run is because it's not avoiding `Resource exhausted`, does anyone know how to work around this problem?
"
"@rybakov I have been trying to resume the CRNN model training by loading the ""best_weights.h5"" in HDF5 format. 
I have changed the [train.py](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py) code by adding 
`model.save_weights(flags.train_dir + 'best_weights.h5')`  after [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L201) 
and loading back the model using `model.load_weights(flags.train_dir + 'best_weights.h5')` [here](https://github.com/google-research/google-research/blob/master/kws_streaming/train/train.py#L129)

But I am getting an error saying `AttributeError: 'GRU' object has no attribute 'reset_after'`

```
~\anaconda3\envs\tf-nightly\lib\site-packages\keras\engine\training_v1.py in load_weights(self, filepath, by_name, skip_mismatch)
    212         raise ValueError('Load weights is not yet supported with TPUStrategy '
    213                          'with steps_per_run greater than 1.')
--> 214     return super(Model, self).load_weights(filepath, by_name, skip_mismatch)
    215 
    216   @tf.__internal__.tracking.no_automatic_dependency_tracking

~\anaconda3\envs\tf-nightly\lib\site-packages\keras\utils\traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

~\anaconda3\envs\tf-nightly\lib\site-packages\keras\saving\hdf5_format.py in _convert_rnn_weights(layer, weights)
    561     if target_class == 'CuDNNGRU':
    562       target = 'CuDNNGRU'
--> 563     elif layer.reset_after:
    564       target = 'GRU(reset_after=True)'
    565     else:

AttributeError: 'GRU' object has no attribute 'reset_after'
```
Previously I used SVDF model in which there was no such error while reloading the ""best_weights.h5"" , the weights were loading successfully for the SVDF model. 

I am using a big custom dataset, that's why I need to resume the model training from previously left accuracy. Can you @rybakov please help me to fix this for CRNN model."
"Hi,

I tried running infer.py in pr_vipe using the checkpoint provided here. [https://sites.google.com/view/pr-vipe](url)

I get this error.

`tensorflow.python.framework.errors_impl.DataLossError:` Unable to open table file checkpoint_path\model.ckpt-02013963.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?`

I am not sure if I am giving the wrong input file or some wrong arguments."
"I am getting the OOM error (Out Of Memory) on running the script for Coltran spatial upsampler.

Getting the same error for the imagenet dataset of 100, 100, 50 and even 10 images for both modes (colorize, recolorize).

The `/configs/spatial_upsampler.py` file already has `config.batch_size = 1`, so cannot reduce further.
The first 2 steps colourizer and color_upsampler are running fine.

Command used
```python
%%time
!rm -rf $IMG_DIR/.ipynb_checkpoints/ $STORE_DIR/stage2/.ipynb_checkpoints
!python -m coltran.custom_colorize --config=coltran/configs/spatial_upsampler.py --logdir=$LOGDIR/spatial_upsampler --img_dir=$IMG_DIR --store_dir=$STORE_DIR --gen_data_dir=$STORE_DIR/stage2 --mode=$MODE
```

env variables 
```
os.environ[""LOGDIR""] = ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/logdir/coltran""
os.environ['IMG_DIR'] = ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/imagenet_50/RGB/train"" 
os.environ['STORE_DIR'] = ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/op_imgnet50_recolor_01""
os.environ['MODE'] = ""recolorize""
```

I am running this on Google Colab

`nvidia-smi`

```
Wed Sep 29 06:34:34 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |
| N/A   40C    P0    65W / 149W |      0MiB / 11441MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

Error
```
2021-09-29 06:33:47.409836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:47.895533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:47.896507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:47.906032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:47.906959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:47.907786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:53.182315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:53.183181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:53.184079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-29 06:33:53.185008: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2021-09-29 06:33:53.185080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7
2021-09-29 06:33:54.140517: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-09-29 06:34:01.320702: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/moving_averages.py:457: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0929 06:34:01.986204 140536467093376 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/moving_averages.py:457: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I0929 06:34:02.988495 140536467093376 train_utils.py:91] Built with exponential moving average.
I0929 06:34:02.995397 140536467093376 train_utils.py:185] Restoring from /content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/logdir/coltran/spatial_upsampler.
I0929 06:34:10.659995 140536467093376 custom_colorize.py:207] Producing sample after 300000 training steps.
I0929 06:34:10.660418 140536467093376 custom_colorize.py:210] 10
2021-09-29 06:34:24.825106: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.25GiB (rounded to 1342177280)requested by op Softmax
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2021-09-29 06:34:24.825193: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2021-09-29 06:34:24.825235: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): 	Total Chunks: 25, Chunks in use: 25. 6.2KiB allocated for chunks. 6.2KiB in use in bin. 113B client-requested in use in bin.
2021-09-29 06:34:24.825262: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): 	Total Chunks: 1, Chunks in use: 0. 768B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825290: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): 	Total Chunks: 3, Chunks in use: 3. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 3.0KiB client-requested in use in bin.
2021-09-29 06:34:24.825318: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): 	Total Chunks: 75, Chunks in use: 74. 151.0KiB allocated for chunks. 149.0KiB in use in bin. 148.0KiB client-requested in use in bin.
2021-09-29 06:34:24.825340: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825395: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): 	Total Chunks: 13, Chunks in use: 12. 104.0KiB allocated for chunks. 96.0KiB in use in bin. 96.0KiB client-requested in use in bin.
2021-09-29 06:34:24.825421: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825452: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825473: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): 	Total Chunks: 2, Chunks in use: 0. 247.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825494: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825514: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825536: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): 	Total Chunks: 10, Chunks in use: 8. 5.50MiB allocated for chunks. 4.50MiB in use in bin. 4.00MiB client-requested in use in bin.
2021-09-29 06:34:24.825559: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): 	Total Chunks: 77, Chunks in use: 76. 79.00MiB allocated for chunks. 78.00MiB in use in bin. 77.19MiB client-requested in use in bin.
2021-09-29 06:34:24.825581: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): 	Total Chunks: 3, Chunks in use: 3. 7.75MiB allocated for chunks. 7.75MiB in use in bin. 7.75MiB client-requested in use in bin.
2021-09-29 06:34:24.825603: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): 	Total Chunks: 1, Chunks in use: 0. 7.50MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825623: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825645: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): 	Total Chunks: 1, Chunks in use: 0. 26.75MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825665: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825687: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): 	Total Chunks: 1, Chunks in use: 0. 73.50MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825709: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): 	Total Chunks: 5, Chunks in use: 0. 829.50MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-29 06:34:24.825730: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): 	Total Chunks: 13, Chunks in use: 9. 9.56GiB allocated for chunks. 6.56GiB in use in bin. 6.56GiB client-requested in use in bin.
2021-09-29 06:34:24.825754: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 1.25GiB was 256.00MiB, Chunk State: 
2021-09-29 06:34:24.825782: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 384.00MiB | Requested Size: 2.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 640.00MiB | Requested Size: 640.00MiB | in_use: 1 | bin_num: -1
2021-09-29 06:34:24.825807: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 640.00MiB | Requested Size: 640.00MiB | in_use: 0 | bin_num: 20, next:   Size: 640.00MiB | Requested Size: 640.00MiB | in_use: 1 | bin_num: -1
2021-09-29 06:34:24.825831: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 896.00MiB | Requested Size: 2.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 1.25GiB | Requested Size: 1.25GiB | in_use: 1 | bin_num: -1
2021-09-29 06:34:24.825855: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 1.12GiB | Requested Size: 2.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 1.25GiB | Requested Size: 1.25GiB | in_use: 1 | bin_num: -1
2021-09-29 06:34:24.825874: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 2097152
2021-09-29 06:34:24.825896: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ae0000 of size 1280 next 1
2021-09-29 06:34:24.825915: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ae0500 of size 256 next 2
2021-09-29 06:34:24.825934: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ae0600 of size 256 next 3
2021-09-29 06:34:24.825953: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ae0700 of size 256 next 4
2021-09-29 06:34:24.825971: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ae0800 of size 256 next 5
2021-09-29 06:34:24.826010: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ae0900 of size 786432 next 6
2021-09-29 06:34:24.826044: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ba0900 of size 256 next 10
2021-09-29 06:34:24.826092: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ba0a00 of size 786432 next 13
2021-09-29 06:34:24.826110: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c60a00 of size 256 next 11
2021-09-29 06:34:24.826127: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c60b00 of size 256 next 9
2021-09-29 06:34:24.826160: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c60c00 of size 256 next 15
2021-09-29 06:34:24.826192: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c60d00 of size 256 next 16
2021-09-29 06:34:24.826211: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c60e00 of size 256 next 21
2021-09-29 06:34:24.826229: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c60f00 of size 256 next 22
2021-09-29 06:34:24.826262: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61000 of size 256 next 26
2021-09-29 06:34:24.826294: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61100 of size 256 next 27
2021-09-29 06:34:24.826318: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61200 of size 1024 next 28
2021-09-29 06:34:24.826336: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61600 of size 256 next 206
2021-09-29 06:34:24.826354: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 703c61700 of size 768 next 30
2021-09-29 06:34:24.826383: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61a00 of size 256 next 33
2021-09-29 06:34:24.826402: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61b00 of size 256 next 34
2021-09-29 06:34:24.826420: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c61c00 of size 8192 next 38
2021-09-29 06:34:24.826456: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c63c00 of size 256 next 39
2021-09-29 06:34:24.826474: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c63d00 of size 256 next 40
2021-09-29 06:34:24.826492: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c63e00 of size 256 next 48
2021-09-29 06:34:24.826509: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c63f00 of size 256 next 50
2021-09-29 06:34:24.826527: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c64000 of size 2048 next 47
2021-09-29 06:34:24.826545: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c64800 of size 2048 next 52
2021-09-29 06:34:24.826563: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c65000 of size 256 next 49
2021-09-29 06:34:24.826580: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c65100 of size 256 next 46
2021-09-29 06:34:24.826598: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c65200 of size 2048 next 54
2021-09-29 06:34:24.826615: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c65a00 of size 2048 next 56
2021-09-29 06:34:24.826639: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c66200 of size 2048 next 57
2021-09-29 06:34:24.826657: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c66a00 of size 2048 next 58
2021-09-29 06:34:24.826675: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c67200 of size 8192 next 44
2021-09-29 06:34:24.826692: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c69200 of size 2048 next 59
2021-09-29 06:34:24.826710: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c69a00 of size 2048 next 65
2021-09-29 06:34:24.826728: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6a200 of size 2048 next 66
2021-09-29 06:34:24.826745: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6aa00 of size 2048 next 69
2021-09-29 06:34:24.826763: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6b200 of size 2048 next 71
2021-09-29 06:34:24.826787: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6ba00 of size 2048 next 72
2021-09-29 06:34:24.826806: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6c200 of size 8192 next 64
2021-09-29 06:34:24.826823: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6e200 of size 2048 next 73
2021-09-29 06:34:24.826841: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6ea00 of size 2048 next 79
2021-09-29 06:34:24.826858: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6f200 of size 2048 next 80
2021-09-29 06:34:24.826876: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c6fa00 of size 2048 next 82
2021-09-29 06:34:24.826893: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c70200 of size 2048 next 84
2021-09-29 06:34:24.826910: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c70a00 of size 2048 next 85
2021-09-29 06:34:24.826928: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c71200 of size 8192 next 78
2021-09-29 06:34:24.826946: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c73200 of size 2048 next 86
2021-09-29 06:34:24.826964: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c73a00 of size 2048 next 92
2021-09-29 06:34:24.826989: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c74200 of size 2048 next 93
2021-09-29 06:34:24.827024: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c74a00 of size 2048 next 95
2021-09-29 06:34:24.827041: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c75200 of size 2048 next 97
2021-09-29 06:34:24.827059: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c75a00 of size 2048 next 98
2021-09-29 06:34:24.827077: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c76200 of size 8192 next 91
2021-09-29 06:34:24.827095: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c78200 of size 2048 next 99
2021-09-29 06:34:24.827112: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c78a00 of size 2048 next 105
2021-09-29 06:34:24.827130: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c79200 of size 2048 next 106
2021-09-29 06:34:24.827148: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c79a00 of size 2048 next 108
2021-09-29 06:34:24.827166: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 703c7a200 of size 2048 next 110
2021-09-29 06:34:24.827184: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7aa00 of size 2048 next 111
2021-09-29 06:34:24.827203: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 703c7b200 of size 8192 next 104
2021-09-29 06:34:24.827220: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7d200 of size 2048 next 112
2021-09-29 06:34:24.827239: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7da00 of size 2048 next 118
2021-09-29 06:34:24.827271: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7e200 of size 2048 next 119
2021-09-29 06:34:24.827307: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7ea00 of size 2048 next 121
2021-09-29 06:34:24.827341: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7f200 of size 2048 next 123
2021-09-29 06:34:24.827359: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c7fa00 of size 3072 next 117
2021-09-29 06:34:24.827387: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c80600 of size 256 next 126
2021-09-29 06:34:24.827405: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c80700 of size 256 next 12
2021-09-29 06:34:24.827428: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 703c80800 of size 126208 next 208
2021-09-29 06:34:24.827447: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c9f500 of size 2048 next 209
2021-09-29 06:34:24.827465: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703c9fd00 of size 2048 next 211
2021-09-29 06:34:24.827482: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca0500 of size 2048 next 213
2021-09-29 06:34:24.827499: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca0d00 of size 2048 next 215
2021-09-29 06:34:24.827517: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca1500 of size 2048 next 217
2021-09-29 06:34:24.827535: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca1d00 of size 2048 next 219
2021-09-29 06:34:24.827552: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca2500 of size 2048 next 221
2021-09-29 06:34:24.827570: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca2d00 of size 2048 next 223
2021-09-29 06:34:24.827587: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca3500 of size 2048 next 225
2021-09-29 06:34:24.827604: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca3d00 of size 2048 next 227
2021-09-29 06:34:24.827622: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca4500 of size 2048 next 229
2021-09-29 06:34:24.827639: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca4d00 of size 2048 next 231
2021-09-29 06:34:24.827656: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca5500 of size 2048 next 234
2021-09-29 06:34:24.827673: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca5d00 of size 1024 next 236
2021-09-29 06:34:24.827691: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca6100 of size 2048 next 238
2021-09-29 06:34:24.827708: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca6900 of size 2048 next 239
2021-09-29 06:34:24.827726: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca7100 of size 2048 next 240
2021-09-29 06:34:24.827744: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca7900 of size 2048 next 241
2021-09-29 06:34:24.827761: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca8100 of size 2048 next 242
2021-09-29 06:34:24.827778: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca8900 of size 2048 next 243
2021-09-29 06:34:24.827796: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca9100 of size 2048 next 244
2021-09-29 06:34:24.827813: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ca9900 of size 2048 next 245
2021-09-29 06:34:24.827830: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703caa100 of size 2048 next 246
2021-09-29 06:34:24.827847: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703caa900 of size 2048 next 247
2021-09-29 06:34:24.827864: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cab100 of size 2048 next 248
2021-09-29 06:34:24.827881: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cab900 of size 2048 next 249
2021-09-29 06:34:24.827898: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cac100 of size 2048 next 250
2021-09-29 06:34:24.827915: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cac900 of size 2048 next 251
2021-09-29 06:34:24.827934: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cad100 of size 2048 next 252
2021-09-29 06:34:24.827953: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cad900 of size 2048 next 253
2021-09-29 06:34:24.827971: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cae100 of size 2048 next 254
2021-09-29 06:34:24.827988: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cae900 of size 2048 next 255
2021-09-29 06:34:24.828011: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703caf100 of size 2048 next 256
2021-09-29 06:34:24.901942: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703caf900 of size 2048 next 257
2021-09-29 06:34:24.901999: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb0100 of size 2048 next 258
2021-09-29 06:34:24.902042: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb0900 of size 2048 next 259
2021-09-29 06:34:24.902076: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb1100 of size 2048 next 260
2021-09-29 06:34:24.902096: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb1900 of size 2048 next 261
2021-09-29 06:34:24.902116: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb2100 of size 8192 next 267
2021-09-29 06:34:24.902137: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb4100 of size 8192 next 272
2021-09-29 06:34:24.902159: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb6100 of size 8192 next 276
2021-09-29 06:34:24.902181: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cb8100 of size 8192 next 281
2021-09-29 06:34:24.902218: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cba100 of size 8192 next 286
2021-09-29 06:34:24.902241: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cbc100 of size 8192 next 291
2021-09-29 06:34:24.902264: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cbe100 of size 2048 next 293
2021-09-29 06:34:24.902285: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cbe900 of size 2048 next 25
2021-09-29 06:34:24.902321: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703cbf100 of size 8192 next 29
2021-09-29 06:34:24.902358: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 703cc1100 of size 126720 next 18446744073709551615
2021-09-29 06:34:24.902399: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 4194304
2021-09-29 06:34:24.902443: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ce0000 of size 256 next 8
2021-09-29 06:34:24.902466: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 703ce0100 of size 524288 next 18
2021-09-29 06:34:24.902488: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703d60100 of size 524288 next 17
2021-09-29 06:34:24.902509: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703de0100 of size 1048576 next 42
2021-09-29 06:34:24.902532: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 703ee0100 of size 2096896 next 18446744073709551615
2021-09-29 06:34:24.902553: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 67108864
2021-09-29 06:34:24.902594: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 704ae0000 of size 7864320 next 204
2021-09-29 06:34:24.902623: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 705260000 of size 3932160 next 203
2021-09-29 06:34:24.902645: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 705620000 of size 28049408 next 182
2021-09-29 06:34:24.902666: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7070e0000 of size 1048576 next 210
2021-09-29 06:34:24.902687: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7071e0000 of size 1048576 next 212
2021-09-29 06:34:24.902708: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7072e0000 of size 1048576 next 214
2021-09-29 06:34:24.902739: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7073e0000 of size 1048576 next 216
2021-09-29 06:34:24.902760: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7074e0000 of size 1048576 next 218
2021-09-29 06:34:24.902782: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7075e0000 of size 1048576 next 220
2021-09-29 06:34:24.902802: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7076e0000 of size 1048576 next 222
2021-09-29 06:34:24.902823: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7077e0000 of size 1048576 next 224
2021-09-29 06:34:24.902845: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7078e0000 of size 1048576 next 226
2021-09-29 06:34:24.902868: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7079e0000 of size 1048576 next 228
2021-09-29 06:34:24.902889: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 707ae0000 of size 1048576 next 230
2021-09-29 06:34:24.902926: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 707be0000 of size 1048576 next 232
2021-09-29 06:34:24.902964: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 707ce0000 of size 1572864 next 233
2021-09-29 06:34:24.902997: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 707e60000 of size 2097152 next 235
2021-09-29 06:34:24.903035: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 708060000 of size 524288 next 237
2021-09-29 06:34:24.903059: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7080e0000 of size 524288 next 262
2021-09-29 06:34:24.903082: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 708160000 of size 524288 next 263
2021-09-29 06:34:24.903105: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7081e0000 of size 1048576 next 264
2021-09-29 06:34:24.903128: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7082e0000 of size 1048576 next 265
2021-09-29 06:34:24.903151: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7083e0000 of size 1048576 next 266
2021-09-29 06:34:24.903174: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7084e0000 of size 1048576 next 268
2021-09-29 06:34:24.903197: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7085e0000 of size 1048576 next 269
2021-09-29 06:34:24.903219: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7086e0000 of size 1048576 next 270
2021-09-29 06:34:24.903242: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7087e0000 of size 1048576 next 271
2021-09-29 06:34:24.903280: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7088e0000 of size 1048576 next 273
2021-09-29 06:34:24.903318: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7089e0000 of size 1048576 next 18446744073709551615
2021-09-29 06:34:24.903341: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 134217728
2021-09-29 06:34:24.903364: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 708ae0000 of size 134217728 next 18446744073709551615
2021-09-29 06:34:24.903385: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 268435456
2021-09-29 06:34:24.903425: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 710ae0000 of size 135266304 next 51
2021-09-29 06:34:24.903450: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 718be0000 of size 1048576 next 53
2021-09-29 06:34:24.903475: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 718ce0000 of size 1048576 next 55
2021-09-29 06:34:24.903496: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 718de0000 of size 1048576 next 60
2021-09-29 06:34:24.903519: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 718ee0000 of size 1048576 next 61
2021-09-29 06:34:24.903541: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 718fe0000 of size 1048576 next 62
2021-09-29 06:34:24.903578: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7190e0000 of size 1048576 next 70
2021-09-29 06:34:24.903600: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7191e0000 of size 1048576 next 63
2021-09-29 06:34:24.903622: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7192e0000 of size 1048576 next 68
2021-09-29 06:34:24.903644: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7193e0000 of size 1048576 next 74
2021-09-29 06:34:24.903665: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7194e0000 of size 1048576 next 75
2021-09-29 06:34:24.903687: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7195e0000 of size 1048576 next 76
2021-09-29 06:34:24.903708: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7196e0000 of size 1048576 next 83
2021-09-29 06:34:24.903729: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7197e0000 of size 1048576 next 77
2021-09-29 06:34:24.903752: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7198e0000 of size 524288 next 296
2021-09-29 06:34:24.903773: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 719960000 of size 524288 next 81
2021-09-29 06:34:24.903795: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7199e0000 of size 1048576 next 87
2021-09-29 06:34:24.903816: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 719ae0000 of size 1048576 next 88
2021-09-29 06:34:24.903837: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 719be0000 of size 1048576 next 89
2021-09-29 06:34:24.903859: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 719ce0000 of size 1048576 next 96
2021-09-29 06:34:24.903881: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 719de0000 of size 1048576 next 90
2021-09-29 06:34:24.903902: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 719ee0000 of size 1048576 next 94
2021-09-29 06:34:24.903925: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 719fe0000 of size 1048576 next 100
2021-09-29 06:34:24.903946: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a0e0000 of size 1048576 next 101
2021-09-29 06:34:24.903967: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a1e0000 of size 1048576 next 102
2021-09-29 06:34:24.903989: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a2e0000 of size 1048576 next 109
2021-09-29 06:34:24.904022: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a3e0000 of size 1048576 next 103
2021-09-29 06:34:24.904062: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a4e0000 of size 1048576 next 107
2021-09-29 06:34:24.904085: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a5e0000 of size 1048576 next 113
2021-09-29 06:34:24.904107: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a6e0000 of size 1048576 next 114
2021-09-29 06:34:24.904130: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a7e0000 of size 1048576 next 115
2021-09-29 06:34:24.904153: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a8e0000 of size 1048576 next 122
2021-09-29 06:34:24.904176: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71a9e0000 of size 1048576 next 116
2021-09-29 06:34:24.904199: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71aae0000 of size 1048576 next 120
2021-09-29 06:34:24.904222: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71abe0000 of size 1048576 next 274
2021-09-29 06:34:24.904245: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71ace0000 of size 1048576 next 275
2021-09-29 06:34:24.904267: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71ade0000 of size 1048576 next 277
2021-09-29 06:34:24.904290: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71aee0000 of size 1048576 next 278
2021-09-29 06:34:24.904313: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71afe0000 of size 1048576 next 279
2021-09-29 06:34:24.904350: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b0e0000 of size 1048576 next 280
2021-09-29 06:34:24.904387: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b1e0000 of size 1048576 next 282
2021-09-29 06:34:24.904423: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b2e0000 of size 1048576 next 283
2021-09-29 06:34:24.904449: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b3e0000 of size 1048576 next 284
2021-09-29 06:34:24.904473: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b4e0000 of size 1048576 next 285
2021-09-29 06:34:24.904494: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b5e0000 of size 1048576 next 287
2021-09-29 06:34:24.904516: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b6e0000 of size 1048576 next 288
2021-09-29 06:34:24.904538: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b7e0000 of size 1048576 next 289
2021-09-29 06:34:24.904559: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b8e0000 of size 1048576 next 290
2021-09-29 06:34:24.904581: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71b9e0000 of size 1048576 next 292
2021-09-29 06:34:24.904603: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71bae0000 of size 1048576 next 294
2021-09-29 06:34:24.904625: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71bbe0000 of size 1572864 next 295
2021-09-29 06:34:24.904647: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71bd60000 of size 2097152 next 125
2021-09-29 06:34:24.904669: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71bf60000 of size 1048576 next 37
2021-09-29 06:34:24.904691: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 71c060000 of size 1048576 next 32
2021-09-29 06:34:24.904712: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 71c160000 of size 77070336 next 18446744073709551615
2021-09-29 06:34:24.904759: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 268435456
2021-09-29 06:34:24.904795: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 720ae0000 of size 524288 next 35
2021-09-29 06:34:24.904818: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 720b60000 of size 1048576 next 23
2021-09-29 06:34:24.904840: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 720c60000 of size 1048576 next 124
2021-09-29 06:34:24.904861: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 720d60000 of size 1048576 next 129
2021-09-29 06:34:24.904882: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 720e60000 of size 264765440 next 18446744073709551615
2021-09-29 06:34:24.904903: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 536870912
2021-09-29 06:34:24.904925: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 730ae0000 of size 335544320 next 205
2021-09-29 06:34:24.904947: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 744ae0000 of size 201326592 next 18446744073709551615
2021-09-29 06:34:24.904969: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 1073741824
2021-09-29 06:34:24.904991: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 750ae0000 of size 671088640 next 202
2021-09-29 06:34:24.905024: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 778ae0000 of size 402653184 next 18446744073709551615
2021-09-29 06:34:24.905047: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 2147483648
2021-09-29 06:34:24.905069: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7914e0000 of size 671088640 next 200
2021-09-29 06:34:24.905091: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7b94e0000 of size 671088640 next 201
2021-09-29 06:34:24.905112: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7e14e0000 of size 671088640 next 198
2021-09-29 06:34:24.905133: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 8094e0000 of size 134217728 next 18446744073709551615
2021-09-29 06:34:24.905154: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 4294967296
2021-09-29 06:34:24.905176: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 8114e0000 of size 671088640 next 197
2021-09-29 06:34:24.905212: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 8394e0000 of size 671088640 next 196
2021-09-29 06:34:24.905234: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 8614e0000 of size 671088640 next 195
2021-09-29 06:34:24.905258: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 8894e0000 of size 1342177280 next 193
2021-09-29 06:34:24.905280: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 8d94e0000 of size 939524096 next 18446744073709551615
2021-09-29 06:34:24.905301: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 2547712000
2021-09-29 06:34:24.905323: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 9114e0000 of size 1342177280 next 191
2021-09-29 06:34:24.905346: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 9614e0000 of size 1205534720 next 18446744073709551615
2021-09-29 06:34:24.905368: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: 
2021-09-29 06:34:24.905426: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 25 Chunks of size 256 totalling 6.2KiB
2021-09-29 06:34:24.905453: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1024 totalling 2.0KiB
2021-09-29 06:34:24.905479: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB
2021-09-29 06:34:24.905504: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 73 Chunks of size 2048 totalling 146.0KiB
2021-09-29 06:34:24.905529: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 3072 totalling 3.0KiB
2021-09-29 06:34:24.905553: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 12 Chunks of size 8192 totalling 96.0KiB
2021-09-29 06:34:24.905577: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 524288 totalling 3.00MiB
2021-09-29 06:34:24.905601: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 786432 totalling 1.50MiB
2021-09-29 06:34:24.905625: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 73 Chunks of size 1048576 totalling 73.00MiB
2021-09-29 06:34:24.905648: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1572864 totalling 3.00MiB
2021-09-29 06:34:24.905671: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 2096896 totalling 2.00MiB
2021-09-29 06:34:24.905695: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 2097152 totalling 4.00MiB
2021-09-29 06:34:24.905717: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 3932160 totalling 3.75MiB
2021-09-29 06:34:24.905742: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 335544320 totalling 320.00MiB
2021-09-29 06:34:24.905766: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 671088640 totalling 3.75GiB
2021-09-29 06:34:24.905789: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1342177280 totalling 2.50GiB
2021-09-29 06:34:24.905812: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 6.65GiB
2021-09-29 06:34:24.905834: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 11345264640 memory_limit_: 11345264640 available bytes: 0 curr_region_allocation_bytes_: 17179869184
2021-09-29 06:34:24.905892: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: 
Limit:                     11345264640
InUse:                      7141325056
MaxInUse:                   8483502336
NumAllocs:                        1162
MaxAllocSize:               1342177280
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-09-29 06:34:24.905930: W tensorflow/core/common_runtime/bfc_allocator.cc:468] *__**_****_*******________********************************************_______*************__________
2021-09-29 06:34:24.905992: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at softmax_op_gpu.cu.cc:219 : Resource exhausted: OOM when allocating tensor with shape[5,256,256,4,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/custom_colorize.py"", line 244, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.7/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/custom_colorize.py"", line 227, in main
    out = model.sample(gray_cond=gray, inputs=prev_gen, mode='argmax')
  File ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/models/upsampler.py"", line 254, in sample
    logits = self.upsampler(inputs, gray_cond, training=False)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/models/upsampler.py"", line 245, in upsampler
    context = self.encoder(channel, training=training)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/models/layers.py"", line 669, in call
    output = layer(inputs)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/content/drive/MyDrive/Colab_Work/HONORS/ColorTrans/coltran_pretrained/google-research/coltran/models/layers.py"", line 612, in call
    weights = tf.nn.softmax(alphas)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 3820, in softmax_v2
    return _wrap_2d_function(logits, gen_nn_ops.softmax, axis, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py"", line 3739, in _wrap_2d_function
    return compute_op(inputs, name=name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 10864, in softmax
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6941, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[5,256,256,4,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]
CPU times: user 370 ms, sys: 82.2 ms, total: 453 ms
Wall time: 1min
```

Please guide me on how to solve this."
"Hi, 
I followed kws_streaming/models/cnn_test.py.  Generate ""feature.npz"" to prepare calibration_data. 
[feature.zip](https://github.com/google-research/google-research/files/7242587/feature.zip)

I used the code below to convert the stream_state_external model to quantize model. But the final model I got is still float32. Is there anything I did wrong?  
quantize_opt_for_size_tflite_stream_state_external folder:  [stream_state_external.zip](https://github.com/google-research/google-research/files/7242640/stream_state_external.zip)

      npzfile = np.load(""feature.npz"",allow_pickle=True)
      ttt=npzfile.f.arr_0
      calibration_data = prepare_calibration_data(model,100, ttt)
      def representative_dataset(dtype):
        def _representative_dataset_gen():
            for i in range(len(calibration_data)):
                yield [
                    calibration_data[i][0].astype(dtype),  # input audio packet
                    calibration_data[i][1].astype(dtype),  # conv state
                    calibration_data[i][2].astype(dtype)  # flatten state
                    ]
        return _representative_dataset_gen


        with quantize.quantize_scope():
            fd.write(
                utils.model_to_tflite(sess=sess, model_non_stream=model, flags=flags, mode=mode, save_model_path=path_model,
                                optimizations=optimizations,inference_type=tf.int8,experimental_new_quantizer=True,
                                representative_dataset=representative_dataset(np.float32)))

Convert parameters: 

  converter.inference_type = inference_type
  converter.experimental_new_quantizer = experimental_new_quantizer
  converter.experimental_enable_resource_variables = True
  if representative_dataset is not None:
    converter.representative_dataset = representative_dataset

  converter.target_spec.supported_ops = [
      tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS
  ]
  converter.inference_input_type = tf.int8
  converter.inference_output_type = tf.int8
  converter.allow_custom_ops = True
  if optimizations:
    converter.optimizations = optimizations
  tflite_model = converter.convert()"
"@rybakov  
I am trying to train the CRNN model on custom dataset having approximately 30,000 .wav files (SR=16KHz). The duration of the audios are within 4 seconds.
I have changed the [clip_duration_ms](https://github.com/google-research/google-research/blob/master/kws_streaming/models/model_params.py#L36) to 4000 ms , [wanted_words](https://github.com/google-research/google-research/blob/e9ca68693aff711f830f196b2c9a6563fdd520ee/kws_streaming/models/model_params.py#L32) as per my dataset and changed the speech extraction properties to 4 times of default parameters.
Some of the flags are listed below
```
# set speech feature extractor properties
FLAGS.mel_upper_edge_hertz = 7600
FLAGS.window_size_ms = 160.0
FLAGS.window_stride_ms = 80.0
FLAGS.mel_num_bins = 160
FLAGS.dct_num_features = 80
FLAGS.feature_type = 'mfcc_tf'
FLAGS.preprocess = 'raw'
FLAGS.causal_data_frame_padding = 0
FLAGS.use_tf_fft = False
FLAGS.mel_non_zero_only = not FLAGS.use_tf_fft

# set training settings
FLAGS.train = 1
FLAGS.how_many_training_steps = '5000,5000'
FLAGS.learning_rate = '0.001,0.0005'
FLAGS.lr_schedule = 'exp'
FLAGS.verbosity = logging.INFO

# data augmentation parameters
FLAGS.resample = 0.15
FLAGS.time_shift_ms = 100
FLAGS.use_spec_augment = 0
FLAGS.time_masks_number = 2
FLAGS.time_mask_max_size = 10
FLAGS.frequency_masks_number = 2
FLAGS.frequency_mask_max_size = 15
FLAGS.pick_deterministically = 0

# CRNN model paramters
 FLAGS.cnn_filters = '16,16'
 FLAGS.cnn_kernel_size = '(3,3),(5,3)'
 FLAGS.cnn_act = ""'relu','relu'""
 FLAGS.cnn_dilation_rate = '(1,1),(1,1)'
 FLAGS.cnn_strides = '(1,1),(1,1)'
 FLAGS.gru_units = '256'
 FLAGS.return_sequences = '0'
 FLAGS.dropout1 = 0.1
 FLAGS.units1 = '128,256'
 FLAGS.act1 = ""'linear','relu'""
 FLAGS.stateful = 1
```

The model summary is
```

Model: ""model_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(100, 64000)]            0         
_________________________________________________________________
speech_features_1 (SpeechFea (100, 49, 80)             0         
_________________________________________________________________
tf_op_layer_ExpandDims_1 (Te [(100, 49, 80, 1)]        0         
_________________________________________________________________
stream_3 (Stream)            (100, 47, 78, 16)         160       
_________________________________________________________________
stream_4 (Stream)            (100, 43, 76, 16)         3856      
_________________________________________________________________
reshape_1 (Reshape)          (100, 43, 1216)           0         
_________________________________________________________________
gru_1 (GRU)                  (100, 1, 256)             1132032   
_________________________________________________________________
stream_5 (Stream)            (100, 256)                0         
_________________________________________________________________
dropout_1 (Dropout)          (100, 256)                0         
_________________________________________________________________
dense_3 (Dense)              (100, 128)                32896     
_________________________________________________________________
dense_4 (Dense)              (100, 256)                33024     
_________________________________________________________________
dense_5 (Dense)              (100, 24)                 6168      
=================================================================
Total params: 1,208,136
Trainable params: 1,208,136
Non-trainable params: 0
_________________________________________________________________

```
Currently I am getting accuracy around 93.5%. Can you please suggest what are the other parameters of the model I need to change in order to get a accuracy more than 95% for the mentioned model.

"
"Hi, 
I had problem to convert stream_state_internal KWS model to tflite. I have tried to convert from both tfpb and h5. The issue is same. Does anyone have any idea? I used ""micro"" as preprocess.

Thanks. 

Model: 
[ds_cnn_stream.zip](https://github.com/google-research/google-research/files/7223822/ds_cnn_stream.zip)

Traceback (most recent call last):
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 496, in _import_graph_def_internal
    results = c_api.TF_GraphImportGraphDefWithResults(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input 0 of node functional_1/stream/AssignVariableOp was passed float from functional_1/stream/ReadVariableOp/resource:0 incompatible with expected resource.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""run_convert.py"", line 16, in <module>
    tfmodel = converter.convert()
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 808, in convert
    _convert_to_constants.convert_variables_to_constants_v2_as_graph(
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 1108, in convert_variables_to_constants_v2_as_graph
    frozen_func = _construct_concrete_function(func, output_graph_def,
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py"", line 999, in _construct_concrete_function
    new_func = wrap_function.function_from_graph_def(output_graph_def,
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 650, in function_from_graph_def
    wrapped_import = wrap_function(_imports_graph_def, [])
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 621, in wrap_function
    func_graph.func_graph_from_py_func(
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 87, in __call__
    return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 93, in wrapped
    return fn(*args, **kwargs)
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py"", line 648, in _imports_graph_def
    importer.import_graph_def(graph_def, name="""")
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 400, in import_graph_def
    return _import_graph_def_internal(
  File ""/home/temp/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/framework/importer.py"", line 501, in _import_graph_def_internal
    raise ValueError(str(e))
ValueError: Input 0 of node functional_1/stream/AssignVariableOp was passed float from functional_1/stream/ReadVariableOp/resource:0 incompatible with expected resource.
"
"I'm not so sure if it's the right place to ask. 
Could anyone from the google-research team kindly help me to understand [this query](https://github.com/tensorflow/models/issues/9446)? "
"@rybakov I have trained the CRNN model as specified in the recipe [here](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn). with training steps 20000,20000

I have got the a good testing accuracy of 97.23% for Non-Streaming model ,  but for the streaming case, it very less about 53.67%  for the both models( internal and external states), I couldn't find what was the issue. Can you please tell what went wrong and how to improve the streaming accuracy?
"
"Hi, can you please share a sample input CSV(the one with normalized x, y coordinates) in order to run infer.py? Although the procedure to make it is given in infer.py code it seems that i can't figure out how to make one. I need to run inference and for that 2D coordinates, a CSV file is needed. 
Thanks"
"Hey @rybakov 
I have trained the SVDF model as per the mentioned recipe. I want to know how the model is getting converted to streaming with internal states and external states. I am searching for the code which does model conversion to streaming mode. Where I can find the written code flow of the conversion in the repo?"
"I tried to train the crnn model mentioned in the recipe [kws_experiments_paper_12_labels.md](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper_12_labels.md#crnn)

When I run the $CMD_TRAIN \ with all the parameters as mentioned, I am getting error

> InvalidArgumentError: Node 'training/Adam/gradients/gradients/gru/cell/while_grad/gru/cell/while_grad': Connecting to invalid output 51 of source node gru/cell/while which has 51 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).

How can I train this model in the bash terminal as mentioned in the recipe?
"
"@rybakov Can you please provide the pretrained model or the best_weights for the KWS_Streaming SVDF model or any other method? 
I couldn't trained it completely on colab due to limitations. If it is possible please provide it, I need it urgently. "
"Hi Team,

Thanks for sharing this piece of work. Really appreciate it. 

This isn't an issue but it is a request to share any guide or links using which I can implement the trained models to spot the keywords on my computer locally using the mic. I could not find any guide in the repo that can help us test the models in the live audio feed from computers or phones. I would really appreciate any heads up in that direction!"
"Hey, 
i am trying to run the temporal fusion transformer network on my own data. 

The test with the default datasets volatility and traffic worked out without any problem. 

When using my own dataset, i receive an error when the network starts computing the validation loss: 

`Done.
Computing best validation loss
Traceback (most recent call last):
  File ""C:\Users\Noah\AppData\Local\Programs\Python\Python37\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\Noah\AppData\Local\Programs\Python\Python37\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Noah\tft\script_train_fixed_params.py"", line 238, in <module>
    use_testing_mode=True)  # Change to false to use original default params
  File ""C:\Users\Noah\tft\script_train_fixed_params.py"", line 151, in main
    val_loss = model.evaluate(valid)
  File ""C:\Users\Noah\tft\libs\tft_model.py"", line 1189, in evaluate
    raw_data = self._batch_data(data)
  File ""C:\Users\Noah\tft\libs\tft_model.py"", line 763, in _batch_data
    data_map[k] = np.concatenate(data_map[k], axis=0)
  File ""<__array_function__ internals>"", line 6, in concatenate
ValueError: zero-dimensional arrays cannot be concatenated`

I followed the instruction of creating a new data-formatterclass for my experiment. I added column-definitions equal to those in traffic and volatility experiment: 
`_column_definition = [
      ('id', DataTypes.REAL_VALUED, InputTypes.ID),
      ('Minutengesamt', DataTypes.REAL_VALUED, InputTypes.TIME),
      ('Gasmengenstrom', DataTypes.REAL_VALUED, InputTypes.TARGET),
      ('Gesamtwasser', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),
      ('Gesamtschlamm', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),
      ('Tag', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('Jahr', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('Stunde', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('Minute', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),
      ('CatID', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),
  ]`

And I am splitting my dataset: 
`def split_data(self, df, valid_boundary=26300, test_boundary=30700):
    """"""Splits data frame into training-validation-test data frames.

    This also calibrates scaling object, and transforms data for each split.

    Args:
      df: Source data frame to split.
      valid_boundary: Starting year for validation data
      test_boundary: Starting year for test data

    Returns:
      Tuple of transformed (train, valid, test) data.
    """"""

    print('Formatting train-valid-test splits.')

    index = df['indall']
    train = df.loc[index < valid_boundary]
    valid = df.loc[(index >= valid_boundary) & (index < test_boundary)]
    test = df.loc[(index >= test_boundary) & (index <= 34631)]`

I did not change anything else in comparison to the volatility default experiment. 
My complete dataset consists out of 35136 rows - so i left out the last lines, like in the volatility experiment it is performed. Although i dont understand so far, why this is done. 

Can somebody explain to me, what I would need to change and why those last lines are left out?

Thank you in advanced!
Hinnerk8


"
"I wanted to get inference for Poem repository, but it seems that it does not provide and pre-trained model. Can anyone please  tell how should I get inference ? Thanks"
[kws_streaming] How can I resume the model (svdf) training from the previous checkpoint or the previous best weights. What changes I have to do in train.py. Anyone please suggest something.
"The project is KWS-streaming.

The training ran successfully. In Inference for ""SVDF Model"", the colab notebook provided, 02_inference.ipynb, There in no output while running the cell 

(convert model to inference mode with batch one)
inference_batch_size = 1
tf.keras.backend.set_learning_phase(0)
flags.batch_size = inference_batch_size  # set batch size
model_non_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.NON_STREAM_INFERENCE)

The cell has been running for too long on the last code line
i.e **model_non_stream = utils.to_streaming_inference(model_non_stream_batch, flags, Modes.NON_STREAM_INFERENCE)**
and there was no output nor any error, seems it is in a loop.

Please help!"
"Hi, 

There is a small issue with the notebook leading to nonfunctioning of the colorization.

Changing the line
os.environ[""LOGDIR""] = ""/content/google-research/coltran/logdir/""
to
os.environ[""LOGDIR""] = ""/content/google-research/coltran/logdir/coltran""

fixes it. 

Thank you very much!
Veit

"
"Hello, 
Looking at your PR-VIPE code I don't fully understand why the 3d keypoints are needed if the idea is to embedd the 2d keypoints to the respective action the subject is doing. "
"Hi, 
I'm trying to run the charformer, and have some questions.

At first,  In ""Integration Steps"" in README.md,  I found a variable `self.num_gsw_layers` and I can't find any clue to set correct value.  what value shoud be set to `self.num_gsw_layers`? 
I used  `self.num_gsw_layers = 1` for now.

Then I tried to run Chaformer_tall. the paper says,

> Our small model follows the T5 small model size with 6 encoder layers and 6 decoder layers, hidden size d_model of 512, 8 heads, d_kv of 32 and d_ff of 2048. This corresponds to bi_v1_small.gin in the T5 codebase. 
>...
> The tall model has 24 encoder layers and 6 decoder layers, while the remainder of its hyperparameters remain identical
to the small model. 

So, I included `bi_v1_small.gin` . and set `num_layers` to 24(encoder) and 6(decoder). My configuration is as follows,
```
import charformer.lib.charformer_layers
include 'models/bi_v1_small.gin'

GradientSubwordLayerV2.key_value_size = %d_kv
GradientSubwordLayerV2.num_heads = %num_heads
GradientSubwordLayerV2.dropout_rate = %dropout_rate
GradientSubwordLayerV2.downsample_query = 3.0
GradientSubwordLayerV2.radius = 8
GradientSubwordLayerV2.low_rank_features = 32
GradientSubwordLayerV2.project_kv = False
GradientSubwordLayerV2.use_ffn = False
GradientSubwordLayerV2.local_gate = False
GradientSubwordLayerV2.num_memory_slots = 0
GradientSubwordLayerV2.local_attention = False
GradientSubwordLayerV2.consider_chars_as_blocks = True
GradientSubwordLayerV2.conv_type = ""conv1d""

encoder/Unitransformer.gradient_subwords = True

make_layer_stack.layer_stack_cls=@charformer_layers.CharformerLayerStack

encoder/Unitransformer.gradient_subword_layer = @charformer_layers.GradientSubwordLayerV2

encoder/transformer.make_layer_stack.num_layers = 24
decoder/transformer.make_layer_stack.num_layers = 6

mesh_train_dataset_fn.pack = False
```

I ran charfomer with this configuration, and pick the number of parameters from the log messages.
```
INFO:tensorflow:Trainable Variables            count: 209     Total size: 102206464        Total slice_size: 102206464      
INFO:tensorflow:All Variables                  count: 221     Total size: 102549376        Total slice_size: 102549376     
```

the paper says chaformer_tall has 134M params, but I got 102,549,376. This is a big difference.   
Could you tell me where this difference come from?

My environment.
* Colab runtime with TPU.
* tensorflow : 2.5.0
* mesh-tensorflow : 0.1.19
* t5 : 0.9.1
* multilingual-t5 : 35f72315
* byt5 : 069d3cbc
* charformer(google-research) : 111c4ff4a
* I borrowed ""byt5_wiki.ja"" task from byt5 for pre-training task."
"Download from https://github.com/bazelbuild/rules_cc/archive/master.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found

Error in download_and_extract: java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_cc/archive/master.zip] 

"
"Hi all,

I create a notebook that allows you to use pre-trained model to colorise your image with just few click!
https://colab.research.google.com/drive/1c8UZL3CkczbIltDxjDfFbq9Y0RnMv93o?usp=sharing

The input must be 64x64 or 128x128. Is there a way to make it work with image of any size?"
"Hi there, 
I am able to run the coltran using custom_colorization.py on the set of images successfully. 
Could you give some direction on how to train for a custom dataset instead of imagenet?"
"I'm confused about the detailed process how ScaNN processes the original vector y and computes q_c(y) and q_p(y). 

Thank you for your help!"
"Hi,
I was trying to reproduce the results of F-Net paper. I downloaded the code and followed the instructions. However when I tried to do unittest after installing required libraries, I got the No GPU/TPU found error. I am sure that I have a GPU device and proper cuda versions. How can I use my GPU devices? Below you can find nvidia-smi and nvcc --version outputs.


+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:02:00.0 Off |                  N/A |
| 23%   26C    P8     8W / 250W |  11425MiB / 12196MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |
|  0%   22C    P8     9W / 280W |   6069MiB / 11177MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     13634      C   /usr/bin/python                            11415MiB |
|    1      4322      C   ...ython/multi_coil_nn/fastmri/bin/python2  6059MiB |
+-----------------------------------------------------------------------------+



nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Apr_24_19:10:27_PDT_2019
Cuda compilation tools, release 10.1, V10.1.168



"
"1. Error message:

ERROR: /root/.cache/bazel/_bazel_root/ef4c81d6fdf57bdb4d4cf1931e4d510a/external/bazel_tools/src/tools/launcher/BUILD:9:14: in cc_binary rule @bazel_tools//src/tools/launcher:launcher: When using LTO. The feature supports_start_end_lib must be enabled.

2. Compile environment:
use docker image: centos7-gcc9-cmake3, CentOS Linux release 7.7.1908 (Core) , gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2), clang version 8.0.1, bazel 4.1.0

"
"I have performed a grayscale image coloring test based on the pre-trained model you provided  @MechCoder**[(https://storage.cloud.google.com/gresearch/coltran/coltran.zip.)**] Why are the results all noisy images?
I have executed the custom_colorize.py in sequence according to the file description you provided.
![(2)](https://user-images.githubusercontent.com/50228574/119218090-1f5da100-bb11-11eb-97c6-5282b0e117f2.jpg)
![(3)](https://user-images.githubusercontent.com/50228574/119218091-208ece00-bb11-11eb-932b-b5e134c11079.jpg)
![(4)](https://user-images.githubusercontent.com/50228574/119218092-21276480-bb11-11eb-9907-9b0af0ffc843.jpg)
![(5)](https://user-images.githubusercontent.com/50228574/119218093-21bffb00-bb11-11eb-8c1c-cf5c1d755593.jpg)
![(6)](https://user-images.githubusercontent.com/50228574/119218094-21bffb00-bb11-11eb-8048-ffa3c26bf177.jpg)
![(7)](https://user-images.githubusercontent.com/50228574/119218095-22589180-bb11-11eb-8fdd-78d496dca7fc.jpg)
![(8)](https://user-images.githubusercontent.com/50228574/119218096-22f12800-bb11-11eb-96a1-ed3b8c73afb6.jpg)
![(9)](https://user-images.githubusercontent.com/50228574/119218097-22f12800-bb11-11eb-87f8-d07e0dc68f67.jpg)
![(10)](https://user-images.githubusercontent.com/50228574/119218098-2389be80-bb11-11eb-8a6f-0a78f16e12fd.jpg)
![(1)](https://user-images.githubusercontent.com/50228574/119218099-24225500-bb11-11eb-936f-b3dc8a180666.jpg)
"
"Hello,

First of all, thank you for sharing great works. I am greatly enjoying reading the code behind some of the great researches Google does.

Recently, I noticed that Google announced VertexAI Matching Engine, in which they claimed that ScaNN is used inside of it. The documentation says one of the features of the engine is [boolean filtering](https://cloud.google.com/vertex-ai/docs/matching-engine/filtering), so naturally, I expected ScaNN in this repository also has that capability.

I searched this repository and found some reminiscent of the features
https://github.com/google-research/google-research/blob/master/scann/scann/proto/restricts.proto
https://github.com/google-research/google-research/blob/master/scann/scann/coscann/v2_restricts.proto
but could not be sure whether the functionality is implemented completely.

I want to ask
1. whether boolean filtering is implemented in this ScaNN repository
2. If implemented, I want to know how to use the feature.

Thank you.




"
""
"As mentioned in [Accelerating Large-Scale Inference with Anisotropic Vector Quantization](https://arxiv.org/abs/1908.10396)

> ""MIPS is equivalent to cosine similarity search when all datapoints are equal-norm, so we adopt our technique to cosine similarity search by unit-normalizing all datapoints at training time."" 

Does this mean that it is not necessary to normalize dataset pre-training as seen in example code? 
"
"`tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'training/Adam/gradients/gradients/gru/cell/while_grad/gru/cell/while_grad': Connecting to invalid output 51 of source node gru/cell/while which has 51 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).`
"
"Hello, 

I recently manage to get tf3d to compile on my machine, win10/wsl, was a bit of a challenge. 
I am working on a 3d encoder / decoder using voxel type data [batch, voxel_idx, voxel_idz, voxel_idy, feature_channels] using keras Conv3d / Conv3dtranspose. My data is very sparse and I was under the impression that the sparse conv 3d block could help. 

I don't have access to the dataset to run your examples yet, so I only looked at your test cases, but I am not really getting how voxel_feature, voxel_indices, num_valid_voxels works. Do you have additional explanations on what they are and how they are related ? 
Also, probably none of the sparse layers would work with my data format, right ?  

Best, "
"Hi,
I tried to train ds_tc_net by default parameters on GPU.
When I saw GPU process, I could see it used GPU memory but the computation didn't seem to run on GPU.
Do I have to pass some other parameters to the script in order to compute training on GPU?
```
$ echo $CUDA_VISIBLE_DEVICES
3
$ nvidia-smi
Wed Apr 21 10:57:52 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
...
+-------------------------------+----------------------+----------------------+
|   3  Quadro RTX 6000     On   | 00000000:3E:00.0 Off |                  Off |
| 33%   29C    P2    60W / 260W |    167MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    3   N/A  N/A     31880      C   python                            163MiB |
+-----------------------------------------------------------------------------+
```

In addition, could you tell me how to try eager execution on kws_streaming project?
I gave it a try to add `run_eagerly = True` to [the line of model.complile](https://github.com/google-research/google-research/blob/34444253e9f57cd03364bc4e50057a5abe9bcf17/kws_streaming/train/train.py#L97) and/or `tf.enable_eager_execution()` before the line but they didn't work.

Sorry for the basic question. Thank you!"
"Dear Mr. @eladeban, I would like to follow your unsupervised algorithm _DMoN_ from the [Repo](https://github.com/google-research/google-research/tree/master/graph_embedding/dmon), but found that it only supports node feature inputs. Does the model support modification to support input of **_edge_** features for learning. As from our data perspective, both edge and node features are critical. Really appreciate your work!"
Is there method in ScaNN to save the model after trained?
"EDIT: Ignore this, I had a fundamental misunderstanding of Performer. Reading the args descriptions made me realize that the final value in the input is meant to be `attention_head_count * feature_count`.

I'm trying to get the [Tensorflow implementation](https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py) of the [Performer](https://arxiv.org/abs/2009.14794) Transformer working and am running into some issues.

I looked at the [fast_attention_test](https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention_test.py) file and saw that `test_fast_attention` provides the following as input: `x = tf.ones([1, length, hidden_size])`. I'm confused how this would translate to my specific use case. My input shape is currently (1, 240, 24). The 240 value is the number of ""time steps"" worth of data I'm passing, with 24 being the number of features each time step has. It seems like I'm supposed to include a `hidden_size` dimension in the input tensor? This value seems to be referenced in `Attention.build` line 413 when creating `attention_initializer` where it passes the last element of the input's shape to `_glorot_initializer`. How can I include the ""feature count"" dimension (value of 24 in this case)? Would the input's shape look something like (1, 240, 24, hidden_size) in my case?"
Is there a method to save the trained model?
"First of all congrats on your paper _Entity Linking in 100 Languages_. I am working on it for a research project that involves entity linking based on the mewsli-9 dataset. However, I noticed that the mapping used to disambiguate mentions to candidate entities is not cited anywhere.
So, I was wondering if you will make it available to download or provide some description of how the mapping was derived from the corpus.

Thanks for the attention @bothameister @eladeban ."
"I can not build AutoML-Zero since the following dependency was added to proto files (commit 3778f19) -
`import ""devtools/staticanalysis/pipeline/analyzers/proto_best_practices/proto/optouts.proto"";`
"
"Hi,
I'm curious of the output of each layers and I want to visualize it on Tensorboard.
I heard there are some ways to pass data to Tensorboard but what is the best way in the kws_streaming project?
I wonder if you guys have some recommended ways.
As far as I see, I guess I need to add some data to [this line](https://github.com/google-research/google-research/blob/0667f4c48f3920f0fa7ee85a54f5c3def92e4f97/kws_streaming/train/train.py#L149) in kws_streaming/train/train.py.  

Thank you!"
""
"The [fastconvnets](https://github.com/google-research/google-research/tree/master/fastconvnets) models contain a tensor with invalid data?

```bash
~: wget https://storage.googleapis.com/fast-convnets/tflite-models/mbv1_100_90_12b4_684.tflite
~: python
```
```python
>>> import tensorflow as tf
>>> interpreter = tf.lite.Interpreter(model_path=""mbv1_100_90_12b4_684.tflite"")
>>> interpreter.allocate_tensors()
>>> all_layers_details = interpreter.get_tensor_details()
Segmentation fault (core dumped)
```
```python
>>> tensor = model.Subgraphs(0).Tensors(14)
>>> tensor.ShapeAsNumpy()
array([64,  1,  1, 32], dtype=int32)
>>> buffer = model.Buffers(tensor.Buffer())
>>> buffer.DataLength()
820
```

@tgale96 @codgeek

lutzroeder/netron#681
"
"Hi,

I'm trying to duplicate some models on the paper (http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf).
I saw the comment below in [kws_streaming/model/cnn.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn.py):
```
It is based on paper:
  Convolutional Neural Networks for Small-footprint Keyword Spotting
  http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf
 ```
So I thought [kws_streaming/model/cnn.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn.py) had the similar structure as `cnn-one-fpool3` for example.
But the paper describes cnn+max_pool as a typical cnn architecture and it uses the model that has the structure but [kws_streaming/model/cnn.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn.py) doesn't seem to have any max pooling layer.
Why doesn't [kws_streaming/model/cnn.py](https://github.com/google-research/google-research/blob/master/kws_streaming/models/cnn.py) have it like tf.keras.MaxPool2D?

In addition, `net = stream.Stream(cell=tf.keras.layers.Flatten())(net)` and `net = tf.keras.layers.Dropout(rate=flags.dropout1)(net)` mean  a linear low-rank layer in the paper?

Thank you for your help!"
Solved. It was an issue with tensorflow probability which was for tensorflow 2.4. Update `requirements.txt`for tf3d
"Absolutely ,you did a excellent job! I'm not familiar tf  and in your paper figure 2 show the input image only have grayscale image,so I want to ask that when training ,Is there only a gray image input? Hope you can reply me about this idiot question,Thanks "
"![image](https://user-images.githubusercontent.com/27515470/111625256-d8081b00-8816-11eb-926f-44e0cee1c4e6.png)
"
"We recently discovered a bug in the camera augmentation module that incorrectly mirrored 2D projections, which was fixed it in 
https://github.com/google-research/google-research/commit/39d025e21187532d611bf4b1bba67cd6b2d357d2.

We reran the training at HEAD before and after this fix (following the default setting in our [ECCV'20 paper](https://arxiv.org/abs/1912.01001), using PersonLab keypoints + 15 async GPU workers, trained to 5M steps) and observed small improvement on the MPII-3DHP dataset:

H3.6M:
Hit@1: before = 73.8%, after = 73.6%.
Hit@10: before = 94.4%, after = 94.1%.
Hit@20: before = 96.8%, after = 96.7%.

3DHP (Chest):
Hit@1: before = 52.0%, after = 52.6%.
Hit@10: before = 62.3%, after = 62.7%.
Hit@20: before = 71.6%, after = 71.7%.

3DHP (All):
Hit@1: before = 48.2%, after = 49.4%.
Hit@10: before = 58.2%, after = 59.4%.
Hit@20: before = 67.7%, after = 68.7%."
"```
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/stuart/google-kws/kws_streaming/train/model_train_eval.py"", line 366, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/stuart/google-kws/kws_streaming/train/model_train_eval.py"", line 147, in main
    train.train(flags)
  File ""/home/stuart/google-kws/kws_streaming/train/train.py"", line 147, in train
    result = model.train_on_batch(train_fingerprints, train_ground_truth)
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 1088, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py"", line 3956, in __call__
    fetched = self._callable_fn(*array_vals,
  File ""/home/stuart/google-kws/venv/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1480, in __call__
    ret = tf_session.TF_SessionRunCallable(self._session._session,
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: Need minval < maxval, got 0 >= 0
	 [[{{node random_uniform_3}}]]
  (1) Invalid argument: Need minval < maxval, got 0 >= 0
	 [[{{node random_uniform_3}}]]
	 [[Func/training/Adam/gradients/gradients/dropout/cond_grad/StatelessIf/then/_455/input/_1130/_217]]
0 successful operations.
0 derived errors ignored.
```"
"Hey All and Admin,

May I know where the data augmentation is happening?


Thanks in advance"
"I'm following line by line this [guide](https://github.com/google-research/google-research/blob/master/tf3d/ops/README.md) for working with Sparse convolution but when I need to do the following command 

`bazel run sparse_conv_ops_py_test  --experimental_repo_remote_exec`

I get the following error:

```
ERROR: Skipping 'sparse_conv_ops_py_test': couldn't determine target from filename 'sparse_conv_ops_py_test'
WARNING: Target pattern parsing failed.
ERROR: couldn't determine target from filename 'sparse_conv_ops_py_test'
INFO: Elapsed time: 0.222s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)
```

I'm running this inside my docker image on my Mac OS. I'm running docker run without runtime nvidia as my mac does not have it

`docker run --privileged  -it -v ${MYFOLDER}:/working_dir -w /working_dir  tensorflow/tensorflow:2.3.0-custom-op-gpu-ubuntu16`

When I run `configure.sh` I get the following which seems to be ok

```
root@562522a0cd6e:/working_dir/tf3d/ops# ./configure.sh
Does the pip package have tag manylinux2010 (usually the case for nightly release after Aug 1, 2019, or official releases past 1.14.0)?. Y or enter for manylinux2010, N for manylinux1. [Y/n] y
Are you building against TensorFlow 2.1(including RCs) or newer?[Y/n] y
Build against TensorFlow 2.1 or newer.
Using installed tensorflow
2021-03-17 10:32:03.125008: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-03-17 10:32:09.258489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
```

Also i had to create a WORKSPACE file before running the bazel command in the same directory"
"Hi guys,

I'm currently trying to use the [tf3d](https://github.com/google-research/google-research/tree/master/tf3d) object detection pipeline together with the Waymo open dataset (training_0000-training_0003). I followed the setup steps as described in the readme and everything seems to works fine.

I adapted the _run_train_locally.sh_ according to my file locations. However when running the script ´bash tf3d/object_detection/scripts/waymo/run_train_locally.sh´ I get the following error message.

```
2021-03-12 17:00:49.642985: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10724020115992582208_7660_400_7680_400$�O�-�@$�O�-�@H���'�@�.�_��@8M��D�?u���{տ�^A�o>V?�h9!F�
2021-03-12 17:00:49.646172: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
$10975280749486260148_940_000_960_000��&4H�@��&4H�@2���䡍@�
                                                           �m
                                                             ʃ@�ܝ4d�?�
�0�ֿi����>N?���[aG�
2021-03-12 17:00:49.647634: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10724020115992582208_7660_400_7680_400$�O�-�@$�O�-�@H���'�@�.�_��@8M��D�?u���{տ�^A�o>V?�h9!F�
2021-03-12 17:00:49.657222: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10963653239323173269_1924_000_1944_000�4�@�4�@1U�%�@�ZOV�E�@�ZL�pب?�P��/�տ@&���w3?�  eU�%Q�
2021-03-12 17:00:49.654120: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10517728057304349900_3360_000_3380_000(B]�.�@(B]�.�@�A_
                                                        Í@j��n�@�El��ȩ?�jUPֿ8%цvZ?&�#�ΘN�
2021-03-12 17:00:49.657316: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
%1172406780360799916_1660_000_1680_000'��I�@'��I�@t�ɤ�|�@�
                                                          8�Ӷ�@�ӑ84�?�i���nտ���s|u@?�3:!��N�
2021-03-12 17:00:49.666025: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
$11318901554551149504_520_000_540_000��@��@�X�B]�@�B�@��EO���?A�E��Կ�J�nU�I?��x�*�D�
2021-03-12 17:00:49.663333: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10231929575853664160_1160_000_1180_000�l����@�l����@l���\�@�#�@q�I��?���ճ�ѿcA��R?��Օ_�
2021-03-12 17:00:49.665340: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
%1146261869236413282_1680_000_1700_000�
                                       [��@�
                                            [��@��_G��@b���
2021-03-12 17:00:49.696678: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&11236550977973464715_3620_000_3640_000`@7��2�@`@7��2�@<_R��1�@ι2��J�@5�u&�X�?""�����ֿ]�a߱E?A��hF�
2021-03-12 17:00:49.696587: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&11388947676680954806_5427_320_5447_320��ќ@�@��ќ@�@d�$<�@���%�@y8ܵ]�?�KaS;ٿ��t��g?�5��1�
2021-03-12 17:00:49.696619: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
�9�3տ#�\]P�^?<�j19015_28= ?360_2909_360���""�-�@���""�-�@�YO1��@�}�Y�&�@���D��?
2021-03-12 17:00:49.697366: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&11252086830380107152_1540_000_1560_000�S�*�@�S�*�@$�[V���@""��]Jm�@O����8�?O�рEտw��b�iA?��~US�
2021-03-12 17:00:49.704963: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&11799592541704458019_9828_750_9848_759&�`5�9&�`5�@�k�d�\�@>���؃�@ G�[:�?��k���ԿD#P�u�T?��_�~c�
2021-03-12 17:00:49.707153: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
�9�3տ#�\]P�^?<�j19015_28= ?360_2909_360���""�-�@���""�-�@�YO1��@�}�Y�&�@���D��?
2021-03-12 17:00:49.716088: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&11004685739714500220_2300_000_2320_000��@��@�X�B]�@�B�@��EO���?A�E��Կ�J�nU�I?��x�*�D�
2021-03-12 17:00:49.718106: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
$11967272535264406807_580_000_600_000�l����@�l����@l���\�@�#�@q�I��?���ճ�ѿcA��R?��Օ_�
2021-03-12 17:00:49.739020: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&11388947676680954806_5427_320_5447_320��ќ@�@��ќ@�@d�$<�@���%�@y8ܵ]�?�KaS;ٿ��t��g?�5��1�
2021-03-12 17:00:49.744177: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10094743350625019937_3420_000_3440_000'��I�@'��I�@t�ɤ�|�@�
                                                           8�Ӷ�@�ӑ84�?�i���nտ���s|u@?�3:!��N�
2021-03-12 17:00:49.745660: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
&10498013744573185290_1240_000_1260_000�l����@�l����@l���\�@�#�@q�I��?���ճ�ѿcA��R?��Օ_�
2021-03-12 17:00:49.745454: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Could not parse example input, value: '
�
%1146261869236413282_1680_000_1700_000�
                                       [��@�
                                            [��@��_G��@b���
Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/datamanagement/code/ai/3D_Object_detection/tf3d/train.py"", line 226, in <module>
    app.run(main)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/datamanagement/code/ai/3D_Object_detection/tf3d/train.py"", line 181, in main
    train(strategy=strategy, write_path=write_path)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/gin/config.py"", line 1069, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/gin/utils.py"", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/gin/config.py"", line 1046, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File ""/home/datamanagement/code/ai/3D_Object_detection/tf3d/train.py"", line 124, in train
    verbose=1 if FLAGS.run_functions_eagerly else 2)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/home/datamanagement/tf3d_env/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa2 in position 138: invalid start byte
  In call to configurable 'train' (<function train at 0x7f8ed49ef8c8>)
```
The system I am using is:
Ubuntu 18.04
Python 3.6.9
GeForce RTX 2080

I haven't modified anything in the repo so far, except adapting the data paths accordingly.

Does anyone have an idea what could cause the error ? 

Thanks already!!!

Max
"
"@MechCoder 
Thanks for your impressive work. I first sample som images from the imagenet dataset, when i try to visualize the intermediate images of them at stage one, I find some images are successfully colored, but some are failed. I am confused about it. Could you give me some suggestions?"
"Thanks, for an interesting project! I would like to ask, is it possible to train the model in multiple GPU mode? in the parameter list, I did not find any suitable settings for using more than one graphics card. In a normal startup, only one is used, despite the detection of the available other by the script. "
"In tf3d's tf3d_dataset.md, it is mentioned that Waymo open dataset, Scannet Dataset, Rio Dataset are currently supported. I can find the first two usage methods and scripts, but I can’t find the usage method and script of Rio Dataset. The corresponding gin config files and scripts cannot be found in the semantic_segmentation, instance_segmention, and object_detection folders.How can I quickly perform a series of tasks such as model training and evaluation based on the RIO dataset?Thanks!"
"`2021-03-06 14:07:51.057994: W tensorflow/c/c_api.cc:326] Operation '{name:'gru/cell/while' id:665 op device:{} def:{{{node gru/cell/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, ..., DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=52, _read_only_resource_inputs=[7, 8, 9], body=gru_cell_while_body_1257[], cond=gru_cell_while_cond_1256[], output_shapes=[[], [], [], [], [100,256], ..., [], [], [], [], []], parallel_iterations=32](gru/cell/while/loop_counter, gru/cell/while/maximum_iterations, gru/cell/time, gru/cell/TensorArrayV2_1, gru/cell/ReadVariableOp, gru/cell/strided_slice, gru/cell/TensorArrayUnstack/TensorListFromTensor, gru/cell/gru_cell/bias, gru/cell/gru_cell/kernel, gru/cell/gru_cell/recurrent_kernel, gru/cell/while/EmptyTensorList, gru/cell/while/EmptyTensorList_1, gru/cell/while/EmptyTensorList_2, gru/cell/while/EmptyTensorList_3, gru/cell/while/EmptyTensorList_4, gru/cell/while/EmptyTensorList_5, gru/cell/while/EmptyTensorList_6, gru/cell/while/EmptyTensorList_7, gru/cell/while/EmptyTensorList_8, gru/cell/while/EmptyTensorList_9, gru/cell/while/EmptyTensorList_10, gru/cell/while/EmptyTensorList_11, gru/cell/while/EmptyTensorList_12, gru/cell/while/EmptyTensorList_13, gru/cell/while/EmptyTensorList_14, gru/cell/while/EmptyTensorList_15, gru/cell/while/EmptyTensorList_16, gru/cell/while/EmptyTensorList_17, gru/cell/while/EmptyTensorList_18, gru/cell/while/EmptyTensorList_19, gru/cell/while/EmptyTensorList_20, gru/cell/while/EmptyTensorList_21, gru/cell/while/EmptyTensorList_22, gru/cell/while/EmptyTensorList_23, gru/cell/while/EmptyTensorList_24, gru/cell/while/EmptyTensorList_25, gru/cell/while/EmptyTensorList_26, gru/cell/while/EmptyTensorList_27, gru/cell/while/EmptyTensorList_28, gru/cell/while/EmptyTensorList_29, gru/cell/while/EmptyTensorList_30, gru/cell/while/EmptyTensorList_31, gru/cell/while/EmptyTensorList_32, gru/cell/while/EmptyTensorList_33, gru/cell/while/EmptyTensorList_34, gru/cell/while/EmptyTensorList_35, gru/cell/while/EmptyTensorList_36, gru/cell/while/EmptyTensorList_37, gru/cell/while/EmptyTensorList_38, gru/cell/while/EmptyTensorList_39, gru/cell/while/EmptyTensorList_40, gru/cell/while/EmptyTensorList_41)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.`

TF 2.4.1 Aarch64 can not figure at all how to run on Aarch64 anyone got any tips why this is different as the same install on x86_64 works fine.

"
"Anybody got a guide or a whl for tensorflow addons as can not currently install without?

"
"Apols about my hacks but playing with the streaming CRNN I just used this to get something going.
Is there any chance you guys could provide some basic best methods of mic input inference.
Also the 'envelope' of the classification whats the best way to process that rather than maybe a simple sum?

Should you always reset the state after another classification detection?

```
import sounddevice as sd
import numpy as np
import timeit
import tensorflow.compat.v1 as tf


# Parameters
debug_time = 0
debug_acc = 0
word_threshold = 10.0
rec_duration = 0.020
sample_rate = 16000
num_channels = 1

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""models2/crnn_state/tflite_stream_state_external/stream_state_external.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(input_details[0]['shape'])
inputs = []
for s in range(len(input_details)):
  inputs.append(np.zeros(input_details[s]['shape'], dtype=np.float32))
    
def sd_callback(rec, frames, time, status):

    
    start = timeit.default_timer()
    
    # Notify if errors
    if status:
        print('Error:', status)
    
    rec = np.reshape(rec, (1, 320))
    
    # Make prediction from model
    interpreter.set_tensor(input_details[0]['index'], rec.astype(np.float32))
    # set input states (index 1...)
    for s in range(1, len(input_details)):
      interpreter.set_tensor(input_details[s]['index'], inputs[s])
  
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    # get output states and set it back to input states
    # which will be fed in the next inference cycle
    for s in range(1, len(input_details)):
      # The function `get_tensor()` returns a copy of the tensor data.
      # Use `tensor()` in order to get a pointer to the tensor.
      inputs[s] = interpreter.get_tensor(output_details[s]['index'])
      
    out_tflite_argmax = np.argmax(output_data)
     
    
    if out_tflite_argmax == 2:
        print('raspberry')
        print(output_data[0][2])
        

    if debug_acc:
        print(out_tflite_argmax)
    
    if debug_time:
        print(timeit.default_timer() - start)

# Start streaming from microphone
with sd.InputStream(channels=num_channels,
                    samplerate=sample_rate,
                    blocksize=int(sample_rate * rec_duration),
                    callback=sd_callback):
    while True:
        pass
```"
"Thanks for the interesting paper,
I have a question concerning the pseudo labels that are computed using the teacher network. In section 2, subsection ""Practical approximation"" of v3 of your paper, you say that you use ""hard pseudo labels from the teacher distribution"" and that this prevents us form simply using backpropagation on equation (3). 
However, is it really the case that you use one-hot-encoded pseudo labels generated by the teacher to train the student? Looking at the code, it seems like you just compute the teacher output with ""build_uda_crossentropy"", store them in ""logits"" and use them as-is without a one-hot-encoding. Am I missing anything?
€dith: I think this is the relevant part of the code:
""
\# for backprop
    cross_entropy['s_on_u'] = tf.losses.softmax_cross_entropy(
        **onehot_labels=tf.stop_gradient(tf.nn.softmax(logits['u_aug'], -1)),**
        logits=logits['s_on_u'],
        label_smoothing=params.label_smoothing,
        reduction=tf.losses.Reduction.NONE)
""
Also, if we actually use soft pseudo labels, why do we still have to use the complicated gradient of gradient/Taylor approximation like in appendix A and can't just use the differentiability of equation (3) as implied in the paper? Maybe I'm just misunderstanding what you mean by ""soft"" and ""hard"" pseudo labels.

Thank you very much for your help"
"@MechCoder
It seems that Colorization Transformer didn't have a script for single grayscale image inference.

I am not familiar with the TF2.0, thus there are also some questions for this open source code:

1. When i try to adapt the sample.py for single image inference, i found it will download the imagenet2012 dataset. Is this nessasary for sample.py?
2. Is there any difference between ”Sampling“ and ”inference“ ?"
"Thanks for your work having a pretty detailed documentation for the CuBERT source.

However, there appears to be some code missing from compared to what is in [the paper](https://arxiv.org/pdf/2001.00059.pdf). This issue is specifically about the finetuneing code for the downstream tasks.

I image there must exist code somewhere that loaded the dataset jsons and fed them into a BERT training loop, and then evaluated at the end. Could this code be released? (Apologies if I'm missing something if it's already there). It might be just be something that is hacked on top a fork of the BERT repo, and if that's the case, that's totally fine. It'd be really useful to see.

This is not just trivial implementation as some choices like how long functions (> 512 tokens) were finetuned/evaluated might effect downstream performance (I assume just truncating?). This makes it somewhat complicated to recreate/benchmark against CuBERT in followup research I am doing without the source.

See also issue #571 about instructions about using the finetuned models (but not necessarily creating them). Related also is my issue  #581 for missing CuBERT code for the pretraining corpus deduping (which could also be helped with the release of more of the original source).

CC: @adityakanade @maniatis"
"When downloading the CuBERT pretraining corpus, I encountered some questions with the deduping process. Section 3.2 of [the paper](https://arxiv.org/pdf/2001.00059.pdf) mentions that there was a deduping process:

> .... This brought the dataset to 14.3 million files. We then further deduplicated the remaining files, by clustering them into equivalence classes holding similar files according to the same similarity metric, and keeping only one exemplar per equivalence class.This helps avoid biasing the pre-trained embedding. Finally,we removed files that could not be parsed. In the end, we were left with 7.4 million Python files containing over 9.3 billion tokens. This is our Python pre-training code corpus.

I downloaded [the manifest files](https://console.cloud.google.com/storage/browser/cubert/20200621_Python/github_python_minus_ethpy150open_deduplicated_manifest). There are indeed ~7.4M blobs. However, only about 4.1M of the SHAs are unique, implying over 3M point to repeat files. There about 0.9M instances of the SHA e69de29bb2d1d6434b8b29ae775ad8c2e48c5391 which appears to correspond to an empty file. There are also thousands that correspond things like Django default files or 2626 instances of a SHA that corresponds to [a file from chardetection](https://github.com/splunk/TA-GitHubStats/blob/ce4eab957e6d9840d4230209b17d85d0162bc3e7/bin/chardet/euckrfreq.py) which could slightly confuse modeling. See [this gist](https://gist.github.com/DNGros/1a1bfaaac0544b1b53bf20a301d61b50) if want to see how I checked the SHAs.

Any thoughts on why there might be identical SHAs? I don't see the scripts used for deduping in the repo, but deduping is rather difficult, so I get there are ways it could go wrong in confusing ways.

CC: @adityakanade @maniatis"
"Hello all,

Thank you for your wonderful work on Tensorflow 3D for lidar point cloud. 

I have a question regarding post processing in [object detection post processing](https://github.com/google-research/google-research/blob/master/tf3d/object_detection/postprocessor.py). 

what is the input shape to the postprocessing? I mean, for an example, usually we get [x,y,z,h,w,l,r] in kitti data set as a output of the model. is input to the postprocessing same as the kitti  based object detection model?

For an example
Could anyone help me to understand?"
"I attempted to run the command outlined in the readme file within the goemotions reop to fine-tune on top of BERT, however I am receiving the following error. Could you please provide some assistance?

<img width=""715"" alt=""Screen Shot 2021-02-12 at 17 16 36"" src=""https://user-images.githubusercontent.com/48035494/107740160-2dec2d80-6d56-11eb-9ab6-ad1d3d8b6cdd.png"">
"
"Details on the error for modeling_test:
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ec2-user/aditya/google-research/smith/modeling_test.py"", line 24, in <module>
    from smith import experiment_config_pb2
  File ""/home/ec2-user/aditya/google-research/smith/experiment_config_pb2.py"", line 16, in <module>
    serialized_pb='\n\x1dsmith/experiment_config.proto\x12\x05smith\""\xd5\x02\n\x0fTrainEvalConfig\x12\x1c\n\x14input_file_for_train\x18\x01 \x01(\t\x12\x1b\n\x13input_file_for_eval\x18\x02 \x01(\t\x12\x1c\n\x10train_batch_size\x18\x04 \x01(\x05:\x02\x33\x32\x12\x1b\n\x0f\x65val_batch_size\x18\x05 \x01(\x05:\x02\x33\x32\x12\x1e\n\x12predict_batch_size\x18\x06 \x01(\x05:\x02\x33\x32\x12\x1b\n\x0emax_eval_steps\x18\x07 \x01(\x05:\x03\x31\x30\x30\x12$\n\x16save_checkpoints_steps\x18\x08 \x01(\x05:\x04\x31\x30\x30\x30\x12!\n\x13iterations_per_loop\x18\t \x01(\x05:\x04\x31\x30\x30\x30\x12!\n\x13\x65val_with_eval_data\x18\n \x01(\x08:\x04true\x12#\n\x18neg_to_pos_example_ratio\x18\x0c \x01(\x02:\x01\x31\""\xc8\x04\n\rEncoderConfig\x12&\n\nmodel_name\x18\x0c \x01(\t:\x12smith_dual_encoder\x12\x17\n\x0finit_checkpoint\x18\x01 \x01(\t\x12\x1a\n\x12predict_checkpoint\x18\x02 \x01(\t\x12\x18\n\x10\x62\x65rt_config_file\x18\x03 \x01(\t\x12\x1c\n\x14\x64oc_bert_config_file\x18\x04 \x01(\t\x12\x12\n\nvocab_file\x18\x05 \x01(\t\x12\x1a\n\x0emax_seq_length\x18\x06 \x01(\x05:\x02\x33\x32\x12\""\n\x17max_predictions_per_seq\x18\x07 \x01(\x05:\x01\x35\x12#\n\x17max_sent_length_by_word\x18\x08 \x01(\x05:\x02\x33\x32\x12&\n\x1amax_doc_length_by_sentence\x18\t \x01(\x05:\x02\x36\x34\x12$\n\x18loop_sent_number_per_doc\x18\n \x01(\x05:\x02\x36\x34\x12!\n\x13sent_bert_trainable\x18\x0b \x01(\x08:\x04true\x12\""\n\x17max_masked_sent_per_doc\x18\x0e \x01(\x05:\x01\x32\x12*\n\x1buse_masked_sentence_lm_loss\x18\x0f \x01(\x08:\x05\x66\x61lse\x12\x15\n\nnum_labels\x18\r \x01(\x05:\x01\x32\x12$\n\x14\x64oc_rep_combine_mode\x18\x10 \x01(\t:\x06normal\x12+\n\x1e\x64oc_rep_combine_attention_size\x18\x11 \x01(\x05:\x03\x32\x35\x36\""3\n\nLossConfig\x12%\n\x1asimilarity_score_amplifier\x18\x01 \x01(\x02:\x01\x36\""\x9c\x01\n\x11\x44ualEncoderConfig\x12,\n\x0e\x65ncoder_config\x18\x01 \x01(\x0b\x32\x14.smith.EncoderConfig\x12\x31\n\x11train_eval_config\x18\x02 \x01(\x0b\x32\x16.smith.TrainEvalConfig\x12&\n\x0bloss_config\x18\x03 \x01(\x0b\x32\x11.smith.LossConfig')
  File ""/home/ec2-user/aditya/py37-venv/lib64/python3.7/site-packages/google/protobuf/descriptor.py"", line 942, in __new__
    return _message.default_pool.AddSerializedFile(serialized_pb)
TypeError: expected bytes, str found"
"Hi,

I'd like to use the Amazon2M dataset in my research project on graph neural networks.
You provided a script to generate this dataset in the Cluster-GCN repository.
However, the script doesn't work right now, since the maintainers of the dataset changed the format.
I'd really appreciate if you could share the adjacency matrix of Amazon2M in a single file, similarly to the node features you shared.
I'll cite you of course :)

Thank you,
Daniel"
"Hi
I would like to compile tensorflow serving with ScaNN.

scann: https://github.com/google-research/google-research/tree/master/scann
https://github.com/google-research/google-research/commit/f0721be6ff386e646536f931f8ea2cfe5773b8ff

tensorflow serving: https://github.com/tensorflow/serving
build instruction for custom op: https://www.tensorflow.org/tfx/serving/custom_op

 I have used patches from https://github.com/google-research/google-research/tree/master/scann/tf_serving/patches

**What I did**: 
1. patch original tensorflow serving repo :https://github.com/marcinkosztolowicz/serving/tree/feature/custom-op-scann-2
2. run build tool: `tools/run_in_docker.sh bazel build tensorflow_serving/model_servers:tensorflow_model_server`

**What did I expect?**:
Tensorflow serving binary which is capable of serve model with ScaNN op.

**What i got**:
Compiling errors linked to ScaNN 

**Question**:
@sammymax  What is the correct way to compile tensorflow serving with ScaNN?"
"@tkipf Do I understand correctly that it's 128x128 as stated in https://github.com/google-research/google-research/blob/master/slot_attention/object_discovery/train.py#L78 (and the convolutional encoder seems to have stride=1 and padding=""same"")?

The image in the repo induces to think that it's 8x8, but 8x8 seems to be the resolution only [after slots broadcasting](https://github.com/google-research/google-research/blob/master/slot_attention/model.py#L195):
![](https://github.com/google-research/google-research/raw/master/slot_attention/slot_attention.png)

How important was this high resolution for relatively simple CLEVR-like images? Would it work if there was more subsampling in the network and e.g. the input resolution to the slot attention was 7x7 or 14x14 (like you would get out of resnet18 with effecive stride=8)

Thanks!"
"Hello! Thanks for sharing this work!

I could train on the sampled test data and have successfully produced the training dataset (100 tfrecord files). would you also suggest the proper modification in the `train.py` such that I could train on the full dataset?
(sorry for this ""green-hand"" question. I'm really new to TensorFlow)

Thank you for your consideration!"
"@tkipf @dosovits, In https://github.com/google-research/google-research/blob/0c1bbe5/slot_attention/model.py#L43-L45:
```python
self.norm_inputs = layers.LayerNormalization()
self.norm_slots = layers.LayerNormalization()
self.norm_mlp = layers.LayerNormalization()
```

With default arguments of [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization), this means that implicitly `axis=-1`, `center=True`, `scale=True`

Does it mean that for it's equivalent (in PyTorch lingo) to:
```python
class SlotAttentionLayerNorm(torch.nn.Module):
   def __init__(self, inputs_size, epsilon = 1e-3):
     super().__init__()
     self.gamma = torch.nn.Parameter(torch.ones(inputs_size))
     self.beta = torch.nn.Parameter(torch.zeros(inputs_size))
     self.epsilon = epsilon

  def forward(self, inputs):
     # inputs` has shape [batch_size, num_inputs, inputs_size].
     var, mean = torch.var_mean(inputs, dim = -1, keepdim = True)
     return (inputs - mean) / (var + self.epsilon).sqrt() * self.gamma + self.beta
```

Ironically, this seems to match what PyTorch LayerNorm does, but it doesn't match what LayerNorm is supposed to be doing (or even InstanceNorm for that matter). i.e. computing stats per whole batch instance `batch[b, ...]` :)
![image](https://user-images.githubusercontent.com/1041752/106493465-746d9c80-64b9-11eb-992d-70b0323696cc.png)

If the above code is true, it's computing stats per `batch[b, h, w]` and standardizes the embedding vector themselves (does it even make sense?).

Thanks!"
"In tft prediction, how can we forecast Target feature value for future timestamps? tft.predict() method uses previous  num_encoder_steps timestamps to predict values (total_time_steps - num_encoder_steps). But, we're feeding these predicted values as input to tft model. 
For E.g.: In volatility model, 
total_time_steps = 252 + 5
num_encoder_steps = 252
So, when I feed 300 records, then after prediction, Target dataframe contains only 295 records and last 5 Input target values are in the prediction columns (t0, t1, t2, t3, t4). Predicted values are exactly same as that of Input target values. 
How can we predict values for future timestamps?
I also tried by adding a dummy row (target column/observed inputs/known inputs = 0 with future timestamp) as the last record in test data but TFT prediction model is just copy paste Input test data as predicted output values.

I don't know if I am missing something.
"
"https://github.com/google-research/google-research/blob/a57332ec23cb0b113d42213842ff1bc121d271f2/meta_pseudo_labels/training_utils.py#L484-L487
In the case of soft pseudo labels, the gradient of this formula seems to be 0, right?"
"`Pseudo label threshold`, `UDA factor`, `UDA temperature`, `batch size for the UDA unlabeled objective`, ...
Which is the best result?"
"Hello, I have two questions about equation 10 of appendix A in your paper.
![image](https://user-images.githubusercontent.com/43927732/105324082-4614d500-5c0e-11eb-8a69-8fd0d5a4f1a4.png)

First, is S(x_u ; theta_T) the correct notation? Based on the content of the research paper and the equations in appendix B, I believe that S(x_u ; theta_T) should be T(x_u ; theta_T). If I am incorrect, would you mind providing me with an explanation as to why the student is using the teacher's weights?
Secondly, I am having difficulty understanding how the variation of the log-gradient trick in the REINFORCE algorithm is applied to equation 10. Would you mind explaining how this log-gradient trick is used?

Thank you for your time and an impressive paper!"
"In the paper(https://arxiv.org/abs/2003.10580, page 15), h is calculated by inner product -
![스크린샷, 2021-01-21 14-13-48](https://user-images.githubusercontent.com/30650335/105283030-f8d73a00-5bf2-11eb-8990-640e286b9d45.png)
or cosine similarity between two gradients. 

But in the repository(https://github.com/google-research/google-research/blob/master/meta_pseudo_labels/training_utils.py), h seems to be calculated by some sort of Taylor expansion - 
CE(y_l, S(x_l, \thtea^(t+1)) - CE(y_l, S(x_l, \thtea^(t)).

Which is right? Is one of them wrong or out of date? If not, do authors using the Taylor expansion in CIFAR and using the inner product in ImageNet?"
"Steps to reproduce:
Follow the instruction in https://github.com/google-research/google-research/tree/master/kws_streaming to download data and model

Re-train dnn model from scratch on data set V1 and run evaluation:
python -m kws_streaming.train.model_train_eval \
--data_url '' \
--data_dir ./data1/ \
--train_dir ./models1/dnn_1/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 100,100,100 \
--learning_rate 0.0005,0.0001,0.00002 \
--window_size_ms 40.0 \
--window_stride_ms 20.0 \
--mel_num_bins 40 \
--dct_num_features 20 \
--resample 0.15 \
--alsologtostderr \
--train 1 \
dnn \
--units1 '64,128' \
--act1 ""'relu','relu'"" \
--pool_size 2 \
--strides 2 \
--dropout1 0.1 \
--units2 '128,256' \
--act2 ""'linear','relu'""



Failure:

  File ""/opt/conda/lib/python3.7/site-packages/tensorflow/python/platform/
app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/opt/conda/lib/python3.7/site-packages/absl/app.py"", line 300, in 
run
    _run_main(main, args)
  File ""/opt/conda/lib/python3.7/site-packages/absl/app.py"", line 251, in 
_run_main
    sys.exit(main(argv))
  File ""/home/wangtz/google-research/kws_streaming/train/model_train_eval.
py"", line 249, in main
    test.tf_stream_state_internal_model_accuracy(flags, folder_name)
  File ""/home/wangtz/google-research/kws_streaming/train/test.py"", line 34
0, in tf_stream_state_internal_model_accuracy
    flags, model_stream, test_fingerprints)
  File ""/home/wangtz/google-research/kws_streaming/train/test.py"", line 71
, in run_stream_inference_classification
    stream_step_size = flags.data_shape[0]
AttributeError: 'Namespace' object has no attribute 'data_shape'"
"Failed in block 25, source:
===
```
# create model with flag's parameters
model_non_stream_batch = models.MODELS[flags.model_name](flags)

# load model's weights
weights_name = 'best_weights'
model_non_stream_batch.load_weights(os.path.join(train_dir, weights_name))
```

Error message:
--------------------------------------------------------------------------
```AttributeError                           Traceback (most recent call last)
<ipython-input-25-a170e05f648d> in <module>
      1 # create model with flag's parameters
----> 2 model_non_stream_batch = models.MODELS[flags.model_name](flags)
      3 
      4 # load model's weights
      5 weights_name = 'best_weights'

~/google-research/kws_streaming/colab/google-research/kws_streaming/models/svdf.py in model(flags)
    104     # it is a self contained model, user need to feed raw audio only
    105     net = speech_features.SpeechFeatures(
--> 106         speech_features.SpeechFeatures.get_params(flags))(
    107             net)
    108 

~/google-research/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py in get_params(flags)
    274     """"""
    275 
--> 276     if flags.time_shift_ms != 0.0 and flags.sp_time_shift_ms != 0.0:
    277       raise ValueError('both time_shift_ms and sp_time_shift_ms are set '
    278                        'only one parameter should be used: '

AttributeError: 'Namespace' object has no attribute 'sp_time_shift_ms'
```

Temporary fix: 
===
```
flags.sp_time_shift_ms = .0
flags.sp_resample = .0
```

Remaining problem
===
--------------------------------------------------------------------------
```AttributeError                           Traceback (most recent call last)
<ipython-input-30-a170e05f648d> in <module>
      1 # create model with flag's parameters
----> 2 model_non_stream_batch = models.MODELS[flags.model_name](flags)
      3 
      4 # load model's weights
      5 weights_name = 'best_weights'

~/google-research/kws_streaming/colab/google-research/kws_streaming/models/svdf.py in model(flags)
    104     # it is a self contained model, user need to feed raw audio only
    105     net = speech_features.SpeechFeatures(
--> 106         speech_features.SpeechFeatures.get_params(flags))(
    107             net)
    108 

~/google-research/kws_streaming/colab/google-research/kws_streaming/layers/speech_features.py in get_params(flags)
    327             flags.use_tf_fft,
    328         'use_spec_cutout':
--> 329             flags.use_spec_cutout,
    330         'spec_cutout_masks_number':
    331             flags.spec_cutout_masks_number,

AttributeError: 'Namespace' object has no attribute 'use_spec_cutout'```

Not sure how to solve it."
"I can't find how to save trained model.
I've seen TensorFlow recommenders have scann integrated in their models, and as such have similar saving process like:
```
scann = tfrs.layers.factorized_top_k.ScaNN(model.user_model, num_reordering_candidates=1000)

...
  scann.save(
    path,
    options=tf.saved_model.SaveOptions(namespace_whitelist=[""Scann""])
  )
```
Any help will be appreciated."
"I see the Hamming-distance named on multiple locations in the code, however not in the README or the `algorithms.md`.

Is there some way already to use the Hamming-distance with ScaNN? If so, how would i do that?"
"The structure of Meena is a Seq-to-Seq model, and it's not mentioned in the paper how can Meena get the context of chatting and generate the output based on the input. 

Does Meena encodes all of the previous utterances in a chat as the embedding? Anyone can help? THx"
"For the MLRC challenge we are evaluating the reproducibility of the results presented in ""Fairness without Demographics through Adversarially Reweighted Learning”.

From [the implementation](https://github.com/google-research/google-research/tree/master/group_agnostic_fairness), we were quickly able to run the code with the dummy data well. However, after downloading the full datasets, we noticed that the program does not yield any results but instead gives errors. 

To fully reproduce the results presented in the paper, we would like to troubleshoot these errors. 

First, we would like to verify the following: what version of Python is used for the implementation (e.g. 3.6, 3.7, 3.8)?

Furthermore, we present the error that was yielded when we ran the code on the full law_school dataset. 
We ran the main_trainer.py for the law_school dataset with ```python main_trainer.py --dataset law_school --dataset_base_dir data/law_school --train_file data/law_school/train.csv --test_file data/law_school/test.csv``` 
This yields us the error: ```tensorflow.python.framework.errors_impl.InvalidArgumentError: Expect 12 fields but have 5 in record 0```."
"Hi, I am trying to train your network(POEM). But I found that using tools/gen_train_tfrecords.py did not save any negative pairs. And in train_base.py, the contents of ""anchor_keypoints"" and ""match_keypoints"" in triplet_loss are exactly the same (I found sum(anchor_keypoints-match_keypoints) == 0).
https://github.com/google-research/google-research/blob/master/poem/train_base.py#L601

 Since there is no evaluation script, it is currently impossible to judge whether the training is correct. So where is ""Online Mining"" method reflected in the paper? 
Looking forward to your reply."
"Since Gwikimatch is not available yet, how should I prepare the data to train and test ""smith""? Could someone please provide guidance on where to fetch the data or what format the program expects so that I could construct my own data? Thank you very much!"
""
""
"There are some directory issues while running `analyze_data.py` , `extract_words.py`, and `ppca.py` .
If the file isn't already present it gives a File doesn't exist error."
2323232
"Thank you for this work. I believe the inference code has been tested on a nightly build and some files like use internal/private ops that aren't available anymore. 

```python
from kws_streaming.models import models
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-5-0cb1216f5109> in <module>()
----> 1 from kws_streaming.models import models

3 frames
/content/google-research/kws_streaming/layers/random_shift.py in <module>()
     17 
     18 from kws_streaming.layers.compat import tf
---> 19 from tensorflow.python.keras.utils import control_flow_util  # pylint: disable=g-direct-tensorflow-import
     20 from tensorflow.python.ops import array_ops  # pylint: disable=g-direct-tensorflow-import
     21 

ImportError: cannot import name 'control_flow_util'
```"
"Is there an implementation of BERT using fast attention?
I saw in the protein_lm example that there is an implementation of FlaxBERT specifically crafted for protein domain, is there an implementation for the language domain that could possibly use pre-trained bert weights?"
Thank you thank you very much.
"hi, 
   I was confused where the anisotropic quantization was used. I thought that anisotropic quantization was used to quantize subsets of each data(50 subsets per data) to one centroid among 16 centroids in each blocks(50 blocks), but the output of hashed_dataset is the same as I change the parameter score_ah(2, anisotropic_quantization_threshold = 0.2) to score_ah(2). Two versions are as follows:
version1: 
searcher = scann.ScannBuilder(normalized_dataset, 10, ""dot_product"").tree(num_leaves = 2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(2, anisotropic_quantization_threshold=0.2).reorder(60).create_pybind()
version 2:
searcher = scann.ScannBuilder(normalized_dataset, 10, ""dot_product"").tree(num_leaves = 2000, num_leaves_to_search=100,
training_sample_size=250000).score_ah(2).reorder(60).create_pybind()


"
"Can you update example guide for new models in stream mode?
I am now confused about which of the many models are streamable and which are not.
Thanks,"
"Kindly Update the readme of POEM repo, with all the steps to download dataset, Train the model and then how to Evaluate and reproduce the same results as mentioned in the paper."
"Can Scann fit more tf versions?  for example ：tf.2.4, tf.2.4-gpu , and more.
Now， it just fit to tf2.3.0 ， not support tf gpu version.
@sammymax"
""
"I downloaded this paper but can't find the supplementary material.
Unsupervised Monocular Depth Learning in Dynamic Scenes [https://arxiv.org/abs/2010.16404](url)
Could someone provide it? Thanks !
"
"```
import jax
import flax.linen as nn

key = jax.random.PRNGKey(0)

ga = jax.jit(make_fast_generalized_attention(qkv_dim=512, lax_scan_unroll=16))
dpa = jax.jit(nn.attention.dot_product_attention)

q = jax.random.normal(key, (8, 1000, 512))

%timeit ga(q, q, q).block_until_ready()
%timeit dpa(q, q, q).block_until_ready()

=>
3.22 ms ± 3.65 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
456 µs ± 748 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

This is 10x difference, so I must be doing something wrong.

Tried on a TPU and on A100 gpus (with lax_scan_unroll {1, 16})."
"I try to sh 'run.sh', and it seems that I meet all experience requirements, but I find the system Finished training suddenly and all assert_file_exists outputs 'not found'.

 I can't understand what happens, Can someone help me? It's my first time to ask questions in English, this may make my question unclear, and I will try my best to think about anybody's response, thanks!



This is my log:

> Building wheels for collected packages: wrapt, termcolor
  Building wheel for wrapt (setup.py) ... done
  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19552 sha256=9bb47c22b3213eae33687d520405a6ad1af358742738563bf5677d2c120cdccf
  Stored in directory: /home/zp/.cache/pip/wheels/4c/8d/0e/ecb228daca7bc2ae7f9a9d713ef75fce4a083de089869418b5
  Building wheel for termcolor (setup.py) ... done
  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=747aa37b47651ca7e68b8a56cac12cbf58b0b9ea7bca97f9540358d75237ea5f
  Stored in directory: /home/zp/.cache/pip/wheels/22/6d/f1/cee5814a13ba0c8ecd5ae67093238282c258bfdd6881b6b638
Successfully built wrapt termcolor
Installing collected packages: six, absl-py, numpy, protobuf, zipp, importlib-metadata, markdown, certifi, chardet, urllib3, idna, requests, grpcio, tensorboard-plugin-wit, werkzeug, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, h5py, google-pasta, keras-preprocessing, wrapt, gast, opt-einsum, termcolor, tensorflow-estimator, astunparse, tensorflow, decorator, dm-tree, cloudpickle, tensorflow-probability, tf-slim
Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 cloudpickle-1.6.0 decorator-4.4.2 dm-tree-0.1.5 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 idna-2.10 importlib-metadata-2.0.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 six-1.15.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 tensorflow-probability-0.11.1 termcolor-1.1.0 tf-slim-1.1.0 urllib3-1.25.11 werkzeug-1.0.1 wrapt-1.12.1 zipp-1.2.0
+ TRAIN_DIR=/tmp/e3d/train
+ mkdir -p /tmp/e3d/train
+ python3 -m poem.train --alsologtostderr --input_table=poem/testdata/tfe-2.tfrecords --train_log_dir=/tmp/e3d/train --batch_size=4 --num_steps=5 --input_shuffle_buffer_size=10 --summarize_percentiles=false
2020-11-03 12:27:16.959669: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/ops/linalg/linear_operator_full_matrix.py:149: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.
Instructions for updating:
Do not pass `graph_parents`.  They will  no longer be used.
W1103 12:27:18.325209 140324728178496 deprecation.py:506] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/ops/linalg/linear_operator_full_matrix.py:149: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.
Instructions for updating:
Do not pass `graph_parents`.  They will  no longer be used.
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/poem/core/models.py:97: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).
W1103 12:27:18.362956 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/poem/core/models.py:97: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W1103 12:27:18.365832 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/keras/legacy_tf_layers/normalization.py:336: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py:458: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W1103 12:27:18.705586 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py:458: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
I1103 12:27:18.933850 140324728178496 pipeline_utils.py:308] Resume latest training checkpoint in: /tmp/e3d/train.
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1103 12:27:19.484864 140324728178496 deprecation.py:506] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tf_slim/learning.py:734: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
W1103 12:27:19.663510 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tf_slim/learning.py:734: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2020-11-03 12:27:19.780182: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-03 12:27:19.801271: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3696000000 Hz
2020-11-03 12:27:19.802094: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8bb7aa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-03 12:27:19.802106: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-11-03 12:27:19.834374: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-11-03 12:27:19.927968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 12:27:19.928318: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8bce8b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-11-03 12:27:19.928330: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-11-03 12:27:19.928462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-11-03 12:27:19.928775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-11-03 12:27:19.928809: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-11-03 12:27:20.214316: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-11-03 12:27:20.218626: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-11-03 12:27:20.219405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-11-03 12:27:20.224505: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-11-03 12:27:20.227276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-11-03 12:27:20.227897: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:/usr/local/cuda-10.2/lib64::/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/usr/local/gcc-5.3.0/lib:/usr/local/gcc-5.3.0/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64
2020-11-03 12:27:20.227946: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-11-03 12:27:20.227998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-03 12:27:20.228025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-11-03 12:27:20.228047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
INFO:tensorflow:Restoring parameters from /tmp/e3d/train/model.ckpt-00000006
I1103 12:27:20.233745 140324728178496 saver.py:1293] Restoring parameters from /tmp/e3d/train/model.ckpt-00000006
WARNING:tensorflow:From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
W1103 12:27:20.375499 140324728178496 deprecation.py:323] From /home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
INFO:tensorflow:Running local_init_op.
I1103 12:27:20.376783 140324728178496 session_manager.py:505] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I1103 12:27:20.393647 140324728178496 session_manager.py:508] Done running local_init_op.
INFO:tensorflow:Starting Session.
I1103 12:27:21.270997 140324728178496 learning.py:746] Starting Session.
INFO:tensorflow:Saving checkpoint to path /tmp/e3d/train/model.ckpt
I1103 12:27:21.287984 140318586431232 supervisor.py:1117] Saving checkpoint to path /tmp/e3d/train/model.ckpt
INFO:tensorflow:Starting Queues.
I1103 12:27:21.288069 140324728178496 learning.py:760] Starting Queues.
INFO:tensorflow:Recording summary at step 6.
I1103 12:27:21.492430 140318569645824 supervisor.py:1050] Recording summary at step 6.
INFO:tensorflow:global_step/sec: 0
I1103 12:27:21.823515 140318578038528 supervisor.py:1099] global_step/sec: 0
INFO:tensorflow:Stopping Training.
I1103 12:27:21.906329 140324728178496 learning.py:769] Stopping Training.
INFO:tensorflow:Finished training! Saving model to disk.
I1103 12:27:21.906489 140324728178496 learning.py:777] Finished training! Saving model to disk.
/home/zp/code/pycharm/Pose_Invariant/env3/lib/python3.5/site-packages/tensorflow/python/summary/writer/writer.py:387: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.
  warnings.warn(""Attempting to use a closed FileWriter. ""
+ assert_file_exists /tmp/e3d/train/all_flags.train.json
+ [[ ! -f /tmp/e3d/train/all_flags.train.json ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/graph.pbtxt
+ [[ ! -f /tmp/e3d/train/graph.pbtxt ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/model.ckpt-00000005.data-00000-of-00001
+ [[ ! -f /tmp/e3d/train/model.ckpt-00000005.data-00000-of-00001 ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/model.ckpt-00000005.meta
+ [[ ! -f /tmp/e3d/train/model.ckpt-00000005.meta ]]
script.sh: 37: script.sh: [[: not found
+ assert_file_exists /tmp/e3d/train/model.ckpt-00000005.index
+ [[ ! -f /tmp/e3d/train/model.ckpt-00000005.index ]]
script.sh: 37: script.sh: [[: not found
"
""
""
"Hello!

Thanks for the amazing work and sharing it! 

Through your recent addition of gen_train_tfrecords.py file under tools I am encountering an error regarding csv row length as follows.

'''
File ""tools/gen_train_tfrecords.py"", line 241, in load_2d_keypoints_and_write_tfrecord_with_3d_keypoints
    raise ValueError('CSV row has length {} but it should have an even'
ValueError: CSV row has length 45 but it should have an evennumber of elements.
'''

From the gen_train_tfrecords.py file (line 22) I am having the row length of 45 itself. Attaching the screen shot for your reference.
![Screenshot from 2020-10-16 17-44-11](https://user-images.githubusercontent.com/38154800/97411758-fb323c80-1900-11eb-9320-f0fe0c5c3df8.png)

I request to help me to correct the csv row length in order to proceed to generate the tfrecod file for training.

Thanks in advance."
"Based on my understanding only.
Thank you, thank you and thank you."
"At line https://github.com/google-research/google-research/blob/master/performer/fast_self_attention/fast_self_attention.py#L357 the r matrix from the q,r decomposition is ignored. This is not good, since the r matrix is needed to ensure that the samples are uniform over the sphere. See for example the init in pytorch: https://pytorch.org/docs/stable/_modules/torch/nn/init.html#orthogonal_

and the paper https://arxiv.org/pdf/math-ph/0609050.pdf

Not performing this fix significantly increases the variance of the estimate."
"Hi, leaving an issue here because the author's statement on the Controlled Noisy Web Label page states, ""Please contact us via controlled-noisy-labels@googlegroups.com, if you have any questions or difficulty in reproducing our results on this dataset."" However, the googlegroups does not exist. 

When downloading the images through the given URLs, too many of the image URLs are broken. In order to perform honest comparisons for progress in the field, could the authors propose an alternative solution? 

Thanks"
"(sorry for duplicated post, as my browser reported error on the first post)"
"Thanks for all the great works on [Closed Book QA w/ T5](https://github.com/google-research/google-research/tree/master/t5_closed_book_qa)  !!

As the paper illustrated only performances of XXL-SSM models, 
I have two questions on other checkpoints:

1) Could you please report the test performances on all datasets of other released checkpoints ?? 
In particular,
T5.1.1-small + SSM + Natural Questions (train)
T5.1.1-XL + SSM + Natural Questions (train)

2) Since T5-XL & XXL are too big to train in Colab-pro (high-ram mode), so
is it possible to release T5.1.1-large + SSM + Natural Questions (train) checkpoint ?

(EDIT : I humbly tag @adarob as the topic author)"
"Hi, if I run the 4th colab cell in the [colab notebook](https://colab.research.google.com/drive/1BYJzTx2MiJWbM4ydFoQvu2yon65banBj) specified in the task_set readme

with
```
print(""Tasks total: %s""%len(tasks))
print(tasks)
```
instead of
```
print(""Tasks total: %s""%len(tasks))
print(tasks[0:20])
```
I get the following error:

`tensorflow.python.framework.errors_impl.NotFoundError: /data/aad/task_set_data/word_rnn_language_model_family_seed75/nadamw_grid_1k_10000_replica5.npz; No such file or directory`
Can you please upload this file and check if other files are also missing? 
Appreciated!"
"Hello!

Thanks for the great work and sharing it publicly! Definitely one of the best papers I've read this summer.

I need help running it though:
1. AFAIK, there are no pre-trained models shared, are there?
2. Looking at the [poem](https://github.com/google-research/google-research/tree/master/poem) code, we have the training code, and it uses a sample tfrecords file, namely `poem/testdata/tfe-2.tfrecords`. Is there a full version of this file? Or is there code to generate a full version? 

Thanks in advance!"
"Hi,
I am going to train the struct2depth model following the instructions at https://github.com/tensorflow/models/tree/archive/research/struct2depth

My environment is Ubuntu 18.04
Python version is Python2
Tensorflow version is 1.9

First, I run the python gen_data*_ command to generate the dataset, but the terminal shows there is no train.txt file, so I create a empty train.txt file in the output directory. Then I started to train the model using
python train.py
--logtostderr
--checkpoint_dir $ckpt_dir
--data_dir $data_dir
--architecture resnet
--imagenet_ckpt $imagenet_ckpt
--imagenet_norm true
--joint_encoder false

Then the terminal shows
tensorflow.python.framework.errors_impl.NotFoundError: CITYSCAPES_SEQ2_LR/train.txt; No such file or directory.

Can you please tell me what the train.txt file used for? And how can I solve this error?

Thank you very much!"
"Is there any inference code for the video_structure repo? 
As far as I can see, there is no code, which uses the keypoints generated from the VRNN (=predicted_keypoints) to generate the predicted image sequence. Instead it uses the keypoints extracted from the input sequence (=observed_keypoints)
The architecture to train the model is like 
image_sequence --> keypoints --> reconstructed_image_sequence-->Recon loss
...................................................|
..................................................V
....................................dynamics_model --> predicted_keypoints -->VRNN losses

reconstructed_images, observed_keypoints, predicted_keypoints is returned from the whole model. Due to this, reconstructed_images is not meaningfull if you want to see generated images from predicted keypoints from the VRNN.


Am I right? Did I get something wrong? As far as I can see, this code is only for training and not inference.
Is there already existing code to generate images from the predicted keypoints/inference? @mjlm "
"Hi, I have question about input_audio layer shape in stream model.
In my case, I got the `512 length` audio packet and feed this data to kws stream model.
So I set the train arguments as input audio properties (`--sample_rate 16000`, `--window_size_ms 32.0`, `--window_stride_ms 16.0`).
Each window and stride size is 512 and 256, So I expect 512 dim. input_audio layer.
But I find the my converted model input_audio shape is `[1, 256]`.
Why?
"
"I put some work to minimize the code. What I exactly do is to move the sublayers inside SpeechFeature layer to model.py(let's say tc_resnet.py for instance). But now I fail to save model.(It can be trained without any bug.) This is not a bug, it's just my dumb question instead but maybe @rybakov know what I should do? I am sorry to bother you again. I've done so many changes but the the code structure doesn't change much.

### To make it clear, this is what I do. I think you would understand instantly 😄
```
  # net = speech_features.SpeechFeatures(
  #       speech_features.SpeechFeatures.get_params(flags),
  #       name='feature_layer')(
  #         net)
  net = dataframe.DataFrame(
        inference_batch_size=1,
        frame_size=frame_size,
        frame_step=frame_stride)(
        net)
  net = windowing.Windowing(
        window_size=frame_size, 
        window_type=flags.window_type)(
        net)
  net = magnitude_rdft_mel.MagnitudeRDFTmel(
        num_mel_bins=flags.mel_num_bins,
        lower_edge_hertz=flags.mel_lower_edge_hertz,
        upper_edge_hertz=flags.mel_upper_edge_hertz,
        sample_rate=flags.sample_rate,
        mel_non_zero_only=flags.mel_non_zero_only)(
        net)
  net = Lambda(lambda x: tf.math.log(tf.math.maximum(x, flags.log_epsilon)))(net)
  net = dct.DCT(num_features=flags.dct_num_features)(net)
```

The error I got when running this line `model.save(path_model, include_optimizer=False, save_format='tf')`
**Error**
`'Not JSON Serializable:', Namespace(background_frequency=0.8, background_volume=0.1, batch_size=1, bn=0, channels='36, 36, 36, 36, 36, 36, 36', clip_duration_ms=1000, data_dir='./data_test', dct_num_features=30, debug=0, desired_samples=16000, dropout=0.0, eval_step_interval=200, first_kernel='(1,1)', generate=0, groups=1, how_many_training_epochs='1', kernel_size='(3,1)', label_count=12, learning_rate='0.001', log_epsilon=1e-12, lr_schedule='linear', mel_lower_edge_hertz=20.0, mel_non_zero_only=1, mel_num_bins=80, mel_upper_edge_hertz=7000.0, mobile=1, optimizer='adam', preemph=0.0, print=0, qkeras=0, resample=0.15, residual=0, sample_rate=16000, save_step_interval=1000, silence_percentage=10.0, spectrogram_length=49, start_checkpoint='', summaries_dir='./tcn/kws_test/logs/', testing_percentage=10, time_shift_ms=100.0, train=1, train_dir='./tcn/kws_test', unknown_percentage=10.0, validation_percentage=10, volume_resample=0.0, wanted_words='yes,no,up,down,left,right,on,off,stop,go', window_size_ms=40.0, window_size_samples=640, window_stride_ms=20.0, window_stride_samples=320, window_type='hann'))`"
"**Description:**

I'm trying to build ScaNN from source. I ran `python configure.py` and followed the fixes on #342 (modified `.bazelrc`) and #363 (replaced all `#include <hash_set>`). But when I ran `bazel build`, I got the following errors:

- `error: unknown type name '__m256'`
- `error: use of undeclared identifier '_mm256_setzero_ps'; did you mean '_mm_setzero_ps'?`
- `error: use of undeclared identifier '_mm256_loadu_ps'; did you mean '_mm_loadu_ps'?`

**Environment:**

I'm using bazel 3.5.0-homebrew on macOS Catalina 10.15.6.

Any idea about this error? Many thanks!

Below is the full error message:

<img width=""1048"" alt=""Screen Shot 2020-09-22 at 6 38 06 PM"" src=""https://user-images.githubusercontent.com/32108904/93955927-b87bb400-fd05-11ea-83ec-aea2d2fcb12a.png"">

<img width=""1048"" alt=""Screen Shot 2020-09-22 at 6 38 37 PM"" src=""https://user-images.githubusercontent.com/32108904/93955944-c6313980-fd05-11ea-843f-a6b2c6f18923.png"">
"
"I followed the instructions in README to build Scann from source.
- python configure.py
- CC=clang-8 bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg
- ./bazel-bin/build_pip_pkg

All three commands passed without any errors.
But after installation, when I'm trying to import scann, it shows the following error
**Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/scann/__init__.py"", line 1, in <module>
    from scann.scann_ops.py.scann_builder import ScannBuilder
  File ""/usr/local/lib/python3.6/dist-packages/scann/scann_ops/py/scann_builder.py"", line 18, in <module>
    from scann.scann_ops.py import scann_ops_pybind
  File ""/usr/local/lib/python3.6/dist-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 30, in <module>
    import scann_pybind
ImportError: /usr/local/lib/python3.6/dist-packages/scann/scann_ops/cc/python/scann_pybind.so: undefined symbol: _Py_ZeroStruct**

Any idea on this error?"
"Hi thanks for the cool project. 
I have trouble to serialize the tensorflow bounded version of scann. It seems this is not implemented yet. 

```
AttributeError                            Traceback (most recent call last)
<ipython-input-51-e26e47784079> in <module>
      1 searcher = scann_builder.create_tf()
----> 2 searcher.serialize('.scann_artefacts/glove/')

AttributeError: 'ScannSearcher' object has no attribute 'serialize'
```"
"Hi, I am trying to get weight values from the network. I am looking at [get_sparsity](https://github.com/google-research/google-research/blob/6beaa7165e832b3b0cbfc4b27e20814d23675ce9/state_of_sparsity/checkpoint_sparsity.py#L63) and seem like [tensor_name](https://github.com/google-research/google-research/blob/6beaa7165e832b3b0cbfc4b27e20814d23675ce9/state_of_sparsity/checkpoint_sparsity.py#L82) stays empty causing `nnz` to be 0.

I am using command line:
```bash
python checkpoint_sparsity.py --checkpoint /home/data/magnitude_pruning/model.ckpt-500000.index
```

Where /home/data/magnitude_pruning has extracted https://storage.googleapis.com/tsos/checkpoints/sparse_transformer/magnitude_pruning_0.5_100000_400000_10000_0.1_0.1.tar .

cc: @tgale96."
"I am a little confused about ah.
Many thanks for answers."
"Hi @gariel-google ,

During calculation of consistency loss, you mask the rgb and depth errors with binary mask, and the average the results:
```  
depth_error = tf.reduce_mean(
      tf.abs(frame2depth_resampled - frame1transformed_depth.depth) *
      frame1_closer_to_camera)

  rgb_error = (
      tf.abs(frame2rgb_resampled - frame1rgb) * tf.expand_dims(
          frame1_closer_to_camera, -1))
  rgb_error = tf.reduce_mean(rgb_error)
```
But  averaging on all the error ""pixels"" reduces the loss. Wouldn't averaging only the valid (masked) error values be more correct?

Thank you,
Adi"
Are there any updates regarding releasing CuBERT dataset and checkpoints?
"I want to test the model in another dataset called ""mobvoi_hotword_dataset"".
And  I changed its file folder like ""speech commond"", but when train the model and run the sess, it happens an error
""UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 17: invalid start byte“

![4](https://user-images.githubusercontent.com/56855140/92067704-85fe2b80-edd7-11ea-8c59-202578e7df1d.gif)

I don't know what will be done in the sess,and can't solve it.It's really thank you, if you can give me some suggestions!
I use the data below.

![1](https://user-images.githubusercontent.com/56855140/92067526-20aa3a80-edd7-11ea-82a0-c27eaa972170.png)

"
"We could get 
datapoint_to_token.npy,
hashed_dataset.npy,
scann_config.pb,
ah_codebook.pb,
serialized-partitioner.pb. from scann.

Then we could get what information from these files. And  which message in the proto should we use to parse these pb file.
many many Thanks for answers~
"
"when use ""searcher.serialize()"",we could get some file,then how to open the .pb file ?"
"@rybakov 
In the paper [streaming keyword spotting on mobile devices](https://arxiv.org/abs/2005.06720).
```
3.8 Temporal Convolution ResNet(TC-ResNet)
`""To improve accuracy of this model we increase number of parameters from 305k to 365k"".`
```
The TC-ResNet14-1.5 is 305K, so I want to know the config of 365k model.Would you supply me with the config of 365k model.

I tried the default hyper parameters of author as follow:
```
$CMD_TRAIN \
--data_url '' \
--data_dir $DATA_PATH/ \
--train_dir $MODELS_PATH/tc_resnet/ \
--mel_upper_edge_hertz 7000 \
--how_many_training_steps 10000,10000,10000 \
--learning_rate 0.1,0.01,0.001 \
--window_size_ms 30.0 \
--window_stride_ms 10.0 \
--mel_num_bins 40 \
--dct_num_features 40 \
--resample 0.15 \
--alsologtostderr \
--train 1 \
--optimizer 'momentum' \
--lr_schedule 'linear' \
--use_spec_augment 1 \
--time_masks_number 2 \
--time_mask_max_size 10 \
--frequency_masks_number 2 \
--frequency_mask_max_size 5 \
tc_resnet \
--kernel_size '(9,1)' \
--channels '24, 36, 36, 48, 48, 72, 72' \
--debug_2d 0 \
--pool_size '' \
--pool_stride 0 \
--l2_weight_decay 0.001 \
--bn_momentum 0.997 \
--bn_center 1 \
--bn_scale 1 \
--bn_renorm 0 \
--dropout 0.5
```

But I can not reproduce the accuracy(96.6% in paper) on V1 dataset.

Another question, why don't you add weight_decay in tc_resnet?
https://github.com/google-research/google-research/blob/3a228f1d94e6369072b2aeb3ae81644a95c53d93/kws_streaming/models/tc_resnet.py#L139-L145
"
"@rybakov 

We use the training script listed [here](https://github.com/google-research/google-research/blob/master/kws_streaming/experiments/kws_experiments_paper.md#att_mh_rnn).

https://github.com/google-research/google-research/blob/898f3471b88411e9e78155e4e5ca050c31c5c56b/kws_streaming/models/att_mh_rnn.py#L202-L203

The default param of `flags.return_softmax` is 0. 

https://github.com/google-research/google-research/blob/898f3471b88411e9e78155e4e5ca050c31c5c56b/kws_streaming/models/model_params.py#L70


If I use softmax, the accuracy will be decreased. 

"
"I installed by source and it was successful after I did this [fix](https://github.com/google-research/google-research/issues/342) but it seems that is not finding scann and when I run `python -c ""import scann""` it shows me this error


`Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/user/.pyenv/versions/3.7.3/lib/python3.7/site-packages/scann/__init__.py"", line 1, in <module>
    from scann.scann_ops.py.scann_builder import ScannBuilder
  File ""/user/.pyenv/versions/3.7.3/lib/python3.7/site-packages/scann/scann_ops/py/scann_builder.py"", line 17, in <module>
    from scann.scann_ops.py import scann_ops
  File ""/user/.pyenv/versions/3.7.3/lib/python3.7/site-packages/scann/scann_ops/py/scann_ops.py"", line 25, in <module>
    ""cc/_scann_ops.so""))
  File ""/user/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow_core/python/framework/load_library.py"", line 57, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/user/.pyenv/versions/3.7.3/lib/python3.7/site-packages/scann/scann_ops/cc/_scann_ops.so, 6): image not found
`


Any ideas on what can this be?  I'm using Mac Catalina 10.15.6

Thanks a lot!
"
"Hello:
   I have install bazel 0.21.0, clang 8 and gcc 9. And the step ""python configure.py"" runs successfully, but when I try step 2: CC=clang-8 bazel build -c opt --copt=-mavx2 --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --cxxopt=""-std=c++17"" --copt=-fsized-deallocation --copt=-w :build_pip_pkg, it fails with:
ERROR: scann/scann_ops/py/BUILD.bazel:28:1: //scann/scann_ops/py:scann_ops_test: no such attribute 'python_version' in 'py_test' rule
ERROR: py/BUILD.bazel:6:1: Target '//scann/scann_ops/py:scann_ops' contains an error and its package is in error and referenced by '//scann/scann_ops/py:scann'
ERROR: scann_ops/py/BUILD.bazel:6:1: Target '//scann/scann_ops/py:scann_ops_pybind' contains an error and its package is in error and referenced by '//scann/scann_ops/py:scann'
ERROR: /BUILD.bazel:6:1: Target '//scann/scann_ops/py:scann' contains an error and its package is in error and referenced by '//:build_pip_pkg'
ERROR: Analysis of target '//:build_pip_pkg' failed; build aborted: Analysis failed
INFO: Elapsed time: 0.038s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 21 targets configured)
 Have you ever meet the same problem? can you help me have a look at this problem?

"
"This question is related to https://github.com/google-research/google-research/tree/master/kws_streaming and https://arxiv.org/pdf/2005.06720.pdf
Batch normalization calculate average and variance and apply it to entire feature map. So the entire feature map is changed, the convolution filter has to compute the entire feature map. Then how is that ""streamable"" and how do you do it? Thanks! "
"Code:
```
def f(x):
       return np.random.rand(100)

train_data['emb'] = train_data['text'].apply(f)
test_data['emb'] = test_data['text'].apply(f)

dataset = np.array(train_data['emb'].tolist(), dtype=np.float32)
queries = np.array(test_data['emb'].tolist(), dtype=np.float32)
```

Error:
```
normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]
# configure ScaNN as a tree - asymmetric hash hybrid with reordering
# anisotropic quantization as described in the paper; see README

#  ----- At this line -----

searcher = scann.ScannBuilder(normalized_dataset, 10, ""dot_product"").tree(
    num_leaves=2000, num_leaves_to_search=100, training_sample_size=250000).score_ah(
    2, anisotropic_quantization_threshold=0.2).reorder(100).create_pybind()

Traceback (most recent call last):
  File ""/home/dev/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-7-c748bb36181c>"", line 6, in <module>
    2, anisotropic_quantization_threshold=0.2).reorder(100).create_pybind()
  File ""/home/dev/.local/lib/python3.6/site-packages/scann/scann_ops/py/scann_builder.py"", line 203, in create_pybind
    self.training_threads)
  File ""/home/dev/.local/lib/python3.6/site-packages/scann/scann_ops/py/scann_ops_pybind.py"", line 60, in create_searcher
    scann_pybind.ScannNumpy(db, scann_config, training_threads))
RuntimeError: Error initializing searcher: 
```"
"Hi, I have a question about input embedding for tft model.
At tft.model.py line 540, I have a Dense layer one to many problem.
Input of the dense layer is regular_inputs[:, 0, i:i + 1], equal [:,1], and first dim is batch
Output of the dense layer is [:,self.hidden_layer_size]
this is a one value to vector problem. Is this reasonable?

    # Static inputs
    if self._static_input_loc:
      static_inputs = [tf.keras.layers.Dense(self.hidden_layer_size)(
          regular_inputs[:, 0, i:i + 1]) for i in range(num_regular_variables)
                       if i in self._static_input_loc] \
          + [embedded_inputs[i][:, 0, :]
             for i in range(num_categorical_variables)
             if i + num_regular_variables in self._static_input_loc]
      static_inputs = tf.keras.backend.stack(static_inputs, axis=1)

"
The code only uses single GPU and causes OOM for batch size == 32
"Hi, 
This is a question for the repo for the ValueDice paper, (perhaps @peterjliu or Ilya Kostrikov is the relevant person?) 
I see that there is a softmax (instead of a log-sum-exp) involved to compute the J_linear part of the valuedice loss : https://github.com/google-research/google-research/blob/7c7fdac0d57ef262896776785a59d824026a076b/value_dice/value_dice.py#L27 
was wondering if you could explain why that is?
Is this something to do with wanting to normalise this part of the loss for some reason?
Would really appreciate any clarification.

Thanks!
Gunshi"
"
Hi, I'm trying to export a mobilebert model to tflite format.

**Environment**
Docker (tensorflow/tensorflow:1.15.0-gpu-py3) image
V100 16GB

As guided in README.md., I followed ""Run Quantization-aware-training with Squad"" then ""Export an integer-only MobileBERT to TF-Lite format."" However, I got an error while converting to quantized tflite model.

> 2020-07-15 10:26:10.934857: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-07-15 10:26:10.934903: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4461 nodes (-1120), 4701 edges (-1124), time = 779.203ms.
2020-07-15 10:26:10.934931: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4461 nodes (0), 4701 edges (0), time = 374.792ms.
Traceback (most recent call last):
  File ""run_squad.py"", line 1517, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""run_squad.py"", line 1508, in main
    tflite_model = converter.convert()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 993, in convert
    inference_output_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py"", line 239, in _calibrate_quantize_model
    inference_output_type, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
    np.dtype(output_type.as_numpy_dtype()).num, allow_float)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
RuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0

I used pre-trained weights (uncased_L-24_H-128_B-512_A-4_F-4_OPT) that mentioned in README.md.
Is it required to distillation process before quantization-aware-training? 


Regards,
Dongjin.
"
"Hi,

I am looking at the genomics OOD dataset and the [paper](https://arxiv.org/pdf/1906.02845.pdf).

The paper says: ""Among all the short sequences, we randomly choose 100,000 sequences for each class for training, validation, and test.""

Is the released [dataset](https://drive.google.com/drive/folders/1Ht9xmzyYPbDouUTl_KQdLTJQYX2CuclR) purely composed of these selected sequences used in the experiments? It doesn't seem to be so on first glance since the OOD test and validation datasets seem to be ~6x larger than the corresponding in-distribution datasets - if all cases had the same number of sequences, then they should be all the same size.

So it seems that the provided google drive actually contains all of the data, and from this dataset 100,000 sequences were subsampled per class for performing the experiments in the paper. Does a class here mean a bacterial name such as ""Actinoplanes, Advenella"" etc? Is there a script or function that performs this random sampling that I can use?

Thanks in advance. The dataset and experiments are very comprehensive and interesting. It would be great if I could have a little more clarity on these minor details.

Best Regards"
"Firstly I would like to thank you for providing the source code about keyword spotting.

The issues are following:

1.When I run inference.ipynb, the error occurs. It can not import ""_HOTWORD_MODEL_PARAMS"", too. It seems that utils_test.py does not declare ""Flags"" and ""_HOTWORD_MODEL_PARAMS"".
= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
ImportError   Traceback (most recent call last)
ipython-input-66-2f238aecf54f in module
        3 import tensorflow.compat.v1 as tf1
        4 import logging
----> 5 from kws_streaming.models.utils_test import Flags, _HOTWORD_MODEL_PARAMS

ImportError: cannot import name 'Flags' from 'kws_streaming.models.utils_test'
= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 

2.How can I know the predicted label after inference? The source code only outputs the index number. There might be 12 classes including ""unknown"" and ""silence"".
print(flags.wanted_words) # 10 predefined words: yes,no,up,down,left,right,on,off,stop,go
"
"I'm very interested in the work presented in the KWS streaming project. Do TFLite models converted to streaming mode (internal or external state) also work (in streaming mode) when used via [TF Lite CoreML delegate](https://www.tensorflow.org/lite/performance/coreml_delegate) on iOS devices? In this case, can `mfcc_op` be used on CoreML/iOS, or only `mfcc_tf` (or do speech feature extraction outside of the model)?
@rybakov "
"@yanfengliu @pigletsc 
Assuming you converted the torch model to tensorflow model and are having the following files for imagenet checkpoint:

init/
  checkpoint
  model.ckpt.data-00000-of-00001 
  model.ckpt.index
  model.ckpt.meta

in this case you have to specify --imagenet_ckpt=.../init/model.ckpt 
So not even an existing filename (and not the .index file, or the init dir)
That works for me

@pigletsc 
Regarding 'Tensor had NaN values'. Issue #162  is about this problem. There is an important bug in CUDA 10.0, and the related tensorflow 1.x (x>=13.1) binaries are built with CUDA 10.0. We built tensorflow 14.0 with CUDA 10.1. It works that way. Or you can try the training only on CPU. :)

_Originally posted by @suvigy in https://github.com/google-research/google-research/issues/130#issuecomment-611591025_"
"I had read your paper in AAAI 2020 about fairness metrics, but when I run your code there occurs error about:
`AttributeError: module 'tensorflow' has no attribute 'function'，`
and the first place in traceback is 
```
AttributeError                            Traceback (most recent call last)
<ipython-input-3-c69db21a2d00> in <module>()
      1 # Tensor flow lattice modules.
----> 2 import tensorflow_lattice as tfl
      3 
      4 # Tensorflow constrained optimization modules.
      5 import tensorflow_constrained_optimization as tfco
```

Please take a look, thank you!

"
"I meet a problem when reading the source code of Distilling Effective Supervision from Severe Label Noise(IEG). In the paper, the re-weighting coefficients are updated in this way:
![image](https://user-images.githubusercontent.com/22375420/85141951-34371200-b27a-11ea-82df-2e3cd8d1d76e.png)
Then I find the corresponding implementation in https://github.com/google-research/google-research/blob/master/ieg/models/model.py:
![image](https://user-images.githubusercontent.com/22375420/85142232-9f80e400-b27a-11ea-98ef-8a10b8c1d2ca.png)
So **raw_weight =  target - grad_target - init_eps_val**
![image](https://user-images.githubusercontent.com/22375420/85142472-07372f00-b27b-11ea-8d45-a1c10680b653.png)
So target is constant: **target = init_eps_val**
Then we simply get: **raw_weight = -grad_target**, so the updating equation would be written in this way:
![image](https://user-images.githubusercontent.com/22375420/85143441-56ca2a80-b27c-11ea-999a-2f48c4093775.png)

and it doesn't satisfy the updating strategy mentioned in the paper. So is there anything wrong with the implementation of code, or the formula in the paper?"
"The project is KWS-streaming.

In training, it is OK for me. But when trying to convert non-streaming model to streaming model about 'STREAM_INTERNAL_STATE_INFERENCE' state, I have met a problem.

- It convert streaming model failed. The information is follow:
```shell
WARNING: failed to convert to SavedModel: 'Node' object has no attribute 'outputs'
```

My question:
1. How could I  convert non-stream model to stream model successfully?
2. My view as follow is right or not ?If right, how to solve it?

The coding is here:
1. [ test.convert_model_saved](https://github.com/google-research/google-research/blob/c880344f15d2453ab7eccbfea0b259c7019094f6/kws_streaming/train/model_train_eval.py#L162)
2. [utils.model_to_saved(model, flags, path_model, mode)](https://github.com/google-research/google-research/blob/c880344f15d2453ab7eccbfea0b259c7019094f6/kws_streaming/train/test.py#L610)
3. [model = to_streaming_inference()](https://github.com/google-research/google-research/blob/c880344f15d2453ab7eccbfea0b259c7019094f6/kws_streaming/models/utils.py#L369)
4. [model_inference = convert_to_inference_model()](https://github.com/google-research/google-research/blob/c880344f15d2453ab7eccbfea0b259c7019094f6/kws_streaming/models/utils.py#L283)
5. [new_model = _clone_model(model, input_tensors)](https://github.com/google-research/google-research/blob/c880344f15d2453ab7eccbfea0b259c7019094f6/kws_streaming/models/utils.py#L243)
6. [models._clone_layers_and_model_config()](https://github.com/google-research/google-research/blob/c880344f15d2453ab7eccbfea0b259c7019094f6/kws_streaming/models/utils.py#L122)

The problem is in six, there hava something wrong when `clone layer configs`. In my opinion, maybe the question is after copying layers config, the new layers losses the `inbound nodes`, so the new layers not support any  `input` or `output`. The fisrt input layer looks ok beacause it have inbounds nodes. However the others is wrong. 

emm, I could not solve it, need some help. thx

> the pc & training information:
- ubuntu 16.04, tf 2.2.0
```shell
--data_dir ../data \
--lr_schedule linear \
--background_volume 0.1 \
--l2_weight_decay 0.0 \
--background_frequency 0.8 \
--split_data 1 \
--time_shift_ms 100.0 \
--how_many_training_steps 10000,5000 \
--eval_step_interval 400 \
--learning_rate 0.0001,0.00005 \
--batch_size 100 \
--wanted_words yes,no,up,down,left,right,on,off,stop,go \
--train_dir ./speech_commands_train/ \
--save_step_interval 100 \
--start_checkpoint '' \
--resample 0.15 \
--train 1 \
--window_size_ms 40 \
--window_stride_ms 40 \
--dct_num_features 20 \
--preprocess mfcc \
--feature_type mfcc_tf \
--mel_lower_edge_hertz 20.0 \
--mel_upper_edge_hertz 7000 \
--fft_magnitude_squared 0 \
--mel_num_bins 40 \
cnn \
--cnn_filters 64,64,64,64,128,64,128 \
--cnn_kernel_size '(3,3),(5,3),(5,3),(5,3),(5,2),(5,1),(3,1)' \
--cnn_dilation_rate '(1,1),(1,1),(1,1),(1,1),(1,1),(1,1),(1,1)'
```"
"I have a question about TCC implementation.
In dataset.py, line 144.
https://github.com/google-research/google-research/blob/541f6d8637effe03994b03d54a9eee7a375bd27f/tcc/datasets.py#L136-L146

""We don't want to encode information from the future. ""
Why not embed future frame jointly?
i thought that using future frames make representation better.

i already read tcc paper, but i couldn't figure out the reason.
i hope @debidatta or other professionals give me answear.
Sorry disturb you, thank you."
"I can't alignment baseball_pitch of Penn Action from embedding data.
Please tell me how to deal with this problem.
```
+ python -m tcc.visualize_alignment --alsologtostderr --video_path /tmp/aligned.mp4 --embs_path /tmp/embeddings.npy
I0609 11:23:38.827201 139762231441152 visualize_alignment.py:214] Found files: ['/tmp/embeddings.npy']
dict_keys(['embs', 'seq_lens', 'steps', 'names', 'seq_labels', 'frames'])
Traceback (most recent call last):
  File ""/home/ubuntu/anaconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/ubuntu/anaconda3/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ubuntu/dev/google-research/tcc/visualize_alignment.py"", line 255, in <module>
    app.run(main)
  File ""/home/ubuntu/dev/google-research/env/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/ubuntu/dev/google-research/env/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/ubuntu/dev/google-research/tcc/visualize_alignment.py"", line 251, in main
    visualize()
  File ""/home/ubuntu/dev/google-research/tcc/visualize_alignment.py"", line 238, in visualize
    interval=FLAGS.interval)
  File ""/home/ubuntu/dev/google-research/tcc/visualize_alignment.py"", line 115, in create_video
    nns = align(embs[query], embs[candidate], use_dtw)
TypeError: list indices must be integers or slices, not NoneType
```"
"@gariel-google
Dear author, Thanks for sharing the source code of the paper. 

I have a question about the scale factor of the predicted translation vector. 
Basically, I tried to use the predicted depth and  the predicted egomotion to warp the image T to image T+1. 
I first project image T using the predicted depth get a 3D pointcloud then use the predicted translation vector, rotation matrix and camera intrinsic to project the 3D pointcloud back to an image space (T+1). However, I can’t get a proper warped image T+1 WITHOUT scaling the translation vector.  After some trial and error, I found that the scale factor for the translation vector is about 0.001. 

Therefore, my question is: do you need such a scaling factor for the translation vector when you try to project image T to image T+1 (or inversely) to compute the reconstruction loss? I am quite confused as I understand the predicted depth’s scale and the predicted egomotion’s scale should be matched and thus we should be able to do reconstruction without scaling those variables.

Any help would be appreciated. Thank you in advance. "
"I  just type 
""python -m depth_from_video_in_the_wild.train \
  --data_dir=depth_from_video_in_the_wild/data_example \
  --checkpoint_dir=/tmp/my_experiment --train_steps=1""
 for test.

following err:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/train.py"", line 231, in <module>
    app.run(main)
  File ""/home/lvzhen/py351/lib/python3.5/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/lvzhen/py351/lib/python3.5/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/train.py"", line 147, in main
    input_file=FLAGS.input_file)
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/model.py"", line 92, in __init__
    self._build_train_graph()
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/model.py"", line 114, in _build_train_graph
    self._build_loss()
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/model.py"", line 301, in _build_loss
    trans, inv_rot, inv_trans), i, j)
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/consistency_losses.py"", line 209, in rgbd_and_motion_consistency_loss
    frame2depth, frame2rgb)
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/consistency_losses.py"", line 65, in rgbd_consistency_loss
    frame2depth_resampled = _resample_depth(frame2depth, pixel_xy)
  File ""/home/lvzhen/depth/depth_from_video_in_the_wild/consistency_losses.py"", line 289, in _resample_depth
    result = contrib_resampler.resampler.resampler(depth, coordinates)
AttributeError: 'function' object has no attribute 'resampler'

tensorboard              1.14.0
tensorboard-plugin-wit   1.6.0.post3
tensorflow               1.14.0
tensorflow-datasets      3.1.0
tensorflow-estimator     1.14.0
tensorflow-gpu           1.14.0
tensorflow-graphics      1.0.0





"
"Hi,

I need to extract the intermediate values after infer. I knew how to do it in Keras, something similar to what is suggested [here](https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction). I am using state_of_sparsity folder of this repository. I tried to get the layers using _layers but it returns empty. 

```
translate_model = registry.model(FLAGS.model)(hparams, Modes.EVAL)
tf.print(translate_model._layers)
```
I have posted my question [here](https://stackoverflow.com/questions/61916210/how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction-in-tens). I was wondering why _layers returns empty. Is there any other way of cutting tensor2tensor models at a specific layer or extracting intermediate values after infer? "
"Hi, so I was wondering if there is any plans on releasing a pre-trained model optimized for tflite, or maybe if there is a way to train the current released model for a specific language, without having to do it from scratch."
"Hi,

I had this error when attempting to run an experiment using a modified script file.

Random seed = 2334608643
[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:317] Error parsing text-format automl_zero.SearchExperimentSpec: 1:672: Expected identifier, got: \
WARNING: Logging before InitGoogleLogging() is written to STDERR
F20200520 00:14:48.154529 26371 definitions.h:229] Check failed: google::protobuf::TextFormat::ParseFromString(str, &proto) 

It has been mentioned that the datasets supported are only MNIST and CIFAR10, but upon looking the generate_datasets.py it might be possible to simply pass a new numpy dataset as it is the class type required prior to ScalarLabelDataset().

I also found out that the linux ""less"" utility that a regular MNIST protobuf file is recognized as binary, but the custom one does not.

If you have time can you check on this. I forked into  arjay55/google-research. You can try running ./run_logic_multiplier.sh for reproduction.

BTW, I got very interested in your work so  that's why I made some changes and try it on discrete data."
"Hi,
I have a large data set and a bigger cnn for spatial encoder that I'm trying to train using the tcc alignment training. In order to get a long enough sequence length I'm trying to run it with distributed training on one machine with 2 GPUs .

Although train.py runs with tf.distribute.MirroredStrategy() as a scope, the whole process runs only on one GPU. It's visible in 2 ways:

1. nvidia-smi - only one GPU is fully utilized while the other one is on 0% (attached)
![image](https://user-images.githubusercontent.com/30262079/82154160-b47de800-9874-11ea-9337-ffa24787d2b7.png)

2. vector shape during run time - adding prints of tensors' shapes during train step reveals that the data and the tensors in one replica have actually the same shape as the full batch data and the data is paralelized.  

Is there anything I'm doing wrong or is this really a bug?

Thanks!"
"I just found the distill code of mobilebert, I want to ask do you provide the pretrain code about IB-BERT LARGE?"
I believe the .scripts folder was from an older version; readme still lists it.
"(EDIT: posted same issue in T5 main repo, which is probably a better fit. I'm okay with this being deleted, which I can't do.)

The example script for WT5 is hanging for me, and I'm not sure why. It can connect to my TPU and downloads/extracts a dataset, but then appears to do nothing after finishing the extraction (TPU shows 0.1% usage, no further prints).

I set it to 2x4 tpu topology and set batch size to 2048 tokens.

Script shown, then console output


# Script based on provided script in  WT5 readme

`export PROJECT=my-project-name
export ZONE=us-central1-b
export BUCKET=gs://my-bucket-name
export TPU=node-2

ctpu up   --name=$TPU   --project=$PROJECT  --zone=$ZONE   --tpu-size=v3-8   --tpu-only   --noconf 

TASK=movie_rationales_explanations_take1000_v010
PRETRAINED_DIR=gs://t5-data/pretrained_models/small
PRETRAINED_STEPS=1000000
FINETUNE_STEPS=20000
MODEL_DIR=""${BUCKET}/${TASK}""

t5_mesh_transformer \
  --tpu=""${TPU}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${MODEL_DIR}"" \
  --gin_file=""dataset.gin"" \
  --gin_file=""${PRETRAINED_DIR}/operative_config.gin"" \
  --gin_file=""wt5/gin/sequence_lengths/movie_rationales_v010.gin"" \
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '2x4'"" \
  --gin_param=""MIXTURE_NAME = '${TASK}'"" \
  --gin_param=""mesh_train_dataset_fn.use_cached=False"" \
  --gin_param=""utils.run.save_checkpoints_steps=100"" \
  --gin_param=""utils.run.batch_size=('tokens_per_batch', 16384)"" \
  --gin_param=""utils.run.train_steps=$((PRETRAINED_STEPS+FINETUNE_STEPS))"" \
  --gin_param=""utils.run.init_checkpoint='${PRETRAINED_DIR}/model.ckpt-${PRETRAINED_STEPS}'"" \
  --gin_param=""utils.run.learning_rate_schedule=@learning_rate_schedules.constant_learning_rate"" \
  --gin_param=""constant_learning_rate.learning_rate=1e-3"" \
  --t5_tfds_data_dir=""${BUCKET}/t5-tfds"" \
  --module_import=""wt5.tasks"" \
  --module_import=""wt5.mixtures"" \
  --gin_location_prefix=""wt5/wt5/gin/""
`
# Console Output. 
(Note it shows no sign of any activity at the end, and doesn't return to bash)


`:~/google-research/wt5$ ./testingWT5scriptGCP_TPU 
2020/05/03 02:05:37 TPU already running.
Operation success; not ssh-ing to GCE VM due to --tpu-only flag.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
INFO:tensorflow:model_type=bitransformer
I0503 02:05:43.398267 140562852878144 utils.py:1685] model_type=bitransformer
INFO:tensorflow:mode=train
I0503 02:05:43.398564 140562852878144 utils.py:1686] mode=train
INFO:tensorflow:sequence_length={'inputs': 2048, 'targets': 512}
I0503 02:05:43.398667 140562852878144 utils.py:1687] sequence_length={'inputs': 2048, 'targets': 512}
INFO:tensorflow:batch_size=8
I0503 02:05:43.398747 140562852878144 utils.py:1688] batch_size=8
INFO:tensorflow:train_steps=1020000
I0503 02:05:43.398822 140562852878144 utils.py:1689] train_steps=1020000
INFO:tensorflow:mesh_shape=Shape[batch=16]
I0503 02:05:43.398901 140562852878144 utils.py:1690] mesh_shape=Shape[batch=16]
INFO:tensorflow:layout_rules=ensemble:ensemble,batch:batch,d_ff:model,heads:model,vocab:model,experts:batch
I0503 02:05:43.398976 140562852878144 utils.py:1691] layout_rules=ensemble:ensemble,batch:batch,d_ff:model,heads:model,vocab:model,experts:batch
INFO:tensorflow:Building TPUConfig with tpu_job_name=None
I0503 02:05:43.403095 140562852878144 utils.py:1706] Building TPUConfig with tpu_job_name=None
I0503 02:05:43.406050 140562852878144 discovery.py:280] URL being requested: GET https://www.googleapis.com/discovery/v1/apis/tpu/v1/rest
I0503 02:05:43.441212 140562852878144 discovery.py:911] URL being requested: GET https://tpu.googleapis.com/v1/projects/t5-ignition/locations/us-central1-b/nodes/node-2?alt=json
I0503 02:05:43.441443 140562852878144 transport.py:151] Attempting refresh to obtain initial access_token
I0503 02:05:43.503969 140562852878144 discovery.py:280] URL being requested: GET https://www.googleapis.com/discovery/v1/apis/tpu/v1/rest
I0503 02:05:43.534800 140562852878144 discovery.py:911] URL being requested: GET https://tpu.googleapis.com/v1/projects/t5-ignition/locations/us-central1-b/nodes/node-2?alt=json
I0503 02:05:43.535020 140562852878144 transport.py:151] Attempting refresh to obtain initial access_token
INFO:tensorflow:Using config: {'_model_dir': 'gs://t5-ignition-bucket/movie_rationales_explanations_take1000_v010', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
cluster_def {
  job {
    name: ""worker""
    tasks {
      key: 0
      value: ""10.9.81.234:8470""
    }
  }
}
isolate_session_state: true
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.9.81.234:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.9.81.234:8470', '_evaluation_master': 'grpc://10.9.81.234:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fd6bce7b908>}
I0503 02:05:43.584227 140562852878144 estimator.py:216] Using config: {'_model_dir': 'gs://t5-ignition-bucket/movie_rationales_explanations_take1000_v010', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
cluster_def {
  job {
    name: ""worker""
    tasks {
      key: 0
      value: ""10.9.81.234:8470""
    }
  }
}
isolate_session_state: true
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.9.81.234:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.9.81.234:8470', '_evaluation_master': 'grpc://10.9.81.234:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7fd6bce7b908>}
INFO:tensorflow:_TPUContext: eval_on_tpu True
I0503 02:05:43.584667 140562852878144 tpu_context.py:221] _TPUContext: eval_on_tpu True
INFO:tensorflow:Querying Tensorflow master (grpc://10.9.81.234:8470) for TPU system metadata.
I0503 02:05:43.709766 140562852878144 tpu_system_metadata.py:72] Querying Tensorflow master (grpc://10.9.81.234:8470) for TPU system metadata.
2020-05-03 02:05:43.711139: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
INFO:tensorflow:Initializing TPU system (master: grpc://10.9.81.234:8470) to fetch topology for model parallelism. This might take a while.
I0503 02:05:43.716908 140562852878144 tpu_system_metadata.py:157] Initializing TPU system (master: grpc://10.9.81.234:8470) to fetch topology for model parallelism. This might take a while.
INFO:tensorflow:Found TPU system:
I0503 02:05:49.398839 140562852878144 tpu_system_metadata.py:140] Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
I0503 02:05:49.399126 140562852878144 tpu_system_metadata.py:141] *** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
I0503 02:05:49.399248 140562852878144 tpu_system_metadata.py:142] *** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
I0503 02:05:49.399342 140562852878144 tpu_system_metadata.py:144] *** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 543705933573447767)
I0503 02:05:49.399434 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 543705933573447767)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9459511554632651815)
I0503 02:05:49.399722 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9459511554632651815)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10405845867630879762)
I0503 02:05:49.399811 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10405845867630879762)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1519635725069975015)
I0503 02:05:49.399931 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1519635725069975015)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16532467575488418548)
I0503 02:05:49.400012 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16532467575488418548)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9988226954971163265)
I0503 02:05:49.400092 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9988226954971163265)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2095320529623114246)
I0503 02:05:49.400168 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 2095320529623114246)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5421484898729298314)
I0503 02:05:49.400383 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5421484898729298314)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10057256119691496601)
I0503 02:05:49.400545 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10057256119691496601)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9720856656554682660)
I0503 02:05:49.400689 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9720856656554682660)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16735225736129679856)
I0503 02:05:49.400792 140562852878144 tpu_system_metadata.py:146] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 16735225736129679856)
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
W0503 02:05:49.405328 140562852878144 deprecation.py:506] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
W0503 02:05:49.405760 140562852878144 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
INFO:tensorflow:Calling model_fn.
I0503 02:05:49.412624 140562852878144 estimator.py:1151] Calling model_fn.
I0503 02:05:49.850056 140562852878144 dataset_info.py:426] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: movie_rationales/0.1.0
I0503 02:05:49.879839 140562852878144 dataset_info.py:357] Load dataset info from /tmp/tmplb_43_hwtfds
I0503 02:05:49.881754 140562852878144 dataset_info.py:397] Field info.description from disk and from code do not match. Keeping the one from code.
I0503 02:05:49.881891 140562852878144 dataset_info.py:397] Field info.citation from disk and from code do not match. Keeping the one from code.
I0503 02:05:50.003428 140562852878144 dataset_builder.py:333] Generating dataset movie_rationales (gs://t5-ignition-bucket/t5-tfds/movie_rationales/0.1.0)
Downloading and preparing dataset movie_rationales/0.1.0 (download: 3.72 MiB, generated: Unknown size, total: 3.72 MiB) to gs://t5-ignition-bucket/t5-tfds/movie_rationales/0.1.0...
Dl Completed...: 0 url [00:00, ? url/s]          I0503 02:05:51.025692 140562852878144 download_manager.py:291] URL http://www.eraserbenchmark.com/zipped/movies.tar.gz already downloaded: reusing gs://t5-ignition-bucket/t5-tfds/downloads/eraserbenchmark.com_zipped_moviesZuGNTmyd-en1VEVysL_pKjlnP3Tsv8OFm0bO2y9bLe4.tar.gz.
Dl Completed...: 0 url [00:00, ? url/s] ? file/s]
Dl Size...: 0 MiB [00:00, ? `MiB/s]`

"
"@gariel-google 
Thanks for sharing the code about your paper.
I want to implement your method (in TF2), and I have got somewhat good results on KITTI dataset. Now, I'm trying to implement ""5.4. Camera intrinsics evaluation"". But I don't know how to ""represented each intrinsic parameter as a separate learned variable"".
Would you share me motion_prediction_net.py (TF1/Slim) that treats intrinsics as trainable_variables?
Thank you in advance."
"First of, I'd like to thank the authors for their interesting work! I'm not sure how to make a pull request to this repository, so I'll leave my thoughts here regarding TabNet training. 

**Problem**:
TabNet training is slow because the parsing of the CSV is performed on the entire file each time, instead of performed over a single batch each time (see [line](https://github.com/google-research/google-research/blob/432ad195830f3722e2d2bc6dfe5f39991c384806/tabnet/data_helper_covertype.py#L126)). This results in overhead in CPU processing, reducing the utilization of the GPU. 

**Solution**:
In order to improve the training speed, we set the batching operation before the mapping operation so that the mapping operation only operates on a smaller set of data each iteration as follows:
```
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.map(parse_csv, num_parallel_calls=n_parallel)

    # Repeat after shuffling, to prevent separate epochs from blending together.
    dataset = dataset.repeat(num_epochs)
    return dataset
```
In terms of GPU utilization on an RTX2080Ti, I am getting about 70% utilization compared to 28% previously. More could potentially be done but this is a start.

**Caveat**:
The unfortunate side-effect of this is that the last small batch would be dropped. Fortunately this has not impacted accuracy on a test set.

Padding was tried, but I think I am not getting the shapes right, so I've used this as a solution in the interim.

**Testing**:
Shown here are the test accuracy and test loss when tested on the CoverType dataset. When tested on the CoverType dataset, the accuracy reached on the test set is 96.23% for the final model.

![image](https://user-images.githubusercontent.com/1995167/79059722-bcb89580-7cc0-11ea-9bb6-3087f2a58fdf.png)

![image](https://user-images.githubusercontent.com/1995167/79059768-2f297580-7cc1-11ea-9027-2acf825d0eb3.png)
"
"I noticed that utils.model_to_tflite() is throwing errors with the GRU model. kws_streaming.models.utils_test fails as well with the following error. I observed this with tf2.1.0 as well as tf2.2.0-dev20200402.

```======================================================================
ERROR: test_model_to_tflite (__main__.UtilsTest)
test_model_to_tflite (__main__.UtilsTest)
TFLite supports stateless graphs.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/abhipray/NeoSensory/git-repos/google-research/kws_streaming/models/utils_test.py"", line 85, in test_model_to_tflite
    self.assertTrue(utils.model_to_tflite(self.sess, self.model, self.flags))
  File ""/Users/abhipray/NeoSensory/git-repos/google-research/kws_streaming/models/utils.py"", line 295, in model_to_tflite
    sess, model_stateless_stream.inputs, model_stateless_stream.outputs)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 792, in from_session
    graph_def = _freeze_graph(sess, input_tensors, output_tensors)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/util.py"", line 256, in freeze_graph
    graph_def, input_tensors, output_tensors, config, graph=sess.graph)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/util.py"", line 218, in run_graph_optimizations
    return tf_optimizer.OptimizeGraph(config, meta_graph)
  File ""/anaconda3/lib/python3.7/site-packages/tensorflow/python/grappler/tf_optimizer.py"", line 58, in OptimizeGraph
    graph_id, strip_default_attributes)
ValueError: Failed to import metagraph, check error log for more info.

----------------------------------------------------------------------
Ran 5 tests in 2.938s```"
How do you run multiwoz2.1 for schema_guided_dst since multiwoz2.1 does not have user intents and slot request actions? Do you set all slots to non-categorical slots?
"It appears that some of the variables in the motion_field_net from [google-research/depth_from_video_in_the_wild/motion_prediction_net.py](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/motion_prediction_net.py) are not being reused when the the model is initialized for the forward and inverse ego-motion prediction.

When inspecting the variables  using
```
tf_path = os.path.abspath('kitti_learned_intrinsics/model-248900')  
tf_vars = tf.train.list_variables(tf_path)
print(tf_vars)
```
and only considering variables in the scope 'MotionFieldNet'
there are four redundant name spaces:

('MotionFieldNet/compute_loss/MotionFieldNet/...
('MotionFieldNet/compute_loss/MotionFieldNet_1/...
('MotionFieldNet/compute_loss/MotionFieldNet_2/...
('MotionFieldNet/compute_loss/MotionFieldNet_3/...

However when I initialize the model and comment out the second call of motion_field_net (as in the following code block from [google-research/google-research/depth_from_video_in_the_wild/model.py](https://github.com/google-research/google-research/blob/master/depth_from_video_in_the_wild/model.py))
```
def _build_egomotion_test_graph(self):
    """"""Builds graph for inference of egomotion given two images.""""""
    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
      self._image1 = tf.placeholder(
          tf.float32, [self.batch_size, self.img_height, self.img_width, 3],
          name='image1')
      self._image2 = tf.placeholder(
          tf.float32, [self.batch_size, self.img_height, self.img_width, 3],
          name='image2')
      # The ""compute_loss"" scope is needed for the checkpoint to load properly.
      with tf.name_scope('compute_loss'):
        rot, trans, _, _ = motion_prediction_net.motion_field_net(
            images=tf.concat([self._image1, self._image2], axis=-1))
        """"""
        inv_rot, inv_trans, _, _ = (
            motion_prediction_net.motion_field_net(
                images=tf.concat([self._image2, self._image1], axis=-1)))
        """"""
        return
```
restoring from the kitti checkpoint and then re-saving yields only a set of variables under the name scope
('MotionFieldNet/compute_loss/MotionFieldNet/...

without the scopes having the _1, _2 & _3 suffixes, Indicating that these variables are being copied instead of reused. It is my understanding that all of the motion_field_net variables should be reused for the forward and inverse ego-motion prediction.
"
"I train the sst and mrpc teacher model and evaluate 'classification'. But the result are 80.96/68.38 respectly, which is much worse than the result in paper(93.2/88.0). 

I use the code with default configure, default trials=1, Is there any wrong for my training? or just try a few more times?"
What does the ***earning only*** mean in the Figure 4 of the AutoML-Zero paper?
"I failed to build.
My environment is Docker container with followings(Ubuntu 16.04, G++ 6.5.0, Bazel 2.2.0).
Is there some way to resolve this error?

```
user@8a4b5a3511d4:/app/sejong/AutoML/google-research/automl_zero$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 16.04.6 LTS
Release:        16.04
Codename:       xenial
user@8a4b5a3511d4:/app/sejong/AutoML/google-research/automl_zero$ g++ --version
g++ (Ubuntu 6.5.0-2ubuntu1~16.04) 6.5.0 20181026
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

user@8a4b5a3511d4:/app/sejong/AutoML/google-research/automl_zero$ bazel --version
bazel 2.2.0
```

This is my log for `run_demo.h`

```
user@8a4b5a3511d4:/app/sejong/AutoML/google-research/automl_zero$ ./run_demo.sh 
DEBUG: Rule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""07d957496cc77efa14c995e491ca78a7819c6c75fa285fa39616fd25136a1b3c""
DEBUG: Call stack for the definition of repository 'rules_cc' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/external/bazel_t
ools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:26:1
DEBUG: Rule 'com_google_absl' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""1dcf8d59a1baeb683bc98339de375c69a581f6d7ab00bb47f13144dbe3c9f190""
DEBUG: Call stack for the definition of repository 'com_google_absl' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/external/
bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:14:1
DEBUG: Rule 'com_google_glog' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""43550f5e8140f819a889a3cd32fcf13888faaa6a441c179cd1163d96bdb82eb3""
DEBUG: Call stack for the definition of repository 'com_google_glog' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/external/
bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:78:1
DEBUG: Rule 'com_github_gflags_gflags' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""5a95d2354322ffd4b7b25b634ee1b23ce8167faa32a9c8fa06df0ef42e
c9a174""
DEBUG: Call stack for the definition of repository 'com_github_gflags_gflags' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/
external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:3:1
DEBUG: Rule 'com_google_googletest' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""51048d8977c45dc5eff96355f186ae0ce9c281cc50f9dbfaff2f4d3bdc0ee
d6c""
DEBUG: Call stack for the definition of repository 'com_google_googletest' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/ext
ernal/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:20:1
INFO: Analyzed target //:run_search_experiment (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /app/sejong/AutoML/google-research/automl_zero/BUILD:286:1: C++ compilation of rule '//:fec_hashing' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE 
-fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 72 argument(s) sk
ipped)

Use --sandbox_debug to see verbose messages from the sandbox
In file included from executor.h:30:0,
                 from fec_hashing.h:21,
                 from fec_hashing.cc:15:
task.h: In function 'bool automl_zero::ItemEquals(const RankT&, const RankT&) [with RankT = double]':
task.h:90:27: error: call of overloaded 'abs(automl_zero::Scalar)' is ambiguous
   return abs(data1 - data2) < kDataTolerance;
                           ^
In file included from /usr/include/c++/6/cstdlib:75:0,
                 from /usr/include/c++/6/ext/string_conversions.h:41,
                 from /usr/include/c++/6/bits/basic_string.h:5429,
                 from /usr/include/c++/6/string:52,
                 from /usr/include/c++/6/bits/locale_classes.h:40,
                 from /usr/include/c++/6/bits/ios_base.h:41,
                 from /usr/include/c++/6/ios:42,
                 from /usr/include/c++/6/istream:38,
                 from /usr/include/c++/6/sstream:38,
                 from definitions.h:30,
                 from fec_hashing.h:20,
                 from fec_hashing.cc:15:
/usr/include/stdlib.h:774:12: note: candidate: int abs(int)
 extern int abs (int __x) __THROW __attribute__ ((__const__)) __wur;
            ^~~
In file included from /usr/include/c++/6/ext/string_conversions.h:41:0,
                 from /usr/include/c++/6/bits/basic_string.h:5429,
                 from /usr/include/c++/6/string:52,
                 from /usr/include/c++/6/bits/locale_classes.h:40,
                 from /usr/include/c++/6/bits/ios_base.h:41,
                 from /usr/include/c++/6/ios:42,
                 from /usr/include/c++/6/istream:38,
                 from /usr/include/c++/6/sstream:38,
                 from definitions.h:30,
                 from fec_hashing.h:20,
                 from fec_hashing.cc:15:
/usr/include/c++/6/cstdlib:180:3: note: candidate: long long int std::abs(long long int)
   abs(long long __x) { return __builtin_llabs (__x); }
   ^~~
/usr/include/c++/6/cstdlib:172:3: note: candidate: long int std::abs(long int)
   abs(long __i) { return __builtin_labs(__i); }
   ^~~
In file included from fec_hashing.h:21:0,
                 from fec_hashing.cc:15:
executor.h: At global scope:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
Target //:run_search_experiment failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.826s, Critical Path: 1.49s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
user@8a4b5a3511d4:/app/sejong/AutoML/google-research/automl_zero$ ./run_demo.sh --sandbox_debug
DEBUG: Rule 'rules_cc' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""07d957496cc77efa14c995e491ca78a7819c6c75fa285fa39616fd25136a1b3c""
DEBUG: Call stack for the definition of repository 'rules_cc' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/external/bazel_t
ools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:26:1
DEBUG: Rule 'com_google_absl' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""1dcf8d59a1baeb683bc98339de375c69a581f6d7ab00bb47f13144dbe3c9f190""
DEBUG: Call stack for the definition of repository 'com_google_absl' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/external/
bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:14:1
DEBUG: Rule 'com_google_glog' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""43550f5e8140f819a889a3cd32fcf13888faaa6a441c179cd1163d96bdb82eb3""
DEBUG: Call stack for the definition of repository 'com_google_glog' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/external/
bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:78:1
DEBUG: Rule 'com_github_gflags_gflags' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""5a95d2354322ffd4b7b25b634ee1b23ce8167faa32a9c8fa06df0ef42e
c9a174""
DEBUG: Call stack for the definition of repository 'com_github_gflags_gflags' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/
external/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:3:1
DEBUG: Rule 'com_google_googletest' indicated that a canonical reproducible form can be obtained by modifying arguments sha256 = ""51048d8977c45dc5eff96355f186ae0ce9c281cc50f9dbfaff2f4d3bdc0ee
d6c""
DEBUG: Call stack for the definition of repository 'com_google_googletest' which is a http_archive (rule definition at /home/user/.cache/bazel/_bazel_user/c7435eb598c0e52a3e7d09039ce632a1/ext
ernal/bazel_tools/tools/build_defs/repo/http.bzl:296:16):
 - <builtin>
 - /app/sejong/AutoML/google-research/automl_zero/WORKSPACE:20:1
INFO: Analyzed target //:run_search_experiment (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /app/sejong/AutoML/google-research/automl_zero/BUILD:286:1: C++ compilation of rule '//:fec_hashing' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE 
-fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 72 argument(s) sk
ipped)

Use --sandbox_debug to see verbose messages from the sandbox
In file included from executor.h:30:0,
                 from fec_hashing.h:21,
                 from fec_hashing.cc:15:
task.h: In function 'bool automl_zero::ItemEquals(const RankT&, const RankT&) [with RankT = double]':
task.h:90:27: error: call of overloaded 'abs(automl_zero::Scalar)' is ambiguous
   return abs(data1 - data2) < kDataTolerance;
                           ^
In file included from /usr/include/c++/6/cstdlib:75:0,
                 from /usr/include/c++/6/ext/string_conversions.h:41,
                 from /usr/include/c++/6/bits/basic_string.h:5429,
                 from /usr/include/c++/6/string:52,
                 from /usr/include/c++/6/bits/locale_classes.h:40,
                 from /usr/include/c++/6/bits/ios_base.h:41,
                 from /usr/include/c++/6/ios:42,
                 from /usr/include/c++/6/istream:38,
                 from /usr/include/c++/6/sstream:38,
                 from definitions.h:30,
                 from fec_hashing.h:20,
                 from fec_hashing.cc:15:
/usr/include/stdlib.h:774:12: note: candidate: int abs(int)
 extern int abs (int __x) __THROW __attribute__ ((__const__)) __wur;
            ^~~
In file included from /usr/include/c++/6/ext/string_conversions.h:41:0,
                 from /usr/include/c++/6/bits/basic_string.h:5429,
                 from /usr/include/c++/6/string:52,
                 from /usr/include/c++/6/bits/locale_classes.h:40,
                 from /usr/include/c++/6/bits/ios_base.h:41,
                 from /usr/include/c++/6/ios:42,
                 from /usr/include/c++/6/istream:38,
                 from /usr/include/c++/6/sstream:38,
                 from definitions.h:30,
                 from fec_hashing.h:20,
                 from fec_hashing.cc:15:
/usr/include/c++/6/cstdlib:180:3: note: candidate: long long int std::abs(long long int)
   abs(long long __x) { return __builtin_llabs (__x); }
   ^~~
/usr/include/c++/6/cstdlib:172:3: note: candidate: long int std::abs(long int)
   abs(long __i) { return __builtin_labs(__i); }
   ^~~
In file included from fec_hashing.h:21:0,
                 from fec_hashing.cc:15:
executor.h: At global scope:
executor.h:803:10: warning: variable templates only available with -std=c++14 or -std=gnu++14
     128> kOpIndexToExecuteFunction = {
          ^~~~~~~~~~~~~~~~~~~~~~~~~
Target //:run_search_experiment failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1.812s, Critical Path: 1.55s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```"
@peterjliu @sisidaisy2018 
"Hi, thank you for the great paper and releasing the code for reproducibility.

According to [this comment](https://github.com/google-research/google-research/issues/46#issuecomment-522846228) from an other issue you planned to release the pretrained checkpoints for models trained on YouTube8M. 

Is that still a possibility?  
(I note that we can re-train a model as the YouTube8M IDs used have been published, but the dataset is quite large so a checkpoint would be very helpful for comparing the performances without re-training)"
"`if precalc:`
      `train_feats = train_adj.dot(feats)`
      `train_feats = np.hstack((train_feats, feats))`"
"We're trying to reproduce the results from a TabNet paper. There is a remarkable 99% accuracy on Poker Hand dataset. We can't reproduce the result, we've got only 54%. Can you share the code, so we can check what's wrong with our setup?
"
"I would like to view these images as interleaved and as one, not as left and right views.. This is for dual_pixels"
">     train = df.loc[index < valid_boundary]
>     valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]

May i know aren't -7 consider leak data from train into valid set ?

https://github.com/google-research/google-research/blob/1fec1abdc5a54d6f2ac8b5ef3420b36d4e6509f0/tft/data_formatters/traffic.py#L72"
"when i try to run the training on gpu , i get below error

> 2020-01-05 17:25:08.166159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
> 2020-01-05 17:25:08.384437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
> 2020-01-05 17:25:09.025964: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
> 2020-01-05 17:25:09.026043: F tensorflow/core/kernels/cudnn_rnn_ops.cc:1643] Check failed: stream->parent()->GetRnnAlgorithms(&algorithms) 
> Aborted (core dumped)


i'm using tensorflow-gpu ver 1.5.0, cudnn 7.6.5.32, cuda 10
"
"I have evaluated the checkpoint of the paper [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345) with the pyrouge and the rouge-score packages. I observe rather large differences, especially for Rouge-L:

With pyrouge:
```
rouge_1_f_score      0.42164
rouge_2_f_score      0.19486
rouge_l_f_score      0.39156
```

With rouge-score:
```
rouge_1_f_score      0.43707
rouge_2_f_score      0.20137
rouge_l_f_score      0.30160
```

My procedure: The `validate` mode of the authors' code ([Github](https://github.com/nlpyang/PreSumm)) extracts candidate and reference summaries in text format. I then pass these to the authors' `test_rouge` function ([link](https://github.com/nlpyang/PreSumm/blob/ce8dc017fbef7c12b1b4bd764f0c3d20911ead5e/src/others/utils.py#L54)) (which to my understanding does no further processing except for preparing the file structure for pyrouge). For the rouge-score package I used the following code:
```python
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
aggregator = BootstrapAggregator()
candidates = [line.strip() for line in open(candidates_path, encoding='utf-8')]
references = [line.strip() for line in open(references_path, encoding='utf-8')]
assert len(candidates) == len(references)
for i, (c, r) in enumerate(zip(candidates, references)):
  aggregator.add_scores(scorer.score(r, c))
results = aggregator.aggregate()
```

Things I've checked:
- There are no empty lines in candidate or reference summaries (after `.strip()`)
- The other flags passed to pyrouge/Rouge-1.5.5 are equal 
- This behavior is consistent with different random seeds

"
"Hi all,

I run one test data file with do_eval and do_predict(then do evaluation) respectively, but the results of evaluation (accuracy) are not same, could you guys give me some hints? "
"Hi all,

I am new to NLP. I was fine tuning the albert and i got error below, could you guys give me some hints? Thanks!

`Traceback (most recent call last):
  File ""run_classifier.py"", line 463, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""run_classifier.py"", line 182, in main
    do_lower_case=FLAGS.do_lower_case)
TypeError: __init__() got an unexpected keyword argument 'use_spm'
`

Here is my .sh script:
`python run_classifier.py \
  --task_name=mytask \
  --do_train=true \
  --do_eval=true \
  --data_dir=$DATA_DIR/ \
  --vocab_file=$BERT_BASE_DIR/assets/30k-clean.vocab \
  --albert_config_file=$BERT_BASE_DIR/assets/albert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/variables/variables \
  --spm_model_file=$BERT_BASE_DIR/assets/30k-clean.model \
  --max_seq_length=320 \
  --train_batch_size=20 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0  --output_dir=./trained_albert_base_maxlen320_batch20_epoch3/ \
  --do_lower_case=false`"
"After 70,000 epoch of training, by obtaining the aligned frame numbers, and extracting the mapping vector of each frame according to the index， and then visualizing with T-SNE, it is found that the T-sne visualization in Figure 8 of your paper is not available. The alignment vectors I get based on the reference video cannot overlap.  Do you have any visual skills for setting up?  Another question, I would like to ask for the annotation of Penn-action, get the json file of your thesis, what kind of annotation tool can be used for you, and how to get the json file, I hope to get your help.I hope you can reply and urgently need your help.....@debidatta "
"I'm finetuning albert xlarge model with my joint training task of intent classification and slot filling,  and I tried many times, the model never converge. 

Then I repalced the albert model with bert base, the code is working.

What's wrong? SOS

"
" I trained both Bert and Albert using same data(classification task).
 After training
       model size of **Bert** frozen graph() - 420MB
      model size of **Allbert** frozen graph() - 48MB.

When I perform inference, both models are taking same amount of time.
        Time take by **Bert** to classify for singe sentence  - 0.64sec (CPU) -- .0.028sec(K-80 GPU).
        Time take by **AllBert** to classify for singe sentence  - 0.627816sec (CPU) -- .0.023sec(K-80GPU).

I have following questions:
1) Using Albert does not result in good inference time behavior. Only model size is reduced.
Is it expected behavior?

2) If not by how much factor **Albert** is faster than **Bert**(if possible please also provide H/W details.
3) is it not recommend to run the inference on CPU platform? 


Thanks






"
"I'm trying to train with ~ 568 tfrecord files totaling upto 1.9 TB, created from ~30GB of text and generated using 128 max_seq_length and custom vocab comparable to the original vocab size.

Here are my training flasgs:
nohup python3 run_pretraining.py \
  --input_file=gs://databucket-* \
  --output_dir=gs://output bucket \
  --export_dir==gs://export bucket \
  --do_train=True \
  --do_eval=True \
  --albert_config_file=gs://<config bucket>\albert_config.json \
  --train_batch_size=512 \
  --eval_batch_size=64 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=2000000 \
  --num_warmup_steps=10000 \
  --learning_rate=1e-4 \
  --use_tpu=True \
  --tpu_name=gorilla-alb-tpu \
  --save_checkpoints_steps=10000 \
  --max_eval_steps=100 \
  --num_tpu_cores=8 \
  --user_tpu=True \
  --keep_checkpoint_max=500 \
  \> iter001.log &

I tried this on the preemptible tpu first and did not even get one checkpoint, but later on the second iteration, I used a dedicated TPU, but after the global step 19000 its started idling and did not produce any checkpoints, as can be seen in the logs below. 
Q1: Any idea what am I missing here? 
Q2: Also, why is the train time so slow compared to BERT, which stepped through 10K/20 mins, here it seems 10K/1.15 hr or more?
Q3: What is the recommended batch size for single TPU instance and how does it impact the train steps and the learning rate?


Extract of the logs showing idling here : 
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:04:59.069434 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:loss = 6.453083, step = 19000 (1816.160 sec)
I1124 07:05:04.021676 139980433229568 basic_session_run_hooks.py:260] loss = 6.453083, step = 19000 (1816.160 sec)
INFO:tensorflow:global_step/sec: 0.550612
I1124 07:05:04.024546 139980433229568 tpu_estimator.py:2307] global_step/sec: 0.550612
INFO:tensorflow:examples/sec: 281.913
I1124 07:05:04.024918 139980433229568 tpu_estimator.py:2308] examples/sec: 281.913
INFO:tensorflow:Enqueue next (1000) batch(es) of data to infeed.
I1124 07:05:04.025995 139980433229568 tpu_estimator.py:600] Enqueue next (1000) batch(es) of data to infeed.
INFO:tensorflow:Dequeue next (1000) batch(es) of data from outfeed.
I1124 07:05:04.026226 139980433229568 tpu_estimator.py:604] Dequeue next (1000) batch(es) of data from outfeed.
INFO:tensorflow:Outfeed finished for iteration (19, 12)
I1124 07:05:28.020126 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 12)
I1124 07:05:29.137331 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:05:29.192949 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:05:59.262590 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:05:59.311209 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:06:29.373492 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:06:29.447152 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 46)
I1124 07:06:29.709674 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 46)
I1124 07:06:59.522179 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:06:59.572463 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:07:29.611606 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:07:29.668322 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 80)
I1124 07:07:31.398825 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 80)
I1124 07:07:59.732999 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:07:59.793257 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:08:29.856629 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:08:29.919902 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 114)
I1124 07:08:33.089343 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 114)
I1124 07:08:59.989946 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:09:00.072418 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:09:30.138386 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:09:30.192759 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 148)
I1124 07:09:34.782029 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 148)
I1124 07:10:00.262878 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:10:00.322918 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:10:30.385881 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:10:30.438601 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 182)
I1124 07:10:36.469368 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 182)
I1124 07:11:00.488457 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:11:00.554763 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:11:30.607665 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:11:30.657622 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 216)
I1124 07:11:38.161521 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 216)
I1124 07:12:00.723495 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:12:00.779349 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:12:30.844133 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:12:30.897756 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 250)
I1124 07:12:39.851271 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 250)
I1124 07:13:00.941801 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:13:00.998180 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:13:31.063151 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:13:31.136875 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 284)
I1124 07:13:41.539725 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 284)
I1124 07:14:01.204144 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:14:01.270991 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:14:31.314057 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:14:31.368036 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 318)
I1124 07:14:43.230399 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 318)
I1124 07:15:01.408405 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:15:01.466477 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:15:31.531250 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:15:31.588218 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 352)
I1124 07:15:44.920365 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 352)
I1124 07:16:01.650633 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:16:01.709294 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:16:31.747833 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:16:31.803840 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 386)
I1124 07:16:46.611431 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 386)
I1124 07:17:01.843595 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:17:01.905045 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:17:31.969744 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:17:32.024375 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
INFO:tensorflow:Outfeed finished for iteration (19, 420)
I1124 07:17:48.304604 139978655139584 tpu_estimator.py:279] Outfeed finished for iteration (19, 420)
I1124 07:18:02.091745 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:18:02.247168 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:18:32.311644 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:18:32.362884 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:19:02.426864 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:19:02.486614 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:19:32.542831 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:19:32.602440 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
I1124 07:20:02.665383 139978646746880 transport.py:157] Attempting refresh to obtain initial access_token
WARNING:tensorflow:TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.
W1124 07:20:02.727149 139978646746880 preempted_hook.py:91] TPUPollingThread found TPU b'gorilla-alb-tpu' in state READY, and health HEALTHY.

[iter001.log](https://github.com/google-research/google-research/files/3883630/iter001.log)

"
"I got this message when do predicting :
> Traceback (most recent call last):
>   File ""google-research/albert/run_squad_sp.py"", line 1333, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
>   File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
>     _run_main(main, args)
>   File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
>     sys.exit(main(argv))
>   File ""google-research/albert/run_squad_sp.py"", line 1326, in main
>     output_nbest_file, output_null_log_odds_file)
>   File ""google-research/albert/run_squad_sp.py"", line 929, in write_predictions
>     doc_offset = feature.tokens.index(""[SEP]"") + 1
> ValueError: '[SEP]' is not in list"
"When inference. I got this error:

> WARNING:tensorflow:From /content/google-research/albert/lamb_optimizer.py:34: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:1333: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:1165: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.
> 
> W1117 16:29:12.304847 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:1165: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:1165: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.
> 
> W1117 16:29:12.305077 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:1165: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.
> 
> WARNING:tensorflow:From /content/google-research/albert/modeling.py:116: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.
> 
> W1117 16:29:12.305286 140072459204480 module_wrapper.py:139] From /content/google-research/albert/modeling.py:116: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:1171: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.
> 
> W1117 16:29:12.306269 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:1171: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.
> 
> WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f64ecc90488>) includes params argument, but params are not passed to Estimator.
> W1117 16:29:14.673645 140072459204480 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f64ecc90488>) includes params argument, but params are not passed to Estimator.
> INFO:tensorflow:Using config: {'_model_dir': 'gs://bilacac/albert/squad/no_shuffled_zalo_train_with_answers100000_raw.json_32bsz_3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
> cluster_def {
>   job {
>     name: ""worker""
>     tasks {
>       key: 0
>       value: ""10.34.45.178:8470""
>     }
>   }
> }
> isolate_session_state: true
> , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f64ebb89d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.34.45.178:8470', '_evaluation_master': 'grpc://10.34.45.178:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f64ebb89390>}
> I1117 16:29:14.674898 140072459204480 estimator.py:212] Using config: {'_model_dir': 'gs://bilacac/albert/squad/no_shuffled_zalo_train_with_answers100000_raw.json_32bsz_3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
> cluster_def {
>   job {
>     name: ""worker""
>     tasks {
>       key: 0
>       value: ""10.34.45.178:8470""
>     }
>   }
> }
> isolate_session_state: true
> , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f64ebb89d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.34.45.178:8470', '_evaluation_master': 'grpc://10.34.45.178:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f64ebb89390>}
> INFO:tensorflow:_TPUContext: eval_on_tpu True
> I1117 16:29:14.675387 140072459204480 tpu_context.py:220] _TPUContext: eval_on_tpu True
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:264: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.
> 
> W1117 16:29:14.676191 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:264: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:1262: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.
> 
> W1117 16:29:14.706956 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:1262: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:1103: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.
> 
> W1117 16:29:14.844320 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:1103: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.
> 
> WARNING:tensorflow:From google-research/albert/run_squad_sp.py:355: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.
> 
> W1117 16:29:14.846428 140072459204480 module_wrapper.py:139] From google-research/albert/run_squad_sp.py:355: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.
> 
> INFO:tensorflow:Converting 0/2678 pos 0 neg 0
> I1117 16:29:14.846674 140072459204480 run_squad_sp.py:356] Converting 0/2678 pos 0 neg 0
> Traceback (most recent call last):
>   File ""google-research/albert/run_squad_sp.py"", line 1333, in <module>
>     tf.app.run()
>   File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
>     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
>   File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
>     _run_main(main, args)
>   File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
>     sys.exit(main(argv))
>   File ""google-research/albert/run_squad_sp.py"", line 1284, in main
>     output_fn=append_feature)
>   File ""google-research/albert/run_squad_sp.py"", line 361, in convert_examples_to_features
>     example.question_text, lower=FLAGS.do_lower_case))
>   File ""/content/google-research/albert/tokenization.py"", line 145, in encode_ids
>     pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)
>   File ""/content/google-research/albert/tokenization.py"", line 113, in encode_pieces
>     pieces = sp_model.EncodeAsPieces(text)
> AttributeError: 'NoneType' object has no attribute 'EncodeAsPieces'"
"After 70,000 epoch of training, by obtaining the aligned frame numbers, and extracting the mapping vector of each frame according to the index， and then visualizing with T-SNE, it is found that the T-sne visualization in Figure 8 of your paper is not available. The alignment vectors I get based on the reference video cannot overlap.  Do you have any visual skills for setting up?  Another question, I would like to ask for the annotation of Penn-action, get the json file of your thesis, what kind of annotation tool can be used for you, and how to get the json file, I hope to get your help.@debidatta "
"Hi! Many thanks for the interesting research!
The paper states:
> Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).

I was wondering if this is deemed mandatory for the model to converge or was this simply a time saving mechanism to avoid starting from scratch?

Any insights would be great!
Many thanks,
Dom"
"When I try using the albert v2  as follow command, got errors as below. while v1 works good as expected.

CUDA_VISIBLE_DEVICES=0 python run_classifier_with_tfhub.py \
    --trainnig_data_dir=./data/train/ \
    --validation_data_dir=./data/eval/ \
    --do_lower_case=False \
    --albert_hub_module_handle=https://tfhub.dev/google/albert_xxlarge/2 \
    --spm_model_file=./model/albert_xxlarge/30k-clean.model \
    --task_name=MRPC \
    --max_seq_length=128 \
    --do_train \
    --do_eval \
    --train_batch_size=16 \
    --eval_batch_size=4 \
    --learning_rate=2e-5 \
    --num_train_epochs=3.0 \
    --output_dir=./xxlarge


Traceback (most recent call last):
  File ""run_classifier_with_tfhub.py"", line 366, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""run_classifier_with_tfhub.py"", line 269, in main
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3035, in train
    rendezvous.raise_errors()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 136, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 696, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3030, in train
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 370, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1161, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1191, in _train_model_default
    features, labels, ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 2857, in _call_model_fn
    config)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1149, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3126, in _model_fn
    features, labels, is_export_mode=is_export_mode)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1663, in call_without_tpu
    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 1994, in _call_model_fn
    estimator_spec = self._model_fn(features=features, **kwargs)
  File ""run_classifier_with_tfhub.py"", line 123, in model_fn
    total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)
  File ""/home/qiangliu/Git/Mine/albert_online/optimization.py"", line 103, in create_optimizer
    grads = tf.gradients(loss, tvars)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gradients_util.py"", line 637, in _GradientsHelper
    (op.name, op.type))
LookupError: **No gradient defined for operation 'module_apply_tokens/bert/encoder/transformer/group_0_11/layer_11/inner_group_0/ffn_1/intermediate/output/dense/einsum/Einsum**' (op type: Einsum)

Did I missed something there?"
"this seems to be the V x E matrix
```python
embedding_table = tf.get_variable(
      name=word_embedding_name,
      shape=[vocab_size, embedding_size],
      initializer=create_initializer(initializer_range))
```
then where is the E x H, this piece of code seems to do the job?
```python
if input_width != hidden_size:
    prev_output = dense_layer_2d(
        input_tensor, hidden_size, create_initializer(initializer_range),
        None, name=""embedding_hidden_mapping_in"")
```
"
"I want to use proxy when I fine-tune model with TF-Hub because China’s network can’t fetch Tensorflow Hub
```
pip install -r albert/requirements.txt
python -m albert.run_classifier_with_tfhub \
  --albert_hub_module_handle=https://tfhub.dev/google/albert_base/1 \
  <additional flags>
```
Is there a proxy additional flag for run_classifier_with_tfhub.py (or just set up environment variable like `http_proxy`)? "
"Hi,
Thank you for sharing the ALBERT code :)
Would it be possible for me to run pretraining from scratch on multi gpu?
Let me know if you have any plans to implement or the ways to do that.
"
"I1031 20:37:34.190690 140505617327872 train.py:123] Iter[27/150000], 27.4s/iter, Loss: 0.708
I1031 20:38:02.018666 140505617327872 train.py:123] Iter[28/150000], 27.8s/iter, Loss: 1.107
I1031 20:38:29.308702 140505617327872 train.py:123] Iter[29/150000], 27.3s/iter, Loss: 0.724
I1031 20:38:56.431218 140505617327872 train.py:123] Iter[30/150000], 27.1s/iter, Loss: 0.744

Process finished with exit code -1
Why did I stop early when I was training? I didn't terminate the program, it ended automatically, and I couldn't continue training. excuse me, can you tell me the reason"
"Hello,

Thank you for the useful tools. Here is a error when I run run_classifier_with_tfhub.py:

`Error while finding module specification for 'run_classifier_with_tfhub.py' (AttributeError: module 'run_classifier_with_tfhub' has no attribute '__path__')`

Could you guys give me some hints? Thanks!"
"@abdolence @ahotrod  Seems that some of the lines in the vocab file are just '\n', hence ```token.strip().split()``` results in an empty list. This error is solved by ignoring such empty lists.

However, around the 15th word in the vocab file, there's a unicode error ``UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 8: invalid start byte`` in the line ``token = convert_to_unicode(reader.readline())``. This is solved by changing ""r"" to ""rb"" in the tensorflow reader.

What should be given for the spm model file? The given files in the tar.gz result in a ```RuntimeError: Internal: unk is not defined.``` error.

_Originally posted by @SahanaRamnath in https://github.com/google-research/google-research/issues/72#issuecomment-546587491_"
"Hi all,

I've tried to test the ALBERT model on TF Hub. I got the following error when just trying to load ALBERT from TF Hub:

```
python
Python 3.6.8 (default, May 16 2019, 05:58:38)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-36.0.1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow_hub as hub
>>> albert_module = hub.Module(
...     ""https://tfhub.dev/google/albert_base/1"",
...     trainable=True)
Traceback (most recent call last):
  File ""<stdin>"", line 3, in <module>
  File ""/home/opc/vuh/tools/pyvenv3-gpu-tf-hub/lib64/python3.6/site-packages/tensorflow_hub/module.py"", line 170, in __init__
    tags=self._tags)
  File ""/home/opc/vuh/tools/pyvenv3-gpu-tf-hub/lib64/python3.6/site-packages/tensorflow_hub/native_module.py"", line 340, in _create_impl
    name=name)
  File ""/home/opc/vuh/tools/pyvenv3-gpu-tf-hub/lib64/python3.6/site-packages/tensorflow_hub/native_module.py"", line 382, in __init__
    op.name for op in self._meta_graph.meta_info_def.stripped_op_list.op})
  File ""/home/opc/vuh/tools/pyvenv3-gpu-tf-hub/lib64/python3.6/site-packages/tensorflow_hub/native_module.py"", line 822, in register_ops_if_needed
    % missing_ops.difference(set(cpp_registry_ops.keys())))
tensorflow.python.framework.errors_impl.NotFoundError: Graph ops missing from the python registry ({'BatchMatMulV2'}) are also absent from the c++ registry.
```

Do anyone know why? Thanks so much in advance?

FYI, in my python3 venv, I am using:

tensorflow-datasets      1.1.0
tensorflow-estimator     1.13.0
tensorflow-gpu           1.13.1
tensorflow-hub           0.6.0
tensorflow-metadata      0.14.0
tensorflow-probability   0.7.0
tf-sentencepiece         0.1.83
sentencepiece            0.1.82
"
"By the note of this line of [code](https://github.com/google-research/google-research/blob/master/albert/create_pretraining_data.py#L469), `the probilities are set to favor longer ngram sequences `, 

but I think after the code of line 469
```python
  ngrams = np.arange(1, FLAGS.ngram + 1, dtype=np.int64)
  pvals = 1. / np.arange(1, FLAGS.ngram + 1)
  pvals /= pvals.sum(keepdims=True)
```

pvals should be ```pvals = array([0.4379562 , 0.2189781 , 0.1459854 , 0.10948905, 0.08759124])``` given ```FLAGS.ngram==5```

and after [line 500](https://github.com/google-research/google-research/blob/master/albert/create_pretraining_data.py#L500),
```
    n = np.random.choice(ngrams[:len(cand_index_set)],
                         p=pvals[:len(cand_index_set)] /
                         pvals[:len(cand_index_set)].sum(keepdims=True))
```

 the code will random choice a n for gram by the pval.
So according to the pval, it will favor shorter ngram sequences, rather than long sequences.

It confused me, so I want to ask is there something I got wrong?"
Can you show me how to do this?
"Dear author, 
Thanks for sharing the source code of the paper.
When training with the pouring dataset, the program stops automatically when training for 17 or 30 epoch, not training, and the program does not report an error. excuse me, can you tell me the reason.
Another one is that you can provide a public json file of the Penn-Action dataset to verify the excellent results of the paper. Thank you very much. bless you@debidatta"
"In the child.py file, why w_skip[0] is not used for L2 weight reg ?
var_s = [w_prev] + w_skip[1:]

https://github.com/google-research/google-research/blob/43058538eeee450e6cdebeb960a78832b1673db3/enas_lm/src/child.py#L69

Does w_skip[0] means the weight of node 1 and node 2 (as the red arrow shows )?

![enas](https://user-images.githubusercontent.com/52318628/67278836-5ebea880-f4fc-11e9-8601-e959b3b123c3.jpg)



"
"Hello, the Penn Action dataset referred to in your paper does not have the key events and phase labels you mentioned after downloading. Can you disclose your work in order to reproduce your work?@debidatta
                                                                                                                                        Thank you"
"Hi @gariel-google, are the models that you provide trained on images 416x128? When I tried inference with other resolutions it doesn't work well at all.

If it's indeed 416x128, have you tried training with higher resolutions? I know some previous work use 416x128 for training, but recently most methods use higher resolutions and experiments have demonstrated higher resolutions lead to better results. Is it something related to the GPU memory issue?"
"Quick question.
What is the unit of the estimated ego-motion? Is it mm, cm, m?
If it is not real world units, how can it be converted?

Thanks a lot
Edit: I am talking about the project depth_from_video_in_the_wild"
"Hi, I get different embeddings when I run `extracting_embeddings.py` with the same input and setups except `--frames_per_batch`. For example:

--frames_per_batch == 1:

```
<tf.Tensor: id=10110, shape=(128,), dtype=float32, numpy=
array([ 0.7134198 ,  0.73715377, -0.40302375, -0.8916309 ,  0.46344516,
        1.0306991 , -0.616913  , -0.3954195 , -1.4110051 ,  0.05280016,
        0.02743702, -0.47917336, -0.7063019 ,  0.00664697, -0.16832595,
       -0.38978115,  0.28905347, -0.71937335,  0.39218065, -0.2223233 ,
        0.24361739, -0.08869804, -0.9321748 , -0.8480654 ,  0.45671615,
       -0.90358734, -1.3239552 , -0.18341677,  0.22246726, -0.84119105,
        0.41529498,  0.2421515 , -0.12988764,  1.2223002 , -1.2660636 ,
        0.31256717,  1.018894  ,  0.6738411 ,  0.18867303, -0.17254871,
       -1.4501228 ,  1.0448513 , -1.07593   , -0.9447051 , -0.38788766,
        0.20399381, -0.46668968, -0.00173404,  0.36895347,  0.49572152,
        0.11630958, -0.4594518 ,  0.11987424,  1.1069762 , -0.4460541 ,
       -0.5169652 ,  0.3923389 , -0.2448386 ,  0.9658608 ,  0.23109348,
        0.16036353, -0.82762504, -0.20896168, -0.5168912 ,  0.07127902,
        0.25286838,  0.02297238, -1.3294024 ,  0.39561197,  0.5771644 ,
        1.5672271 ,  0.6967077 , -0.5723567 ,  0.3235898 ,  0.7618949 ,
        0.91371095, -0.26501146,  0.10041602,  0.3094164 , -0.27465546,
       -0.083619  , -0.49142212, -0.49883616,  0.08733553, -0.02642126,
       -0.46290073,  0.58624184,  0.8576831 ,  0.23795792,  0.26929522,
        0.40708646,  0.96988857, -1.0064505 ,  0.8797252 ,  0.6761626 ,
        0.970005  , -0.05762405, -0.51743686,  1.468217  , -1.2853948 ,
        0.7068154 ,  0.18635778, -0.16756667,  0.25616115, -1.3703632 ,
       -0.09829516,  0.38073325, -0.5046847 ,  0.30129814, -1.6381143 ,
       -0.41361213, -0.37367654, -0.00442669,  0.24185468, -0.23331967,
        0.46492097, -0.874537  , -0.11011802, -0.29310864, -0.6257624 ,
       -0.13382947, -1.569699  , -0.60893744, -0.9276579 , -0.27602094,
       -0.10803759, -0.69294   ,  0.72386044], dtype=float32)>
```

--frames_per_batch == 10, and first embedding I get is:

```
(Pdb) p emb_feats[0]
<tf.Tensor: id=10111, shape=(128,), dtype=float32, numpy=
array([ 0.69585   ,  0.7938789 , -0.40682542, -0.9104449 ,  0.44788656,
        1.0696075 , -0.6545431 , -0.39065826, -1.4388212 ,  0.07879334,
        0.07056048, -0.4912063 , -0.6906045 ,  0.05483497, -0.1918716 ,
       -0.4480733 ,  0.2745942 , -0.69838053,  0.33875132, -0.12655792,
        0.20775412, -0.07518089, -0.9712121 , -0.8582906 ,  0.4940186 ,
       -0.9054945 , -1.3323177 , -0.1797872 ,  0.21466169, -0.842904  ,
        0.4238041 ,  0.18746372, -0.16505198,  1.2667109 , -1.3068607 ,
        0.34267744,  1.0190938 ,  0.6861747 ,  0.10797802, -0.1484639 ,
       -1.4422283 ,  1.0217737 , -1.0802454 , -1.0336714 , -0.3141645 ,
        0.20748997, -0.5034793 , -0.01016946,  0.38425395,  0.4835653 ,
        0.17350678, -0.48503914,  0.09994452,  1.171491  , -0.44424957,
       -0.5473531 ,  0.31822965, -0.22695364,  0.9586529 ,  0.23552223,
        0.19058312, -0.87618846, -0.2797962 , -0.4805327 ,  0.06645918,
        0.21550886, -0.0261419 , -1.3357202 ,  0.36320177,  0.5580914 ,
        1.5055286 ,  0.6597656 , -0.587545  ,  0.3728123 ,  0.78129125,
        0.9374202 , -0.26444304,  0.07730328,  0.28997815, -0.29767683,
       -0.05417863, -0.5795138 , -0.52169895,  0.17690559, -0.04242799,
       -0.5164021 ,  0.563585  ,  0.86161417,  0.2007392 ,  0.1909954 ,
        0.46636117,  1.003835  , -0.98218834,  0.8831477 ,  0.6481319 ,
        0.9996616 , -0.04697989, -0.49288058,  1.4274698 , -1.319891  ,
        0.6997739 ,  0.17412183, -0.15896532,  0.23632345, -1.3237288 ,
       -0.10929373,  0.36972514, -0.5411339 ,  0.30892903, -1.6210634 ,
       -0.3404241 , -0.35616207,  0.01820717,  0.22067611, -0.23308174,
        0.42857918, -0.8491688 , -0.0896553 , -0.35461238, -0.635367  ,
       -0.16244371, -1.6196773 , -0.5958075 , -0.9506139 , -0.3112893 ,
       -0.12266631, -0.6807967 ,  0.72940886], dtype=float32)>
```

I thought the embeddings should be independent with `--frames_per_batch` param and should be able to get consistent results, is there something I'm missing?"
"My understand is it seems like we don't need per frame label to train the TCC model, right? But when I feed the tfrecord without per frame label, it throws an error:

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: {{function_node __inference_<lambda>_61009}} Name: , Feature list 'frame_labels' is required but could not be found.  Did you \
mean to include it in feature_list_dense_missing_assumed_empty or feature_list_dense_defaults?
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext]]
         [[conv3_block4_1_bn/beta0/buckets/cond/else/_865/Identity/_2076]]
  (1) Invalid argument: {{function_node __inference_<lambda>_61009}} Name: , Feature list 'frame_labels' is required but could not be found.  Did you \
mean to include it in feature_list_dense_missing_assumed_empty or feature_list_dense_defaults?
         [[{{node ParseSingleSequenceExample/ParseSingleSequenceExample}}]]
         [[MultiDeviceIteratorGetNextFromShard]]
         [[RemoteCall]]
         [[IteratorGetNext]]
0 successful operations.
0 derived errors ignored. [Op:__inference_<lambda>_61009]
```
Seems like the `frame_label` is required here. So can I just fill in dummy labels here?"
"@gariel-google  Dear author, Thanks for sharing the source code of the paper.   
I was trying to reproduce the result of the paper using your code. However, with your default setting (batch size=4, learning_rate=0.0002, etc.) training from scratch, the result I got it's quite far from what you stated in the paper (Abs Rel 0.147 for the best checkpoint within around 370k-th step vs 0.128 in the paper). For your information, I am using the evaluation code from [sfmlearner](https://github.com/tinghuiz/SfMLearner/tree/master/kitti_eval) as what struct2depth does.  
Therefore, may I know what's setting for obtaining the paper's result? Or is there anything critical part missing in the current released code (maybe pretrained checkpoint for example)?   
Thank you in advance."
"We've been doing some symbolic computation/mathematics for PyMC in the [`symbolic-pymc`](https://github.com/pymc-devs/symbolic-pymc) project, and, since we're moving to TensorFlow [Probability] and you folks have also done [related things in TFP & Edward2](https://github.com/google-research/google-research/tree/master/edward2_autoreparam), I would like to get your input on this kind of work in the context of TF[P].

More specifically, is anyone else working on tools for symbolic assessment/manipulation of TF graphs?  We've had to do a bit of work to make TF graphs ""symbolically manipulatable"" and I'm always wondering if there's a better way, or if I'm missing out on any larger, concerted efforts to do so.
"
"I was trying to train a BAM model using the command like `python -m bam.run_classifier rte-mrpc-bam-model $BAM_DIR '{""task_names"": [""rte"", ""mrpc""], ""distill"": true, ""teachers"": {""rte"": ""rte-model"", ""mrpc"": ""mrpc-model""}}'` given in the Readme.md，an error occured like:
```
Traceback (most recent call last):
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.1.2\helpers\pydev\pydevd.py"", line 1758, in <module>
    main()
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.1.2\helpers\pydev\pydevd.py"", line 1752, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.1.2\helpers\pydev\pydevd.py"", line 1147, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2019.1.2\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/yirongli/Desktop/bam/run_classifier.py"", line 281, in <module>
    tf.app.run()
  File ""C:\Python36\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""C:/Users/yirongli/Desktop/bam/run_classifier.py"", line 274, in main
    model_runner.write_outputs([task], trial, split)
  File ""C:/Users/yirongli/Desktop/bam/run_classifier.py"", line 196, in write_outputs
    distill_input_fn, _, _ = self._preprocessor.prepare_predict(tasks, split)
  File ""C:\Users\yirongli\Desktop\bam\data\preprocessing.py"", line 69, in prepare_predict
    return self._serialize_dataset(tasks, False, split)
  File ""C:\Users\yirongli\Desktop\bam\data\preprocessing.py"", line 108, in _serialize_dataset
    self.serialize_examples(examples, is_training, tfrecords_path)
  File ""C:\Users\yirongli\Desktop\bam\data\preprocessing.py"", line 127, in serialize_examples
    tf_example = self._example_to_tf_example(example, is_training)
  File ""C:\Users\yirongli\Desktop\bam\data\preprocessing.py"", line 136, in _example_to_tf_example
    example, is_training))
  File ""C:\Users\yirongli\Desktop\bam\task_specific\classification\classification_tasks.py"", line 136, in featurize
    self._distill_inputs[eid])
KeyError: 2490
```
When I tried to figure out, i found that in `configure.py` line 131, model always load distill_outputs from _train_predictions_1.pkl no matter the student model requiring training or testing prediction by teacher model in `run_classifier.py` line 269. 
So it obviously goes wrong when get the testing prediction, hope to fix it"
"After working-around a problem in `example.py` as described in [previous issue](https://github.com/google-research/google-research/issues/33) I could load NIN models but not resnet models.  The error is below:

```
I0716 13:20:53.269759 140097876026944 saver.py:1280] Restoring parameters from data/demogen_models/RESNET_CIFAR10/resnet_wide_1.0x_batchnorm__decay_0.0_1/model.ckpt-150000
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib64/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""~/google-research/demogen/example.py"", line 62, in <module>
    tf.app.run(main)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""~/.local/lib64/python2.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""~/.local/lib64/python2.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""~/google-research/demogen/example.py"", line 57, in main
    load_and_run(model_config, root_dir)
  File ""~/google-research/demogen/example.py"", line 44, in load_and_run
    sess.run(logits)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnimplementedError: Generic conv implementation only supports NHWC tensor format for now.
         [[node resnet/conv2d/Conv2D (defined at /tmp/tmpdCZJAJ.py:12) ]]

Errors may have originated from an input operation.
Input Source operations connected to node resnet/conv2d/Conv2D:
 transpose (defined at demogen/data_util.py:79)
 resnet/conv2d/kernel/read (defined at demogen/models/resnet.py:136)

Original stack trace for u'resnet/conv2d/Conv2D':
  File ""/usr/lib64/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/usr/lib64/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""~/google-research/demogen/example.py"", line 62, in <module>
    tf.app.run(main)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""~/.local/lib64/python2.7/site-packages/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""~/.local/lib64/python2.7/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""~/google-research/demogen/example.py"", line 57, in main
    load_and_run(model_config, root_dir)
  File ""~/google-research/demogen/example.py"", line 41, in load_and_run
    logits = model_fn(image, is_training=False)
  File ""demogen/models/resnet.py"", line 391, in __call__
    strides=self.conv_stride, data_format=self.data_format)
  File ""demogen/models/resnet.py"", line 136, in conv2d_fixed_padding
    data_format=data_format)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/convolutional.py"", line 424, in conv2d
    return layer.apply(inputs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1479, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/layers/base.py"", line 537, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 634, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 146, in wrapper
    ), args, kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 450, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/tmpdCZJAJ.py"", line 12, in tf__call
    outputs = ag__.converted_call('_convolution_op', self, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (inputs, self.kernel), None)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 356, in converted_call
    return _call_unconverted(f, args, kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/autograph/impl/api.py"", line 255, in _call_unconverted
    return f(*args)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1079, in __call__
    return self.conv_op(inp, filter)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 635, in __call__
    return self.call(inp, filter)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 234, in __call__
    name=self.name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1953, in conv2d
    name=name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1071, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""~/.local/lib64/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

```"
"Hi, 

Questions regarding denoiser models from paper ""Unprocessing Images for Learned Raw Denoising"". Can I run the inference for some arbitrary already noisy image (RGB) if I don't have access to noise information (shot and read noises) from metadata?

Specifically in the 'dnd_denoise.py' file, I can see the feed_dict comprised of noisy image, read and shot noise tensors. 

Thanks!   "
"Hello, 
Thanks for releasing the code for 'Unprocessing Images ... Raw Denoising'. Upon trying the training process, I see that the training simulation gets stuck at this point -
![Capture](https://user-images.githubusercontent.com/27413823/60852822-ef15fd80-a22b-11e9-903e-d549f8a62ea2.PNG)

This is my run command -
```python train.py --model_dir='./ckpts/' --train_pattern=/disk1/aashishsharma/Datasets/MIRFlickr_Dataset/train/* --test_pattern=/disk1/aashishsharma/Datasets/MIRFlickr_Dataset/test/*```

Anybody knows this problem? any workaround? Thanks!
"
"    saver = tf.train.import_meta_graph(FLAGS.model_ckpt + '.meta')

  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py"", line 1449, in import_meta_graph
    **kwargs)[0]

  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py"", line 1463, in _import_meta_graph_with_return_elements
    meta_graph_def = meta_graph.read_meta_graph_file(meta_graph_or_file)

  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\meta_graph.py"", line 695, in read_meta_graph_file
    text_format.Merge(file_content.decode(""utf-8""), meta_graph_def)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 1: invalid continuation byte"
"Hi, 
I was wondering if you could provide the pre-trained models for CVPR2019 paper title 'Unprocessing Images for Learned Raw Denoising'. I have looked into 'unprocessing' dir, no models or inference code, do you intend to release that or not?

Thanks you!
Touqeer"
"In GUI render mode at  _del_ function I am getting object has no attribute 'cid' error.

How can i render GUI while training?"
"**I try to run run_classifier.sh  in cpu, it runs ok. But when I run in gpu, sometimes good, sometimes bad.** I think it might have something to do with seq_max_length and batch_size. But it's useless to reduce seq_max_length and batch_size. 

following is my run_classifier.sh
> python3.5 run_classifier.py \
  --task_name=sim \
  --do_train=true \
  --do_eval=true \
  --do_predict=true \
  --data_dir=$MY_DATASET \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=2.0 \


following is error log

> > 2019-03-21 14:21:12.342706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3
2019-03-21 14:21:13.673814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-03-21 14:21:13.673891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3
2019-03-21 14:21:13.673904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N N N
2019-03-21 14:21:13.673912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N N N
2019-03-21 14:21:13.673918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N N
2019-03-21 14:21:13.673925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N N N
2019-03-21 14:21:13.675250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15119 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pc
i bus id: 0000:00:07.0, compute capability: 6.0)
2019-03-21 14:21:13.676003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15119 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pc
i bus id: 0000:00:08.0, compute capability: 6.0)
2019-03-21 14:21:13.676420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15119 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pc
i bus id: 0000:00:09.0, compute capability: 6.0)
2019-03-21 14:21:13.676764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15119 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pc
i bus id: 0000:00:0a.0, compute capability: 6.0)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into ./mayi_model_2/model.ckpt.
2019-03-21 14:22:44.995671: E tensorflow/core/kernels/check_numerics_op.cc:185] abnormal_detected_host @0x1085900ab00 = {1, 0} Found Inf or NaN global norm.
INFO:tensorflow:Error recorded from training_loop: Found Inf or NaN global norm. : Tensor had NaN values
         [[node VerifyFinite/CheckNumerics (defined at /var/log//bert_chi/optimization.py:74)  = CheckNumerics[T=DT_FLOAT, message=""Found Inf or NaN global norm."", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](global_n
orm/global_norm)]]"
"@sarahooker I really enjoyed the paper. I'd like to engage in a little speculation and I hope you'll indulge me.

### Knowledge transfer during iterative sparsification

The lottery ticket result surprises me. I think you should be able to retrain to much closer to the same accuracy given the same initialization and a sparse mask. However I speculate that the magnitude pruning method induces knowledge transfer which prevents this.

Because the sparsity inducing mask changes during the iterative process you're dealing with some number of subnets. If they were fully disjoint you would transfer knowledge using one as a teacher and the other as the student. However in the iterative process you have a gradual knowledge transfer. Which means that the representations (and ultimate accuracy of the sparsified network) are no longer a function of sparse initial weights + training, but the full initial weights and the sparsification procedure. 

If this is the case I suspect that if you do a single-step sparsification at the end of training and use *that* sparse mask along with the same initial weights (lottery ticket) you should see much closer accuracies. 

(Iterative pruning is still a *better* way to do pruning of course.)

### Knowledge reconstitution

I'm curious how much work has been done in the area of densifying sparse nets. For example, can you perfectly reverse the accurracy loss curves by increasing sparsity and retraining? Does it work better if you do this in one step (go from 90% sparsity to 70% sparsity by initializing a lot of random weights) or iteratively (90->85->80->75->70)

Ultimately the question is do you think a sparse bottleneck + densification + retraining procedure can produce a highly efficient and compressed version of finetuning?"
"We tried to recreate some of the results in dql_grasping. After setting up the environment according to requirements, we ran run_random_collect_oss.sh, and then run_train_collect_eval_oss.sh with dqn on-policy and dqn off-policy. The results shown in images below suggest the training didn't converge on policies with expected success rate, what steps should we take to reproduce similar results to those presented in the paper?

![eval1](https://user-images.githubusercontent.com/46766340/52463626-74d23780-2b2c-11e9-9be0-aa4dca945e5d.png)
![eval2](https://user-images.githubusercontent.com/46766340/52463630-78fe5500-2b2c-11e9-8d24-b714ba4aeff6.png)
![test1](https://user-images.githubusercontent.com/46766340/52463633-7b60af00-2b2c-11e9-875f-db651344326e.png)
![test2](https://user-images.githubusercontent.com/46766340/52463635-7e5b9f80-2b2c-11e9-81c9-511bc1e53c73.png)
"
"Hi, what the hell tensorflow.google is?  I checked you official tf API and related topics on websites, but found nothing. My tf version is 1.12.0.
Thanks"
"It takes 5 sec to get this on a normal windows pc, but takes over 35 secs on mac m1. After the first `Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}` appears, it would stock for 30 secs. Is there any way to solve this problem?
```
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
model ignore: /Users/xxx/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
model ignore: /Users/xxx/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
find model: /Users/xxx/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
model ignore: /Users/xxx/.insightface/models/buffalo_l/genderage.onnx genderage
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
model ignore: /Users/xxx/.insightface/models/buffalo_l/w600k_r50.onnx recognition
set det-size: (640, 640)
```"
"Hi,

I wonder if there is any chance that you can release pretrained weights of ResNet200 model? Or you have released but is not able to be found from this github page, sincerely thank you from replying me."
"Dear Team,

Thanks so much for releasing this great work together with the dataset. I wonder if your team would be interested  in sharing your models, dataset and the demo on HuggingFace Hub?

The Hub offers **free** hosting of over 93k models, 14k datasets and it would make your work more accessible and visible to the rest of the ML community. We can help you set up a Deepinsight organization. 

Some of the benefits of sharing your models and weightss through the Hub would be:
* Free hosting of super large dataset thanks to git-lfs and fast distribution with global CDNs (including China)
* versioning, commit history and diffs
* repos provide useful metadata about their tasks, metrics, etc. that make them discoverable
* wider reach of your work to the ecosystem

Creating the repos and adding new models should be a relatively straightforward process if you've used Git before. Here are some step-by-step guide explaining the process in case you're interested. 
- Add a model https://huggingface.co/docs/hub/adding-a-model
- Add a dataset https://huggingface.co/docs/hub/datasets-adding
- Add a Space demo https://huggingface.co/blog/gradio-spaces

Example spaces with repos:
github: https://github.com/salesforce/BLIP
Spaces: https://huggingface.co/spaces/akhaliq/BLIP

github: https://github.com/facebookresearch/omnivore
Spaces: https://huggingface.co/spaces/akhaliq/omnivore

Example dataset: https://huggingface.co/datasets/glue


Please let us know if you would be interested and if you have any questions.

Happy to hear your thoughts,
Tiezhen
HuggingFace"
"Hello, could you tell me how to solve this problem?

"
"Hi. 
When I used the “onnx_ijbc.py ” file to test the accuracy of the trained ViT-s model on IJBC, I encountered the following errors:

![image](https://user-images.githubusercontent.com/46049620/204463901-1f9bc0d5-2530-4ec4-90a7-b72f7ed40f86.png)

Can someone tell me how to solve this problem?  Thanks a lot!

"
"Hi,
Thanks so much for sharing such great work!
I would like to ask how to generate anchors that adapts to my own dataset. For the 640*640 resolution of scrfd's input, does it be generated by k-means clustering? whether needs to consider resize the original image to 640*640? obtained anchors  in this way, and then calculate the parameters as scales/ratios/base_size/stride in config files related anchor?
Thanks a lot!"
"Hello everyone,

I am having a scenario where gallery contains 10000 Identities,and probe is from the megaface testset. Lets say i take a subset,around 50000 of these Identities for testing,i am getting out of memory error to store a 50000x10000 matrix. How to handle such large scale datasets? Is there any implementation for Identification & Verification evaluation in python ? 

regards"
"Hi there,

I find the process of model training from scratch very confusing. Can anybody provide guides/advices/tips and other sort of help on how to train **ArcFace by insightface**, please?

I have already read this [post](https://github.com/deepinsight/insightface/issues/791), but still do not get a clear picture of this process.

Any help will be much appreciated."
"Good morning, all

Please I would like to have VGGFace dataset the first version published in 2015
cause I couldn't find it 

"
"Hello Everyone,

If in case there are multiple images per person in the gallery. Typically say at different poses , during evalution in 1:N what strategies we can use to get the final score/scores.

Probe = 10 images (Each person 1 image, Total 10 person,IDs = 21 to 29 and ID = 4)
Gallery = 5 folders (Each folder contains 10 images , Total 5 person & 50 images , IDs = 0 to 4)
Note: Each ID belongs to one unique person. In the above dataset probe image of ID 4 is same identity/person  as in Gallery of ID 4 

Lets say probe image of  ID 4 is macthed with the gallery,i will get 1x50 scores. Now how to select the Top1 result. Within Gallery of ID 4,as there are multiple images per person, what startegy to follow to arrive at top1 result.  Does the current evaluation_1N function in [here](https://github.com/deepinsight/insightface/blob/master/recognition/_evaluation_/ijb/ijb_evals.py#:~:text=def%20evaluation_1N(query_feats%2C%20gallery_feats%2C%20query_ids%2C%20reg_ids%2C%20fars%3D%5B0.01%2C%200.1%5D)%3A) supports this type of evaluations as well ?

 "
"I have trained a mobilenet_0_25 model using wilderface dataset and train.py on detection/retinaface/train.py, during traing process, the Loss and RPNAcc_BG keep changing but the RPNAcc_FG always staying at 0.0000 after many epochs. 

This is my config:
INFO:root:Called with argument: Namespace(begin_epoch=0, dataset='retinaface', dataset_path='data/retinaface', end_epoch=100, frequent=20, image_set='train', kvstore='device', lr=0.01, lr_step='30,60,80', network='mnet', no_flip=False, no_shuffle=False, prefix='./model', pretrained='model/mobilenet_0_25', pretrained_epoch=0, root_path='data', wd=0.0005, work_load_list=None)
gpu num: 1
INFO:root:{'BBOX_MASK_THRESH': 0,
 'CASCADE': 0,
 'CASCADE_BBOX_STRIDES': [64, 32, 16, 8, 4],
 'CASCADE_CLS_STRIDES': [64, 32, 16, 8, 4],
 'CASCADE_MODE': 1,
 'COLOR_JITTERING': 0.125,
 'COLOR_MODE': 1,
 'CONTEXT_FILTER_RATIO': 1,
 'DENSE_ANCHOR': False,
 'FACE_LANDMARK': True,
 'FIXED_PARAMS': ['^stage1', '^.*upsampling'],
 'HEAD_BOX': False,
 'HEAD_FILTER_NUM': 64,
 'HEAD_MODULE': 'SSH',
 'IMAGE_STRIDE': 0,
 'LANDMARK_LR_MULT': 2.5,
 'LAYER_FIX': True,
 'LR_MODE': 0,
 'MIXUP': 0.0,
 'MORE_SMALL_BOX': True,
 'NET_MODE': 2,
 'NUM_ANCHORS': 2,
 'NUM_CLASSES': 2,
 'NUM_CPU': 4,
 'ORIGIN_SCALE': False,
 'PIXEL_MEANS': array([0., 0., 0.]),
 'PIXEL_SCALE': 1.0,
 'PIXEL_STDS': array([1., 1., 1.]),
 'PRE_SCALES': [(1200, 1600)],
 'RANDOM_FEAT_STRIDE': False,
 'RPN_ANCHOR_CFG': {'16': {'ALLOWED_BORDER': 9999,
                           'BASE_SIZE': 16,
                           'NUM_ANCHORS': 2,
                           'RATIOS': [1.0],
                           'SCALES': [8, 4]},
                    '32': {'ALLOWED_BORDER': 9999,
                           'BASE_SIZE': 16,
                           'NUM_ANCHORS': 2,
                           'RATIOS': [1.0],
                           'SCALES': [32, 16]},
                    '8': {'ALLOWED_BORDER': 9999,
                          'BASE_SIZE': 16,
                          'NUM_ANCHORS': 2,
                          'RATIOS': [1.0],
                          'SCALES': [2, 1]}},
 'RPN_FEAT_STRIDE': [32, 16, 8],
 'SCALES': [(640, 640)],
 'SHARE_WEIGHT_BBOX': False,
 'SHARE_WEIGHT_LANDMARK': False,
 'TEST': {'BATCH_IMAGES': 1,
          'CXX_PROPOSAL': True,
          'HAS_RPN': False,
          'IOU_THRESH': 0.5,
          'NMS': 0.3,
          'RPN_NMS_THRESH': 0.3,
          'RPN_POST_NMS_TOP_N': 3000,
          'RPN_PRE_NMS_TOP_N': 1000,
          'SCORE_THRESH': 0.05},
 'TRAIN': {'ASPECT_GROUPING': False,
           'BATCH_IMAGES': 32,
           'BBOX_STDS': [1.0, 1.0, 1.0, 1.0],
           'CASCADE_OVERLAP': [0.4, 0.5],
           'END2END': True,
           'IMAGE_ALIGN': 0,
           'LANDMARK_STD': 1.0,
           'MIN_BOX_SIZE': 0,
           'OHEM_MODE': 1,
           'RPN_BATCH_SIZE': 256,
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_ENABLE_OHEM': 2,
           'RPN_FG_FRACTION': 0.25,
           'RPN_FORCE_POSITIVE': False,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_POSITIVE_OVERLAP': 0.5},
 'USE_3D': False,
 'USE_BLUR': False,
 'USE_CROP': True,
 'USE_DCN': 0,
 'USE_FPN': True,
 'USE_MAXOUT': 0,
 'USE_OCCLUSION': False,
 'dataset': 'retinaface',
 'max_feat_channel': 8888,
 'network': 'mnet'}
origin image size 12880
retinaface_train gt roidb loaded from data/cache/retinaface_train_train_gt_roidb.pkl
roidb size 12876
INFO:root:retinaface_train append flipped images to roidb
flipped roidb size 25752
INFO:root:loading model/mobilenet_0_25,0
[03:42:31] ../src/nnvm/legacy_json_util.cc:208: Loading symbol saved by previous version v1.3.0. Attempting to upgrade...
[03:42:31] ../src/nnvm/legacy_json_util.cc:216: Symbol successfully upgraded!

This is the traning log of epoch 10
INFO:root:Saved checkpoint to ""./model-0000.params""
INFO:root:Epoch[10] Train-RPNAcc_s32=0.974243
INFO:root:Epoch[10] Train-RPNAcc_s32_BG=1.000000
INFO:root:Epoch[10] Train-RPNAcc_s32_FG=0.000000
INFO:root:Epoch[10] Train-RPNL1Loss_s32=0.394602
INFO:root:Epoch[10] Train-RPNLandMarkL1Loss_s32=0.902790
INFO:root:Epoch[10] Train-RPNAcc_s16=0.970903
INFO:root:Epoch[10] Train-RPNAcc_s16_BG=1.000000
INFO:root:Epoch[10] Train-RPNAcc_s16_FG=0.000000
INFO:root:Epoch[10] Train-RPNL1Loss_s16=0.283116
INFO:root:Epoch[10] Train-RPNLandMarkL1Loss_s16=0.572992
INFO:root:Epoch[10] Train-RPNAcc_s8=0.983558
INFO:root:Epoch[10] Train-RPNAcc_s8_BG=1.000000
INFO:root:Epoch[10] Train-RPNAcc_s8_FG=0.000000
INFO:root:Epoch[10] Train-RPNL1Loss_s8=0.282509
INFO:root:Epoch[10] Train-RPNLandMarkL1Loss_s8=0.621004
INFO:root:Epoch[10] Time cost=989.293

After about 10 epochs, the RPNAcc_s32_FG, RPNAcc_s16_FG and RPN_Acc_s8_FG keep staying at 0.
Please help me to fix it."
Hi can someone provide the overlapping/mapping between those two datasets?
""
""
"The margin_list in glint and webface configs in arcface_torch repository seems to be cosface instead of arcface
https://github.com/deepinsight/insightface/blob/6140283500701f1319fd820b7fa5d07c66ee334f/recognition/arcface_torch/configs/wf4m_r100.py#L8"
"Hi! So I trained the alignment hourglass model using the [train script](https://github.com/deepinsight/insightface/tree/master/alignment/heatmap/train.py).

Then I used the [`mxnet_to_onnx.py` script](https://github.com/deepinsight/insightface/tree/master/alignment/heatmap) and I got the following error:
```plaintext
Traceback (most recent call last):
  File ""./mxnet_to_onnx.py"", line 109, in <module>
    converted_model_path = onnx_mxnet.export_model(sym, all_args, [input_shape], np.float32, args.output)
  File ""/home/stf/.local/lib/python3.8/site-packages/mxnet/contrib/onnx/__init__.py"", line 53, in export_model
    return export_model_(*args, **kwargs)
  File ""/home/stf/.local/lib/python3.8/site-packages/mxnet/onnx/mx2onnx/_export_model.py"", line 127, in export_model
    onnx_graph = converter.create_onnx_graph_proto(sym, params, in_shapes,
  File ""/home/stf/.local/lib/python3.8/site-packages/mxnet/onnx/mx2onnx/_export_onnx.py"", line 335, in create_onnx_graph_proto
    converted, dtypes = MXNetGraph.convert_layer(
  File ""/home/stf/.local/lib/python3.8/site-packages/mxnet/onnx/mx2onnx/_export_onnx.py"", line 96, in convert_layer
    ret = convert_func(node, **kwargs)
  File ""/home/stf/.local/lib/python3.8/site-packages/mxnet/onnx/mx2onnx/_op_translations/_op_translations_opset12.py"", line 299, in convert_deconvolution
    assert len(input_nodes) == 3, 'Deconvolution takes 3 input if no_bias==False'
AssertionError: Deconvolution takes 3 input if no_bias==False
```

I tried a lot of things but I did not manage to solve this. How can I get past this error? I made no changes to the mxnet models.

I saw that @nttstar is the one who created that script. Do you have any insights? Thanks."
"Hi all,

The face detection fails to detect the face if it is taking up most of the image. Is there a parameter that can be changed to fix this issue?

Thanks!"
"Hi,

I tried to generate a new large scale lfw benchmark, the protocol is similar to blurf (http://www.cbsr.ia.ac.cn/users/scliao/projects/blufr/)

From the model zoo, I test glint360k_r50 and got 99% accuracy at FAR 1e-5, but I can't reproduce the result from the training code. my result is only 95% 

any idea?"
"I want to use 
`torchrun --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=12581 train.py configs/myconfig.py` 
to run the [example](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/train.py) rather than
 `python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=12581 train.py configs/myconfig.py`.

However, there are some errors:
```
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Training: 2022-11-08 09:01:32,354-rank_id: 0
2022-11-08 09:01:32.515047: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
Traceback (most recent call last):
  File ""train.py"", line 216, in <module>
  File ""train.py"", line 216, in <module>
        main(parser.parse_args())main(parser.parse_args())

  File ""train.py"", line 61, in main
  File ""train.py"", line 61, in main
        train_loader = get_dataloader(train_loader = get_dataloader(

  File ""/workspace/workfile/Projects/arcface/dataset.py"", line 57, in get_dataloader
  File ""/workspace/workfile/Projects/arcface/dataset.py"", line 57, in get_dataloader
        train_sampler = DistributedSampler(train_sampler = DistributedSampler(

  File ""/workspace/workfile/Projects/arcface/utils/utils_distributed_sampler.py"", line 100, in __init__
  File ""/workspace/workfile/Projects/arcface/utils/utils_distributed_sampler.py"", line 100, in __init__
        self.seed = sync_random_seed(seed)self.seed = sync_random_seed(seed)

  File ""/workspace/workfile/Projects/arcface/utils/utils_distributed_sampler.py"", line 77, in sync_random_seed
  File ""/workspace/workfile/Projects/arcface/utils/utils_distributed_sampler.py"", line 77, in sync_random_seed
        dist.broadcast(random_num, src=0)dist.broadcast(random_num, src=0)

  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py"", line 1193, in broadcast
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py"", line 1193, in broadcast
        work = default_pg.broadcast([tensor], opts)work = default_pg.broadcast([tensor], opts)

RuntimeErrorRuntimeError: : NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1191, invalid usage, NCCL version 2.10.3
ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1191, invalid usage, NCCL version 2.10.3
ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).

ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 686359) of binary: /usr/bin/python
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py"", line 765, in <module>
    main()
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py"", line 345, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py"", line 761, in main
    run(args)
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/run.py"", line 752, in run
    elastic_launch(
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py"", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-11-08_09:01:40
  host      : 7fc50ceb0af6
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 686360)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-11-08_09:01:40
  host      : 7fc50ceb0af6
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 686359)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
```"
"Hello,

I want to know which database was used for the table:
![image](https://user-images.githubusercontent.com/62914173/200373246-81a16521-aec2-4d53-8722-be1ff6182149.png)

especially for the demographic results.

Thanks in advance,"
"excuse me. when I run run_test.sh,this error always occurs:
./devkit/bin/Identification: error while loading shared libraries: libtbb.so.2: cannot open shared object file: No such file or directory
I've tried many things online, but they can't solve it"
"hi, I want to regenerate scrfd_10g_bnkps.onnx. Where is the pth file? I can only get pth model of scrfd_10g_kps.Are they the same?"
"Hi, Thanks for this great repo!
I have an issue with the training time, I have a machine with 7 GPUs, one node.
When I'm using your training scripts on all 7 GPUs I get about 1500+- samples/sec.
What can cause it? I should note that all the data is on the machine disk itself.
Also, I can't mount on ram, can this produce the issue? takes me about twice the time for training :(
Thanks.
"
"Hi,
here
https://github.com/deepinsight/insightface/blob/6140283500701f1319fd820b7fa5d07c66ee334f/recognition/partial_fc/mxnet/callbacks.py#L94
we save the **model_average** for the last 10k. But how about the **best checkpoint** saving for the whole run? 
Thank you!"
"Hi Team,

Thank you for releasing this great project. I've a got a few questions about the RetinaFace face detector and would really appreciate your response.

There are two RetinaFace papers. 

[1](https://arxiv.org/pdf/1905.00641.pdf). RetinaFace: Single-stage Dense Face Localisation in the Wild
[2](https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_RetinaFace_Single-Shot_Multi-Level_Face_Localisation_in_the_Wild_CVPR_2020_paper.pdf). RetinaFace: Single-shot Multi-level Face Localisation in the Wild


- Both of these papers have different accuracy on WiderFace listed
- Retinaface's README shows a different accuracy of [`WiderFace validation mAP: Easy 96.5, Medium 95.6, Hard 90.4.`](https://github.com/deepinsight/insightface/tree/afda338f887bb766ae0b0ee0deeebaddeb1d31dd/detection/retinaface#:~:text=WiderFace%20validation%20mAP%3A%20Easy%2096.5%2C%20Medium%2095.6%2C%20Hard%2090.4.)
- SCRFD's README shows a different accuracy of [`RetinaFace (CVPR20)	ResNet50	94.92	91.90	64.17	29.50	37.59	21.7`](https://github.com/deepinsight/insightface/tree/afda338f887bb766ae0b0ee0deeebaddeb1d31dd/detection/scrfd#:~:text=RetinaFace%20(CVPR20),21.7)

Four questions here:
1. Why is accuracy different for `RetinaFace-ResNet50` in [RetinaFace's README](https://github.com/deepinsight/insightface/tree/afda338f887bb766ae0b0ee0deeebaddeb1d31dd/detection/retinaface#:~:text=WiderFace%20validation%20mAP%3A%20Easy%2096.5%2C%20Medium%2095.6%2C%20Hard%2090.4.) and [SCRFD's README](https://github.com/deepinsight/insightface/tree/afda338f887bb766ae0b0ee0deeebaddeb1d31dd/detection/scrfd#:~:text=RetinaFace%20(CVPR20),21.7)?
2. From where is the Retinaface's README accuracy of [`WiderFace validation mAP: Easy 96.5, Medium 95.6, Hard 90.4.`](https://github.com/deepinsight/insightface/tree/afda338f887bb766ae0b0ee0deeebaddeb1d31dd/detection/retinaface#:~:text=WiderFace%20validation%20mAP%3A%20Easy%2096.5%2C%20Medium%2095.6%2C%20Hard%2090.4.) sourced?
3. [RetinaFace-R50 pretrained mode](https://github.com/deepinsight/insightface/tree/afda338f887bb766ae0b0ee0deeebaddeb1d31dd/detection/retinaface#:~:text=Pretrained%20Model%3A%20RetinaFace%2DR50%20(baidu%20cloud%20or%20googledrive) ) is from which paper?
4. Is there any chance that improved models will be released?

Could you please clarify these things?

Thanks again for your response and contributing to the open source community.

"
"Hi all. I use the transformer model `vit_l_dp005_mask_005` on a dataset of 5 million identities.
Use `arcface_torch` version. 1 machine - 8 х V100 (32Gb)
Training params:
```
Training: 2022-10-27 06:32:28,027-: margin_list              [1.0, 0.0, 0.4]
Training: 2022-10-27 06:32:28,027-: network                  vit_l_dp005_mask_005
Training: 2022-10-27 06:32:28,028-: resume                   True
Training: 2022-10-27 06:32:28,028-: output                   ./work_dirs/VIT/weights
Training: 2022-10-27 06:32:28,028-: embedding_size           256
Training: 2022-10-27 06:32:28,028-: sample_rate              0.3
Training: 2022-10-27 06:32:28,028-: interclass_filtering_threshold0
Training: 2022-10-27 06:32:28,028-: fp16                     True
Training: 2022-10-27 06:32:28,028-: batch_size               256
Training: 2022-10-27 06:32:28,028-: optimizer                adamw
Training: 2022-10-27 06:32:28,028-: lr                       0.001
Training: 2022-10-27 06:32:28,028-: momentum                 0.9
Training: 2022-10-27 06:32:28,028-: weight_decay             0.1
Training: 2022-10-27 06:32:28,028-: verbose                  2000
Training: 2022-10-27 06:32:28,028-: frequent                 10
Training: 2022-10-27 06:32:28,028-: dali                     False
Training: 2022-10-27 06:32:28,028-: gradient_acc             1
Training: 2022-10-27 06:32:28,028-: seed                     2048
Training: 2022-10-27 06:32:28,028-: num_workers              0
Training: 2022-10-27 06:32:28,028-: epoch_resume             29
Training: 2022-10-27 06:32:28,028-: rec                      /media/Data2/ramdisk/
Training: 2022-10-27 06:32:28,028-: rec_val                  /media/Data2/valid
Training: 2022-10-27 06:32:28,028-: num_classes              5000000
Training: 2022-10-27 06:32:28,028-: num_image                79341362
Training: 2022-10-27 06:32:28,028-: num_epoch                70
Training: 2022-10-27 06:32:28,028-: warmup_epoch             10
Training: 2022-10-27 06:32:28,028-: val_targets              ['lfw', 'agedb_30', 'cfp_ff', 'cfp_fp', 'cplfw', 'vgg2_fp']
Training: 2022-10-27 06:32:28,029-: total_batch_size         2048
Training: 2022-10-27 06:32:28,029-: warmup_step              387400
Training: 2022-10-27 06:32:28,029-: total_step               2711800
```
After stopping training and resuming, there is an increase in loss, as if this is new data for the model. Moreover, the loss first starts to grow, and then stabilizes.
Mixing in the dataloader is enabled and working.
Has anyone encountered such problems? In which direction can you look?

Loss Plot:
![image](https://user-images.githubusercontent.com/54171614/198216253-a0524a1c-f59a-47a6-b31d-e16feae96be0.png)
"
How can I get the jmlr render mesh from predict 3d verts. whether I need some base model like the facial triangle indices ?
""
"I was wondering if the ArcFace loss is as effective in other domains as it is in the Face domain.

For example could one use ArcFace instead of Triplet Loss//Contrastive Learning/... losses to solve metric learning for other domains such as object recognition? and if it's doable, is it really stronger on that front too or is it specifically made for alignable data (like faces)?"
本地运行的结果比Web Demo的人脸检测效果差很多，请问下，Web Demo使用的模型或者预处理方式可以说明下吗
"Currently, detection using scrfd_10g_bnkps uses 5kps to norm_crop faces. However, this also introduces some background into the aligned faces. 

Hence, is it possible to use 2d106 for kps alignment? while still retaining scrfd_10g_bnkps as the detector."
"I tried to train your ViT implementation and other different backbones (like ConvNeXt, MaxViT, NFNet, CoAtNet, etc.) with the ArcFace loss function, and the loss and accuracies do not seem to converge. The loss either becomes stagnant at a value of about 20 or reduces to NaNs (with the default learning rate of 0.1 with SGD optimiser). The same backbones trained with CosFace loss are able to converge properly.

The ResNet backbones however, perform well when trained with ArcFace.

Any insights on why these losses perform so differently even though they are intuitively very similar, and how we can get the backbones to converge with ArcFace?

Any help would be highly appreciated @anxiangsir "
"I want to create my own Val dataset only for Asians. Can anyone help me with a complete and accurate step?
Thanks very much."
"When training arcface_torch project with glint360k  dataset, speed up through DALI. After training for an hour, the following error occurred, why is this?
Error when executing Mixed operator decoders__Image encountered:
Error in thread 7: [/opt/dali/dali/operators/decoder/nvjpeg/nvjpeg_decoder_decoupled_api.h:608] [/opt/dali/dali/image/image_factory.cc:102] Unrecognized image format. Supported formats are: JPEG, PNG, BMP, TIFF, PNM, JPEG2000 and WebP.
![image](https://user-images.githubusercontent.com/35326947/195569063-811d3fb6-e2ae-48f2-9af1-ec1197b4d1ed.png)
data directory：
![image](https://user-images.githubusercontent.com/35326947/195569317-1e8fd039-e144-4ee4-acd3-c943e7fb5da9.png)
"
"Hi there, I'm looking over your scrfd example [here](https://github.com/deepinsight/insightface/blob/master/detection/scrfd/tools/scrfd.py).

First of all, great work. 

The example shows how to obtain the bounding boxes, however it does not demonstrate how to obtain the 5 face landmark points (eyes, nose, mouth corners).

Can you please walk me through how I would modify the example to obtain the face landmark coordinates? "
May I know what does 0.45G indicate (https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch#1-training-on-single-host-gpu)? The final model size or number of params? Thanks
"Thanks for sharing your work.
scrfd_person_2.5g.onnx model for [person detection](https://github.com/deepinsight/insightface/tree/master/examples/person_detection) is in not provided. please share it.
I want to test your model for [MOTChallenge ](https://motchallenge.net/) pedestrian tracking.
I really appreciate it if you answer these questions. Thanks very much."
"Hi, wondering if there are parameters I can change to allow the face to be recognised in this image.  I have seen other FR algorithms detect the face.  No feedback is given when I use the python api to look for a face here.


![imgj php](https://user-images.githubusercontent.com/25236569/194796796-4a37094c-be7d-4282-a0be-640f750c4cff.jpeg)
"
"Hi, it missed the evaluation file in jmlr.Could you please share it? @nttstar "
"Thanks for your great work. Is there any plan to release a toolbox for multi-person face tracking (i.e., simultaneously online or offline detection and tracking)? I think this will also be a widely-used downstream application. Thanks."
I'm trying to train and prepare custom dataset for 3 days. Can you please provide some guide to train arcface_torch? 
"**code**
`
from insightface.app import FaceAnalysis
app = FaceAnalysis()
`

**output / error**
`
download_path: /home/akhil/.insightface/models/buffalo_l
Downloading /home/akhil/.insightface/models/buffalo_l.zip from http://insightface.cn-sh2.ufileos.com/models/buffalo_l.zip...
100%|██████████| 281857/281857 [10:35<00:00, 443.81KB/s] 
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [37], line 2
      1 from insightface.app import FaceAnalysis
----> 2 app = FaceAnalysis()

File /root/jupyterhub/lib/python3.9/site-packages/insightface/app/face_analysis.py:31, in FaceAnalysis.__init__(self, name, root, allowed_modules, **kwargs)
     29 onnx_files = sorted(onnx_files)
     30 for onnx_file in onnx_files:
---> 31     model = model_zoo.get_model(onnx_file, **kwargs)
     32     if model is None:
     33         print('model not recognized:', onnx_file)

File /root/jupyterhub/lib/python3.9/site-packages/insightface/model_zoo/model_zoo.py:83, in get_model(name, **kwargs)
     81 assert osp.isfile(model_file), 'model_file should be file'
     82 router = ModelRouter(model_file)
---> 83 model = router.get_model(providers=kwargs.get('providers'), provider_options=kwargs.get('provider_options'))
     84 return model

File /root/jupyterhub/lib/python3.9/site-packages/insightface/model_zoo/model_zoo.py:39, in ModelRouter.get_model(self, **kwargs)
     38 def get_model(self, **kwargs):
---> 39     session = PickableInferenceSession(self.onnx_file, **kwargs)
     40     print(f'Applied providers: {session._providers}, with options: {session._provider_options}')
     41     input_cfg = session.get_inputs()[0]

File /root/jupyterhub/lib/python3.9/site-packages/insightface/model_zoo/model_zoo.py:24, in PickableInferenceSession.__init__(self, model_path, **kwargs)
     23 def __init__(self, model_path, **kwargs):
---> 24     super().__init__(model_path, **kwargs)
     25     self.model_path = model_path

File /root/jupyterhub/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:347, in InferenceSession.__init__(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)
    344 disabled_optimizers = kwargs[""disabled_optimizers""] if ""disabled_optimizers"" in kwargs else None
    346 try:
--> 347     self._create_inference_session(providers, provider_options, disabled_optimizers)
    348 except ValueError:
    349     if self._enable_fallback:

File /root/jupyterhub/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:375, in InferenceSession._create_inference_session(self, providers, provider_options, disabled_optimizers)
    373 if providers == [] and len(available_providers) > 1:
    374     self.disable_fallback()
--> 375     raise ValueError(
    376         ""This ORT build has {} enabled. "".format(available_providers)
    377         + ""Since ORT 1.9, you are required to explicitly set ""
    378         + ""the providers parameter when instantiating InferenceSession. For example, ""
    379         ""onnxruntime.InferenceSession(..., providers={}, ...)"".format(available_providers)
    380     )
    382 session_options = self._sess_options if self._sess_options else C.get_default_session_options()
    383 if self._model_path:

ValueError: This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)
`"
"Dear author, 

Thanks again for your excellent job, but when I run the  demo, the program is stacked on this line.
Do you have some suggestions to locate it?

app = FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])

Best
Jiaxiang"
"dear @nttstar
What detector used on Recognition demo at http://demo.insightface.ai:7008?

Thanks in advance."
"1、I used windows10
2、I build datasets from LFW  origin  picture.

This error is blow:
(face) D:\facerecogntion\insightface\recognition\arcface_torch>python -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=12581 train.py configs/ms1mv2_mbf
NOTE: Redirects are currently not supported in Windows or MacOs.
D:\pyproject\conda\envs\face\lib\site-packages\torch\distributed\launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See
https://pytorch.org/docs/stable/distributed.html#launch-utility for
further instructions

  FutureWarning,
[W C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\distributed\c10d\socket.cpp:558] [c10d] The client socket has failed to connect to [activate.navicat.com]:12581 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\distributed\c10d\socket.cpp:558] [c10d] The client socket has failed to connect to [activate.navicat.com]:12581 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\distributed\c10d\socket.cpp:558] [c10d] The client socket has failed to connect to [activate.navicat.com]:12581 (system error: 10049 - 在其上下文中，该请求的地址无效。).
[W C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\distributed\c10d\socket.cpp:558] [c10d] The client socket has failed to connect to [activate.navicat.com]:12581 (system error: 10049 - 在其上下文中，该请求的地址无效。).
Training: 2022-10-01 08:49:58,472-rank_id: 0
D:\pyproject\conda\envs\face\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
D:\pyproject\conda\envs\face\lib\site-packages\tensorflow\python\util\nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  _pywrap_tensorflow.RegisterType(""Mapping"", _collections.Mapping)
D:\pyproject\conda\envs\face\lib\site-packages\tensorflow\python\util\nest.py:1287: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  _pywrap_tensorflow.RegisterType(""Sequence"", _collections.Sequence)
D:\pyproject\conda\envs\face\lib\site-packages\tensorflow\python\training\tracking\object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
D:\pyproject\conda\envs\face\lib\site-packages\tensorflow\python\training\tracking\object_identity.py:112: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  class ObjectIdentitySet(collections.MutableSet):
D:\pyproject\conda\envs\face\lib\site-packages\tensorflow\python\training\tracking\data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
D:\pyproject\conda\envs\face\lib\site-packages\keras_preprocessing\image\utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS
D:\pyproject\conda\envs\face\lib\site-packages\torch\nn\parallel\distributed.py:1737: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  ""You passed find_unused_parameters=true to DistributedDataParallel, ""
Training: 2022-10-01 08:50:01,847-: margin_list              [1.0, 0.5, 0.0]
Training: 2022-10-01 08:50:01,848-: network                  mbf
Training: 2022-10-01 08:50:01,849-: resume                   False
Training: 2022-10-01 08:50:01,853-: save_all_states          False
Training: 2022-10-01 08:50:01,854-: output                   work_dirs\ms1mv2_mbf
Training: 2022-10-01 08:50:01,854-: embedding_size           128
Training: 2022-10-01 08:50:01,855-: sample_rate              1.0
Training: 2022-10-01 08:50:01,856-: interclass_filtering_threshold0
Training: 2022-10-01 08:50:01,863-: fp16                     True
Training: 2022-10-01 08:50:01,863-: batch_size               2
Training: 2022-10-01 08:50:01,863-: optimizer                sgd
Training: 2022-10-01 08:50:01,864-: lr                       0.1
Training: 2022-10-01 08:50:01,864-: momentum                 0.9
Training: 2022-10-01 08:50:01,865-: weight_decay             0.0005
Training: 2022-10-01 08:50:01,873-: verbose                  2000
Training: 2022-10-01 08:50:01,873-: frequent                 10
Training: 2022-10-01 08:50:01,874-: dali                     False
Training: 2022-10-01 08:50:01,874-: gradient_acc             1
Training: 2022-10-01 08:50:01,875-: seed                     2048
Training: 2022-10-01 08:50:01,875-: num_workers              0
Training: 2022-10-01 08:50:01,875-: rec                      ../_datasets_/faces_lfw
Training: 2022-10-01 08:50:01,876-: num_classes              6
Training: 2022-10-01 08:50:01,885-: num_image                58
Training: 2022-10-01 08:50:01,885-: num_epoch                5
Training: 2022-10-01 08:50:01,886-: warmup_epoch             0
Training: 2022-10-01 08:50:01,887-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2022-10-01 08:50:01,894-: total_batch_size         2
Training: 2022-10-01 08:50:01,894-: warmup_step              0
Training: 2022-10-01 08:50:01,895-: total_step               145
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
torch.Size([12000, 3, 112, 112])
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
torch.Size([14000, 3, 112, 112])
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
torch.Size([12000, 3, 112, 112])
D:\pyproject\conda\envs\face\lib\site-packages\torch\optim\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  ""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"", UserWarning)
Training: 2022-10-01 08:50:45,541-Reducer buckets have been rebuilt in this iteration.
Training: 2022-10-01 08:50:46,859-Speed 27.43 samples/sec   Loss 38.3868   LearningRate 0.075510   Epoch: 0   Global Step: 20   Fp16 Grad Scale: 64   Required: 0 hours
Training: 2022-10-01 08:50:47,678-Speed 24.54 samples/sec   Loss 34.1343   LearningRate 0.064000   Epoch: 1   Global Step: 30   Fp16 Grad Scale: 64   Required: 0 hours
Training: 2022-10-01 08:50:48,408-Speed 27.43 samples/sec   Loss 34.6079   LearningRate 0.053441   Epoch: 1   Global Step: 40   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:49,147-Speed 27.06 samples/sec   Loss 35.2048   LearningRate 0.043834   Epoch: 1   Global Step: 50   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:49,936-Speed 25.35 samples/sec   Loss 34.2611   LearningRate 0.035177   Epoch: 2   Global Step: 60   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:50,683-Speed 26.77 samples/sec   Loss 34.6517   LearningRate 0.027472   Epoch: 2   Global Step: 70   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:51,426-Speed 26.92 samples/sec   Loss 32.0162   LearningRate 0.020718   Epoch: 2   Global Step: 80   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:52,204-Speed 25.74 samples/sec   Loss 31.9312   LearningRate 0.014916   Epoch: 3   Global Step: 90   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:52,952-Speed 26.77 samples/sec   Loss 31.7183   LearningRate 0.010064   Epoch: 3   Global Step: 100   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:53,692-Speed 27.03 samples/sec   Loss 35.9666   LearningRate 0.006164   Epoch: 3   Global Step: 110   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:54,470-Speed 25.74 samples/sec   Loss 30.7639   LearningRate 0.003215   Epoch: 4   Global Step: 120   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:55,206-Speed 27.21 samples/sec   Loss 29.2269   LearningRate 0.001218   Epoch: 4   Global Step: 130   Fp16 Grad Scale: 32   Required: 0 hours
Training: 2022-10-01 08:50:55,939-Speed 27.32 samples/sec   Loss 35.6505   LearningRate 0.000171   Epoch: 4   Global Step: 140   Fp16 Grad Scale: 32   Required: 0 hours
Exception ignored in: <function MXRecordIO.__del__ at 0x000001B30E0313A8>
Traceback (most recent call last):
  File ""D:\pyproject\conda\envs\face\lib\site-packages\mxnet\recordio.py"", line 84, in __del__
  File ""D:\pyproject\conda\envs\face\lib\site-packages\mxnet\recordio.py"", line 217, in close
TypeError: super() argument 1 must be type, not None


3、How to deal with it?  
 This is  building datasets having error or others?

Best regards,
Star"
"      //            s = d[dim - 1]
      //            d[dim - 1] = -1
      //            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))
      //            d[dim - 1] = s
1.      int s = d.at<float>(dim - 1, 0) = -1;
2.     d.at<float>(dim - 1, 0) = -1;

3.     T.rowRange(0, dim).colRange(0, dim) = U * V;
4.   cv::Mat diag_ = cv::Mat::diag(d);
5.   cv::Mat twp = diag_ * V; // np.dot(np.diag(d), V.T)
6.   cv::Mat B = cv::Mat::zeros(3, 3, CV_8UC1);
7.   cv::Mat C = B.diag(0);
8.    T.rowRange(0, dim).colRange(0, dim) = U * twp;
9.   d.at<float>(dim - 1, 0) = s;

I think there is some bug and meaningless code, should it be the following code

1.      int s = d.at<float>(dim - 1, 0);
2.     d.at<float>(dim - 1, 0) = -1;
4.     cv::Mat diag_ = cv::Mat::diag(d);
5.     cv::Mat twp = diag_ * V;
8.     T.rowRange(0, dim).colRange(0, dim) = U * twp;
9.     d.at<float>(dim - 1, 0) = s;"
""
"The link to LFW and others is not available, why is this?
Best regards and thank you very much,
Daniel

"
"Thanks for sharing your excellent work!
I am looking through your code, and there seem to be some differences with the paper shared on arxiv (or I misunderstood something).
Are the data preparation, training, and configuration files the same as those used to train the pretrained-model you share?
Some examples:
1. in `rec_builder.py` line 79, `cfg.input_size = 512`. My understanding is that this results in image with size 512, not 256 as described in the paper
2. in `train.py` line 245, we have `iter_loss.backward()`, but `iter_loss = dloss['Loss']`, which does not include the ""bone_losses"". I.e. We only have L_vert + L_land in Eq. 4 in the paper on arxiv.

Thanks in advance."
"i am doing model conversion trt, so i need it .thanks"
"Hi.
thank you for sharing your work.
What would be the steps to further map the face texture onto the 3d model
given the 2d to 3d correspondences and the transformation matrices
"
"windows10
Traceback (most recent call last):
  File ""D:/AIProject/insightface0.4/test1.py"", line 2, in <module>
    checkpoint = torch.load(""insightface-master/detection/scrfd/weights/scrfd_10g_bnkps.pth"", map_location='gpu')
  File ""D:\Miniconda3\envs\insightface0.4\lib\site-packages\torch\serialization.py"", line 608, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""D:\Miniconda3\envs\insightface0.4\lib\site-packages\torch\serialization.py"", line 787, in _legacy_load
    result = unpickler.load()
  File ""D:\Miniconda3\envs\insightface0.4\lib\site-packages\torch\serialization.py"", line 743, in persistent_load
    deserialized_objects[root_key] = restore_location(obj, location)
  File ""D:\Miniconda3\envs\insightface0.4\lib\site-packages\torch\serialization.py"", line 824, in restore_location
    return default_restore_location(storage, map_location)
  File ""D:\Miniconda3\envs\insightface0.4\lib\site-packages\torch\serialization.py"", line 180, in default_restore_location
    + location + "")"")
don't know how to restore data location of torch.FloatStorage (tagged with gpu)
**I dont know why**
### My solution
torch.load(map_location='cuda:0')  "
"hello. I wanna try the example in Readme but i got this error (warning). can somebody give me a advice?

> Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
find model: /home/mohammad/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
find model: /home/mohammad/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
find model: /home/mohammad/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
find model: /home/mohammad/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0
Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}
find model: /home/mohammad/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5
set det-size: (640, 640)
/home/mohammad/Project/nextaibox/insightface_master/python-package/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.
To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.
  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4

what should I do?!"
" python -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=0 --master_addr=""192.168.8.131"" --master_port=12581 train.py configs/ms1mv2_mbf


python -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=1 --master_addr=""192.168.8.131"" --master_port=12581 train.py configs/ms1mv2_mbf




/home/pc/anaconda3/envs/face19/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 192.168.8.131:12581
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_4a5rychg/none__fkba0g3
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/home/pc/anaconda3/envs/face19/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=192.168.8.131
  master_port=12581
  group_rank=0
  group_world_size=2
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[2]
  global_world_sizes=[2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_4a5rychg/none__fkba0g3/attempt_0/0/error.json
0
0
Training: 2022-09-15 11:06:52,012-rank_id: 0
Training: 2022-09-15 11:06:55,830-: margin_list              [1.0, 0.5, 0.0]
Training: 2022-09-15 11:06:55,830-: network                  mbf
Training: 2022-09-15 11:06:55,834-: resume                   False
Training: 2022-09-15 11:06:55,834-: save_all_states          False
Training: 2022-09-15 11:06:55,834-: output                   work_dirs/ms1mv2_mbf
Training: 2022-09-15 11:06:55,834-: embedding_size           512
Training: 2022-09-15 11:06:55,834-: sample_rate              1.0
Training: 2022-09-15 11:06:55,834-: interclass_filtering_threshold0
Training: 2022-09-15 11:06:55,834-: fp16                     True
Training: 2022-09-15 11:06:55,834-: batch_size               256
Training: 2022-09-15 11:06:55,834-: optimizer                sgd
Training: 2022-09-15 11:06:55,834-: lr                       0.1
Training: 2022-09-15 11:06:55,834-: momentum                 0.9
Training: 2022-09-15 11:06:55,834-: weight_decay             0.0001
Training: 2022-09-15 11:06:55,834-: verbose                  2000
Training: 2022-09-15 11:06:55,834-: frequent                 10
Training: 2022-09-15 11:06:55,834-: dali                     False
Training: 2022-09-15 11:06:55,834-: gradient_acc             1
Training: 2022-09-15 11:06:55,834-: seed                     2048
Training: 2022-09-15 11:06:55,834-: num_workers              4
Training: 2022-09-15 11:06:55,834-: rec                      /home/pc/faces_webface_112x112
Training: 2022-09-15 11:06:55,834-: num_classes              10572
Training: 2022-09-15 11:06:55,834-: num_image                494194
Training: 2022-09-15 11:06:55,834-: num_epoch                40
Training: 2022-09-15 11:06:55,835-: warmup_epoch             0
Training: 2022-09-15 11:06:55,835-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2022-09-15 11:06:55,835-: total_batch_size         512
Training: 2022-09-15 11:06:55,835-: warmup_step              0
Training: 2022-09-15 11:06:55,835-: total_step               38600
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
torch.Size([12000, 3, 112, 112])
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
torch.Size([14000, 3, 112, 112])
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
torch.Size([12000, 3, 112, 112])
/home/pc/fc/face/insightface/recognition/arcface_torch/train.py:163: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)
/home/pc/anaconda3/envs/face19/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(""Detected call of `lr_scheduler.step()` before `optimizer.step()`. ""
Training: 2022-09-15 11:07:37,277-Reducer buckets have been rebuilt in this iteration.
Training: 2022-09-15 11:07:55,067-Speed 518.42 samples/sec   Loss 44.2595   LearningRate 0.099902   Epoch: 0   Global Step: 20   Fp16 Grad Scale: 8192   Required: 13 hours
Training: 2022-09-15 11:08:04,952-Speed 517.94 samples/sec   Loss 45.0456   LearningRate 0.099850   Epoch: 0   Global Step: 30   Fp16 Grad Scale: 8192   Required: 12 hours
Training: 2022-09-15 11:08:14,893-Speed 515.12 samples/sec   Loss 45.5388   LearningRate 0.099798   Epoch: 0   Global Step: 40   Fp16 Grad Scale: 8192   Required: 12 hours
Training: 2022-09-15 11:08:24,767-Speed 518.53 samples/sec   Loss 45.7875   LearningRate 0.099746   Epoch: 0   Global Step: 50   Fp16 Grad Scale: 8192   Required: 12 hours
Training: 2022-09-15 11:08:34,667-Speed 517.22 samples/sec   Loss 45.5845   LearningRate 0.099695   Epoch: 0   Global Step: 60   Fp16 Grad Scale: 8192   Required: 11 hours
Training: 2022-09-15 11:08:44,533-Speed 518.98 samples/sec   Loss 45.6968   LearningRate 0.099643   Epoch: 0   Global Step: 70   Fp16 Grad Scale: 8192   Required: 11 hours




(face19) ubuntu@ubuntu-X10SRA:~/fc/face/insightface/recognition/arcface_torch$ python -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=1 --master_addr=""192.168.8.131"" --master_port=12581 train.py configs/ms1mv2_mbf
/home/ubuntu/anaconda3/envs/face19/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 192.168.8.131:12581
  rdzv_configs     : {'rank': 1, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_bcc_b24k/none_nbf6ckxx
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/home/ubuntu/anaconda3/envs/face19/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=192.168.8.131
  master_port=12581
  group_rank=1
  group_world_size=2
  local_ranks=[0]
  role_ranks=[1]
  global_ranks=[1]
  role_world_sizes=[2]
  global_world_sizes=[2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_bcc_b24k/none_nbf6ckxx/attempt_0/0/error.json
sgd
/home/ubuntu/fc/face/insightface/recognition/arcface_torch/train.py:166: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)
/home/ubuntu/anaconda3/envs/face19/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(""Detected call of `lr_scheduler.step()` before `optimizer.step()`. ""
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)

"
"Hi
I'm trying to create a Arcface code using Tensorflow2.x
I have a problem in custom layer and the whole ArcFace Model
Please help me if you can
This is my implementation for arcface layer based on the algorithm from your article

```
class MyArcFaceLayer(tf.keras.layers.Layer):
    """"""ArcMarginPenaltyLogists""""""
    def __init__(self, num_classes, kernel_regularizer, margin=0.5, logist_scale=64., **kwargs):
        super(MyArcFaceLayer, self).__init__(**kwargs)
        self.num_classes = num_classes
        self.margin = margin
        self.logist_scale = logist_scale
        self.kernel_regularizer = kernel_regularizer

    def build(self, input_shape):
        self.w = self.add_weight(name=""arcface_weights"", initializer='glorot_uniform', shape=[512, self.num_classes], trainable=True, regularizer=self.kernel_regularizer)
        self.pi = tf.constant(pi)

    def call(self, embds, labels):
        
        normed_embds = tf.nn.l2_normalize(embds, axis=1, name='normed_embd')
        normed_w = tf.nn.l2_normalize(self.w, axis=0, name='normed_weights')

        fc7 = tf.matmul(normed_embds, normed_w, name='fc7')
        theta = tf.math.acos(fc7)
        
        marginal_target_logit = tf.math.maximum(tf.math.cos(theta + self.margin), tf.math.cos(self.pi - self.margin)) 
        
        original_target_logit = tf.math.cos(theta)
        print(""original_target_logit = {}"".format(original_target_logit.shape))
        
        fc7 = fc7 + labels * (marginal_target_logit - original_target_logit)
        fc7 = fc7 * self.logist_scale        
        return fc7
        
```

Do you see a difference or a problem in that?
I found some implementation from tensorflow1 that you suggested in your ReadMe but they are different from your algorithm
Here is the implementation from https://github.com/auroua/InsightFace_TF/blob/master/losses/face_losses.py

```
def arcface_loss(embedding, labels, out_num, w_init=None, s=64., m=0.5):
    '''
    :param embedding: the input embedding vectors
    :param labels:  the input labels, the shape should be eg: (batch_size, 1)
    :param s: scalar value default is 64
    :param out_num: output class num
    :param m: the margin value, default is 0.5
    :return: the final cacualted output, this output is send into the tf.nn.softmax directly
    '''
    cos_m = math.cos(m)
    sin_m = math.sin(m)
    mm = sin_m * m  # issue 1
    threshold = math.cos(math.pi - m)
    with tf.variable_scope('arcface_loss'):
        # inputs and weights norm
        embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)
        embedding = tf.div(embedding, embedding_norm, name='norm_embedding')
        weights = tf.get_variable(name='embedding_weights', shape=(embedding.get_shape().as_list()[-1], out_num),
                                  initializer=w_init, dtype=tf.float32)
        weights_norm = tf.norm(weights, axis=0, keep_dims=True)
        weights = tf.div(weights, weights_norm, name='norm_weights')
        # cos(theta+m)
        cos_t = tf.matmul(embedding, weights, name='cos_t')
        cos_t2 = tf.square(cos_t, name='cos_2')
        sin_t2 = tf.subtract(1., cos_t2, name='sin_2')
        sin_t = tf.sqrt(sin_t2, name='sin_t')
        cos_mt = s * tf.subtract(tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name='cos_mt')

        # this condition controls the theta+m should in range [0, pi]
        #      0<=theta+m<=pi
        #     -m<=theta<=pi-m
        cond_v = cos_t - threshold
        cond = tf.cast(tf.nn.relu(cond_v, name='if_else'), dtype=tf.bool)

        keep_val = s*(cos_t - mm)
        cos_mt_temp = tf.where(cond, cos_mt, keep_val)

        mask = tf.one_hot(labels, depth=out_num, name='one_hot_mask')
        # mask = tf.squeeze(mask, 1)
        inv_mask = tf.subtract(1., mask, name='inverse_mask')

        s_cos_t = tf.multiply(s, cos_t, name='scalar_cos_t')

        output = tf.add(tf.multiply(s_cos_t, inv_mask), tf.multiply(cos_mt_temp, mask), name='arcface_loss_output')
    return output
```

This is my implementation for my whole ArcFace Model

```
def create_face_verification_model_v2(input_shape=(112, 112), num_class=8732, weight_decay=0.0001):
    rgb_input_shape = input_shape + (3, )
    input_layer = Input(rgb_input_shape)
    
    global_initializer = 'glorot_uniform'
    global_regularizer = l2(weight_decay)
    global_bias = False
    
    backbone = tf.keras.applications.ResNet101V2(input_shape=rgb_input_shape, weights=None, include_top=False)
    
    backbone_output = backbone(input_layer)
    x = BatchNormalization(gamma_regularizer=global_regularizer, beta_regularizer=global_regularizer)(backbone_output)
    x = Dropout(0.5)(x)

    # x = GlobalAveragePooling2D()(x)
    x = Flatten()(x)
    x = Dense(512, kernel_initializer=global_initializer, kernel_regularizer=global_regularizer, bias_regularizer=global_regularizer, use_bias=True)(x)
    x = BatchNormalization(gamma_regularizer=global_regularizer, beta_regularizer=global_regularizer)(x)
    
    embed_model = tf.keras.models.Model(input_layer, x)
    embed_model.summary()
    
    # NESSESERY?
    # x = BatchNormalization()(x)
    
    # OPTION 1 (ARCFACE)
    label_inputs = Input((num_class, ))    
    x, original_target_logit = ArcFaceLayer(num_class, kernel_regularizer=global_regularizer)(x, label_inputs)
    arcface_model = tf.keras.models.Model([input_layer, label_inputs], [x, original_target_logit])
    
    # OPTION 2 (SOFTMAX)
    # x = Dense(num_class, kernel_initializer=global_initializer, kernel_regularizer=global_regularizer, bias_regularizer=global_regularizer, use_bias=True)(x)    
    # arcface_model = tf.keras.models.Model([input_layer, label_inputs], [x, x])
    
    arcface_model.summary()
    
    for var in arcface_model.trainable_variables:
        print(var.name)
    
    return embed_model, arcface_model
```

When in your article you said that you trained your model with weight_decay=0.0005, which layers did you meant?

Thank you "
"The dropbox and gdrive links are broken at:
- https://github.com/deepinsight/insightface/tree/master/alignment/coordinate_reg
- https://github.com/deepinsight/insightface/tree/master/detection/retinaface

Any help as to how to download PyTorch models for face landmark detection would be appreciated."
How to use TripletLoss in arcface_torch？
"Newbie for insightface.

I like to get code 

1. how to train model for insightface 
2. how to use model for insightface, we need to compare two faces

Anyone can help please 


"
"When running ins_get_image() over multiple images, memory usage builds up until the program crashes.  Exiting python releases the memory, but exiting the function does not.  Commenting out this line fixed the issue for me:
https://github.com/deepinsight/insightface/blob/ce3600a74209808017deaf73c036759b96a44ccb/python-package/insightface/data/image.py#L25

OS: Windows 10
Python: 3.10.7
Insightface:0.6.2"
"I printed the model structure.
```python
  (bbox_head): SCRFDHead(
    (loss_cls): QualityFocalLoss()
    (loss_bbox): DIoULoss()
    (relu): ReLU(inplace=True)
    (cls_stride_convs): ModuleDict(
      ((8, 8)): ModuleList(
        (0): DepthwiseSeparableConvModule(
          (depthwise_conv): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
```
And then I read the model layer by layer. I found that the loss cls of each head branch is called the same name. 
**I want to use different names to distinguish the loss of each branch**, like the `cls_stride_convs`, which uses the `(8, 8), (16, 16), (32, 32)` to distinguish each branch.

The result I want may be like this:
```python
(loss_cls): ModuleDict(
(8, 8): QualityFocalLoss()
(16,16): QualityFocalLoss()
(32,32): QualityFocalLoss()
```
"
IResNet50 vs ResNet50(in object detection): 43.77M  vs 33.71M 
"The file [`insightface/reconstruction/PBIDR/code/evaluation/eval.py`](https://github.com/deepinsight/insightface/blob/master/reconstruction/PBIDR/code/evaluation/eval.py) imports the `'cvxpy'` library, but it isn't listed at the file [`insightface/reconstruction/PBIDR/requirements.txt`](https://github.com/deepinsight/insightface/blob/master/reconstruction/PBIDR/requirements.txt) to be installed.

Maybe the [`insightface/reconstruction/PBIDR/requirements.txt`](https://github.com/deepinsight/insightface/blob/master/reconstruction/PBIDR/requirements.txt) should be like that:

```
----------------------------------
vtk
numpy
opencv-python
scikit-image
scipy
Pillow
argparse
GPUtil
pyhocon
plotly
cvxpy
----------------------------------
```

In addition, the command `'pip install requirements.txt'` at the file [insightface/blob/master/reconstruction/PBIDR/INSTALL.md](https://github.com/deepinsight/insightface/blob/master/reconstruction/PBIDR/INSTALL.md#install-other-requirments) requires a `'-r'` parameter to install all libs, like that:

```
----------------------------------
pip install -r requirements.txt
----------------------------------
```

I hope it helps!
"
"Hi,

Will converting the final output of arcface embeddings 512 dimensions from float32 to float16 significantly reduce the search accuracy?

Regards,
Kevin"
"Hi, when I used to run the ONNX converted model using scrfd.py, it seems like using the CPU not the GPU"
"Works fine:
```
docker run -it python:3.9 bash
pip install poetry==1.1.15
poetry init
poetry add insightface
```

Doesn't work:
```
docker run -it python:3.9 bash
pip install poetry==1.2.0
poetry init
poetry add insightface
```

It fails with this error:
```
Package operations: 39 installs, 0 updates, 0 removals

  • Installing numpy (1.23.2)
  • Installing joblib (1.1.0)
  • Installing pillow (9.2.0)
  • Installing pyparsing (3.0.9)
  • Installing scipy (1.6.1)
  • Installing threadpoolctl (3.1.0)
  • Installing imageio (2.21.2)
  • Installing networkx (2.8.6)
  • Installing opencv-python-headless (4.6.0.66)
  • Installing packaging (21.3)
  • Installing pywavelets (1.3.0)
  • Installing scikit-learn (1.1.2)
  • Installing six (1.16.0)
  • Installing tifffile (2022.8.12)
  • Installing tomli (2.0.1)
  • Installing typing-extensions (4.3.0)
  • Installing certifi (2022.6.15): Installing...
  • Installing charset-normalizer (2.1.1): Installing...
  • Installing cycler (0.11.0): Installing...
  • Installing fonttools (4.37.1): Downloading... 80%
  • Installing certifi (2022.6.15)
  • Installing charset-normalizer (2.1.1)
  • Installing cycler (0.11.0)
  • Installing fonttools (4.37.1)
  • Installing idna (3.3)
  • Installing kiwisolver (1.4.4)
  • Installing protobuf (3.20.1)
  • Installing python-dateutil (2.8.2)
  • Installing pyyaml (6.0)
  • Installing qudida (0.0.4)
  • Installing scikit-image (0.19.3)
  • Installing setuptools-scm (6.4.2)
  • Installing urllib3 (1.26.12)
  • Installing wcwidth (0.2.5)
  • Installing albumentations (1.2.1)
  • Installing cython (0.29.32)
  • Installing easydict (1.9)
  • Installing matplotlib (3.5.3)
  • Installing onnx (1.12.0)
  • Installing prettytable (3.4.0)
  • Installing requests (2.28.1)
  • Installing tqdm (4.64.0)
  • Installing insightface (0.6.2): Failed

  CalledProcessError

  Command '['/root/.cache/pypoetry/virtualenvs/-il7asoJj-py3.9/bin/python', '/usr/local/lib/python3.9/site-packages/virtualenv/seed/wheels/embed/pip-22.2.2-py3-none-any.whl/pip', 'install', '--use-pep517', '--disable-pip-version-check', '--prefix', '/root/.cache/pypoetry/virtualenvs/-il7asoJj-py3.9', '--no-deps', '/root/.cache/pypoetry/artifacts/55/27/59/47a73e5a3d4d9cfcd8e40a5378f1bcf1080dc5d72b8f0bc45162bb3270/insightface-0.6.2.tar.gz']' returned non-zero exit status 1.

  at /usr/local/lib/python3.9/subprocess.py:528 in run
       524│             # We don't call process.wait() as .__exit__ does that for us.
       525│             raise
       526│         retcode = process.poll()
       527│         if check and retcode:
    →  528│             raise CalledProcessError(retcode, process.args,
       529│                                      output=stdout, stderr=stderr)
       530│     return CompletedProcess(process.args, retcode, stdout, stderr)
       531│ 
       532│ 

The following error occurred when trying to handle this error:


  EnvCommandError

  Command ['/root/.cache/pypoetry/virtualenvs/-il7asoJj-py3.9/bin/python', '/usr/local/lib/python3.9/site-packages/virtualenv/seed/wheels/embed/pip-22.2.2-py3-none-any.whl/pip', 'install', '--use-pep517', '--disable-pip-version-check', '--prefix', '/root/.cache/pypoetry/virtualenvs/-il7asoJj-py3.9', '--no-deps', '/root/.cache/pypoetry/artifacts/55/27/59/47a73e5a3d4d9cfcd8e40a5378f1bcf1080dc5d72b8f0bc45162bb3270/insightface-0.6.2.tar.gz'] errored with the following return code 1, and output: 
  Processing /root/.cache/pypoetry/artifacts/55/27/59/47a73e5a3d4d9cfcd8e40a5378f1bcf1080dc5d72b8f0bc45162bb3270/insightface-0.6.2.tar.gz
    Installing build dependencies: started
    Installing build dependencies: finished with status 'done'
    Getting requirements to build wheel: started
    Getting requirements to build wheel: finished with status 'error'
    error: subprocess-exited-with-error
    
    × Getting requirements to build wheel did not run successfully.
    │ exit code: 1
    ╰─> [17 lines of output]
        Traceback (most recent call last):
          File ""/tmp/tmp_y82h47w_in_process.py"", line 363, in <module>
            main()
          File ""/tmp/tmp_y82h47w_in_process.py"", line 345, in main
            json_out['return_val'] = hook(**hook_input['kwargs'])
          File ""/tmp/tmp_y82h47w_in_process.py"", line 130, in get_requires_for_build_wheel
            return hook(config_settings)
          File ""/tmp/pip-build-env-xrx5mnt8/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 338, in get_requires_for_build_wheel
            return self._get_build_requires(config_settings, requirements=['wheel'])
          File ""/tmp/pip-build-env-xrx5mnt8/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 320, in _get_build_requires
            self.run_setup()
          File ""/tmp/pip-build-env-xrx5mnt8/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 482, in run_setup
            super(_BuildMetaLegacyBackend,
          File ""/tmp/pip-build-env-xrx5mnt8/overlay/lib/python3.9/site-packages/setuptools/build_meta.py"", line 335, in run_setup
            exec(code, locals())
          File ""<string>"", line 5, in <module>
        ModuleNotFoundError: No module named 'numpy'
        [end of output]
    
    note: This error originates from a subprocess, and is likely not a problem with pip.
  error: subprocess-exited-with-error
  
  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  

  at /usr/local/lib/python3.9/site-packages/poetry/utils/env.py:1473 in _run
      1469│                 output = subprocess.check_output(
      1470│                     command, stderr=subprocess.STDOUT, env=env, **kwargs
      1471│                 )
      1472│         except CalledProcessError as e:
    → 1473│             raise EnvCommandError(e, input=input_)
      1474│ 
      1475│         return decode(output)
      1476│ 
      1477│     def execute(self, bin: str, *args: str, **kwargs: Any) -> int:

The following error occurred when trying to handle this error:


  PoetryException

  Failed to install /root/.cache/pypoetry/artifacts/55/27/59/47a73e5a3d4d9cfcd8e40a5378f1bcf1080dc5d72b8f0bc45162bb3270/insightface-0.6.2.tar.gz

  at /usr/local/lib/python3.9/site-packages/poetry/utils/pip.py:51 in pip_install
       47│ 
       48│     try:
       49│         return environment.run_pip(*args)
       50│     except EnvCommandError as e:
    →  51│         raise PoetryException(f""Failed to install {path.as_posix()}"") from e
       52│ 
```"
"Hi, Thanks for this awesome work. I'm trying to use ArcFace on FFHQ. However, it seems there's domain gap so the running mean and variance does not directly fit to FFHQ. Has anyone tried to fine-tune ArcFace on other datasets and how to handle the BatchNorm layers?

I also tried to freeze the BatchNorm layers, which unfortunately leads to NaN."
""
@nttstar I checked the pre-trained model that was made available and there is no fc7_*_weight layers - referred in Line 100 of drop.py (Using r100's pretrained models)I think these layers comes from parallal_local_module_v1? Is there any new push to be done to this repo? Trying to run drop.py -- any suggestions?
"I want to infer some samples in jmlr ,but not find prjection_matrix.txt!"
"Hi @nttstar 

I would like to try out:
https://github.com/deepinsight/insightface/tree/master/detection/retinaface_anticov

but the model on dropbox link seems to be dead :(

Any chance the model can be reuploaded to one of the cloud providers? gdrive or dropbox please - not able to use baidu :(

thank you!

"
@nttstar How would I estimate the GPU memory requirements for a custom dataset? Let's say I have 1M id's and 45M images - How would I know how much memory I need? Does the memory requirement linearly distribute for multi GPU settings?
"Is there a guide on how I can train ArcFace on custom dataset?
"
"I 've found that Margin base Softmax is config different for each dataset in Arcface_torch source. 
But in Arcface_torch config only have margin base softmax( config.margin_list) for Glint360k, ms1mv, wf. I want to ask how to estimate or get the best params for each dataset. Such as Casia basically"
"![Uploading image.png…]()

please help me solve this problem

thanks all

"
"As `embeddings` is normed in line 269, why is it needed to norm it in line 265 at first?
https://github.com/deepinsight/insightface/blob/c3f2e2c70672a4656a76e667029a98000e56059d/recognition/arcface_torch/eval/verification.py#L264-L269"
"I have a pretrain for face recognition already. Now I want to evaluate it on Umdfaces instead of the basic valid dataset(age30, lfw, ..). Can someone can share the script. thank you a alot"
"Hello.
I want to train arcface_mxnet recognition model. I installed cuda10.0 and mxnet-cu100 and got this error after running train.py:
![image](https://user-images.githubusercontent.com/10956392/184878004-a3fcda5a-1ba6-42f6-ac63-45bcc1e899d9.png)
By the way, I try cuda10.2 and mxnet-cu102 but I got following error too:
**OSError: libcudnn.so.8: cannot open shared object file: No such file or directory**
I appreciate your help.
So many thanks in advance."
"I download the resnet 18 pretrained model(R18 | Glint360K | 72.07) for face encoding or face embedding or anything we call it, and it is onnx format. I do not know how to preprocess the aligned face image before feed it into this onnx model. I use the another face detect model and the alignment of dlib library. in the face embedding, I'd like use insightface pretrained model. I'd appeciated if any one can help me."
"First off, thank you very much for this repo. It has been very insightful and performs nicely.

Now, I have a problem where I'd like to use face recognition as a base for transfer learning a different task.
As such I'd like to use the ArcFace weights and finetune them on my own task, updating the representations in the process.

However, I am not sure how to do this using the ONNX model weights you have provided.
Could anyone please shed some light on that for me?

Thanks in advance!"
"insightface->detection->retinaface->retinaface.py (line 464)

bbox_pred (line 761)

Dimension issue
![MicrosoftTeams-image (1)](https://user-images.githubusercontent.com/26607565/183900375-57e15cb1-54e1-4a20-8fc3-7fb93d66be44.png)

"
"![Screenshot from 2022-08-10 18-48-17](https://user-images.githubusercontent.com/48430251/183871739-5116d799-51d9-4356-9de6-f5c1a6c82f69.png)
"
"```python3
app = FaceAnalysis(name=""antelope"")
```

```
    raise RuntimeError(""Failed downloading url %s"" % url)
RuntimeError: Failed downloading url http://insightface.cn-sh2.ufileos.com/models/antelopev2.zip
```

Insightface version : insightface==0.6.2
Python Version : 3.10.4
Help please."
"In arcface, is there any difference set no_bias=True or False?
```python
In recognition/arcface_mxnet/train.py
fc7 = mx.sym.FullyConnected(data=nembedding,
                                    weight=_weight,
                                    no_bias=True,
                                    num_hidden=config.num_classes,
                                    name='fc7')
```"
"I find that it is forced to use partial fc in arcface_torch, but I just want to use arcface. Is there any offical implement with pytorch? Or need    I implement it by myself (any advice to do this, I'm worried about some tricks I didn't think of...)?  Thanks!"
"I have recently found this amazing repo and I was looking at the model zoo.

I see that there is a ResNet50 model trained on WebFace600k that yields better results than the model trained with Glint360k. But for ResNet100 there is one model trained with Glint360k but the trained model on WebFace600k is missing. It is very likely that a ResNet100 trained on WebFace600k will outperform the current published models. 

Do you have plans of training a ResNet100 on WebFace600k?

Thanks :)
Julio"
"Hi team,

I have problem when make in retinaface.


![image](https://user-images.githubusercontent.com/87979189/181710046-e23b54a6-2f61-454e-a6ba-4ae372f8f119.png)



"
"If it ""yes"" HOW should I do it? Maybe I should normalize brightness or something like this?!"
face parse has lots of errors
"Hello! I know this might be a stupid question, but i'm really strugling to understand.

I have an implementation of a ViT and i want to train it with ArcFace loss on a small dataset just to test and nothing else.
I took your implementation of ArcFace loss except that i don't understand what are the inputs (logits, labels).

I guess it's the image embedding infered by the ViT and it's class (one hot encoded) but this does not work .. ! i could download the code and try running to understand but god, this implementation costs a lot of time to run and data are huge.

 thanks in advance!"
"Hello,
for the [Resnet34, Arcface, MS1MV3], the fc weights might not be correct. The number of classes in the fc is inconsistent to the number of classes in MS1MV3 (91180!=93431) in both baidu and one drive.

https://pan.baidu.com/s/1CL-l4zWqsI1oDuEEYVhj-g#list/path=%2Fsharelink3494273403-838107067808247%2Farcface_torch%2Fms1mv3_arcface_r34_fp16&parentPath=%2Fsharelink3494273403-838107067808247

https://onedrive.live.com/?authkey=%21AFZjr283nwZHqbA&id=4A83B6B633B029CC%215579&cid=4A83B6B633B029CC

Would it be possible that you re-upload them?
Many thanks"
"There is no clear documentation for how to prepare the validation datasets for arcface training. Can someone point out how it is done? I am trying to use arcface_torch, Thank you!"
"Hello,

I'm getting the following error while trying to convert any of the SCRFD .pth models to onnx using scrfd2onnx.py

Here's the used command: 
`python3 tools/scrfd2onnx.py configs/scrfd/scrfd_2.5g.py tools/models/SCRFD_2.5G.pth `

Here's what I'm getting:

> set to dynamic input with dummy shape: (1, 3, 640, 640)
load checkpoint from local path: tools/models/SCRFD_2.5G.pth
/home/yousef/Desktop/IN3/insightface/detection/scrfd/mmcv/mmcv/onnx/symbolic.py:481: UserWarning: DeprecationWarning: This function will be deprecated in future. Welcome to use the unified model deployment toolbox MMDeploy: https://github.com/open-mmlab/mmdeploy
  warnings.warn(msg)
Traceback (most recent call last):
  File ""tools/scrfd2onnx.py"", line 198, in <module>
    pytorch2onnx(
  File ""tools/scrfd2onnx.py"", line 84, in pytorch2onnx
    torch.onnx.export(
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/onnx/__init__.py"", line 350, in export
    return utils.export(
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/onnx/utils.py"", line 163, in export
    _export(
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/onnx/utils.py"", line 1074, in _export
    graph, params_dict, torch_out = _model_to_graph(
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/onnx/utils.py"", line 727, in _model_to_graph
    graph, params, torch_out, module = _create_jit_graph(model, args)
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/onnx/utils.py"", line 602, in _create_jit_graph
    graph, torch_out = _trace_and_get_graph_from_model(model, args)
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/onnx/utils.py"", line 517, in _trace_and_get_graph_from_model
    trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/jit/_trace.py"", line 1175, in _get_trace_graph
    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/yousef/Desktop/IN3/insightface/detection/scrfd/venv/lib/python3.8/site-packages/torch/jit/_trace.py"", line 95, in forward
    in_vars, in_desc = _flatten(args)
RuntimeError: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: numpy.ndarray
"
"Hi, I've read some discussion(https://discuss.pytorch.org/t/checkpoint-with-batchnorm-running-averages/17738) about pytorch checkpoint. The running mean/var of BN layer seems update twice when checkpoint is used in training. The way to make batch statistic updating correct is changing momentum to (1 - sqrt(1- original momentum)). 
In the recognition project, It seems that the checkpoint is simply used without changing BN momentum. Will this affect the model training? Is it necessary to change momentum manually? @anxiangsir "
"With lower version of Pytorch(1.6.0 for example), problem occurs when using exported ONNX model to inference with a dynamic input.
`2022-07-19 17:04:28.618850992 [E:onnxruntime:, sequential_executor.cc:333 Execute] Non-zero status code returned while running Reshape node. Name:'Reshape_5' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:43 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{32,256,12,12}, requested shape:{1,256,144}`

It is a flattening operation in the VIT backbone, which is designed to flatten the last two dimensions of the tensor.

The model was exported with dummy input (1,3,112,112). To make dynamic batch size feasible, the input shape dim_param was changed to 'None' afterwards.
`graph.input[0].type.tensor_type.shape.dim[0].dim_param = 'None'` (see recognition/arcface_torch/torch2onnx.py)

However, the Reshape node does not convey this message. It hard codes the destined shape.
![image](https://user-images.githubusercontent.com/62649623/179912603-9fde9409-7c0e-44a2-b7c2-d44406af3625.png)
When input shape is dynamic (32, 256, 12, 12), the destined shape fails to become (32,256,144) and remains (1,256,144).

Pytorch 1.11.0 fixed this problem. For lower version users, using torch.reshape instead of flatten enables a correct conversion.
Original Code (recognition/arcface_torch/backbones/vit.py):
`x = self.proj(x).flatten(2).transpose(1, 2)`
Use Reshape:
`x = self.proj(x)`
`batch_size, channels, height, width = x.shape`
`x = torch.reshape(x, (batch_size, channels, height*width))`
`x = x.transpose(1,2)`

It passes the onnx_helper check and passes the MFR online test.







"
Everything is on the title :D
The face recognition accuracy decreases because of face angles and this is the biggest hindrance. If a highly accurate Face Frontalization is developed this can solve the problem permanently. This is a no brainer. I would suggest the owners of the repo to move into this direction. 
"Thanks for your great work. I notice that the synthetics dataset that your train the face alignment model has 70 landmark but the ResNet50d model only outputs 68 of them, missing the 2 pupil centers. Can you give a pre-trained model that contains 70 landmarks including 2 pupil centers for a more comprehensive downstream usage？Thanks."
Is it possible to add pruning to models with onnx ?
"How to search by Euclidian distance using Milvus?
Can someone please share a simple example?

"
"@yingfeng  大佬你好，就是我是一名大二学生，然后是在中北大学的robomaster战队里负责用神经网络识别装甲板实现自动瞄准，不过就是之前我用yolo系列训练出来的模型最后实际测试时得到的bbox和装甲板的轮廓并不能很好的拟合，导致后续使用pnp进行姿态解算时会有较大误差，所以我想将传统yolo的数据集格式改为用四个角点的归一化坐标，现在的数据集格式是像这样：1 0.673029 0.373564 0.678429 0.426232 0.830433 0.401262 0.824525 0.351212，第一个数字是类别id，后面八个数字是归一化后的装甲板的四个角点坐标，之前我使用yolov5-face已经训练出来一个可以直接定位装甲板四个角点的模型，效果如下：
![image](https://user-images.githubusercontent.com/53631206/177446867-353f2b4d-9197-4f0e-b161-1e82418b3973.png)
然后很早之前就想尝试一些其他的人脸检测模型对比一下关键点定位精度，最开始是想用retinaface，但是当时也是因为数据集使用的widerface，我标注的yolo格式不能直接用于训练，当时也想过离线数据转换，就是写一个脚本将yolo格式转化为widerface格式，不过由于忙其他事一直没什么时间，后来就看到了scrfd这个模型，在人脸检测的速度和精度上都有不错的表现，而且使用的mmdet框架训练，注册机制对于修改模型很方便，但是现在我同样遇到了数据集格式转化的问题
# 0--Parade/0_Parade_marchingband_1_849.jpg 1024 1385
449.00000 330.00000 571.00000 479.00000 488.90601 373.64301 0.00000 542.08899 376.44199 0.00000 515.03101 412.82999 0.00000 485.17401 425.89301 0.00000 538.35699 431.49100 0.00000
这是在你们提供的标签文件中的一部分，按照你们README.md里所说的，前四个应该是左上和右下点的坐标，后面依次是5个关键点的坐标，而且每个点之间用0.00000隔开，就是这里我有疑问，因为在后面的部分中有些时候0.00000有变为-1.00000或者1.00000，如下图：
![image](https://user-images.githubusercontent.com/53631206/177448166-9bac4835-dc91-4476-8a6f-9d83436b64dc.png)
所以我想请教一下你们0.00000还有-1.00000和1.00000分别代表什么意思，而且还有个问题就是我想实现多个装甲板的分类的话是否需要改动模型结构，之前使用yolov5face只需要更改yaml中的nc和names就行，希望大佬能不吝赐教！"
""
"I tried to replicate the results for ViT Base model trained on WebFace42M but the model does not seem to converge. The loss starts at 53 and stagnates at about 22 after a few epochs of training. I have used the exact same config, with the max. learning rate scaled according to my batch size. I am using 4 GPUs for training and the config variables are as below:

`
config.network = ""vit_b""

config.embedding_size = 256

#Partial FC
config.sample_rate = 1

config.fp16 = True
config.batch_size = 1500

#For AdamW
config.optimizer = ""adamw""
config.lr = 0.00025
config.weight_decay = 0.1

config.verbose = 1415
config.dali = False

config.rec = ""/media/data/Webface42M_rec""
config.num_classes = 2059906
config.num_epoch = 40
config.warmup_epoch = config.num_epoch//10
`

Do you have any insights on why this could be happening? 

Any help would be highly appreciated  @anxiangsir 
"
"运行scrdf报错
Traceback (most recent call last):
  File ""D:/yolo/insightface-master/detection/scrfd/tools/scrfd.py"", line 318, in <module>
    bboxes, kpss = detector.detect(img, 0.5, input_size = (640, 640))
  File ""D:/yolo/insightface-master/detection/scrfd/tools/scrfd.py"", line 225, in detect
    scores_list, bboxes_list, kpss_list = self.forward(det_img, thresh)
  File ""D:/yolo/insightface-master/detection/scrfd/tools/scrfd.py"", line 195, in forward
    bboxes = distance2bbox(anchor_centers, bbox_preds)
  File ""D:/yolo/insightface-master/detection/scrfd/tools/scrfd.py"", line 38, in distance2bbox
    x1 = points[:, 0] - distance[:, 0]
ValueError: operands could not be broadcast together with shapes (19200,) (12800,) 
"
"Hi,
How do you extraxt 3D landmarks from an image?

Cannot find in docs / README

Thank you"
"当网络中使用到torch.split或torch.chunk的时候，会造成测试服务器CPU出现内存泄露并死机。
在本地服务器上测试发现当cudnn==8.0.5会造成这个问题，升级cudnn的版本可以解决这个问题。
麻烦升级 MFR测试服务器的cudnn版本来解决这个问题，多谢！"
"
https://github.com/deepinsight/insightface/blob/e78eee59caff3515a4db467976cf6293aba55035/parsing/dml_csr/dataset/datasets.py#L179
return 的结果缺少 semantic_edges？

https://github.com/deepinsight/insightface/blob/e78eee59caff3515a4db467976cf6293aba55035/parsing/dml_csr/train.py#L302
这里最后的 _ 应该是meta 吧
缺少了semantic_edges，可以描述下怎么生成的吗？
谢谢"
"I get error of ""Illegal Instruction (Core Dumped)"" on trying to run the train.py in insightface/recognition/arcface_mxnet"
""
Not able to download the above mentioned model packs 
how to apply adam and adamw optimizer to the partialfc training?
"I did some modifications on the SCRFD model and its performance failed my expectation.
I expanded 5 keypoints output to 68 and added a separate conv-bn module just like for bbox and cls_score.
Also I added a mask module to output the mask probability of each face.

The complexity is shown blow:
`ON VGA:
4.1717028 G
0.931747 M`

The benchmark inference time is approximately 13ms (2080ti).
Also I have doubled the FLOPs comparing to original SCRFD_2.5G, but is this inference time reasonable?"
""
"if want train 2d106det,how to train 2d106det？"
"I saw that you are normalizing the embedding output [here](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/verification.py#L321). Do we need to normalize the embedding? Since, if I removed the normalization part, the performance drop significantly. But, how does it run on the real-time implementation if we need to normalize the embedding? Thank you"
"1. why the classification loss have two value, i think one value is enough, such as 0 means no face, 1 means having face.
2. why there is a crop after unsample:
![image](https://user-images.githubusercontent.com/43233772/173868677-2d70ddb3-3331-4875-acbe-76acb864e63d.png)

"
"insightface published 8 checkpoint of partial fc. Each checkpoint corresponds to a part of FC layer. For example, glint360k has 360000 identities. Each checkpoint has shape (360000/8, 512). 
Please help me
1. How can I merge all of 8 checkpoints to one fc? Because I want to use full FC layer for pretrained
2. If I have 3 gpus, how can I load 8 checkpoints to 3 gpus?
Thank you"
"In the [forward function](https://github.com/deepinsight/insightface/blob/e78eee59caff3515a4db467976cf6293aba55035/recognition/arcface_torch/losses.py#L76), we have the following logic - 

``` Python
if self.easy_margin:
  final_target_logit = torch.where(target_logit > 0, cos_theta_m, target_logit)
else:
  final_target_logit = torch.where(target_logit > self.theta, cos_theta_m, target_logit - self.sinmm)
```

From what I understand, this logic is meant to calculate the final_target_logit that is used to compute the loss function. I have two questions here - 
1. Why was this conditional statement added in the code when there is no mention of this logic in the research paper. 
2. What is the logic behind calculating `self.sinmm` and `self.theta`"
"Training: 412-Speed 556.82 samples/sec   Loss 42.4856   LearningRate 0.001128   Epoch: 0   Global Step: 600   Fp16 Grad Scale: 256   Required: 426 hours

>>> 2 gpu, bs=320, wf42m
>>>num_workers=16, sample_rate=0.2
>>>dali_data_iter(..., num_threads=16, ...)
>>>cuda 10.1 -->install nvidia-dali-cuda102 / nvidia-dali-cuda100
why?"
""
您好，我用多卡训练partialFC分类层，之前用50万分类时，四张显卡占用显存基本一致。但是当分类增加到116万时，有两张显卡的占用就特别的高(25G)，另外两张卡占用7G左右。这两张占用较多显存的卡分到的分类每个ID图片数量较多，平均50张，剩下两张占用很少显存的显卡分到的分类每个ID只有两三张图片，这种会造成各个显卡占用不均吗？
"The License section of the readme mentions:
""The training data containing the annotation (and the models trained with these data) are available for **non-commercial research purposes only**.""

If an organization/activity does not fall under this definition, are there any model buying/licensing options available or can these models never be used in this context?

Could someone clarify how both **""non-commercial""** and **""research purpose""** are exactly defined?
Does it need to be both ""non-commercial"" AND ""research purpose"" or is it ""non-commercial"" OR ""research purpose""?

Some additional questions that could help clarify the statement:
- Is a non-profit organization considered non-commercial?
- Can a commercial company use the models for internal research projects (non-commercial research project, but commercial company)?
- Does the ""research purpose"" mean that these models can never be used in a production environment, even if the purpose is non-commercial?"
"I want to use arcface torch to train the CASIA webface dataset. But I encountered difficulties.
I download the dataset from [here](https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_).Some blogs told me to set ""num_classes"" to 10572, but I found that the number of categories in the paper is 10575.The blog was published in 2020. ""[insightface](https://github.com/deepinsight/insightface)"" is different from that time.I can't find the documentation that tells me how to set up the config file."
"Thanks for sharing the RetinaFace Face Detector training code.

The links provided for the ImageNet pre-trained models,

- ImageNet ResNet50
- ImageNet ResNet152
are down( both baidu_cloud and dropbox). [here](https://github.com/deepinsight/insightface/blob/4c61cde106908abe9eac35a17535dcd9563fa3ae/detection/retinaface/README.md#training)  is the link
Please provide the fresh links, so models can be downloaded. 
"
"- I have researched some marginal softmax as a head of Face recognition model. There are variable architectures such as ArcFace, CosFace, AdaCos ... Most of the heads based on marginal and we need to choose some parameter s, m ...
- Are there any experiments that choose these parameters by training them?
- When facing to masked face recognition problem, should we add weights to the marginal softmax layers depending on class masked or no masked?"
"My experiment is lauched with information
- GPU: 2
- Model: IResNet50
- Pretrained: No
- Marginal softmax: ArcFace (s=30, m=0.5), do not use partial_fc
- Batchsize : 64*2=128
- Warm up: 0
- Optimizer: AdamW
- Init learning rate: 1e-1
- Loss function: cross entropy
- Dataset: MS1Mv2
But I am facing to some weird cases
- Model could not converge and loss function is about 16 in many epochs (It does not like log of Insightface repo, loss seems converge so fast)
Thank you for your attention"
"could author add CurricularFace loss to your insightface?
CurricularFace loss:  https://github.com/HuangYG123/CurricularFace/blob/master/head/metrics.py

it was occur error to mask = cos_theta > cos_theta_m with multi gpu

RuntimeError: The size of tensor a (6387) must match the size of tensor b (110) at non-singleton dimension 1"
"I tried SwinTransformer with partialfc, the 'embed_dim' of  swintransformer is set to 96 and embedding_size of partialfc is set to 512. The img_size is 224*224."
"Which algorithm was used to align the WebFaces42M dataset? Any script of reference to follow?

Thanks in advance."
"i run on 2 machines use arcface_torch. The following prompt is displayed：
/home/lzdx/.virtualenvs/torchenv/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************


my run code:
node1:
python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=""192.168.1.3"" --master_port=12581 my_train.py configs/faces_glintasia > run_log/run_ip_node1.log 2>&1 &
node2:
python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=""192.168.1.2"" --master_port=12581 my_train.py configs/faces_glintasia  > run_log/run_ip_node2.log 2>&1 &

i am sorry that how to  modify it and  the code run. can u help me!!!Thanks!!!
"
""
"Hi, I found some engineering tricks in your ViT backbone code([ViT](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/backbones/vit.py)).



in Attention Module

![image](https://user-images.githubusercontent.com/94514604/169200241-ad7ca3ce-a06c-4230-816c-4d205d448df2.png)

in Block Module
![image](https://user-images.githubusercontent.com/94514604/169200310-eaceab77-3dd0-4d37-b33d-2a266916c2ea.png)

in forward_features of ViT
![image](https://user-images.githubusercontent.com/94514604/169200392-2f8aa8fd-2561-4783-9586-b648929f642e.png)


Could you explain the reason of using these codes? Thanks.
"
i can't fin https://github.com/deepinsight/insightface/blob/master/recognition/tools/cpp-align/FacePreprocess.h 
"could author add adaface loss to your insightface?
adaface code: https://github.com/mk-minchul/AdaFace/blob/master/head.py"
"Pretrained Model: RetinaFace-R50 downloading link not working please look into that
link for  Pretrained Model: https://www.dropbox.com/s/53ftnlarhyrpkg2/retinaface-R50.zip?dl=0"
""
"Hi, could you please tell me where can i find `generate_edge.py`? I think that your face-parsing approach greatly meets my requirements. I looked inside `parsing` directory but i was not able to find this scirpt.

Thank you!"
"Hello everyone,
I am facing an issue with `train.py` when `--launcher` is set to `pytorch` the validation step fails with the following stack trace: 
```python
  File ""test.py"", line 208, in <module>
    main()
  File ""test.py"", line 185, in main
    args.gpu_collect)
  File ""/home/blablabla/task3/scrfd/detection/scrfd/mmdet/apis/test.py"", line 97, in multi_gpu_test
    result = model(return_loss=False, rescale=True, **data)
  File ""/home/blablabla/anaconda3/envs/scrfd/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/blablabla/anaconda3/envs/scrfd/lib/python3.7/site-packages/torch/nn/parallel/distributed.py"", line 886, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File ""/home/blablabla/anaconda3/envs/scrfd/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/home/blablabla/task3/scrfd/detection/scrfd/mmcv/mmcv/runner/fp16_utils.py"", line 84, in new_func
    return old_func(*args, **kwargs)
  File ""/home/blablabla/task3/scrfd/detection/scrfd/mmdet/models/detectors/base.py"", line 182, in forward
    return self.forward_test(img, img_metas, **kwargs)
  File ""/home/blablabla/task3/scrfd/detection/scrfd/mmdet/models/detectors/base.py"", line 149, in forward_test
    img_meta[img_id]['batch_input_shape'] = tuple(img.size()[-2:])
TypeError: 'DataContainer' object is not subscriptable
```
I used `test.py` for debugging to save the training time. After some debugging, I noticed that `forward_test()` in [mmdet/models/detectors/base.py](https://github.com/deepinsight/insightface/blob/b5958bc1d81988dbcc113ababdfdc82125a0717a/detection/scrfd/mmdet/models/detectors/base.py#L130) expects `img_metas` to be `(List[List[dict]])` but under distributed training it's a `(List[DataContainer(List[List[dict()]])])` check below 

```python
        print('\ntype img_metas:', type(img_metas))
        print('type img_metas[0]:', type(img_metas[0]))
        print('type img_metas[0].data:', type(img_metas[0].data))
        print('type img_metas[0].data[0]:', type(img_metas[0].data[0]))
        print('type img_metas[0].data[0][0]:', type(img_metas[0].data[0][0]))
        exit()
#Output:
type img_metas: <class 'list'>
type img_metas[0]: <class 'mmcv.parallel.data_container.DataContainer'>
type img_metas[0].data: <class 'list'>
type img_metas[0].data[0]: <class 'list'>
type img_metas[0].data[0][0]: <class 'dict'>
```
Relaunching with `--launcher` set to `none` works normally. Also modifying the code to deal with the `DataContainer` works but is there a solution that works for both distributed and non distributed flow?

"
"I want to finetune the arcface with the ms1mv3, but  I have only one gpu. How to load all weight into softmax layer from rank_0_softmax_weight.pt to rank_7_softmax_weight.pt ?"
Can you share ageDB-30 dataset? It's a great dataset to do research on metric learning. Thank you!
"Hi! Thanks for awesome repo!
I try to use it on my data and something get wrong. Pls, help me with that

1) I make some face embeddings with ""R100 | Glint360K onnx"" recognition model
For embedding I use `cv2.dnn.blobFromImage` with `scalefactor=1.0 / 128.0, mean=(127.5, 127.5, 127.5), size=(112, 112), swapRB=True`
(face and flip face with a ~20px face paddings **1 QUESTION**: _should I do paddings here?_)
<p><img src=""https://user-images.githubusercontent.com/58026014/167069048-74b07f21-99a6-437e-a9e4-5712c2b7d0fe.jpg"" height=""150"">
<img src=""https://user-images.githubusercontent.com/58026014/167069059-debe99d4-5e12-440e-94c8-5bccf32c7341.jpg"" height=""150"">
<img src=""https://user-images.githubusercontent.com/58026014/167069763-3da46fd6-a895-4d41-9538-02c7e84adab8.jpg"" height=""150"">
<img src=""https://user-images.githubusercontent.com/58026014/167069774-77e6d77b-54c7-40c5-94dd-c8f9ca24c5e9.jpg"" height=""150""></p>
NOT ONLY THAT PHOTO (SORRY BRUCE)!


2) I detect ""new face"" on the photo (RetinaFace)

3) I do 2 new face embeddings (face and flip face)

4) I try to calculate emb distance with sqeuclidean metric with all persons:
- ""new face"" & each person
- ""new face"" & flip each person
- flip ""new face"" & each person
- flip ""new face"" & flip each person

for example:
1. new face & Bruce1 face
2. new face & flip Bruce1 face
3. flip new face & Bruce1 face
4. flip new face & flip Bruce1 face
5. new face & Bruce2 face
6. new face & flip Bruce2 face
7. flip new face & Bruce2 face
8. flip new face & flip Bruce2 face

(And the same for each photo and person)

5) I get mean(all person distances) and get final distance number (for each person)

6) I labeling ""new face"" with ""nearest person"" by distance number form 5)


What could be the problem with my data? (Oh, questions... Finaly!)

**1) Should I do paddings for face embedding?
2) bad jpg quality original image?
3) Should I do all this embedding manipulations or did I misunderstand something?
4) Should I align face?
5) How head turns affect quality?
6) how lighting and background affect quality?
7) Should I train the model on my data to improve the quality?**

P.S. Thanks in advance! Sorry for my bad English :)"
""
"I can't find any training documentation about CASIA-Webface dataset, how to configure this data for training?"
" If I want to train 1k3d68.onnx  with our own dataset , are there any reference documents?"
""
"Hey,

I have some thousand images that I want to face_align and resize to 112x112, the images are 1024x1024 atm. And I dont understand how I should use the script and reference my images unfortunately, any tips would be super helpful!

I've tried python face_align.py while having the images in the same folder as face_align.py,
python face_align --/home/......./image folder
and so forth"
非常感谢你们的工作，也感谢你们的分享，请问一下ambiguity-aware的数据集什么时候开源吗？ 或者是怎么处理human3.6原数据集的，处理的代码开源吗？或者能说一下过程吗？
"你好，我现在在用scrfd的onnx转tensorrt报了这个错误，貌似是算子不支持动态输入
我的转换命令是./trtexec --onnx=scrfd_10g.onnx --saveEngine=scrfd_10g_fp32.trt  --workspace=4096  --minShapes=input:1x3x640x640  --optShapes=input:8x3x640x640 --maxShapes=input:32x3x640x640 --shapes=input:8x3x640x640，请问这是什么原因导致的？"
"where is mtcnn_detector?  i dont find deploy file?  can you tell me how to reslove the problem ?
insightface/alignment/heatmap$ python test.py 
../deploy
Traceback (most recent call last):
  File ""test.py"", line 12, in <module>
    from mtcnn_detector import MtcnnDetector
ModuleNotFoundError: No module named 'mtcnn_detector'"
"can you explain how to run test.py. 
pretrained weights do not load into the model"
"-------------------------------------------------------------------------------------------
README: https://drive.google.com/file/d/1xTVBwoeNWiPS-KbH_6OFV32zME45_Z6q/view?usp=sharing

[Asian-celeb dataset]

- Training data(Asian-celeb)

The dataset consists of the crawled images of celebrities on the he web.The ima images are covered under a Creative Commons Attribution-NonCommercial 4.0 International license (Please read the license terms here. e. http://creativecommons.org/licenses/by-nc/4.0/).

-------------------------------------------------------------------------------------------
[train_msra.tar.gz]

MD5:c5b668f2204c400099b14f367069aef5

Content: Train dataset called MS-Celeb-1M-v1c with 86,876 ids/3,923,399 aligned images cleaned from MS-Celeb-1M dataset.

This dataset has been excluded from both LFW and Asian-Celeb.

Format: *.jpg

Google: https://drive.google.com/file/d/1aaPdI0PkmQzRbWErazOgYtbLA1mwJIfK/view?usp=sharing
-------------------------------------------------------------------------------------------
[msra_lmk.tar.gz]

MD5:7c053dd0462b4af243bb95b7b31da6e6

Content: A list of five-point landmarks for the 3,923,399 images in MS-Celeb-1M-v1c.

Format:      ..... 

while  is the path of images in tar file train_msceleb.tar.gz.

Label is an integer ranging from 0 to 86,875.

(x,y) is the coordinate of a key point on the aligned images.

left eye
right eye
nose tip
mouth left
mouth right

Google: https://drive.google.com/file/d/1FQ7P4ItyKCneNEvYfJhW2Kff7cOAFpgk/view?usp=sharing
-------------------------------------------------------------------------------------------
[train_celebrity.tar.gz]

MD5:9f2e9858afb6c1032c4f9d7332a92064

Content: Train dataset called Asian-Celeb with 93,979 ids/2,830,146 aligned images.

This dataset has been excluded from both LFW and MS-Celeb-1M-v1c.

Format: *.jpg

Google: https://drive.google.com/file/d/1-p2UKlcX06MhRDJxJukSZKTz986Brk8N/view?usp=sharing
-------------------------------------------------------------------------------------------
[celebrity_lmk.tar.gz]

MD5:9c0260c77c13fbb32692fc06a5dbfaf0

Content: A list of five-point landmarks for the 2,830,146 images in Asian-Celeb.

Format:      ..... 

while  is the path of images in tar file train_celebrity.tar.gz.

Label is an integer ranging from 86,876 to 196,319.

(x,y) is the coordinate of a key point on the aligned images.

left eye
right eye
nose tip
mouth left
mouth right

Google: https://drive.google.com/file/d/1sQVV9epoF_8jS3ge6DqbilpWk3UNE8U7/view?usp=sharing
-------------------------------------------------------------------------------------------
[testdata.tar.gz]

MD5:f17c4712f7562ea6d45f0a158e59b792

Content: Test dataset with 1,862,120 aligned images.

Format: *.jpg

Google: https://drive.google.com/file/d/1ghzuEQqmUFN3nVujfrZfBx_CeGUpWzuw/view?usp=sharing
-------------------------------------------------------------------------------------------
[testdata_lmk.tar]

MD5:7e4995eb9976a2cfd2b23db05d76572c

Content: A list of five-point landmarks for the 1,862,120 images in testdata.tar.gz.

Features should be extracted in the same sequence and with the same amount with this list.

Format:     ..... 

while  is the path of images in tar file testdata.tar.gz.

(x,y) is the coordinate of a key point on the aligned images.

left eye
right eye
nose tip
mouth left
mouth right

Google: https://drive.google.com/file/d/1lYzqnPyHXRVgXJYbEVh6zTXn3Wq4JO-I/view?usp=sharing
-------------------------------------------------------------------------------------------
[feature_tools.tar.gz]

MD5:227b069d7a83aa43b0cb738c2252dbc4

Content: Feature format transform tool and a sample feature file.

Format: We use the same format as Megaface(http://megaface.cs.washington.edu/) except that we merge all files into a single binary file.

Google: https://drive.google.com/file/d/1bjZwOonyZ9KnxecuuTPVdY95mTIXMeuP/view?usp=sharing
-------------------------------------------------------------------------------------------
"
"The SimSwap uses insightface and requires to download antelope.zip, and obviously provides a link to a proper model. Insightface as a default now uses buffalo_l ; I would want to test how it would work with this model. However, I can't find a link to buffalo_l models; could you provide one?"
"As it is designed, learning rate raises up linearly in the warmup stage and decreases exponentially in training stage.
During warmup stage, the loss would first decrease before it increases with the growing of learning rate.
The optimal loss pattern is that it will reach minima just at the time when learning rate finishes warming up.
However, recently I found the loss bouncing back so quick when I transferred the training from a 8gpu machine to a 4gpu machine (the sum cuda memory is approximately same).
Is this unexpected loss pattern related to number of GPUs?
Are there any parameters I could adjust?
![image](https://user-images.githubusercontent.com/62649623/163515103-1fe3bf5b-6571-4953-bf0e-f2bafe1192f6.png)
"
您好，最近使用您工程里的VIT模型代码，训练了一版人脸识别模型，但是在我自己的测试集上，比相同数据训练出来的resnet50模型低了10个点左右。参数配置全部是按照工程里设置的，想问下您在训练时是否使用了预训练模型，或者有什么训练的小技巧？
"Hi @QingpingZheng, seems like requirements.txt  is missing from the repo."
"I am trying to make an inference using the partialfc r100 and r50 models, and I have downloaded them from:
https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc#5-pretrain-models

I see that the models are named `16_**` and `16*`

Is it safe to assume that these are fp16 models?

So, in line:

https://github.com/deepinsight/insightface/blob/17cdeab12a35efcebc2660453a8cbeae96e20950/recognition/arcface_torch/inference.py#L22

should I be setting it as:

```
 net = get_model(name, fp16=True)
```

is this the correct way to set the model for inference on partialfc using the given pretrained model?

thank you for the awesome package.

"
""
"Hello everyvone! I do not know what to do( I ran ""pip install -U insightface"" and got an error:

Collecting insightface
  Using cached insightface-0.6.2.tar.gz (433 kB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: numpy in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (1.21.5)
Requirement already satisfied: onnx in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (1.11.0)
Requirement already satisfied: tqdm in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (4.64.0)
Requirement already satisfied: requests in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (2.27.1)
Requirement already satisfied: matplotlib in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (3.5.1)
Requirement already satisfied: Pillow in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (9.1.0)
Requirement already satisfied: scipy in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (1.7.3)
Requirement already satisfied: scikit-learn in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (1.0.2)
Requirement already satisfied: scikit-image in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (0.19.2)
Requirement already satisfied: easydict in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (1.9)
Requirement already satisfied: cython in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (0.29.28)
Requirement already satisfied: albumentations in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (1.1.0)
Requirement already satisfied: prettytable in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from insightface) (3.2.0)
Requirement already satisfied: opencv-python-headless>=4.1.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from albumentations->insightface) (4.5.5.64)
Requirement already satisfied: PyYAML in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from albumentations->insightface) (6.0)
Requirement already satisfied: qudida>=0.0.4 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from albumentations->insightface) (0.0.4)
Requirement already satisfied: networkx>=2.2 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-image->insightface) (2.6.3)
Requirement already satisfied: imageio>=2.4.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-image->insightface) (2.16.2)
Requirement already satisfied: tifffile>=2019.7.26 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-image->insightface) (2021.11.2)
Requirement already satisfied: packaging>=20.0 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-image->insightface) (21.3)
Requirement already satisfied: PyWavelets>=1.1.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-image->insightface) (1.3.0)
Requirement already satisfied: cycler>=0.10 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from matplotlib->insightface) (0.11.0)
Requirement already satisfied: python-dateutil>=2.7 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from matplotlib->insightface) (2.8.2)
Requirement already satisfied: kiwisolver>=1.0.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from matplotlib->insightface) (1.4.2)
Requirement already satisfied: pyparsing>=2.2.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from matplotlib->insightface) (3.0.8)
Requirement already satisfied: fonttools>=4.22.0 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from matplotlib->insightface) (4.32.0)
Requirement already satisfied: protobuf>=3.12.2 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from onnx->insightface) (3.20.0)
Requirement already satisfied: typing-extensions>=3.6.2.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from onnx->insightface) (4.1.1)
Requirement already satisfied: wcwidth in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from prettytable->insightface) (0.2.5)
Requirement already satisfied: importlib-metadata in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from prettytable->insightface) (4.11.3)
Requirement already satisfied: idna<4,>=2.5 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from requests->insightface) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from requests->insightface) (2021.10.8)
Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from requests->insightface) (2.0.12)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from requests->insightface) (1.26.9)
Requirement already satisfied: joblib>=0.11 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-learn->insightface) (1.1.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from scikit-learn->insightface) (3.1.0)
Requirement already satisfied: six>=1.5 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->insightface) (1.16.0)
Requirement already satisfied: zipp>=0.5 in /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages (from importlib-metadata->prettytable->insightface) (3.8.0)
Building wheels for collected packages: insightface
  Building wheel for insightface (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [109 lines of output]
      WARNING: pandoc not enabled
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.9-x86_64-cpython-37
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface
      copying insightface/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/mask_renderer.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/common.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/face_analysis.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/download.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/filesystem.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/transform.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/storage.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/face_align.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/constant.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/landmark.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/arcface_onnx.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/retinaface.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/attribute.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/model_zoo.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/scrfd.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/model_store.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/insightface_cli.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/rec_add_mask_param.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/model_download.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/rec_builder.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/pickle_object.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/image.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty
      copying insightface/thirdparty/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d
      copying insightface/thirdparty/face3d/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/light.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/render.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/io.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/vis.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/transform.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/light.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/render.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/io.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/vis.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/transform.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/morphabel_model.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/fit.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/load.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      running egg_info
      writing insightface.egg-info/PKG-INFO
      writing dependency_links to insightface.egg-info/dependency_links.txt
      writing entry points to insightface.egg-info/entry_points.txt
      writing requirements to insightface.egg-info/requires.txt
      writing top-level names to insightface.egg-info/top_level.txt
      reading manifest file 'insightface.egg-info/SOURCES.txt'
      writing manifest file 'insightface.egg-info/SOURCES.txt'
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/Tom_Hanks_54745.png -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_black.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_blue.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_green.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_white.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/t1.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/objects
      copying insightface/data/objects/meanshape_68.pkl -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/objects
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core.cpp -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core.h -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.c -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.pyx -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/setup.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      running build_ext
      building 'insightface.thirdparty.face3d.mesh.cython.mesh_core_cython' extension
      creating build/temp.macosx-10.9-x86_64-cpython-37
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -Iinsightface/thirdparty/face3d/mesh/cython -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/include -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -c insightface/thirdparty/face3d/mesh/cython/mesh_core.cpp -o build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core.o
      gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -Iinsightface/thirdparty/face3d/mesh/cython -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/include -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -c insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp -o build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.o
      In file included from insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp:721:
      In file included from /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:
      In file included from /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:
      In file included from /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:
      /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: ""Using deprecated NumPy API, disable it with ""          ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-W#warnings]
      #warning ""Using deprecated NumPy API, disable it with "" \
       ^
      1 warning generated.
      g++ -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core.o build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.o -o build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpython-37m-darwin.so
      ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/lib/libc++.tbd' for architecture x86_64
      clang: error: linker command failed with exit code 1 (use -v to see invocation)
      error: command '/usr/bin/g++' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for insightface
  Running setup.py clean for insightface
Failed to build insightface
Installing collected packages: insightface
  Running setup.py install for insightface ... error
  error: subprocess-exited-with-error
  
  × Running setup.py install for insightface did not run successfully.
  │ exit code: 1
  ╰─> [111 lines of output]
      WARNING: pandoc not enabled
      running install
      /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.
        setuptools.SetuptoolsDeprecationWarning,
      running build
      running build_py
      creating build
      creating build/lib.macosx-10.9-x86_64-cpython-37
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface
      copying insightface/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/mask_renderer.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/common.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      copying insightface/app/face_analysis.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/app
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/download.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/filesystem.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/transform.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/storage.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/face_align.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      copying insightface/utils/constant.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/utils
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/landmark.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/arcface_onnx.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/retinaface.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/attribute.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/model_zoo.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/scrfd.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      copying insightface/model_zoo/model_store.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/model_zoo
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/insightface_cli.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/rec_add_mask_param.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      copying insightface/commands/model_download.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/commands
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/rec_builder.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/pickle_object.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      copying insightface/data/image.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty
      copying insightface/thirdparty/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d
      copying insightface/thirdparty/face3d/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/light.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/render.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/io.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/vis.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      copying insightface/thirdparty/face3d/mesh/transform.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/light.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/render.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/io.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/vis.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      copying insightface/thirdparty/face3d/mesh_numpy/transform.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh_numpy
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/morphabel_model.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/fit.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      copying insightface/thirdparty/face3d/morphable_model/load.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/morphable_model
      running egg_info
      writing insightface.egg-info/PKG-INFO
      writing dependency_links to insightface.egg-info/dependency_links.txt
      writing entry points to insightface.egg-info/entry_points.txt
      writing requirements to insightface.egg-info/requires.txt
      writing top-level names to insightface.egg-info/top_level.txt
      reading manifest file 'insightface.egg-info/SOURCES.txt'
      writing manifest file 'insightface.egg-info/SOURCES.txt'
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/Tom_Hanks_54745.png -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_black.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_blue.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_green.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/mask_white.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      copying insightface/data/images/t1.jpg -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/images
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/objects
      copying insightface/data/objects/meanshape_68.pkl -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/data/objects
      creating build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core.cpp -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core.h -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.c -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.pyx -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      copying insightface/thirdparty/face3d/mesh/cython/setup.py -> build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      running build_ext
      building 'insightface.thirdparty.face3d.mesh.cython.mesh_core_cython' extension
      creating build/temp.macosx-10.9-x86_64-cpython-37
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh
      creating build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython
      gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -Iinsightface/thirdparty/face3d/mesh/cython -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/include -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -c insightface/thirdparty/face3d/mesh/cython/mesh_core.cpp -o build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core.o
      gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch x86_64 -g -Iinsightface/thirdparty/face3d/mesh/cython -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include -I/Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/include -I/Library/Frameworks/Python.framework/Versions/3.7/include/python3.7m -c insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp -o build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.o
      In file included from insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp:721:
      In file included from /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:
      In file included from /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:
      In file included from /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:
      /Users/karimkabirov/.local/share/virtualenvs/insightface-RgMakY-1/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: ""Using deprecated NumPy API, disable it with ""          ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-W#warnings]
      #warning ""Using deprecated NumPy API, disable it with "" \
       ^
      1 warning generated.
      g++ -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core.o build/temp.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.o -o build/lib.macosx-10.9-x86_64-cpython-37/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpython-37m-darwin.so
      ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/lib/libc++.tbd' for architecture x86_64
      clang: error: linker command failed with exit code 1 (use -v to see invocation)
      error: command '/usr/bin/g++' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─> insightface

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.


Please help me!"
how cloud i test in my own data only single image input
"Hi,

I am trying old mxnet based  ga model . What I saw mxnet consumes almost 4 Gig even context is gpu ! Have you ever came cross like this ? 

Is there any export for ageGender model of the old mxnet one ?  Or is there any tested better accuracy model than this?

Best

@nttstar "
""
""
""
"I encountered the following problem when retraining the data, how should I solve it?
![image](https://user-images.githubusercontent.com/45253712/161754883-3a18e6fd-bdda-4286-9e1d-e63304e71563.png)
"
"Hello! I want to recognize it in real time through webcam. What should I do?
Do I need to change the argument?
Thank you in advance."
Will checkpoints for ViT trained on WF42M be available?
to new version.
"hello, newbie here trying to install insightface locally. 

I keep getting ""python setup.py bdist_wheel did not run successfully.""

I am using: Windows 10
Python: 3.10.4

If you have seen this issue before, could someone pls advise? Thanks!"
"@nttstar Hi, I see that you have done the work of eys mesh prediction and gaze estimation before. Will this part of content be open source in the future ?"
"Hi, I have converted SCFRD_10g.pt to onnx and then to TensorRT. When I run it and check the binding shapes, I more binding outputs than input/output.
```
for binding in fd_engine:
	print(fd_engine.get_binding_shape(binding))
	if fd_engine.binding_is_input(binding):
		print('input')
		fd_device_input = cuda.mem_alloc(trt.volume(fd_engine.get_binding_shape(binding)) * fd_engine.max_batch_size * np.dtype(np.float32).itemsize)
	else:
		print('not input')
		fd_host_output = cuda.pagelocked_empty(trt.volume(fd_engine.get_binding_shape(binding)) * fd_engine.max_batch_size, dtype=np.float32)
		fd_device_output = cuda.mem_alloc(fd_host_output.nbytes)
```
The output is as follows:
![image](https://user-images.githubusercontent.com/48631399/160373793-04bb57d3-63fd-4502-8717-7dbf8a24f45f.png)
The input size is correct. As for the rest, I expected to have one output, but received many. When I tested conversion (I am using TensorRT 8.0.1) with Yolov5sface, I obtained:
![image](https://user-images.githubusercontent.com/48631399/160374102-ef11e245-7ec3-4388-9eef-a80bf0807575.png)
May I check why this is so?

Referring to the link here (https://github.com/SthPhoenix/InsightFace-REST/issues/37), it says that it is anchor center and bbox prediction. Is this correct? And why are there three different values?"
"Hi,

@nttstar @yingfeng
Is there any r101 model or training example ?

Best"
"Hi, I am using an Ubuntu 18.04. I have installed mmc 1.3.3, then run pip install -r requirements/build.txt, then pip install -v -e ., and installed mmdet 2.7.0

When I run the following command:
`python3 tools/scrfd2onnx.py config configs/scrfd/scrfd_2.5g.py checkpoint pytorch\ weights/scfrd_2-5g.pth --input-img 0.jpg --shape 640`
to convert scfrd to onnx, I get the following error:
```
Traceback (most recent call last):
  File ""tools/scrfd2onnx.py"", line 10, in <module>
    from mmdet.core import (build_model_from_cfg, generate_inputs_and_wrap_model,
  File ""/home/va/Desktop/scrfd/mmdet/core/__init__.py"", line 2, in <module>
    from .bbox import *  # noqa: F401, F403
  File ""/home/va/Desktop/scrfd/mmdet/core/bbox/__init__.py"", line 7, in <module>
    from .samplers import (BaseSampler, CombinedSampler,
  File ""/home/va/Desktop/scrfd/mmdet/core/bbox/samplers/__init__.py"", line 9, in <module>
    from .score_hlr_sampler import ScoreHLRSampler
  File ""/home/va/Desktop/scrfd/mmdet/core/bbox/samplers/score_hlr_sampler.py"", line 2, in <module>
    from mmcv.ops import nms_match
  File ""/home/va/Desktop/scrfd/venv/lib/python3.6/site-packages/mmcv/ops/__init__.py"", line 15, in <module>
    from .fused_bias_leakyrelu import FusedBiasLeakyReLU, fused_bias_leakyrelu
  File ""/home/va/Desktop/scrfd/venv/lib/python3.6/site-packages/mmcv/ops/fused_bias_leakyrelu.py"", line 10, in <module>
    ext_module = ext_loader.load_ext('_ext', ['fused_bias_leakyrelu'])
  File ""/home/va/Desktop/scrfd/venv/lib/python3.6/site-packages/mmcv/utils/ext_loader.py"", line 13, in load_ext
    assert hasattr(ext, fun), f'{fun} miss in module {name}'
AssertionError: fused_bias_leakyrelu miss in module _ext
```

Could someone advise?"
"May be the loss.py only cover loss functions which revise the positive example's logits(such as arcface and cosface). If I want revise the postive examle's logits and negative example's logits(such as curriculum face), how? Thank you  "
"- I trained my arcface model using arcface mxnet with fp16 precision. Training code:
`CUDA_VISIBLE_DEVICES='0,1,2,3,4,5' python -u train_parall.py --network r100 --loss arcface --dataset webface --fp16-scale 1.0`

- After finish training, I tried to convert it to onnx format using mxnet's onnx converter. Code:
```
import onnx
import mxnet as mx
from onnx import helper
from mxnet import onnx as onnx_mxnet

sym, arg_params, aux_params = mx.model.load_checkpoint('./r100-arcface-webface/model', 20)
input_shape = (1,) + tuple([int(x) for x in '3,112,112'.split(',')] )
all_args = {}
all_args.update(arg_params)
all_args.update(aux_params)
onnx_mxnet.export_model(sym, all_args, [input_shape], np.float32, './r100_arcface.onnx')
```

- It returns this error:
![image](https://user-images.githubusercontent.com/40227850/159461353-0a282634-f2f1-4159-a2c9-37139d95aa51.png)

- Can you help me ? Thanks for your help!

- Dependencies:
```
mxnet==1.9.0
onnx==1.8.0
```

"
"I make a inference about ""Glint_r50.pth"" and ""ms1mv3_r50.pth"", but the former seems to be much slower than the latter. why?"
"When the insigtface model is asked for different images simultaneously, the bounding box information in the images is also drawn in the wrong images.

Does the model support concurrent queries? Can you help with this?"
It's difficult for me to download Asian-celeb dataset from baidu. Would you mind uploading Asian-celeb dataset to google drive ? I think there are also many people who need it.
"Hi, I'm currently reading the source code and try to understand PartialFC.
The indexing part and how it samples the ids that would be used in an iteration is understandable. But I still couldn't wrap my head around the `searchsorted` part? Why would we need to change the label?
Thank you! "
Could you please explain how you cropped loose_crop IJBA ... the size are not aligned with IJB-C metadata
which repository is used to get onnx models for the recognition model? arcface-torch or arcface-mxnet
"* I downloaded **MS1M_v3: MS1M-RetinaFace** dataset from https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_  gdrive. 

* Extract MXNet Dataset to images
   `python tools/mx_recordio_2_images.py --root_dir ms1m-retinaface-t1/ --output_dir MS1M_v3/`

* After finishing unzipping the dataset, the folder structure is as follows.
  ```
   MS1M_v3

    |_ images

    |  |_ 00000001.jpg

    |  |_ ...

    |  |_ 05179510.jpg

    |_ label.txt

    |_ agedb_30.bin

    |_ cfp_ff.bin

    |_ cfp_fp.bin

    |_ lfw.bin
```

* For Single Node, Single GPU Training, I ran the below script.

      ```
     python tools/train.py \

    --config_file configs/ms1mv2_mobileface.py \

    --embedding_size 128 \

    --sample_ratio 1.0 \

    --loss ArcFace \

    --batch_size 512 \

    --dataset MS1M_v2 \

    --num_classes 85742 \

    --data_dir MS1M_v2/ \

    --label_file MS1M_v2/label.txt \

    --fp16 False
```
    
    The above training script resulted in a TypeError, How can I fix this pls.



```
Exception in thread Thread-4:
Traceback (most recent call last):
  File ""c:\users\glodaris\appdata\local\programs\python\python38\lib\threading.py"", line 932, in _bootstrap_inner
    self.run()
  File ""c:\users\glodaris\appdata\local\programs\python\python38\lib\threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\Glodaris\OneDrive\Desktop\Glodaris\NIS\insight\insightface\lib\site-packages\paddle\fluid\dataloader\dataloader_iter.py"", line 212, in _thread_loop
    batch = self._dataset_fetcher.fetch(indices,
  File ""C:\Users\Glodaris\OneDrive\Desktop\Glodaris\NIS\insight\insightface\lib\site-packages\paddle\fluid\dataloader\fetcher.py"", line 121, in fetch
    data.append(self.dataset[idx])
  File ""C:\Users\Glodaris\OneDrive\Desktop\Glodaris\NIS\insight\Projects\insightface\recognition\arcface_paddle\datasets\common_dataset.py"", line 69, in __getitem__
    img = transform(img)
  File ""C:\Users\Glodaris\OneDrive\Desktop\Glodaris\NIS\insight\Projects\insightface\recognition\arcface_paddle\datasets\common_dataset.py"", line 35, in transform
    img = (img - 127.5) * 0.00784313725
TypeError: unsupported operand type(s) for -: 'NoneType' and 'float'
```
    
    "
"I don't understand the column headings in iJBC/IJBC meta folder. what is ""sightings_id"",? and how is the ijbc_1N_probe_mixed.csv created? Also, are ijbc_1N_gallery_g1.csv and ijbc_1N_gallery_g2.csv the same as in IJBC folder"
""
"I wonder why the backbone was trained with only one dataset every time. Why can't the backbone be trained with multiple dataset. As you upload ""ResNet50@WebFace600K""

Looking forward to your reply
"
"Dear friends
I've installed insightface 0.2.0 and then try to run the program. The error is as follows

 ModuleNotFoundError: No module named 'insightface.data'

Please suggest me a solution in this regard"
"![image](https://user-images.githubusercontent.com/59432975/155885339-d5b22014-5920-40a8-ae2b-7cdc4a7d4c87.png)

Have you modified the official eval_tools code?"
Hi! Thank you very much for your work! would you mind uploading the pretrained arcface model r2060 on google drive instead of Baidu Yun?
"Hi! Can you add the RetinaFace model (e.g., R50) to the insightface python package so it can be directly called just like the SCRFD? Thanks!"
"Hello,

Can you please provide the pretrained ArcFace model on WebFace42M dataset?

WebFace42M-PartialFC-0.2 | r50(bs4k) | 93.83 | 97.53 | 96.16 | (8 GPUs)~5900
WebFace42M-PartialFC-0.2 | r50(bs8k) | 93.96 | 97.46 | 96.12 | (16GPUs)~11000
WebFace42M-PartialFC-0.2 | r50(bs4k) | 94.04 | 97.48 | 95.94 | (32GPUs)~17000
WebFace42M-PartialFC-0.2 | r100(bs4k) | 96.69 | 97.85 | 96.63 | (16GPUs)~5200

The pretrained models for the above configurations are not provided in the oneDrive."
"I downloaded the Glint360k dataset via Baidu cloud, and the link was written on the Readme file. After the downloading finished, I tried to decompress the file using the `cat glint360k_* | tar -xzvf -` command. 

The decompression process was failed and reported an error like the below:
```
cat glint360k_* | tar -xzvf -
glint360k/
glint360k/lfw.bin
glint360k/cfp_fp.bin
glint360k/cfp_ff.bin
glint360k/train.rec

gzip: stdin: invalid compressed data--format violated
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now

```
 I can ensure that I haven't done anything to corrupt the file. Please tell me what's going on. Anything helpful will be appreciated.
"
"Supported methods:
which method was applied in face alignment step？
It is shown there are two methods in the module of home page.
 1) SDUNets
 2) SimpleRegression"
"i have downloaded antelope which is the pretrain model right?
i get this issue whenever trying to call the detection or recognition model in the quick example

Traceback (most recent call last):
  File ""D:/insightface/python-package/main.py"", line 8, in <module>
    detector = insightface.model_zoo.get_model('glintr100.onnx')
  File ""D:\insightface\python-package\insightface\model_zoo\model_zoo.py"", line 80, in get_model
    assert osp.exists(model_file), 'model_file should exist'
AssertionError: model_file should exist 


so am wondering what is the correct way of setting this model?"
""
"<img width=""1440"" alt=""Screenshot 2022-02-09 at 3 55 39 PM"" src=""https://user-images.githubusercontent.com/37455387/153179650-441b319c-b1d2-4bd6-a0b3-d3248215ffa3.png"">



Running the command ` pip install -U insightface`"
"is it same model insightface (Quickstart) and retinaface?

if not it is tell me what is the model in insightface

what model is better?__?

help me"
"I am working with CFP-FP dataset. When I use data that is provided by Insight-face I get above the reported accuracy. However, When I align CFP myself with mtcnn, the performance drops. Could you please provide information about the preprocessing? "
"Thanks for the great work!
I have several photos collected from social networks. I'm trying to get embeddings from this images in Colab (onnxruntime-gpu==1.8).
After a few iterations inference slows down (from 13 it/s to 1.5 it/s)

Code:
```
app = FaceAnalysis(det_name='retinaface_mnet025_v2', rec_name='arcface_r100_v1', ga_name='genderage_v1')
app.prepare(ctx_id=0, det_size=(640, 640))

def main(db, app):
    d = 512
    number_of_faces = 56233
    xb = np.ndarray(shape=(number_of_faces, d)).astype('float32')
    a = []
    for photo_path in tqdm.tqdm(db):
        embeddings = app.get(get_image(os.path.join(""my/path"", photo_path)))
        for e in embeddings:
            a.append([photo_path[0:-5], e.embedding.astype('float32')])
            if len(a) and len(a) % 1000 == 0:
                db_np = np.array(a)
                print(""---"", sys.getsizeof(a), ""---"")
                np.save(""/embeddings/path/"" + str(len(a)), db_np)
    return a

a = main(os.listdir(""my/path""), app)

```

![image](https://user-images.githubusercontent.com/45150310/152003406-c1758def-5106-4dd4-a9d9-ac4fb01bfd8f.png)

I also have noticed that GPU memory is occupied even after the main code finishes and I can't execute the cell again due to the memory allocation error.
Could you please help me to get over this trouble.
P.S. I have changed get_image so that not to cache images.
"
"Thanks for holding the challenge. 
I've send several emails about some concerns on the challenge (to insightface.challenge@gmail.com), but no response is received after 1 week. So I'm pasting my questions here, hoping to get some feedback.
1.	Upon registration, do we need to sign any form of participation agreement (like NIST FRVT)?  
2.	How will you handle submitted models for avoiding risks (in terms of security or interest conflict)?             
3.	Based on the results of this competition, can we publish press release? If yes, are there any conditions or requirements?

Appreciate your prompt reply!
"
"I would like to test scrfd34G Model.
Where can I get the onnx file?

thank you..."
"I'm really curious.
how measure two embedding distance 
"
"I think http://storage.insightface.ai is down, are there any alternative links for the model from which I can download the model and set it in  `/root/.insightface/models/` manually? 

Code:
```
from insightface.app import FaceAnalysis
app = FaceAnalysis()
```

Output:
```
download_path: /root/.insightface/models/buffalo_l
Downloading /root/.insightface/models/buffalo_l.zip from http://storage.insightface.ai/files/models/buffalo_l.zip...
---------------------------------------------------------------------------
TimeoutError                              Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/urllib3/connection.py in _new_conn(self)
    158             conn = connection.create_connection(
--> 159                 (self._dns_host, self.port), self.timeout, **extra_kw)
    160 

23 frames
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fc7b71ce910>: Failed to establish a new connection: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
MaxRetryError: HTTPConnectionPool(host='storage.insightface.ai', port=80): Max retries exceeded with url: /files/models/buffalo_l.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc7b71ce910>: Failed to establish a new connection: [Errno 110] Connection timed out'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPConnectionPool(host='storage.insightface.ai', port=80): Max retries exceeded with url: /files/models/buffalo_l.zip (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc7b71ce910>: Failed to establish a new connection: [Errno 110] Connection timed out'))
```"
"Retina face is detecting non human faces like animal, cartoon etc. Is there anyway to accept only real human faces "
"I did a test with the LFW dataset，however pretrained-MS1MV3-Resnet50 which the accuracy is only 88.9.
The same test showed different results on other data, with an accuracy rate of at least 99.8.
I don't know why, hope someone will test it to see if it has the same problem

"
"1、when i use 3d datasets to train a 68 points alignement model, there is no problem.
2、but, when i use 2d datasets to train a 106 points alignement model, there is a problem
![1643007313(1)](https://user-images.githubusercontent.com/25764883/150735964-300ba351-f2ce-4915-9d09-2ef8c335b2ae.jpg)
I think it is related to the datasets.
So, my question is :
1、what dataset should I use when i train a 106 points alignement model ？
2、What else should i pay attention to when train a 106 points alignement model ？"
"Traceback (most recent call last):
  File ""train.py"", line 151, in <module>
    main(parser.parse_args())
  File ""train.py"", line 108, in main
    loss: torch.Tensor = module_partial_fc(local_embeddings, local_labels, opt)
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/data/AI-GPU/chenguote/face_recognition/insightface/recognition/arcface_torch/partial_fc.py"", line 243, in forward
    distributed.all_gather(_gather_labels, local_labels)
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py"", line 1863, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: All tensor operands to scatter/gather must have the same number of elements
Killing subprocess 2701979
Killing subprocess 2701980
Killing subprocess 2701982
Killing subprocess 2701983
Killing subprocess 2701984
Killing subprocess 2701987
Killing subprocess 2701990
Killing subprocess 2701993
Traceback (most recent call last):
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/site-packages/torch/distributed/launch.py"", line 340, in <module>
    main()
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/site-packages/torch/distributed/launch.py"", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File ""/train_tmp/miniconda3/envs/dali/lib/python3.7/site-packages/torch/distributed/launch.py"", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/train_tmp/miniconda3/envs/dali/bin/python', '-u', 'train.py', '--local_rank=7', 'configs/config.py']' returned non-zero exit status 1.


config.dali set False is ok and got 4400 samples/sec
"
"My work needs to crop the face chip from my own img dataset. I just need to crop and align the face from the image, no matter who is he/she/it. 

I have use dlib library before but it cannot align the face cropped(may be I didnt  find it). And I find the example img in this project, the people in ""Friends"" img are all cropped and aligned. How to detect and crop the face with insightface please? Is the .py is what I need https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/inference.py?  How to use please? Thanks."
已经通过修改训练代码解决
How do I use my own image for face recognition?
"Excuse me, Is the feat obtained by inputting a picture in inference.py is the feature of the extracted face picture?"
"Hi, i'm currently having an attendance project and working with mobilefacenet architecture. I used faces_emore dataset. Although the accuracy for eval set is pretty high (98~99%) but still my model is having trouble recognize people and often make wrong decision. After testing with multiple cases, I have draw a conclusion that It's not good enough for Asian people. What shoud I do to improve it? Is there anyway? Thanks :D"
"import os, sys, datetime
import numpy as np
import os.path as osp
import cv2
import insightface
from insightface.app import MaskRenderer
from insightface.data import get_image as ins_get_image

if __name__ == ""__main__"":
    #make sure that you have download correct insightface model pack.
    #make sure that BFM.mat and BFM_UV.mat have been generated
    tool = MaskRenderer()
    tool.prepare(ctx_id=0, det_size=(128,128))
    image = ins_get_image('Tom_Hanks_54745')
    mask_image  = ""mask_black""
    params = tool.build_params(image)
    mask_out = tool.render_mask(image, mask_image, params)

    cv2.imwrite('output_mask.jpg', mask_out)

![output_mask](https://user-images.githubusercontent.com/49524647/148673773-f891e8a7-0764-4deb-a9cf-11c8ba5b65a5.jpg)
@nttstar
"
"We are interested in the model, and would like to know the reference paper for the model."
"Hi all,
I read this benchmark on the model_zoo page; I'm curious about the benchmark in South Asian and East Asian. How to do these benchmarks? Which is the dataset?
https://github.com/deepinsight/insightface/tree/master/model_zoo"
When will you release the pre-trained model 2d106track?
"Do you thing the angle of the face should be label face and landmark ?
![image](https://user-images.githubusercontent.com/59388989/147434988-10a6d840-2300-4f6b-8e59-e6784c2e82e2.png)
"
"I saw the code in face_align.py:
      arcface_src = np.array(
        [[38.2946, 51.6963], [73.5318, 51.5014], [56.0252, 71.7366],
        [41.5493, 92.3655], [70.7299, 92.2041]],
     dtype=np.float32)

but what's the specific method for acquiring those values ?"
论文中将原图缩放至640*640进行evaluation生成prediction文件，官方提供的matlab代码是对原图进行评估绘制出来的PR曲线，请问你对matlab源码进行了修改吗？
Has anyone trained this dataset with R18 by pytorch ?I wonder what the result is.I need a pre-training model of the R18.Thanks.
"This library support video realtime and how do it? 
please guide me, Thank you."
What does the 2060 number refer to in iResNet2060?
"```
# our RAM is 256G 
mount -t tmpfs -o size=140G  tmpfs /train_tmp
```
How to find my computer size
htop ? Mem?"
what is the difference between using ur datasets or using my own dataset in  creating model can anyone answered me
"If I understand correctly you base SCRFD on MMDet?
It would be amazing if you could contribute your SCRFD code to MMDet, so that they can maintain it, and it's up to date with changes in MMDet. "
"@yingfeng @nttstar @nihui @ppwwyyxx @leondgars 你们好，就是我在使用archface_torch工程训练mobilefacenet网络loss一直无法收敛，想问下是不是我参数设置有问题，期待你们的回答。
![image](https://user-images.githubusercontent.com/58413357/144156176-b4e8b1ab-5041-4a23-8e43-ca0febc310e6.png)
![image](https://user-images.githubusercontent.com/58413357/144156500-d7f3e063-2b7b-477d-9bca-cf0f5eeb5374.png)

"
"Hello, I am kinda a noob, so I'm sorry for asking such a forward question. But how can I use my own model to detect faces on a picture?

I have this code snippet:

```python
import cv2
import numpy as np
import insightface
from insightface.app import FaceAnalysis
from insightface.data import get_image as ins_get_image

detector = insightface.model_zoo.get_model('retinaface_r50_v1.onnx')
detector.prepare(ctx_id=0, det_size=(640, 640))
```

But I don't know how to go on. Can somebody help me please?"
"such as Color jitter,   random crop"
"Hi.

Thanks for this awesome repository and all your work involved in it.

I was wondering if it is possible to fine-tune your models using four channel images. In my particular case, I've already manipulated the data in order to have four channels instead of the classic three. Also, I've modified the image_iter.py so the ""return io.DataBatch(...)"" in the next(self) function, returns a batch_data of shape (batch_size, 4, 112, 112) instead of (batch_size, 3, 112, 112).

I've succesfully performed a lot of fine-tune with three channels data and your models previously, but when I try to execute this ""four channels image"" fine-tune I get the following error message: 

MXNetError: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node  at 0-th output: expected [32,4,112,112], got [32,3,112,112]. 

Also, if I modify the ""self.provide_data"" to have data_shape (4,112,112) instead of (3,112,112), I recieve the following error message:

MXNetError: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node  at 0-th output: expected [64,3,3,3], got [64,4,3,3]. 

So I was wondering if it is possible to fine-tune your models with four channel data or it's just impossible and I have to train a model from scratch.

Finally, I was able to performe four channel data training from scratch (no pretrained model) with this modifications with no problem, but another issue appears. After verbose batches (i.e. evaluation time) the training is interrumpted and the following message shows up:

MXNetError: Shape of unspecifie arg: conv0_weight changed. This can cause the new executor to not share parameters with the old one. Please check for error in network.If this is intended, set partial_shaping=True to suppress this warning.

I'm presuming this is because my .bin files for evaluations where created from three channels images. Should I add a fourth channel to them also? Or am I missing something and it's possible to train with four channel data and validate on three channel data?

I know my questions are extensive and might be confusing, so let me know if there is any specific information I could provide to help me solve them. 
Thanks in advance."
"@jiankangdeng 
Does VPL training give improvements over Arcface on large training sets?

When we train with 3 Million identities, we observed that only 4% of identities are VPL trained (4% identities have lambda=0.15, rest 96% identities have lambda=0) at allowed_delta=200.  In other words, only 4% identities use features saved from past to compute variational prototype , for the rest class centre is used as it is.

Is VPL effective when such low percentage of identities have valid features in queue?
How do we increase this percentage?

Observation:
We experimented with the effect of allowed_delta with on MS1MV3 with 128x4 batch size without momentum.
at allowed_delta=100, around 25% of identities are VPL trained, for the rest lambda=0.
at allowed_delta=200, around 50% of identities are VPL trained
We observed slightly better accuracies at allowed_delta=100. 

vpl code from insightface repo uses a concept called VPL momentum. What is its effect? How to use VPL momentum effectively? Batch size reduces by 50% when VPL momentum is used.

"
如题。
"it was convenience that paddle frame can training by one gpu, but saving model is not good."
"Hi there,

I would like to evaluate mobilefacenet (pytorch version) as a part of my thesis, and although I can see the model file:
https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/backbones/mobilefacenet.py

the modelzoo does not seem to have the weights to attach it to, so that I can perform inference.

or am I missing something? The only relevant thing I can see are the MXNET weights, but no pytorch compatible wieghts.

thank you for the repo!

"
"I noticed that embeddings for the same face photo are totally different if you use default face detection and face recognition models from Insightface 0.1.5 and Insightface 0.5.

Is it a desirable and expected behaviour or am I doing something wrong? 

If the embeddings are different, is there a formula to calculate old embeddings to the new ones?"
"python tools/export.py --is_static False --export_type paddle --backbone MobileFaceNet --checkpoint_dir /face/test_data/paddle-mslm/ori/MobileFaceNet/29 --output_dir /face/test_data/paddle-mslm/model/


File ""/home/work-test/2021/paddle/paddle_insightface/lib/python3.6/site-packages/paddle/fluid/dygraph/container.py"", line 98, in forward
        def forward(self, input):
            for layer in self._sub_layers.values():
                input = layer(input)
                ~~~~~~~~~~~~~~~~~~~~ <--- HERE
            return input

    UnboundLocalError: local variable 'layer' referenced before assignment"
"非常感谢各位大佬的贡献！
我有一个小问题，insightface可以用来做换别的数据来做识别吗？比如说掌纹，指纹等其他数据。
谢谢各位。"
"Hoe can I free the memory from a model when I no longer need it on runtime?

The only way I see momery is being freed is when the program completly stops. I tried using del but nothing, the memory is still being used, how can I free it?"
测试scrfd的demo，发现scrfd的检测的人脸置信度低，比retinaface的置信度低，请问这是正常的情况吗
"I installed insightface on a virtual environment for python (virtualenv) but when I try to get the model I get this

>>> model = insightface.model_zoo.get_model('retinaface_r50_v1')
>>> model.prepare(ctx_id = 0, nms=0.4)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'NoneType' object has no attribute 'prepare'

If I try to do the same ouside an environment, just run the basic model, works just fine.

Anyone knows how to make it work inside a virtual environment?"
"执行以下命令
python tools/test_recognition.py --det --rec  --index=index.bin --input=tmp.png --output=""./output""

我想达到效果：没有在索引库中的人被识别出人脸并锚框并提示未知，在索引库中的人被识别出人脸并锚框并提示人名，请问这个能做到吗，如何做呢"
"使用opencv的dnn模块部署SCRFD人脸检测，包含C++和Python两种版本的程序
https://github.com/hpc203/scrfd-opencv"
"when I run detection model, only cpu is used.
my package version：
insightface==0.5
onnx==1.10.2
onnxruntime-gpu==1.9.0

even if I uninstalled insightface and installed it again."
"Hi!
I planed to train SCRFD with new dataset  based the pretrained weights, 
but I didn't find the code to specified the weights file in the train.py and config files .

Does it support traing based the pretrained weights ? 

Thanks a lot!
qc."
"Still dog or other animal faces are detected in scrfd face detecto
![face_13](https://user-images.githubusercontent.com/3781062/140783193-3b471f26-9723-4722-9943-88239fdd83a7.jpg)
r."
"Because of the system is Windows10. 
When I changed : ""dist.init_process_group('nccl')"" into ""dist.init_process_group('gloo')""

execute the train.py under the arcface_torch dictionary.
Got this Error:
**""RuntimeError: ProcessGroupGloo does not support reduce_scatter""**

I haven't seen the other scatter support gloo, Which scatter should I use?"
"`DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, random_state=None,`"
"arcface_torch resume fail:

  I used 4 GPUs for training for a period of time, and then trained on 3 GPUs, only modified the value of embedding_size，both use tehe partial_fc method.

  **but the resume failure error occurred during training**"
"Training: 2021-11-02 19:58:58,427 - Load checkpoint from '/home/bbs/Datasets/kjj/insightface/recognition/arcface_paddle/MS1M_v2_arcface_MobileFaceNet_128_0.1/MobileFaceNet_128/120'.
Traceback (most recent call last):
  File ""tools/train.py"", line 35, in <module>
    train(args)
  File ""/home/bbs/Datasets/kjj/insightface/recognition/arcface_paddle/dynamic/train.py"", line 168, in train
    backbone, classifier, optimizer, for_train=True)
  File ""/home/bbs/Datasets/kjj/insightface/recognition/arcface_paddle/dynamic/utils/io.py"", line 229, in load
    classifier.state_dict(), dist_param_state_dict)
  File ""/home/bbs/Datasets/kjj/insightface/recognition/arcface_paddle/dynamic/utils/io.py"", line 220, in map_actual_param_name
    state_dict[name] = load_state_dict[param.name]
KeyError: 'dist@fc@rank@00000'
"
如题
"'/insightface/recognition/arcface_torch/backbones/iresnet.py':
line 98:         self.features = nn.BatchNorm1d(num_features, eps=1e-05)
line 99:         nn.init.constant_(self.features.weight, 1.0)
line 100:       self.features.weight.requires_grad = False
--------------------------------------------------------------------------------------------------------
What was your purpose of setting 'self.features.weight.requires_grad' as 'False'? To avoid some problem of optimizing? Or others?"
Where can I find and download onnx version of retinaface?
"Is the test time of onnxruntime-GPU related to the version of onnxRuntime-GPU? Why do I use NVIDIA 2080Ti to test scrfd2.5g.onnx，The test results are quite different from the paper.The time fluctuation of the test is also relatively large.I'm sure it's running on a GPU.
>all cost: 440.408
all cost: 15.984000000000002
all cost: 26.214000000000002
all cost: 23.334
all cost: 24.61
all cost: 25.314
all cost: 25.215
all cost: 28.423000000000002
all cost: 25.496000000000002
all cost: 14.052999999999999
all cost: 10.357
all cost: 19.223
all cost: 24.367
all cost: 22.378
all cost: 11.537
all cost: 11.037
all cost: 17.98
all cost: 11.129
all cost: 11.238
all cost: 26.776
all cost: 23.774
all cost: 22.689
all cost: 22.601
all cost: 23.650000000000002
all cost: 26.808
all cost: 26.38
all cost: 17.712
all cost: 13.366
all cost: 9.564
all cost: 22.082
all cost: 22.371
all cost: 17.003
all cost: 10.183
all cost: 9.893
all cost: 10.001
"
"To verify the effect of VPL , I had trained two models with arcface loss:
One model with VPL {'start_iters': 20000, 'allowed_delta': 80, 'lambda': 0.15, 'mode': 0, 'momentum': False} , and theaccuracy of it is LFW 99.85, CFP_FP 99.15, AGEDB_30 98.38, IJBC 96.66 (TAR@FAR=0.0001) 99.57(AUC of ROC in IJBC)

And another model without VPL {'start_iters': 20000, 'allowed_delta': 80, 'lambda': 0.15, 'mode': -1, 'momentum': False}, and the accuracy of it is LFW 99.85, CFP_FP 99.10, AGEDB_30 98.43, IJBC 96.79 (TAR@FAR=0.0001) 99.55(AUC of ROC in IJBC)

other settings are same;

The VPL method don't seem to be working..."
"When I run the torch2onnx.py file(recognition/arcface_torch/), I get an error in the code below.
assert os.path.exists(input_file)

The cause seems to be that ""input="" is included in ""input_file"".
So, I modified a single line of code, can I run it without modification?
If not, I'd like to send you a pull request.
thank you."
"Hi, Thanks for your nice work!

I am using Colab to train ArcFace-Torch on Asian-Celeb dataset. After Colab disconects, I want to continue training from where it left by setting config.resume = True and successfully loading the latest backbone.pth file. However, the training process starts all over from epoch 0, acc and loss seems to be same as acc and loss of the first time I train from scratch, and even worse. Did I successfully resume training? What should I do to fix it? Thank in advance."
"I'm experimenting with face recognition to do a people counter from video. Every time I have a new frame it detects faces and uses norm embeddings to compare with previously detected faces. In general, it works pretty well and fast (better than SORT trackers) but I'm seeing now that there are some face poses that his 512 embedding are very ""general"" and new faces are detected as this face/person Those faces are people with masks getting out from a lift or door (not a frontal view) where we only see one eye like [this](https://drive.google.com/file/d/1v68fSqiQPSnMgRkZNc_i1FMWmRshB-pw/view?usp=sharing) one

I decided to only keep some pose faces  (frontal view) and wanted to use landmarks (distance between eyes) to do it but I see that buffalo_s (or L) 's detection model is ""guessing"" KPS that are not seen in the image 

A[ face example](https://drive.google.com/file/d/1LR9ntJjrBAKcM0LozmK5A1SSQqHGhff8/view?usp=sharing,) another [face example](https://drive.google.com/file/d/1ZED1VNGuzDv8sKdyWFesl1giF3qYprDN/view?usp=sharing)

A [nice face image](https://drive.google.com/file/d/1bwtSyChIFm6cTnIbLldgP1De5ihqVozC/view?usp=sharing) that I will love to use.

I'm guessing that those false landmarks are returned by the face alignment and I don't want to avoid face alignment so, is there any way to have those kps previous face alignment? 

Thank you very much for your work! Juan Luis"
"Hi,

I setup onnxruntme_gpu and trying to inference . but Onnx model always falling back to the CPUProvider.


model
""glintr100.onnx""

installed Cuada11.4 cudnn 8.2.4 and onnxruntime,-gpu 1.9 

without error its running but always falling back to CPUProviders

availableProviders : both Cuda and CPU 

Why is tat happening ? 

Can we use glintr100.onnx and other insightface model with GPU ? or only available for CPU ?"
"![image](https://user-images.githubusercontent.com/15704302/138022352-de6251ef-db6f-483c-9764-d5607e5d63f6.png)

I train arcface using arcface_torch, then I test the saved model, the issue is torch.load():
deserialized_objects[key]
RuntimeError: storage has wrong size: expected 333232334534 got 128

How deal it ?"
"**Hi**

Is there any example to get face pose and rotations in (x, y, z) directions and also save it as image?
Thanks in advance"
"**Hi**

Is there any example to have 3D or even 2D visualize of face alignments like 3D Mesh and 2D 68 Landmarks in this photo?

Is face.normed_embedding from FaceAnalysis, 3D alignments?
please mention how to get alignments data and how to save it as image

Thanks in advance

[https://insightface.ai/assets/img/custom/thumb_retinaface.png](https://insightface.ai/assets/img/custom/thumb_retinaface.png)
![3D Mesh and 2D 68 Landmarks](https://insightface.ai/assets/img/custom/thumb_retinaface.png)

"
"Installation of insightface on my computer fails

__environment__
* macOS 10.15.7
* python 3.8.11
* numpy 1.19.5

__error message__
```
  Running setup.py clean for insightface
Failed to build insightface
Installing collected packages: insightface
    Running setup.py install for insightface ... error
    ERROR: Command errored out with exit status 1:
     command: /Users/nastasia/.virtualenvs/FAME/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-install-yj60xau_/insightface_2984320efb7a404e88a6f45072335612/setup.py'""'""'; __file__='""'""'/private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-install-yj60xau_/insightface_2984320efb7a404e88a6f45072335612/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-record-4ldmnzzt/install-record.txt --single-version-externally-managed --compile --install-headers /Users/nastasia/.virtualenvs/FAME/include/site/python3.8/insightface
         cwd: /private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-install-yj60xau_/insightface_2984320efb7a404e88a6f45072335612/
    Complete output (96 lines):
    running install
    running build
    running build_py
    creating build
    creating build/lib.macosx-10.13-x86_64-3.8
    creating build/lib.macosx-10.13-x86_64-3.8/insightface
    copying insightface/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/app
    copying insightface/app/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/app
    copying insightface/app/mask_renderer.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/app
    copying insightface/app/common.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/app
    copying insightface/app/face_analysis.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/app
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    copying insightface/utils/download.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    copying insightface/utils/filesystem.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    copying insightface/utils/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    copying insightface/utils/storage.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    copying insightface/utils/face_align.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    copying insightface/utils/constant.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/utils
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/landmark.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/arcface_onnx.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/retinaface.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/attribute.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/model_zoo.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/scrfd.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    copying insightface/model_zoo/model_store.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/model_zoo
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/commands
    copying insightface/commands/insightface_cli.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/commands
    copying insightface/commands/rec_add_mask_param.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/commands
    copying insightface/commands/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/commands
    copying insightface/commands/model_download.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/commands
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/data
    copying insightface/data/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/data
    copying insightface/data/rec_builder.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/data
    copying insightface/data/image.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/data
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty
    copying insightface/thirdparty/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d
    copying insightface/thirdparty/face3d/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    copying insightface/thirdparty/face3d/mesh/light.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    copying insightface/thirdparty/face3d/mesh/render.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    copying insightface/thirdparty/face3d/mesh/io.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    copying insightface/thirdparty/face3d/mesh/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    copying insightface/thirdparty/face3d/mesh/vis.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    copying insightface/thirdparty/face3d/mesh/transform.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    copying insightface/thirdparty/face3d/mesh_numpy/light.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    copying insightface/thirdparty/face3d/mesh_numpy/render.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    copying insightface/thirdparty/face3d/mesh_numpy/io.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    copying insightface/thirdparty/face3d/mesh_numpy/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    copying insightface/thirdparty/face3d/mesh_numpy/vis.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    copying insightface/thirdparty/face3d/mesh_numpy/transform.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh_numpy
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/morphable_model
    copying insightface/thirdparty/face3d/morphable_model/morphabel_model.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/morphable_model
    copying insightface/thirdparty/face3d/morphable_model/fit.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/morphable_model
    copying insightface/thirdparty/face3d/morphable_model/__init__.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/morphable_model
    copying insightface/thirdparty/face3d/morphable_model/load.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/morphable_model
    running egg_info
    writing insightface.egg-info/PKG-INFO
    writing dependency_links to insightface.egg-info/dependency_links.txt
    writing entry points to insightface.egg-info/entry_points.txt
    writing requirements to insightface.egg-info/requires.txt
    writing top-level names to insightface.egg-info/top_level.txt
    reading manifest file 'insightface.egg-info/SOURCES.txt'
    writing manifest file 'insightface.egg-info/SOURCES.txt'
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    copying insightface/data/images/Tom_Hanks_54745.png -> build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    copying insightface/data/images/mask_black.jpg -> build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    copying insightface/data/images/mask_blue.jpg -> build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    copying insightface/data/images/mask_green.jpg -> build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    copying insightface/data/images/mask_white.jpg -> build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    copying insightface/data/images/t1.jpg -> build/lib.macosx-10.13-x86_64-3.8/insightface/data/images
    creating build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    copying insightface/thirdparty/face3d/mesh/cython/mesh_core.cpp -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    copying insightface/thirdparty/face3d/mesh/cython/mesh_core.h -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.c -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    copying insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.pyx -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    copying insightface/thirdparty/face3d/mesh/cython/setup.py -> build/lib.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    running build_ext
    building 'insightface.thirdparty.face3d.mesh.cython.mesh_core_cython' extension
    creating build/temp.macosx-10.13-x86_64-3.8
    creating build/temp.macosx-10.13-x86_64-3.8/insightface
    creating build/temp.macosx-10.13-x86_64-3.8/insightface/thirdparty
    creating build/temp.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d
    creating build/temp.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh
    creating build/temp.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython
    clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -Iinsightface/thirdparty/face3d/mesh/cython -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/usr/local/opt/tcl-tk/include -I/Users/nastasia/.virtualenvs/FAME/include -I/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/include/python3.8 -c insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp -o build/temp.macosx-10.13-x86_64-3.8/insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.o
    insightface/thirdparty/face3d/mesh/cython/mesh_core_cython.cpp:646:10: fatal error: 'numpy/arrayobject.h' file not found
    #include ""numpy/arrayobject.h""
             ^~~~~~~~~~~~~~~~~~~~~
    1 error generated.
    error: command 'clang' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /Users/nastasia/.virtualenvs/FAME/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-install-yj60xau_/insightface_2984320efb7a404e88a6f45072335612/setup.py'""'""'; __file__='""'""'/private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-install-yj60xau_/insightface_2984320efb7a404e88a6f45072335612/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/2p/6vyqyvp50h36_cn3k49mpnch0000gn/T/pip-record-4ldmnzzt/install-record.txt --single-version-externally-managed --compile --install-headers /Users/nastasia/.virtualenvs/FAME/include/site/python3.8/insightface Check the logs for full command output.
```

"
"Hi ,
      I had trained a FR model with arcFace_torch method in 8 GPUs, and the accuracy of it is LFW 99.85, CFP_FP 99.12, AGEDB_30 98.48；Simultaneously, I had train another model with VPL_arcface ( 'vpl': {'start_iters': 20000, 'allowed_delta': 80, 'lambda': 0.15, 'mode': 0, 'momentum': True}) in 4 GPUs, and the accuracy of it is LFW 99.85, CFP_FP 99.14, AGEDB_30 98.43. The VPL method don't seem to be working,I don't know the reason."
"Hi, how can we work on age estimation and gender detection"
"I want to get MobileNet pretrained of RetinaFace model for face mask detection, but your Dropbox link seemed to be unavailable and I have Baidu link only. I couldn't sign up and get it from Baidu, are there any alternative ways?"
"Hi ,

I am trying to calc distance in new insightface (onnx insightface latest 0.5) as below code . but got strance result.

what is the correct  way to calculate dist / sim.?

```
def distance(f1,f2):
   
     diff = np.subtract(f1, f2)
     dist = np.sum(np.square(diff), 1)
    return dist

app = FaceAnalysis()
app.prepare(ctx_id=0, det_size=(640, 640))
img = ins_get_image('/Users/tulpar/Projects/person1')
img2 = ins_get_image(""/Users/tulpar/Projects/person2"")
faces1 = app.get(img)
faces2 = app.get(img2)

dis1 = distance(faces1[0].embedding , faces2[0].embedding)
```

distance gives high numbers : like 700 , 50 etc"
"Since version `0.3` of the library numpy is imported in the setup file of the python-package (https://github.com/deepinsight/insightface/blob/master/python-package/setup.py) this causes issues with creating requirements files and creating environments, as installation fails if numpy was not pre-installes. Is it possible to change it in future versions? "
你们是怎么下载到了mxnet version: 1.2.1 版本的
""
"![image](https://user-images.githubusercontent.com/43314406/136639108-08f2d0f1-1487-4445-955d-09c0f5f93aa8.png)

Run recognition will take a long time to come out.


My setup (window)is :

python - 3.7
insightface==0.5
cuda=11.0.3
cudnn=8
numpy==1.18.5
onnxruntime-gpu==1.9.0
"
"Hi,

which face detector is used in buffalo_l model zoo in insightface 0.5 (file name: det_10g.onnx)? Is it retinaface or SCRFD?

These sources suggest SCRFD:
https://github.com/deepinsight/insightface/tree/master/python-package
https://pypi.org/project/insightface/0.5/

These sources suggest retinaface:
https://github.com/deepinsight/insightface/tree/master/model_zoo
https://github.com/deepinsight/insightface/blob/master/python-package/insightface/model_zoo/model_zoo.py

If it is retinaface, why did you go back from SCRFD to retinaface?

"
"I want to measure the accuracy of the recognition problem in the pytorch.
I'm not sure how to implement it.

Currently, the accuracy code is implemented in Verification, but the recognition problem is not. How can I solve this problem?"
""
how can i get acc and loss for plotting using matplotlib? look forward to help!
""
how to deal with loss between teacher with student?
""
how to train my data using insightface/alignment/coordinate_reg?
"other model, eg., crossformer
https://github.com/cheerss/CrossFormer

INFO:root:Speed 480.88 samples/sec   Loss 33.9955   LearningRate 0.0500   Epoch: 0   Global Step: 1900   Fp16 Grad Scale: 8192   Required: 69 hours
Training: 2021-09-23 09:56:18,663-Speed 480.88 samples/sec   Loss 33.9955   LearningRate 0.0500   Epoch: 0   Global Step: 1900   Fp16 Grad Scale: 8192   Required: 69 hours
INFO:root:Speed 488.33 samples/sec   Loss 33.4204   LearningRate 0.0500   Epoch: 0   Global Step: 1950   Fp16 Grad Scale: 8192   Required: 69 hours
Training: 2021-09-23 09:56:44,876-Speed 488.33 samples/sec   Loss 33.4204   LearningRate 0.0500   Epoch: 0   Global Step: 1950   Fp16 Grad Scale: 8192   Required: 69 hours
INFO:root:Speed 468.55 samples/sec   Loss 32.6157   LearningRate 0.0500   Epoch: 0   Global Step: 2000   Fp16 Grad Scale: 16384   Required: 69 hours
Training: 2021-09-23 09:57:12,194-Speed 468.55 samples/sec   Loss 32.6157   LearningRate 0.0500   Epoch: 0   Global Step: 2000   Fp16 Grad Scale: 16384   Required: 69 hours
testing verification..
(12000, 512)
infer time 42.669530999999964
INFO:root:[lfw][2000]XNorm: 26.403959
Training: 2021-09-23 09:57:57,470-[lfw][2000]XNorm: 26.403959
INFO:root:[lfw][2000]Accuracy-Flip: 0.50000+-0.00000
Training: 2021-09-23 09:57:57,470-[lfw][2000]Accuracy-Flip: 0.50000+-0.00000
INFO:root:[lfw][2000]Accuracy-Highest: 0.50000
Training: 2021-09-23 09:57:57,470-[lfw][2000]Accuracy-Highest: 0.50000"
"Sorry, can you give me some references about TAR, FAR, TAR@FAR... I don't understand the metric and how to calculate it. I would like to learn more but searching on google is pretty hopeless."
I just want to know how to align the face image? Or where is the face_align.py?
"Hi, 

Previous there was a separate section on how we can get a 512D face embedding in the deploy folder. It would be helpful if you can provide steps to get face embedding using the updated model zoo. 

Thanks."
"![image](https://user-images.githubusercontent.com/31848222/133921014-0b41bc2a-ba2b-4994-b490-baeaa81c2fc4.png)

As you can see i ran the loop 10 times to confirm the result.

![image](https://user-images.githubusercontent.com/31848222/133921053-0c1ac222-eac7-40bc-bef8-48db6cd880c0.png)

The results are way off the time given by you. Am i missing something ?

Note:
Onnx file exported from the model.pth file given at the following link.
https://1drv.ms/u/s!AswpsDO2toNKqyYWxScdiTITY4TQ?e=DjXof9


"
没有多卡不知怎么训练的r50。。。跑半年么？
whether better solution using insightface?
""
"hello, how can i get the identity of person in a photo any help please!!!"
"Hi, since there is no guide here so I think at first I must have a training dataset just like faces_emore but when I use common/rec_builder.py, I got this:
![image](https://user-images.githubusercontent.com/37320876/133127486-abc8cf0b-30d0-4940-9711-1fedb47044b9.png)
Is there any tutorial on how to train a new recognition model? :(
"
""
"I retrain scrfd model with scrfd_10g_bnkps.py config file on 4 A100 gpus. But training speed is very low. The log file show ""lr: 1.666e-05, eta: 76 days, 3:05:39, time: 0.565, data_time: 0.139, memory: 8009, loss_cls: 0.0006, loss_bbox: 2.0173, loss_kps: 2.2101, loss: 4.2280"". How long does it take you to train the model?"
@nttstar @yingfeng Could you inform how to change the scale of retinaface bounding box  ? I want to crop and align the image to 112x112 to feed the feature extraction network. So I need to scale the bounding box from rectangle(default) to square. I also want to larger the boundingbox area to cover more face area. Any feedback given be much appreciated. 
"hi,
how to modify the tools/scrfd.py to enable support batch-size input data? 

and i have converted the scrfd.onnx with dynamic batch-size, but I  can not get the correct post-process results 
 I change the scrfd .py (mainly forward and detect function)  as follow:

    def forward(self, bs_imgs, thresh,batch_size):
        scores_list = []
        bboxes_list = []
        kpss_list = []

        # input_size = tuple(bs_imgs[0].shape[0:2][::-1])
        # print('input_size',input_size)
        blob = cv2.dnn.blobFromImages(bs_imgs, 1.0 / 128, self.input_size, (127.5, 127.5, 127.5), swapRB=True)
        st = time.time()
        net_outs = self.session.run(self.output_names, {self.input_name: blob})
        # net_outs=[net_outs[i].reshape(batch_size,-1,net_outs[i].shape[-1]) for i in range(len(net_outs))]

        # print('scrfd inference time...',time.time()-st)
        stfp = time.time()
        input_height = blob.shape[2]
        input_width = blob.shape[3]
        fmc = self.fmc
        self.total_centers=[]
        for idx, stride in enumerate(self._feat_stride_fpn): #merge outputs

            #####get preds
            scores_preds = net_outs[idx]
            bbox_preds = net_outs[idx + fmc]* stride
            kps_preds = net_outs[idx + fmc * 2] * stride

            ####together preds with each branch outputs, and reshape to batch-size 
            scores_list.append(scores_preds.reshape(batch_size,-1,1)) #(bs*640*480/)
            bboxes_list.append(bbox_preds.reshape(batch_size,-1,4))
            kpss_list.append(kps_preds.reshape(batch_size,-1,10))
            # for n in range
            ###get anchor_centers in each branch
            height = input_height // stride
            width = input_width // stride
            key = (height, width, stride)
            if key in self.center_cache:
                self.anchor_centers = self.center_cache[key]
            else:

                self.anchor_centers = np.stack(np.mgrid[:height, :width][::-1], axis=-1).astype(np.float32)
                self.anchor_centers = (self.anchor_centers * stride).reshape((-1, 2))
                if self._num_anchors > 1:
                    self.anchor_centers = np.stack([self.anchor_centers] * self._num_anchors, axis=1).reshape((-1, 2))
                if len(self.center_cache) < 100:
                    self.center_cache[key] = self.anchor_centers

            print(idx, self.anchor_centers.shape)
            self.total_centers.append(self.anchor_centers)
        return scores_list, bboxes_list, kpss_list


    def prepare_img(self,img,input_size=None):
        assert input_size is not None or self.input_size is not None
        input_size = self.input_size if input_size is None else input_size
        im_ratio = float(img.shape[0]) / img.shape[1]
        model_ratio = float(input_size[1]) / input_size[0]
        if im_ratio > model_ratio:
            new_height = input_size[1]
            new_width = int(new_height / im_ratio)
        else:
            new_width = input_size[0]
            new_height = int(new_width * im_ratio)
        self.det_scale = float(new_height) / img.shape[0]
        resized_img = cv2.resize(img, (new_width, new_height))
        det_img = np.zeros((input_size[1], input_size[0], 3), dtype=np.uint8)
        det_img[:new_height, :new_width, :] = resized_img
        # print('det size: ',det_img.shape)
        return det_img

    def detect(self, bs_imgs, thresh=0.5, max_num=0, metric='default'):

        bs_imgs = [self.prepare_img(i) for i in bs_imgs]
        stfor=time.time()
        scores_list, bboxes_list, kpss_list = self.forward(bs_imgs, thresh,len(bs_imgs))

        bs_det=[]
        bs_kpss=[]

        for i in range(len(bs_imgs)):
            # for j in range(len(scores_list)):
            pos_scores_p3, pos_bboxes_p3, pos_kpss_p3=self.filter_by_scores(scores_list[0][i],
                                                                             bboxes_list[0][i],
                                                                             kpss_list[0][i],
                                                                             self.total_centers[0])
            pos_scores_p4, pos_bboxes_p4, pos_kpss_p4 = self.filter_by_scores(scores_list[1][i],
                                                                               bboxes_list[1][i],
                                                                               kpss_list[1][i],
                                                                               self.total_centers[1])
            pos_scores_p5, pos_bboxes_p5, pos_kpss_p5 = self.filter_by_scores(scores_list[2][i],
                                                                               bboxes_list[2][i],
                                                                               kpss_list[2][i],
                                                                               self.total_centers[2])

            i_score = np.vstack([pos_scores_p3,pos_scores_p4,pos_scores_p5])
            i_bbox = np.vstack([pos_bboxes_p3,pos_bboxes_p4,pos_bboxes_p5]) / self.det_scale
            i_kps = np.vstack([pos_kpss_p3,pos_kpss_p4,pos_kpss_p5]) / self.det_scale

            # topK
            scores_ravel = i_score.ravel()
            order = scores_ravel.argsort()[::-1]
            pre_det = np.hstack((i_bbox, i_score)).astype(np.float32, copy=False)
            pre_det = pre_det[order, :]
            stnms = time.time()
            keep = self.nms(pre_det)
            print('keeeppppp,order....',len(keep),len(order))
            # print('nms time : ',time.time()-stnms)
            det = pre_det[keep, :]
            kpss = i_kps[order, :, :]
            kpss = kpss[keep, :, :]
            # else:
            #     kpss = None
            if max_num > 0 and det.shape[0] > max_num:
                img=bs_imgs[i]
                area = (det[:, 2] - det[:, 0]) * (det[:, 3] -
                                                  det[:, 1])
                img_center = img.shape[0] // 2, img.shape[1] // 2
                offsets = np.vstack([
                    (det[:, 0] + det[:, 2]) / 2 - img_center[1],
                    (det[:, 1] + det[:, 3]) / 2 - img_center[0]
                ])
                offset_dist_squared = np.sum(np.power(offsets, 2.0), 0)
                if metric == 'max':
                    values = area
                else:
                    values = area - offset_dist_squared * 2.0  # some extra weight on the centering
                bindex = np.argsort(
                    values)[::-1]  # some extra weight on the centering
                bindex = bindex[0:max_num]
                det = det[bindex, :]
                if kpss is not None:
                    kpss = kpss[bindex, :]

            bs_det.append(det)
            bs_kpss.append(kpss)
        return bs_det, bs_kpss
"
"In ./recongnition/_tools/mask_render.py, II executed `CUDA_VISIBLE_DEVICES=0 python mask_renderer.py t`he utilization rate of GPU is 0, but the CPU is used to complete the data generation. How to use gpu to realize the generation of face mask data
In the readme document, the following method is to use gpu. I tried it, but to no avail。
`tool.prepare(ctx_id=0, det_size=(128,128)) #use gpu`"
":i don't know why i dah in my model file .json 3 heads:
i tried to convert it to onnx format but it turn error ....
when i delete the last two heads the convertion worked but  the get model get error  how can i solve this 
thank you 


####  ""heads"": [[1133, 0, 0], [1151, 0, 0], [1158, 0, 0]], 
  ""attrs"": {""mxnet_version"": [""int"", 10800]}
}#####"
how can i get the identification of persons after converting my model to onnx
"when I start 
!CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --network r100 --loss arcface --dataset emore

I have one Error: 
config.imageshape:  3
prefix ./models/r100-arcface-emore/model
image_size [112, 112]
num_classes 4
Called with argument: Namespace(batch_size=16, ckpt=2, ctx_num=1, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.01, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=16, pretrained='../models/arcface_r100_v1/model', pretrained_epoch=0, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'emore', 'dataset_path': '../src/data/dataset/4sv', 'num_classes': 4, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'r100', 'num_workers': 1, 'batch_size': 16, 'per_batch_size': 16}
loading ../models/arcface_r100_v1/model 0
[04:50:10] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.0.0. Attempting to upgrade...
[04:50:10] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
0 1 E 3 prelu False
config.loss_name:  margin_softmax
config.loss_s:  64.0
Network FLOPs: 24.2G
mx.gpu():  gpu(0)
triplet <= 0.
INFO:root:loading recordio ../src/data/dataset/4sv/train.rec...
loading idx...  ../src/data/dataset/4sv/train.idx
Traceback (most recent call last):
  File ""train_softmax.py"", line 480, in <module>
    main()
  File ""train_softmax.py"", line 476, in main
    train_net(args)
  File ""train_softmax.py"", line 332, in train_net
    images_filter=config.data_images_filter
  File ""/content/drive/My Drive/khoaluantn/recognition/image_iter.py"", line 40, in __init__
    s = self.imgrec.read_idx(0)
  File ""/usr/local/lib/python3.7/dist-packages/mxnet/recordio.py"", line 317, in read_idx
    self.seek(idx)
  File ""/usr/local/lib/python3.7/dist-packages/mxnet/recordio.py"", line 279, in seek
    pos = ctypes.c_size_t(self.idx[idx])
KeyError: 0"
"Hi, 

Could someone give a source on why exactly 512 D vector is used in the last layer? what if say we use 1024 instead or higher? should it give a boost?

Thank you!"
"Hello,

I am training a mobilefacenet(embedding=128) with CASIA_Webface using arcface_torch. The loss has decreased from 53.7805 to 2.8381. However, the verification accuracies of lfw, cfp_fp, and agedb_30 remain 0.5, which does not make any sense. 
Could you please point out where I might go wrong? Thank you.

----------------------------------------------------------------------------------------------------------------------------------------------
Training: 2021-09-03 04:05:58,634-Speed 322.19 samples/sec   Loss 2.6971   LearningRate 0.0500   Epoch: 1   Global Step: 2000   Required: 14 h                                                                                               ours
testing verification..
(12000, 128)
infer time 44.320912999999905
Training: 2021-09-03 04:06:46,999-[lfw][2000]XNorm: 9.465224
Training: 2021-09-03 04:06:46,999-[lfw][2000]Accuracy-Flip: 0.50000+-0.00000
Training: 2021-09-03 04:06:46,999-[lfw][2000]Accuracy-Highest: 0.50000
testing verification..
(14000, 128)
infer time 51.230545000000006
Training: 2021-09-03 04:07:42,615-[cfp_fp][2000]XNorm: 9.465774
Training: 2021-09-03 04:07:42,616-[cfp_fp][2000]Accuracy-Flip: 0.49986+-0.00043
Training: 2021-09-03 04:07:42,616-[cfp_fp][2000]Accuracy-Highest: 0.49986
testing verification..
(12000, 128)
infer time 43.916425999999944
Training: 2021-09-03 04:08:30,471-[agedb_30][2000]XNorm: 9.467841
Training: 2021-09-03 04:08:30,471-[agedb_30][2000]Accuracy-Flip: 0.50033+-0.00067
Training: 2021-09-03 04:08:30,471-[agedb_30][2000]Accuracy-Highest: 0.50033"
"When importing insightface I'm keep having 
'libcudart.s.0.11.0 : cannot open shared object file : No such file or directory ' error
I think this is because of CUDA version. 

So my question is what version of insightface is using under CUDA 10 version? "
"Hello,

I would like start testing and using _insightface_ for face recognition. At the main repository page no tutorial or guideline and there are many external links for other repositories and pages. This confuse me a lot.

Simply I look up to use this project announced ""_InsightFace 2D&3D deep face analysis toolbox, mainly based on PyTorch and MXNet._""  may you advise me please from where I can start? I confirm that I look up for face recognition project showing all details for testing and implementation in a clear way.

Thank you in advance."
"scrfd训练过程中在测试验证集会报下面这个错：
img_meta[img_id]['batch_input_shape'] = tuple(img.size()[-2:])
TypeError: 'DataContainer' object does not support indexing
           
"
"Hi,

Thanks for releasing the SCRFD approach, i have a query as in i couldn't find examples for person and face detection together in SCFRD approach, furthermore the example of person detection is in SCFRD 2.5v, can we do it for 0.5 v too? Would highly appreciate your response. Thanks in advance

Ajay "
"Dear Sir, 

The resume training is not working well. Every time I tries resume training, it start a new training."
"Thanks for the good work.
https://github.com/deepinsight/insightface/tree/master/recognition/_evaluation_/ijb 
It seems like the two dropbox sharing link of IJB-B and IJB-C meta files are broken on the readme page.
Could you please fix it or update with other links for getting the files? Thanks a lot!"
"thanks for your great work. I run your code with A100 GPU, it works! I have some questions about model inference。
1. I train the face_emore datasets and get the .pth model. Now i want to recognize a video with the .pth and watch the effect as your video demon. As i know, I should use the .pth update the face database, then work on the test video. but i havenot found how to finish it.  Hope to get your reply~"
"Hi, has anybody tried to use `insightface` pip package in windows OS?  Does it work well?
Any feedback is welcome!"
"I've tried to quantized an scrfd onnx model but was faced with 
`onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from qq.onnx failed:This is an invalid model. Error: two nodes with same node name (neck.fpn_convs.1.conv.bias_scale_node).`

tried to rename all duplicated name but it doesn't solve.
this problem occurs for all kinds of scrfd models such as 10g, .., 500

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04:
- ONNX Runtime version: 1.8.1
- Python version: 3.8
"
CelebA数据集应该是没有年龄标注的，请问是怎么训练的呢
""
"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument find_unused_parameters=True to torch.nn.parallel.DistributedDataParallel, and by
making sure all forward function outputs participate in calculating loss.

may because that multi input to model"
"Training: 2021-08-18 05:59:59,872-Speed 463.03 samples/sec   Loss 44.1258   LearningRate 0.0781   Epoch: 0   Global Step: 2750   Fp16 Grad Scale: 12800   Required: 237 hours
  1050	Training: 2021-08-18 06:00:43,232-Speed 461.27 samples/sec   Loss 43.0147   LearningRate 0.0781   Epoch: 0   Global Step: 2800   Fp16 Grad Scale: 12800   Required: 236 hours
  1051	Training: 2021-08-18 06:01:24,801-Speed 481.14 samples/sec   Loss nan   LearningRate 0.0781   Epoch: 0   Global Step: 2850   Fp16 Grad Scale:  0   Required: 236 hours
  1052	Training: 2021-08-18 06:02:06,191-Speed 483.22 samples/sec   Loss nan   LearningRate 0.0781   Epoch: 0   Global Step: 2900   Fp16 Grad Scale:"
"@littletomatodonkey
hi, thanks for your great project of paddle partialFC. 
and i'm waiting for the  update of distributed paddle partialFC .

"
How can we use scrfd face detector in tensorflow? is there any savedmodel (pb) format of pre-trained model available?
How can we use pre-trained models of scrfd in C++ for inference? Can someone please provide a minimum working example?
"Hi, how can I use the pytorch version of insightface to impliment 1:1 face recognition?"
https://github.com/eeric/Face_recognition_cnn/blob/main/code/insightface/arcface_mxnet/inference.py
"Hello, thank you for grait framework with MXNet support!

I download casia-webface from your dataset-zoo.
Then i download mobilefacenet model and get 99% quality on LFW.
But when i start training on casia-webface, no matter what lr i use, model accuracy decreases to 97% or smaller. (I use lr=1e-15 and verbose=200. Training log says that training is called exactly with this lr).

I trayed this with another training dataset and situation is the same. Model performance is decreasing immediately."
"Hello~ I'm learning with my custom dataset with arcface_torch code but,

Even though it is not an LR adjustment interval, the loss drops significantly 
and then rises at the start of the Epoch. What's the problem?

<img width=""878"" alt=""스크린샷 2021-08-11 오후 5 44 36"" src=""https://user-images.githubusercontent.com/11434363/128998805-85a0fcdb-4b51-4e80-8903-11a2dcc26f95.png"">

<img width=""874"" alt=""스크린샷 2021-08-11 오후 5 45 30"" src=""https://user-images.githubusercontent.com/11434363/128998826-eb0efd1a-d072-4330-820e-5566216fe319.png"">
"
"I got this error while training retinaface using mobilenet.25 as backbone.

> Traceback (most recent call last):
  File ""train.py"", line 503, in <module>
    main()
  File ""train.py"", line 499, in main
    lr_step=args.lr_step)
  File ""train.py"", line 391, in train_net
    num_epoch=end_epoch)
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/executor_group.py"", line 280, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/executor_group.py"", line 383, in bind_exec
    self._collect_arrays()
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/executor_group.py"", line 312, in _collect_arrays
    for name, _ in self.data_shapes]
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/executor_group.py"", line 312, in <listcomp>
    for name, _ in self.data_shapes]
  File ""/usr/local/lib/python3.6/site-packages/mxnet/module/executor_group.py"", line 311, in <listcomp>
    self.data_arrays = [[(self.slices[i], e.arg_dict[name]) for i, e in enumerate(self.execs)]
  File ""/usr/local/lib/python3.6/site-packages/mxnet/executor.py"", line 277, in arg_dict
    self._symbol.list_arguments(), self.arg_arrays)
  File ""/usr/local/lib/python3.6/site-packages/mxnet/executor.py"", line 95, in _get_dict
    raise ValueError('Duplicate names detected, %s' % str(names))


It warns me that the variable name 'face_rpn_landmark_pred_stride32_weight' is used more than once. What adjustments should I make to solve this problem?
"
"Hello, does anybody share mirror repository or links to Mobilefacenet pretrained model with pytorch? 
thank you!"
"hi. I am able to detect faces in an image with scrfd. If there is more than one face in the image, I have to give the faces one by one to the Recognition model, respectively.
How can I do this in such a way that I get all the faces in one go (at once ) and get the features in less time?

```
bboxes, kpss = self.det_model.detect(img,
                                             max_num=max_num,
                                             metric='default')
        if bboxes.shape[0] == 0:
            return []
        ret = []
        for i in range(bboxes.shape[0]):
            
            bbox = bboxes[i, 0:4]
            det_score = bboxes[i, 4]
            kps = None
            if kpss is not None:
                kps = kpss[i]
            face = Face(bbox=bbox, kps=kps, det_score=det_score)

            for taskname, model in self.models.items():

                if taskname=='detection':
                    continue
                model.get(img, face)
            ret.append(face)
            
        
        return ret
```


get feature : 
```
def get_feat(self, imgs):
        if not isinstance(imgs, list):
            imgs = [imgs]
 
        input_size = self.input_size
        
        blob = cv2.dnn.blobFromImages(imgs, 1.0 / self.input_std, input_size,
                                      (self.input_mean, self.input_mean, self.input_mean), swapRB=True)
   
        net_out = self.session.run(self.output_names, {self.input_name: blob})[0]
   
        return net_out
```



"
"Hi. I can do face recognition in real time using the Python insightface package and onnx pre-trained models.
(https://github.com/deepinsight/insightface/tree/master/python-package)
I really face a lot of questions and challenges if you please help me.

cuda 11.1
mxnet :1.8.1 from source installed
onnxruntime-gpu:1.7.0
numpy:1.17.0

I use the following code to identify faces for 1000 feature extracted faces:
```
import cv2
import sys
import time
import numpy as np
import insightface
from insightface.app import FaceAnalysis
from imutils import paths
import os
import pickle
from numpy as np
from numpy.linalg import norm


parser = argparse.ArgumentParser()
parser.add_argument('--ctx', default=0, type=int, help='GPU')
args = parser.parse_args()
app = FaceAnalysis(name='models')
app.prepare(ctx_id=args.ctx, det_size=(640, 640))

database = {}

def compare(feat1, feat2):

    distance = np.dot(feat2, feat1) / norm(feat2) * norm(feat1))

    return distance

verification_threshhold = 0.3
database = {}

data = pickle.loads(open('encodings.pickle', ""rb"").read())

encoding = np.empty(512, )
imagePaths = list(paths.list_images('dataset'))

for (i, imagePath) in enumerate(imagePaths):
       
       name = os.path.splitext(os.path.basename(imagePath))[0]
       image = cv2.imread(imagePath)

        t1 = time.time()
        encodings = app.get(frame)
        t2 = time.time()
        print('elapsed time for extract encodings: ',(t2-t1))

        ts = time.time()
        for num, e in enumerate(encodings):

            encoding = e.embedding

            identity = ''
            for (name, db_enc) in data.items():

                dist = compare(encoding, db_enc)

                if 0.35 < dist:
                    identity = name
        te = time.time()
        print('elapsed time for  compare face in 1000 data')
```
Here I use different images for identification and each image has a different number of faces.
For example, one image has one face and the other image has 6 faces and the other has 15 faces.
After testing different images, I came up with the following outputs:

picture 1 (1 face ) >>> elapsed time for extract encodings: 0.019 s , elapsed time for compare face in 1000 data: 0.009 s
picture 2 (6 faces ) >>> elapsed time for extract encodings: 0.057 s , elapsed time for compare face in 1000 data: 0.05 s
picture 2 (15 faces ) >>> elapsed time for extract encodings: 0.19 s , elapsed time for compare face in 1000 data: 0.22 s

This is not at all useful for my purpose, considering real-time detection using multiple cameras with at least 20,000 faces for comparison.
How can I reduce this time?
Am I doing the right thing?
Thank you in advance for your reply"
"对图像转张量后，不做减均值除方差了吗？如下：
     img = cv2.imread(image_path)
     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
     img = img.transpose(2, 0, 1)
     img = img[np.newaxis, :]
     array = mx.nd.array(img)"
只有1个gpu，能否训练？
"How to use to make custom Data set, or how to use this network to train  Custom  face data set, how to set up the network?"
"Hi , Does the repo contain keypoints evaluation codes?

"
"Hi, thank you for your work!

I've faced a problem during trying to load retinaface_mnet025_v2.zip weights (i'm using insightface==0.1.5).

When I do
```model = insightface.model_zoo.get_model('retinaface_mnet025_v2')``` 

, i get the following error:

```
Model file is not found. Downloading.
Downloading /root/.insightface/models/retinaface_mnet025_v2.zip from http://insightface.ai/files/models/retinaface_mnet025_v2.zip...
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-33-8935f9a6e230> in <module>()
----> 1 model = insightface.model_zoo.get_model('retinaface_mnet025_v2')

4 frames
/usr/local/lib/python3.7/dist-packages/insightface/utils/download.py in download(url, path, overwrite, sha1_hash)
     68         r = requests.get(url, stream=True)
     69         if r.status_code != 200:
---> 70             raise RuntimeError(""Failed downloading url %s""%url)
     71         total_length = r.headers.get('content-length')
     72         with open(fname, 'wb') as f:

RuntimeError: Failed downloading url http://insightface.ai/files/models/retinaface_mnet025_v2.zip
```

How can I resolve this problem?"
"我使用了自己的图片数据集做成了bin文件，label.txt 也是按照您给的格式，agedb_30.bin，cfp_fp.bin，lfw.bin是下载的MS1M_bin数据里自带的。cpu,paddle2.1.1,Ubuntu 18.04 LTS训练时出现以下错误,batch_size 给定为1，希望能够解答，非常感谢。
  EPS = np.finfo(np.float).eps
finish reading file, image num: 100
softmax weight init successfully!
softmax weight mom init successfully!
Epoch 0: LambdaDecay set learning rate to 0.1.
Epoch 0: LinearWarmup set learning rate to 0.0.
Epoch 0: LambdaDecay set learning rate to 0.1.
Epoch 0: LinearWarmup set learning rate to 0.0.
Total Step is: 3200
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
/root/miniconda3/lib/python3.8/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  if data.dtype == np.object:
/root/miniconda3/lib/python3.8/site-packages/paddle/nn/layer/norm.py:640: UserWarning: When training, we now always track global mean and variance.
  warnings.warn(
Traceback (most recent call last):
  File ""train.py"", line 156, in <module>
    main(args)
  File ""train.py"", line 105, in main
    x_grad, loss_v = module_partial_fc.forward_backward(
  File ""/insightface-master/insightface-master (2)/insightface-master/recognition/arcface_paddle/partial_fc.py"", line 136, in forward_backward
    logits = self.forward(total_features, norm_weight)
  File ""/insightface-master/insightface-master (2)/insightface-master/recognition/arcface_paddle/partial_fc.py"", line 115, in forward
    logits = linear(total_features, paddle.t(norm_weight))
  File ""/root/miniconda3/lib/python3.8/site-packages/paddle/nn/functional/common.py"", line 1450, in linear
    core.ops.matmul(x, weight, pre_bias, 'transpose_X', False,
ValueError: (InvalidArgument) The fisrt matrix width should be same as second matrix height,but received fisrt matrix width 104448, second matrix height 128
  [Hint: Expected dim_a.width_ == dim_b.height_, but received dim_a.width_:104448 != dim_b.height_:128.] (at /paddle/paddle/fluid/operators/math/blas_impl.h:1201)
  [operator < matmul > error]
"
"环境：Ubuntu 18.04 LTS   paddle 2.1.1    cpu
arcface_paddle里运行 python train.py     --network 'MobileFaceNet_128'     --lr=0.1     --batch_size 512     --weight_decay 2e-4     --embedding_size 128     --logdir=""log""     --output ""emore_arcface""     --resume 0命令报错如下：
--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::SignalHandle(char const*, int)
1   paddle::platform::GetCurrentTraceBackString[abi:cxx11]()

----------------------
Error Message Summary:
----------------------
FatalError: `Segmentation fault` is detected by the operating system.
  [TimeInfo: *** Aborted at 1627371549 (unix time) try ""date -d @1627371549"" if you are using GNU date ***]
  [SignalInfo: *** SIGSEGV (@0x7fae3e783000) received by PID 1044 (TID 0x7fb02c1c0740) from PID 1048064000 ***]"
"Thanks for your great job first. But when I run the bash ```CUDA_VISIBLE_DEVICES=""0,1,2,3"" PORT=29701 bash ./tools/dist_train.sh ./configs/scrfd/scrfd_2.5g.py 4```. The ap only achieve 62.4, which is about 10 less than your code(77.13). "
"Hello, I would like to reproduce an experiment on CASIA dataset but it seems like you do not set random seeds. How can I achieve the same results as in the paper?"
"Hi,

Can I ask which version of MS1M is used in your paper['Variational Prototype Learning for Deep Face Recognition'](https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Variational_Prototype_Learning_for_Deep_Face_Recognition_CVPR_2021_paper.pdf)?

Is it MS1M-retina? I also checked in your repo that arcface+ resnet100 would have 96.81 on IJB-C while in your paper arcface+resnet100 is 96.03, can you make a bit explanation?"
"Hi, thank you for your work!

I'm wondering if it's possible to inference this python module https://github.com/deepinsight/insightface/tree/master/python-package for a batch of images? In order to use it fast for several images. And also the question is if it is using gpu/cuda on inference and if it's not -- is it possible to use it's advantage to improve the inference in terms of speed? "
I'm training SCRFD but till 60 epoch still no checkpoint is saved?
"import cv2
import numpy as np
import insightface
from insightface.app import FaceAnalysis
from insightface.data import get_image as ins_get_image

handler = insightface.model_zoo.get_model('your_recognition_model.onnx')
handler.prepare(ctx_id=0)"
"Arcface-Paddle训练数据。
我有自己的原始数据集，是bmp格式，试图转成bin格式，报错。似乎tools里py代码需要修改才能正常运行。"
"First of all, thanks to all contributors for these valuable resources.

I downloaded `r100` model trained with glint360k from [this link](https://onedrive.live.com/?authkey=%21AFZjr283nwZHqbA&id=4A83B6B633B029CC%215582&cid=4A83B6B633B029CC) 

And, when evaluating it with [this code](https://github.com/deepinsight/insightface/blob/996d844e12ed315b44935131c1f25bc4aef4e15e/recognition/arcface_torch/utils/utils_callbacks.py#L12-L48) and dataset file shared in this repo .

It reported the 97.x accuracy for `cfp_fp`. 
![image](https://user-images.githubusercontent.com/5276432/126592132-7455407d-13ec-4f41-8948-2be182471d4b.png)


However, I am afraid that this value is not consistent with that of README where the accuracy is reported as 99.x for `cfp_fp`.
Below is the provided log.
![image](https://user-images.githubusercontent.com/5276432/126592155-d765eed0-4536-44cf-8db7-78f62b7396fc.png)


Is anyone who has ever encountered this issue or knows the reason?

Thank you!
"
"please how can i test my recognition models 
how to test recognition help please"
how to convert from test image to .bin format.
"![image](https://user-images.githubusercontent.com/38750855/126446235-4fee034b-f8d1-474b-9f47-55d2a19c1d7d.png)
 look https://github.com/ttanzhiqiang/onnx_tensorrt_project"
"introduction
https://github.com/eeric/Face_recognition_cnn/blob/main/code/insightface/README.md"
"I have problem running test.py 
[32, 16, 8] {'32': {'SCALES': (32, 16), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}, '16': {'SCALES': (8, 4), 'BASE_SIZE': 16, 
'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}, '8': {'SCALES': (2, 1), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}}
[18:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:209: Loading symbol saved by previous version v1.3.0. Attempting 
to upgrade...
[18:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:217: Symbol successfully upgraded!
means [0. 0. 0.]
use_landmarks True
cascade 0
sym size: 9
Traceback (most recent call last):
  File ""e:/DataTraining/insightface-master/insightface-master/detection/RetinaFace/test.py"", line 19, in <module>
    print(img.shape)
AttributeError: 'NoneType' object has no attribute 'shape'
------
Does this error mean my image is not valid? how can i fix it?.
thank you"
"I try to use cosface pre-trained model of glint360 to train the arcface，why the loss is very large, the loss value is 48.0 at the beginning, shouldn’t it be a relatively small loss?"
"**when i train arcface_torch configs/ms1mv3_r18, have trouble for Loss nan**

Here's train log!

(pytorch1.6) admin0:~/deepinsight/insightface/recognition/arcface_torch$ srun -N 1 --gres=gpu:8 -p algorithm-chejian python -m torch.distributed.launch --nproc_per_node=8 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=1234 train.py configs/ms1mv3_r18
srun: job 102171 queued and waiting for resources
srun: job 102171 has been allocated resources
Training: 2021-07-16 20:56:48,645-rank_id: 0
Training: 2021-07-16 20:57:00,703-softmax weight init successfully!
Training: 2021-07-16 20:57:00,703-softmax weight mom init successfully!
Training: 2021-07-16 20:57:00,704-: loss                     arcface
Training: 2021-07-16 20:57:00,704-: network                  r18
Training: 2021-07-16 20:57:00,704-: resume                   False
Training: 2021-07-16 20:57:00,704-: output                   work_dirs/ms1mv3_r18
Training: 2021-07-16 20:57:00,705-: dataset                  ms1m-retinaface-t1
Training: 2021-07-16 20:57:00,705-: embedding_size           512
Training: 2021-07-16 20:57:00,705-: sample_rate              1.0
Training: 2021-07-16 20:57:00,705-: fp16                     True
Training: 2021-07-16 20:57:00,705-: momentum                 0.9
Training: 2021-07-16 20:57:00,705-: weight_decay             0.0005
Training: 2021-07-16 20:57:00,705-: batch_size               384
Training: 2021-07-16 20:57:00,705-: lr                       0.1
Training: 2021-07-16 20:57:00,705-: rec                      /lustre/users/chenguang/data/face/faces_emore
Training: 2021-07-16 20:57:00,705-: num_classes              93431
Training: 2021-07-16 20:57:00,705-: num_image                5179510
Training: 2021-07-16 20:57:00,705-: num_epoch                25
Training: 2021-07-16 20:57:00,705-: warmup_epoch             -1
Training: 2021-07-16 20:57:00,706-: decay_epoch              [10, 16, 22]
Training: 2021-07-16 20:57:00,706-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2021-07-16 20:57:00,706-: resume_model             backbone.pth
Training: 2021-07-16 20:57:00,706-: warmup_step              -1895
Training: 2021-07-16 20:57:00,706-: total_step               47375
Training: 2021-07-16 20:57:00,706-: decay_step               [18953, 30326, 41698]
...
...
Training: 2021-07-16 21:11:10,631-Speed 2243.92 samples/sec   Loss 25.8109   LearningRate 0.6000   Epoch: 1   Global Step: 2050   Fp16 Grad Scale: 3072   Required: 5 hours
Training: 2021-07-16 21:11:25,216-Speed 10532.00 samples/sec   Loss 25.8466   LearningRate 0.6000   Epoch: 1   Global Step: 2100   Fp16 Grad Scale: 3072   Required: 5 hours
Training: 2021-07-16 21:11:40,205-Speed 10247.92 samples/sec   Loss 25.9128   LearningRate 0.6000   Epoch: 1   Global Step: 2150   Fp16 Grad Scale: 6144   Required: 5 hours
Training: 2021-07-16 21:11:56,593-Speed 9373.46 samples/sec   Loss 26.4034   LearningRate 0.6000   Epoch: 1   Global Step: 2200   Fp16 Grad Scale: 6144   Required: 5 hours
Training: 2021-07-16 21:12:13,489-Speed 9091.42 samples/sec   Loss 26.0906   LearningRate 0.6000   Epoch: 1   Global Step: 2250   Fp16 Grad Scale: 12288   Required: 5 hours
Training: 2021-07-16 21:12:31,352-Speed 8598.87 samples/sec   Loss 25.9126   LearningRate 0.6000   Epoch: 1   Global Step: 2300   Fp16 Grad Scale: 3072   Required: 5 hours
Training: 2021-07-16 21:12:48,455-Speed 8981.49 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2350   Fp16 Grad Scale:  1   Required: 5 hours
Training: 2021-07-16 21:13:03,711-Speed 10068.48 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2400   Fp16 Grad Scale:  0   Required: 5 hours
Training: 2021-07-16 21:13:19,859-Speed 9512.47 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2450   Fp16 Grad Scale:  0   Required: 5 hours
Training: 2021-07-16 21:13:35,858-Speed 9600.90 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2500   Fp16 Grad Scale:  0   Required: 5 hours
Training: 2021-07-16 21:13:52,046-Speed 9488.67 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2550   Fp16 Grad Scale:  0   Required: 5 hours
Training: 2021-07-16 21:14:08,330-Speed 9432.86 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2600   Fp16 Grad Scale:  0   Required: 5 hours
Training: 2021-07-16 21:14:24,139-Speed 9716.21 samples/sec   Loss nan   LearningRate 0.6000   Epoch: 1   Global Step: 2650   Fp16 Grad Scale:  0   Required: 5 hours
"
"I wanna create my recognition benchmark using insightface/recognition/arcface_mxnet/common/build_eval_pack.py. 
As i understood i need to have pairs_label.txt to do this. My data set is masked LFW so the pairs_label.txt from orignal LFW is what i'm looking for. Where can i find this file?
"
I use the scrfd_2.5g_bnkps model. how can I get 5 keypoints from it? Now the model only outputs bbox for image. Thanks
I guess it is in my server. It takes about a week. How long do you generally take? Is there a way to make this process faster??
"Hi.
Can you tell me what kind of coordinate system is used in 3d68 landmarks model in python package?
Where is the starting point of z axis?
Thanks"
https://github.com/deepinsight/insightface/blob/master/examples/person_detection/scrfd_person.py
"https://github.com/deepinsight/insightface/blob/66739ca6b8d053679f0434fd20d2c308ac487cb9/recognition/arcface_torch/backbones/iresnet2060.py#L164
Why the feature has BatchNorm1d? And set the weight.requires_grad=False?"
"Hi, expert
May you tell me what test data used to evaluate this detection model? (or any paper reference from yours)
I would like to know the distribution of mask scores which you test.
"
"Hi,
I highly appreciate your works!
I see you've updated new great person detection model recently. Could you tell me how to retrain person detection with my own dataset? Thank you very much!"
"**this is my result**
INFO:root:Epoch[200] Batch [0-80]	Speed: 28.29 samples/sec	RPNAcc_s32=0.994834	RPNAcc_s32_BG=0.999217	RPNAcc_s32_FG=0.832069	RPNL1Loss_s32=0.095835	RPNLandMarkL1Loss_s32=0.110469	RPNAcc_s16=0.993185	RPNAcc_s16_BG=0.998837	RPNAcc_s16_FG=0.813887	RPNL1Loss_s16=0.102131	RPNLandMarkL1Loss_s16=0.111187	RPNAcc_s8=0.989686	RPNAcc_s8_BG=0.999001	RPNAcc_s8_FG=0.437806	RPNL1Loss_s8=0.168240	RPNLandMarkL1Loss_s8=0.283297
**this is your result**
INFO:root:Epoch[80] Batch [20]	Speed: 22.20 samples/sec	RPNAcc_s32=0.996012	RPNAcc_s32_BG=0.999618	RPNAcc_s32_FG=0.860803	RPNL1Loss_s32=0.105018	RPNLandMarkL1Loss_s32=0.117736	RPNAcc_s16=0.994158	RPNAcc_s16_BG=0.998816	RPNAcc_s16_FG=0.832971	RPNL1Loss_s16=0.103016	RPNLandMarkL1Loss_s16=0.104968	RPNAcc_s8=0.990917	RPNAcc_s8_BG=0.998928	RPNAcc_s8_FG=0.497361	RPNL1Loss_s8=0.161664

my config as below:

INFO:root:===================
INFO:root:{'BBOX_MASK_THRESH': 0,
 'CASCADE': 0,
 'CASCADE_BBOX_STRIDES': [64, 32, 16, 8, 4],
 'CASCADE_CLS_STRIDES': [64, 32, 16, 8, 4],
 'CASCADE_MODE': 1,
 'COLOR_JITTERING': 0.125,
 'COLOR_MODE': 1,
 'CONTEXT_FILTER_RATIO': 1,
 'DENSE_ANCHOR': False,
 'FACE_LANDMARK': True,
 'FIXED_PARAMS': ['^stage1', '^.*upsampling'],
 'HEAD_BOX': False,
 'HEAD_FILTER_NUM': 64,
 'HEAD_MODULE': 'SSH',
 'IMAGE_STRIDE': 0,
 'LANDMARK_LR_MULT': 2.5,
 'LAYER_FIX': True,
 'LR_MODE': 0,
 'MIXUP': 0.0,
 'MORE_SMALL_BOX': True,
 'NET_MODE': 2,
 'NUM_ANCHORS': 2,
 'NUM_CLASSES': 2,
 'NUM_CPU': 4,
 'ORIGIN_SCALE': False,
 'PIXEL_MEANS': array([0., 0., 0.]),
 'PIXEL_SCALE': 1.0,
 'PIXEL_STDS': array([1., 1., 1.]),
 'PRE_SCALES': [(1200, 1600)],
 'RANDOM_FEAT_STRIDE': False,
 'RPN_ANCHOR_CFG': {'16': {'ALLOWED_BORDER': 9999,
                           'BASE_SIZE': 16,
                           'NUM_ANCHORS': 2,
                           'RATIOS': [1.0],
                           'SCALES': [8, 4]},
                    '32': {'ALLOWED_BORDER': 9999,
                           'BASE_SIZE': 16,
                           'NUM_ANCHORS': 2,
                           'RATIOS': [1.0],
                           'SCALES': [32, 16]},
                    '8': {'ALLOWED_BORDER': 9999,
                          'BASE_SIZE': 16,
                          'NUM_ANCHORS': 2,
                          'RATIOS': [1.0],
                          'SCALES': [2, 1]}},
 'RPN_FEAT_STRIDE': [32, 16, 8],
 'SCALES': [(640, 640)],
 'SHARE_WEIGHT_BBOX': False,
 'SHARE_WEIGHT_LANDMARK': False,
 'TEST': {'BATCH_IMAGES': 1,
          'CXX_PROPOSAL': True,
          'HAS_RPN': False,
          'IOU_THRESH': 0.5,
          'NMS': 0.3,
          'RPN_NMS_THRESH': 0.3,
          'RPN_POST_NMS_TOP_N': 3000,
          'RPN_PRE_NMS_TOP_N': 1000,
          'SCORE_THRESH': 0.05},
 'TRAIN': {'ASPECT_GROUPING': False,
           'BATCH_IMAGES': 16,
           'BBOX_STDS': [1.0, 1.0, 1.0, 1.0],
           'CASCADE_OVERLAP': [0.4, 0.5],
           'END2END': True,
           'IMAGE_ALIGN': 0,
           'LANDMARK_STD': 1.0,
           'MIN_BOX_SIZE': 0,
           'OHEM_MODE': 1,
           'RPN_BATCH_SIZE': 256,
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_ENABLE_OHEM': 2,
           'RPN_FG_FRACTION': 0.25,
           'RPN_FORCE_POSITIVE': False,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_POSITIVE_OVERLAP': 0.5},
 'USE_3D': False,
 'USE_BLUR': False,
 'USE_CROP': True,
 'USE_DCN': 0,
 'USE_FPN': True,
 'USE_MAXOUT': 0,
 'USE_OCCLUSION': False,
 'dataset': 'retinaface',
 'max_feat_channel': 8888,
 'network': 'mnet'}
origin image size 12880
retinaface_train gt roidb loaded from data/cache/retinaface_train_train_gt_roidb.pkl
roidb size 12876
INFO:root:retinaface_train append flipped images to roidb
flipped roidb size 25752
INFO:root:loading model/mobilenet,0,batchsize 16
[02:13:22] ../src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.3.0. Attempting to upgrade...
[02:13:22] ../src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[02:13:22] ../src/storage/storage.cc:199: Using Pooled (Naive) StorageManager for CPU
=========lr_iters===========
[1609, 3219, 4828, 6438, 8047, 88522, 109446, 128760, 209235, 321900]
0.001
INFO:root:lr 0.001000 lr_epoch_diff [1, 2, 3, 4, 5, 55, 68, 80, 130, 200] lr_steps [(1609, 0.1), (3219, 0.1), (4828, 0.1), (6438, 0.1), (8047, 0.1), (88522, 0.1), (109446, 0.1), (128760, 0.1), (209235, 0.1), (321900, 0.1)]"
"i am converting model to onnx my command is below:
`python scrfd2onnx  ../configs/scrfd/scrfd_500m.py scrfd_500m.pth --shape -1 -1 --input-img face.jpg`

error is :
`KeyError: 'SCRFD is not in the models registry'`

do anyone have some ideas ? @nttstar "
"Retina face anti-cov repo. suggests to download weight files from following two links:

https://pan.baidu.com/s/16ihzPxjTObdbv0D6P6LmEQ

https://www.dropbox.com/s/6rhhxsbh2qik65k/cov2.zip?dl=0

But none of the links is working. Can someone please update the links?"
"Hi, Thank you for your work.
Could you explain difference between test.py vs test_widerface.py.

After first script on Val widerface your best model has next result: 
+-------+-------+--------+--------+-------+
| class | gts   | dets   | recall | ap    |
+-------+-------+--------+--------+-------+
| FG    | 39697 | 697770 | 0.771  | 0.713 |
+-------+-------+--------+--------+-------+
| mAP   |       |        |        | 0.713 |
+-------+-------+--------+--------+-------+
OrderedDict([('mAP', 0.7131168842315674)])

After second script on Val widerface your best model has next result: APS: [0.9605776871050001, 0.9491621912334129, 0.8532284473260425]
"
how can i work with recognition and test it help please
"hi,
how can i test if recognition work or not please help 
any code for testing recognition 
any guide help please"
"Hello,

Where can I get pre-trained model files?

Thank you."
"I found that mnet has some fix params, ""stage1*"" and ""upsampling"" and layers whose index less than 7,such as
**def get_fixed_params(symbol, fixed_param):
    if not config.LAYER_FIX:
        return []
    fixed_param_names = []
    #for name in symbol.list_arguments():
    #  for f in fixed_param:
    #    if re.match(f, name):
    #      fixed_param_names.append(name)
    #pre = 'mobilenetv20_features_linearbottleneck'
    idx = 0
    for name in symbol.list_arguments():
        #print(idx, name)
        if idx < 7 and name != 'data':
            fixed_param_names.append(name)
        #elif name.startswith('stage1_'):
        #  fixed_param_names.append(name)
        if name.find('upsampling') >= 0:
            fixed_param_names.append(name)

        idx += 1
    return fixed_param_names**
I don't konw why."
"Can't find train code,just has inference code"
"Hi, I've read your recent work VPL and tried to use it in our project, I implemented Arcface with VPL in Mxnet and I have met several questions:
1. In the memory injection part,  the Dimensional of embedding in Memory bank (M) is significantly different with the weight (W, which is [0,1] ). In order to do the injection, I normalized M before adding it to W. And the final weight = **Norm( 0.15 * Norm(M)  + 0.85 * W )**  Is this assumption matches your implementation of VPL ?
2. Is there any ideas of hyper-parameter settings in different data distribution ? Which hyper-parameter is more important, t or M_ratio (which is decided by t)? Because in my datasets, I have to set t =2000 to reach M_ratio = 40% and memory bank might contain features from early steps.

Thanks !"
"Hi, guys.

arcface_torch log is ` ""Speed %.2f samples/sec   Loss %.4f   Epoch: %d   Global Step: %d   Required: %1.f hours""` 

but I want to log accuracy too. 
How can I calculate the accuracy?"
"I saw the inference example at : https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/inference.py

How can I compare two features ?

I used     dist = np.sum(np.square(f1 - f2))

it gives 500 ,  700 like numbers.

model is : ms1mv3_arcface_r100_fp16



Best "
"  if ""yes"" in cb_data:
      Config.feedback.remove(m.from_user.id)
      feedtext = m.message.reply_to_message
      button = [[InlineKeyboardButton(""↩ Reply"", callback_data=f""reply+{m.from_user.id}"")]]
      markup = InlineKeyboardMarkup(button)
      for i in Config.OWNER:
          NS = await feedtext.forward(int(i))
          await NS.reply_text(""💬 Send The Reply"", reply_markup=markup, quote=True)
      await m.message.delete()
      await c.send_message(chat_id=m.message.chat.id, text=""✅ Feedback Sent Successfully. Hope you will get reply soon"")


**EROR**

File ""/app/plugins/callback.py"", line 43, in cb_handler
2021-07-03T14:07:02.652205+00:00 app[worker.1]:     NS = await feedtext.forward(int(i))
2021-07-03T14:07:02.652206+00:00 app[worker.1]: AttributeError: 'NoneType' object has no attribute 'forward'"
"I have observed that the under either sample rates, the trianing speeds are almost identical. Is changing the config.sample_rate=0.1 really triggering the partial fc operation? or am I missing something?"
where is  BFM.mat and BFM_UV.mat?
"/usr/local/lib/python3.7/dist-packages/mxnet/__init__.py
gpu num: 1
config.imageshape:  3
prefix ./models/r100-arcface-emore/model
image_size [112, 112]
num_classes 7
Called with argument: Namespace(batch_size=16, ckpt=2, ctx_num=1, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.01, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=16, pretrained='../models/arcface_r100_v1/model', pretrained_epoch=0, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'emore', 'dataset_path': '../src/data/dataset/7sv', 'num_classes': 7, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'r100', 'num_workers': 1, 'batch_size': 16, 'per_batch_size': 16}
loading ../models/arcface_r100_v1/model 0
[03:19:52] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.0.0. Attempting to upgrade...
[03:19:53] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
0 1 E 3 prelu False
config.loss_name:  margin_softmax
config.loss_s:  64.0
Network FLOPs: 24.2G
mx.gpu():  gpu(0)
triplet <= 0.
INFO:root:loading recordio ../src/data/dataset/7sv/train.rec...
loading idx...  ../src/data/dataset/7sv/train.idx
Traceback (most recent call last):
  File ""train_softmax.py"", line 480, in <module>
    main()
  File ""train_softmax.py"", line 476, in main
    train_net(args)
  File ""train_softmax.py"", line 332, in train_net
    images_filter=config.data_images_filter
  File ""/content/drive/My Drive/khoaluantn/recognition/image_iter.py"", line 40, in __init__
    s = self.imgrec.read_idx(0)
  File ""/usr/local/lib/python3.7/dist-packages/mxnet/recordio.py"", line 317, in read_idx
    self.seek(idx)
  File ""/usr/local/lib/python3.7/dist-packages/mxnet/recordio.py"", line 279, in seek
    pos = ctypes.c_size_t(self.idx[idx])
KeyError: 0"
"When converting the r50 model trained from ""arcface-pytorch"" to the caffe model, there is an error.

Which part should I check during the torch -> onnx -> caffe converting process?

```
./onnx2caffe.sh: line 1: 52722 Floating point exception(core dumped) python3 convertCaffe.py ./backbone.onnx ./arcface_r50.prototxt ./arcface_r50.caffemodel
```

_operators.py
```
opset_version = 11

  File ""/home_sw1/phj8498/insightface/tools/onnx2caffe/onnx2caffe/_operators.py"", line 33, in make_input
    ""ONNX opset version {} is not supported,only opset 9 is supported,you can export onnx by setting opset_version like torch.onnx.export(model, '', opset_version=9, verbose=True).\n"".format(version )
TypeError: ONNX opset version 11 is not supported,only opset 9 is supported,you can export onnx by setting opset_version like torch.onnx.export(model, '', opset_version=9, verbose=True).
```
"
"Hi, I read your paper about vpl and want to try it for our project.
When I may try the code? And, is it based on MXNet? 

Thanks"
There is a MobileFaceNet pre-trained model that train on the Glint360k dataset?
when you public mobilefacenet pre-train pytorch version ? 
"What preprocessing do I need to do for a face image if I want to correctly get its feature by using inference.py in arcface_torch? 

I need this for my other tasks, and I have bunch of face images (from FFHQ), could any one tell me what processing (land mark detection, cropping, alignment, resizing etc) do I need to do? In other words, what kind of face image is expected by the pretrained arcface model? 
"
"I have problem running test.py in gender-age
error message as follows:
PS E:\DataTraining\insightface-master\insightface-master> & ""C:/Program Files/Python36/python.exe"" e:/DataTraining/insightface-master/insightface-master/gender-age/test.py
loading ./model/model 0
[14:30:05] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:209: Loading symbol saved by previous version v1.3.0. Attempting to upgrade...
[14:30:05] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:217: Symbol successfully upgraded!
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:217: Symbol successfully upgraded!
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:217: Symbol successfully upgraded!
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:217: Symbol successfully upgraded!
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:30:08] C:\Jenkins\workspace\mxnet-tag\mxnet\src\nnvm\legacy_json_util.cc:217: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""e:/DataTraining/insightface-master/insightface-master/gender-age/test.py"", line 29, in <module>
    img = model.get_input(img)
  File ""e:\DataTraining\insightface-master\insightface-master\gender-age\face_model.py"", line 79, in get_input
    ret = self.detector.detect_face(face_img, det_type=self.args.det)
  File ""e:\DataTraining\insightface-master\insightface-master\gender-age\mtcnn_detector.py"", line 338, in detect_face
    height, width, _ = img.shape
AttributeError: 'NoneType' object has no attribute 'shape'
---------------------------------
What is the problem I am facing?
What do I need to do to fix the error?
thanks.
"
I compared partial fc (r=0.2， pytorch version) with full softmax for my dataset (1 million identities). The precision of partial fc (r=0.2) is lower than that of softmax. Could you give some advices? Thanks!
"**I have a problem running the file 'image_infer.py'**
------
Error: 
PS E:\DataTraining\insightface-master\insightface-master> & ""C:/Program Files/Python36/python.exe"" e:/DataTraining/insightface-master/insightface-master/alignment/coordinateReg/image_infer.py
loading ./model/2d106det 0
Traceback (most recent call last):
  File ""e:/DataTraining/insightface-master/insightface-master/alignment/coordinateReg/image_infer.py"", line 149, in <module>
    handler = Handler('./model/2d106det', 0, ctx_id=7, det_size=640)
  File ""e:/DataTraining/insightface-master/insightface-master/alignment/coordinateReg/image_infer.py"", line 88, in __init__ 
    self.detector.prepare(ctx_id=ctx_id)
AttributeError: 'NoneType' object has no attribute 'prepare'
---------------------
I have downloaded the model and put it in the correct link , What is the problem I am facing and what do I need to do to fix it.
thanks."
"I don't load the pretrained-model, which version of pytorch is suitable?"
"I'd like to run some codes below to use trained model to inference test set.
The thing is I saved full model to predict class not embedding. (ckpt_embedding = False)

sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
model = mx.mod.Module(symbol=sym, label_names=None, context=mx.cpu())
model.bind(for_training=False, data_shapes=[('data', (1,3,112,112)])
model.set_params(arg_params, aux_params, allow_missing=True)

but I got MXNetError
simple_bind error. Arguments:
data: (1, 3, 112, 112)
MXNetError: Error in operator one_hot0 ... : Index type must be set for one_hot operator

Does anyone know how to get class with pretrained full model?
"
"Hi there, I tried to convert scrfd model to onnx with batch size = 8. The original converting script doesn't have an option to modify batch size so I added the following code:

        model = onnx.load(ori_output_file)
        inputs = model.graph.input
        for input in inputs:
            # Checks omitted.This assumes that all inputs are tensors and have a shape with first dim.
            # Add checks as needed.
            dim1 = input.type.tensor_type.shape.dim[0]
            # update dim to be a symbolic value
            #dim1.dim_param = sym_batch_dim
            # or update it to be an actual value:
            dim1.dim_value = 8

But it doesn't work, so any idea about this?"
"I follow the instructions of ""READ FILE.txt""
Type 'Make' is to execute commands in MAKEFILE in retinaface directory?
-----------------
When I run the commands in MAKEFILE on terminal.
With :
 python setup.py build_ext --inplace
----------------
The error I got:
PS E:\DataTraining\insightface-master\insightface-master\detection\RetinaFace\rcnn\cython> python setup.py build_ext --inplace
Skipping GPU_NMS
running build_ext
Traceback (most recent call last):
  File ""setup.py"", line 164, in <module>
    cmdclass={'build_ext': custom_build_ext},
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\setuptools\__init__.py"", line 143, in setup
    return distutils.core.setup(**attrs)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\distutils\core.py"", line 148, in setup
    dist.run_commands()
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\distutils\dist.py"", line 955, in run_commands
    self.run_command(cmd)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\distutils\dist.py"", line 974, in run_command
    cmd_obj.run()
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\Cython\Distutils\old_build_ext.py"", line 186, in run
    _build_ext.build_ext.run(self)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\distutils\command\build_ext.py"", line 339, in run
    self.build_extensions()
  File ""setup.py"", line 121, in build_extensions
    customize_compiler_for_nvcc(self.compiler)
  File ""setup.py"", line 94, in customize_compiler_for_nvcc
    default_compiler_so = self.compiler_so
AttributeError: 'MSVCCompiler' object has no attribute 'compiler_so'
------------------------------------
and with the command:
cd rcnn/pycocotools/; python setup.py build_ext --inplace
---------------------------------------
then the error i get:
PS E:\DataTraining\insightface-master\insightface-master\detection\RetinaFace\rcnn\pycocotools> python setup.py build_ext --inplace
Compiling _mask.pyx because it depends on C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\Cython\Includes\numpy\__init__.pxd.
[1/1] Cythonizing _mask.pyx
C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\Cython\Compiler\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2) . This will change in a later release! File: E:\DataTraining\insightface-master\insightface-master\detection\RetinaFace\rcnn\pycocotools\_mask.pyx
  tree = Parsing.p_module(s, pxd, full_module_name)
running build_ext
building '_mask' extension
error: Unable to find vcvarsall.bat
-----------------------------------------------------------------------
before that i used :'pip install cython' to install cython module.
With the above problems I need to do?.
Thanks"
你好，非常感谢你的代码。我想用你的代码来训练自己的数据，但是数据集的标注有两个地方没看懂。5个人脸关键点是用0或者1隔开的，这个0或者1代表什么含义呢？然后标注的最后一列的置信度得分是怎么得到的呢？
"Hello.

I don't know, maybe it is somehow mxnet related or cuda related, but when i'm trying to launch image_infer (little bit different since i've placed antelope face detector) i'm getting following error
```
Could not load library libcudnn_cnn_train.so.8. Error: /home/daddywesker/anaconda3/envs/insightface/bin/../lib/libcudnn_ops_train.so.8: undefined symbol: _Z22cudnnGenericOpTensorNdILi3EE13cudnnStatus_tP12cudnnContext16cudnnGenericOp_t21cudnnNanPropagation_tPKdPKvPK17cudnnTensorStructS8_S8_SB_S8_S8_SB_Pv, version libcudnn_ops_infer.so.8
Please make sure libcudnn_cnn_train.so.8 is in your library path!
```

on the line 
`self.model.forward(db, is_train=False)`

I've tried to install cuda 11.0, 11.2, 11.3 via Nvidia's runfile (local) installation.
I've tried to apply cudnn 8.1 from nvidia site using tar installation.
I've tried to install cudnn using anaconda installation `conda install -c conda-forge cudnn`
Moreover, if i'm not installing cudnn and cudatoolkit inside anaconda  using conda install it won't find my /usr/local/cuda installation. Probably problem with anaconda environment or so.
I can't try to use cuda 10.2 since i'm using laptop with 3070 and 10.2 doesn't support 3070.  I'm actually out of ideas right now. Only if use dlib's landmarks instead, which is kinda slower. Any suggestions?"
"https://github.com/deepinsight/insightface/blob/369824fd1d75d1a353c7dffb9e1b1c591b884d48/recognition/tools/image_3d68.py#L13
may bestest  than
https://github.com/deepinsight/insightface/blob/be3f7b3e0e635b56d903d845640b048247c41c90/common/face_align.py#L59


"
"使用recognition/tools/mask_renderer.py  中加口罩方法出错
![图片](https://user-images.githubusercontent.com/39591200/122525014-5e690e80-d04b-11eb-8013-06b37ba000e2.png)

就是在~/.insightface/models/retinaface_r50_v1/ 目录下找不到retinaface_r50_v1.onnx模型文件。官方只提供了mxnet的模型文件"
"when I run test.py once in  RetinaFaceAntiCov,prediction is succesful but cause a error of ""segmentation fault:11"" .
when I run it in a python  deamo server, I found memory leaks...

"
"Hi, 

I have been trying to replicate your results and I was able to do so with the Resnet-100 backbone on the glint360k dataset but not with the Resnet-50 backbone. This makes me wonder if you have a separate config file for the Resnet-50 model and if so, could you please share the config file?

Thanks a lot!"
"When adding data to glint360k, I get wrong index in the new record file.

The new record file's index 0 header is ```HEADER(flag=2, label=array([17186136., 17547740.], dtype=float32), id=0, id2=0)```

When it should be ```HEADER(flag=2, label=array([17186135., 17547740.], dtype=float32), id=0, id2=0)```

I can confirm that the face2rec2.py file runs correctly, but still the above error happens anytime the number of ids in the dataset exceeds 360232. Why does this occur and how can I resolve it? @anxiangsir "
"I got this error after about 100k iterations:
```
W horovod/common/stall_inspector.cc:105] One or more tensors were submitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock. 
Tue Jun 15 02:26:23 2021[0]<stderr>:Stalled ranks:
```
Every time I train, I will get a deadlock in 100,000 iterations. how to fix it?"
"At `model_zoo.face_detection`  code imports mxnet and mxnet.ndarray inside FaceDetector __init__ function which makes the code unusable for example, 

```
from insightface.model_zoo.face_detection import retinaface_r50_v1
model = retinaface_r50_v1()
model.prepare(ctx_id = -1, nms=0.4)
```
which will result in
```
NameError: name 'mx' is not defined
```
importing them at top of the module will fix the issue or doing any correct way of local importing
since `model_zoo.get_model` has changed and can't be used to use the models anymore, this was the easiest way I could find to load models"
"Run scrfd.py on the t2.jpg with an onnx model converted from the pytorch model (using scrfd2onnx.py), for example scrfd_2.5g_bnkps_shape640x640.onnx, the bounding boxes and kps are shifted to the right for some of the faces of the sample image t2.jpg. See the attached.

![t2_2 5g_bnkps](https://user-images.githubusercontent.com/9027847/121816031-4d815d00-cc47-11eb-932b-d4dd56aa1418.jpg)
"
"Can anyone kindly send a working link for the IJB-B, IJB-C evaluation download files. None of the links are working on the page. "
"The SCRFD is very interesting. But I have the following concern:

It explores using the WIDER Face validation set and reports results on the validation set. It might overfit the validation set. 

Does SCRFD generalize? Or does it generalize to the WIDER Face test set? 

Thanks!
"
can you upload the pretrained models to baidu drive?
"The bin file cannot be uploaded, how can I test it?

<img width=""523"" alt=""微信图片_20210611101719"" src=""https://user-images.githubusercontent.com/85657045/121621191-43702a80-ca9e-11eb-968a-f24fc75bddc2.png"">
"
"Me when running execute the command
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore.
Error: 
--------------------
python -u train.py --network r100 --loss arcface --dataset emore
gpu num: 4
prefix ./models\r100-arcface-emore\model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=512, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, 
verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'is_shuffled_rec': False, 'fp16': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'r100', 'num_workers': 1, 'batch_size': 512, 'per_batch_size': 128}
0 1 E 3 prelu False
Network FLOPs: 24.2G
loading: ../datasets/faces_emore\train.rec False
INFO:root:loading recordio ../datasets/faces_emore\train.rec...
header0 label [5822654. 5908396.]
id2range 85742
5822653
rand_mirror True
call reset()
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
Traceback (most recent call last):
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\symbol\symbol.py"", line 1832, in simple_bind
    ctypes.byref(exe_handle)))
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\base.py"", line 246, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: Traceback (most recent call last):
  File ""C:\Jenkins\workspace\mxnet-tag\mxnet\src\storage\storage.cc"", line 119
MXNetError: Compile with USE_CUDA=1 to enable GPU usage

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 485, in <module>
    main()
  File ""train.py"", line 481, in main
    train_net(args)
  File ""train.py"", line 475, in train_net
    epoch_end_callback=epoch_cb)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\module.py"", line 429, in bind
    state_names=self._state_names)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\executor_group.py"", line 280, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\executor_group.py"", line 384, in bind_exec
    shared_group))
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\executor_group.py"", line 678, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\symbol\symbol.py"", line 1838, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
Traceback (most recent call last):
  File ""C:\Jenkins\workspace\mxnet-tag\mxnet\src\storage\storage.cc"", line 119
MXNetError: Compile with USE_CUDA=1 to enable GPU usage
--------------------------
Can you help me see what's wrong with it?. Thank you"
"http://trillionpairs.deepglint.com/submits   网站现在不能上传bin文件了吗，我这显示一直1%然后就提示
<img width=""523"" alt=""微信图片_20210610100806"" src=""https://user-images.githubusercontent.com/85657045/121453520-e4de7a00-c9d3-11eb-998d-35be180ddcc6.png"">请问有人知道是为什么吗，多谢各位大佬！"
"While using scrfd2onnx.py, what should be the values of build_model_from_cfg, generate_inputs_and_wrap_model, and preprocess_example_input in `from mmdet.core import (build_model_from_cfg, generate_inputs_and_wrap_model, preprocess_example_input)`"
I want to go through a cleaning process to eliminate photos of same identity on different datasets like between MS-1m-v1 and CASIA_webface. Is there any document about the name of the identities in the datasets?
"训练SCRFD的时候，loss全是nan
`2021-06-05 22:37:29,092 - mmdet - INFO - Epoch [1][100/403]     lr: 6.693e-04, eta: 1 day, 4:41:52, time: 0.401, data_time: 0.225, memory: 2939, loss_cls: nan, loss_bbox: nan, loss: nan

2021-06-05 22:37:48,393 - mmdet - INFO - Epoch [1][200/403]     lr: 1.335e-03, eta: 21:15:04, time: 0.193, data_time: 0.022, memory: 2939, loss_cls: nan, loss_bbox: nan, loss: nan

2021-06-05 22:38:07,722 - mmdet - INFO - Epoch [1][300/403]     lr: 2.001e-03, eta: 18:46:18, time: 0.193, data_time: 0.021, memory: 2939, loss_cls: nan, loss_bbox: nan, loss: nan

2021-06-05 22:38:26,780 - mmdet - INFO - Epoch [1][400/403]     lr: 2.667e-03, eta: 17:28:49, time: 0.191, data_time: 0.021, memory: 2939, loss_cls: nan, loss_bbox: nan, loss: nan

2021-06-05 22:39:08,506 - mmdet - INFO - Epoch [2][100/403]     lr: 3.353e-03, eta: 19:38:28, time: 0.404, data_time: 0.216, memory: 2939, loss_cls: nan, loss_bbox: nan, loss: nan

2021-06-05 22:39:27,880 - mmdet - INFO - Epoch [2][200/403]     lr: 4.019e-03, eta: 18:40:26, time: 0.194, data_time: 0.020, memory: 2939, loss_cls: nan, loss_bbox: nan, loss: nan

2021-06-05 22:39:47,128 - mmdet - INFO - Epoch [2][300/403]     lr: 4.685e-03, eta: 17:58:03, time: 0.192, data_time: 0.020, memory: 3052, loss_cls: nan, loss_bbox: nan, loss: nan`

训练代码：CUDA_VISIBLE_DEVICES=""0,1,2,3"" PORT=29701 bash ./tools/dist_train.sh ./configs/scrfd/scrfd_10g.py 4
什么参数都没改，就用作者开源的，请问下大佬是啥原因呢？谢谢@nttstar"
"Hello,

I got an error in insightface package of retinaFace. 

```
import insightface
model = insightface.model_zoo.get_model('retinaface_r50_v1')
model.prepare(ctx_id=-1, nms=0.4)
```

error:

AttributeError: 'NoneType' object has no attribute 'prepare'"
"How to participate:
[https://github.com/deepinsight/insightface/tree/master/challenges/iccv21-mfr](https://github.com/deepinsight/insightface/tree/master/challenges/iccv21-mfr)

Submission server:
http://iccv21-mfr.com/


Workshop homepage:
https://ibug.doc.ic.ac.uk/resources/masked-face-recognition-challenge-workshop-iccv-21/

Any issue can be left in this thread."
"Hi guys, 


I could not find a code to specify the epochs.
if anyone knows how to adjust epochs, pls help me.

https://github.com/deepinsight/insightface/blob/4c7ba959f3d43345e8403d77129f743aba31391a/recognition/partial_fc/mxnet/train_memory.py#L172"
"Hello, thank you for your code.

I haven't found answer to my queston, but does this insightface could detect eye's bounding box? Or only eye's center?

THanks in advance for the answer."
Is it possible to implement ArcFace in golang using python onnx models?
"I used arcface_torch to train with my own dataset as follow:
CUDA_VISIBLE_DEVICES=0,1,2,3 nohup python -m torch.distributed.launch --nproc_per_node=4 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=1234 train.py --network r50 --loss arcface
But it encountered the loss NAN problem as follow:
Training: 2021-06-01 16:22:55,751-Speed 219.17 samples/sec   Loss 2.3655   Epoch: 0   Global Step: 14550   Required: 102 hours
Training: 2021-06-01 16:23:39,606-Speed 218.91 samples/sec   Loss 2.4382   Epoch: 0   Global Step: 14600   Required: 102 hours
Training: 2021-06-01 16:24:23,457-Speed 218.93 samples/sec   Loss 2.4757   Epoch: 0   Global Step: 14650   Required: 102 hours
Training: 2021-06-01 16:25:07,307-Speed 218.93 samples/sec   Loss 2.4868   Epoch: 0   Global Step: 14700   Required: 102 hours
Training: 2021-06-01 16:25:51,183-Speed 218.82 samples/sec   Loss nan   Epoch: 0   Global Step: 14750   Required: 102 hours
Training: 2021-06-01 16:26:33,973-Speed 224.36 samples/sec   Loss nan   Epoch: 0   Global Step: 14800   Required: 102 hours
Training: 2021-06-01 16:27:16,771-Speed 224.32 samples/sec   Loss nan   Epoch: 0   Global Step: 14850   Required: 102 hours
Training: 2021-06-01 16:27:59,556-Speed 224.38 samples/sec   Loss nan   Epoch: 0   Global Step: 14900   Required: 102 hours
Training: 2021-06-01 16:28:42,450-Speed 223.81 samples/sec   Loss nan   Epoch: 0   Global Step: 14950   Required: 102 hours

Someone said that reducing LR can avoid this mistake.
but even I reduced the learning rate to 0.001, it still happens."
"Does anyone have a requirements.txt with all the dependencies needed for the model to work? 

I am trying to start it in pycharm.

I am struggling for 2 weeks with various issues. And i can't manage to start the model. 

A lot of errors with Cuda, i have tried with Cuda 9.0 and it was crashing, with Cuda10.2 also and with Cuda 11.3 as well.

Pytorch sees my gpu, but somehow i get and error that i don't have any cuda devices.

I am new to all of this and i would appreciate the help. I am trying to start it on a Ge force gtx 1060.

C:\Users\Andrei\FaceRecognition\insightface\venv_38\Scripts\python.exe C:/Users/Andrei/FaceRecognition/insightface/recognition/ArcFace/train.py
Traceback (most recent call last):
  File ""C:/Users/Andrei/FaceRecognition/insightface/recognition/ArcFace/train.py"", line 13, in <module>
    import mxnet as mx
  File ""C:\Users\Andrei\FaceRecognition\insightface\venv_38\lib\site-packages\mxnet\__init__.py"", line 82, in <module>
    from . import gluon
  File ""C:\Users\Andrei\FaceRecognition\insightface\venv_38\lib\site-packages\mxnet\gluon\__init__.py"", line 36, in <module>
    from . import data
  File ""C:\Users\Andrei\FaceRecognition\insightface\venv_38\lib\site-packages\mxnet\gluon\data\__init__.py"", line 26, in <module>
    from .dataloader import *
  File ""C:\Users\Andrei\FaceRecognition\insightface\venv_38\lib\site-packages\mxnet\gluon\data\dataloader.py"", line 278
    generator = lambda: [(yield self._batchify_fn([self._dataset[idx] for idx in batch]))
                        ^
SyntaxError: 'yield' inside list comprehension

Process finished with exit code 1"
"def cosin_metric(x1, x2):
    return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))
我使用上述公式计算两个人脸特征的相似度评分，相同id的特征评分大约再0.4左右，请问有什么方法去提升这个评分，让它接近1？"
"how can i get that ""/model/mnet_cov2-symbol.json"" file?"
and  Is speed benchmark based on ms1mv3 dataset？
"scrfd.py have a key error.

height = input_height // stride
width = input_width // stride

height = math.ceil(input_height / stride)
width = math.ceil(input_width / stride)"
"Hi, I am testing ""webcam_demo.py"" in insightface/scrfd.
However, I could not do it cuz 

"""" KeyError: 'SCRFD is not in the models registry """"

Im using pre-trained model ::
configs/scrfd/SCRFD_2.5G_BNKPS.py
checkpoints/SCRFD_2.5G_KPS.pth

I cant find how to do. Can u help me ??
Thanks"
"Hi @nttstar 

We cant download from Baidu Torch model (Baidu Yun Pan: e8pw).

Is it possible to put DropBox ?"
"First of all, nice repo and big congratulation on your achievement in the NIST-FRVT 1:1 competition!

I've got a few questions which I cant seem to answer by screening through the codebase.

I noticed the included modules are mainly, face detection, alignment and also recognition. 

Is there face verification module with pretrained model available?

Thanks a ton!"
"I have enabled the eval_metric,  but there is no any acc ad lossvalue during training.

""""""
    metric1 = AccMetric()
    eval_metrics = [mx.metric.create(metric1)]
    if config.ce_loss:
        metric2 = LossValueMetric()
        eval_metrics.append(mx.metric.create(metric2))
    model.fit(
        train_dataiter,
        begin_epoch=begin_epoch,
        num_epoch=999999,
        eval_data=val_dataiter,
        eval_metric = eval_metrics,
        kvstore=args.kvstore,
        optimizer=[opt, opt_fc7],
        #optimizer_params = optimizer_params,
        initializer=initializer,
        arg_params=arg_params,
        aux_params=aux_params,
        allow_missing=True,
        batch_end_callback=_batch_callback,
        epoch_end_callback=epoch_cb)
""""""

""""""
INFO:root:Iter[0] Batch [2320]  Speed: 1260.89 samples/sec
INFO:root:Iter[0] Batch [2340]  Speed: 1264.86 samples/sec
INFO:root:Iter[0] Batch [2360]  Speed: 1240.20 samples/sec
INFO:root:Iter[0] Batch [2380]  Speed: 1259.37 samples/sec
INFO:root:Iter[0] Batch [2400]  Speed: 1260.10 samples/sec
INFO:root:Iter[0] Batch [2420]  Speed: 1262.98 samples/sec
INFO:root:Iter[0] Batch [2440]  Speed: 1260.58 samples/sec
INFO:root:Iter[0] Batch [2460]  Speed: 1265.03 samples/sec
INFO:root:Iter[0] Batch [2480]  Speed: 1266.52 samples/sec
INFO:root:Iter[0] Batch [2500]  Speed: 1274.07 samples/sec
INFO:root:Iter[0] Batch [2520]  Speed: 1241.78 samples/sec
INFO:root:Iter[0] Batch [2540]  Speed: 1264.50 samples/sec
INFO:root:Iter[0] Batch [2560]  Speed: 1268.71 samples/sec
INFO:root:Iter[0] Batch [2580]  Speed: 1265.33 samples/sec
INFO:root:Iter[0] Batch [2600]  Speed: 1255.14 samples/sec
INFO:root:Iter[0] Batch [2620]  Speed: 1274.51 samples/sec
""""""


Look forward your help!"
"when use insightface.app.FaceAnalysis(name):
there is no onnx,
meanwhile ,when use insightface==0.1.5:
cannot open url to load weights,like this: 
**RuntimeError: Failed downloading url 
http://insightface.ai/files/models/retinaface_r50_v1.zip**
what is the problem?
"
"Telegram group Ingles language

https://t.me/joinchat/4GVnP8oI6y8xYzA5"
"Why do you use a square image when proceeding with face recognition? 
An error occurs when using a rectangular image, and I want to solve it.
"
"The given link for dropbox to download the retinaface_gt_v1.1.zip is not working. Could you please give me a link to downloaded that? @yingfeng @szad670401 @anxiangsir 

https://www.dropbox.com/s/7j70r3eeepe4r2g/retinaface_gt_v1.1.zip?dl=0

This is not working.

Thank you.
Anuraj"
"can anyone please share the google drive link of MS1M-Retinaface dataset, unable to access baidu link"
"Hi,

Thank you for this amazing project, the work you're doing here is great. 

I evaluated the pretrained pytorch R100 partialFC model against the equivalent Mxnet model and saw that the Mxnet model does better on IJB-C. Furthermore, when I try to retrain the partialFC model using the arcface_torch module, I still see the same result (Mxnet < Pytorch). 

Were you able to replicate the Mxnet result with Pytorch? If so, can you share the training strategy you followed to make that model.

Thank you again for the excellent work. "
as the title
Are the retinaface pretrained models going to be reuploaded to dropbox?
"I run ""git clone --recursive https://github.com/deepinsight/insightface.git"" and then I want to test the pre-trained models,but I didn't find the dirctory ""$INSIGHTFACE/src/eval"""
"Since the models is on the google drives, it is difficult to download them in china. please put the models on the baidu yun driver"
"I want to extract 512-D Feature Embedding. I download the pretrained model and run the /insightface/deploy/test.py, but 
![image](https://user-images.githubusercontent.com/44988023/118638371-5b3ef080-b809-11eb-8752-71dda50d4c38.png)
what should I do?
"
"新的glint360k merge了之后正常了,但是train merga_data的时候出现一个问题
```
if header.flag>0:
              print('header0 label', header.label)
              self.header0 = (int(header.label[0]), int(header.label[1]))
              self.imgidx = []
              self.id2range = {}
              self.seq_identity = range(int(header.label[0]), int(header.label[1]))
              for identity in self.seq_identity:
                s = self.imgrec.read_idx(identity)
                header, _ = recordio.unpack(s)        
                print('header000 label', header.label)
```
里面的header0 label [18322620. 18732154], header000 label 409532.0, header000不是一个区间所以报错。这是什么原因呢？我分别train merge前的datasets都是可以train的。"
"I want to modify the megaface test process and get the thresholds or other results used in magaface test.
But the results on megaface are given by some execution file (maybe written by C I guess?).
So how can I dig into the source code?

_Originally posted by @liquor233 in https://github.com/deepinsight/insightface/issues/242#issuecomment-841808422_"
"When I want to evalute my model on MegaFace dataset, I find the Deployment Kit needed to be downloaded as (http://megaface.cs.washington.edu/participate/challenge.html). A week passed after I filled out the form requested, but I haven't receive any information. So anyone can share the Deployment Kit for me? Thank a lot!"
RT
"Hi! I'm testing your new SCRFD face detector and have noticed some issues with onnx inference code and network outputs:

1. In  [scrfd.py](https://github.com/deepinsight/insightface/blob/f85e523d1355ef26c837e6381e3415a841b3a041/detection/scrfd/tools/scrfd.py#L275) line 275 you are filtering `bboxes`, but later at line 278 you return `det`, so `max_num` parameter have no effect and may cause exceptions.

2. Later at line [335](https://github.com/deepinsight/insightface/blob/f85e523d1355ef26c837e6381e3415a841b3a041/detection/scrfd/tools/scrfd.py#L335) you are calling detector without providing input shape, which wont work with model having dynamic shape. However it won't be an issue when called from `face_analysis.py` 

3. I have noticed that detector returns very low scores or even fails on faces occupying >40% of image, it's especially visible for square shaped images, when there can't be provided additional padding during resize process. Also I have noticed that in such cases accuracy increases when lowering detection size (i.e. 480x480), and decreases when increasing it (i.e 1024x1024). 
    Here is an example of detection at 640x640 scale: 
![1](https://user-images.githubusercontent.com/17834919/118366122-1bc59980-b5a8-11eb-8e07-cf17b797fcf4.jpg)
Original image size is 1200x1200.
As you can see when detection is run with resize to 640x640 score is 0.38
For `480x480` score is `0.86`, and for `736x736` score is `0.07`.
Same behavior is noticed for both `scrfd_10g_bnkps` and `scrfd_2.5g_bnkps` models.
In some cases it might be fixed by adding fixed padding around image, but it might lead to decreased accuracy for other image types, so it can't be applied by default.

BTW: Thanks for your great work!"
"Hey there! - I really appreciate that insightface uses [interrogate](https://github.com/econchick/interrogate)! It's super flattering!

I'm wondering, may I mention that insightface uses interrogate on our [docs site](https://interrogate.readthedocs.io/)? Something just as simple as a section of ""Projects that use Interrogate"" with the name and link under it. If you'd like, I can include the insightface logo, and/or even a quote about why it's used 😅

I totally understand if you'd prefer not to! I won't put anything up until I hear from you.

Thanks!"
"Hi

Thanks for this awesome project! Unfortunately all dropbox links in dataset zoo are dead (404 Error). Could you please renew them? I am looking for Casia Webface dataset.

"
"hi，I test scrfd_2.5g model on Tesla P40 GPU, each image has a speed of 20 milliseconds, could not reach 4.2 milliseconds"
"I try to test my own model on my own dataset,but there is only eval_ijbc.py that doesn't support any dataset except ijbc. And i find verification.py in the eval folder but i don't know how to use it. Can anyone please tell me how to test the accuracy?"
I think the MS1M-Arcface link is dead. Can you renew the link? Thank you.
"While running the insightface/recognition/arcface_torch/eval_ijbc.py, I encounter the following error. What is the problem?

```
Time: 0.40 s. 
Time: 3.46 s. 
files: 469375
Traceback (most recent call last):

  File ""/home/tw/Projects/torch/insightface/recognition/arcface_torch/eval_ijbc.py"", line 370, in <module>
    model_path, 0, gpu_id)

  File ""/home/tw/Projects/torch/insightface/recognition/arcface_torch/eval_ijbc.py"", line 168, in get_image_feature
    embedding = Embedding(model_path, data_shape, batch_size)

  File ""/home/tw/Projects/torch/insightface/recognition/arcface_torch/eval_ijbc.py"", line 62, in __init__
    resnet.load_state_dict(weight)

  File ""/home/tw/.virtualenvs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 1045, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))

RuntimeError: Error(s) in loading state_dict for IResNet:
	Missing key(s) in state_dict: ""layer1.2.bn1.weight"", ""layer1.2.bn1.bias"", ""layer1.2.bn1.running_mean"",
```"
"How to run a pytorch pre-trained model given at https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch ?
I'm new to pytorch and expect a python file like deploy/test.py"
"Hi, thank you for the good work.

How to use batchsize in onnxruntime? "
"Dataset is no longer available on DropBox. Could you update it, or refer another platform to download it and work with it?"
"Download is not available from the current link
Are there any other download links?"
"Thanks for your work, it's a great project.

However, it's unclear to train the own datasets.

I find can using `face2rec2.py` to generate datasets for training, however, could you offer a document details describle how to make the own datasets? Because the project changed a lot and I have no idea how to train the own datasets.

Thanks, looking forward to your reply."
"Hi, where can I find and download the most up-to-date model - ArcFace-100? I think I have the previous version of the model at the moment, and I saw that there was a new one in the NIST benchmark.

Thanks"
"when train arcface_torch
python -m torch.distributed.launch --nproc_per_node=1 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=1234 train.py
"
"Hello, I'm getting the following error while I'm trying to train the Retinaface algorithm with the Widerface dataset:

Traceback (most recent call last):
  File ""train.py"", line 502, in <module>
    main()
  File ""train.py"", line 490, in main
    train_net(args,
  File ""train.py"", line 66, in train_net
    roidbs = [
  File ""train.py"", line 67, in <listcomp>
    load_gt_roidb(args.dataset,
  File ""/home/rh667/Desktop/FP-Projects/insightface-master/detection/RetinaFace/rcnn/utils/load_data.py"", line 13, in load_gt_roidb
    imdb = eval(dataset_name)(image_set_name, root_path, dataset_path)
  File ""/home/rh667/Desktop/FP-Projects/insightface-master/detection/RetinaFace/rcnn/dataset/retinaface.py"", line 42, in __init__
    assert name is not None
AssertionError

Any idea to solve this error?
"
"Hi, thank you for the good work.

I have converted arcface model from both mxnet and pytorch to onnx but when I tested this on LFW, there is a performance drop (from 99% to 90%).  I wanted to ask if this is just me, or if anyone else noticed this too. "
"In insightface.app, how do I specify the folder path where I want insightface to download models to?  "
""
"Why do we explicitly need to code the backward part in forward_backward in partial_fc.py for arcface_torch?

can't PyTorch auto-differentiation take care of this?

I would really appreciate help on this.

Thanks"
"Where is the src folder? I couldn't find it in the repo, could anyone share a link? thanks!"
"In the verification.py code, there is a dist list.

I want to know the range. I was trying to figure it out. But it's so hard.

or is there any figure which can be used for comparing image's matching rate?"
Why does loss NaN appear when the loss drops by 3% in the ARCFACE_TORCH training?Can you give me some advice
"Dear @anxiangsir :

i changed the code into this (add the gradient of two classification loss then backward), but find the result is very bad...and finally Nan encountered!!

may i ask what's wrong in my revision?
the input of my network is image pairs, in each pair, two images share one same label.

if [partial_fc.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/partial_fc.py), i changed it into
```

    def forward_backward(self, label, features, optimizer):
        total_label, norm_weight = self.prepare(label, optimizer)
        total_features = torch.zeros(
            size=[self.batch_size * self.world_size, self.embedding_size], device=self.device)

        # copy features to different gpus
        dist.all_gather(list(total_features.chunk(self.world_size, dim=0)), features.data)
        total_features.requires_grad = True

        logits = self.forward(total_features, norm_weight)
        logits = self.margin_softmax(logits, total_label)

        with torch.no_grad():
            max_fc = torch.max(logits, dim=1, keepdim=True)[0]
            dist.all_reduce(max_fc, dist.ReduceOp.MAX)

            # calculate exp(logits) and all-reduce
            logits_exp = torch.exp(logits - max_fc)
            logits_sum_exp = logits_exp.sum(dim=1, keepdims=True)
            dist.all_reduce(logits_sum_exp, dist.ReduceOp.SUM)

            # calculate prob
            logits_exp.div_(logits_sum_exp)

            # get one-hot
            grad = logits_exp
            index = torch.where(total_label != -1)[0]
            one_hot = torch.zeros(size=[index.size()[0], grad.size()[1]], device=grad.device)
            one_hot.scatter_(1, total_label[index, None], 1)

            # calculate loss
            loss = torch.zeros(grad.size()[0], 1, device=grad.device)
            loss[index] = grad[index].gather(1, total_label[index, None])
            dist.all_reduce(loss, dist.ReduceOp.SUM)
            loss_v = loss.clamp_min_(1e-30).log_().mean() * (-1)

            # calculate grad
            grad[index] -= one_hot
            grad.div_(self.batch_size * self.world_size)

        # logits.backward(grad)
        return grad, logits,loss_v, total_features
```

in main() of [train.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/train.py), i changed it into:
```

def main(args):
    world_size = int(os.environ['WORLD_SIZE'])
    rank = int(os.environ['RANK'])
    dist_url = ""tcp://{}:{}"".format(os.environ[""MASTER_ADDR""], os.environ[""MASTER_PORT""])
    dist.init_process_group(backend='nccl', init_method=dist_url, rank=rank, world_size=world_size)
    local_rank = args.local_rank
    torch.cuda.set_device(local_rank)

    if not os.path.exists(cfg.output) and rank is 0:
        os.makedirs(cfg.output)
    else:
        time.sleep(2)

    log_root = logging.getLogger()
    init_logging(log_root, rank, cfg.output)
    trainset = MXFaceDataset(root_dir=cfg.rec, local_rank=local_rank)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        trainset, shuffle=True)
    train_loader = DataLoaderX(
        local_rank=local_rank, dataset=trainset, batch_size=cfg.batch_size,
        sampler=train_sampler, num_workers=0, pin_memory=True, drop_last=True,
    worker_init_fn = worker_init_fn)  # for reproducibility 

    dropout = 0.4 if cfg.dataset is ""webface"" else 0
    backbone = eval(""backbones.{}"".format(args.network))(False, dropout=dropout, fp16=cfg.fp16).to(local_rank)

    if args.resume:
        try:
            backbone_pth = os.path.join(cfg.output, ""backbone.pth"")
            backbone.load_state_dict(torch.load(backbone_pth, map_location=torch.device(local_rank)))
            if rank is 0:
                logging.info(""backbone resume successfully!"")
        except (FileNotFoundError, KeyError, IndexError, RuntimeError):
            logging.info(""resume fail, backbone init successfully!"")

    for ps in backbone.parameters():
        dist.broadcast(ps, 0)
    backbone = torch.nn.parallel.DistributedDataParallel(
        module=backbone, broadcast_buffers=False, device_ids=[local_rank])
    backbone.train()

    margin_softmax = eval(""losses.{}"".format(args.loss))()
    module_partial_fc = PartialFC(
        rank=rank, local_rank=local_rank, world_size=world_size, resume=args.resume,
        batch_size=cfg.batch_size, margin_softmax=margin_softmax, num_classes=cfg.num_classes,
        sample_rate=cfg.sample_rate, embedding_size=cfg.embedding_size, prefix=cfg.output)

    opt_backbone = torch.optim.SGD(
        params=[{'params': backbone.parameters()}],
        lr=cfg.lr / 512 * cfg.batch_size * world_size,
        momentum=0.9, weight_decay=cfg.weight_decay)
    opt_pfc = torch.optim.SGD(
        params=[{'params': module_partial_fc.parameters()}],
        lr=cfg.lr / 512 * cfg.batch_size * world_size,
        momentum=0.9, weight_decay=cfg.weight_decay)

    scheduler_backbone = torch.optim.lr_scheduler.LambdaLR(
        optimizer=opt_backbone, lr_lambda=cfg.lr_func)
    scheduler_pfc = torch.optim.lr_scheduler.LambdaLR(
        optimizer=opt_pfc, lr_lambda=cfg.lr_func)

    start_epoch = 0
    total_step = int(len(trainset) / cfg.batch_size / world_size * cfg.num_epoch)
    if rank is 0: logging.info(""Total Step is: %d"" % total_step)

    interv = 400 
    callback_verification = CallBackVerification(interv, rank, cfg.val_targets, cfg.rec) # 150 for debug, self.frequent = 1000
    callback_logging = CallBackLogging(50, rank, total_step, cfg.batch_size, world_size, None) # verbose = 50
    callback_checkpoint = CallBackModelCheckpoint(interv, rank, cfg.output)

    loss1 = AverageMeter()
    loss2 = AverageMeter()

    global_step = 0

    grad_scaler = MaxClipGradScaler(cfg.batch_size, 128 * cfg.batch_size, growth_interval=100) if cfg.fp16 else None
    for epoch in range(start_epoch, cfg.num_epoch):
        train_sampler.set_epoch(epoch)
        for step, (img, label) in enumerate(train_loader):
            global_step += 1
            # features = F.normalize(backbone(img))
            
            img1, img2 = img[:,:,:,:112], img[:,:,:,112:]
            assert not cfg.fp16

            # branch 1
            fc1Features1 = F.normalize(backbone(img1))
            fc7grad1, logits1, loss_v1, total_features1 = module_partial_fc.forward_backward(label, fc1Features1, opt_pfc)

            fc1Features2 = F.normalize(backbone(img2))
            fc7grad2, logits2, loss_v2, total_features2 = module_partial_fc.forward_backward(label, fc1Features2, opt_pfc)

            fc7addGrad = fc7grad1  + fc7grad2 
            logits1.backward(fc7addGrad)

            if total_features1.grad is not None:
                total_features1.grad.detach_()
            x_grad1: torch.Tensor = torch.zeros_like(fc1Features1, requires_grad=True)
            # feature gradient all-reduce
            dist.reduce_scatter(x_grad1, list(total_features1.grad.chunk(world_size, dim=0)))
            x_grad1 = x_grad1 * world_size
            fc1Features1.backward(x_grad1)
 
            clip_grad_norm_(backbone.parameters(), max_norm=5, norm_type=2)
            opt_backbone.step()
            opt_pfc.step()
            module_partial_fc.update()
            opt_backbone.zero_grad()
            opt_pfc.zero_grad()
            loss1.update(loss_v1, 1)
            loss2.update(loss_v2, 1)

            if global_step > interv and global_step % interv == 0 and dist.get_rank()==0:
                lr = opt_backbone.state_dict()['param_groups'][0]['lr']
                logging.info('LR %s'%lr)

            callback_logging(global_step, loss1, loss2, epoch, cfg.fp16, grad_scaler)
            callback_verification(global_step, backbone)
            callback_checkpoint(global_step, backbone, module_partial_fc)

        scheduler_backbone.step()
        scheduler_pfc.step()

    dist.destroy_process_group()
```

the results:

```
Training: 2021-04-29 17:16:38,932-Speed 2689.27 samples/sec   Loss1 43.6098   Loss2 44.0652   Epoch: 3   Global Step: 2400   Required: 3 hours
testing verification..
(12000, 512)
infer time 19.456014000000057
Training: 2021-04-29 17:17:01,106-[lfw][2400]XNorm: 22.560986
Training: 2021-04-29 17:17:01,107-[lfw][2400]Accuracy-Flip: 0.49983+-0.00050
Training: 2021-04-29 17:17:01,107-[lfw][2400]Accuracy-Highest: 0.50400
testing verification..
(14000, 512)
infer time 22.705947999999964
Training: 2021-04-29 17:17:26,860-[cfp_fp][2400]XNorm: 25.310066
Training: 2021-04-29 17:17:26,860-[cfp_fp][2400]Accuracy-Flip: 0.50014+-0.00119
Training: 2021-04-29 17:17:26,860-[cfp_fp][2400]Accuracy-Highest: 0.50314
testing verification..
(12000, 512)
infer time 19.476336999999983
Training: 2021-04-29 17:17:49,061-[agedb_30][2400]XNorm: 27.397241
Training: 2021-04-29 17:17:49,061-[agedb_30][2400]Accuracy-Flip: 0.50050+-0.00107
Training: 2021-04-29 17:17:49,061-[agedb_30][2400]Accuracy-Highest: 0.51433

...

Training: 2021-04-29 17:52:21,601-Speed 2653.84 samples/sec   Loss1 43.9708   Loss2 43.6432   Epoch: 8   Global Step: 5600   Required: 3 hours
Training: 2021-04-29 17:52:44,205-[lfw][5600]XNorm: 21.012763
Training: 2021-04-29 17:52:44,206-[lfw][5600]Accuracy-Flip: 0.49983+-0.00050
Training: 2021-04-29 17:52:44,206-[lfw][5600]Accuracy-Highest: 0.50400
Training: 2021-04-29 17:53:10,282-[cfp_fp][5600]XNorm: 17.821486
Training: 2021-04-29 17:53:10,282-[cfp_fp][5600]Accuracy-Flip: 0.50043+-0.00181
Training: 2021-04-29 17:53:10,282-[cfp_fp][5600]Accuracy-Highest: 0.50314
Training: 2021-04-29 17:53:32,873-[agedb_30][5600]XNorm: 19.934401
Training: 2021-04-29 17:53:32,873-[agedb_30][5600]Accuracy-Flip: 0.50000+-0.00236
Training: 2021-04-29 17:53:32,873-[agedb_30][5600]Accuracy-Highest: 0.51433
Training: 2021-04-29 17:53:57,506-Speed 688.18 samples/sec   Loss1 43.8332   Loss2 43.9438   Epoch: 8   Global Step: 5650   Required: 3 hours
Training: 2021-04-29 17:54:21,778-Speed 2719.32 samples/sec   Loss1 43.8296   Loss2 43.8148   Epoch: 8   Global Step: 5700   Required: 3 hours
Training: 2021-04-29 17:54:46,199-Speed 2702.61 samples/sec   Loss1 44.0131   Loss2 43.6397   Epoch: 8   Global Step: 5750   Required: 3 hours
Training: 2021-04-29 17:55:10,660-Speed 2698.16 samples/sec   Loss1 43.9690   Loss2 43.6218   Epoch: 8   Global Step: 5800   Required: 3 hours
Training: 2021-04-29 17:55:35,123-Speed 2698.04 samples/sec   Loss1 43.6915   Loss2 43.9429   Epoch: 8   Global Step: 5850   Required: 3 hours
Training: 2021-04-29 17:55:59,642-Speed 2691.88 samples/sec   Loss1 43.9401   Loss2 43.6547   Epoch: 8   Global Step: 5900   Required: 3 hours
Training: 2021-04-29 17:56:25,154-Speed 2587.05 samples/sec   Loss1 43.7101   Loss2 43.9016   Epoch: 9   Global Step: 5950   Required: 3 hours
Training: 2021-04-29 17:56:49,175-LR 0.025781250000000002
Training: 2021-04-29 17:56:49,176-Speed 2747.59 samples/sec   Loss1 nan   Loss2 nan   Epoch: 9   Global Step: 6000   Required: 3 hours
```
"
"I basically follow the step which described in this link
https://github.com/umairanis03/facial-recognition/blob/master/validation_prep.md

when i run 
python generate_pairs.py --image_dir data --pairs_file_name pairs.txt --num_folds 2 --num_matches_mismatches 3

it never ends.. i waited 3 hours and it was still working without any message. 

But if i change the parameter num_matches_mismatches 0, it made a txt file.

what does ""num_matches_mismatches"" mean? and how can i make pairs.txt file?"
"I am not able to download glint360k dataset even from torrent as no one is seeding. Its been on my torrent client since last month.
can anyone seed and update please!

Hello, we have released glint360k's downloading magnet link,  you can try it.

https://github.com/deepinsight/insightface/blob/master/recognition/partial_fc/README.md#download

_Originally posted by @anxiangsir in https://github.com/deepinsight/insightface/issues/1264#issuecomment-717100093_"
"
"
"python train.py 
MXNetError: Check failed: compileResult == NVRTC_SUCCESS (5 vs. 0) : NVRTC Compilation failed. Please set environment variable MXNET_USE_FUSION to 0.
nvrtc: error: invalid value for --gpu-architecture (-arch)

who know how to solve it?   thanks~~~~~~~~~~~~·


"
"I m training arcface, my current situation is as follow:

![image](https://user-images.githubusercontent.com/19206896/115679693-83971480-a35b-11eb-8171-c5c65c392040.png)

![image](https://user-images.githubusercontent.com/19206896/115679913-c0630b80-a35b-11eb-969d-59f705553bf7.png)
 
my learning rate is (0.000001)

also my (acc, lossvalue) looks like stable and does not change.

is it correct for training or I have to stay for more computation"
"In the coordinate regression for facial landmarks, is the model: Model 2d106track, available?

"
"When I run the following command:
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore
Then the following message showing:
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
Killed

Any solution to complete the process?
"
"It seems that the Baidu Pan link of MS1M-RetinaFace in  https://github.com/deepinsight/insightface/wiki/Dataset-Zoo is named as MS1M-RetinaFace-T1.
And in arcface_torch, some models are trained on MS1MV3. So where can I get MS1MV3 dataset (I think it is MS1M-RetinaFace-T2, but now sure...)?

Thank you!"
"Hey,
Can anyone provide a link to download the pre-trained arcface pytorch weights **not** from the Baidu app? "
"face-pose-detect，face-detect and face-key-point-detect merge
link: https://github.com/gengyanlei/Pytorch_Retinaface

人脸姿态估计+人脸检测+人脸关键点检测 3个任务合并至1个任务链接：
https://github.com/gengyanlei/Pytorch_Retinaface

"
"I have a dataset image here, how could I fine-tune model with my dataset using triplet loss? "
"Hi team, 

We are trying to finetune the mobilenet model available in the model-zoo using asian-glint dataset using combined loss function. The training accuracy stays in 0.3 even after 50 epochs and the loss is around 7. Can you please share your training logs and your training config for the same. "
"Hi,
This more a feature request, can you make a guide on how I can use the arcface_r50_v1 pre-train model as part of my model and update its weights, and ready for adding one or two ""fine-tuning"" layers to it.
Honestly, I wanted to do it my self but I lack the skills and knowledge to do so
preferably with Keras and TensorFlow

Thanks
"
"Hi, i'm new to machine learning

I have a faces database - it is just a directory with faces image which are named as man's name on photo. I need to recognize all people from the database in input image
Can you please tell me how to run face recognition module to complete this task? I dont see test.py

Thanks"
"Is training strategy adopted in arcface_pytorch same as described in arcface paper or is it like descibed in patial FC paper ?

Asking since, need to understand how model parallel trategy differes in both the cases?"
"File ""E:\env\lib\site-packages\torch\distributed\rendezvous.py"", line 82, in rendezvous
    
raise RuntimeError(""No rendezvous handler for {}://"".format(result.scheme))
RuntimeError: No rendezvous handler for tcp://

在执行arcface_pytorch的train.py时碰到了这个错误，请问这是由于什么原因导致的，该怎么解决呢
"
"I m using google colab pro for training arcface (mxnet) model with 'emore' dataset

I have tried several time for training because my resource have been disconnected every 20 hours.

also my model would not completely trained with this time 

![image](https://user-images.githubusercontent.com/19206896/113500333-c2d50100-9525-11eb-8809-c81bb817e450.png)

also every time while saving checkpoint it will be saved to the same file './models/r50-arcface-emore/model-0001.params'

if someone let me know how to save every check point to the different file or any other solution for return to the last status and iteration after reconnect to my resources."
"I ran mask augmentation in recognition/tools with a sample image, but my result differ dramatically from posted in repo
![image](https://user-images.githubusercontent.com/32299228/113306120-6b0f7d80-930c-11eb-816d-101ae0d4ed87.png)
![image](https://user-images.githubusercontent.com/32299228/113306260-9003f080-930c-11eb-9bc0-83ee31431b89.png)
What  causes that?
"
"currently I m training arcface model with rasnet50,

![image](https://user-images.githubusercontent.com/19206896/113142476-95d9d300-9233-11eb-8745-f0a908644bca.png)

any one can review my current result?
also what is my current epoch?
how many epoch I should take?"
"
![2021-03-31 172251](https://user-images.githubusercontent.com/44435640/113123044-cbd48280-9246-11eb-8067-5f3a62bc2861.png)

when I run insightface-master/detection/RetinaFace/test.py with GPU, I got this error message, How do I deal with this problem? Thanks."
"Training: 2021-03-31 11:28:15,352-Speed 415.63 samples/sec   Loss 36.7039   Epoch: 1   Global Step: 2000   Required: 1 hours
testing verification..
(472, 512)
infer time 1.2834219999999998
Traceback (most recent call last):
  File ""train.py"", line 132, in <module>
    main(args_)
  File ""train.py"", line 118, in main
    callback_verification(global_step, backbone)
  File ""/root/arcface_torch/utils/utils_callbacks.py"", line 48, in __call__
    self.ver_test(backbone, num_update)
  File ""/root/arcface_torch/utils/utils_callbacks.py"", line 27, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(
  File ""/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/root/arcface_torch/eval/verification.py"", line 275, in test
    _, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=nfolds)
  File ""/root/arcface_torch/eval/verification.py"", line 191, in evaluate
    val, val_std, far = calculate_val(thresholds,
  File ""/root/arcface_torch/eval/verification.py"", line 148, in calculate_val
    _, far_train[threshold_idx] = calculate_val_far(
  File ""/root/arcface_torch/eval/verification.py"", line 175, in calculate_val_far
    far = float(false_accept) / float(n_diff)
ZeroDivisionError: float division by zero
Killing subprocess 3993
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/launch.py"", line 340, in <module>
    main()
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/launch.py"", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File ""/usr/local/lib/python3.8/dist-packages/torch/distributed/launch.py"", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/local/bin/python', '-u', 'train.py', '--local_rank=0']' returned non-zero exit status 1.
"
"Hi,

https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/backbones/iresnet.py#L152

when training arcface-torch, you guys convert last fc layer to fp32.

why do you convert last layer to fc32?"
""
"Hi @yingfeng @nttstar @ppwwyyxx @leondgarse @jiankangdeng 
Every boby can share  the paper ""D. Zhang.  A distributed training solution for face recogni-tion.DeepGlint, 2018. 9"" in Arcface paper via e-mail: thanhphq87@gmail.com
Thanks
Best regrads, 
PeterPham "
"您好我在训练启动时发生了错误能否得到您的帮助
![image](https://user-images.githubusercontent.com/57883284/112297982-7a217a80-8cd1-11eb-893d-e52ea0d5f1d4.png)
"
"https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch

Can you please provide google drive link to download?"
"while i trained the arcface pytorch with 2 gpus , which used the command below , it worked fine :
`python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=1234 train.py`

but failed by set the nproc_per_node=4 , my command is :
`python -m torch.distributed.launch --nproc_per_node=4 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=1234 train.py`

the error is:
```
  File ""train.py"", line 135, in <module>
Traceback (most recent call last):
  File ""train.py"", line 135, in <module>
    main(args_)
  File ""train.py"", line 26, in main
    rank=args.local_rank)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 455, in init_process_group
    barrier()
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 1960, in barrier
    main(args_)
  File ""train.py"", line 26, in main
    rank=args.local_rank)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 455, in init_process_group
    barrier()
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 1960, in barrier
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
Traceback (most recent call last):
  File ""train.py"", line 135, in <module>
    main(args_)
  File ""train.py"", line 26, in main
    rank=args.local_rank)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 455, in init_process_group
    barrier()
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 1960, in barrier
    work = _default_pg.barrier()
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:784, unhandled system error, NCCL version 2.7.8
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py"", line 260, in <module>
    main()
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py"", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'train.py', '--local_rank=3']' returned non-zero exit status 1.
```

@anxiangsir do you have any idea ? great thanks  ! 
"
"easydict  
mxnet  
sklearn  "
"when running insightface's vectorizing functions with `joblib.Parallel` jobs, they complete, but dont return (hangs). Found [this joblib issue](https://github.com/joblib/joblib/issues/138), says that it's due to MKL internal bug.

I'm running this script `MXNET_CPU_WORKER_NTHREADS=4 python run.py --fpath ./imgs.txt --parallel 1`, tried to tweak ENV var, but no result.

Please advise how to parallel insightface's vectorizing using shared model memory as in multiprocessing model per CPU consumes a lot of RAM. Thought that joblib `sharedmem` should solve it, but no luck

Source [code here](https://gist.github.com/Novitoll/28c4796de265da8ccd9e7981f00e021e#file-script-py-L60)"
"Hi, i'm new to programming

1. Did i understand correctly that recognition module is for comparing 2 photos and answering if it is one man on photos?
2. How to run recognition module? I don't see test.py in folder

Thanks!"
"The original article says that the learning rate for the Glint360 dataset divided by 10 at 200k, 400k, 500k, 550k iterations
and finish at 600K iterations. Are there any recommendations for a strategy to reduce LR for datasets of 1 million, 10 million, 100 million identities? Thanks in advance for your reply"
"
I found in the code , that the backbone in wrapped by DistributedDataParallel , so the model will  sync different gradient in different gpus  ,  but conversely  the module_partial_fc isn't wrapped by DistributedDataParallel ,  will the center's weight in different gpu be different  ? and should the center be the same in different gpus ? 

@anxiangsir

```

    backbone = torch.nn.parallel.DistributedDataParallel(
        module=backbone, broadcast_buffers=False, device_ids=[local_rank])
    backbone.train()

    margin_softmax = eval(""losses.{}"".format(args.loss))()
    module_partial_fc = PartialFC(
        rank=rank, local_rank=local_rank, world_size=world_size, resume=args.resume,
        batch_size=cfg.batch_size, margin_softmax=margin_softmax, num_classes=cfg.num_classes,
        sample_rate=cfg.sample_rate, embedding_size=cfg.embedding_size, prefix=cfg.output)


```"
"In the first stage of backward,
https://github.com/deepinsight/insightface/blob/48a9e0bb5268137ba8ba3391c94a7d4425ed75aa/recognition/arcface_torch/partial_fc.py#L153
To monitor changes of training loss, I use

`y = grad * logits`
`loss1 = y.sum()`
`loss1.backward()`

to replace it. 
No matter the nember of training data, after dozens of epochs, loss always becomes negative.
![image](https://user-images.githubusercontent.com/41863500/111112427-1e514600-859b-11eb-94ea-03d801efdd55.png)
"
"您好，我在训练https://github.com/deepinsight/insightface/tree/master/alignment/heatmapReg 
时，训练几个epoch就会报错，请问时什么原因呢？

`1400][300W]NME: 0.075751
saving 7
INFO:root:Saved checkpoint to ""model/A-0007.params""
INFO:root:Epoch[3] Batch [220-240]	Speed: 31.91 samples/sec	lossvalue=0.000559
INFO:root:Epoch[3] Batch [240-260]	Speed: 88.76 samples/sec	lossvalue=0.000574
INFO:root:Epoch[3] Batch [260-280]	Speed: 88.93 samples/sec	lossvalue=0.000616
INFO:root:Epoch[3] Batch [280-300]	Speed: 89.82 samples/sec	lossvalue=0.000596
INFO:root:Epoch[3] Batch [300-320]	Speed: 89.01 samples/sec	lossvalue=0.000638
INFO:root:Epoch[3] Batch [320-340]	Speed: 89.06 samples/sec	lossvalue=0.000578
INFO:root:Epoch[3] Batch [340-360]	Speed: 88.96 samples/sec	lossvalue=0.000578
INFO:root:Epoch[3] Batch [360-380]	Speed: 88.92 samples/sec	lossvalue=0.000583
INFO:root:Epoch[3] Train-lossvalue=0.000595
INFO:root:Epoch[3] Time cost=207.517
train size after reset 15683
INFO:root:Epoch[4] Batch [0-20]	Speed: 89.72 samples/sec	lossvalue=0.000555
*** Error in `python': double free or corruption (top): 0x00007fdba0028dc0 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x7c619)[0x7fdd8bd37619]
/home/XXXX/anaconda3/envs/mxnet/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x76db660)[0x7fdd14b4e660]
/home/XXXX/anaconda3/envs/mxnet/lib/python3.7/site-packages/mxnet/libmxnet.so(_ZN5mxnet6engine14ThreadedEngine15ExecuteOprBlockENS_10RunContextEPNS0_8OprBlockE+0x5da)[0x7fdd14b4effa]
/home/XXXX/anaconda3/envs/mxnet/lib/python3.7/site-packages/mxnet/libmxnet.so(_ZN5mxnet6engine23ThreadedEnginePerDevice9GPUWorkerILN4dmlc19ConcurrentQueueTypeE0EEEvNS_7ContextEbPNS1_17ThreadWorkerBlockIXT_EEERKSt10shared_ptrINS3_11ManualEventEE+0x150)[0x7fdd14b61b30]
/home/XXXX/anaconda3/envs/mxnet/lib/python3.7/site-packages/mxnet/libmxnet.so(_ZNSt17_Function_handlerIFvSt10shared_ptrIN4dmlc11ManualEventEEEZZN5mxnet6engine23ThreadedEnginePerDevice13PushToExecuteEPNS6_8OprBlockEbENKUlvE2_clEvEUlS3_E_E9_M_invokeERKSt9_Any_dataS3_+0x46)[0x7fdd14b61db6]
/home/XXXX/anaconda3/envs/mxnet/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x76da8e4)[0x7fdd14b4d8e4]
/home/XXXX/anaconda3/envs/mxnet/bin/../lib/libstdc++.so.6(+0xc8421)[0x7fdd81574421]
/lib64/libpthread.so.0(+0x7e25)[0x7fdd8c085e25]
/lib64/libc.so.6(clone+0x6d)[0x7fdd8bdb334d]
======= Memory map: ========
200000000-200200000 ---p 00000000 00:00 0 
200200000-200400000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
200400000-202400000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
202400000-205400000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
205400000-206000000 ---p 00000000 00:00 0 
206000000-206200000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
206200000-206400000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
206400000-206600000 rw-s 206400000 00:05 43395                           /dev/nvidia-uvm
206600000-206800000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
206800000-206a00000 ---p 00000000 00:00 0 
206a00000-206c00000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
206c00000-207000000 ---p 00000000 00:00 0 
207000000-207200000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
207200000-209200000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
209200000-20c200000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
20c200000-20ce00000 ---p 00000000 00:00 0 
20ce00000-20d000000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
20d000000-20d200000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
20d200000-20d400000 rw-s 20d200000 00:05 43395                           /dev/nvidia-uvm
20d400000-20d600000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
20d600000-20d800000 ---p 00000000 00:00 0 
20d800000-20da00000 rw-s 00000000 00:05 36495                            /dev/nvidiactl
20da00000-400200000 ---p 00000000 00:00 0 
10000000000-10208000000 ---p 00000000 00:00 0 
7fd960000000-7fd960021000 rw-p 00000000 00:00 0 
7fd960021000-7fd964000000 ---p 00000000 00:00 0 
7fd968000000-7fd9687a1000 rw-p 00000000 00:00 0 
7fd9687a1000-7fd96c000000 ---p 00000000 00:00 0 
7fd96f57f000-7fd972000000 rw-p 00000000 00:00 0 
7fd972000000-7fd98c000000 ---p 00000000 00:00 0 
7fd98e000000-7fd9b8000000 ---p 00000000 00:00 0 
7fd9b957f000-7fd9bc000000 rw-p 00000000 00:00 0 
7fd9bc000000-7fdba0000000 ---p 00000000 00:00 0 
7fdba0000000-7fdba00d9000 rw-p 00000000 00:00 0 
7fdba00d9000-7fdba4000000 ---p 00000000 00:00 0 
7fdba6000000-7fdba8000000 ---p 00000000 00:00 0 
7fdba8000000-7fdba80f0000 rw-p 00000000 00:00 0 
7fdba80f0000-7fdbac000000 ---p 00000000 00:00 0 
7fdbac000000-7fdbac0de000 rw-p 00000000 00:00 0 
7fdbac0de000-7fdbb0000000 ---p 00000000 00:00 0 
7fdbb0000000-7fdbb2bb2000 rw-p 00000000 00:00 0 
7fdbb2bb2000-7fdbb4000000 ---p 00000000 00:00 0 
7fdbb4000000-7fdbb40f6000 rw-p 00000000 00:00 0 
7fdbb40f6000-7fdbb8000000 ---p 00000000 00:00 0 
7fdbb8000000-7fdbb81b0000 rw-p 00000000 00:00 0 
7fdbb81b0000-7fdbbc000000 ---p 00000000 00:00 0 
7fdbbe000000-7fdbc0000000 ---p 00000000 00:00 0 
7fdbc0000000-7fdbc3ecd000 rw-p 00000000 00:00 0 
7fdbc3ecd000-7fdbc4000000 ---p 00000000 00:00 0 
7fdbc6000000-7fdbcfe00000 ---p 00000000 00:00 0 
7fdbcfe00000-7fdbd0000000 rw-s 00000000 00:04 2725743                    /dev/zero (deleted)
7fdbd0000000-7fdbd7800000 ---p 00000000 00:00 0 
7fdbd7800000-7fdbd7a00000 rw-s 00000000 00:04 2724406                    /dev/zero (deleted)
7fdbd7a00000-7fdbe0000000 ---p 00000000 00:00 0 
7fdbe0000000-7fdbe166a000 rw-p 00000000 00:00 0 
7fdbe166a000-7fdbe4000000 ---p 00000000 00:00 0 
7fdbe4000000-7fdbe4021000 rw-p 00000000 00:00 0 
7fdbe4021000-7fdbe8000000 ---p 00000000 00:00 0 
7fdbe8000000-7fdbebff3000 rw-p 00000000 00:00 0 
7fdbebff3000-7fdbec000000 ---p 00000000 00:00 0 
7fdbee000000-7fdbf0000000 ---p 00000000 00:00 0 
7fdbf0000000-7fdbf3fe8000 rw-p 00000000 00:00 0 
7fdbf3fe8000-7fdbf4000000 ---p 00000000 00:00 0 
7fdbf6000000-7fdbf8000000 ---p 00000000 00:00 0 
7fdbf8000000-7fdbf81af000 rw-p 00000000 00:00 0 
7fdbf81af000-7fdbfc000000 ---p 00000000 00:00 0 
7fdbfd57f000-7fdc00000000 rw-p 00000000 00:00 0 
7fdc00000000-7fdc03fff000 rw-p 00000000 00:00 0 
7fdc03fff000-7fdc04000000 ---p 00000000 00:00 0 
7fdc04ffe000-7fdc04fff000 ---p 00000000 00:00 0 
7fdc04fff000-7fdc057ff000 rw-p 00000000 00:00 0 
7fdc057ff000-7fdc05800000 ---p 00000000 00:00 0 
7fdc05800000-7fdc06000000 rw-p 00000000 00:00 0                          [stack:15282]
7fdc06000000-7fdc0fe00000 ---p 00000000 00:00 0 
7fdc0fe00000-7fdc10000000 rw-s 00000000 00:04 2725742                    /dev/zero (deleted)
7fdc10000000-7fdc17800000 ---p 00000000 00:00 0 
7fdc17800000-7fdc17a00000 rw-s 00000000 00:04 2724399                    /dev/zero (deleted)
7fdc17a00000-7fdc19600000 ---p 00000000 00:00 0 
7fdc19600000-7fdc19800000 rw-s 00000000 00:04 2724402                    /dev/zero (deleted)
7fdc19800000-7fdc19a00000 rw-s 00000000 00:05 36495                      /dev/nvidiactl
7fdc19a00000-7fdc19c00000 rw-s 00000000 00:04 2724403                    /dev/zero (deleted)
7fdc19c00000-7fdc19e00000 ---p 00000000 00:00 0 
7fdc19e00000-7fdc1a000000 rw-s 00000000 00:05 36495                      /dev/nvidiactl
7fdc1a000000-7fdc1a200000 ---p 00000000 00:00 0 
7fdc1a200000-7fdc1a400000 rw-s 00000000 00:04 2724405                    /dev/zero (deleted)
7fdc1a400000-7fdc1a6d6000 rw-s 00000000 00:05 36495                      /dev/nvidiactl
7fdc1a6d6000-7fdc20000000 ---p 00000000 00:00 0 
7fdc20000000-7fdc20001000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20001000-7fdc20002000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20002000-7fdc20003000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20003000-7fdc20004000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20004000-7fdc20005000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20005000-7fdc20006000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20006000-7fdc20007000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20007000-7fdc20008000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20008000-7fdc20009000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20009000-7fdc2000a000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc2000a000-7fdc2000b000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc2000b000-7fdc2000c000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc2000c000-7fdc2000d000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc2000d000-7fdc2000e000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc2000e000-7fdc2000f000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc2000f000-7fdc20010000 rw-s 00000000 00:05 24451                      /dev/nvidia1
7fdc20010000-7fdc20011000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20011000-7fdc20012000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20012000-7fdc20013000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20013000-7fdc20014000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20014000-7fdc20015000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20015000-7fdc20016000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20016000-7fdc20017000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20017000-7fdc20018000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20018000-7fdc20019000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc20019000-7fdc2001a000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc2001a000-7fdc2001b000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc2001b000-7fdc2001c000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc2001c000-7fdc2001d000 rw-s 00000000 00:05 36597                      /dev/nvidia0
7fdc2001d000-7fdc2001e000 rw-s 00000000 00:05 36597                      /dev/nvidia0
`"
"For training ArcFace models by millions of IDs, we may meet some time efficiency problems. 

=====
**_P1: There are too many classes that my GPUs can not handle._**

Solutions:

1) To reduce memory usage of the classification layer, [model-parallelism](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace)  and [partial-fc](https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc) can be the good ideas.

2) Enable FP16 can further reduce the GPU memory usage and also get acceleration on modern NVIDIA GPUs. For example, we can enable fp16 training by a simple ``fp16-scale`` parameter:

  ```
  export CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' 
  python -u train_parall.py --network r50 --dataset emore --loss arcface --fp16-scale 1.0
  ```

  or change the following setting in partial-fc MXNet implementation.
  ```
  config.fp16 = True
  ```
3) Use distributed training.

=====
**_P2. The training dataset is too huge, io cost is high which leads to very low training speed._**

Solutions:

1) Sequential data loader instead of random access.
         Right now the default face recognition datasets(*.rec) are indexed key-value databases, called `MXIndexedRecordIO`. So the data loader is required to randomly access the items in these datasets while doing the training. The performance is acceptable only if the data is located on ram-filesystem or very fast SSD.  For general hard disks, we must use an alternative method to avoid random access. 

   a. Use [recognition/common/rec2shufrec.py](https://github.com/deepinsight/insightface/blob/master/recognition/common/rec2shufrec.py) to convert any indexed '.rec' dataset to a shuffled sequential one called `MXRecordIO`
  b. In [ArcFace](https://github.com/deepinsight/insightface/tree/master/recognition/ArcFace), set `is_shuffled_rec=True` in config file to use the converted shuffled dataset. Please check `get_face_image_iter()` function in `image_iter.py` for detail information.
  c. Shuffled dataset-loader requires sequential scanning only, and provides data shuffling in a small in-memory buffer.
  d. Shuffled dataset can also benefit from the c++ runtime of MXNet record reader which accelerates the image processing.

=====
Any question or discussion can be left in this thread.

"
"@nttstar 
Hi, thank you for the great job in the repo. 
Can you direct me to the method/paper you used to train the coordinateReg model? 
Thank you!"
"The file model-symbol.json in two model(training the y1 network with the [recognition/arcface](https://github.com/deepinsight/insightface/tree/master/recognition/arcface) and pre-trained model 'model-y1-test2') are different in name 'fc1_weight'.


The pre-trained model 'model-y1-test2'(provided in Baidu netdisk) is called 'fc1_weight':

```json
	{
      ""op"": ""null"", 
      ""name"": ""fc1_weight"", 
      ""attrs"": {
        ""__lr_mult__"": ""1.0"", 
        ""__shape__"": ""(128, 512)"", 
        ""__wd_mult__"": ""10.0""
      }, 
      ""inputs"": []
    }, 
```
while the normal trained model is called 'pre_fc1_weight':
```json
	{
      ""op"": ""null"", 
      ""name"": ""pre_fc1_weight"", 
      ""attrs"": {""num_hidden"": ""128""}, 
      ""inputs"": []
    },
```



"
insightface model is good on asian face? how about the accuracy on asian people
"**I trained the alignment model, heatmapReg, and I try to convert the model to onnx model.
But following errors occurred,**

Traceback (most recent call last):
  File ""model_onnx.py"", line 12, in <module>
    converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file)
  File ""/mnt/data2/anaconda3/envs/feathernet/lib/python3.7/site-packages/mxnet/contrib/onnx/mx2onnx/export_model.py"", line 79, in export_model
    verbose=verbose)
  File ""/mnt/data2/anaconda3/envs/feathernet/lib/python3.7/site-packages/mxnet/contrib/onnx/mx2onnx/export_onnx.py"", line 249, in create_onnx_graph_proto
    idx=idx
  File ""/mnt/data2/anaconda3/envs/feathernet/lib/python3.7/site-packages/mxnet/contrib/onnx/mx2onnx/export_onnx.py"", line 86, in convert_layer
    raise AttributeError(""No conversion function registered for op type %s yet."" % op)
AttributeError: No conversion function registered for op type UpSampling yet.

**python code is,**

import mxnet as mx
import numpy as np
from mxnet.contrib import onnx as onnx_mxnet
import logging

sym = './A-hourglass-symbol.json'
params = './A-hourglass-0150.params'

input_shape = (1, 3, 128, 128)

onnx_file = './A-hourglass.onnx'
converted_model_path = onnx_mxnet.export_model(sym, params, [input_shape], np.float32, onnx_file)

**mxnet version is,**

>>> import mxnet as mx
>>> mx.__version__
'1.7.0'

My trained model is,
[A-hourglass-0150.zip](https://github.com/deepinsight/insightface/files/6088093/A-hourglass-0150.zip)


**I have two questions,**
1) How could I solve the above problem,
2) After solving this problem, what do I need to do to make inferences in ncnn?

Sincerely yours.
"
"Hi, I'm trying to use Face Mask Renderer in insightface-recognition-tools.
But, I have problem to download 3d68 landmark detector because of access baiducloud.
Only chinese can create baidu account. Bacause of i'm not chinese, i can't download pretrained model file.
Can you upload the model file to other cloud such as google drive?"
 Can someone please provide a guide to process my our dataset for training of MobileFaceNet? My current datasets are raw images arranged in the folder format. 
"Hi everybody,
Anybody can share the way how to build dataset with mask for arcface?
Best regards,
PeterPham"
请问coordinateReg和基于heatmap的人脸关键点哪个定位更准确，以及实时性的对比？
"Hi,

What is the meaning of params `threshold`, `scales`, `do_flip` in RetinaFace.detect(img, threshold=0.5, scales=[1.0], do_flip=False)?

Best"
https://github.com/gengyanlei/Pytorch_Retinaface
"Hello, I'm facing the following issue while trying to Fine-tune the Softmax model with Triplet loss:
Command: CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network m1 --loss triplet --lr 0.005 --pretrained ./models/m1-softmax-emore,1

Error: 
gpu num: 4
prefix ./models/m1-triplet-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=240, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='triplet', lr=0.005, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='m1', per_batch_size=60, pretrained='./models/m1-softmax-emore,1', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 256, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'GDC', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'triplet', 'images_per_identity': 5, 'triplet_alpha': 0.3, 'triplet_bag_size': 7200, 'triplet_max_ap': 0.0, 'per_batch_size': 60, 'lr': 0.05, 'net_name': 'fmobilenet', 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'triplet', 'network': 'm1', 'num_workers': 1, 'batch_size': 240}
loading ./models/m1-softmax-emore,1 1
Traceback (most recent call last):
  File ""train.py"", line 486, in <module>
    main()
  File ""train.py"", line 482, in main
    train_net(args)
  File ""train.py"", line 285, in train_net
    _, arg_params, aux_params = mx.model.load_checkpoint(
  File ""/home/antik/Desktop/Projects/Proctoring-AI-master/face_detection/venv/lib/python3.8/site-packages/mxnet/model.py"", line 476, in load_checkpoint
    symbol = sym.load('%s-symbol.json' % prefix)
  File ""/home/antik/Desktop/Projects/Proctoring-AI-master/face_detection/venv/lib/python3.8/site-packages/mxnet/symbol/symbol.py"", line 2948, in load
    check_call(_LIB.MXSymbolCreateFromFile(c_str(fname), ctypes.byref(handle)))
  File ""/home/antik/Desktop/Projects/Proctoring-AI-master/face_detection/venv/lib/python3.8/site-packages/mxnet/base.py"", line 246, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: Traceback (most recent call last):
  [bt] (2) /home/antik/Desktop/Projects/Proctoring-AI-master/face_detection/venv/lib/python3.8/site-packages/mxnet/libmxnet.so(MXSymbolCreateFromFile+0x61) [0x7f9b01978b51]
  [bt] (1) /home/antik/Desktop/Projects/Proctoring-AI-master/face_detection/venv/lib/python3.8/site-packages/mxnet/libmxnet.so(+0x91c9a1a) [0x7f9b02d0ea1a]
  [bt] (0) /home/antik/Desktop/Projects/Proctoring-AI-master/face_detection/venv/lib/python3.8/site-packages/mxnet/libmxnet.so(+0x91d1f21) [0x7f9b02d16f21]
  File ""src/io/local_filesys.cc"", line 209
LocalFileSystem: Check failed: allow_null: :Open ""./models/m1-softmax-emore,1-symbol.json"": No such file or directory


![image](https://user-images.githubusercontent.com/4254488/107865398-c91bfa80-6e5d-11eb-8c58-55f68a1bc09d.png)

Please guide to solve this issue.
Thanks.
"
"I tried various torrent clients on several PCs, but it didn't help me download the dataset. Are there other ways to download Glint360k without using Baidu Drive ?"
"I am trying to evaluate the performance of my model on IJB-B dataset. I am using the code in the link below:
https://github.com/deepinsight/insightface/blob/master/evaluation/IJB/IJB_11.py

However, i can see that in get_image_feature, there is no call to forward_db function, so it gives me an output with the shape (227630,2) which is wrong. It should be (227630,512) as 512 is the embedding dimension. Then, I tried the following code:
https://github.com/deepinsight/insightface/blob/master/recognition/partial_fc/pytorch/IJB/IJB_11_Batch.py   
It generates the embedding with correct dimension, but the evaluation results are very poor. 

  Methods      | 1e-06 | 1e-05 | 0.0001 | 0.001 | 0.01 |  0.1  |
+------------------+-------+-------+--------+-------+------+-------+
| insightface-IJBB |  0.00 |  0.00 |  0.05  |  0.30 | 2.19 | 19.59 |
+------------------+-------+-------+--------+-------+------+-------+

Can someone tell me how to solve this problem? Thank you.."
" it's look like download something
[10:16:20] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.2.0. Attempting to upgrade...
[10:16:20] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
download failed due to URLError(ConnectionRefusedError(111, 'Connection refused')), retrying, 2 attempts left
download failed due to URLError(TimeoutError(110, 'Connection timed out')), retrying, 1 attempt left
WARNING:root:Failed to download tophub package for cuda: <urlopen error [Errno 110] Connection timed out>"
"I've noticed you enforce backbone's gradient to multiply a scaler world_size , can you explain the insight behind this motivation ?

https://github.com/deepinsight/insightface/blob/79aacd2bb3323fa50a125b828bb1656166604487/recognition/partial_fc/pytorch/partial_fc.py#L190
"
"Hi, 
I was wondering what is the purpose of the _mode_ (by default set to ""arcface"") in the _norm_crop_ function?
I didn't find any use of this _mode_ in this repository
"
"我下载的faces_emore中的train_ofrecord文件夹下有16个子文件，请问这16个文件是怎么生成的？
991M	part-00000
991M	part-00001
990M	part-00002
991M	part-00003
991M	part-00004
991M	part-00005
991M	part-00006
991M	part-00007
991M	part-00008
991M	part-00009
990M	part-00010
990M	part-00011
991M	part-00012
991M	part-00013
991M	part-00014
991M	part-00015

因为我用tools/dataset_convert/mx_recordio_2_ofrecord.py跑glint360k数据，只得到一个130G的part-0，然后我就把config中的train_data_part_num改成1，但是训练的时候会报错如下：
F0127 19:56:54.910080 38389 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00001: No such file or directory [2]
F0127 19:56:54.910089 38390 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00050: No such file or directory [2]F0127 19:56:54.910231 38392 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00150: No such file or directory [2]F0127 19:56:54.910259 38391 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00100: No such file or directory [2]
F0127 19:56:54.910089 38390 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00050: No such file or directory [2]F0127 19:56:54.910231 38392 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00150: No such file or directory [2]F0127 19:56:54.910259 38391 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00100: No such file or directory [2]
F0127 19:56:54.910089 38390 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00050: No such file or directory [2]F0127 19:56:54.910231 38392 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00150: No such file or directory [2]F0127 19:56:54.910259 38391 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00100: No such file or directory [2]
F0127 19:56:54.910089 38390 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00050: No such file or directory [2]F0127 19:56:54.910231 38392 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00150: No such file or directory [2]F0127 19:56:54.910259 38391 posix_file_system.cpp:94] Check failed: fd >= 0 Fail to open file train_ofrecord/train/part-00100: No such file or directory [2]

@nttstar @nlqq "
"I encountered a problem generating pairs.txt files for my own dataset. I don't know how to set --num_matches_mismatches. Could you help me?
parser.add_argument('--num_matches_mismatches',
type=int,

                    required=True,

                    help='Number of matches/mismatches per fold.')"
"For face detection, when a face show the bound of image and it only show the eyes,nose.(no mouth)
How to solve it with labeling. Drop it or label it ?
"
"Hi, Thanks to your great work.
However, I cant not print the acc value of subcenter arcface trian code, even if I add the metric to the code. I do not  know the reasons."
"Hi there, 
I just have a simple question, I like to know that the models inputs are in RGB or BGR??"
"How much megaface data is used for testing？ 
I have test  **`LResNet100E-IR,ArcFace@ms1m-refine-v2`**  in model zoon,   
however I just get rank 1: 0.059101, however the result l in model zoon  is heigher. 
pelease help me to understand it, Thanks so much.
```
Done matching! Score matrix size: 3530 1000000
Saving to ../../insight_r100_results/otherFiles/facescrub_megaface_insight_r100_1000000_1.bin
Computing test results with 1000000 images for set 1
Loaded 3530 probes spanning 80 classes
Loading from ../../insight_r100_results/otherFiles/facescrub_facescrub_insight_r100.bin
Probe score matrix size: 3530 3530
distractor score matrix size: 3530 1000000
Done loading. Time to compute some stats!
Finding top distractors!
Done sorting distractor scores
Making gallery!
Done Making Gallery!
Allocating ranks (1000080)
Rank 1: 0.059101

``` @yingfeng @nttstar @ppwwyyxx @leondgarse @jiankangdeng "
"i code the SVX loss of pytorch , like this:
-------------------------------------------------------------
    def forward(self, cos_theta, label):
        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability
        batch_size = label.size(0)
        gt = cos_theta[torch.arange(0, batch_size), label].view(-1, 1)  # ground truth score
        index = torch.where(label != -1)[0]
        print('index shape', cos_theta.shape)
        if self.xtype == 'MV-AM':
            mask = cos_theta > gt - self.m
            hard_vector = cos_theta[mask]
            cos_theta[mask] = (self.t + 1.0) * hard_vector + self.t  # adaptive
            final_gt = gt - self.m
        elif self.xtype == 'MV-Arc':
            sin_theta = torch.sqrt(1.0 - torch.pow(gt, 2))
            cos_theta_m = gt * self.cos_m - sin_theta * self.sin_m  # cos(gt + margin)
            mask = cos_theta > cos_theta_m
            hard_vector = cos_theta[mask]
            cos_theta[mask] = (self.t + 1.0) * hard_vector + self.t  # adaptive
            final_gt = cos_theta_m
            # final_gt = torch.where(gt > cos_theta_m, cos_theta_m, gt)
        else:
            raise Exception('unknown xtype!')

        #cos_theta.scatter_(1, label.data.view(-1, 1), final_gt)
        cos_theta.scatter_(1, label[index,None], final_gt)
        cos_theta *= self.s
        return cos_theta
------------------------------------------------------
Traceback (most recent call last):
  File ""partial_fc.py"", line 291, in <module>
    main(args.local_rank)
  File ""partial_fc.py"", line 235, in main
    logits.backward(grad)
  File ""/home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/tensor.py"", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function ScatterBackward0 returned an invalid gradient at index 1 - got [29, 1] but expected shape compatible with [256, 1]
Exception raised from validate_outputs at /pytorch/torch/csrc/autograd/engine.cpp:602 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f7cdfd231e2 in /home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x336b180 (0x7f7d1a0e3180 in /home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #2: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x3fd (0x7f7d1a0e83fd in /home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #3: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f7d1a0e9fa1 in /home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #4: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f7d1a0e2119 in /home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)
frame #5: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f7d2788234a in /home/lthpc/.conda/envs/partial_fc/lib/python3.6/site-packages/torch/lib/libtorch_python.so)
frame #6: <unknown function> + 0xb8408 (0x7f7d287d4408 in /opt/anaconda3/lib/libstdc++.so.6)
frame #7: <unknown function> + 0x76ba (0x7f7d29e346ba in /lib/x86_64-linux-gnu/libpthread.so.0)
frame #8: clone + 0x6d (0x7f7d29b6a4dd in /lib/x86_64-linux-gnu/libc.so.6)
---------------------------------------------------------------------------------------

thanks"
How could I use the pre-trained model to recognition on my image with my facebank?
"can you support Split and also  Transpose have problem   where i Run have this problem  AttributeError: permute_param
 "
"Hi, I found that the provided calculation of 1:N evaluation in IJBC_1N.py is different from the definition of [DIR_FAR curve ](http://www.cbsr.ia.ac.cn/users/scliao/papers/Liao-IJCB14-BLUFR.pdf)for open-set identification test. Do you know what metrics are the code calculating? And, what's more, do you know where to get the official code for DIR_FAR curve calculation? Thank you very much!"
"When I run:
`python convertCaffe.py ./model/mmdet.onnx ./model/a.prototxt ./model/a.caffemodel`
I have error:
Traceback (most recent call last):
  File ""convertCaffe.py"", line 11, in <module>
    import onnx2caffe._operators as cvt
  File ""/home/ubuntu/retinafce/02.Codes/insightface/tools/onnx2caffe/onnx2caffe/_operators.py"", line 9, in <module>
    from MyCaffe import Function as myf
ModuleNotFoundError: No module named 'MyCaffe'
Help me!"
"I downloaded (from Dropbox) ms1m-arcface and deepglint multiple times each and attempted unzipping with 7-Zip. Every time, it gets a decent amount into train.rec then says there's a data error and aborts. Windows Explorer says the zips themselves are invalid when I try to open them from there.

Am I missing something?"
"When I was training, I encountered the following error。MxNet version is 1.6.0
**This is my config info:**

Called with argument: Namespace(batch_size=128, ckpt=3, ctx_num=1, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='softmax', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='m1', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 256, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'GDC', 'net_multiplier': 1.0, 'val_targets': ['lfw'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'softmax', 'net_name': 'fmobilenet', 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'softmax', 'network': 'm1', 'num_workers': 1, 'batch_size': 128, 'per_batch_size': 128}

**The error info:**
Traceback (most recent call last):
  File ""train.py"", line 483, in <module>
    main()
  File ""train.py"", line 479, in main
    train_net(args)
  File ""train.py"", line 473, in train_net
    epoch_end_callback=epoch_cb)
  File ""/opt/mxnet/python/mxnet/module/base_module.py"", line 536, in fit
    self.update_metric(eval_metric, data_batch.label, False, data_batch.pad)
  File ""/opt/mxnet/python/mxnet/module/module.py"", line 777, in update_metric
    self._exec_group.update_metric(eval_metric, labels, pre_sliced, label_pads)
  File ""/opt/mxnet/python/mxnet/module/executor_group.py"", line 661, in update_metric
    out.shape[0], islice_batch_size)
AssertionError: output length 1 not a multiple of slice batch_size 128

"
"Dear,

It might be a noob but I'm wondering which kind of image quality is bad for face feature extractor?

Searching around in Google brought me some concepts about image blur level, contrast, illumination, sharpness,...too many things and I'm not sure which one is bad for face extractor, which one can bring the best performance.
What is your idea about it? Do you have any experiments with image enhancement methods for face recognition? Which one brought the best performance?

Thanks,
Can 
"
 我在modelzoo 下载了你们的人脸识别模型model-r34  里面文件有model-0000.params和model-symbol.json   然后把它们转成了rknn格式 发现结果不太正常，对于不同的照片，dot后的分数也很高。0000是不是表示第一轮训练的结果？
"Hi,

alignment/coordinateReg/

Model 2d106track
Given landmarks bounding box, predict 2d-106 landmarks. Used for video landmarks tracking.

Download link: coming soon

When the download for the model is available. would be great to have a try..
Thank you for your attention!

Longchuan
"
""
"Hi， the linked  http://insightface.ai/files/models/arcface_r100_v1.zip and  http://insightface.ai/files/models/retinaface_r50_v1.zip all are failed, Can I download these models from other link?  thinks"
"Hi, i used the vggface2 to train the resnet50 model, the config is in blow.
batch size:512
metric:arcface
scale:64
margin:0.5
start learning rate:0.1
The accuracy of the training data vggface2 is 0 all the time. the loss goes down normally. The verification accuracy in LFW  is normally increased.

Could you tell me what happened when you were training and why?
@yingfeng @nttstar "
""
"The code i used: https://github.com/deepinsight/insightface/blob/master/evaluation/IJB/IJB_11.py

I got different test results with different batch size:

for IJB-C TAR@FAR=1E-5:


  | bs=32 (default) | bs=1024
-- | -- | --
Model1 | 52.55 | 57.87
Model2 | 53.09 | 58.31
Model3 | 51.27 | 56.07
Model4 | 53.87 | 59.48
Model5 | 53.93 | 58.84

The IJB-B also has similar behavior
It seems that bigger batch size will get better result, it's weird...
"
"Hello everyone, here are some scripts that can convert insightface params to onnx model. These scripts have been sorted out various methods of exporting MXNet params or insightface params on the GitHub or CSDN, and can export various models of insightface, **RetinaFace**,  **arcface**,  **2d106det** and **gender-age** models are all supported. The repo address is: [https://github.com/linghu8812/tensorrt_inference](https://github.com/linghu8812/tensorrt_inference)

supported models|scripts
---|---
RetinaFace|[export_retinaface](https://github.com/linghu8812/tensorrt_inference/blob/master/project/RetinaFace/export_onnx.py)
arcface|[export_arcface](https://github.com/linghu8812/tensorrt_inference/blob/master/project/arcface/export_onnx.py)
2d106det|[export_2d106det](https://github.com/linghu8812/tensorrt_inference/blob/master/project/face_alignment/export_onnx.py)
gender-age|[export_gender-age](https://github.com/linghu8812/tensorrt_inference/blob/master/project/gender-age/export_onnx.py)

### Export RetinaFace params to ONNX

For RetinaFace model, RetinaFace-R50, RetinaFace-MobileNet0.25 and RetinaFaceAntiCov are both supported. copy [project/RetinaFace/export_onnx.py](https://github.com/linghu8812/tensorrt_inference/blob/master/project/RetinaFace/export_onnx.py) to `./detection/RetinaFace` or `./detection/RetinaFaceAntiCov`, with the following comman can export those models.

- export resnet50 model
```
python3 export_onnx.py
```
- export mobilenet 0.25 model
```
python3 export_onnx.py  --prefix ./model/mnet.25
```
- export RetinaFaceAntiCov model
```
python3 export_onnx.py  --prefix ./model/mnet_cov2 --network net3l
```
An inference code with tensorrt has also been supplied, the following are inference results:

- RetinaFace-R50 result

![image](https://user-images.githubusercontent.com/36389436/101184216-6bb04400-368b-11eb-89e8-59db5958be7a.png)

- RetinaFaceAntiCov result

![image](https://user-images.githubusercontent.com/36389436/101184429-ab772b80-368b-11eb-87a3-e35845f10917.png)

### Export arcface params to ONNX

For arcface model, it has added reshape the shape of PRelu params, so the exported PRelu node structure is shown as following:

![image](https://user-images.githubusercontent.com/36389436/101187135-efb7fb00-368e-11eb-99f3-629c8d1a73da.png)

### Export gender-age and 2d106det params to ONNX

The following is a TensorRT result for 2d106det model, now it's run alone, not with retinaface

![image](https://user-images.githubusercontent.com/36389436/101187567-7bca2280-368f-11eb-8e5a-cd1e0adce2db.png)


"
"Hi, here is two questions about MegaFace test:

1. Did you follow the same procedure to preprocess the data in megaface testpack as in MS1M-ArcFace, Asian-Celeb, Glint360k and DeepGlint? They are all totally out-of-box datasets if I understand correctly.

2. For the ids overlap with probe set in megaface testpack, did you remove them from the training sets mentioned above?

You guys are awesome, thanks."
"No module named 'rcnn.config' 
no floder config? is the sample_config.py?"
"Hi, I found when I do the inference, the forward pass time (i.e. the time of result = net(input)) varies greatly among images with a single face and images with many tiny faces? May I ask is this normal or they should have the same time. (they are under the same image size)"
I want to know how the 300w facial landmark dataset format can be converted to the Insightface dataset format. I don't know how the coordinates of the 68 points labeled in the dataset are converted. I just found the code for how to convert the face recognition data set.
"Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    boxes1, lmds1 = detector.detect(img1, threshold=0.8)
  File ""/app/tomcat/bin/executions/7786857/116678/FaceRecog.zip/FaceRecog/model_zoo/face_detection.py"", line 341, in detect
  File ""/home/superuser/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py"", line 2566, in asnumpy
    ctypes.c_size_t(data.size)))
  File ""/home/superuser/anaconda3/lib/python3.6/site-packages/mxnet/base.py"", line 246, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: Traceback (most recent call last):
  File ""src/storage/./pooled_storage_manager.h"", line 161
MXNetError: cudaMalloc retry failed: out of memory"
找不到src/data/face2rec2.py这个文件了
[Here](https://github.com/deepinsight/insightface/tree/master/alignment/coordinateReg) i can download only 106 lm model. Where i can download 68 lm model?
"when I used insightface/deploy/test.py ，I have found that There were a few photos that don’t identified a face。For example 
![1](https://user-images.githubusercontent.com/35398080/99774849-4a0e7300-2b49-11eb-8b5a-b23565038715.jpg)
![2](https://user-images.githubusercontent.com/35398080/99774854-4d096380-2b49-11eb-8600-30b0de77bec9.jpg)
![3](https://user-images.githubusercontent.com/35398080/99774858-4ed32700-2b49-11eb-9f1e-fdb0bd79bf62.jpg)
How to solve this problem, please
"
I want to detect face and head at same time. but the code seems only support one class detection
你好，我想咨询一下。 LFR2019(ICCV)数据集中人脸对齐的方式和  https://github.com/deepinsight/insightface/blob/master/python-package/insightface/utils/face_align.py  里面提到的是一样的吗？还是说用了其他的对齐方式呢？
"Hi, If I make the dataset like rec style , can it speed up the train speed?"
Does color jitter have to be done by every ITER? Why not set a probability of color jitter
"Hi,

first of all thank you for this huge dataset and congrats on the PartialFC paper, both look really promising.

In the paper you mention that the dataset consists of a cleaning of the Celeb500k data and some other publicly available datasets. I have some questions that you maybe can help me out with.

1) Do you mind sharing which other datasets are included? I would like to know whether there is overlap with like ms1m, vgg, asian celeb, so I avoid mixing or testing on false assumptions.
2) Do you have any info for the mapping from the original datasets' label/file ids to the label/file ids in glint360k? I would like to reallign the original source images to a different crop and format, but as far as I can tell in glint360k the label names are just enumeration indices.

Thanks so much in advance!"
"can anyone tell me? where is the src dir, why can't I find it"
"When using the multi-gpus training model, there is always one gpu with a particularly low utilization rate. May I ask how to solve this problem"
模型对双胞胎的识别效果很差，怎么优化，测试的双胞胎都识别错了
"Hi,

Question: What hyper-parameter settings were used with the MTCNN MXNet model to get detections on all LFW images."
"We are interesting participating on IFRVT but we have following questions.

1. As you mention that we should send with model under  insightface.challenge(AT)gmail.com, if the model is big, do you have any idea?
2. How long will we get the result?  Is it possible to get the result before the mid of Nov, 2020?

"
"hi, I tried partial_fc  for training. As installed the same version of the requests and set  the same settings in default.py, I got the segmentation fault:11. I used emore for training.
The  error occured  in  _arg_params_rank_0 = self.broadcast_parameters(_arg_params)
https://github.com/deepinsight/insightface/blob/master/recognition/partial_fc/mxnet/memory_module.py#L68.
Hope for any suggestions."
"Hello, thanks for the amazing repo. 
I have one question, what dataset was RetinaFace-R50 trained on? WIDER or WIDER and AFLW?

Thanks!"
"Hi, I want to run a script using the resnet101 model but Im not finding the process very easy. I have try the verification.py script inside /recognition/ArcFace folder and I got errors with pytest ""fixture 'data_set' not found"". Which are the main steps to get working ArcFace without training? 

For deploy.py script I dont understand what format needs to have the model and ga-model. 

Can work all the code in CPU?

Thanks."
"I started to train y2 model on retina dataset and it works fine (loss it not NaN) : 
`CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --dataset retina --network y2 --loss arcface`

When i switch to vargfacenet network the loss get NaN value right away in the first batch : 
`CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --dataset retina --network vargfacenet --loss arcface`
I thought this is because of the batch size so i tried other values for per-batch-size (64, 32) but it still fails.

Can someone explain that ?"
"no Face Attribute models in baidu yun link in [ModelZoo](https://github.com/deepinsight/insightface/wiki/Model-Zoo)
the link is: https://pan.baidu.com/s/1f8RyNuQd7hl2ItlV-ibBNQ

shows:
'此链接分享内容可能因为涉及侵权、色情、反动、低俗等信息，无法访问！'"
"My model always detect black face shadow as face,I want use these wrong face image to train my model."
"Hi,  I want to save results of the prediction of bbox and corresponding landmark. In the test_widerface.py, If I just want to save the bbox prediction, I will set 'vote' = True in detector. But when I set 'vote' = False in detector, I found it run the line 443-445(self.nms) rather than the line 451-453(self.bbox_vote), which leads to around 1000 more redundant results in my situation.   I wonder how to get the right prediction of bbox and landmarks after nms. 

Thanks."
"Hi. I am unable to download the dataset through Baidu drive. Would it please be possible to put the data in a different place?
Thanks."
Where can I find model file for loading in test.py file ?
"I got this error after about 100k iterations:


Fri Oct 23 17:16:00 2020[0]<stderr>:[2020-10-23 17:16:00.707119: W horovod/common/stall_inspector.cc:105] One or more tensors were submitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock. 
Fri Oct 23 17:16:00 2020[0]<stderr>:Stalled ranks:
Fri Oct 23 17:16:00 2020[0]<stderr>:0: [horovod_allreduce.noname.691941]
Fri Oct 23 17:16:00 2020[0]<stderr>:1: [horovod_allreduce.noname.691943]
Fri Oct 23 17:16:00 2020[0]<stderr>:2: [horovod_allreduce.noname.691943]
Fri Oct 23 17:16:00 2020[0]<stderr>:3: [horovod_allreduce.noname.691943]
Fri Oct 23 17:16:00 2020[0]<stderr>:4: [horovod_allreduce.noname.691943]
Fri Oct 23 17:16:00 2020[0]<stderr>:5: [horovod_allreduce.noname.691943]
Fri Oct 23 17:16:00 2020[0]<stderr>:6: [horovod_allreduce.noname.691943]
Fri Oct 23 17:16:00 2020[0]<stderr>:7: [horovod_allreduce.noname.691943]

How can i fix this issue? Thank you
"
"@nttstar  Can we train MTCNN from scratch using our own dataset as provided for RetinaFace? Is yes, then which files are meant for training MTCNN to reproduce your results ?"
"Hi , do you have plan to release Glint360k model zoo?"
您好，论文中提到在这个新数据集上resnet50和100的模型能否开源提供？谢谢
"model : r50-arcface
command: python convert_onnx.py

mxnet version: 1.5.0
onnx version: 1.2.1
input-shape: (1, 3, 112, 112)

THEN DO

command:  onnx2trt r50.onnx -o test.trt

Input filename:   r50.onnx
ONNX IR version:  0.0.3
Opset version:    7
Producer name:    
Producer version: 
Domain:           
Model version:    0
Doc string:       
----------------------------------------------------------------
Parsing model
While parsing node number 5 [PRelu -> ""relu0""]:
ERROR: /opt/onnx-tensorrt/ModelImporter.cpp:143 In function importNode:
[8] No importer registered for op: PRelu
"
"saving 58
INFO:root:Saved checkpoint to ""./models/r50-arcface-emore/model-0058.params""
[116000]Accuracy-Highest: 0.79100
INFO:root:Epoch[0] Batch [115980-116000]	Speed: 3.24 samples/sec	acc=0.001563	lossvalue=35.065247
INFO:root:Epoch[0] Batch [116000-116020]	Speed: 58.07 samples/sec	acc=0.004687	lossvalue=34.443248

The arcface model saving 58
Is it the 58 epoch ?
"
"loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()

I run the arcface
the status still continue,
no message show ,next up "
"https://github.com/deepinsight/insightface/wiki/Dataset-Zoo
could you provide the google link?
thanks"
"@jiankangdeng I am getting errors after running train.py file in RetinaFace.
Solved.
How can we print the loss curve vs epoch ?"
"I modified the get_symbol() function in train.py in recognition in order to train recognition model with circle loss, with reference to another repository https://github.com/zjysnow/CircleLoss , but the training result is stange, when m=0.25,s=64, the acc is always 0, and when the loss value didn't decrese any longer, the ver acc of lfw/cfp/agedb is still low(99.03/87.88/91.93). What‘s even worse, when I set s=256, the loss value keep always 'nan'. Can anyone help me to see what's the problem of my code? Thanks sooo much.
"
"Hi, thanks a lot for your outstanding work. 

I downloaded your shared data sets, pretrained model,  and ran the evaluation and got the following results, 

python -u IJB_11.py --model-prefix ./pretrained_models/r100-arcface/model --model-epoch 0 --gpu 0 --target IJBB --job arcface
+--------------+-------+-------+--------+-------+-------+-------+
|   Methods    | 1e-06 | 1e-05 | 0.0001 | 0.001 |  0.01 |  0.1  |
+--------------+-------+-------+--------+-------+-------+-------+
| arcface-IJBB | 42.75 | 90.76 | 94.75  | 96.48 | 97.64 | 98.67 |
+--------------+-------+-------+--------+-------+-------+-------+

python -u IJB_11.py --model-prefix ./pretrained_models/r100-arcface/model --model-epoch 0 --gpu 0 --target IJBC --job arcface
+--------------+-------+-------+--------+-------+-------+-------+
|   Methods    | 1e-06 | 1e-05 | 0.0001 | 0.001 |  0.01 |  0.1  |
+--------------+-------+-------+--------+-------+-------+-------+
| arcface-IJBC | 89.91 | 94.47 | 96.28  | 97.52 | 98.36 | 99.08 |
+--------------+-------+-------+--------+-------+-------+-------+

At FAR=1E-4, the TARs are 0.9475, and 0.9628  .  
In your ArcFace paper  https://arxiv.org/pdf/1801.07698.pdf, in Table 7, you had at FAR=1E-4,  
MS1MV2, R100, ArcFace 0.942 0.956

The results I got with your evaluation code are 0.05 better than your paper.    So what has changed since you published your paper? 


"
""
"Hi,
I have to deploy  pre-trained arcface model in the server . I have already implemented image embadded vector comparition with registered db images  and face recognition . for this my local machine consuming more time to compare the images (I used multiprocessing in python for high performance ) .
1.My quistion  is what are the minimum requirments for deploy the  pre-trained arcface model in the server ?
2.In this above case available  best server providers ?"
"I am able to forward a single image with `f1 = model.get_feature(img)` in test.py in the deploy folder.
Now, I would like to process a batch of images.
I tried with `numpy.stack([img ... img])` obtaining, for example, a [2, 3, 112, 112] input ndarray, but now I have the following error:

`mxnet.base.MXNetError: Error in operator conv0: [19:16:34] C:\Jenkins\workspace\mxnet-tag\mxnet\src\operator\nn\convolution.cc:152: Check failed: dshp.ndim() == 4U (5 vs. 4) : Input data should be 4D in batch-num_filter-y-x`

Thanks in advance!
"
"I have to implement insightface module  with aws lambda and aws api gateway when I run the code need to invoke insightface and mxnet libraries. both are nearly 500MB because of large size library i couldnt upload into lambda function .
Is there are any way to implement insightface module using AWS lambda ?  or any suggition ?"
"Hi, and thanks for the great project.
I wanted to know what kind of alignment and face detection was used for making the pretrained models?
I'm looking at  model-r100-arcface-ms1m-refine-v2... 

Thanks"
"Read:
https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/simple-neural-network-upgrade-boosts-ai-performance

Speed+Accuracy

Official implementation:
https://github.com/iVMCL/AOGNet-v2"
"Hi, I installed insightface, but I had many errors, like missing modules etc. So I downloaded them no problem, But these specific modules face_preprocess face_image, are just nowhere I am kinda desperate , my python version is 3.8.3, I am using anaconda to download packages, and I am working on Windows."
Can you add this todo list ?
"Hello, I have been using for packages for a while but sadly your documentation is poorly written. with the recent changes, I would love to update my codes and adjust them to the new changes but frankly, I can't tell what file is what. I would love if you'd kindly add code and documentation, explaining how to use recognition and different parts of your codes."
"Hi, I followed your instruction to train a model with MS1M training set, and I ended up achieving 99+@lfw, 95+@cfp, 95+@AgeDB. This pretrained model also achieved 97+ on my own testing set.

Then I used this pretrained model of mine to do transfer learning on my own training set, which is quite different from MS1M/LFW/CFP/AgeDB.... When I finished the tranfer learning, the final model achieved 99+ on my own testing set, but only 80+@lfw, 70+@cfp, 70+@AgeDB.

I wonder if I made any mistakes? Is there any ways to imporve the performance on my own testing set while remaining fair enough good performance on other testing sets?

Thank you for your time."
"Hello, I have a need for face recognition in friends series, just like in the [video](https://www.youtube.com/watch?v=y-D1tReryGA&t=81s) that is embedded in readme file

How can I recreate these results? Thanks!"
""
"@jiankangdeng  @nttstar 
I lately read your paper 'Sub-center ArcFace: Boosting Face Recognitionby Large-scale Noisy Web Faces'. I wonder  if MS1MV3 and the code will be released?"
According to https://pypi.org/project/insightface/ this package in on Apache 2.0 license which means it can be used for commercial purposes. Does it mean that this package can be used commercially including its default models?
"使用单张GeForce GTX 750 Ti显卡，显存是2G，在训练RetinaFace时报`MXNetError: cudaMalloc retry failed: out of memory`错误，如果不增加显卡的情况下能否解决这个问题。

贴一下显卡信息
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 750 Ti  Off  | 00000000:01:00.0  On |                  N/A |
| 33%   34C    P8     1W /  38W |     98MiB /  1999MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4240      G   /usr/lib/xorg/Xorg                 96MiB |
+-----------------------------------------------------------------------------+

```"
"1. RetinaFaceAntiCov
2.  coordinateReg

can these model train?
Or it only provide test?"
"https://paperswithcode.com/paper/vargfacenet-an-efficient-variable-group/review/

Accuracy of the model increases under the guide of a better teacher. My question is while training how do i specify the teacher ?

@nttstar 
Please take 2 mins to answer this question."
"Hi all,
    I changed config.py with making 'network.resnet.pretrained' empty string. I want to train a new model from the very begin. But some error comes as 

AttributeError: 'NoneType' object has no attribute 'get_internals'

   Anybody can help me? Thanks"
"Hi, I have read through the paper and have a question above. I guess focal loss's performance usually better than OHEM in imbalance problems. 
Thanks in advance."
""
"Hello, So when I am using deploy/test.py with an image with no face it pops up an error""raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: Error in operator conv0: [22:06:53] C:\Jenkins\workspace\mxnet-tag\mxnet\src\operator\nn\convolution.cc:152: Check failed: dshp.ndim()
== 4U (1 vs. 4) : Input data should be 4D in batch-num_filter-y-x"" . Does everyimage require it to have a face? If so how do I deploy it on videos? Or is there any data_preprocessing?"
according to the paper “Cascade Multi-view Hourglass Model for Robust 3D Face Alignment”，the author unifys both semi-frontal (68 landmarks) and profile (39landmarks) representation. is there any one know the algorithm？
glint里面有一些动漫图片，这些不是噪音吗？
Is there any way liveness detection for anti spoofing with insightface? 
您好，请问口罩分支3类是指 戴口罩，未戴口罩，还有1类是什么呢？未知？背景？
"parser.add_argument('--model', default='', help='path to load model.')
parser.add_argument('--ga-model', default='', help='path to load model.')
which model should  be configured?? I used pretraing model"
您好，请问你们为啥没考虑加入姿态估计分支呢？是因为姿态估计分支效果不好么？还是其它的原因呢？
"Hi all,
Do anyone have password for AgeDB original dataset. i sent a mail to the given email-id but i did not get any reply from them.
Thanks in advance. "
"E5-2650 v4 2.20GHz @ 24 Threads | 45ms

@nttstar 

Q1. Can you clarify the above mentioned is for single thread or multiple threads ?
Q2. What is the best single thread time for LResNet100E(dim512) model ?

Thanks."
"Hello author, have you considered using mosaic data enhancement operations in face detection?"
This is not strictly related to insightface but is there a GPU-enabled version of mxnet available for use with CUDA 11?
"

I have tried to convert the  RetinaFace(R-50 model)  to openVino but the accuracy and no. of detection is getting less. I am getting  FPS of 7 on GPU and 0.01 on CPU with original R-50 model and no. of detection is 427 and precision - 0.721 recall 0.744. But after conversion FPS on CPU increased but no of detection  decreased to 12. 

Please help if you have tried. 
"
Can you explain why not using MS1MV2? 
"Hello, thanks for your nice repo

I tested face recognition with test.py . But I want to  check the face alignment and it sounds that something is wrong with the face_preprocess.py output and aligned faces are cut at the chin . for example this image:

![0161_02](https://user-images.githubusercontent.com/43753499/90326920-96bf3c80-dfa3-11ea-9caa-8296340df89e.jpg)

convert to this one after alignment:

![0161_02_alligned](https://user-images.githubusercontent.com/43753499/90326927-a8a0df80-dfa3-11ea-86bb-420153a9b39c.jpg)


what is the problem?"
"Hi, thanks for providing such a great repo.
As you have stated that MTCNN is used in face detection and alignment stage for training data, I'd like to know if Lnet is used to get accurate landmarks in the alignment stage as well?
Thanks for your help in advance,"
"Do you normalize the input images to the network? If yes, could you please tell me how do you normalize it or reffer me to the section of the code where you are doing it?
"
"CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore


File ""train.py"", line 18, in <module>
    from metric import *
ModuleNotFoundError: No module named 'metric'

btw, so many train.py,  It is better to specify clearly which one is used."
"in the paper it said“annotate
five facial landmarks (i.e. eye centres, nose tip and mouth
corners) on faces that can be annotated from the WIDER
FACE training and validation subsets. In total, we have annotated 84.6k faces on the training set and 18.5k faces on
the validation set.”
but in the baudu cloud file zip the val label.txt just have the bbox label,so i want to ask where is the 18.5k facial landmarks label?"
"我在重新训练fmobilefacenet-v1的时候，模型的训练精度acc最大0.43，lossvalue最小5.9，
但是验证精度lfw=0.99417，cfp_fp=0.93286, agedb_30=0.96017,
之后训练精度acc，lossvalue不变；"
"I use PYTHON 3.6.3 ,when i run your code,i get the error: File ""eval/verification.py"", line 186, in load_bin bins, issame_list = pickle.load(open(path, 'rb')) UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)
I have tried many method ,but can't solve it.....should i change the python version to 2.7 or is there any other method,,,,please help"
"Hi, I prepared the data for training insightface as described in the [guide](https://github.com/Talgin/preparing_data) of @Talgin . However, when the training start, this following error occured:

> **$ CUDA_VISIBLE_DEVICES='' python -u train.py --network r100 --loss arcface --dataset ktnv**
use cpu
prefix ./models/r100-arcface-ktnv/model
image_size [112, 112]
num_classes 37
Called with argument: Namespace(batch_size=128, ckpt=3, ctx_num=1, dataset='ktnv', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'ktnv', 'dataset_path': '../datasets/ktnv', 'num_classes': 37, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'r100', 'num_workers': 1, 'batch_size': 128, 'per_batch_size': 128}
0 1 E 3 prelu False
Network FLOPs: 24.2G
INFO:root:loading recordio ../datasets/ktnv/train.rec...
header0 label [38. 42.]
id2range 4
37
rand_mirror True
loading bin 0
(456, 3, 112, 112)
ver lfw
lr_steps [100000, 160000, 220000]
call reset()
/home/extreme45nm/anaconda3/lib/python3.7/site-packages/mxnet/module/base_module.py:504: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
optimizer_params=optimizer_params)
Traceback (most recent call last):
File ""train.py"", line 379, in
main()
File ""train.py"", line 376, in main
train_net(args)
File ""train.py"", line 370, in train_net
epoch_end_callback = epoch_cb )
File ""/home/extreme45nm/anaconda3/lib/python3.7/site-packages/mxnet/module/base_module.py"", line 520, in fit
next_data_batch = next(data_iter)
File ""/home/extreme45nm/anaconda3/lib/python3.7/site-packages/mxnet/io/io.py"", line 230, in next
return self.next()
File ""/home/extreme45nm/anaconda3/lib/python3.7/site-packages/mxnet/io/io.py"", line 476, in next
raise StopIteration
StopIteration

I am stuck at this point, please give me some hint to solve this."
"Hi,

In the third party implementations section of the Retinaface directory, there is a RetinaFace-MobileNet0.25 implementation with a link to baidu drive to download the weights. When following the link, we are asked for a password. Do you know if the weights are still public and if so, how to download them ?"
""
"非常感谢作者的辛苦工作和贡献~

请问是否有做android或iOS工程迁移支持的计划呢，非常期待和感谢~"
"Hi, 

I have downloaded the `LResNet100E-IR, ArcFace@ms1m-refine-v2` model and I am planning to use it as a feature extractor. 
When I list the last 10 layers in the pre-trained network I see the following:
```
['bn1_output',
 'dropout0_output',
 'pre_fc1_weight',
 'pre_fc1_bias',
 'pre_fc1_output',
 'fc1_gamma',
 'fc1_beta',
 'fc1_moving_mean',
 'fc1_moving_var',
 'fc1_output']
```
I need to remove the fully connected layers and I am not sure what do `pre_fc1_weight`,  `pre_fc1_bias`, and
 `pre_fc1_output` do? Is there a sketch of the network available anywhere? Are these layers fully connected layers?
I would really appreciate if you could help me with my questions.

Thanks
"
"![a1](https://user-images.githubusercontent.com/67097720/88789724-45503880-d1c1-11ea-93f4-8eb4f9a96333.PNG)
"
"hello, I want to reproduce iQIYI benchmark.

In ArcFace papers, MLP+ensemble+context boosts accuracy by 8.85%p.

My question is this.
1. What ensemble method did you use?
2. How can I context features?

Thank you :) "
"@nttstar 您好，
Arcface用在dfw2019比赛上，请问官方提供的testing_data_Mask_matrix该如何使用呢？和制作测试集有关吗？

谢谢回覆"
"Arcface had good performance on dfw 2019.
And I wonder how can I use dfw testing data mask matix? 
Does anyone know how can I use mask matrix to make dfw test data follow 3 protocols?

The dfw paper is below here:
In order to correctly follow the protocols mentioned above,and report corresponding accuracies, training and testing mask matrices are also provided along with the dataset.

Thank you if you know it,please reply me."
"博主你好，有两个问题请教下，一是检测速度问题，测试时出现如下：Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)，百度后发现是需要将输入图片固定分辨率，这个分辨率有什么讲究吗，因为我设定的值作用于图片之后漏检较多，另一个是标注工具的选用，想在训练时加入自定义的数据，不知道用什么标注工具，盼复，谢谢。"
"Hello,

First of all thank you for your great work on face algorithms that has been very useful to me over the years.

As I wanted to use retinaface in tf and couldnt find a satisfactory implementation, I recently converted your model to tensorflow2.0+ :
https://github.com/StanislasBertrand/RetinaFace-tf2

I have achieved almost the same accuracy on widerface, though inference is a little bit slower.

Since there are quite a few questions about tf/keras implementations being available, I thought I could leave this issue here. If this is against your policy, please let me know and I will remove the issue."
"![det_007](https://user-images.githubusercontent.com/6825570/87764946-c286bf80-c849-11ea-9f22-8c6e7cc46f7f.jpg)
![det_008](https://user-images.githubusercontent.com/6825570/87764951-c4508300-c849-11ea-9fb9-da7fbdd7bad9.jpg)
![det_002](https://user-images.githubusercontent.com/6825570/87764953-c4e91980-c849-11ea-8923-af814f3ae525.jpg)
![det_003](https://user-images.githubusercontent.com/6825570/87764956-c581b000-c849-11ea-9576-7dbe96be8919.jpg)
"
"Can you provide details of differences between the models and data used in the following algorithms submitted to NIST for testing in the Face Recognition Vendor Testing (FRVT) 1:1 program? thanks!

- ""imperial-000"" submitted on 2019-03-01
- ""imperial-001"" submitted on 2019-03-01
- ""imperial-002"" submitted on 2019-08-28

[https://pages.nist.gov/frvt/reports/11/frvt_11_report.pdf](https://pages.nist.gov/frvt/reports/11/frvt_11_report.pdf)"
您好！您可以上传一个retinaface-mobilenet的预训练模型吗？
"I'm trying to retrain retinaface with a custom dataset. My images are of resolution 1920x1080. The average width and height of faces in the images are ~20 pixels. I have around 10k images for training. So far, the model is not able to identify faces. Is there any preprocessing i can do like resizing and cropping which can help in improving the detection accuracy?"
"Hi,

I am new to mxnet.

How can I send batch images for face detection instead  of one by one to RetinaFAce? 

is this improve the over all speed ?

Best . "
"when I trained retinaface using pretrained model R50, it occurs errors:
INFO:root:loading model/retina,0
(0, 'data', 648, (1, 3, 640, 640))
(1, 'bn_data_gamma', 648, (3,))
(2, 'bn_data_beta', 648, (3,))
(3, 'bn_data_moving_mean', 648, (3,))
(4, 'bn_data_moving_var', 648, (3,))
(5, 'bn_data_output', 648, (1, 3, 640, 640))
(6, 'conv0_weight', 648, (64, 3, 7, 7))
(7, 'conv0_output', 648, (1, 64, 320, 320))
(8, 'bn0_gamma', 648, (64,))
(9, 'bn0_beta', 648, (64,))
(10, 'bn0_moving_mean', 648, (64,))
(11, 'bn0_moving_var', 648, (64,))
(12, 'bn0_output', 648, (1, 64, 320, 320))
(13, 'relu0_output', 648, (1, 64, 320, 320))
(14, 'pooling0_output', 648, (1, 64, 160, 160))
(15, 'stage1_unit1_bn1_gamma', 648, (64,))
(16, 'stage1_unit1_bn1_beta', 648, (64,))
(17, 'stage1_unit1_bn1_moving_mean', 648, (64,))
(18, 'stage1_unit1_bn1_moving_var', 648, (64,))
(19, 'stage1_unit1_bn1_output', 648, (1, 64, 160, 160))
(20, 'stage1_unit1_relu1_output', 648, (1, 64, 160, 160))
(21, 'stage1_unit1_conv1_weight', 648, (64, 64, 1, 1))
(22, 'stage1_unit1_conv1_output', 648, (1, 64, 160, 160))
(23, 'stage1_unit1_bn2_gamma', 648, (64,))
(24, 'stage1_unit1_bn2_beta', 648, (64,))
(25, 'stage1_unit1_bn2_moving_mean', 648, (64,))
(26, 'stage1_unit1_bn2_moving_var', 648, (64,))
(27, 'stage1_unit1_bn2_output', 648, (1, 64, 160, 160))
(28, 'stage1_unit1_relu2_output', 648, (1, 64, 160, 160))
(29, 'stage1_unit1_conv2_weight', 648, (64, 64, 3, 3))
(30, 'stage1_unit1_conv2_output', 648, (1, 64, 160, 160))
(31, 'stage1_unit1_bn3_gamma', 648, (64,))
(32, 'stage1_unit1_bn3_beta', 648, (64,))
(33, 'stage1_unit1_bn3_moving_mean', 648, (64,))
(34, 'stage1_unit1_bn3_moving_var', 648, (64,))
(35, 'stage1_unit1_bn3_output', 648, (1, 64, 160, 160))
(36, 'stage1_unit1_relu3_output', 648, (1, 64, 160, 160))
(37, 'stage1_unit1_conv3_weight', 648, (256, 64, 1, 1))
(38, 'stage1_unit1_conv3_output', 648, (1, 256, 160, 160))
(39, 'stage1_unit1_sc_weight', 648, (256, 64, 1, 1))
(40, 'stage1_unit1_sc_output', 648, (1, 256, 160, 160))
(41, '_plus0_output', 648, (1, 256, 160, 160))
Traceback (most recent call last):
  File ""train.py"", line 373, in <module>
    main()
  File ""train.py"", line 370, in main
    lr=args.lr, lr_step=args.lr_step)
  File ""train.py"", line 93, in train_net
    sym = eval('get_' + args.network + '_train')(sym)
  File ""/home/admin/tools/insightface-master/RetinaFace/rcnn/symbol/symbol_resnet.py"", line 409, in get_resnet_train
    return get_sym_train(sym)
  File ""/home/admin/tools/insightface-master/RetinaFace/rcnn/symbol/symbol_common.py"", line 526, in get_sym_train
    conv_fpn_feat = get_sym_conv(data, sym)
  File ""/home/admin/tools/insightface-master/RetinaFace/rcnn/symbol/symbol_common.py"", line 207, in get_sym_conv
    stride2layer[stride] = all_layers[name]
  File ""/usr/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 518, in __getitem__
    raise ValueError('There are multiple outputs with name \""%s\""' % index)
ValueError: There are multiple outputs with name ""_plus0_output""
"
could you push anthoer one?
"**I just trained my customized dataset with following parameters (by recognition/train.py):**
CUDA_VISIBLE_DEVICES='0' python train.py --dataset emore --loss cosface --network r50 --lr 0.1 --lr-steps 80000,120000,1000000 --pretrained /home/dev/facial_workspace/training/pretrained-models/model-r50-am-lfw/model --models-root /home/dev/facial_workspace/training/all_vip_countries/models --per-batch-size 128

**And then I tried to fine tuning my model with following parameters (by recognition/train.py):**
CUDA_VISIBLE_DEVICES='0' python train.py --dataset emore --loss triplet --network mnas05 --lr 0.01 --lr-steps 80000,130000,1000000 --pretrained /home/dev/facial_workspace/training/all_vip_countries/models/r50-models/model --models-root /home/dev/facial_workspace/training/all_vip_countries/models

**The fine tuning calling argument details:**
Called with argument: Namespace(batch_size=60, ckpt=3, ctx_num=1, dataset='emore', dataset_path='/home/dev/facial_workspace/training/all_vip_countries/output', frequent=20, image_channel=3, image_shape=[112, 112, 3], kvstore='device', loss='triplet', lr=0.01, lr_steps='80000,130000,1000000', models_root='/home/dev/facial_workspace/training/all_vip_countries/models', mom=0.9, network='mnas05', num_classes=172460, per_batch_size=60, pretrained='/home/dev/facial_workspace/training/all_vip_countries/models/r50-models/model', pretrained_epoch=0, rescale_threshold=0, val_targets=['lfw', 'cfp_ff', 'agedb_30'], verbose=2000, wd=0.0005

**But I encounter this error:**

File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/io/io.py"", line 399, in prefetch_func
    self.next_batch[i] = self.iters[i].next()
  File ""/data/junjiexun/insightface/recognition/triplet_image_iter.py"", line 481, in next
    self.reset()
  File ""/data/junjiexun/insightface/recognition/triplet_image_iter.py"", line 390, in reset
    self.select_triplets()
  File ""/data/junjiexun/insightface/recognition/triplet_image_iter.py"", line 253, in select_triplets
    self.mx_model.forward(db, is_train=False)
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/module/module.py"", line 593, in forward
    assert self.binded and self.params_initialized
AssertionError

Traceback (most recent call last):
  File ""train.py"", line 436, in <module>
    main()
  File ""train.py"", line 433, in main
    train_net(args)
  File ""train.py"", line 407, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/module/base_module.py"", line 502, in fit
    allow_missing=allow_missing, force_init=force_init)
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/module/module.py"", line 309, in init_params
    _impl(desc, arr, arg_params)
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/module/module.py"", line 297, in _impl
    cache_arr.copyto(arr)
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py"", line 2629, in copyto
    return _internal._copyto(self, out=other)
  File ""<string>"", line 27, in _copyto
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py"", line 107, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/base.py"", line 255, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [20:05:00] src/operator/contrib/./../elemwise_op_common.h:135: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node  at 0-th output: expected [512], got [256]
Stack trace:
  [bt] (0) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x6b8b5b) [0x7f14b3157b5b]
  [bt] (1) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x878f39) [0x7f14b3317f39]
  [bt] (2) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x8797f2) [0x7f14b33187f2]
  [bt] (3) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x9cacfc) [0x7f14b3469cfc]
  [bt] (4) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::imperative::SetShapeType(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, mxnet::DispatchMode*)+0x1d27) [0x7f14b6334437]
  [bt] (5) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x1db) [0x7f14b633c9bb]
  [bt] (6) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x3759cef) [0x7f14b61f8cef]
  [bt] (7) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/site-packages/mxnet/libmxnet.so(MXImperativeInvokeEx+0x62) [0x7f14b61f92b2]
  [bt] (8) /home/dev/anaconda3/envs/intelligent_platform/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f14fca81ec0]

Can any expert could help on this issue?

Thanks in advance!
"
Multi-server distributed computing
"Hi,
Could you please provide the threshold used in `$INSIGHTFACE/src/eval/verification.py` 
(The one that you used in your paper for test)

Thanks!"
"hello
when i train i have below error?
call reset()
/usr/local/lib/python3.5/dist-packages/mxnet/module/base_module.py:505: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
  optimizer_params=optimizer_params)
Traceback (most recent call last):
  File ""train.py"", line 379, in <module>
    main()
  File ""train.py"", line 376, in main
    train_net(args)
  File ""train.py"", line 371, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/module/base_module.py"", line 523, in fit
    next_data_batch = next(data_iter)
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/io/io.py"", line 228, in __next__
    return self.next()
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/io/io.py"", line 474, in next
    raise StopIteration
StopIteration
"
"hello
i genarate .lst and .rec and .idx
but when i tarin 
Give me this error
Network FLOPs: 24.2G
INFO:root:loading recordio ../datasets/train.rec...
Traceback (most recent call last):
  File ""train.py"", line 378, in <module>
    main()
  File ""train.py"", line 375, in main
    train_net(args)
  File ""train.py"", line 243, in train_net
    images_filter        = config.data_images_filter,
  File ""/home/ml/insightface-master/recognition/image_iter.py"", line 39, in __init__
    s = self.imgrec.read_idx(0)
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/recordio.py"", line 313, in read_idx
    self.seek(idx)
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/recordio.py"", line 275, in seek
    pos = ctypes.c_size_t(self.idx[idx])
KeyError: 0
[14:51:49] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice

please help me"
"Is net3 a lightweight retinaface model? We are  loading the weights of pre-trained retinaface model, RetinaFace-R50, using net3 for testing.  

If I understand correctly, RetinaFace-50 are weights obtained by training network with Resnet50? This is net3. And net5 has Resnet-152 as its backbone. 

It would help if you could clarify the differences in the network explicitly. Thank you."
多谢指教
你好，我想问一下这个数据集的人脸crop和对齐方式。 请问是通过mtcnn检测的原始ms_celeb_1m人脸的人脸关键点，然后crop+对齐的吗？ 
"Hello @nttstar ,

Currently, I'm training my customized dataset by recognition/training.py, the parameters as following:
batch_size=64, ckpt=3, ctx_num=1, dataset='emore', dataset_path=' ./facial_workspace/training/output', frequent=20, image_channel=3, image_shape=[112, 112, 3], kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', num_classes=90374, per_batch_size=64, pretrained='', pretrained_epoch=0, rescale_threshold=0, val_targets=['lfw', 'cfp_fp', 'agedb_30'], verbose=2000, wd=0.0005

My question is : 
Can I stop this training as I found the ""Accuracy-Highest"" (value is 0.71433 that is not high enough) has not been changed long time. 

![image](https://user-images.githubusercontent.com/519028/84893143-59d4e780-b0d1-11ea-9cc7-71c62925574a.png)

"
"**Is there a way that we can limit the Memory Allocation used by this Model to allow for Concurrent Models to Run?**

After loading the first model the GPU Mem stats reflect: 

`utilization.gpu 74 utilization.memory 0 memory.free 13353 memory.used 2777 memory.total 16130`

After running the first inference through, it balloons, but the `GPU Utilization` is still very low at `3`: 

`utilization.gpu 3 utilization.memory 0 memory.free 9789 memory.used 6341 memory.total 16130`

This makes me think that we should be able to load more models onto the same GPU, but unfortunately the memory is already allocated to MXNet.


---
**Solutions Tried:** 

1. Trying `ctx.empty_cache()` between calls to the model - https://mxnet.apache.org/api/python/docs/api/mxnet/context/index.html#mxnet.context.Context.empty_cache
2. Trying `MXNET_GPU_MEM_POOL_RESERVE：60` -  https://discuss.mxnet.io/t/how-to-limit-gpu-memory-usage/6304/3
2. Using `gc.collect()` https://stackoverflow.com/questions/47661254/reset-gpu-memory-using-keras-1-2-2-with-mxnet-backend/48939575#48939575

But none of these worked. Any thoughts?

  [1]: https://github.com/deepinsight/insightface
  [2]: https://i.stack.imgur.com/yTcXN.png
  [3]: https://i.stack.imgur.com/j2cYW.png"
"Hello:

I use this code to detect bounding box in multiple images with GPU:

```
model = insightface.app.FaceAnalysis()
ctx_id = 0
model.prepare(ctx_id = ctx_id, nms=0.8)
faces = model.get(img)
for idx, face in enumerate(faces):
            box=face.bbox.astype(np.int).flatten()
            list_box.append(box)
```
When I process 500 images I get an error:

cudaMalloc failed: out of memory

How can I fix the problem?


Regards,
Roberto


"
"How to create a tarin.idx?

"
"I have collected 10k identical subject IDs from internet. I combined the collected data with emore dataset (85742 subject IDs). So, I have total of subject ID is 86742 subject IDs. I want to use my pretrained model (trained from emore dataset with setting ` config.ckpt_embedding = False`  for recognition task.

https://github.com/deepinsight/insightface/blob/3866cd77a6896c934b51ed39e9651b791d78bb57/recognition/sample_config.py#L10

Should I retrain the network from scratch or finetune it with pretrained model from Emore dataset? If I finetune on Emore dataset, which layers should I finetune, i.e fc1, and fc7?"
"Hi,

why do you use the following points for image alignment when doing preprocessing for training with arcface loss?

`arcface_src = np.array([
  [38.2946, 51.6963],
  [73.5318, 51.5014],
  [56.0252, 71.7366],
  [41.5493, 92.3655],
  [70.7299, 92.2041] ], dtype=np.float32 )`

It seems that first of all eyes and mouth points do not have the same y coordinates. Furthermore when doing mirroring along the y axis (which is a common data augmentation operation) the position of the landmarks changes slightly which could have a bad influence on training?

So where do these coordinates come from and why are they not designed such that the mirroring transformation does not change the landmark locations?

Kind regards,

Christian"
"Hello
How are you?
Thanks for contributing the _Arcface_ paper and this project.
I have a trouble to set the scale parameter s in training an _Arcface_ model.
I used _Resnet50_ and _ms1m_ dataset.
You and many developers use _s_=64.0 but I could not find how to set the scale parameter _s_.
I read a paper for _AdaCos_.
As you know, _AdaCos_ uses _s=sqrt(2)*ln(classes-1)_ as the scale parameter in the fixed parameter mode.
For _ms1m_ dataset (85K classes), _s_ is about 16.06.
I would like to hear your opinion for this.
Thank you."
"could you tell me where is the code of image process (image mean...) ?

Is the images process merged with model ?
So you dont do the mean , std "
"Hello! The download link (4.1 gender and age) has failed. Could you please update it again?
https://github.com/deepinsight/insightface/wiki/Model-Zoo"
"1*GPU,   batch_size=128,训练集5.8k_id,21W, speed 886 samples/sec
1*GPU，batch_size=128,训练集95K_id, 280W, speed 99 samples/sec
2*GPU,   batch_size=128,训练集95K_id, 280W, speed  136 samples/sec
这是哪里的问题呢？ fc7? 训练集？等等？
同样的训练集95k_id,280W,训练r100，2*GPU,batch_size=64, speed  256 samples/sec
运行的是../recognition/train.py"
"```2019.04.04: Arcface achieved state-of-the-art performance (7/109) on the NIST Face Recognition Vendor Test (FRVT) (1:1 verification) report (name: Imperial-000 and Imperial-001). Our solution is based on [MS1MV2+DeepGlintAsian, ResNet100, ArcFace loss].```
这里面的MS1MV2是指ms1m-retinaface-t1.zip，DeepGlintAsian是指faces_glintasia.zip吗？"
"I am trying to train a ResNet-34 on the VGGFace2 dataset using ArcFace loss (custom Keras implementation), but I can not get better results than regular Softmax Cross-Entropy loss. 

In the published ArcFace paper, on Table 7, you report the results of a (VGG2, R50, ArcFace) model which I am trying to replicate, but you do not specify anywhere the paper the training arguments / hyperparameters when training on VGG2, other than m=0.3, and I can not find them anywhere in this repo.

Can anyone please tell me where can I find them (learning rate, steps, dropout, preprocessing, etc)"
很期待大神的prnet效果，大神打算什么时候开源呢 @nttstar 
一台电脑同时运行多个insightface模型时，会容易显存溢出，cudaMalloc failed: out of memory和MapPlanKernel ErrStr:out of memory。发现一个进程运行insightface显存波动很大，怎么能降低insightface显存浮动幅度？避免造成显存溢出
I need continue trains trained model. How Can I do this?
"Hi,

I'm am trying to use face biometrics to identify peoples. I have a few questions:

1- How can we use multiple images of a person to better identification?
* One way is using [SVM](https://github.com/ageitgey/face_recognition/blob/5fe85a1a8cbd1b994b505464b555d12cd25eee5f/examples/face_recognition_svm.py) classifier to find the closest embedding to the input embedding.
* Another heuristic way that I'm not sure about it, but it results better accuracy is that I choose one of  the embeddings as (the best) a baseline embedding for a person and use that embedding for identification. I choose an embedding which has the closest average distance to all other embeddings of the same person (If we have two embeddings for one person, we choose one of them randomly). I think this is very crazy. When we choose one embedding as the best, other embeddings are not useful anymore. 

2- What is the best threshold for identification. I mean how should I choose a threshold for similarity to get a 95% accuracy. If I set it 0.6 False-Positive rate is high (false person is accepted) and when it is 0.8 False-Negative rate is high (true person is rejected). I know that it depends on application, but after trying different thresholds in different places, with different peoples, cameras, and hardware resource (different frame rated) I still cannot get a fix number for my application which is using face biometrics for access control with a 95% accuracy."
您好，阅读了源码，我发现在用默认设置训练的时候，发现数据增强只用了随机镜像翻转。然后我看别人的人脸识别训练代码时候也发现数据增强这块往往用的很少（比如颜色抖动阿，mixup等等）。 请问是为什么呢
""
"Thanks you for the great repository !
You provide a link to your version of MS1MV2, but the names list is missing. Can you please provide it ? 
It's important to have it when testing on different test sets, so we can avoid overlapping identities."
iQIYI_VID_FACE的align方式是基于retinaface的5个关键点进行对齐的吗？
Why it is 1.24
Is is possible to train with a reduced size faces_emore ? Takes too long in my computer
"There are bunch of dependencies in the python package, that are, I believe, unused (e.g. scikit-learn). That creates unnecessary conflicts in my virtual environment. Could they be removed? Thanks."
"I am training vargfacenet with faces_glint dataset, below are my result 

testing verification..
(12000, 512)
infer time 10.401622999999994
[lfw][177500]XNorm: 23.159211
[lfw][177500]Accuracy-Flip: 0.99700+-0.00314

testing verification..
(14000, 512)
infer time 11.917347999999999
[cfp_fp][177500]XNorm: 21.214161
[cfp_fp][177500]Accuracy-Flip: 0.91829+-0.01338

testing verification..
(12000, 512)
infer time 10.231041000000001
[agedb_30][177500]XNorm: 23.092572
[agedb_30][177500]Accuracy-Flip: 0.96883+-0.00837


The accuracy on verification datasets looks good, but when i manually check the similarity  between two different faces the similarity score comes higher than 50%. 

Can anyone explain me why this is happening?

@nttstar "
"On your Dataset-Zoo page there are several training datasets :
https://github.com/deepinsight/insightface/wiki/Dataset-Zoo

I didn't find any place a script which shows how these datasets were created. 
Let's take vggface2 as an example - after i download it from the official website what script should i run in order to get the aligned vggface2 like you got ?"
"Hi.
I am trying to calculate the similarity between two embedding features.
The metrics learning is done using arcmargin, so the computation of similarity requires finding the theta through the dot product between the two features.
However, if you look at the evaluation source, use L2 distance.
I want to know why.
Thank you
"
"Hi, 

What are the latest available pretrained models?
The referenced R50 is more than a year old, but the mnet0.25 cov is much newer.

What's the latest and best?

br,
Peter"
"  File ""train.py"", line 897, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, args.batch_size, 10, data_extra, label_shape)
  File ""eval/verification.py"", line 265, in test
    _embeddings = net_out[0].asnumpy()
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/ndarray/ndarray.py"", line 1996, in asnumpy
    ctypes.c_size_t(data.size)))
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:57:59] include/mxnet/././tensor_blob.h:290: Check failed: this->shape_.Size() == static_cast<size_t>(shape.Size()) (2632734720 vs. 18446744072047319040) : TBlob.get_with_shape: new and old shape do not match total elements
Stack trace:
  [bt] (0) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x4b09db) [0x7fba235f29db]
  [bt] (1) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x558579) [0x7fba2369a579]
  [bt] (2) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x23caed9) [0x7fba2550ced9]
  [bt] (3) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(mxnet::imperative::PushFCompute(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)::{lambda(mxnet::RunContext)#1}::operator()(mxnet::RunContext) const+0x307) [0x7fba257aabc7]
  [bt] (4) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x25b46e4) [0x7fba256f66e4]
  [bt] (5) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x25c2301) [0x7fba25704301]
  [bt] (6) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x25c5644) [0x7fba25707644]
  [bt] (7) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x25c0a64) [0x7fba25702a64]
  [bt] (8) /home/xl/anaconda3/envs/test/bin/../lib/libstdc++.so.6(+0xc8421) [0x7fba7758e421]
"
"header0 label [1. 2.]
id2range 1
0 0 2
c2c_stat [0, 1]
0
rand_mirror 1
(3,)
Traceback (most recent call last):
  File ""train.py"", line 1025, in <module>
    main()
  File ""train.py"", line 1022, in main
    train_net(args)
  File ""train.py"", line 887, in train_net
    data_set = verification.load_bin(path, image_size)
  File ""eval/verification.py"", line 185, in load_bin
    bins, issame_list = pickle.load(open(path, 'rb'))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)

the commond is CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100  --data-dir   /home/xl/1mxnetinsight/insightface-master/datasets/17_faces_datasets  --ckpt 2  --version-se 0 --per-batch-size 3 --prefix  /media/xl/绿叶启动盘/save/model  --verbose 10 --lr 0.00001
"
"/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/module/base_module.py:504: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.3333333333333333). Is this intended?
  optimizer_params=optimizer_params)
call reset()
Traceback (most recent call last):
  File ""/home/xl/1mxnetinsight/insightface-master/src/data.py"", line 873, in next
    label, s, bbox, landmark = self.next_sample()
  File ""/home/xl/1mxnetinsight/insightface-master/src/data.py"", line 766, in next_sample
    raise StopIteration
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 1025, in <module>
    main()
  File ""train.py"", line 1022, in main
    train_net(args)
  File ""train.py"", line 1016, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/module/base_module.py"", line 520, in fit
    next_data_batch = next(data_iter)
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/io/io.py"", line 230, in __next__
    return self.next()
  File ""/home/xl/1mxnetinsight/insightface-master/src/data.py"", line 945, in next
    raise StopIteration
StopIteration

the commond is  : CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100  --data-dir   /home/xl/1mxnetinsight/insightface-master/datasets/17_faces_datasets  --ckpt 2 --target 17_faces --version-se 0 --per-batch-size 3 --prefix  /media/xl/绿叶启动盘/save/model  --verbose 10 --lr 0.00001

"
"the error is  UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)
when I use this commond :
CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100  --data-dir   /home/xl/1mxnetinsight/insightface-master/datasets/17_faces_datasets  --ckpt 2  --version-se 0 --per-batch-size 3 --prefix  /media/xl/绿叶启动盘/save/model  --verbose 10 --lr 0.00001
"
"About detecting multiple faces,Minor change on ""get_input"": an attribute of FaceModel in face_model.py
def get_multi_input(self, face_img):
        """"""
        get multi-inputs/aligned
        """"""
        ret = self.detector.detect_face(face_img, det_type=self.args.det)
        if ret is None:
            return None
        bboxs, points = ret
        if bboxs.shape[0] == 0:
            return None
        aligned_list = []
        for b, p in zip(bboxs, points):
            bbox = b[0:4]
            points = p.reshape((2, 5)).T
            # print(bbox)
            # print(points)
            # face_acc = b[-1]
            # print(""Face Acc: {0:.3f}"".format(face_acc))
            nimg = face_preprocess.preprocess(face_img, bbox, points, image_size='112,112')
            nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)
            aligned = np.transpose(nimg, (2, 0, 1))
            aligned_list.append(aligned)
        return np.array(aligned_list)"
"How can i make the pre-trained model return me the person name on basis of which the training has been made. For eg. if i am using the Tom Hanks image from the deploy folder, can i get the model return the name of Tom Hanks and corresponding embedding "
"hi

i ran model r100 on tvm but the out put is different .

i tuned it 1000 times.

my code is :

`import os

import numpy as np
import mxnet as mx
import tvm

from tvm import autotvm
from tvm import relay
import tvm.relay.testing
from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner

import tvm.contrib.graph_runtime as runtime



def get_network(name, batch_size):
    
    input_shape = (batch_size, 3, 112, 112)
    output_shape = (batch_size, 512)

    if ""resnet"" in name:
        n_layer = int(name.split('-')[1])
        mod, params = relay.testing.resnet.get_workload(num_layers=n_layer, batch_size=batch_size, dtype=dtype)
    elif ""vgg"" in name:
        n_layer = int(name.split('-')[1])
        mod, params = relay.testing.vgg.get_workload(num_layers=n_layer, batch_size=batch_size, dtype=dtype)
    elif name == 'mobilenet':
        mod, params = relay.testing.mobilenet.get_workload(batch_size=batch_size, dtype=dtype)
    elif name == 'squeezenet_v1.1':
        mod, params = relay.testing.squeezenet.get_workload(batch_size=batch_size, version='1.1', dtype=dtype)
    elif name == 'inception_v3':
        input_shape = (1, 3, 299, 299)
        mod, params = relay.testing.inception_v3.get_workload(batch_size=batch_size, dtype=dtype)
    elif name == 'mxnet':
        # an example for mxnet model

        block = '/home/hbr-ubuntu/insightface-master/models/model-r100-ii/model'
        sym, args, auxs = mx.model.load_checkpoint(block, 0)

        mod, params = relay.frontend.from_mxnet(sym, shape={'data': input_shape}, dtype=dtype, arg_params=args, aux_params=auxs)
        net = mod[""main""]
        net = relay.Function(net.params, relay.nn.softmax(net.body), None, net.type_params, net.attrs)
        mod = tvm.ir.IRModule.from_expr(net)
    else:
        raise ValueError(""Unsupported network: "" + name)

    return mod, params, input_shape, output_shape


target = tvm.target.cuda()


network = 'mxnet'
log_file = ""%s.log"" % network
dtype = 'float32'

tuning_option = {
    'log_filename': log_file,

    'tuner': 'xgb',
    'n_trial': 1000,
    'early_stopping': 600,

    'measure_option': autotvm.measure_option(
        builder=autotvm.LocalBuilder(timeout=10),
        runner=autotvm.LocalRunner(number=20, repeat=3, timeout=4, min_repeat_ms=150),

    ),
}


def tune_tasks(tasks,
               measure_option,
               tuner='xgb',
               n_trial=500,
               early_stopping=None,
               log_filename='tuning.log',
               use_transfer_learning=True):
    # create tmp log file
    tmp_log_file = log_filename + "".tmp""
    if os.path.exists(tmp_log_file):
        os.remove(tmp_log_file)

    for i, tsk in enumerate(reversed(tasks)):
        prefix = ""[Task %2d/%2d] "" %(i+1, len(tasks))

        # create tuner
        if tuner == 'xgb' or tuner == 'xgb-rank':
            tuner_obj = XGBTuner(tsk, loss_type='rank')
        elif tuner == 'ga':
            tuner_obj = GATuner(tsk, pop_size=100)
        elif tuner == 'random':
            tuner_obj = RandomTuner(tsk)
        elif tuner == 'gridsearch':
            tuner_obj = GridSearchTuner(tsk)
        else:
            raise ValueError(""Invalid tuner: "" + tuner)

        if use_transfer_learning:
            if os.path.isfile(tmp_log_file):
                tuner_obj.load_history(autotvm.record.load_from_file(tmp_log_file))

        # do tuning
        tsk_trial = min(n_trial, len(tsk.config_space))
        tuner_obj.tune(n_trial=tsk_trial,
                       early_stopping=early_stopping,
                       measure_option=measure_option,
                       callbacks=[
                           autotvm.callback.progress_bar(tsk_trial, prefix=prefix),
                           autotvm.callback.log_to_file(tmp_log_file)
                       ])

  
    autotvm.record.pick_best(tmp_log_file, log_filename)
    os.remove(tmp_log_file)




def tune_and_evaluate(tuning_opt):
    
    print(""Extract tasks..."")
    mod, params, input_shape, out_shape = get_network(network, batch_size=1)
    tasks = autotvm.task.extract_from_program(mod[""main""], target=target,
                                              params=params,
                                              ops=(relay.op.get(""nn.conv2d""),))

    
    print(""Tuning..."")
    tune_tasks(tasks, **tuning_opt)

    
    with autotvm.apply_history_best(log_file):
        print(""Compile..."")
        with relay.build_config(opt_level=3):
            graph, lib, params = relay.build_module.build(
                mod, target=target, params=params)

        # export library

        lib.export_library(""./deploy_tuned_lib.so"")
        with open(""./deploy_tuned_graph.json"", ""w"") as fo:
            fo.write(graph)
        with open(""./deploy_tuned_param.params"", ""wb"") as fo:
            fo.write(relay.save_param_dict(params))
        
        ctx = tvm.context(str(target), 0)
        module = runtime.create(graph, lib, ctx)
        data_tvm = tvm.nd.array((np.random.uniform(size=input_shape)).astype(dtype))
        module.set_input('data', data_tvm)
        module.set_input(**params)

       
        print(""Evaluate inference time cost..."")
        ftimer = module.module.time_evaluator(""run"", ctx, number=1, repeat=600)
        prof_res = np.array(ftimer().results) * 1000  # convert to millisecond
        print(""Mean inference time (std dev): %.2f ms (%.2f ms)"" %
              (np.mean(prof_res), np.std(prof_res)))
		



tune_and_evaluate(tuning_option)`


thanks for your help"
"I'm running insightface in my local machine.

I'm doing features embedding and I have specify the model like shown bellow but whenever I run test.py it gives me the error shown bellow

**Error Statement:**
'Namespace' object has no attribute 'model'

"
"model = insightface.app.FaceAnalysis()
ctx_id = 0
model.prepare(ctx_id = ctx_id, nms=0.4)

This is how i build model. with ctx_id param i can select only one gpu for model, 

how can i choose all 4 gpus for model, is there any way ? putting in list [0,1,2,3] does not work btw.

Thanks!
"
"CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100  --data-dir      /home/xl/1mxnetinsight/insightface-master/datasets/20_faces_datasets   --prefix ../save/               --ckpt 2 --target lfw --version-se 0 

when I train, 
...
call reset()
INFO:root:Epoch[11] Train-acc=0.166667
INFO:root:Epoch[11] Time cost=0.271
call reset()
INFO:root:Epoch[12] Train-acc=0.166667
INFO:root:Epoch[12] Time cost=0.258
call reset()
INFO:root:Epoch[13] Train-acc=0.500000
INFO:root:Epoch[13] Time cost=0.263


"
"Hello
I want to estimate our model in CP-LFW and CA-LFW datasets. Where is the .bin documents of CP-LFW and CA-LFW datasets. Thank you!!"
"Hi,Our team is trying to download the trillion pair dataset from the following link
http://trillionpairs.deepglint.com/overview.
But after downloading almost 14 GB ,the download fails and we end up getting either a network error or forbidden error.
Any reasons as to why is this happening,and what can be done to fix it.
Thank you"
"I have run the test.py from the gender-age folder for the below image

Input:
![image](https://user-images.githubusercontent.com/64628244/81032727-8381d880-8eae-11ea-84a2-34380601088c.png)

For the above image , I got the following result

```
('time cost', 0.0008956350000000001)
('gender is', 1)
('age is', 27)

```
Please let me know, if any changes i need to do to get more accurate result.
"
""
"I have run the deploy/test.py and run it for lfw dataset, The test.py file uses the MTCNN for face detction and landmark.For few of the images face detction fails.

I want to try it using the Retinaface detection ,How to change the facdetection to Retinaface in deploy/test.py?"
"Hello,
I have a problem when evaluating the algorithm on the raw LFW (by following the preprocessing steps stated in this issue 'https://github.com/deepinsight/insightface/issues/791' to produce "".bin"" file), I get an accuracy of 93%!
But when using your uploaded "".bin"" file I get an accuracy of 99.8%

What do you think I might have a problem with? 
Thanks"
"I have run the deploy/test.py to test the face verificcation.
For one of my image combination i got the Dist value 1.235 for two different person image.
image 1:
![00139_931230_fb](https://user-images.githubusercontent.com/64628244/80784037-4dd0ad00-8b99-11ea-9276-a157073541a0.png)
image 2:
![00131_931230_fa](https://user-images.githubusercontent.com/64628244/80784061-6214aa00-8b99-11ea-9f0d-fabf4815fc98.png)


The default threshold was mentioned as 1.24.

Whether the threshold change with respect to image?
"
thanks 
"how to fine turning ,

I don't want to train from scratch
"
"call reset()
Traceback (most recent call last):
  File ""train.py"", line 1025, in <module>
    main()
  File ""train.py"", line 1022, in main
    train_net(args)
  File ""train.py"", line 1016, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/module/base_module.py"", line 520, in fit
    next_data_batch = next(data_iter)
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/io/io.py"", line 230, in __next__
    return self.next()
  File ""/home/xl/1mxnetinsight/insightface-master/src/data.py"", line 860, in next
    self.reset()
  File ""/home/xl/1mxnetinsight/insightface-master/src/data.py"", line 753, in reset
    random.shuffle(self.seq)
  File ""/home/xl/anaconda3/envs/test/lib/python3.5/random.py"", line 281, in shuffle
    x[i], x[j] = x[j], x[i]
TypeError: 'range' object does not support item assignment
(test) xl@xl-Alienware-13-R3:~/1mxnetinsight/insightface-master/src$ 
"
"  File ""/home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [18:36:16] src/executor/graph_executor.cc:935: Shape of unspecifie arg: conv0_weight changed. This can cause the new executor to not share parameters with the old one. Please check for error in network.If this is intended, set partial_shaping=True to suppress this warning.
Stack trace:
  [bt] (0) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x4b09db) [0x7f759515b9db]
  [bt] (1) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(mxnet::exec::GraphExecutor::Reshape(bool, bool, mxnet::Context const&, std::map<std::string, mxnet::Context, std::less<std::string>, std::allocator<std::pair<std::string const, mxnet::Context> > > const&, std::unordered_map<std::string, mxnet::TShape, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::TShape> > > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*)+0x819) [0x7f75972b92d9]
  [bt] (2) /home/xl/anaconda3/envs/test/lib/python3.5/site-packages/mxnet/libmxnet.so(MXExecutorReshapeEx+0x98c) [0x7f75971f526c]
  [bt] (3) /home/xl/anaconda3/envs/test/lib/python3.5/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f75c934aec0]
  [bt] (4) /home/xl/anaconda3/envs/test/lib/python3.5/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7f75c934a87d]
  [bt] (5) /home/xl/anaconda3/envs/test/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f75deeb5cbe]
  [bt] (6) /home/xl/anaconda3/envs/test/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x136e5) [0x7f75deeb66e5]
  [bt] (7) /home/xl/anaconda3/envs/test/bin/python(PyObject_Call+0x3a) [0x55a5ed54a61a]
  [bt] (8) /home/xl/anaconda3/envs/test/bin/python(PyEval_EvalFrameEx+0x4e69) [0x55a5ed5f0fb9]

model = face_model.FaceModel(args)
img = cv2.imread('1.jpg')
print(img)
img = model.get_input(img)
f1 = model.get_feature(img)
print(f1[0:10])
gender, age = model.get_ga(img)
print(gender)
print(age)
# sys.exit(0)
img = cv2.imread('1.jpg')
print (img)
f2 = model.get_feature(img)
dist = np.sum(np.square(f1-f2))
print(dist)
sim = np.dot(f1, f2.T)
print(sim)
#diff = np.subtract(source_feature, target_feature)
#dist = np.sum(np.square(diff),1)

"
"I went over the README and couldn't find clear explanation on the face alignment process (which is a preprocessing for the training). 
I will assume this is a relevant script - ""src/align/align_celeb.py"". Am i right ? 

However, this script has dependencies - the main one is MTCNN tensorflow version. Can you please mention the exact version of MTCNN you used (with url) ? 
There are many versions out there, i have seen your note :
""Any public available MTCNN can be used to align the faces, and the performance should not change.""
But still it would be helpful to know which library you used, and if there are any other dependencies that should be installed (not mentioned at all at the README, except for mxnet)"
"I am training vargfacenet with inital learning rate as 0.01 because learining rate 0.1 gives nan loss.
Other accuracy seems fine but cfp_fp accuracy is too low.

dataset : retina


testing verification..
(12000, 512)
infer time 10.723083
[lfw][84000]XNorm: 11.444277
[lfw][84000]Accuracy-Flip: 0.99117+-0.00460
FAR : 0.0013333333333333335
testing verification..
(14000, 512)
infer time 11.187130000000005
[cfp_fp][84000]XNorm: 270.588836
[cfp_fp][84000]Accuracy-Flip: 0.68929+-0.01869
FAR : 0.001142857142857143
testing verification..
(12000, 512)
infer time 9.597576
[agedb_30][84000]XNorm: 10.858622
[agedb_30][84000]Accuracy-Flip: 0.91533+-0.01802
FAR : 0.0013333333333333335"
"I noticed 6 filters in face_rpn_type_score, so three classes are trained. 
I guess it's ""not_face, face, face_with_mask"" ?
@nttstar "
"Could you explain how to choose the scales constant in test.py? 

The model is loaded in retinaface.py in __init__, (around line 158):

`image_size = (640, 640)`
`self.model = mx.mod.Module(symbol=sym, context=self.ctx, label_names = None) elf.model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))], for_training=False) self.model.set_params(arg_params, aux_params)
`

So I expect to only being able to provide inputs to the network of shape [1, 3, 640, 640]. Am I wrong?

However... then detect() in retinaface.py takes the scale (which is defined in test.py) as input and does some operations, which lead to an input size that could very well be different than [1, 3 ,640, 640].

How should we choose the constant scales = [..., ...] in test.py?

This question was already asked in issue #684 , with many people supporting the question, but no answer was provided. 请指教 @nttstar @jiankangdeng 

"
"Hi,

Is there any retinaface c++ version mxnet which is the same accuracy and the accurately point landmarks ? 

Best

"
"I am running the pre-trained model RetinaFace-R50 and with my mac pro it takes about 4s (0.25fps) to process an image (only inference time, w/o loading/pre/post-processing).

The paper says that the bigger RetinaFace model, with ResNet152 backbone, runs in. 75 ms (13fps). This is more than 50 times faster than what I experience with a CPU and on a smaller model. Is this normal? 

To draw an analogy, using the GPU on the MobileNet-0.25 model, the execution is about 12 times faster than with the CPU. However in my case I am using the CPU on a smaller model and, despite that, it is 50 times slower. 

From the paper: 
<img width=""346"" alt=""Screenshot 2020-04-17 at 10 59 30"" src=""https://user-images.githubusercontent.com/17983807/79564011-287a7400-80ae-11ea-8212-1e08c7475858.png"">

"
"I have the .params and .json files (parameters and model), now what?
I would like to use this model. I am looking at the test.py sript but I have no idea how to use it with the downloaded files. Should I pass them as an argument somewhere? Should I save them somewhere speifically?

Could you write a few lines of explanation to help beginners to make this run?

Thank you in advance.  "
"On model zoo, there is a pre-trained model (LResNet100E-IR,ArcFace@ms1m-refine-v2) that by the name seems to be trained with ArcFace loss.
However, if you look into the log file, the margins are set as margin_a=1.0, margin_b=0.2, margin_m=0.3, which is combined margin loss.

So, my question is: which loss was LResNet100E-IR,ArcFace@ms1m-refine-v2 model actually trained on?

Thank you!"
"Hello, I want to reproduce iQIYI_VID benchmark using ArcFace. In ArcFace paper, ArcFace gets 79.8% MAP without MLP and 88.65% with MLP.

I already checked out MLP code and trained it.

I can get frame-level face features and face label. But, i can't get video-level face features and face label without MLP which is introduced in your github.

Please tell me how to get iQIYI_VID MAP without MLP. 

thank you :)"
"Whether the network structure of retinaface can be adjusted for face recognition?
"
""
I want to speed up the Retinaface-Mnet training and the bottle neck is IO (because GPU loading is low). Is the data loading on only 1 thread? is there other way that use multi-thread to load the data?
"Hi, the NIST FRVT report link ( https://www.nist.gov/sites/default/files/documents/2019/04/04/frvt_report_2019_04_04.pdf ) in the README seems invalid. Could you provide a new link? Thanks a lot!"
Do you have plan to release the dataset for training RetinaFace Anti Cov? And how to train it ? Thanks
"@nttstar i want to merge deepglint and ms1m-arcface datasets to create a larger dataset,  but i cannot map the label of ms1m-arcface with deepglint while both ms1m-arc and asian-celeb are labeled with numbers. is it feasible?"
"I got face locations from face.bbox but how can i display it? 
""""""
  top, right, bottom, left = face.bbox.astype(np.int)
  face_image = img[top:bottom, left:right]
  pil_image = Image.fromarray(face_image)
  pil_image.show()
""""""
these block returns  tile cannot extend outside image. Any idea ?"
"Why Retinaface not use multi-scale training? 
I had to change the code for multi-scale training, but i failed........"
您好，想请教一下年龄与性别分类的模型在训练和验证的时候，分别是使用了哪些资料集呢？
"I want to fine-tune res50 pretrained model on VGG2 dataset. The res50 is originally trained on MS1M. Fc7 layer's weights of res50 are not provided and only base network weights are available. I want to freeze pretrained weights and first only train classification layer for VGG2. Once classification loss is reduced then I want to train entire network. Can anyone help, how to update train.py script for freezing layers before fc7?"
"Hi,
I am splitting the LFW  dataset into train,validation and test(70% train, 10% validation , 20% test).
and keep getting this error message at testing verification :

` raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [16:00:07] include/mxnet/././tensor_blob.h:290: Check failed: this->shape_.Size() == static_cast<size_t>(shape.Size()) (17492755746 vs. 312886562) : TBlob.get_with_shape: new and old shape do not match total elements
`

1. Do you have an example of splitting the LFW into train,validation and test.?
2. Do you have any idea why I keep getting this error message?"
"Hi,

First of all thanks for such power full face detection algorithm.

How can I a get a score for detected face confidence ? 

And is there any way to get whether landmark visible or not ? 

I attached few test images and I don't want them detected.

Any idea ? 
Best 


![Screenshot from 2020-03-09 18-14-24](https://user-images.githubusercontent.com/17505439/76230590-f49d6a80-6234-11ea-8a49-c41d9402db57.png)
![Screenshot from 2020-03-09 18-13-37](https://user-images.githubusercontent.com/17505439/76230594-f6672e00-6234-11ea-8214-0551ebb08c4d.png)
![Screenshot from 2020-03-09 18-13-28](https://user-images.githubusercontent.com/17505439/76230598-f7985b00-6234-11ea-9c33-ee1803c32885.png)
"
"Hi,

I came across XNorm in verification.py. From the code, It's the mean of vector norm of each embedding in test set. Does the value have anything to do with model accuracy? what's the interpretation of high/low XNorm?

Thanks
Peeranat F."
"```
import numpy as np
import nnvm.compiler
import nnvm.testing
import tvm
from tvm.contrib import graph_runtime
import mxnet as mx
from mxnet import ndarray as nd

prefix,epoch = ""/home/xyz/Downloads/github/insightface/gender-age/model/model"",0
sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
image_size = (112, 112)
opt_level = 3

shape_dict = {'data': (1, 3, *image_size)}
#target = tvm.target.create(""llvm -mcpu=skylake"")
#target = tvm.target.cuda(""llvm device=0"")
# ""target"" means your target platform you want to compile.

target = tvm.target.create(""llvm -mcpu=broadwell"")
nnvm_sym, nnvm_params = nnvm.frontend.from_mxnet(sym, arg_params, aux_params)
with nnvm.compiler.build_config(opt_level=opt_level):
   graph, lib, params = nnvm.compiler.build(nnvm_sym, target, shape_dict, params=nnvm_params)
lib.export_library(""./model_ga_cpu.so"")
print('lib export succeefully')
with open(""./model_ga_cpu.json"", ""w"") as fo:
   fo.write(graph.json())
with open(""./model_ga_cpu.params"", ""wb"") as fo:
   fo.write(nnvm.compiler.save_param_dict(params))
```

Output:

```
[16:35:49] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.3.0. Attempting to upgrade...
[16:35:49] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 3, 112, 112, 'float32'), (8, 3, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 8, 112, 112, 'float32'), (8, 1, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 8, 112, 112, 'float32'), (16, 8, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 16, 112, 112, 'float32'), (16, 1, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 16, 56, 56, 'float32'), (32, 16, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 32, 56, 56, 'float32'), (32, 1, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 32, 56, 56, 'float32'), (32, 32, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 32, 56, 56, 'float32'), (32, 1, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 32, 28, 28, 'float32'), (64, 32, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 64, 28, 28, 'float32'), (64, 1, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 64, 28, 28, 'float32'), (64, 64, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 64, 28, 28, 'float32'), (64, 1, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 64, 14, 14, 'float32'), (128, 64, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 128, 14, 14, 'float32'), (128, 1, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 128, 14, 14, 'float32'), (128, 128, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 128, 14, 14, 'float32'), (128, 1, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 128, 7, 7, 'float32'), (256, 128, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('depthwise_conv2d_nchw', (1, 256, 7, 7, 'float32'), (256, 1, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=broadwell, workload=('conv2d', (1, 256, 7, 7, 'float32'), (256, 256, 1, 1, 'float32'), (1, 1), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Traceback (most recent call last):
  File ""convert.py"", line 23, in <module>
    lib.export_library(""./model_ga_cpu.so"")
  File ""/opt/softwares/tvm/python/tvm/module.py"", line 128, in export_library
    fcompile(file_name, files, **kwargs)
  File ""/opt/softwares/tvm/python/tvm/contrib/cc.py"", line 33, in create_shared
    _linux_shared(output, objects, options, cc)
  File ""/opt/softwares/tvm/python/tvm/contrib/cc.py"", line 58, in _linux_shared
    raise RuntimeError(msg)
RuntimeError: Compilation error:
/usr/bin/ld: /tmp/tmpiczs4sg5/lib.o: relocation R_X86_64_32S against `.rodata.cst4' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: Nonrepresentable section on output
collect2: error: ld returned 1 exit status

```"
Any way we can control gpu usage ? 
"QQ Group ID: 711302608.

For English developers and who can not use QQ:

1. Install WeChat: https://www.wechat.com/en/
2. Add me as your contact. 
3. I will invite you to our WeChat group.
4. You can also access WeChat web at https://web.wechat.com/.

Any problem can be left in this issue.
"
"I have a set of 1900+ 512-D Facial embeddings/vectors, I'd like to group all similar individuals/faces. There are also an unknown number of distinct faces.

I've employed [sklearn.cluster.DBSCAN][1] similar to the suggestion in [PyImageSearch Face Clustering with Python][2]. However, it can't cluster effectively, returning 0 clusters. I believe the matrix is too sparse. And believe there are a couple options: 

 - Calculate Euclidean similarity for each of 1900 combinations - slow, even with matrix multiply, but it works
 - Employ dimensionality reduction/PCA to 128-D vector and try to use DBSCAN 
 - Use Nearest-Neighbors - I would have to know how many different people there are beforehand
 - Chinese Whisper Clustering

In the process of trying the different methodologies right now, but perhaps there is a well-known method/approach I'm missing?

  [1]: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html
  [2]: https://www.pyimagesearch.com/2018/07/09/face-clustering-with-python/"
"Hi,
when i run test.py, I meet an error like this: ModuleNotFoundError: No module named 'RetinaFaceAntiCov.rcnn.cython'. can you help me,thank you "
"Please, can anyone help me about how to visualize 2D or 3D feature mapping of losses like Fig.3 in the paper using my own dataset? I want to get 7 classes not 8."
""
"### my computer don't have GPU. I modify:
```
detector = RetinaFace('./model/R50', -1, gpuid, 'net3')
```

### when run Test.py. This shows error:

```
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [09:04:55] /tmp/build/80754af9/libmxnet_1564766659613/work/3rdparty/dmlc-core/src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open ""./model/R50--001.params"": No such file or directory
Stack trace:
  [bt] (0) /home/tu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(dmlc::io::LocalFileSystem::Open(dmlc::io::URI const&, char const*, bool)+0x5a9) [0x7f006ebb08a9]
  [bt] (1) /home/tu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(dmlc::Stream::Create(char const*, char const*, bool)+0x241) [0x7f006eb80251]
  [bt] (2) /home/tu/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(MXNDArrayLoad+0xa9) [0x7f006c7ceb89]
  [bt] (3) /home/tu/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f007c671ec0]
  [bt] (4) /home/tu/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7f007c67187d]
  [bt] (5) /home/tu/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f007c887f7e]
  [bt] (6) /home/tu/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x139b4) [0x7f007c8889b4]
  [bt] (7) python(_PyObject_FastCallKeywords+0x49b) [0x55e1848e9d2b]
  [bt] (8) python(_PyEval_EvalFrameDefault+0x537e) [0x55e1849457ae]
```

"
https://github.com/apache/incubator-tvm/issues/4830
"Why my progress was killed without /var/log?And it is ok just yesterday,I never change anything!
"
""
"Hi, I want to re-train your age/gender model using my dataset. Firstly I should prepare .lst file. As I understand structure of file will be like this:
`idx \t gender_label \t age_label \t image_path`
Is it true?
Also, could you explain in which format should be labels (gender: 1=male, 0=female; and age in range from 0 to 99?) "
"I have already used Dlib and its not accurate. 
Can anyone suggest what can i do to just fetch face landmarks ? "
"So I'm using the pre-trained model arcface_r100_v1 and img='https://drive.google.com/open?id=16PKySktkjj3579S1F9a9mmCmMRm1fH8u', I get the error AttributeError: 'NoneType' object has no attribute 'shape'

I am using the python 2.7 insightface package.

Here is the log:

Traceback (most recent call last):
  File ""FaceRecognitionTest.py"", line 55, in <module>
    emb2 = model.get_embedding(img2)
  File ""/usr/local/lib/python2.7/dist-packages/insightface/model_zoo/face_recognition.py"", line 48, in get_embedding
    assert img.shape[2]==3 and img.shape[0:2]==self.image_size
AttributeError: 'NoneType' object has no attribute 'shape'"
"ArcFace is excellent! And I have created a series of blogs, _namely Build your own face recognition model_  . and I hope the blogs are helpful for others.

[Blogs Github](https://github.com/siriusdemon/Build-Your-Own-Face-Model/tree/master/recognition)"
"Hi,
Is there a way to create .bin & .dex files  out of LFW train file ""pairsDevTrain.txt""([http://vis-www.cs.umass.edu/lfw/pairsDevTrain.txt](url))?

Thanks"
""
"Hi,

I have questions concerning the connection between the alignment pipeline and its influence on the recognition performance.

1. Which pipeline are you using for the FRVT submission? Training on MTCNN aligned images with Resnet 101/Arcface loss and using Retina Face only in the inference step?

2. It seems that you only provide MTCNN-aligned training DBs, wouldn´t it make sense to use Retina Face alignment also for the training DBs (not only at the test/production stage) ?

3. Are there benchmarks showing the influence of the alignment pipeline on the overall recognition performance?

Kind regards,

Christian

"
"I'm running Python 2.7 in Google Colab and downloaded the pre-trained model as well from the model zoo. While running the verification.py file I'm getting the following error:- 

`IOErrorTraceback (most recent call last)
/content/insightface/src/eval/verification.py in <module>()
    518   args = parser.parse_args()
    519 
--> 520   prop = face_image.load_property(args.data_dir)
    521   image_size = prop.image_size
    522   print('image_size', image_size)

/content/insightface/src/eval/../common/face_image.py in load_property(data_dir)
      8 def load_property(data_dir):
      9   prop = edict()
---> 10   for line in open(os.path.join(data_dir, 'property')):
     11     vec = line.strip().split(',')
     12     assert len(vec)==3

IOError: [Errno 2] No such file or directory: 'property'`

I'm new to Insightface. Can anyone help me out or tell me what additional info is needed to solve the problem? "
"想请教一下，为什么model-zoo的pretrained model的checkpoint都是0？
这些model的名字后面都是0000，导入的时候checkpoint就都是0了，我试着手动改为1也没问题，想请教一下，使用pretrained model时这里的checkpoint可以更改为别的吗？"
"mxnet.base.MXNetError: [16:59:51] /Users/travis/build/dmlc/mxnet-distro/mxnet-build/3rdparty/tvm/nnvm/include/nnvm/graph.h:263: Check failed: it != attrs.end(): Cannot find attribute shape in the graph
Stack trace:
  [bt] (0) 1   libmxnet.so                         0x0000001c19046929 mxnet::op::NDArrayOpProp::~NDArrayOpProp() + 4473
  [bt] (1) 2   libmxnet.so                         0x0000001c19045d19 mxnet::op::NDArrayOpProp::~NDArrayOpProp() + 1385
  [bt] (2) 3   libmxnet.so                         0x0000001c1a55df53 mxnet::exec::GraphExecutor::Backward(std::__1::vector<mxnet::NDArray, std::__1::allocator<mxnet::NDArray> > const&, bool) + 4275
  [bt] (3) 4   libmxnet.so                         0x0000001c1a55b30c std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, mxnet::NDArray, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, mxnet::NDArray> > >::~unordered_map() + 1724
  [bt] (4) 5   libmxnet.so                         0x0000001c1a4f256e MXExecutorForward + 46
  [bt] (5) 6   libffi.6.dylib                      0x0000000102420884 ffi_call_unix64 + 76
  [bt] (6) 7   ???                                 0x00007000028506e0 0x0 + 123145344583392"
The code is not clear.
"Hi! Is there a way to adjust the accuracy of facial recognition so that no faces are detected in the examples I've attached?

![sample1](https://user-images.githubusercontent.com/59647158/71971956-2722eb00-3225-11ea-9d5f-3971536886e6.jpg)

And

![sample2](https://user-images.githubusercontent.com/59647158/71971997-3bff7e80-3225-11ea-9eba-b63a6942c7c1.jpg)

Thank you very much!"
"The training speed is mentioned in paper(8*1080it,8*64,800samples/s),i really want to know the training time with the class number ranging from 10k to 100M.How long does it take to converge?Do you use pretrained model?
Looking for your replay."
"运行Evaluation/Megaface的run.sh评估代码的时候报错，如下：
`reading mf 1027000
707
./run.sh: line 10: cd: /raid5data/dplearn/megaface/devkit/experiments: No such file or di
python: can't open file 'run_experiment.py': [Errno 2] No such file or directory
./run.sh: line 12: cd: OLDPWD not set`

请问**run_experiment.py**这个文件在哪里？"
"![image](https://user-images.githubusercontent.com/6336709/71877643-ad2b2d00-3164-11ea-84af-b4a4f3fc2590.png)

The image above shows the visualization of 0_Parade_marchingband_1_657.jpg

bboxes and landmarks are from retinaface_gt_v1.1 dataset

As can be seen from the training code, landmark score 1 means invisible, 0 means visible.
landmark color: black indicates visible point, red indicates invisible point.

But as can be seen from the above picture, these points are all visible."
"i had following the step to do face detection using retina face and got rectangle bounding boxes output as like on example, could someone please guide me to do realtime recognition like shown in video? i'm currently pretty new at face recognition task, thank you."
"![image](https://user-images.githubusercontent.com/41290343/71776563-aff41980-2fce-11ea-9381-716e5f132d59.png)
![image](https://user-images.githubusercontent.com/41290343/71776589-17aa6480-2fcf-11ea-9c0f-a14b3f5478dd.png)
![image](https://user-images.githubusercontent.com/41290343/71778784-2f90e100-2fed-11ea-85d9-82232ffcf4a8.png)

because of the memory limit of GPU,so i only use one verfication data LFW，and batchsize is 128， and the network is resnet34, loss is arcface, GPU is one Tesla P100, after epoch 16, as you can see in the picture, so what's going on? can anybody give a hand? Thanks in advance."
"![image](https://user-images.githubusercontent.com/41290343/71776453-e6309980-2fcc-11ea-8a8e-e29f29535b6e.png)
There is no `residual_unit_v4` in the fresnet.py file.  `residual_unit_v4`  function is `residual_unit_v3_x `function or something?"
"Hello there and thank you very much for your great contribution.
If I may, I'd like to ask for the code wich lets me replicate your results on LFW ""Face Verification"" result. 
I had a look in the repository but couldnt really find any model for that or the way it is trained or evaluated. 
I'd greatly appreciate it if you could kindly direct me in the right direction. 
"
"我想用test.py测试一下预训练的ResNet-50速度，但是结果如下：
A uses 9.729929 seconds
A uses 9.730986 seconds
A uses 9.732174 seconds
X1 uses 9.735658 seconds
X2 uses 9.740575 seconds
X3 uses 9.742364 seconds
显卡是GTX1080Ti，图片尺寸是640*480，系统是Ubuntu16.04；
我把输入部分改成了：
`faces, landmarks = detector.detect([img,img,img], thresh, scales=scales, do_flip=flip)`
但是三次时间都一样
这个速度远低于ResNet-152，但是不知道原因在哪，求大佬解答"
"I've trained a retinaface model on my own dataset,but i do not know how to use the trained retinaface to crop and align a batch of face images,can you tell me how to do this?Thank you."
"I have download cfp_fp dataset.but there is not .mat file,I see your data/cfp2pack.py use loadmat to generate cfp_fp.bin,could you tell me how to make cfp_fp.bin?"
Isn't RTX2080Ti faster than P40?
"Hello, I am a new user, install all dependencies, while compiling the train.py file it gives bellow error and in my computer, there is not support of GPU
Traceback (most recent call last):
  File ""insightface/RetinaFace/train.py"", line 14, in <module>
    from rcnn.symbol import *
  File ""insightface\RetinaFace\rcnn\symbol\__init__.py"", line 2, in <module>
    from .symbol_mnet import *
  File ""insightface\RetinaFace\rcnn\symbol\symbol_mnet.py"", line 9, in <module>
    from rcnn.symbol.symbol_common import get_sym_train
  File ""insightface\RetinaFace\rcnn\symbol\symbol_common.py"", line 5, in <module>
    from rcnn.PY_OP import rpn_fpn_ohem3, cascade_refine
  File ""insightface\RetinaFace\rcnn\PY_OP\cascade_refine.py"", line 9, in <module>
    from ..processing.generate_anchor import generate_anchors, anchors_plane
  File ""insightface\RetinaFace\rcnn\processing\generate_anchor.py"", line 8, in <module>
    from ..cython.anchors import anchors_cython
ModuleNotFoundError: No module named 'rcnn.cython.anchors'
 "
"After calling detector.detect(), randomly the total GPU memory consumption will increase. 
It will cause out of memory error in the end."
"I try convert model to MMdnn. I'm do not understand img size.
`python3 -m mmdnn.conversion._script.convertToIR -f mxnet -n R50-symbol.json -w R50-0000.params -d resnet50 --inputShape 3,?,?`"
"@yingfeng 
python test.py 
Traceback (most recent call last):
  File ""test.py"", line 1, in <module>
    import face_model
  File ""/media/administrator/hdd/Age_gender_identification/insightface/gender-age/face_model.py"", line 11, in <module>
    import mxnet as mx
  File ""/home/administrator/gender_identification/lib/python3.7/site-packages/mxnet/__init__.py"", line 24, in <module>
    from .context import Context, current_context, cpu, gpu, cpu_pinned
  File ""/home/administrator/gender_identification/lib/python3.7/site-packages/mxnet/context.py"", line 24, in <module>
    from .base import classproperty, with_metaclass, _MXClassPropertyMetaClass
  File ""/home/administrator/gender_identification/lib/python3.7/site-packages/mxnet/base.py"", line 213, in <module>
    _LIB = _load_lib()
  File ""/home/administrator/gender_identification/lib/python3.7/site-packages/mxnet/base.py"", line 204, in _load_lib
    lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_LOCAL)
  File ""/usr/local/lib/python3.7/ctypes/__init__.py"", line 364, in __init__
    self._handle = _dlopen(self._name, mode)
**OSError: libcudart.so.9.0: cannot open shared object file: No such file or directory**
"
"Please fix this issue. These are results based on ResNet-50 encoder. Could you release ResNet-152 model please?
I'm using it to get some hard negatives, but it seems I get a lot of False Positives with high confidence.

![13982937013_ae8270fcff_b](https://user-images.githubusercontent.com/1523071/70895860-5bc1cf00-1ff8-11ea-8711-0379abf7a911.jpg)


![4033666125_fd7e8e8abb_b](https://user-images.githubusercontent.com/1523071/70895780-2fa64e00-1ff8-11ea-9fb9-492be76f26a5.jpg)

![31412037288_0d8472755a_b](https://user-images.githubusercontent.com/1523071/70895832-4b115900-1ff8-11ea-95d8-dc1c71b21ff1.jpg)

![2832001568_7b7ff6c7f4_b](https://user-images.githubusercontent.com/1523071/70895887-67ad9100-1ff8-11ea-833f-d3690115a699.jpg)


"
"How to use gender-age?
Here is a documentation：
https://github.com/gengyanlei/insightFace-gulon/blob/master/gender-age-symbol-instructions.pdf"
"I want to give two input to the recognition network. the one is the image, the other is the rotation of the face in image. How can I do it?"
"Hello @yangfly @vizakshat @MyraBaba  @yingfeng @nttstar 
I have change the code detection code from mtcnn to retina face 
`ret = self.detector.detect_face(face_img, det_type = self.args.det)` to retina face  ` detector.detect(img, thresh, scales=scales, do_flip=flip)` 
but when `nimg = face_preprocess.preprocess(face_img, bbox, points1, image_size='112,112')` 
is called i am not getting the proper face i am getting full frame insterd of crop align face 
is there anything I am missing."
"when I am trying to use train_parall.py  to train a network, I found the used memory(not gpu memory) is increase, has anyone met this issue before?"
"INFO:root:output shape {'blockgrad0_output': (8, 800),
 'blockgrad1_output': (8, 8, 400),
 'blockgrad2_output': (8, 20, 400),
 'blockgrad3_output': (8, 3200),
 'blockgrad4_output': (8, 8, 1600),
 'blockgrad5_output': (8, 20, 1600),
 'blockgrad6_output': (8, 12800),
 'blockgrad7_output': (8, 8, 6400),
 'blockgrad8_output': (8, 20, 6400),
 'face_rpn_bbox_loss_stride16_output': (8, 8, 1600),
 'face_rpn_bbox_loss_stride32_output': (8, 8, 400),
 'face_rpn_bbox_loss_stride8_output': (8, 8, 6400),
 'face_rpn_cls_prob_stride16_output': (8, 2, 3200),
 'face_rpn_cls_prob_stride32_output': (8, 2, 800),
 'face_rpn_cls_prob_stride8_output': (8, 2, 12800),
 'face_rpn_landmark_loss_stride16_output': (8, 20, 1600),
 'face_rpn_landmark_loss_stride32_output': (8, 20, 400),
 'face_rpn_landmark_loss_stride8_output': (8, 20, 6400)}
fixed []
INFO:root:lr 0.001000 lr_epoch_diff [1, 2, 3, 4, 5, 55, 68, 80] lr_steps [(3219, 1.5849), (6438, 1.5849), (9657, 1.5849), (12876, 1.5849), (16095, 1.5849), (177045, 0.1), (218892, 0.1), (257520, 0.1)]
Error in rpn_fpn_ohem3.create_operator: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/operator.py"", line 968, in create_operator_entry
    op = op_prop.create_operator(ctx, shapes, dtypes)
  File ""/home1/houjr/project/insightface/RetinaFace/rcnn/PY_OP/rpn_fpn_ohem3.py"", line 162, in create_operator
    return RPNFPNOHEM3Operator(self.stride, self.network, self.dataset, self.prefix)
  File ""/home1/houjr/project/insightface/RetinaFace/rcnn/PY_OP/rpn_fpn_ohem3.py"", line 19, in __init__
    self.mode = config.TRAIN.OHEM_MODE #0 for random 10:245, 1 for 10:246, 2 for 10:30, mode 1 for default
AttributeError: 'EasyDict' object has no attribute 'OHEM_MODE'

Traceback (most recent call last):
  File ""train.py"", line 375, in <module>
    main()
  File ""train.py"", line 372, in main
    lr=args.lr, lr_step=args.lr_step)
  File ""train.py"", line 321, in train_net
    arg_params=arg_params, aux_params=aux_params, begin_epoch=begin_epoch, num_epoch=end_epoch)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 1629, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
face_bbox_weight_stride8: (8, 8, 6400)
face_landmark_weight_stride32: (8, 20, 400)
face_label_stride8: (8, 12800)
face_landmark_target_stride8: (8, 20, 6400)
face_landmark_target_stride16: (8, 20, 1600)
face_landmark_weight_stride16: (8, 20, 1600)
face_landmark_target_stride32: (8, 20, 400)
face_label_stride32: (8, 800)
face_label_stride16: (8, 3200)
face_bbox_target_stride32: (8, 8, 400)
face_bbox_weight_stride16: (8, 8, 1600)
face_bbox_target_stride16: (8, 8, 1600)
face_bbox_weight_stride32: (8, 8, 400)
face_landmark_weight_stride8: (8, 20, 6400)
data: (8, 3, 640, 640)
face_bbox_target_stride8: (8, 8, 6400)
[11:00:51] src/operator/custom/custom.cc:282: Check failed: reinterpret_cast<CustomOpCreateFunc>( params.info->callbacks[kCustomOpPropCreateOperator])( os.str().c_str(), shapes.size(), shapes.data(), ndims.data(), in_type.data(), op_info, params.info->contexts[kCustomOpPropCreateOperator]): 
Stack trace:
  [bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x4958fb) [0x7fac7d5e18fb]
  [bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x7d09d9) [0x7fac7d91c9d9]
  [bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x7c4715) [0x7fac7d910715]
  [bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x25bd50d) [0x7fac7f70950d]
  [bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x25c0b68) [0x7fac7f70cb68]
  [bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(mxnet::exec::GraphExecutor::FinishInitGraph(nnvm::Symbol, nnvm::Graph, mxnet::Executor*, std::unordered_map<nnvm::NodeEntry, mxnet::NDArray, nnvm::NodeEntryHash, nnvm::NodeEntryEqual, std::allocator<std::pair<nnvm::NodeEntry const, mxnet::NDArray> > > const&)+0x793) [0x7fac7f739bc3]
  [bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(mxnet::exec::GraphExecutor::Init(nnvm::Symbol, mxnet::Context const&, std::map<std::string, mxnet::Context, std::less<std::string>, std::allocator<std::pair<std::string const, mxnet::Context> > > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::unordered_map<std::string, mxnet::TShape, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::TShape> > > const&, std::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, int> > > const&, std::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, int> > > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::unordered_map<std::string, mxnet::NDArray, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::NDArray> > >*, mxnet::Executor*, std::unordered_map<nnvm::NodeEntry, mxnet::NDArray, nnvm::NodeEntryHash, nnvm::NodeEntryEqual, std::allocator<std::pair<nnvm::NodeEntry const, mxnet::NDArray> > > const&)+0x73d) [0x7fac7f74037d]
  [bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(mxnet::Executor::SimpleBind(nnvm::Symbol, mxnet::Context const&, std::map<std::string, mxnet::Context, std::less<std::string>, std::allocator<std::pair<std::string const, mxnet::Context> > > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::unordered_map<std::string, mxnet::TShape, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::TShape> > > const&, std::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, int> > > const&, std::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, int> > > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::unordered_map<std::string, mxnet::NDArray, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::NDArray> > >*, mxnet::Executor*)+0x8a8) [0x7fac7f741228]
  [bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBindEx+0x221b) [0x7fac7f682d9b]"
"Thank you for sharing the project.
I ran a verification on a pre-trained model as written in the README.md file:

$INSIGHTFACE/src/eval/verification.py

and the scores:

[agedb_30]XNorm: 22.654594
[agedb_30]Accuracy: 0.00000+-0.00000
[agedb_30]Accuracy-Flip: 0.98250+-0.00712
Max of [agedb_30] is 0.98250

Can you explained what is the difference between the scores & especially between Accuracy and Accuracy-Flip?"
您好，请问CFP-FP人脸数据集标准的测试方法是什么？我是从http://www.cfpw.io/网站下载的数据，然后经过检测-对齐-识别整个流程，发现在对齐那块就差很多，很多profile脸图像对齐以后，留出了大黑边。
"人脸检测对齐使用两种不同的mtcnn实现
1.第一种完全基于opencv实现mtcnn
2.第二种使用insightface中deploy目录下的mtcnn实现

数据集：
1.自己制作的小的数据集（6000张测试数据，400张底图）

比对：
使用欧式距离作为比对结果

阈值：1.24

结果：
1.两种实现，最后结果差异在5个像素以内
2.设置相同的阈值（1.24），对同样的数据集进行测试，最后精确率和召回率会相差很大
3..使用第一种mtcnn实现做人脸检测，精确率很低但是召回率很高
    使用第二种mtcnn实现做人脸检测，召回率低但是精确率高

做了如下尝试
1.使用第一种mtcnn实现做人脸检测，并不断的调低阈值，最终能够得到一个接近第二种实现的结果

问题：
人脸检测的结果是否会影响到阈值的选择？
是否更换人脸检测模块？
需要重新测试获取一个合适的阈值？
为什么人脸检测结果的差异会导致阈值发生偏移呢？


"
"The readme is out of line with the project, for example in readme it says to perform the training as follows:

`CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore`

but the option --loss and --dataset doesn´t exists, in fact now is --loss-type and --data-dir, and looking at the code, I realized that these options work completely differently from the previous ones."
"你好，我用你提供的widerface数据和代码训练retinaface人脸检测，但是FGacc为0。你能提供一些帮助么
INFO:root:Epoch[79] Train-RPNAcc_s32=0.995380
INFO:root:Epoch[79] Train-RPNAcc_s32_BG=0.999160
INFO:root:Epoch[79] Train-RPNAcc_s32_FG=0.000000
INFO:root:Epoch[79] Train-RPNL1Loss_s32=0.097950
INFO:root:Epoch[79] Train-RPNLandMarkL1Loss_s32=0.109029
INFO:root:Epoch[79] Train-RPNAcc_s16=0.993666
INFO:root:Epoch[79] Train-RPNAcc_s16_BG=0.998725
INFO:root:Epoch[79] Train-RPNAcc_s16_FG=0.000000
INFO:root:Epoch[79] Train-RPNL1Loss_s16=0.101463
INFO:root:Epoch[79] Train-RPNLandMarkL1Loss_s16=0.114548
INFO:root:Epoch[79] Train-RPNAcc_s8=0.991024
INFO:root:Epoch[79] Train-RPNAcc_s8_BG=0.999038
INFO:root:Epoch[79] Train-RPNAcc_s8_FG=0.000000
INFO:root:Epoch[79] Train-RPNL1Loss_s8=0.157918
INFO:root:Epoch[79] Train-RPNLandMarkL1Loss_s8=0.234281
INFO:root:Epoch[79] Time cost=1157.834"
"when I running RetinaFace/train.py   it raise ValueError: duplicate names detected at line 317   where the code is mod.fit(....), so what can i do ？"
"I am trying to accelerate the face detection model RetinaFace on CPU. So I deployed the optimization tool OpenVINO. However, after successfully converting the mxnet model to the openvino mode (Intermediate Representation), the speed of the openvino model is much slower than that of the pure mxnet deployment. (~2s vs ~0.001s)

Is there anyone that has the experience converting this model to openvino? I would like to know the reason why. "
""
"你好，
我用你提供的数据，mobilenet0.25进行从头开始训练，现在训练了70epoch，做预测时候预测的分数非常低，可能的原因是什么，预测是用test.py默认的配置
修改过的地方，use_crop=False和fixed_param_names=None
下面是我画出来的loss和预测结果
![predicet](https://user-images.githubusercontent.com/43072388/70017013-de8c6800-15bc-11ea-94c9-e166d0f9606a.JPG)
![fixed](https://user-images.githubusercontent.com/43072388/70017014-de8c6800-15bc-11ea-8225-1553ccb4bf12.JPG)
![loss](https://user-images.githubusercontent.com/43072388/70017015-df24fe80-15bc-11ea-9459-f47acdcede02.JPG)
"
""
"From the documentation(https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.SimilarityTransform), I notice that the order given to estimate is swapped. 

https://github.com/deepinsight/insightface/blob/4a4b8d03fec981912fdef5b3232a37a827cbeed6/src/common/face_preprocess.py#L61-L72

Is there any reason to swap the dst and src? Is that a mistake? If it's intended, it would be more clear if it's written as estimate(src=dst, dst=src) to avoid any confusion. "
"Hi insightface,
In this code which python version(2.7 or 3.6)are you using?
best regards,
PeterPham"
"Hello, when I train vargfacenet with arcface, the problem always exists. The value of xnorm is very big and accuracy is very low in 'cfp_fp'.  But in others validation datasets, it is normal. Could you help me? Thanks a lot.
![image](https://user-images.githubusercontent.com/18735130/69781230-6b63aa00-11e8-11ea-8a16-e1a93d348ccf.png)
My command line is below:
``` shell
python -u train.py --network vargfacenet --loss arcface --dataset retina --lr 0.01
```
The Xavier method is below:
```python
initializer = mx.init.Xavier(rnd_type='gaussian', factor_type='out', magnitude=2)
```"
"Hi,
How did you annotate the blur for each face's bounding box?
Did you do it manually or did you use some pretrained model for this task? If it's the latter, can you tell which model did you use to annotate the blur level?"
"Hello, I'm newbie at this field and this question would be very naive.
How many training images is it enough to inference good in your model?
and if your model is not supporting one or few-shot learning for face recognition, can you please tell me whats is SOTA model in those way?"
"I have this question, can you help me? thank you very much!

call reset()
Traceback (most recent call last):
  File ""train.py"", line 378, in <module>
    main()
  File ""train.py"", line 375, in main
    train_net(args)
  File ""train.py"", line 370, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""/home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 1629, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (4, 3, 112, 112)
softmax_label: (4,)
[13:19:41] src/engine/./../common/cuda_utils.h:310: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: invalid device ordinal
Stack trace:
  [bt] (0) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x4b03ab) [0x7f031c9d03ab]
  [bt] (1) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x25cc459) [0x7f031eaec459]
  [bt] (2) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2e64d96) [0x7f031f384d96]
  [bt] (3) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2e6a96f) [0x7f031f38a96f]
  [bt] (4) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(mxnet::NDArray::NDArray(mxnet::TShape const&, mxnet::Context, bool, int)+0x5d0) [0x7f031ea515b0]
  [bt] (5) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(mxnet::common::InitZeros(mxnet::NDArrayStorageType, mxnet::TShape const&, mxnet::Context const&, int)+0x5c) [0x7f031eb052ac]
  [bt] (6) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(mxnet::common::ReshapeOrCreate(std::string const&, mxnet::TShape const&, int, mxnet::NDArrayStorageType, mxnet::Context const&, std::unordered_map<std::string, mxnet::NDArray, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::NDArray> > >*, bool)+0x3a1) [0x7f031eb189f1]
  [bt] (7) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(mxnet::exec::GraphExecutor::InitArguments(nnvm::IndexedGraph const&, std::vector<mxnet::TShape, std::allocator<mxnet::TShape> > const&, std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, mxnet::Executor const*, std::unordered_map<std::string, mxnet::NDArray, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::NDArray> > >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*)+0xb10) [0x7f031eb209c0]
  [bt] (8) /home/ai/anaconda3/envs/arcface/lib/python2.7/site-packages/mxnet/libmxnet.so(mxnet::exec::GraphExecutor::Init(nnvm::Symbol, mxnet::Context const&, std::map<std::string, mxnet::Context, std::less<std::string>, std::allocator<std::pair<std::string const, mxnet::Context> > > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::vector<mxnet::Context, std::allocator<mxnet::Context> > const&, std::unordered_map<std::string, mxnet::TShape, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::TShape> > > const&, std::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, int> > > const&, std::unordered_map<std::string, int, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, int> > > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::unordered_set<std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::string> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> >*, std::unordered_map<std::string, mxnet::NDArray, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, mxnet::NDArray> > >*, mxnet::Executor*, std::unordered_map<nnvm::NodeEntry, mxnet::NDArray, nnvm::NodeEntryHash, nnvm::NodeEntryEqual, std::allocator<std::pair<nnvm::NodeEntry const, mxnet::NDArray> > > const&)+0x6bc) [0x7f031eb2ed9c]
"
"[VarGFaceNet](https://github.com/zma-c-137/VarGFaceNet) is a new state-of-the-art light-weight neural network presented in [LFR 2019](https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/), and I want to ask if it is possible to deploy this on mobile? Anyone has hint on doing so?
Since it is light weight, it would be interesting to try running it real-time on mobile environment.

Thanks for your attention!"
"Hi authors,
When I run this code by command line: CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --prefix ./model/retina --network resnet. I am facing the issue like this:
Traceback (most recent call last):
  File ""train.py"", line 367, in <module>
    main()
  File ""train.py"", line 350, in main
    args = parse_args()
  File ""train.py"", line 344, in parse_args
    parser.add_argument('--wd', help='weight decay', default=default.wd, type=float)
AttributeError: 'EasyDict' object has no attribute 'wd'
Please give me some advice???
Best regards,
PeterPham
"
"https://github.com/deepinsight/insightface/blob/4a4b8d03fec981912fdef5b3232a37a827cbeed6/RetinaFace/train.py#L344

I am trying to train RetinaFace using resnet as backbone network. I ran into this error

```
Traceback (most recent call last):
  File ""train.py"", line 367, in <module>
    main()
  File ""train.py"", line 350, in main
    args = parse_args()
  File ""train.py"", line 344, in parse_args
    parser.add_argument('--wd', help='weight decay', default=default.wd, type=float)
AttributeError: 'EasyDict' object has no attribute 'wd'
```
Seems like there should be a `default.wd` specified in `config.py`. What's the proper weight decay?
"
"Hi all,
how can i convert demo.py to real time(webcam) recognition any idea?
Best regards,
PeterPham"
"environment: 
(1) retinaface(R50) for face detection and alignment
(2) arcface(r50) for face feature extraction
(3) cosine similarity 

I test LFW face pairs using arcface, the same face pairs has higher cosine similarity, around 0.8~0.9, but different face pairs also has high cosine similarity, around 0.6, i think it is abnormal, but i can not make out the issue. can you share me with some idea?"
"Hi 
Which IDE use for this paper (RetinaFace)?
Best regards,
PeterPham "
"I tried start deploy/test.py and have this errors:
[16:21:12] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.2.0. Attempting to upgrade...
[16:21:12] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""/home/kudryavcev-ra/PycharmProjects/insightface-masterTEST/deploy/test.py"", line 18, in <module>
    model = face_model.FaceModel(args)
  File ""/home/kudryavcev-ra/PycharmProjects/insightface-masterTEST/deploy/face_model.py"", line 53, in __init__
    self.model = get_model(ctx, image_size, args.model, 'fc1')
  File ""/home/kudryavcev-ra/PycharmProjects/insightface-masterTEST/deploy/face_model.py"", line 39, in get_model
    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 1529, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 3, 112, 112)
[16:21:14] src/storage/storage.cc:116: Compile with USE_CUDA=1 to enable GPU usage
what i need to do if i want test on cpu and how i can fix this errors"
"Thanks for your awesome work!

In the PyPI package insightface, the model 'arcface_mfn_v1' is missing, where can I download it? Can I just replace it with the model 'MobileFaceNet, ArcFace@ms1m-refine-v1' in https://github.com/deepinsight/insightface/wiki/Model-Zoo?

Looking for your reply, thanks."
How i can get the feature vector from this model?
"I got confused on the learning rate scheduler in [recognition/train.py](https://github.com/deepinsight/insightface/blob/4a4b8d03fec981912fdef5b3232a37a827cbeed6/recognition/train.py#L291-L303)

Relevant code is:
```
    def _batch_callback(param):
      #global global_step
      global_step[0]+=1
      mbatch = global_step[0]
      for step in lr_steps:
        if mbatch==step:
          opt.lr *= 0.1
          print('lr change to', opt.lr)
          break

      _cb(param)
      if mbatch%1000==0:
        print('lr-batch-epoch:',opt.lr,param.nbatch,param.epoch)
```

So what's the difference between `mbatch`, `nbatch` and `global_step` here?

For me I trained with the default configurations
(where `lr_steps='100000,160000,220000`) with batch-size=110 (the maximal bs I can have, due to the GPUs Mem limitation) on a 4-GPUs machine.

From my log the learning rate changes at a very strange batch index:
```
lr change to 0.010000000000000002
lr-batch-epoch: 0.010000000000000002 7368 7
```

In my understanding the lr should decrease for the first time when `mbatch=100K`.
But in my practice it decreases at 

Can someone explain the mechanism here？"
"您好！
我的设备自动产出4k幅面的图片，但是将这种图片输入进insightface后总是提示显卡显存overload，我的机器用的2080（8g）的显卡，请问如何能让insightface在不更换显卡的前提下处理4k幅面的图片？

My device automatically produces a 4k format image, but after importing this image into the Insightface, it always prompts the graphics card memory to be overloaded. My machine uses a 2080 (8g) graphics card. How can I make the Insightface processing pictures in 4k format without replacing the graphics card ?"
"I have commented the gender & age section in test.py. Getting following error in get_feature. How do i resolve this error 

 File ""deploy/test.py"", line 21, in <module>
    f1 = model.get_feature(img)
  File ""/home//Documents//insightface/deploy/face_model.py"", line 90, in get_feature
    self.model.forward(db, is_train=False)
AttributeError: 'NoneType' object has no attribute 'forward'"
"INFO:root:Epoch[24] Batch [520-540]     Speed: 1727.27 samples/sec      acc=0.983984    lossvalue=2.195642
INFO:root:Epoch[24] Batch [540-560]     Speed: 1288.51 samples/sec      acc=0.983203    lossvalue=2.237722
INFO:root:Epoch[24] Batch [560-580]     Speed: 1296.01 samples/sec      acc=0.983594    lossvalue=2.236295
INFO:root:Epoch[24] Batch [580-600]     Speed: 1339.65 samples/sec      acc=0.982617    lossvalue=2.273337
INFO:root:Epoch[24] Batch [600-620]     Speed: 2007.56 samples/sec      acc=0.983398    lossvalue=2.299879
INFO:root:Epoch[24] Batch [620-640]     Speed: 1343.99 samples/sec      acc=0.983789    lossvalue=2.221828
INFO:root:Epoch[24] Batch [640-660]     Speed: 1290.73 samples/sec      acc=0.983984    lossvalue=2.208789
INFO:root:Epoch[24] Batch [660-680]     Speed: 1371.16 samples/sec      acc=0.984375    lossvalue=2.210849
INFO:root:Epoch[24] Batch [680-700]     Speed: 2012.76 samples/sec      acc=0.983984    lossvalue=2.210747
INFO:root:Epoch[24] Batch [700-720]     Speed: 1319.51 samples/sec      acc=0.984180    lossvalue=2.203272
INFO:root:Epoch[24] Batch [720-740]     Speed: 1287.94 samples/sec      acc=0.984180    lossvalue=2.205525
INFO:root:Epoch[24] Batch [740-760]     Speed: 1382.29 samples/sec      acc=0.983984    lossvalue=2.197704
INFO:root:Epoch[24] Batch [760-780]     Speed: 2022.44 samples/sec      acc=0.983594    lossvalue=2.228923
INFO:root:Epoch[24] Batch [780-800]     Speed: 1306.10 samples/sec      acc=0.983984    lossvalue=2.219678
INFO:root:Epoch[24] Batch [800-820]     Speed: 1290.28 samples/sec      acc=0.984180    lossvalue=2.214591
INFO:root:Epoch[24] Batch [820-840]     Speed: 1295.13 samples/sec      acc=0.442383    lossvalue=nan
INFO:root:Epoch[24] Batch [840-860]     Speed: 1689.81 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [860-880]     Speed: 1493.23 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [880-900]     Speed: 1303.18 samples/sec      acc=0.000000    lossvalue=nan

INFO:root:Epoch[24] Batch [1760-1780]   Speed: 1374.06 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1780-1800]   Speed: 1300.23 samples/sec      acc=0.000195    lossvalue=nan
INFO:root:Epoch[24] Batch [1800-1820]   Speed: 1303.87 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1820-1840]   Speed: 1606.54 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1840-1860]   Speed: 1593.05 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1860-1880]   Speed: 1305.64 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Batch [1880-1900]   Speed: 1301.60 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[24] Train-acc=0.000000
INFO:root:Epoch[24] Train-lossvalue=nan
INFO:root:Epoch[24] Time cost=363.141
call reset()
INFO:root:Epoch[25] Batch [0-20]        Speed: 1585.96 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[25] Batch [20-40]       Speed: 1304.34 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[25] Batch [40-60]       Speed: 1302.62 samples/sec      acc=0.000000    lossvalue=nan
INFO:root:Epoch[25] Batch [60-80]       Speed: 1786.20 samples/sec      acc=0.000195    lossvalue=nan
lr-batch-epoch: 1e-05 99 25
testing verification..
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 369, in train_net
    epoch_end_callback = epoch_cb )
  File ""/software/python-3.6/lib/python3.6/site-packages/mxnet/module/base_module.py"", line 553, in fit
    callback(batch_end_params)
  File ""train.py"", line 305, in _batch_callback
    acc_list = ver_test(mbatch)
  File ""train.py"", line 274, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, args.batch_size, 10, None, None)
  File ""eval/verification.py"", line 272, in test
    embeddings = sklearn.preprocessing.normalize(embeddings)
  File ""/software/python-3.6/lib/python3.6/site-packages/sklearn/preprocessing/data.py"", line 1614, in normalize
    estimator='the normalize function', dtype=FLOAT_DTYPES)
  File ""/software/python-3.6/lib/python3.6/site-packages/sklearn/utils/validation.py"", line 542, in check_array
    allow_nan=force_all_finite == 'allow-nan')
  File ""/software/python-3.6/lib/python3.6/site-packages/sklearn/utils/validation.py"", line 56, in _assert_all_finite
    raise ValueError(msg_err.format(type_err, X.dtype))
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
"
"开集识别, 客户来到前台能识别到，如果不在人脸库就不识别到; 看了 megaface 的识别，不知道怎么转成这种模式的应用"
"![RetinaFace_idea](https://user-images.githubusercontent.com/38787542/68566684-0262ff00-0492-11ea-9236-f0400eeeafb4.jpg)
这个项目中提供的人脸检测算法Retinaface是简化版吗？包含有5个关键点的extra-supervision部分，不包含self-supervision部分。在训练的代码代码中并没有找到关于3D人脸相关的，烦请各位知道的同学告知下。"
"Face bounding boxes are present but five facial landmarks are missing in annotations in download  [link](https://www.dropbox.com/s/7j70r3eeepe4r2g/retinaface_gt_v1.1.zip?dl=0)
Are you going to release ground truth for five facial landmarks for WIDER Face, which you have used in paper?"
"excuse me,i have a question about the meaning of landmark's label, what's the meaning of 1.0,0.0, and the score in the last? "
We have managed to get recognition/train.py to work and are getting some good training accuracies? How do we do the testing though? Is there any code provided that can let us visualize some test videos like the Friends youtube video provided?
"import logging
logger = logging.getLogger() # logging对象
#fh = logging.FileHandler(""test.log"") # 文件对象
sh = logging.StreamHandler() # 输出流对象
fm = logging.Formatter('%(asctime)s-%(message)s') # 格式化对象
#fh.setFormatter(fm) # 设置格式
sh.setFormatter(fm) # 设置格式
#logger.addHandler(fh) # logger添加文件输出流
logger.addHandler(sh) # logger添加标准输出流（std out）
logger.setLevel(logging.DEBUG) # 设置从那个等级开始提示

因为我用python3 没有INFO 信息，找了半天没找到原因，发现是 py3的logging 代码需要改进"
"I'm training on poor single rtx2070. Training speed is extremely slow. However, testing verification after several batchs costs a huge time (up to 80s , almost as much as training).  How to stop testing? I'm not sure how to modify config.py to make it"
"Hi, can the face recognition model network be used for vehicle feature extraction?"
""
这个训练过程是不是没有添加可视化呢    怎么样使用tensorboard可视化呢
"你好 ：
非常感谢能够开源这个项目， 论文中提到在1151个人的那张照片中检测到900个人， 但是提供的训练模型检测结果只有456个人，是我用的图片不一样吗？
![lumia_selfie_730_result](https://user-images.githubusercontent.com/38787542/67939971-a2f12d80-fc0d-11e9-9182-2c337c163606.jpg)
"
Is there any way to get percentage from the similarity value of face match in the test.py of deploy
"Hi,everyone.I'm having a problem with the replication of ""Training Data"" part 4 ""Fine-turn the above Softmax model with Triplet loss"". 
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network m1 --loss triplet --lr 0.005 --pretrained ./models/m1-softmax-emore,1
return is ""LocalFileSystem::Open ""./models/m1-softmax-emore,1-symbol.json"": No such file or directory"".
so I ivoked ""CUDA_VISIBLE_DEVICES='0,1' python -u train.py --network m1 --loss triplet --lr 0.005 --pretrained ./models/m1-softmax-emore/model""
return is:
 File ""/usr/lib/python2.7/site-packages/mxnet/module/module.py"", line 593, in forward
    assert self.binded and self.params_initialized
AssertionError
Question:
1. What is ""m1-softmax-emore,1""?
2. Where to call bind function?
Thanks."
""
"Hi all,

I'd like to use RetinaFace face for detection and arcface for recognition, since RetinaFace can bring a better recognition result. But the provided datasets are all aligned by MTCNN. Does anyone know where I can download the uncropped MS1MV2 dataset.

Many thanks."
"INFO:root:Epoch[0] Batch [0-20]	Speed: 0.96 samples/sec	acc=0.005952	lossvalue=6.244117
INFO:root:Epoch[0] Batch [20-40]	Speed: 0.99 samples/sec	acc=0.000000	lossvalue=6.229731
INFO:root:Epoch[0] Batch [40-60]	Speed: 0.95 samples/sec	acc=0.000000	lossvalue=6.254212
INFO:root:Epoch[0] Batch [60-80]	Speed: 0.94 samples/sec	acc=0.006250	lossvalue=6.224050
INFO:root:Epoch[0] Batch [80-100]	Speed: 0.95 samples/sec	acc=0.006250	lossvalue=6.221473
INFO:root:Epoch[0] Batch [100-120]	Speed: 0.96 samples/sec	acc=0.006250	lossvalue=6.220169
INFO:root:Epoch[0] Batch [120-140]	Speed: 0.94 samples/sec	acc=0.006250	lossvalue=6.214798
INFO:root:Epoch[0] Batch [140-160]	Speed: 0.95 samples/sec	acc=0.000000	lossvalue=6.251216
INFO:root:Epoch[0] Batch [160-180]	Speed: 0.96 samples/sec	acc=0.000000	lossvalue=6.192453
INFO:root:Epoch[0] Batch [180-200]	Speed: 0.98 samples/sec	acc=0.000000	lossvalue=6.244839
INFO:root:Epoch[0] Batch [200-220]	Speed: 1.00 samples/sec	acc=0.000000	lossvalue=6.203299
INFO:root:Epoch[0] Batch [220-240]	Speed: 1.00 samples/sec	acc=0.012500	lossvalue=6.215086
INFO:root:Epoch[0] Batch [240-260]	Speed: 1.00 samples/sec	acc=0.000000	lossvalue=6.204084
INFO:root:Epoch[0] Batch [260-280]	Speed: 1.00 samples/sec	acc=0.000000	lossvalue=6.177462
INFO:root:Epoch[0] Batch [280-300]	Speed: 1.00 samples/sec	acc=0.012500	lossvalue=6.184724
INFO:root:Epoch[0] Batch [300-320]	Speed: 1.00 samples/sec	acc=0.000000	lossvalue=6.201993
INFO:root:Epoch[0] Batch [320-340]	Speed: 1.00 samples/sec	acc=0.006250	lossvalue=6.204159
INFO:root:Epoch[0] Batch [340-360]	Speed: 1.00 samples/sec	acc=0.000000	lossvalue=6.172345
INFO:root:Epoch[0] Batch [360-380]	Speed: 1.00 samples/sec	acc=0.000000	lossvalue=6.195843
INFO:root:Epoch[0] Batch [380-400]	Speed: 1.00 samples/sec	acc=0.006250	lossvalue=6.186938
INFO:root:Epoch[0] Batch [400-420]	Speed: 1.00 samples/sec	acc=0.006250	lossvalue=6.145568
INFO:root:Epoch[0] Train-acc=0.007812
INFO:root:Epoch[0] Train-lossvalue=6.164720
INFO:root:Epoch[0] Time cost=3569.542
"
"CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore
gpu num: 4
prefix ./models/r100-arcface-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=256, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=64, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_act': 'prelu', 'emb_size': 512, 'data_rand_mirror': True, 'num_layers': 100, 'loss_name': 'margin_softmax', 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'net_input': 1, 'image_shape': [112, 112, 3], 'net_blocks': [1, 4, 6, 2], 'fc7_lr_mult': 1.0, 'ckpt_embedding': True, 'net_unit': 3, 'net_output': 'E', 'count_flops': True, 'num_workers': 1, 'batch_size': 256, 'memonger': False, 'data_images_filter': 0, 'dataset': 'emore', 'num_classes': 85742, 'fc7_no_bias': False, 'loss': 'arcface', 'data_color': 0, 'loss_s': 64.0, 'dataset_path': '../datasets/faces_emore', 'data_cutoff': False, 'net_se': 0, 'net_multiplier': 1.0, 'fc7_wd_mult': 1.0, 'network': 'r100', 'per_batch_size': 64, 'net_name': 'fresnet', 'workspace': 256, 'max_steps': 0, 'bn_mom': 0.9}
0 1 E 3 prelu False
Network FLOPs: 24.2G
INFO:root:loading recordio ../datasets/faces_emore/train.rec...
header0 label [5822654. 5908396.]
id2range 85742
5822653
rand_mirror True
../datasets/faces_emore/lfw.bin
Image isze
[112, 112]
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
../datasets/faces_emore/cfp_fp.bin
Image isze
[112, 112]
Traceback (most recent call last):
  File ""train.py"", line 389, in <module>
    main()
  File ""train.py"", line 385, in main
    train_net(args)
  File ""train.py"", line 276, in train_net
    data_set = verification.load_bin(path, image_size)
  File ""eval/verification.py"", line 197, in load_bin
    data = nd.empty((len(issame_list) * 2, 3, image_size[0], image_size[1]))
  File ""/home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/ndarray/utils.py"", line 103, in empty
    return _empty_ndarray(shape, ctx, dtype)
  File ""/home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 3955, in empty
    return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
  File ""/home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 141, in _new_alloc_handle
    ctypes.byref(hdl)))
  File ""/home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [15:17:21] src/storage/./cpu_device_storage.h:75: Failed to allocate CPU Memory
Stack trace:
  [bt] (0) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x4b03ab) [0x7f4ad02313ab]
  [bt] (1) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2e618c1) [0x7f4ad2be28c1]
  [bt] (2) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2e6a96f) [0x7f4ad2beb96f]
  [bt] (3) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/libmxnet.so(mxnet::NDArray::NDArray(mxnet::TShape const&, mxnet::Context, bool, int)+0x5d0) [0x7f4ad22b25b0]
  [bt] (4) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/site-packages/mxnet/libmxnet.so(MXNDArrayCreateEx+0x22d) [0x7f4ad22b307d]
  [bt] (5) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/lib-dynload/_ctypes.so(ffi_call_unix64+0x4c) [0x7f4b04f1857c]
  [bt] (6) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/lib-dynload/_ctypes.so(ffi_call+0x1f5) [0x7f4b04f17cd5]
  [bt] (7) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/lib-dynload/_ctypes.so(_ctypes_callproc+0x3e6) [0x7f4b04f0f376]
  [bt] (8) /home/testuser/anaconda2/envs/pytorch/lib/python2.7/lib-dynload/_ctypes.so(+0x9db3) [0x7f4b04f06db3]

"
"Hi,
Can you provide your on MS1M-IBUG(MS1M-V1) trained models for Sphereface, Cosface, Normface and the combined margin Models, that lead to this table: https://github.com/deepinsight/insightface#verification-results-on-combined-margin.

Thank you in advance


"
如题，有同样的问题吗？加载提特征需要5分钟以上，并且内存占用由小变大再突然变小n次
请问哪位大神在windows中成功编译过insightface的提特征模型么？
hi  friends，Could you help me know that how to train age-gender  network？and where is the datasets？or how to make the datasets？
""
"Hi,  in your paper [Retina Face](https://arxiv.org/pdf/1905.00641.pdf), you said

> ""We take advantage of TVM [7] to accelerate the model inference and timing is performed on the NVIDIA Tesla P40 GPU""

How about the benchmark time for RetinaFace base on TVM? Any deployment suggestions or demo are grateful."
"Hi, is there any range of this 512D feature? I want to generate a random 512D feature?  Thank you!!"
""
"why set export MXNET_CPU_WORKER_NTHREADS=24
the current cpu load is up to 100%
![2](https://user-images.githubusercontent.com/44575765/66811004-e50a4600-ef62-11e9-98b1-9e9dfaf6b335.PNG)

when export MXNET_CPU_WORKER_NTHREADS=1
the process is running"
"I'm converting retina face to onnx. I get output like the image. How I can get bounding box and landmark?
![Screenshot from 2019-10-15 13-07-16](https://user-images.githubusercontent.com/37315746/66804520-d9ac2000-ef4c-11e9-9759-0e5f5ff5dce0.png)
"
"I use the same data train with insightface and facenet ,facenet turns out to be better ,so "
"Hello everybody,im using arcface and mtcnn for real time face recognition and opencv for grab rtsp stream from my cameras,four ip cameras  is processing for face recognition and i want to use more cameras for this project.
When i use four cameras my cpu usage is 90% and i couldnt to add more cameras.
How i can to use 20cameras for this project?

My system info is :gpu 1080ti,cpu core i7 9700k,32gb ram.
Isn't my hardware enough for this purpose?
How do I choose the right hardware?
Is retinaface a better target for my project?
How to run the whole process on gpu?
Please help me"
"Hi all,

I'm using tools from this repo [https://github.com/GarrickLin/MXNet2Caffe](url) to convert resnet50 insightface model to caffe.

I've tested the pretrained model, the conversion is success and the result is exactly the same.

However, when I tried it out on my trained model, the inference output is completely different. The command used to train my model is `python -u train.py --network r50 --loss arcface --dataset emore`.

Compared with the original setting, I had two modifications. First, I changed the value of config.ckpt_embedding to False in config.py, in order to save the last fc layer for future finetune. And I pruned fc7 layer afterward and the model's performance is as expected. The second difference is that I changed the dataset.emore.image_shape in config file to (64,64,3) to make the model accept 64x64 face images. And after the execution of json2prototxt.py, I manually changed the input shape to 1, 3, 64, 64. After generate the caffemodel, I can do inference, but the output is not the same as mxnet.

I suspect that changing the input size may lead to the conversion failure. But I have no idea how to solve this problem. Any idea how to make it work for 64x64 input image? @nttstar @GarrickLin

Many thanks."
"the version of mxnet is mxnet-cu100,  the version of cuda is cuda-10, When I run the scripts 
 of train.py ,the process of trainning will kill itself,why? Is it possible for the bug of mxnet-cu100?"
"Hi, I apply insightface for face recognition with my own dataset by simply add 2 fc layers and output the index of the class.
Firstly I trained it on mxnet, fixed the feature network(pretrained model: r100), only finetune that 2 additional fc layers. Then I re-implemented it with tensorflow, the parameters and optimizer and initialization is the same. However, I cannot reproduce the same result as on mxnet.
Any suggestion, please? Is there any detail could I ignored?
Thank you."
""
Which datasets did you use for training age and gender?? @yingfeng @nttstar @ppwwyyxx @kernel8liang 
"As the title shows, I've trained a model on WiderFace, but I want to continue fine-tune the trained model on another new dataset. I pass in parameters in the following way:

``` text
python -u train.py --prefix ./model/retina-p1 --pretrained model/retina-50 --pretrained_epoch 60
```

The models I trained are `retina-50-symbol.json` and `retina-50-60.params`. But the runtime reported the following error:

``` text
...
INFO:root:loading model/retina-50,60
Traceback (most recent call last):
  File ""train.py"", line 333, in <module>
    main()
  File ""train.py"", line 330, in main
    lr=args.lr, lr_step=args.lr_step)
  File ""train.py"", line 93, in train_net
    sym = eval('get_' + args.network + '_train')(sym)
  File ""/home/temp/insightface/RetinaFace/rcnn/symbol/symbol_resnet.py"", line 409, in get_resnet_train
    return get_sym_train(sym)
  File ""/home/temp/insightface/RetinaFace/rcnn/symbol/symbol_common.py"", line 448, in get_sym_train
    conv_fpn_feat = get_sym_conv(data, sym)
  File ""/home/temp/insightface/RetinaFace/rcnn/symbol/symbol_common.py"", line 206, in get_sym_conv
    stride2layer[stride] = all_layers[name]
  File ""/home/temp/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 511, in __getitem__
    raise ValueError('There are multiple outputs with name \""%s\""' % index)
ValueError: There are multiple outputs with name ""_plus0_output""
```

**Does the current code support fine-tuning?**"
"I am trying to extract the embeddings for about 2,000 images and I am using the LResNet100E-IR,ArcFace@ms1m-refine-v2 pretrained model. 

I aligned the images using MTCNN from FaceNet to 112 x 112 px and then used the code from deploy/test.py

I was able to get the embeddings for all the images except for the one below:

![Roger_Maltbie_golf](https://user-images.githubusercontent.com/12972261/66206404-4e12d380-e6b0-11e9-8c98-333d520f53e9.png)

I got the error
`free(): invalid pointer
`
What is the cause of the error and how could I solve it?

Thank you!"
"Could anyone help to understand why a block gradient is set just after building the whole net architecture? As far as I am concerned, this step deactivates any effect of backward propagation. 

Thanks in advance. 

"
"What is the purpose of implementing a BatchNorm layer at the beginning of each block? 

In particular, I'm analysing residual_unit_v3 basic blocks (bottle_neck=False). 

As far as we are concerned, previous block already returns normalized layers. 

Thanks in advance 

 "
"Good morning. We are working on your arcface.
This time we are going to megaface. We downloaded the megaface from washington university. But the dataset at washington is not refined. The arcface paper seems to have refined the megaface. Can I share the refined megaface?"
"Hi. I've downloaded both AgeDB-30 by yours and the original AgeDB by iBUG. How ever the AgeDB-30 is aligned already and I wanna do align by myself. I've looked into AgeDB-30 and the original AgeDB but I couldn't find out the relationship between this two.

So my question is that what's the relationship between the two db? Is there any private dataset in the  AgeDB-30 ?"
"
testing verification..
Traceback (most recent call last):
  File ""train_softmax.py"", line 598, in <module>
    main()
  File ""train_softmax.py"", line 595, in main
    train_net(args)
  File ""train_softmax.py"", line 589, in train_net
    epoch_end_callback = epoch_cb )
  File ""D:\python\lib\site-packages\mxnet\module\base_module.py"", line 553, in fit
    callback(batch_end_params)
  File ""train_softmax.py"", line 527, in _batch_callback
    acc_list = ver_test(mbatch)
  File ""train_softmax.py"", line 488, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, args.batch_size, 10, None, None)
  File ""eval\verification.py"", line 247, in test
    _embeddings = net_out[0].asnumpy()
  File ""D:\python\lib\site-packages\mxnet\ndarray\ndarray.py"", line 1996, in asnumpy
    ctypes.c_size_t(data.size)))
  File ""D:\python\lib\site-packages\mxnet\base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [16:49:45] c:\jenkins\workspace\mxnet-tag\mxnet\include\mxnet\./tensor_blob.h:290: Check failed: this->shape_.Size() == static_cast<size_t>(shape.Size()) (3010560000 vs. 18446744072425144320) : TBlob.get_with_shape: new and old shape do not match total elements
，就是会报这个错误，您能给解释一下么   "
"do i need to divide the whole process into some stages to training arcface?(eg. first softmax ,second triplet....,final arcface,or reverse)
or just run :
`CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore
`
is enough?"
我在运行rcnn/cython中的setup.py文件时，出现No module named Cython.Distutils的错误，请问有什么办法可以解决。
"Hi:
   I use below cpp code to calculate cosine similarity between 2 face features:

float FaceFeaturesCompare_CosSimilarity(const float *f1,const float *f2,int nb)
{
	float  product = 0;
	float l2_sum0 = 0;
	float  l2_sum1 = 0;
	for (size_t i = 0; i < nb; i++)
	{
		product += f1[i] * f2[i];
		l2_sum0 += f1[i] * f1[i];
		l2_sum1 += f2[i] * f2[i];
	}
	return abs(product / (std::sqrt(l2_sum0) * std::sqrt(l2_sum1)));
}

because I use abs() function,so the return similar is in [0,1], but I found many people use below python code to calculate:
sim = np.dot(t1,t2.T)
the returned similar is in [-1,1], so my question is whether cosine similarity has negative value? if 2 face features similar is -0.9, is it a same person? or not?"
""
ANy MLP info described in paper and for friends video demo?
"The given link https://pan.baidu.com/s/1P1ypO7VYUbNAezdvLm2m9w does not work, it apparently requires a code."
"Hello,
Do you plan releasing annotations for landmarks/blurriness also for the validation data?"
"So in recognition/train.py, the fit method is called with no optimizer_params dict which raise this exception because default value in fit method is not a dict.

is it mxnet versioning? ( I am at 1.5.0) or do I have to specifically config optimizer_parmas?

Thanks! "
"How to perform batch processing while inference ? 
I'm Using Retina Face for Detection and FaceModel for Recognition, Same as test.py script in deploy/ .
Instead of Single Image I want to use array of images and get output array of results from both the models."
实际使用在老人和小孩上表现比较差，不同的老人和老人，小孩和小孩相似的都很高，有什么办法吗？  有覆盖老人和小孩的数据集重新训练一下吗？
"I use cosine similarity to calculate 2 features similarity,but I am not sure whether it is the best way, so whether cosine similarity is suite for arcface's feature comparing? Is there any other ways to do the feature comparing?"
"I am trying to finetune a pretrained model, I can run the command that is mentioned in the readme without any problem. However, it starts with a bad accuracy in train (I can understand that is because the fc7 layer was removed) but it also have a poor accuracy (60%) in verification with the lfw dataset, which should be quite better, because it is using the embedding layer and not the fc7.

Thanks"
""
"https://books.google.co.in/books?id=QeJEDwAAQBAJ&pg=PA503&lpg=PA503&dq=""G-Log+algorithm""&source=bl&ots=ceQWD29NdA&sig=ACfU3U3b0dLmBZDGbnz85AEzvVDp9a8brQ&hl=en&sa=X&ved=2ahUKEwiW86PBkLzkAhWMK48KHawiBpYQ6AEwAHoECAYQAQ#v=onepage&q=""G-Log%20algorithm""&f=false

An implementation of G-Log image enhancement Algorithm will definitely boost accuracy. 
I suggest we will implement this algorithm in this repository. "
"index 1824 is out of bounds for axis 0 with size 1824
请问这个问题如何解决？"
"INFO:root:loading recordio /data/puke/face/faces_glint/train.rec...
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 242, in train_net
    images_filter        = config.data_images_filter,
  File ""/home/puke/insightface/recognition/image_iter.py"", line 39, in __init__
    s = self.imgrec.read_idx(0)
  File ""/home/puke/envs/face/local/lib/python2.7/site-packages/mxnet/recordio.py"", line 318, in read_idx
    return self.read()
  File ""/home/puke/envs/face/local/lib/python2.7/site-packages/mxnet/recordio.py"", line 209, in read
    ctypes.byref(size)))
  File ""/home/puke/envs/face/local/lib/python2.7/site-packages/mxnet/base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [09:44:10] src/recordio.cc:65: Check failed: header[0] == RecordIOWriter::kMagic: Invalid RecordIO File

@nttstar 能帮我解答一下吗"
"are both online. 
Any suggestion is welcome!
For pip source code, please refer to [https://github.com/deepinsight/insightface/tree/master/python-package](https://github.com/deepinsight/insightface/tree/master/python-package)


### 2021.05.15 Update:

pip insightface-0.2.0 is ready now. Please update with `pip install -U insightface`

For insightface pip-package <= 0.1.5, we use MXNet as inference backend, please download all models from [onedrive](https://1drv.ms/u/s!AswpsDO2toNKrUy0VktHTWgIQ0bn?e=UEF7C4), and put them all under `~/.insightface/models/` directory.

Starting from insightface>=0.2, we use onnxruntime as inference backend, please download our **antelope** model release from [onedrive](https://1drv.ms/u/s!AswpsDO2toNKrU0ydGgDkrHPdJ3m?e=iVgZox), and put it under `~/.insightface/models/`, so there're onnx models at `~/.insightface/models/antelope/*.onnx`.

The **antelope** model release contains `ResNet100@Glint360K recognition model` and `SCRFD-10GF detection model`. Please check `deploy/test.py` for detail."
"HI, I have some question for annotations，show
below：
![{1B376AC9-10D7-4A86-9F86-45F5CAFAE851}_20190904143617](https://user-images.githubusercontent.com/43072388/64231175-72e52100-cf21-11e9-9720-7a31709ff04c.jpg)




thanks for your help

"
"# 1--Handshaking/1_Handshaking_Handshaking_1_292.jpg
612 111 96 125 635.929 163.571 1.0 686.214 158.857 1.0 663.429 189.5 0.0 646.929 203.643 0.0 681.5 199.714 0.0 0.77
709 105 60 112 749.0 153.0 0.0 763.0 148.0 1.0 765.0 173.0 0.0 745.0 196.0 0.0 757.0 193.0 0.0 0.7
313 193 71 89 353.062 211.125 0.0 376.688 207.75 0.0 379.5 234.188 0.0 354.75 255.562 0.0 375.562 252.188 0.0 0.66
177 218 64 109 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 0.68

in the retinaface paper, the author describe ""In total, we have annotated 84.5k faces on the training set and 18.5k faces on the validation set"", and I download the label from [https://pan.baidu.com/s/1Laby0EctfuJGgGMgRRgykA](url), the layout seems as ""x1 y1 w h p1_x p1_y p1_conf p2_x p2_y p2_conf..."". But there are three confidence? What's the meaning of ""-1.0 0.0 1.0""?"
"Hi,

Thanks for this great work.

I am not able to understand some of the code (and mostly because of no background in mxnet; I do know tensorflow)

```python
        zy = mx.sym.pick(fc7, gt_label, axis=1)
        cos_t = zy/s
        t = mx.sym.arccos(cos_t)
        if config.loss_m1!=1.0:
          t = t*config.loss_m1
        if config.loss_m2>0.0:
          t = t+config.loss_m2
        body = mx.sym.cos(t)
        if config.loss_m3>0.0:
          body = body - config.loss_m3
        new_zy = body*s
        diff = new_zy - zy
        diff = mx.sym.expand_dims(diff, 1)
        gt_one_hot = mx.sym.one_hot(gt_label, depth = config.num_classes, on_value = 1.0, off_value = 0.0)
        body = mx.sym.broadcast_mul(gt_one_hot, diff)
        fc7 = fc7+body
```

I believe fc7 is the output layer of your network. 

In the very last line in the above code snippet you do `fc7 = fc7 + body`.

Does this mean that you are updating the weights of fc7 ?

It seems that fc7 is a trainable layer i.e. its weights would be updated by the backprop. Why do you add 'body' to fc7 ?

Would appreciate if you can help it understand. 

Regards
Kapil




"
"im trying to train insightFace on CASIA - Webface, and i tryed to train arcface from scratch, but it seems difficult, although the accuracy could be high, but the validation rate on lfw is very low, likely acc 70% but validation rate just 5%.
could anyone give me some advice on how to train arcface? 
BTW, when i train triplet loss, i also encoutner the same problem, accuracy could be high , but validation rate is very low, Am i wrong somewhere? hope someone could help me, thanks in advance."
"Hello,

  I was just wondering which one is easiest to use and possibly train among the third party examples listed.  Also if there are any differences among them what are they.  Thanks.

Nobu"
"A fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging."
I wanted to know how can I re-train the r34 architecture.
"Using Python3.6, arcface loss, r34 architecture.


INFO:root:Epoch[0] Batch [41820-41840]  Speed: 204.01 samples/sec       acc=0.991797    lossvalue=4.493365
INFO:root:Epoch[0] Batch [41840-41860]  Speed: 204.65 samples/sec       acc=0.991797    lossvalue=4.486666
INFO:root:Epoch[0] Batch [41860-41880]  Speed: 203.98 samples/sec       acc=0.992188    lossvalue=4.490592
INFO:root:Epoch[0] Batch [41880-41900]  Speed: 204.16 samples/sec       acc=0.992188    lossvalue=4.476994
INFO:root:Epoch[0] Batch [41900-41920]  Speed: 204.53 samples/sec       acc=0.991797    lossvalue=4.494334
INFO:root:Epoch[0] Batch [41920-41940]  Speed: 207.40 samples/sec       acc=0.991797    lossvalue=4.485728
INFO:root:Epoch[0] Batch [41940-41960]  Speed: 205.71 samples/sec       acc=0.942578    lossvalue=nan
INFO:root:Epoch[0] Batch [41960-41980]  Speed: 206.47 samples/sec       acc=0.000000    lossvalue=nan
lr-batch-epoch: 0.001 41999 0
testing verification..
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 369, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python3.6/dist-packages/mxnet/module/base_module.py"", line 560, in fit
    callback(batch_end_params)
  File ""train.py"", line 305, in _batch_callback
    acc_list = ver_test(mbatch)
  File ""train.py"", line 274, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, args.batch_size, 10, None, None)
  File ""eval/verification.py"", line 272, in test
    embeddings = sklearn.preprocessing.normalize(embeddings)
  File ""/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py"", line 1614, in normalize
    estimator='the normalize function', dtype=FLOAT_DTYPES)
  File ""/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py"", line 542, in check_array
    allow_nan=force_all_finite == 'allow-nan')
  File ""/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py"", line 56, in _assert_all_finite
    raise ValueError(msg_err.format(type_err, X.dtype))
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
"
Center parallelism has been implemented in train_parallel.py in MxNet. I wonder if anyone deploy it in TensorFlow?
"I wanted to know how can I join two datasets given in the data zoo.
I also wanted to know if there is anything else that needs to be done if my dataset is containing only two images per person and the files are being created via custom for-loops in dir_to_rec.py file.
I have labeled the data as 0,0,1,1,2,2,3,3, ... 
While training the accuracy and loss values are shown to be zero forever. r100, arc face loss

Attached is my custom dir2rec and build_

[build_eval.txt](https://github.com/deepinsight/insightface/files/3540293/build_eval.txt)
[dir2rec.txt](https://github.com/deepinsight/insightface/files/3540294/dir2rec.txt)

eval code, images are pre-cropped via MTCNN:
"
"Hi,
A quick question.
I realize that the standard input to the Retinaface model is (640,640). For the sake of speed, do we need to resize the input image to (640,640) before feed it into the model or just leave as what it is?
Thanks!"
"In the RetinaFace config.py file,the default setting of  mnet train batch size is 16 and resnet train batch size is 8,why use so small batch size？ Can I use batch size of 128 or even bigger to get better training result when the memory of GPU is enough?"
"For image normalization shouldn't the formula be (pixel_val-127.5/127.5) instead of (pixel_val-127.5/128) ?

or is the standard deviation calculated = 128?

What does 128 represent? I understand that 127.5 represents the mean value of pixel if the pixel values range from [0,255]."
""
"raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 3, 640, 640)
[16:14:50] src/storage/storage.cc:119: Compile with USE_CUDA=1 to enable GPU usage
Stack trace:"
"如题。
请问是因为增广后会导致数据量膨胀，训练速度变慢吗？还是其它什么原因？"
"I do not believe that this question has been asked, but apologies if I missed and issue thread.
I have finished training the dataset and am able to run the evaluation script in /insightface/Evaluation/Megaface/run.sh . I want to slightly change the evaluation dataset into the same photos, but downsampled by some degree. So I either want to change the file path of where this evaluation script pulls from, or access the folder the evaluation script is pulling from. I have read run_experiments.py and have tinkered around, but am unable to find both. It is not behaving the way I expect, and I would greatly appreciate any help with this. Thanks."
"But default rpn_anchor_cfg for resnet-152 have 5 strides
and the anchor_scale between the two backbone is different
Is this because 3 strides fit the mnet or just smaller FLOPs?
是因为三种strides更适合较小的backbone，还是单纯因为三种strides有更小的FLOPs？"
"hey, when I trained LResnet100-IR on glintasia dataset from scratch, the cpu inference time is aroud 8 seconds, but when i fintune same model on glintasia dataset from pretrained  LResnet100-IR（download from BaiduYun）, the cpu inference time is arourd 300 ms， I am wondering why? Any help will be appreciate!
(当在glintasia数据集上从头训练LResnet100E-IR 和 利用百度云上的LResnet100E-IR 在数据集上finetune得到的模型，他们在cpu上的推理速度相差非常大，分别是8秒中和300毫秒，非常奇怪, 困扰了我很久了，有人知道原因吗? 非常感谢！）"
"Hi, I'm inspired when I read your code, but I can't get the MS1MV2 and DeepGlint-Face , Could you sent the download link to me, my email is 1607625471@qq.com. Thanx !"
"@nttstar thanks for your great work.
I tried to feed batch to
https://github.com/deepinsight/insightface/blob/5aa877e1ac861bbe6fb1e1f2002bd9d1351bca5f/deploy/face_model.py#L90
but in the speed of model compared to a single image not change.
why this is happening?"
"Hi,
I followed the script for megaface evaluation and removed images using yours noise removal files. I found not all images mentioned in noise removal.txt were present in feature list.
When I run the code run_experiment.py for evaluation, its fails as many feature files are missing.
Can you please help me in this?

"
"That is not an issue, but a question. Have someone converted RetinaFace to Keras or Tensorflow model?"
能否提供下MS1MV2+DeepGlintAsian的训练集？
MS1M-IBUG和MS1M-ArcFace有什么区别？
Has anyone made it?
"Hi,
I tried to convert retina-R50 model to onnx following http://mxnet.incubator.apache.org/versions/master/tutorials/onnx/export_mxnet_to_onnx.html instruction. But I got an error. I also tried insightface model and everything just worked. Do you have any idea how that happens?
Any help will be appreciate!"
"Hi @nttstar  ,thanks for the great work and for the effort of sharing it here.

1. I use stride[4, 8, 16, 32, 64] with mobilenet0.25 backbone on retinaface,and get 0.928, 0.918, 0.833 on widerface easy, medium, hard set respectively, which is a better result than just using stride [8, 16, 32]. But I can detect 150 less faces than just using stride [8, 16, 32].
  I can't figure out why.

2. Why I get worse result with using dense anchor on 'World Largest Selfie'?
  I just add dense anchor, but detect 100 less faces than not using dense anchor, and the minimum face I can detect became bigger

Looking forward to your reply
Thanks in advance"
"How to get AgeDB-30 verification pair?
I read the paper of AgeDB, but I did not find a description of the verification pair in this dataset. Does this verification pair have an official division, or is it randomly divided according to the rules in the paper?"
""
"I tested with my own data set, with 30,000 pairs of positive and negative samples respectively. This problem occurred when running train.py. Could you help me？
![image](https://user-images.githubusercontent.com/41284417/62986371-d70a4d00-be6d-11e9-8df7-95c8d3134ca2.png)
"
Can you make the landmark annotations on the validation set of WIDER FACE public?
""
"These are the few question I get from peoples but no one know the answer. If you know any one of it please reply 

1. How china is doing accurate face recognition?
2. How are they training there model?
3. Whats the algorithm they are using?
4. The main question ""How are they saving the face features (encoding),(points) and doing it so quickly?"" 

What is the best way to save 1m face encoding I have tried knn pickel file, pytorch .pth file but this all required training for 1m faces it require more than 4 hours if I want to add new Images to training data I need to train the model again so need to wait for 4 hours 
Many People said to try mysql, Apache solr, so we can avoid training the model.
so whats the best way to do save face features and run recognition.

This is not an issues just few question feel free to close it."
"Hello, as the WiderFace Challenge (ICCV 2019) has now closed, is it possible to release your best pre-trained RetinaFace model? Thank you!"
"@nttstar 
* Are the all datasets availabe from this repo is aligned by mtcnn or retinaface?
* if the datasets are aligned with mtcnn, retinaface should be used only for inference?
* During the inference, should we use mtcnn or retinaface?"
"The given link https://pan.baidu.com/s/1P1ypO7VYUbNAezdvLm2m9w does not work, it apparently requires a code."
"                     Methods                             | 1e-06  | 1e-05  | 0.0001 | 0.001  |  0.01  |  0.1   |
+----------------------------------------------------------------+--------+--------+--------+--------+--------+--------+
|          glint-Mobilefacenet     | 0.5317 | 0.7009 | 0.8463 | 0.9272 | 0.9667 | 0.9867 |
|            glint-ResNet50           | 0.8722 | 0.9262 | 0.9501 | 0.9673 | 0.9803 | 0.9892 |
**|          glint_vggfine_res50     | 0.0021 | 0.0986 | 0.6721 | 0.9003 | 0.9731 | 0.9898 |**

请问你碰到过这个现象吗
ijbc 测试中代码我没有更改，只是换了模型读取，far 1e-06 1e-05 出现断崖式减少，请问是可能是什么原因吗，我总觉得有错误，但是我只是改动了模型的读取路径
@jiankangdeng 
@nttstar 
"
"When I run recongnition /train.py,this question occured,I don't know the reason,Can you help me?
 The parameters I set are as follows:
CUDA_VISIBLE_DEVICES='0' python -u train.py --network y1   --lr 0.005 --per-batch-size 80  --ckpt 2  --pretrained /home/mi5/insightface/models/model-y1/models  --models-root ../models/model-y1-7
![image](https://user-images.githubusercontent.com/41284417/62750301-3fce7f80-ba92-11e9-9611-3bb4a5ca0979.png)

"
"Thanks for sharing your excellent work.
How to calculate the failure rate of results of face alignment on the WIDER FACE validation dataset?
Any advice will be appreciated, thanks"
caffe
"麻烦咨询一下关于RetinaFace检测速度问题，使用项目中提供的RetinaFace代码，模型使用的是项目提供的基于ResNet50训练模型，使用的gpu是Tesla P100，同一张图片第一次预测时间是
![image](https://user-images.githubusercontent.com/16226299/62589787-ee40bc00-b8fc-11e9-8f5d-a80bf39c9667.png)第二次预测时间是
![image](https://user-images.githubusercontent.com/16226299/62591419-17b01680-b902-11e9-8d62-97776a0768fa.png)
不同的照片第一次预测都是一样都时间
"
"Hi @nttstar, thanks for the great work and for the effort of sharing it here. I have three questions.

1. I can't reproduce the ~900 faces identified on the largest selfie image on the paper (RetinaFace: Single-stage Dense Face Localisation in the Wild). Using the threshold of `0.5` as indicated in the paper's figure label gave me 388 faces as shown below:

From the paper 900 faces
![900](https://user-images.githubusercontent.com/959623/62493283-fe5b7d00-b7d0-11e9-8532-d6288cc0482c.png)

From the code on this repository using threshold of `0.5` and `net3` ""only"" 388 faces
![388](https://user-images.githubusercontent.com/959623/62493193-cf450b80-b7d0-11e9-8f04-b8aa78568474.jpg)

2. What is the trade-off of the `treshold` itself, how (de) increasing it will change the capacity of the detection?

3. On the paper you have compared a 68-landmark model. Are you going to publish pre-trained model with 68 landmarks?

Thanks in advance."
"I want to finetune this model, because 'label' contains 'gender and age'. I wonder what the 'label' format is in the .lst file."
"![image](https://user-images.githubusercontent.com/24749356/62337712-6461af00-b508-11e9-9a94-8b79644dd7d7.png)
"
"_weight = mx.symbol.Variable(""fc7_weight"", shape=(config.num_classes, config.emb_size), 
        lr_mult=config.fc7_lr_mult, wd_mult=config.fc7_wd_mult, init=mx.init.Normal(0.01))
i set config.fc7_lr_mult=0, config.fc7_wd_mult=0,why _weight update when i train the network"
Thanks very much!
"Hi,
Thanks for your great contribution. I am a researcher that want to reproduce your results, but I didn't find in the code of RetinaFace project the Dense Regression Loss from the differentiable 3D mesh renderer. It seems its function is imported inside symbol_common: 
from rcnn.PY_OP import rpn_3d_mesh
but this file (rpn_3d_mesh) is missing in this repository. Is it possible to share this file publicly?
Also, is the pretrained model of Retinaface trained with this dense regression loss from the 3D mesh renderer?"
what tools do you use to label the landmark point 
"Hello!

I've tried several images to go through RetinaFace/test.py. I found that for a group photo, it's easy for RetinaFace to detect all faces:

![detector_t1](https://user-images.githubusercontent.com/38978109/62110752-b5dc2500-b2e1-11e9-9ad6-8444b76e2812.jpg)

But when it comes to a single big face in one image as below (which in fact is a crop from a big photo), RetinaFace either fails to detect any faces, or returns a wrong bounding box or landmark.

![detector_t3](https://user-images.githubusercontent.com/38978109/62110753-b5dc2500-b2e1-11e9-8dab-a46c37af999e.jpg)
![detector_t4](https://user-images.githubusercontent.com/38978109/62110754-b674bb80-b2e1-11e9-969d-7a336c22460c.jpg)
![detector_t5](https://user-images.githubusercontent.com/38978109/62110755-b674bb80-b2e1-11e9-9f0e-f22fd444c509.jpg)

I think that this is because some parameters (scale, network, decay4 or something) should be adapt for detecting a big face. In MTCNN, I can feed the whole image into R-Net directly to solve this problem, but I have no idea what to do with RetinaFace. Can anybody help me on this question?

Thank you!"
"我用faces_glintasia,arcface训练了一个模型，同一个id的相似度在0.7左右，请问有什么办法可以提高这个相似度吗"
"hi, how to label image's blurness  on widerface dataset."
"Hi. I'm a student who studies your work.

Thanks for sharing such a nice code.

I've just started to study CV and not skilled..

I want to execute your code like your face recognition picture

but I don't have idea how to execute

after finish training then which code I have to execute?

It would be very thankful if you can answer.."
如题，Additive Angular Margin Loss  中文翻译成什么好呢
"TVM project proudly dropped support of Python2.
The minimal Python requirement is Python 3.5?
how to solve this problem"
Has anyone tried insightface inference to run on android?
"Hi,

I found that the whole process of  retinaface inference is quite slow, because the mxnet.ndarray operations execute in asynchronous mode. So that for the first time you run ndarray op (`scores = net_out[idx].asnumpy()` at line 221 of retinaface.py), GPU/CPU must wait for the other computations. Here, it seem that it wait for the foward command. 

Do you have any idea to change this by other one, in ordet to reduce the retinaface inference time ? "
"hi，大家好。我使用for循环调用100万次np.dot(fc1,fc2.T)，总共耗时3秒。如果我有大量特征需要进行比对（1：N），请问是否有其他方法可以提升这个效率?"
"max-in-out predict cp+cn scores for each anchor,  but when we use sigmoid focal loss,the predict convolution only predict positive score"
I trained model mobilenetV2 according to the parameters provided in **src/train_softmax.py** and I didn't modify the **fmobilenetv2.py** in **src/symbols/**. Why is the model generated so large as 298M?
"I want to make deploy_lib.so for android.
If i do     target = ""llvm -target=%s-linux-android"" % arch

I get this output:

Traceback (most recent call last):
  File ""my.py"", line 39, in <module>
    lib.export_library(""./deploy_lib.so"")
  File ""/home/hi/.local/lib/python3.6/site-packages/tvm-0.6.dev0-py3.6-linux-x86_64.egg/tvm/module.py"", line 144, in export_library
    fcompile(file_name, files, **kwargs)
  File ""/home/hi/.local/lib/python3.6/site-packages/tvm-0.6.dev0-py3.6-linux-x86_64.egg/tvm/contrib/cc.py"", line 49, in create_shared
    _linux_shared(output, objects, options, cc)
  File ""/home/hi/.local/lib/python3.6/site-packages/tvm-0.6.dev0-py3.6-linux-x86_64.egg/tvm/contrib/cc.py"", line 106, in _linux_shared
    raise RuntimeError(msg)
RuntimeError: Compilation error:
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/usr/bin/ld: /tmp/tmp1o1yjo8w/lib.o: Relocations in generic ELF (EM: 183)
/tmp/tmp1o1yjo8w/lib.o: error adding symbols: File in wrong format
collect2: error: ld returned 1 exit status

P.S. If I do target = ""llvm"" it is work


"
how can i convert demo.py to real time(webcam) recognition any idea?
"INFO:root:Epoch[0] Batch [1140-1160]	Speed: 247.30 samples/sec	acc=0.000000	lossvalue=21.100723
INFO:root:Epoch[0] Batch [1160-1180]	Speed: 245.00 samples/sec	acc=0.000000	lossvalue=19.769028
INFO:root:Epoch[0] Batch [1180-1200]	Speed: 245.06 samples/sec	acc=0.000000	lossvalue=18.628158
INFO:root:Epoch[0] Batch [1200-1220]	Speed: 244.43 samples/sec	acc=0.000000	lossvalue=17.109090
INFO:root:Epoch[0] Batch [1220-1240]	Speed: 246.55 samples/sec	acc=0.000000	lossvalue=15.743760
INFO:root:Epoch[0] Batch [1240-1260]	Speed: 242.86 samples/sec	acc=0.000000	lossvalue=14.274475
INFO:root:Epoch[0] Batch [1260-1280]	Speed: 243.37 samples/sec	acc=0.000000	lossvalue=12.779066
INFO:root:Epoch[0] Batch [1280-1300]	Speed: 240.69 samples/sec	acc=0.000000	lossvalue=11.271037
INFO:root:Epoch[0] Batch [1300-1320]	Speed: 249.24 samples/sec	acc=0.000391	lossvalue=9.407644
INFO:root:Epoch[0] Batch [1320-1340]	Speed: 246.67 samples/sec	acc=0.054297	lossvalue=7.082070
INFO:root:Epoch[0] Batch [1340-1360]	Speed: 245.84 samples/sec	acc=0.349219	lossvalue=5.001186
INFO:root:Epoch[0] Batch [1360-1380]	Speed: 245.75 samples/sec	acc=0.686719	lossvalue=3.881259
INFO:root:Epoch[0] Batch [1380-1400]	Speed: 249.07 samples/sec	acc=0.846875	lossvalue=3.216524
INFO:root:Epoch[0] Batch [1400-1420]	Speed: 246.71 samples/sec	acc=0.851562	lossvalue=3.583631
INFO:root:Epoch[0] Batch [1420-1440]	Speed: 245.26 samples/sec	acc=0.933594	lossvalue=2.859563
INFO:root:Epoch[0] Batch [1440-1460]	Speed: 246.78 samples/sec	acc=0.951172	lossvalue=2.529359
INFO:root:Epoch[0] Batch [1460-1480]	Speed: 247.23 samples/sec	acc=0.969141	lossvalue=2.240702
INFO:root:Epoch[0] Batch [1480-1500]	Speed: 247.20 samples/sec	acc=0.966406	lossvalue=2.259592
INFO:root:Epoch[0] Batch [1500-1520]	Speed: 246.31 samples/sec	acc=0.903906	lossvalue=3.127749
INFO:root:Epoch[0] Batch [1520-1540]	Speed: 246.39 samples/sec	acc=0.955469	lossvalue=2.873047
INFO:root:Epoch[0] Batch [1540-1560]	Speed: 243.54 samples/sec	acc=0.968750	lossvalue=2.468465
INFO:root:Epoch[0] Batch [1560-1580]	Speed: 248.75 samples/sec	acc=0.966797	lossvalue=2.411550
INFO:root:Epoch[0] Batch [1580-1600]	Speed: 248.47 samples/sec	acc=0.972656	lossvalue=2.284695
INFO:root:Epoch[0] Batch [1600-1620]	Speed: 247.98 samples/sec	acc=0.976953	lossvalue=2.192420
INFO:root:Epoch[0] Batch [1620-1640]	Speed: 246.16 samples/sec	acc=0.978516	lossvalue=2.177217
INFO:root:Epoch[0] Batch [1640-1660]	Speed: 247.84 samples/sec	acc=0.979297	lossvalue=2.128415
INFO:root:Epoch[0] Batch [1660-1680]	Speed: 242.92 samples/sec	acc=0.976953	lossvalue=2.154863
INFO:root:Epoch[0] Batch [1680-1700]	Speed: 241.34 samples/sec	acc=0.981250	lossvalue=2.103897
INFO:root:Epoch[0] Batch [1700-1720]	Speed: 243.85 samples/sec	acc=0.982031	lossvalue=2.055088
INFO:root:Epoch[0] Batch [1720-1740]	Speed: 248.80 samples/sec	acc=0.983594	lossvalue=2.012406
INFO:root:Epoch[0] Batch [1740-1760]	Speed: 245.14 samples/sec	acc=0.982422	lossvalue=2.022595
INFO:root:Epoch[0] Batch [1760-1780]	Speed: 246.92 samples/sec	acc=0.983594	lossvalue=1.981786
INFO:root:Epoch[0] Batch [1780-1800]	Speed: 252.34 samples/sec	acc=0.981250	lossvalue=2.062618
INFO:root:Epoch[0] Batch [1800-1820]	Speed: 251.27 samples/sec	acc=0.979688	lossvalue=2.217473
INFO:root:Epoch[0] Batch [1820-1840]	Speed: 246.47 samples/sec	acc=0.979688	lossvalue=2.153644
INFO:root:Epoch[0] Batch [1840-1860]	Speed: 246.94 samples/sec	acc=0.945703	lossvalue=2.676979
INFO:root:Epoch[0] Batch [1860-1880]	Speed: 244.38 samples/sec	acc=0.977344	lossvalue=2.311590
INFO:root:Epoch[0] Batch [1880-1900]	Speed: 245.20 samples/sec	acc=0.982031	lossvalue=2.193411
INFO:root:Epoch[0] Batch [1900-1920]	Speed: 245.41 samples/sec	acc=0.982422	lossvalue=2.135135
INFO:root:Epoch[0] Batch [1920-1940]	Speed: 248.04 samples/sec	acc=0.982812	lossvalue=2.132701
INFO:root:Epoch[0] Batch [1940-1960]	Speed: 248.79 samples/sec	acc=0.982031	lossvalue=2.102737
INFO:root:Epoch[0] Batch [1960-1980]	Speed: 243.51 samples/sec	acc=0.982422	lossvalue=2.144044
lr-batch-epoch: 0.1 1999 0
testing verification..
(12000, 512)
infer time 29.372585"
"Hi! 
first of all, thank you for your great works! 
I have a simple question about code. 
In retinaface.py (below) 

https://github.com/deepinsight/insightface/blob/be3f7b3e0e635b56d903d845640b048247c41c90/RetinaFace/retinaface.py#L294

what is role of ""decay4"" in the code? 
why decay scores when stride is equals to 4? 

thank you"
"Can you confirm whether the alignment algorithm given for c++ and python are same ?

Least-squares estimation of transformation parameters between two point patterns
https://github.com/deepinsight/insightface/blob/master/cpp-align/FacePreprocess.h

Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment
https://github.com/deepinsight/insightface/tree/master/alignment 

Thank you.

"
"  File ""/data/code/RetinaFace/insightface/RetinaFace/retinaface.py"", line 233, in detect
    scores = scores[:, self._num_anchors['stride%s'%s]:, :, :]
IndexError: too many indices for array

请问这个是什么错误 ？ 如何解决？ 谢谢"
"Hi,

Below  image from LFW face can be detected by MTCNN (one face) . but retinaface cant detect the face ?

Any idea how we can do ? In many cases retina superior than mtcnn but such cases I am suprised.

image : 

![000326_00926089](https://user-images.githubusercontent.com/17505439/61389773-263a7d80-a8c2-11e9-881d-defa9c2e805f.jpg)

I am testing witht default deploy/test.py . (mtcnn) . and Retinaface/test.py

Best

"
"I try to load pretrained model but get zero accuracy in the begining. In train.py I am doing the following way:

` sym, arg_params, aux_params = mx.model.load_checkpoint(os.path.join(args.pretrained, ""model""), args.pretrained_epoch)
    sym = get_symbol(args)
    model = mx.mod.Module(
        context=ctx,
        symbol=sym,
    )
    model.bind(data_shapes=train_dataiter.provide_data, label_shapes=train_dataiter.provide_label)
    model.set_params(arg_params, aux_params)

    model.fit(train_dataiter,
        num_epoch=999999,
        eval_data=val_dataiter,
        eval_metric=eval_metrics,
        kvstore=args.kvstore,
        optimizer=opt,
        initializer=initializer,
        allow_missing=True,
        batch_end_callback=_batch_callback,
        arg_params=arg_params,
        aux_params=aux_params)`
"
""
假如在insightface/recognition/symbol/中写了一个`[mobileNetV3.py](https://github.com/wsqshiqing/MobileNetV3/blob/master/mobilenetv3.py)`的文件，如果要执行insightface/recognition/train.py还需要修改insightface/recognition/下的哪些文件？求助~
"@nttstar @jiankangdeng 
Hi, all:
Could we design a end2end model to recognize age, gender and face id in one architecture？Or, use the face recognition model to finetune the gender-age model to recognize face id, age and gender?
Thanks.
"
"Thanks for sharing your excellent work, especially the landmak annotations. Can you provide more ablation studies results other than models in the paper. For example, SSH module, FPN, deformable layers, context module designed by you, and so on. Thanks"
"Hi,

in the deploy / test.py

`img_pre , points = model.get_input(img)
`

Face not detected in first image ,  but when I resize image smaller png its detecting the face.

Any idea why it does so ? 

First image big one no face :



Code details for mtcnn 
```
class FaceModel:
  def __init__(self, args):
    self.args = args
    ctx = mx.cpu(args.gpu)
    _vec = args.image_size.split(',')
    assert len(_vec)==2
    image_size = (int(_vec[0]), int(_vec[1]))
    self.model = None
    self.ga_model = None
    if len(args.model)>0:
      self.model = get_model(ctx, image_size, args.model, 'fc1')
    if len(args.ga_model)>0:
      self.ga_model = get_model(ctx, image_size, args.ga_model, 'fc1')

    self.threshold = args.threshold
    self.det_minsize = 50
    self.det_threshold = [0.6,0.7,0.8]
    #self.det_factor = 0.9
    self.image_size = image_size
    mtcnn_path = os.path.join(os.path.dirname(__file__), 'mtcnn-model')
    if args.det==0:
      detector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=1, accurate_landmark = True, threshold=self.det_threshold)
    else:
      detector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=1, accurate_landmark = True, threshold=[0.0,0.0,0.2])
    self.detector = detector
```"
"I am trapped in the big confusion on how to make a dataset that can be used to train arcface model out of my own face data directory, in the structure as below:
```
.
├── PersonA
│   ├── 001.jpeg
│   └── 002.jpeg
├── PersonB
│   ├── 001.jpeg
│   └── 002.jpeg
└── PersonC
    ├── 001.jpeg
    ├── 002.jpeg
    └── 003.jpeg

3 directories, 7 files
```
It would be better to show me the steps in detail in the following form:
1. ...
2. ...
3. ...
...

Much appreciation and thanks!"
"Has anyone managed to get this to compile on windows and with just CPU? 

-F-RCNN seems to require a GPU so .pyx files won't build in the rcnn dir
"
"I have read some issues, and found some discuss about dataset.
ms1m-v1 = ms1m-ibug, include 85k IDS/3.8M images
ms1m-v2 = ms1m-arc=emore include 85K IDS/5.8M images
I found in arc-loss paper,  ms1m-ibug is used，while the pretrained model in open model zoo, ms1m-arc is used.

But sitll have some confusion:
1. this two datasets have same ids, but why the second one has 2M images more? someone said data augment is used? if really, augment types?

2. Also DeepGlint(in dataset zoo 181K IDS/6.75 images) is same with ori deepglint website(http://trillionpairs.deepglint.com/data)? or any other operations?

3. Asian-celeb(94K IDS/2.8M images) has relationship with deepglint? 
@nttstar "
"Hi,

Which one is better chance to have good landmark and recognition these person  (mtcnn or retina-face):

We indexed frontal view and want to recognise from side view.

Is it possible ? which way we should follow.

<img width=""1392"" alt=""Screen Shot 2019-07-10 at 22 18 32"" src=""https://user-images.githubusercontent.com/17505439/60998056-eb37c780-a360-11e9-9cca-2fc8e0fe22ff.png"">
"
"Hi,

We are testing the the poor face images some of them really poor bkack6white and old. Mtcnn and others can not detect in some cases. 

Is it useful to pin-point 5 landmark & align  by manually and than send them for feature extraction ? "
"hello

Could anyone let me know on what dataset is the pretrained model trained ?
[Pretrained model link](https://github.com/deepinsight/insightface/tree/master/RetinaFace#retinaface-pretrained-models)

Is it on WIDERFACE or on anything else as well ?

Thanks"
"Just want to know the training time cost when training the RetinaFace?
Separately for ResNet50 and ResNet152.


Thank you!

"
"Could someone share an Asian celeb pretrained model?

@nttstar Are you going to release a ""MS1MV2+DeepGlintAsian"" model?

Greetings
"
"采用  --loss-type = 4 的时候 acc=0，lossvalue始终很小，epoch也已经达到20了，请问是怎么回事

lr-batch-epoch: 0.1 279 20
INFO:root:Epoch[20] Batch [280] Speed: 223.32 samples/sec       acc=0.000000    lossvalue=0.000012
INFO:root:Epoch[20] Batch [300] Speed: 223.86 samples/sec       acc=0.000000    lossvalue=0.000016
INFO:root:Epoch[20] Batch [320] Speed: 223.30 samples/sec       acc=0.000000    lossvalue=0.000016
INFO:root:Epoch[20] Batch [340] Speed: 223.48 samples/sec       acc=0.000000    lossvalue=0.000018
INFO:root:Epoch[20] Batch [360] Speed: 223.33 samples/sec       acc=0.000000    lossvalue=0.000010
INFO:root:Epoch[20] Batch [380] Speed: 223.15 samples/sec       acc=0.000000    lossvalue=0.000013
INFO:root:Epoch[20] Batch [400] Speed: 222.99 samples/sec       acc=0.000000    lossvalue=0.000011
INFO:root:Epoch[20] Batch [420] Speed: 223.47 samples/sec       acc=0.000000    lossvalue=0.000024
INFO:root:Epoch[20] Batch [440] Speed: 223.31 samples/sec       acc=0.000000    lossvalue=0.000085
INFO:root:Epoch[20] Batch [460] Speed: 223.44 samples/sec       acc=0.000000    lossvalue=0.000058
INFO:root:Epoch[20] Batch [480] Speed: 223.40 samples/sec       acc=0.000000    lossvalue=0.000043
INFO:root:Epoch[20] Batch [500] Speed: 223.35 samples/sec       acc=0.000000    lossvalue=0.000032"
"Hi,
 I have downloaded the datasets from NIST but I don't know which images to use for 1-1 face verification. I am able to see only 138836 (images +frames) in the downloaded set. But 
you are using 469,375 images in IJB-C dataset evaluation script [https://github.com/deepinsight/insightface/blob/master/Evaluation/IJB/IJBC_Evaluation_VGG2.ipynb]

You have mentioned that you are using loose crop from VGG group. Are these images different from what is given by NIST?
Please can you help me in evaluating 1-1 face verification on IJB-B and C datasets?
Thanking you."
"hi 
i need some help to figure out the args in face embedding and Face model example 
especially  i would like to know what are and how to use this :
prefix 
epoch
ctx
layer
than k you"
"Hi, I have trouble in reproduce the recognition result just shown in 'ArcFace Video Demo' section. It seems that the code in recognition folder is for testing accuracy. I'm not sure if I was missed something.
Thank you!"
""
GridFace的思路来源在STN。我看到STN就觉得可能可以应用到人脸检测阶段中。STN应用人脸方面的研究只搜到GridFace。但GridFace的STN思路不是应用在检测阶段，而是直接参与识别模型的训练。所以我就想如果引入到ArcFace中，会不会有效果？
"![detector_test](https://user-images.githubusercontent.com/35062849/60575328-8e0ea580-9da5-11e9-8eae-482bf35570a2.jpg)
Hi,
I ran /Retina/test.py and get wrong result ( double rectangle on a face) and cant detector another face.
Please tell me what's happend"
is there a demo code to test  face recognization on vide or on the image instead of comparing 
""
"Error in operator _minus0: [15:16:52] /work/mxnet/3rdparty/mshadow/../../src/operator/tensor/../elemwise_op_common.h:135: Check failed: assign(&dattr, vec.at(i)) Incompatible attr in node _minus0 at 1-th input: expected [10,512], got [11,512]"
"@MyraBaba 
hello,i also meet this issue,how did you resolved it
File ""/Users/xxx/Projects/new_/insight_10May/insightface/RetinaFace/retinaface.py"", line 228, in detect
scores = scores[:, self._num_anchors['stride%s'%s]:, :, :]
IndexError: too many indices for array
please help

_Originally posted by @aneesh0 in https://github.com/deepinsight/insightface/issues/669#issuecomment-503545429_"
can you share your train log about resnet50?
"----------
Based on open model(trained on ms1m,test acc are 99.77/98.27/98.28), I finetuning the model on Deepglint, but test acc on lfw/cfp_fp/ageddb are 99.77/96.61/98.15, show above. Besides, I collect several face in real as another testset, this test results also shows the finetuning model has no improvement for face recognition. So anyone did something similar? 

-------training logs-----------
testing verification..
(12000, 512)
infer time 23.246731
[lfw][444000]XNorm: 20.568385
[lfw][444000]Accuracy-Flip: 0.99783+-0.00259
testing verification..
(14000, 512)
infer time 26.563688
[cfp_fp][444000]XNorm: 22.903938
[cfp_fp][444000]Accuracy-Flip: 0.96614+-0.01038
testing verification..
(12000, 512)
infer time 22.508262
[agedb_30][444000]XNorm: 23.183821
[agedb_30][444000]Accuracy-Flip: 0.98150+-0.00736
saving 111
INFO:root:Saved checkpoint to ""../models/model-r100-glint-arc2/model-0111.params""
[444000]Accuracy-Highest: 0.98150
INFO:root:Epoch[14] Batch [23820]	Speed: 109.18 samples/sec	acc=0.981185
INFO:root:Epoch[14] Batch [23880]	Speed: 338.00 samples/sec	acc=0.980519
INFO:root:Epoch[14] Batch [23940]	Speed: 343.30 samples/sec	acc=0.980000
INFO:root:Epoch[14] Batch [24000]	Speed: 344.53 samples/sec	acc=0.982222
INFO:root:Epoch[14] Batch [24060]	Speed: 342.03 samples/sec	acc=0.982074

----------
1.Anyone finetuning the model? Is there any improvement?
2.I checked ms1m has 5.8M img and 85K IDs(65 imgs/id), but deepglint has 6.75M imgs and 181K IDs(37 imgs/id), deepglint has more ids, but the total imgs almost same. So if keep total imgs same(10M), more id(400K,25imgs/id), less id(200K,50imgs/id) which is more useful for face recognition training?
3.Anyone trys to merge ms1m and deepglint and trains model from beginning? any improvement?

Glad to discuss! "
"Hi,
Do you have the loss curve over the training set of the vggface2 dataset ? 
I'm currently training your model on the vggface2 dataset and my error after 5 epochs is around 0.03. However the the performance of that model over the some tests from the same training set is not good. 
Thanks !"
"Dear @nttstar @yingfeng @jiankangdeng,

Thank you for your wondeful works.

I have one question about evaluation on IJB-C.
Now, I try to use RetinaFace-R50 to detect face.
 I found that there have some templates (image group of certain ID) in which RetinaFace-R50 fails to detect all images. So, if you failed to detect all face regions in one template (image group of certain ID), how did you treat this case in calculate ROC? Ignore this template or count this template to false positive or label manully the image which RetinaFace-R50 fails to give any face?

Looking forward to your reply.
Thank you in advance.
"
"Hi, @nttstar 
I want to know how to set the margin(align_dataset_mtcnn.py, line 134), when use new training dataset?
https://github.com/deepinsight/insightface/blob/339dd2611f220f5458bc3cb22ff7dac8d32ab27a/src/align/align_dataset_mtcnn.py#L134

Thanks."
"@nttstar 
Hey, how to run mxnet in single-thread mode? 
```
Implementations must run in single-threaded mode, because NIST will parallelize the test 
by dividing the workload across many cores and many machines. Implementations must ensure 
that there are no issues with their software being parallelized via the fork() function. 
Developers should take caution with checking threading when using third-party frameworks 
(e.g., TensorFlow, MXNet, etc.).
```"
I have reviewed the code and cannot find it? can anyone tell me how?thanks
"@nttstar 
Hi, nttstar
When run the train_parall.py, the acc and loss_value is nan:
INFO:root:Epoch[0] Batch [60-80]        Speed: 606.04 samples/sec       acc=nan lossvalue=nan
and I want to know the root cause of this.
Thanks."
"Thanks for your code.
pip install builtins 
ERROR: Could not find a version that satisfies the requirement builtins (from versions: none)
ERROR: No matching distribution found for builtins
i cant find any useful information.
thanks.
"
"  File ""/data/RetinaFace/retinaface.py"", line 13, in <module>
    from nms import gpu_nms_wrapper
  File ""/data/RetinaFace/nms.py"", line 2, in <module>
    from gpu_nms import gpu_nms
ImportError: No module named gpu_nms
"
retinaface-r50 detect module adopt SSH or retina training
""
"in fresnet.py resnet50 units = [3, 4, 14, 3] and  filter_list = [64, 64, 128, 256, 512], not the same to
 https://github.com/tornadomeet/ResNet/blob/master/symbol_resnet.py
and the paper:
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. ""Identity Mappings in Deep Residual Networks"", is there something wrong?"
""
"In retinaface paper, the authors test five point landmarks accuracy on both aflw dataset and wider face validation dataset. But I did not find wider face validation dataset's landmark annotations in the given baiduyun link and dropbox link."
当ID数太多，训练时报GPU显存不够，有办法么？例如三千万ID。能改改recognition/train_parall.py，来支持多机器多GPU训练么？非常感谢！
"I just follow your `test.py` and add some code like following:
```
count = 10
for c in range(count):
    start = time.time()
    faces, landmarks = detector.detect(img, thresh, scales=[im_scale])
    end = time.time()
    print(end - start)
    print(c, faces.shape, landmarks.shape)
```
but find it running slowly, it takes 10 more seconds to forward an image.
```
12.197124958
(0, (1, 5), (1, 5, 2))
0.05788397789
(1, (1, 5), (1, 5, 2))
0.0318050384521
(2, (1, 5), (1, 5, 2))
0.030042886734
(3, (1, 5), (1, 5, 2))
0.0293989181519
(4, (1, 5), (1, 5, 2))
0.0300710201263
(5, (1, 5), (1, 5, 2))
0.0289559364319
(6, (1, 5), (1, 5, 2))
0.0285160541534
(7, (1, 5), (1, 5, 2))
0.0282299518585
(8, (1, 5), (1, 5, 2))
0.0288300514221
(9, (1, 5), (1, 5, 2))
```

Can you please help me to solve this problem? I am using `mxnet-cu90 (1.4.1)` testing on 1080Ti. Thanks!"
"I think pairs.txt was made according to your example pairs_label.txt. Could you tell me what's wrong with it? (in pairs_label.txt we have 500 of positive (name/name_0001.png name/name_0002.png 1, etc.) and 500 negative (name1/name1_0001.png name2/name2_0001.png 0, etc) like labels. Total of 1000 rows of such info.  
Error code is here:   
File ""build_eval_pack.py"", line 112, in <module>
    im1 = get_norm_crop(path1)
  File ""build_eval_pack.py"", line 77, in get_norm_crop
    bbox, landmark = detector.detect(im, threshold=0.5, scales=[im_scale])
  File ""..\..\RetinaFace\retinaface.py"", line 204, in detect
    self.model.forward(db, is_train=False)
  File ""C:\Users\TengriLab\AppData\Local\Programs\Python\Python37\lib\site-packages\mxnet\module\module.py"", line 625, in forward
    self.reshape(new_dshape, new_lshape)
  File ""C:\Users\TengriLab\AppData\Local\Programs\Python\Python37\lib\site-packages\mxnet\module\module.py"", line 472, in reshape
    self._exec_group.reshape(self._data_shapes, self._label_shapes)
  File ""C:\Users\TengriLab\AppData\Local\Programs\Python\Python37\lib\site-packages\mxnet\module\executor_group.py"", line 396, in reshape
    self.bind_exec(data_shapes, label_shapes, reshape=True)
  File ""C:\Users\TengriLab\AppData\Local\Programs\Python\Python37\lib\site-packages\mxnet\module\executor_group.py"", line 372, in bind_exec
    allow_up_sizing=True, **dict(data_shapes_i + label_shapes_i))
  File ""C:\Users\TengriLab\AppData\Local\Programs\Python\Python37\lib\site-packages\mxnet\executor.py"", line 455, in reshape
    ctypes.byref(handle)))
  File ""C:\Users\TengriLab\AppData\Local\Programs\Python\Python37\lib\site-packages\mxnet\base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [10:18:24] C:\Jenkins\workspace\mxnet-tag\mxnet\src\executor\graph_executor.cc:858: Shape of unspecifie arg: pre_fc1_weight changed. This can cause the new executor to not share parameters with the old one. Please check for error in network.If this is intended, set partial_shaping=True to suppress this warning.
"
""
"I took datasets from DatasetZoo of main repo. What should I do besides preinstalls to run pretrained retinaface detector on this datasets. Files are .rec, .bin, .lst, .idx after unzipping. Please give me short instructions how to process all with your detector."
"1 NVIDIA TITAN Xp, MS-1M_v2, batch_size : 64
How long does it take to train a resnet100 model with environment and configuration?"
""
"Hi,

What is the best config for training single 2080txi GpU?

I downloaded faces_emore and start training with : 
`` CUDA_VISIBLE_DEVICES='0' python3 -u train.py --network m1 --loss softmax --dataset emore`

it give below error : 

> gpu num: 1
prefix ./models/m1-softmax-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=128, ckpt=3, ctx_num=1, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='softmax', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='m1', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'ckpt_embedding': True, 'net_unit': 3, 'num_workers': 1, 'net_act': 'prelu', 'data_rand_mirror': True, 'data_color': 0, 'data_images_filter': 0, 'image_shape': [112, 112, 3], 'max_steps': 0, 'emb_size': 256, 'net_input': 1, 'num_classes': 85742, 'bn_mom': 0.9, 'network': 'm1', 'fc7_no_bias': False, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'workspace': 256, 'loss': 'softmax', 'dataset': 'emore', 'memonger': False, 'ce_loss': True, 'net_output': 'GDC', 'dataset_path': '../datasets/faces_emore', 'data_cutoff': False, 'net_name': 'fmobilenet', 'batch_size': 128, 'per_batch_size': 128, 'net_multiplier': 1.0, 'fc7_wd_mult': 1.0, 'count_flops': True, 'loss_name': 'softmax', 'net_se': 0, 'fc7_lr_mult': 1.0, 'net_blocks': [1, 4, 6, 2]}
Network FLOPs: 1.1G
INFO:root:loading recordio ../datasets/faces_emore/train.rec...
5908396
rand_mirror True
[12:20:09] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()
/usr/local/lib/python3.5/dist-packages/mxnet/module/base_module.py:504: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
  optimizer_params=optimizer_params)
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/io/io.py"", line 399, in prefetch_func
    self.next_batch[i] = self.iters[i].next()
  File ""/retin/iface/recognition/image_iter.py"", line 198, in next
    label, s, bbox, landmark = self.next_sample()
  File ""/retin/iface/recognition/image_iter.py"", line 114, in next_sample
    s = self.imgrec.read_idx(idx)
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/recordio.py"", line 318, in read_idx
    return self.read()
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/recordio.py"", line 209, in read
    ctypes.byref(size)))
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/base.py"", line 254, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [12:20:31] src/recordio.cc:65: Check failed: header[0] == RecordIOWriter::kMagic: Invalid RecordIO File
Stack trace:
  [bt] (0) /usr/local/lib/python3.5/dist-packages/mxnet/libmxnet.so(+0x2d7d992) [0x7fd40ca38992]
  [bt] (1) /usr/local/lib/python3.5/dist-packages/mxnet/libmxnet.so(MXRecordIOReaderReadRecord+0x2a) [0x7fd40c0a854a]
  [bt] (2) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7fd46435de20]
  [bt] (3) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7fd46435d88b]
  [bt] (4) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7fd46435801a]
  [bt] (5) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x9fcb) [0x7fd46434bfcb]
  [bt] (6) python3(PyObject_Call+0x47) [0x5c20e7]
  [bt] (7) python3(PyEval_EvalFrameEx+0x4ed6) [0x53b656]
  [bt] (8) python3(PyEval_EvalFrameEx+0x4b14) [0x53b294]


Before that I edit 👍 
vim /retin/iface/recognition/image_iter.py
line : 39:
`s = self.imgrec.read_idx(0)`

to

`s = self.imgrec.read_idx(1)`

when idx is 0 ti gives 👍 

> INFO:root:loading recordio ../datasets/faces_emore/train.rec...
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 242, in train_net
    images_filter        = config.data_images_filter,
  File ""/retin/iface/recognition/image_iter.py"", line 39, in __init__
    s = self.imgrec.read_idx(0)
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/recordio.py"", line 318, in read_idx
    return self.read()
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/recordio.py"", line 209, in read
    ctypes.byref(size)))
  File ""/usr/local/lib/python3.5/dist-packages/mxnet/base.py"", line 254, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [12:25:28] src/recordio.cc:65: Check failed: header[0] == RecordIOWriter::kMagic: Invalid RecordIO File
Stack trace:
  [bt] (0) /usr/local/lib/python3.5/dist-packages/mxnet/libmxnet.so(+0x2d7d992) [0x7f738e36a992]
  [bt] (1) /usr/local/lib/python3.5/dist-packages/mxnet/libmxnet.so(MXRecordIOReaderReadRecord+0x2a) [0x7f738d9da54a]
  [bt] (2) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7f73e5ccfe20]
  [bt] (3) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7f73e5ccf88b]
  [bt] (4) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7f73e5cca01a]
  [bt] (5) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x9fcb) [0x7f73e5cbdfcb]
  [bt] (6) python3(PyObject_Call+0x47) [0x5c20e7]
  [bt] (7) python3(PyEval_EvalFrameEx+0x4ed6) [0x53b656]
  [bt] (8) python3(PyEval_EvalFrameEx+0x4b14) [0x53b294]


[12:25:28] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice


Is Anybody show me the direction to start working my 2080txi on this training?

Best


"
"Hi @nttstar 

Which model is more accurate ? Front page and the wiki has different number:

Which model I should use for best accuracy ?


FronPage:

> 

LResNet100E-IR network trained on MS1M-Arcface dataset with ArcFace loss:


Method | LFW(%) | CFP-FP(%) | AgeDB-30(%)
-- | -- | -- | --
Ours | 99.80+ | 98.0+ | 98.20+


Model_zoo:

> 
3.1 LResNet100E-IR,ArcFace@ms1m-refine-v2

Method | LFW(%) | CFP-FP(%) | AgeDB-30(%) | MegaFace(%)
-- | -- | -- | -- | --
Ours | 99.77 | 98.27 | 98.28 | 98.47


3.3 LResNet34E-IR,ArcFace@ms1m-refine-v1

Method | LFW(%) | CFP-FP(%) | AgeDB-30(%) | MegaFace(%)
-- | -- | -- | -- | --
Ours | 99.65 | 92.12 | 97.70 | 96.70



"
"I had a problem. When i apply arcface implementation, i got error.

From this line code:

```
face_imgs_resized = np.array(face_imgs_resized)
face_imgs_resized = np.rollaxis(face_imgs_resized, 3, 1)

data = self.mx.nd.array(face_imgs_resized)
db = self.mx.io.DataBatch(data=(data,))
self.model.forward(db, is_train=False)
```

And the error:

```
    self.model.forward(db, is_train=False)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/module/module.py"", line 625, in forward
    self.exec_group.forward(data_batch, is_train)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/module/executor_group.py"", line 450, in forward
    load_data(data_batch, self.data_arrays, self.data_layouts)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/module/executor_group.py"", line 74, in _load_data
    _load_general(batch.data, targets, major_axis)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/module/executor_group.py"", line 48, in _load_general
    d_src[slice_idx.start:slice_idx.stop].copyto(d_dst)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py"", line 506, in __getitem
    return self._get_nd_basic_indexing(key)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py"", line 787, in _get_nd_basic_indexing
    return self._slice(key.start, key.stop)
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py"", line 902, in _slice
    start, stop, _ = _get_index_range(start, stop, self.shape[0])
  File ""/root/miniconda/envs/roy/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py"", line 2327, in _get_index_range
    raise IndexError('Slicing stop %d exceeds limit of %d' % (stop, length))
IndexError: Slicing stop 2 exceeds limit of 1
```

What is the problem with my data, so anyone ever met this error before? And please show me how to fix it."
"I don't have access to huge gpu machines ... can anyone train the ms1m model on EfficientNet.
https://github.com/lukemelas/EfficientNet-PyTorch

EfficientNet-B1 gives the same accuracy on Imagenet as Resnet152 with 10 times less computation power.  Worth a try !"
"Thank you very much for providing your code, I hope you can provide the aligned YTF dataset.
I didn't get the desired results on my own data set.
thank you very much"
""
"Hello, I'd like to ask some questions about RetinaFace 

(1) Paper illustrates training data would apply color distortion or random crop to do  data augmentation. 

I've seen ""rcnn.core.loader"" Does this implement random crop or which file can I check to do ""random crop"" and ""color distortion"" 
 
(2) Landmark loss used L1 loss, if we want to use L2 or smoothL1 instead, we can add ""class RPN_ooo_Metric(mx.metric.EvalMetric):"" to implement? 

 "
"If you want to use RetinaFace in deploy, please, remove ""reshape(2, 5).T"" for RetinaFace landmarks, because the are already ordered as follows -> [[x1, y1], [x2, y2], ... [x5, y5]]"
What would happen if I train the network using Retinaface aligned dataset and test it on MTCNN aligned dataset? Would the performance improve by using Retina face in training?
"![image](https://user-images.githubusercontent.com/14344027/58616408-2968b280-82f0-11e9-8ead-d55380221f26.png)
"
"Hi Jack, i have referenced the instruction about how to run on TVM. however i still have some questions.
1.  the MobileFace running time is not the same as you report.I deploy the model on Huawei Mate20 and it use 61ms@400%CPU while as you report Firefly Rk 3399@A72 * 1 | 69.24ms. it only use 100% CPU?

2. the tvm benchmark models are auto-fined with autoTVM? 

3. Try to remove minimal value(<e-4) in model parameters,These value will cause bad performance. it means set these minimal value to 0,or remove it? can you give me the example code ?

4. i plan deploy the LResNet50-IR on arm device,but the model size is very big,can you give me some advise about how to slim them?

thanks very much, and looking forward you feedback


"
Any workaround available for changing UpSampling operation? mx2onnx currently doesn't support this.
"My age-gender model training accuracy is not increasing more than 53%, fill me with details

"
"My age-gender model training accuracy is not increasing more than 53%, fill me with details"
"硬件配置：
GPU:nvidia 1080ti *4
CPU:amd 2960x
内存:128G
操作系统：ubuntu 18.04
python:2.7
nvidia显卡驱动：nvidia-driver-430
cuda:10.0
mxnet:mxnet-cu100

<
wangting@wangting:~/insightface/recognition$ CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore
gpu num: 4
prefix ./models/r100-arcface-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=64, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=16, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_act': 'prelu', 'emb_size': 512, 'data_rand_mirror': True, 'num_layers': 100, 'loss_name': 'margin_softmax', 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'net_input': 1, 'image_shape': [112, 112, 3], 'net_blocks': [1, 4, 6, 2], 'fc7_lr_mult': 1.0, 'ckpt_embedding': True, 'net_unit': 3, 'net_output': 'E', 'count_flops': True, 'num_workers': 1, 'batch_size': 64, 'memonger': False, 'data_images_filter': 0, 'dataset': 'emore', 'num_classes': 85742, 'fc7_no_bias': False, 'loss': 'arcface', 'data_color': 0, 'loss_s': 64.0, 'dataset_path': '/home/wangting/insightface/datasets/faces_ms1m_112x112', 'data_cutoff': False, 'net_se': 0, 'net_multiplier': 1.0, 'fc7_wd_mult': 1.0, 'network': 'r100', 'per_batch_size': 16, 'net_name': 'fresnet', 'workspace': 256, 'max_steps': 0, 'bn_mom': 0.9}
0 1 E 3 prelu False
Network FLOPs: 24.2G
INFO:root:loading recordio /home/wangting/insightface/datasets/faces_ms1m_112x112/train.rec...
header0 label [3804847. 3890011.]
id2range 85164
3804846
rand_mirror True
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()
/home/wangting/.local/lib/python2.7/site-packages/mxnet/module/base_module.py:505: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.015625). Is this intended?
  optimizer_params=optimizer_params)
>
程序运行到此处就不再往下运行了，没有报错
<img width=""634"" alt=""image"" src=""https://user-images.githubusercontent.com/25969464/58548996-18129e00-823d-11e9-877b-9a1e283617d7.png"">


sudo kill -9 27155强行kill进程时无效，系统直接死机。
我反复重装系统，确保没有其他干扰，尝试过cuda9.0+mxnet-cu90和cuda10.0+mxnet-cu100，都是同样的效果。
我在windows10下运行则一切正常，排除硬件故障。请问我应该如何做才能正常运行？
"
"hi, 

Is there anyone who tried only Asian-Celeb data set for model training ?  I tried but could only get 98.5% accuracy on LFW,  87.9 on CFP_FP and 90.1  on AgeDB30.  

Is there any reason to explain why the accuracy is so low by only using asian-celeb data ?  My silly doubt is that the training data is with **asian face**, but LFW, CFP_FP data are not... 

Any comments are welcoming ! "
"Hi,
would love to know if
- Have you tried converting the model to TRT and running inference on it (through ONNX)?
- Do you have the code written in TensorFlow?

Thank you."
"The requirement of Track1 is ""< 50ms on ARM"", and I want to know which device will be used to test and what is the available op?"
"(base) pf@pf:~/insightface/recognition$ CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100 --loss arcface --dataset emore
[09:05:04] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
gpu num: 1
prefix ./models/r100-arcface-emore/model
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 167, in train_net
    args.image_channel = config.image_shape[2]
AttributeError: 'EasyDict' object has no attribute 'image_shape'


anyone met these problem?
or dose it require the size of gpu?
"
"In IJBC_Evaluation_MS1MV2.ipynb, we set ""F2"", i.e. `use_flip_test=True`
Then `img_input_feats = img_feats[:,0:img_feats.shape[1]/2] + img_feats[:,img_feats.shape[1]/2:]`
I wanna know What this code is doing? I cannot get it.
Thank you.

"
"I tried deploying the arcface model and found some issues.
1. The models can be hybridized to speed up the inferences. Does it need gluon interface to be hybridized?
2. MTCNN uses deprecated method FeedForward. Can anyone help on using it with mx.mod.Module?
"
https://github.com/deepinsight/insightface/blob/6d5813c06c098a17b73546fe6364f34e7c038e22/gender-age/train.py#L27
"I convert the mnet.25 mxnet model (trained by [yangfly](https://github.com/yangfly) ) to caffe model, and implement a simple c++ code
please check https://github.com/Charrin/RetinaFace-Cpp"
"![20190524233757](https://user-images.githubusercontent.com/17681580/58339855-0e73e980-7e7d-11e9-8e44-ea73c5169972.png)
`ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()
[15:35:02] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
/mxnet/python/mxnet/module/base_module.py:466: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.125). Is this intended?
  optimizer_params=optimizer_params)
terminate called after throwing an instance of 'dmlc::Error'
  what():  [15:35:02] src/engine/./threaded_engine.h:359: [15:35:02] src/operator/contrib/./../linalg_impl.h:140: Check failed: e == CUBLAS_STATUS_SUCCESS (13 vs. 0) cuBLAS: CUBLAS_STATUS_EXECUTION_FAILED

Stack trace returned 10 entries:
[bt] (0) /mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5a) [0x7f36c804e59a]
[bt] (1) /mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f36c804f138]
[bt] (2) /mxnet/python/mxnet/../../lib/libmxnet.so(void linalg_gemm<mshadow::gpu, float>(mshadow::Tensor<mshadow::gpu, 2, float> const&, mshadow::Tensor<mshadow::gpu, 2, float> const&, mshadow::Tensor<mshadow::gpu, 2, float> const&, float, float, bool, bool, mshadow::Stream<mshadow::gpu>*)+0x40a) [0x7f36cac5667a]
[bt] (3) /mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::FullyConnectedOp<mshadow::gpu, float>::Forward(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xb1c) [0x7f36cadc6cdc]
[bt] (4) /mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::op::OperatorState::Forward(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x363) [0x7f36ca5969a3]
[bt] (5) /mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::exec::StatefulComputeExecutor::Run(mxnet::RunContext, bool)+0x69) [0x7f36cab3a549]
[bt] (6) /mxnet/python/mxnet/../../lib/libmxnet.so(+0x33f1170) [0x7f36cab0d170]
[bt] (7) /mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x93) [0x7f36caa88983]
[bt] (8) /mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker<(dmlc::ConcurrentQueueType)0>(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0>*, std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>)+0xcb) [0x7f36caa90cab]
[bt] (9) /mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#3}::operator()() const::{lambda(std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>&&)+0x63) [0x7f36caa90ea3]


A fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 9 entries:
[bt] (0) /mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5a) [0x7f36c804e59a]
[bt] (1) /mxnet/python/mxnet/../../lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f36c804f138]
[bt] (2) /mxnet/python/mxnet/../../lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x332) [0x7f36caa88c22]
[bt] (3) /mxnet/python/mxnet/../../lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker<(dmlc::ConcurrentQueueType)0>(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0>*, std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>)+0xcb) [0x7f36caa90cab]
[bt] (4) /mxnet/python/mxnet/../../lib/libmxnet.so(std::_Function_handler<void (std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#3}::operator()() const::{lambda(std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>&&)+0x63) [0x7f36caa90ea3]
[bt] (5) /mxnet/python/mxnet/../../lib/libmxnet.so(std::thread::_Impl<std::_Bind_simple<std::function<void (std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>)> (std::shared_ptr<mxnet::engine::ThreadPool::SimpleEvent>)> >::_M_run()+0x4a) [0x7f36caa8b07a]
[bt] (6) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f36d83ebc80]
[bt] (7) /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f36ef85d6ba]
[bt] (8) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f36ef5933dd]
`"
">C:\Users\wangting\iCloudDrive\Pycharm\DeepLearning\face\insightface-master\recognition>python -u train.py --network r100 --loss triplet --dataset emore --per-batch-size 50 --lr 0.005 --pretrained D:\insightface\models\r100-arcface-emore\model --pretrained-epoch 1

输出如下错误。loss选择arcface或softmax不会报错。
C:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
gpu num: 4
prefix ./models\r100-triplet-emore\model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=200, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='triplet', lr=0.005, lr_steps='50000,80000,110000', models_root='./models', mom=0.9, network='r100', per_batch_size=50, pretrained='D:\\insightface\\models\\r100-arcface-emore\\model', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': False, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'triplet', 'images_per_identity': 5, 'triplet_alpha': 0.3, 'triplet_bag_size': 7200, 'triplet_max_ap': 0.0, 'per_batch_size': 50, 'lr': 0.05, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'emore', 'dataset_path': 'F:\\facedataset\\faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'triplet', 'network': 'r100', 'num_workers': 1, 'batch_size': 200}
loading D:\insightface\models\r100-arcface-emore\model 1
0 1 E 3 prelu False
Network FLOPs: 24.2G
INFO:root:loading recordio F:\facedataset\faces_emore\train.rec...
header0 label [5822654. 5908396.]
id2range 85742
5822653
rand_mirror True
5 10 6
triplet_seq 427507
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [50000, 80000, 110000]
call reset()
eval 7200 images.. 0
triplet time stat [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\site-packages\mxnet\symbol\symbol.py"", line 1523, in simple_bind
    ctypes.byref(exe_handle)))
  File ""C:\Anaconda3\lib\site-packages\mxnet\base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: Error in operator _minus0: [14:39:52] c:\jenkins\workspace\mxnet-tag\mxnet\src\operator\tensor\../elemwise_op_common.h:135: Check failed: assign(&dattr, vec.at(i)) Incompatible attr in node _minus0 at 1-th input: expected [16,512], got [17,512]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 369, in train_net
    epoch_end_callback = epoch_cb )
  File ""C:\Anaconda3\lib\site-packages\mxnet\module\base_module.py"", line 499, in fit
    for_training=True, force_rebind=force_rebind)
  File ""C:\Anaconda3\lib\site-packages\mxnet\module\module.py"", line 429, in bind
    state_names=self._state_names)
  File ""C:\Anaconda3\lib\site-packages\mxnet\module\executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""C:\Anaconda3\lib\site-packages\mxnet\module\executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""C:\Anaconda3\lib\site-packages\mxnet\module\executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""C:\Anaconda3\lib\site-packages\mxnet\symbol\symbol.py"", line 1529, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (50, 3, 112, 112)
softmax_label: (50,)
Error in operator _minus0: [14:39:52] c:\jenkins\workspace\mxnet-tag\mxnet\src\operator\tensor\../elemwise_op_common.h:135: Check failed: assign(&dattr, vec.at(i)) Incompatible attr in node _minus0 at 1-th input: expected [16,512], got [17,512]"
"Thanks for your excellent work very much. It is very helpful for me. But when I run train.py which in the recognition directory on the windows system, a RuntimeError error occurs. I have read issues questioned by others. Someone seemed to have the same problem but the error is not exactly the same.  The error is below.
![image](https://user-images.githubusercontent.com/46669305/58142539-09931800-7c7a-11e9-90de-33ae8bc96445.png)
Thanks again, and I am looking forward to your request.

"
Not familiar with mxnet， anyone plan to implement retinaface in tensorflow or caffe. Hope to work together.
请问在config.py文件中有设置widerface测试数据集为easy? medium? 或者hard？ 还是我需要自己到代码中修改
how we use  insightface/deploy/test.py  with COSFACE loss?
"Thanks for your open. I want to  fine-tune the gender-age model on my own data, can you show  the steps of how to fine-tune the model? "
""
Thank you for such wonderful work. I’m reading the code now. But didn't find the Dense Regression Loss. Do i miss something? Please help me.
"Hi, many thanks for sharing remarkable work for retinaface!
I notice that five-point face landmark has been added as supplementary for original annotation in widerface. But it seems that both false positive and false negative annotations exist in original widerface dataset. Have you fixed these wrong annotations in your public version?"
"Hi @nttstar 

Could you please elaborate on the values of `scales` when testing RetinaFace?

For example if we have (640,640) images as input, should we make `scales = [640,640]` or should we leave it at `scales = [1024, 1980]` ?

Is there any documentation on how `scales` works? I read the paper but couldn't pinpoint how to exactly use `scales` in the test code.

Thanks as always for your great support."
"@nttstar
Hi, thank you for wonderful work.
In your ArcFace paper (v3), I found that you got the best performance on Trillion-Pairs by MS1MV2_asian method.  Could I ask you a few questions?
1) Did you evaluate the method on IJB-C? If did, how about is the performance?
2) Where can I get training .rec or how can I merger the two .rec of   MS1MV2 and asian if I want to reproduce the result from scratch?

Thank you in advance."
"Hi,

I ran the test.py with 2Mp video on the i9 & 2080Ti Gpu and got only 16 fps.

On the other hand Gpu only utilised %10 .

How we can push the full limit for RetinaFace on gpu ?

any idea ? 


best"
"Would you please consider supporting Windows for RetinaFace?

The `bbox` and other cython related codes are written to make use of Linux's GCC to compile and without Linux we can't test the code on Windows platforms.

Thanks for the great support.

"
"Hi, @nttstar @jiankangdeng using your [tutorial](https://github.com/deepinsight/insightface/wiki/Tutorial:-Deploy-Face-Recognition-Model-via-TVM#inferencing-with-cpp-via-tvm-runtime) I am able inference tvm runtime for llvm I am facing issue while Inferencing with Cuda and OpenCL.

            DLTensor* input;
            constexpr int dtype_code = kDLFloat;
            constexpr int dtype_bits = 32;
            constexpr int dtype_lanes = 1;
            constexpr int device_type = kDLGPU;
            constexpr int device_id = 0;
            constexpr int in_ndim = 4;
            const int64_t in_shape[in_ndim] = {1, 3, 112, 112};
            TVMArrayAlloc(in_shape, in_ndim, dtype_code, dtype_bits, dtype_lanes, device_type, device_id, &input);//

here when I try to access or do memset(input->data, 0, img_size); i am getting segmentation fault.

"
"hi, first very thank you for this great work!
I have some questiones about the alignment process in the data preprocess:
(1)what is the matrix 'src 'in the preprocess function in your code face_preprocess.py? How to get this matrix?:
  if landmark is not None:
    assert len(image_size)==2
    src = np.array([
      [30.2946, 51.6963],
      [65.5318, 51.5014],
      [48.0252, 71.7366],
      [33.5493, 92.3655],
      [62.7299, 92.2041] ], dtype=np.float32 )
    if image_size[1]==112:
      src[:,0] += 8.0
    dst = landmark.astype(np.float32)
(2)did you try some other alignment algorithm? if the landmark is more than 5, I say for example 68 or more, how to align the images?

thanks very much!"
"Hi,

I read in the paper that we can deetct 900 faces from the mentioned selfie ? Wha is the configuration for the test.pf for this ? I can get only 515 faces ?"
"I just tried the latest retinaface on video on my GTX 1060

nvidia-smi -i 0 --query-gpu=index,timestamp,utilization.gpu,power.draw,temperature.gpu --format=csv -l 1

0, 2019/05/11 18:23:01.597, 46 %, 48.72 W, 73
0, 2019/05/11 18:23:02.599, 41 %, 48.10 W, 73
0, 2019/05/11 18:23:03.601, 52 %, 54.85 W, 73
0, 2019/05/11 18:23:04.603, 40 %, 45.49 W, 73
0, 2019/05/11 18:23:05.606, 51 %, 49.02 W, 73
0, 2019/05/11 18:23:06.611, 39 %, 33.45 W, 73
0, 2019/05/11 18:23:07.614, 52 %, 68.18 W, 73
0, 2019/05/11 18:23:08.620, 39 %, 32.07 W, 73
0, 2019/05/11 18:23:09.623, 39 %, 52.84 W, 73
0, 2019/05/11 18:23:10.634, 45 %, 53.29 W, 73
0, 2019/05/11 18:23:11.636, 41 %, 42.51 W, 73
0, 2019/05/11 18:23:12.638, 40 %, 59.88 W, 73
0, 2019/05/11 18:23:13.640, 50 %, 34.41 W, 73
0, 2019/05/11 18:23:14.642, 40 %, 57.18 W, 73
0, 2019/05/11 18:23:15.645, 46 %, 35.81 W, 73
0, 2019/05/11 18:23:16.649, 41 %, 84.70 W, 73
0, 2019/05/11 18:23:17.658, 40 %, 34.26 W, 73
0, 2019/05/11 18:23:18.660, 41 %, 52.11 W, 73
0, 2019/05/11 18:23:19.663, 38 %, 33.10 W, 73
0, 2019/05/11 18:23:20.665, 48 %, 44.43 W, 73

nvidia-smi
|    0     12286      C   python3                                      623MiB |

It seems to be taking less memory but taking lot of cuda cores.

Thanks."
Hello. Is there any reasons that insight-face uses RCNN instead of YOLO model? I thought the latter is much more powerful on both accuracy and speed.
"is there any c++ implementation of the RetinaFace ?

"
"Hi,

How can I test RetinaFace with cpu ? 

Best"
"RetinaFace 牛逼！:two_hearts:  InsightFace 赛高！:two_hearts:

开放 mobilenet0.25 版 RetinaFace 同人版模型
更完整的模型、日志和WiderFace测试截图 [**百度云**](https://pan.baidu.com/s/1P1ypO7VYUbNAezdvLm2m9w) 提取码:nzof  [**GoogleDrive**](https://drive.google.com/drive/folders/1OTXuAUdkLVaf78iz63D1uqGLZi4LbPeL?usp=sharing)

![image](https://user-images.githubusercontent.com/15797180/57526606-67f9f580-7360-11e9-836a-6df7cf5483de.png)

模型说明，使用的是Gluon Model Zoo 的标准版 MobileNet0.25 预训练模型。（没有 fast downsampling，模型大小 1.68Mb）
Batch-size 32x2（两块1080Ti），其他参数都是 RetinaFace 默认的。

WIDER Face Hard **单尺度测试：0.791**
![val-hard-quick](https://user-images.githubusercontent.com/15797180/57523356-55c78980-7357-11e9-9bbd-2dbbdf073ea6.jpg)

WIDER Face Hard **多尺度测试：0.825**
![val-hard-slow](https://user-images.githubusercontent.com/15797180/57523720-4d238300-7358-11e9-9741-0ffca752e7cf.jpg)

欢迎调参大佬，公开更给力的结果。:beer:"
"1. IJB-C原始数据中，1:1 verification中match.txt是template-based，代码中应该是对每个template的features进行了np.mean，请问为什么使用的vgg2项目中的list而不是使用的IJB数据集中自带的？
2. vgg2项目中生成filelist的原理是什么呢？

不胜感激"
"Hi, Thanks for your great work.
But could you tell me what is the annotation format in the labels.txt of RetinaFace?
I noticed that after each lmk point, there is a -1.0/0.0/1.0 what does this value mean? And after each face, there is a float value like 0.82 in the first line of train/labels.txt, what does this value mean?
@nttstar 
Best, Bin"
"Hi, i'm trying training using faces glint, however I encountered this error 

Traceback (most recent call last):
  File ""src/data/dataset_merge.py"", line 306, in <module>
    main(args)
  File ""src/data/dataset_merge.py"", line 133, in main
    s = imgrec.read_idx(0)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/recordio.py"", line 265, in read_idx
    return self.read()
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/recordio.py"", line 163, in read
    ctypes.byref(size)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 251, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [14:53:28] src/recordio.cc:65: Check failed: header[0] == RecordIOWriter::kMagic Invalid RecordIO File

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x39022a) [0x7ff053b9922a]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x3209d33) [0x7ff056a12d33]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXRecordIOReaderReadRecord+0x2a) [0x7ff0562851ea]
[bt] (3) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7ff0aefcfdae]
[bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7ff0aefcf71f]
[bt] (5) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x2a4) [0x7ff0af1e2b04]
[bt] (6) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(+0x10505) [0x7ff0af1e2505]
[bt] (7) python(PyEval_EvalFrameEx+0x54a0) [0x563c040db420]
[bt] (8) python(PyEval_EvalCodeEx+0x6da) [0x563c040d3d0a]
[bt] (9) python(PyEval_EvalFrameEx+0x5cb8) [0x563c040dbc38]"
"src/train.py里面在[0,pi]之间使用的是arcface，在[0,-pi]使用的是cosface
recognition/train.py里面[-pi,pi]使用的都是arcface
请问这样差异的初衷是什么，哪一种效果更好呢"
"Hi @nttstar @jiankangdeng, very great work on 1:1 verification! and thanks for sharing your solution on face recognition model. 
If you don't mind, is it possible to share with us which face detector did you use? Is it based on MTCNN or RetinaFace (resnet based, mobilenet based) or others? thank you very much!

> 2019.04.04: Arcface achieved state-of-the-art performance (5/109) on the NIST Face Recognition Vendor Test (FRVT) (1:1 verification) report (name: Imperial-000). Our solution is based on [MS1MV2+DeepGlintAsian, ResNet100, ArcFace loss]."
"Dear all people,

I'm trying to work on a research about how to identify people well saying that if he is/is not in the batabase given the images as database and videos as anchors(image to video/image tests).

As far as I know now, most of the tests are all verification in pairs(LFW, YTF,...). MegaFace has identification but it's image to image identification. IQIYI seems to be video to video identification. The dataset that I need is still to video dataset actually.

Does anyone know if there's any large scale(maybeto 100k or 1M) still to video sets that I can use for this research?

Thanks in advance"
"I was trying to run RetinaFace using ResNet50 as backbone. I am using CUDA 10.0 so I installed mxnet-cu100 using `pip install mxnet-cu100`. The code is working the only probelm is that it seems the GPU is not being used. I tried to measure the time on GPU and CPU and it is the same. Did I miss something?
I am using Python3.6 and CUDA 10.0.
"
"when i used ""retinaface"" to train the widerface dataset,the problem occurred,how to solve it?"
"I want to employ the arcface loss for a three-class classification task, but I don't know how to decide the values of the scale s and margin m. I want to know if there are any criteria to decide the values? And also, does anyone know how to draw the theta distribution like in Figure 4 in the Arcface paper. Thanks."
"can you give me a link of  the raw refined-MS1M dataset? my input size is larger than 112*112, and I want to align face using other method  and output a larger size of aligned face. Really appreciate.

"
"I want to train asian dataset and follow #256  . When I run  python face2rec2.py /home/xxx/insightface/src/data/glint_data, I get the error.
multiprocessing not available, fall back to single threaded encoding
OpenCV Error: Assertion failed (src.cols > 0 && src.rows > 0) in warpAffine, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 5584
Traceback (most recent call last):
  File ""face2rec2.py"", line 253, in <module>
    image_encode(args, i, item, q_out)
  File ""face2rec2.py"", line 99, in image_encode
    img = face_preprocess.preprocess(img, bbox = item.bbox, landmark=item.landmark, image_size='%d,%d'%(args.image_h, args.image_w))
  File ""../common/face_preprocess.py"", line 106, in preprocess
    warped = cv2.warpAffine(img,M,(image_size[1],image_size[0]), borderValue = 0.0)
cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:5584: error: (-215) src.cols > 0 && src.rows > 0 in function warpAffine"
"如96X112,96X96"
"Hi，nttstar：
        Is it possible to use the trained model to finetune on the other dataset？The gender age model is trained on fmobilenet？And what is the idx should be?name gender age?or name age gender?
Thanks"
"Hello!
I saw the megaface small data size leaderboard top1 is called the ''iBug (Reported by Author)'', which reached 80.277% on facescrub (uncleaned).
However, no one reference is given there. I'm wondering what algorithm and dataset do you use, and can you please tell me which paper does it refer to?
thank you!
![Screenshot from 2019-04-27 12-08-41](https://user-images.githubusercontent.com/38978109/56844596-5d157e80-68e5-11e9-8f9d-a4de791cf601.png)
"
"As I set softmax(logits) as pred and sent it to AccMetric,the accuracy of faces_emore that is used as train dataset was up to 0.998.But I think it should been in the range of 0.6 to 0.9.Is there any problem with this phenomenon?"
"Hi 
I tried to execute the Youtube Face Dataset as per your YTF file ""https://github.com/deepinsight/insightface/blob/master/src/eval/ytf.py"" on the pre-trained model of LResNet100E-IR and I am getting the accuracy of 91%.
It is a request, if you can upload your cropped and aligned Youtube Face Dataset in Dataset Zoo, it would be a great help.

Thanks"
"https://github.com/deepinsight/insightface/blob/1a8f0fc6b6aad73594c5e8f45704192c4185e168/recognition/parall_module_local_v1.py#L148
why not Initialize arg_params and aux_params here? esym:[fc1,label].asym[fc7],only initialize parameter until fc1.

_Originally posted by @maryhh in https://github.com/deepinsight/insightface/issues/574#issuecomment-485400205_"
"I want to deploy MTCNN based on TVM, Can anyone give me some advices? thx a lot~"
"Hello guys,

The CPU I used is Xeon(R) CPU E5-2650 v3. 

LResNet100E-IR run on mxnet cpu takes about 300ms to get one face feature.

LResNet100E-IR compiled with tvm target llvm -mcpu=haswell takes 100ms to get one face feature when I run in a tvmai/demo-cpu docker container. The container has a tvm 0.4 version. The result is not so impressive as yours in tvm benchmark which is 45ms.

Then I git clone the latest tvm 0.6 src to the tvmai/demo-cpu container and build it. The new built tvm get following warnings when compile LResNet100E-IR with the same code as before.

Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 3, 112, 112, 'float32'), (64, 3, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 64, 112, 112, 'float32'), (64, 64, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 64, 112, 112, 'float32'), (64, 64, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 64, 112, 112, 'float32'), (64, 64, 1, 1, 'float32'), (2, 2), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 64, 56, 56, 'float32'), (128, 64, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 128, 56, 56, 'float32'), (128, 128, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 128, 28, 28, 'float32'), (256, 128, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 256, 28, 28, 'float32'), (256, 256, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 256, 14, 14, 'float32'), (512, 256, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=haswell, workload=('conv2d', (1, 512, 14, 14, 'float32'), (512, 512, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=llvm -mcpu=haswell, workload=('dense', (1, 25088, 'float32'), (512, 25088, 'float32'), (512, 'float32')). A fallback configuration is used, which may bring great performance regression.

Although the model is compiled, it costs 190ms, very great performance regression.

So is there any trick when you build tvm to get the 45ms performance? How do you set the config.cmake file? Thanks."
"您们好，我可以请教两大问题吗？
1. 目前的环境是 Centos7+python2.7+mxnet+cpu。于是想将代码中有关的GPU改成CPU，改着改着，错误百出。所以请问作者有何解决方案。
2.目前想用mtcnn（人脸检测）+insightface（人脸识别），来实现人脸识别功能。。但是无法从根目录《readme》查找的执行程序的入口。。请问最新版本中，哪个文件是生成底图的特征值；哪个文件是基于欧氏距离比对图形的。

Lastly，I am  in shanghai too。 Thanks!"
Which face detector did you use in NIST's FRVT 1:1?
"For some reason, I may need to finetune models on emore datasets with 'fc7' layers weights, since I don't want to retrain the last 'fc7' layers. so is there any chance we could still get the not deleted version of models? 

Thanks a lot for your great works again!
"
"**Challenge Information:**

[HomePage](https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/), please read carefully about the training and testing rules.


**How To Start:**

Please check [https://github.com/deepinsight/insightface/tree/master/iccv19-challenge](https://github.com/deepinsight/insightface/tree/master/iccv19-challenge) for detail.

Any question can be left here.
"
Is there an example of training spherenet?
"Hi,

I am testing a c++  library from [mtcn_ligt](https://github.com/AlphaQi/MTCNN-light/blob/master/src/mtcnn.cpp)..

it's capturing the face 112x122 and sending to insightface for the processing.

some of the faces didn't recognize by the insight face python mtcnn even its detected by c++ mtcnn and send to processing.

What is the differences? any configuration difference? "
"Hi all,

I'm trying to finetune the pretrained mobilefacenet model with triplet loss on my private dataset, which is quite small (250 images from 25 identities).

But when I finetune it, it seems that the per_batch_size and triplet_bag_size is causing the training to fail, no matter how small I set those two parameters. I'm wondering what does the triplet_bag_size parameter stand for and is it possible to do finetune on such small datasets, if not, what is the smallest dataset size that is acceptable for finetuning?

Many thanks!"
"This is a very new dataset released by IBM recently. Compared to those facial image dataset released before, DiF(Diversity in Faces Dataset) provides a more balanced distribution and broader coverage of facial images.

Anyone want to give it a try? 
We can show our results in this issue. "
"Hi，假设我有A、B、C三个id的数据，每个id3个相片：
A:A1.jpg,A2.jpg,A3.jpg
B:B1.jpg,B2.jpg,B3.jpg
C:C1.jpg,C2.jpg,C3.jpg
我描述一下生成这个文件的规则，看看我的理解是否有误：
1、相同id之间进行组合，每行三个参数：ID+两张图片的路径
2、不同id之间的组合，每行四个参数：ID1+ID1图片+ID2+ID2图片
根据这样的逻辑，我生成的数据是这样的：
A A1.jpg A2.jpg
A A1.jpg A3.jpg
A A2.jpg A3.jpg
B B1.jpg B2.jpg
B B1.jpg B3.jpg
B B2.jpg B3.jpg
C C1.jpg C2.jpg
C C1.jpg C3.jpg
C C2.jpg C2.jpg
A A1.jpg B B1.jpg
A A1.jpg B B2.jpg
A A1.jpg B B3.jpg
A A2.jpg B B1.jpg
A A2.jpg B B2.jpg
A A2.jpg B B3.jpg
A A3.jpg B B1.jpg
A A3.jpg B B2.jpg
A A3.jpg B B3.jpg
A A1.jpg C C1.jpg
......

"
"Hello,
How can I run the age/gender detection test.py file without using GPU?
I get the following error regarding GPU value : 

`RuntimeError: simple_bind error. Arguments:
data: (1, 3, 112, 112)
[14:32:59] C:\Jenkins\workspace\mxnet-tag\mxnet\src\storage\storage.cc:143: Compile with USE_CUDA=1 to enable GPU usage`"
"hi insightface team have you compare both alignment method ?
which one is better , im training the insightface alignment at this time"
"Hi @nttstar, I read your paper on Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment from [here](
https://github.com/deepinsight/insightface/tree/master/alignment) and would like to run the experiment. 
However, I do not have access to baidu pan to download the training and testing dataset, just wondering could you provide the download link using google drive or dropbox? Thank you.
"
"i use the model resnet_v1_50 for face representation .when i get the representation of two different people, embedding is almost same ,then i calculate the similarity, i find result is almost 1. it is really wired!"
"@nttstar .I know how to extract image from train.rec in Ms1M-ArcFace.And I want to divide different people's images into different folder.Obviously,it's too slow classified by people.Is there any data structure in train.rec to record image's class？Thank you for any reply."
"Hi~

Thank you for the amazing work.

The facescrub_noise.txt lists 605 files.
We found that there are 24 overlapped files between the noise list and the feature list (facescrub_uncropped_features_list.json for run_experiment.py) for megaface validation.

We are curious about the improvement introduced by removing the megaface noise since the improvement is significant (>15%) as stated in the paper, while the improvement is limited in our case (<1%).
Is the number of the overlapped files 24 or up to 605?

Thank you.
"
"Dear all!
I want to fine-tune a face recognition model for Asian, and I plan to use author's pre-trained model(ResNet50-arcface-msra1m) as base model and to fine-tune it  on glint-asian dataset. My confusion is that how to evaluate this asian model? use LFW/CFP/AgeDB? I think if my target is to recognize Asian people, just using LFW/CFP/AgeDB as validation dataset is not fair. Am I correct? or it is not necessary to use a validation set which just contains Asian people? or I 
can split glint-asian dataset to new train_set and val_set? which one is best? Any other idea? Appreciate any reply!"
"author said he will update the new face detector and video demo code this month, but he has not done it yet. I tried to implement the video demo code myself, but found the result is far worse than the author's .
can you pls update the code?"
""
"Hi 
While executing the IJB evaluaion file, I am getting shape mis-match error while executing ""Step 4"" (Pic atached). I am just running the given file as  it is, on pretrained model.   
https://github.com/deepinsight/insightface/blob/master/Evaluation/IJB/IJBB_Evaluation_MS1MV2.ipynb

Please guide me, Thanks

![Screenshot from 2019-03-21 14-36-33](https://user-images.githubusercontent.com/16646791/54736775-a9cfb080-4be7-11e9-8454-cab57edd325e.png)
"
"When the number of ID is large say 1000k, it became very difficult to converge using weight_decay 0.0005 for all the layers. Do I need to reduce the weight_decay of last fc layer using fc7_wd_mult? Will it lead to overfitting? Thanks."
"When i follow your steps, i meet the problem,how should i do?
 CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100 --loss arcface --dataset emore
gpu num: 1
prefix ./models/r100-arcface-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=128, ckpt=3, ctx_num=1, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_act': 'prelu', 'emb_size': 512, 'data_rand_mirror': True, 'num_layers': 100, 'loss_name': 'margin_softmax', 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'net_input': 1, 'image_shape': [112, 112, 3], 'fc7_lr_mult': 1.0, 'ckpt_embedding': True, 'net_unit': 3, 'net_output': 'E', 'num_workers': 1, 'data_images_filter': 0, 'dataset': 'emore', 'num_classes': 85742, 'fc7_no_bias': False, 'loss': 'arcface', 'data_color': 0, 'loss_s': 64.0, 'dataset_path': '../datasets/face', 'data_cutoff': False, 'net_se': 0, 'net_multiplier': 1.0, 'fc7_wd_mult': 1.0, 'network': 'r100', 'net_name': 'fresnet', 'workspace': 256, 'max_steps': 0, 'bn_mom': 0.9}
0 1 E 3 prelu
INFO:root:loading recordio ../datasets/face/train.rec...
header0 label [ 3804847.  3890011.]
id2range 85164
3804846
rand_mirror True
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()
/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/base_module.py:505: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
  optimizer_params=optimizer_params)
[20:14:28] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[20:14:28] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
Traceback (most recent call last):
  File ""train.py"", line 366, in <module>
    main()
  File ""train.py"", line 363, in main
    train_net(args)
  File ""train.py"", line 358, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 539, in fit
    self.update_metric(eval_metric, data_batch.label)
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/module.py"", line 773, in update_metric
    self._exec_group.update_metric(eval_metric, labels, pre_sliced)
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 639, in update_metric
    eval_metric.update_dict(labels_, preds)
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/metric.py"", line 304, in update_dict
    metric.update_dict(labels, preds)
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/metric.py"", line 132, in update_dict
    self.update(label, pred)
  File ""/home/wr506/insightface/recognition/metric.py"", line 20, in update
    pred_label = pred_label.asnumpy().astype('int32').flatten()
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 1980, in asnumpy
    ctypes.c_size_t(data.size)))
  File ""/home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [20:14:28] src/operator/nn/./cudnn/cudnn_convolution-inl.h:909: Failed to find any forward convolution algorithm.  with workspace size of 268435456 bytes, please consider reducing batch/model size or increasing the workspace size

Stack trace returned 10 entries:
[bt] (0) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x3f042a) [0x7fbe41f8742a]
[bt] (1) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x3f0a41) [0x7fbe41f87a41]
[bt] (2) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x37096b4) [0x7fbe452a06b4]
[bt] (3) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x370da8d) [0x7fbe452a4a8d]
[bt] (4) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x36f975c) [0x7fbe4529075c]
[bt] (5) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x36f9beb) [0x7fbe45290beb]
[bt] (6) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x36faf53) [0x7fbe45291f53]
[bt] (7) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x36fbfc3) [0x7fbe45292fc3]
[bt] (8) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x3701ff1) [0x7fbe45298ff1]
[bt] (9) /home/wr506/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2cbba39) [0x7fbe44852a39]
"
"I follow your steps, but i meet the problem:
 src/storage/./pooled_storage_manager.h:143: cudaMalloc failed: out of memory.
When i set the  default.per_batch_size = 1, it also meet the problem. How should i do?
"
"Hi~

There are 100k images in facescrub.
However, the number of output images for aligned facescrub is only around 3k.
Is this correct for applying align_facescrub.py?

Thank you."
"Hi~

Thank you for the great work!

We are confused about the usage of align_facescrub.py.
Should we apply align_facescrub.py to the cropped facescrub dataset?
Or Is the align_facescrub.py only utilized for the uncropped one?

Thank you."
"@nttstar  非常感谢你的工作，让我获益匪浅。我把自己的数据crop到[96,96]，除开网络方面，您能指导我一下还有那些地方需要修改吗？非常感谢的你回复。"
"I am un-able to find  ""run_experiment.py"" for Megaface testing
please guide me "
"Hello,I have two servers:

Server 1: System is Ubuntu16.04,  hard disk israid5, use command ""hdparm-t /dev/sdb""  test hard disk reading speed is 900M/s, GPU is  NVIDIA GTX 1080 8g, training Insightface speed is 80samples/s.

Server 2: System is Ubuntu16.04, system hard disk is raid5, use command ""hdparm-t /dev/sda"" test hard disk reading speed is 350M/s; Data hard disk SSD is raid0, using command ""hdparm-t /dev/sdb"" test hard disk reading speed is 1600M/s; GPU is NVIDIA V100 32G, training Insightface speed is also about 80 samples/s (training data on raid0 SSD disk).

Server 2, whether GPU or data hard disk, performance is much better than server 1, why Insightface training speed is not improved?  

Thanks for your attentions.
"
"请教各位一个问题：
训练代码中直接加载 bin 数据进行训练，没有人脸照片的预处理逻辑。请问在实际模型调用阶段，应该对图片先做什么样的处理，以保持和训练数据一致，获得正确的 embedding 结果？
谢谢。"
"I want transfer learning from your pretrained MobileFaceNet,ArcFace@ms1m-refine-v1 model. As I understood the only way out is to get the gluon architecture of MobileFaceNets. How can i get that?"
"Hi I am going to be testing insight face at an upcoming event and I was wondering what would be the best  strategy for implementing insightface. So far I have thought of the following:

1. Run mtcnn detector on RTSP stream along with tracking and send 5 cropped faces per tracker id to a queue.
2. Run the client app to fetch the data from queue and send the cropped face to the insightface inference server and calculate the distance and update the database.

Please share your thoughts regarding this architecture and suggest improvements to implement insightface in the best possible manner.

Thanks.

"
"Hi, I would like to ask two questions:

1. Do you have the cleaned, scaled dataset of CASIA and YTF?

I saw a paper saying that using CASIA dataset with arcface could get better performance. However, the original YTF dataset and CASIA have different scale compared to the dataset that was used for training(emore_v2) and testing(LFW). Do you also have the scaled version of them?


2. May I request the corresponding names of those celebrities in MS1M-ArcFace-v2?
Since I know there are some overlaps between ms1m and LFW, I would like to clean that a bit.
I saw there was one issue asking for the corresponding id before but I couldn't find it...


Thanks in advance

Joe"
Could i train you MobileFaceNets to recognize facial expression?
"When I use align_facescrub.py to aligh the facescrub dataset of the megaface, I can't find the file facescrub_uncropped_features_list.json. Where can I download one?"
"Issue statement:
Traing SDU: train.py --network sdu, the loss value is NAN after about 300 batch.

Test environment:
Ubuntu 16.04, Python 2.7, Mxnet-cu80
CPU: I7
GPU: 1080Ti

Reproduce steps:
1. Download 2d face training validation dataset according to README.md.
2. copy sample_config.py as config.py
3. Run: train.py --network sdu

Log:
(venv) xxx@xxx-desktop:~/Workspace/insightface/alignment$ python train.py --network sdu
Call with Namespace(batch_size=20, ckpt=1, ctx_num=1, dataset='i2d', exf=1, frequent=20, lr=0.00025, lr_step='16000,24000,30000', network='sdu', norm=0, optimizer='nadam', per_batch_size=20, prefix='model/A', pretrained='', verbose=200, wd=0.0) {'net_binarize': False, 'net_d
cn': 3, 'gaussian': 0, 'net_block': 'cab', 'dataset': 'i2d', 'record_img_size': 384, 'landmark_type': '2d', 'net_coherent': False, 'base_scale': 256, 'val_targets': ['ibug', 'cofw_testset', '300W'], 'network': 'sdu', 'net_stacks': 2, 'losstype': 'heatmap', 'input_img_size': 
128, 'net_sta': 1, 'multiplier': 1.0, 'num_classes': 68, 'output_label_size': 64, 'label_xfirst': False, 'per_batch_size': 20, 'net_n': 3, 'dataset_path': './data_2d'}
INFO:root:loading recordio ./data_2d/train.rec...
('train size', 15683)
('train size after reset', 15683)
binarize False
use_coherent False
use_STA 1
use_N 3
use_DCN 3
per_batch_size 20
128 64 2
initializing upsampling_weight sta0_upsampling_w3_h3_weight
initializing upsampling_weight sta1_upsampling_w2_h3_weight
initializing stack1_ll_conv_offset_weight
initializing heatmap_offset_bias
initializing upsampling_weight sta0_upsampling_w2_h2_weight
initializing heatmap0_offset_weight
initializing stack0_out3_offset_bias
initializing stack0_ll_conv_offset_weight
initializing heatmap0_offset_bias
initializing upsampling_weight sta1_upsampling_w2_h2_weight
initializing upsampling_weight sta0_upsampling_w2_h3_weight
initializing heatmap_offset_weight
initializing upsampling_weight sta1_upsampling_w3_h3_weight
initializing stack0_out3_offset_weight
initializing stack0_ll_conv_offset_bias
initializing stack1_ll_conv_offset_bias
lr-steps [16000, 24000, 30000]
/home/elvin/Workspace/insightface/venv/local/lib/python2.7/site-packages/mxnet/module/base_module.py:505: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.05). Is this intended?
  optimizer_params=optimizer_params)
[12:59:53] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[12:59:58] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:Epoch[0] Batch [0-20] Speed: 18.89 samples/sec        lossvalue=0.001960
INFO:root:Epoch[0] Batch [20-40]        Speed: 19.08 samples/sec        lossvalue=0.001518
INFO:root:Epoch[0] Batch [40-60]        Speed: 19.14 samples/sec        lossvalue=0.001268
INFO:root:Epoch[0] Batch [60-80]        Speed: 19.22 samples/sec        lossvalue=0.001184
INFO:root:Epoch[0] Batch [80-100]       Speed: 19.01 samples/sec        lossvalue=0.001074
INFO:root:Epoch[0] Batch [100-120]      Speed: 18.99 samples/sec        lossvalue=0.001023
INFO:root:Epoch[0] Batch [120-140]      Speed: 18.84 samples/sec        lossvalue=0.000993
INFO:root:Epoch[0] Batch [140-160]      Speed: 18.46 samples/sec        lossvalue=0.000965
INFO:root:Epoch[0] Batch [160-180]      Speed: 18.63 samples/sec        lossvalue=0.000941
INFO:root:loading recordio ./data_2d/ibug.rec...
('train size', 135)
[200][ibug]NME: 0.196805
INFO:root:loading recordio ./data_2d/cofw_testset.rec...
('train size', 507)
[200][cofw_testset]NME: 0.163290
INFO:root:loading recordio ./data_2d/300W.rec...
('train size', 600)
[200][300W]NME: 0.171441
saving 1
INFO:root:Saved checkpoint to ""model/A-0001.params""
INFO:root:Epoch[0] Batch [180-200]      Speed: 7.98 samples/sec lossvalue=0.000900
INFO:root:Epoch[0] Batch [200-220]      Speed: 18.55 samples/sec        lossvalue=0.000865
INFO:root:Epoch[0] Batch [220-240]      Speed: 19.11 samples/sec        lossvalue=0.000879
INFO:root:Epoch[0] Batch [240-260]      Speed: 19.05 samples/sec        lossvalue=0.000852
INFO:root:Epoch[0] Batch [260-280]      Speed: 18.93 samples/sec        lossvalue=0.000839
INFO:root:Epoch[0] Batch [280-300]      Speed: 18.91 samples/sec        lossvalue=0.000843
INFO:root:Epoch[0] Batch [300-320]      Speed: 18.94 samples/sec        lossvalue=0.000861
**INFO:root:Epoch[0] Batch [320-340]      Speed: 19.85 samples/sec        lossvalue=nan
INFO:root:Epoch[0] Batch [340-360]      Speed: 19.35 samples/sec        lossvalue=nan
INFO:root:Epoch[0] Batch [360-380]      Speed: 19.67 samples/sec        lossvalue=nan**
INFO:root:loading recordio ./data_2d/ibug.rec...
('train size', 135)
[400][ibug]NME: 0.569571
"
"Hi，
My gpu is 1050ti , I want to train this net but the dataset or testset is too large .Could you tell me how to modify the code or which dataset/testset I can run in it？"
"I would love to compare the accuracy of the MTCNN against Yolov3-Tiny.
If you guys can provide the training scripts. I would publish the results here.
In this thread,

Thanks."
What is the threshold to use since it is calculating 128 points ?
@nttstar Can you provide the pretrained model for sdu-net ?
"hi @nttstar thank you for the great work 
i want to train the 2D alignment 
but the baidu cloud is to slow for me
is that possible upload to dropbox or google drive? 

Thank for your assistance"
"It's a feedback rather than an issue report. I did a simple test with 'train.py' and 'train_parallel.py' and observed that 'train_parallel.py' got a relatively lower validation accuracy compared to 'train.py'

Data:
`~0.9M identities, >20M images`

env1: 'train.py':
`V100 32GB x 4, batch: 128, ~310 samples/second`

env2: 'train_parallel'
`2080Ti 11GB x4, batch:50 (got OOM if set to 64) ~450 samples/second`

So with same samples processed(about more than one epoch, lr=0.1), the validation results are:

env1:
```
infer time 13.888804
[lfw][2000]XNorm: 22.394393
[lfw][2000]Accuracy-Flip: 0.98817+-0.00398
testing verification..
(14000, 512)
infer time 16.613949
[cfp_fp][2000]XNorm: 18.647367
[cfp_fp][2000]Accuracy-Flip: 0.85086+-0.02107
testing verification..
(14000, 512)
infer time 16.212121
[cfp_ff][2000]XNorm: 21.757638
[cfp_ff][2000]Accuracy-Flip: 0.98071+-0.00614
testing verification..
(10000, 512)
infer time 11.568479
[vgg2_fp][2000]XNorm: 19.112889
[vgg2_fp][2000]Accuracy-Flip: 0.87240+-0.01637
testing verification..
(12000, 512)
infer time 12.080508
[agedb_30][2000]XNorm: 21.364253
[agedb_30][2000]Accuracy-Flip: 0.90033+-0.01751
```


env2:
```
infer time 8.597218
[lfw][190000]XNorm: 22.441565
[lfw][190000]Accuracy-Flip: 0.98000+-0.00711
testing verification..
(14000, 512)
infer time 10.06084
[cfp_fp][190000]XNorm: 19.034261
[cfp_fp][190000]Accuracy-Flip: 0.81586+-0.01070
testing verification..
(14000, 512)
infer time 9.790307
[cfp_ff][190000]XNorm: 21.557386
[cfp_ff][190000]Accuracy-Flip: 0.96214+-0.00724
testing verification..
(10000, 512)
infer time 6.909437
[vgg2_fp][190000]XNorm: 19.837395
[vgg2_fp][190000]Accuracy-Flip: 0.82740+-0.01302
testing verification..
(12000, 512)
infer time 8.352047
[agedb_30][190000]XNorm: 21.250625
[agedb_30][190000]Accuracy-Flip: 0.84017+-0.01907
[190000]Accuracy-Highest: 0.85383
```


"
"Hi,

First of all thanks for taking time and helping us out.

I have converted the models to TVM and now checking the output of the encodings. 

Is it normal to have completely different encodings from the normal mxnet output ?
Is there an accuracy drop when we switch to TVM ?"
"Hello,@nttstar I trained the model successfully,but when I tried to load the pretrained model ,it threw a wrong message.My batch_size is 16.Here is the message:


Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    mod.bind(for_training=False, data_shapes=[('data', (1, 3, 112, 112))])
  File ""/home/cbd109/applications/anaconda3/envs/pyzhc27/lib/python2.7/site-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/home/cbd109/applications/anaconda3/envs/pyzhc27/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/home/cbd109/applications/anaconda3/envs/pyzhc27/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""/home/cbd109/applications/anaconda3/envs/pyzhc27/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/home/cbd109/applications/anaconda3/envs/pyzhc27/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 1528, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 3, 112, 112)
[13:36:17] src/storage/./pooled_storage_manager.h:143: cudaMalloc failed: out of memory
"
""
"Dear @nttstar, @yingfeng, @yuzhichang, @ppwwyyxx and @zhangxu19830126,
Thank you for your fantastic repository. I have attempted to convert your pre-trained models into PyTorch via [MMdnn](https://github.com/Microsoft/MMdnn); however, I couldn't do that (some error occurred during converting the model weights into the MMdnn IR representation). I think the issue related to the dimension of your models. Have you ever attempted to convert the pre-trained models into PyTorch? Would you please kindly help me to address this issue?
 "
"huzhipeng@ubuntu:~/imageAI/insightface/alignment$ python train.py --network sdu
gpu num: 2
Call with Namespace(batch_size=40, ckpt=1, ctx_num=2, dataset='i2d', exf=1, frequent=20, lr=0.00025, lr_step='16000,24000,30000', network='sdu', norm=0, optimizer='nadam', per_batch_size=20, prefix='model/A', pretrained='', verbose=200, wd=0.0) {'net_binarize': False, 'net_dcn': 3, 'gaussian': 0, 'net_block': 'cab', 'dataset': 'i2d', 'record_img_size': 384, 'landmark_type': '2d', 'net_coherent': False, 'base_scale': 256, 'val_targets': ['ibug', 'cofw_testset', '300W'], 'network': 'sdu', 'net_stacks': 2, 'losstype': 'heatmap', 'input_img_size': 128, 'net_sta': 1, 'multiplier': 1.0, 'num_classes': 68, 'output_label_size': 64, 'label_xfirst': False, 'per_batch_size': 20, 'net_n': 3, 'dataset_path': './data_2d'}
INFO:root:loading recordio ./data_2d/train.rec...
('train size', 15683)
('train size after reset', 15683)
binarize False
use_coherent False
use_STA 1
use_N 3
use_DCN 3
per_batch_size 20
128 64 2
initializing upsampling_weight sta0_upsampling_w3_h3_weight
initializing upsampling_weight sta1_upsampling_w2_h3_weight
initializing stack1_ll_conv_offset_weight
initializing heatmap_offset_bias
initializing upsampling_weight sta0_upsampling_w2_h2_weight
initializing heatmap0_offset_weight
initializing stack0_out3_offset_bias
initializing stack0_ll_conv_offset_weight
initializing heatmap0_offset_bias
initializing upsampling_weight sta1_upsampling_w2_h2_weight
initializing upsampling_weight sta0_upsampling_w2_h3_weight
initializing heatmap_offset_weight
initializing upsampling_weight sta1_upsampling_w3_h3_weight
initializing stack0_out3_offset_weight
initializing stack0_ll_conv_offset_bias
initializing stack1_ll_conv_offset_bias
lr-steps [16000, 24000, 30000]
Traceback (most recent call last):
  File ""train.py"", line 202, in <module>
    main(args)
  File ""train.py"", line 179, in main
    epoch_end_callback = None,
  File ""/home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""/home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 1528, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (20, 3, 128, 128)
softmax_label: (20, 68, 64, 64)
[14:48:09] src/storage/storage.cc:137: Compile with USE_CUDA=1 to enable GPU usage

Stack trace returned 10 entries:
[bt] (0) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x1d86a2) [0x7f1b83dd76a2]
[bt] (1) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x1d8cb8) [0x7f1b83dd7cb8]
[bt] (2) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x32e5bbd) [0x7f1b86ee4bbd]
[bt] (3) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x32e9a7d) [0x7f1b86ee8a7d]
[bt] (4) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x32eb846) [0x7f1b86eea846]
[bt] (5) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2bc9da1) [0x7f1b867c8da1]
[bt] (6) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2bc9ef4) [0x7f1b867c8ef4]
[bt] (7) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2bd735c) [0x7f1b867d635c]
[bt] (8) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2bdda50) [0x7f1b867dca50]
[bt] (9) /home/huzhipeng/.local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2bebc18) [0x7f1b867eac18]

is anyone met this before and know why?"
"Thank you for your great work!

Issue #24 shows that MS1M Arcface already remove the overlapped data from FaceScrub.
We would like to know the overlapped data of LFW is also removed or not.

Thank you."
"Hi,

In the below link, you can find the code I work on.
https://github.com/galip/insightface/blob/master/deploy/sample.py

I want to evaluate face identification performance for this project. I used uVa-Nemo database for this and aligned the images with dlib(not with mtcnn as in your project), their size is 112*112. I get representation vectors from fc1 layer for this.
The model is : model-r50-am-lfw.
After that I use 4 Fold Cross Validation process to see the performance for identification under distortion.
I also did the same process with resnet50_ft.caffemodel(trained on VGGFace2).

The problem is that the performance is lower than resnet50_ft. But in your paper, you have better results than VGGFace2.

What is wrong here? If you have an idea, please share."
"Hello @yingfeng 

I have Downloaded faces_emore and faces_glint data but everything is in .rec or .bin formate I want to convert it into images formate .jpg or.png formate anyone know how can i do this "
"Dear @nttstar,
I want to use your pre-trained models (just for test [i.e., inference]) in a python multi-processing manner. However it has the following error: 
```
File ""/usr/local/lib/python3.5/dist-packages/mxnet/base.py"", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [14:33:22] src/storage/storage.cc:65: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: initialization error
```

Would you please kindly help me to address this problem?
"
"I have ported the models to TVM but how do i call functions in the original code like get_input and get_feature in TVM to get the vector of the face ?

Thanks."
Is there any implementation of arcface loss function that can be used with siamese architecture?
"Is there any chance of improving the speed of get_input method as it seems to be taking 300 ms each time..

Thanks."
"@nttstar 
I want to train mobilenet model with Asian-Celeb and emore. What is the best way to do this?
Not able to figure out how to use dataset_merge.py to complete this.

Thanks,"
"Hi, thanks for your great work. I would like to ask some easy questions:

1. I've downloaded MS1M-ArcFace( which is 85k identites, 5.8M, 112x112 images). Are these the images that have already been preprocessed by MTCNN? And did you directly use these images for training? I have this question since the images still seem to have some background behind.

2. When I use dataloader to load the images, what are the mean and std should I use to normalize them? Are them all [0.5,0.5,0.5]?

Thanks in advance since I kept trying to use the orginal ResNet34(imported from pytorch) +center loss and with my own cropped face dataset, I only got 98.6% on LFW.

Joe"
"Hello @nttstar 
I train LResNet100E-IR with emore + asian dataset on 1080Ti gpu. The accuracy on trillionpairs challenge is very low:

             m         per_batch_size    num_gpu      batch_size     identification         verification
      1     0.3               62           8              496              71%                  70.6%
      2     0.5               62           7              434              55.4%                55%
      3     0.5               180          6              1080             29.7%                27%
(In third experiment, I used mxnet-memonger to decrease memory).

I am very puzzled. Can you point out my mistake, please ? Thank you very much."
"Like the paper, train/validate/test on which data set, how long it takes to process a face etc.
Accuracy of gender, accuracy of age on each range etc."
"I use LResNet100E-IR （arcface loss） train my data, and ininference it spend more than 1000ms, Is it normal? thanks!
GPU: GeForce GTX 1080TI
here is my codes(deploy/test.py):

t1 = time.time()
f1 = model.get_feature(img)
t2 = time.time()
print('time:' + str((t2 - t1) * 1000))   =>time:1082.615852355957"
"The symbol in train.py outputs [embedding,softmax,extra_loss], but in center_loss.py the metric of acc and loss use wrong index.


class Accuracy(mx.metric.EvalMetric):
    def __init__(self, num=None):
        super(Accuracy, self).__init__('accuracy', num)

    def update(self, labels, preds):
        mx.metric.check_label_shapes(labels, preds)

        if self.num is not None:
            assert len(labels) == self.num

        #pred_label = mx.nd.argmax_channel(preds[0]).asnumpy().astype('int32')
        pred_label = mx.nd.argmax_channel(preds[1]).asnumpy().astype('int32')
        label = labels[0].asnumpy().astype('int32')

        mx.metric.check_label_shapes(label, pred_label)

        self.sum_metric += (pred_label.flat == label.flat).sum()
        self.num_inst += len(pred_label.flat)


\# define some metric of center_loss
class CenterLossMetric(mx.metric.EvalMetric):
    def __init__(self):
        super(CenterLossMetric, self).__init__('center_loss')

    def update(self, labels, preds):
        #self.sum_metric +=preds[1].asnumpy()[0]
        self.sum_metric += preds[2].asnumpy()[0]
        self.num_inst += 1`"
"In the article of the insightface, it said the MS1M dataset has been cleaned will be made public available within a binary file, but I cann't find where to download it. Who can tell me how to download it?"
"Have anyone trained model using multi-task learning as the following figure , for example ms1m+vgg2+two softmax .
![image](https://user-images.githubusercontent.com/16490080/52939674-be7a1800-339f-11e9-9ed2-24be20fd580b.png)
"
"Hi @nttstar 
The pre-trained model of .params (model-r100-ii) is 249M, but my .params file size is 435.4M, why it's so large, and which may cause this?
config:
batch_size =32
run script: 
python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir /workspace/dataset/ms1m/third_party/faces_ms1m_112x112 --prefix ../model-r100

Thanks."
"When I train sdu-net with 3D datasets, after training for about 500 batches, the lossvalue becomes nan and NME of AFLW2000-3D shapely increased. I don't know why. 

Here are the config parameters I used.
```
gpu num: 1
Call with Namespace(batch_size=16, ckpt=1, ctx_num=1, dataset='i3d', exf=1, frequent=20, lr=0.00025, lr_step='16000,24000,30000', network='sdu', norm=0, optimizer='nadam', per_batch_size=16, prefix='model/sdu', pretrained='', verbose=200, wd=0.0) {'net_binarize': False, 'net_dcn': 3, 'gaussian': 0, 'net_block': 'cab', 'dataset': 'i3d', 'record_img_size': 384, 'landmark_type': '3d', 'net_coherent': False, 'base_scale': 256, 'val_targets': ['AFLW2000-3D'], 'network': 'sdu', 'net_stacks': 2, 'losstype': 'heatmap', 'input_img_size': 128, 'net_sta': 1, 'multiplier': 1.0, 'num_classes': 68, 'output_label_size': 64, 'label_xfirst': False, 'per_batch_size': 16, 'net_n': 3, 'dataset_path': '/media/3T_disk/my_datasets/sdu_net/data_3d'}
INFO:root:loading recordio /media/3T_disk/my_datasets/sdu_net/data_3d/train.rec...
('train size', 61225)
('train size after reset', 61225)
binarize False
use_coherent False
use_STA 1
use_N 3
use_DCN 3
per_batch_size 16
128 64 2
```

Here is the output information during training.
```
[200][AFLW2000-3D]NME: 0.124324
saving 1
INFO:root:Saved checkpoint to ""model/sdu-0001.params""
INFO:root:Epoch[0] Batch [200]  Speed: 4.03 samples/sec lossvalue=0.001130
INFO:root:Epoch[0] Batch [220]  Speed: 14.01 samples/sec        lossvalue=0.001072
INFO:root:Epoch[0] Batch [240]  Speed: 13.94 samples/sec        lossvalue=0.001073
INFO:root:Epoch[0] Batch [260]  Speed: 13.93 samples/sec        lossvalue=0.001054
INFO:root:Epoch[0] Batch [280]  Speed: 13.92 samples/sec        lossvalue=0.001047
INFO:root:Epoch[0] Batch [300]  Speed: 13.91 samples/sec        lossvalue=0.001041
INFO:root:Epoch[0] Batch [320]  Speed: 13.90 samples/sec        lossvalue=0.001011
INFO:root:Epoch[0] Batch [340]  Speed: 13.88 samples/sec        lossvalue=0.001001
INFO:root:Epoch[0] Batch [360]  Speed: 13.89 samples/sec        lossvalue=0.001021
INFO:root:Epoch[0] Batch [380]  Speed: 13.90 samples/sec        lossvalue=0.000992
INFO:root:loading recordio /media/3T_disk/my_datasets/sdu_net/data_3d/AFLW2000-3D.rec...
('train size', 2000)
[400][AFLW2000-3D]NME: 0.060307
saving 2
INFO:root:Saved checkpoint to ""model/sdu-0002.params""
INFO:root:Epoch[0] Batch [400]  Speed: 4.04 samples/sec lossvalue=0.000992
INFO:root:Epoch[0] Batch [420]  Speed: 13.94 samples/sec        lossvalue=0.000975
INFO:root:Epoch[0] Batch [440]  Speed: 13.89 samples/sec        lossvalue=0.000965
INFO:root:Epoch[0] Batch [460]  Speed: 13.88 samples/sec        lossvalue=0.000950
INFO:root:Epoch[0] Batch [480]  Speed: 13.88 samples/sec        lossvalue=0.000936
INFO:root:Epoch[0] Batch [500]  Speed: 13.88 samples/sec        lossvalue=0.000960
INFO:root:Epoch[0] Batch [520]  Speed: 14.49 samples/sec        lossvalue=nan
INFO:root:Epoch[0] Batch [540]  Speed: 14.55 samples/sec        lossvalue=nan
INFO:root:Epoch[0] Batch [560]  Speed: 14.55 samples/sec        lossvalue=nan
INFO:root:Epoch[0] Batch [580]  Speed: 14.57 samples/sec        lossvalue=nan
INFO:root:loading recordio /media/3T_disk/my_datasets/sdu_net/data_3d/AFLW2000-3D.rec...
('train size', 2000)
[600][AFLW2000-3D]NME: 0.536341
saving 3
INFO:root:Saved checkpoint to ""model/sdu-0003.params""
INFO:root:Epoch[0] Batch [600]  Speed: 4.11 samples/sec lossvalue=nan
INFO:root:Epoch[0] Batch [620]  Speed: 14.33 samples/sec        lossvalue=nan
```
"
Why the SSH detector has been removed?
"Hi, I got an error that the same with #528. I followed README and sample_config.py but cannot solve this bug.

Here is my command:
CUDA_VISIBLE_DEVICES='0,1' python -u train.py --network r100 --loss triplet --lr 0.005 --pretrained ../models/model-r100-ii/model --pretrained-epoch 0

Ouput:
gpu num: 2
prefix ./models/r100-triplet-emore/model
image_size [112, 112]
num_classes 344
Called with argument: Namespace(batch_size=120, ckpt=3, ctx_num=2, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='triplet', lr=0.005, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=60, pretrained='../models/model-r100-ii/model', pretrained_epoch=0, rescale_threshold=0, verbose=2000, wd=0.0005) {'net_act': 'prelu', 'emb_size': 512, 'data_images_filter': 0, 'num_layers': 100, 'loss_name': 'triplet', 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'net_input': 1, 'image_shape': [112, 112, 3], 'lr': 0.05, 'triplet_bag_size': 1720, 'fc7_lr_mult': 1.0, 'ckpt_embedding': True, 'triplet_max_ap': 0.0, 'net_unit': 3, 'net_output': 'E', 'data_rand_mirror': True, 'num_workers': 1, 'triplet_alpha': 0.3, 'dataset': 'emore', 'num_classes': 344, 'fc7_no_bias': False, 'loss': 'triplet', 'data_color': 0, 'dataset_path': '../datasets/split_data/train', 'data_cutoff': False, 'images_per_identity': 5, 'net_se': 0, 'net_multiplier': 1.0, 'fc7_wd_mult': 1.0, 'network': 'r100', 'per_batch_size': 60, 'net_name': 'fresnet', 'workspace': 256, 'max_steps': 0, 'bn_mom': 0.9}
loading ../models/model-r100-ii/model 0
[09:47:02] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.2.0. Attempting to upgrade...
[09:47:02] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
0 1 E 3 prelu
INFO:root:loading recordio ../datasets/split_data/train/train.rec...
header0 label [49296. 49640.]
id2range 344
49295
rand_mirror True
5 12 1744
triplet_seq 1720
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
loading bin 15000
loading bin 16000
loading bin 17000
loading bin 18000
loading bin 19000
loading bin 20000
loading bin 21000
loading bin 22000
loading bin 23000
(23458L, 3L, 112L, 112L)
ver lfw
lr_steps [100000, 160000, 220000]
call reset()
eval 1720 images.. 0
triplet time stat [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/home/anhvo/miniconda2/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/home/anhvo/miniconda2/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/home/anhvo/miniconda2/lib/python2.7/site-packages/mxnet/io.py"", line 401, in prefetch_func
    self.next_batch[i] = self.iters[i].next()
  File ""/home/anhvo/insightface/recognition/triplet_image_iter.py"", line 480, in next
    self.reset()
  File ""/home/anhvo/insightface/recognition/triplet_image_iter.py"", line 390, in reset
    self.select_triplets()
  File ""/home/anhvo/insightface/recognition/triplet_image_iter.py"", line 253, in select_triplets
    self.mx_model.forward(db, is_train=False)
  File ""/home/anhvo/miniconda2/lib/python2.7/site-packages/mxnet/module/module.py"", line 591, in forward
    assert self.binded and self.params_initialized
AssertionError

/home/anhvo/miniconda2/lib/python2.7/site-packages/mxnet/module/base_module.py:504: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.5 vs. 0.00833333333333). Is this intended?
  optimizer_params=optimizer_params)"
"Hello,is there anyone know where is the code for the video demo?Thanks."
"Hello!
From time to time we get Exception Failed to allocate CPU memory.
From the debugging, we find it occurs on 362 - 364 lines while face detecting 
in mtcnn_detector.py 
It seems like sometimes stage of memory released is omitted by python.
We use python x86 (try both 3.6, 2.7) on 64 bit windows 7.
Exception occurs only for relatively large images (>400 KB)"
"Hi ，thanks for sharing your work.I want to try to finetune my model by freezon the layers before the layer of  ""fc7"",what I should do and how to modify the codes.Can you help me?Thx a lot!"
"Hello,
In this page, https://github.com/deepinsight/insightface/wiki/Model-Zoo, you report the performance of different models on different databases.

Question 1: For LFW and CFP-FP databases, is the % that you get TAR ? (And in this case, how about the associated FAR ?)

Question 2: For CFP-FP, we compare between frontal face vs. profile face. It is difficult to apply the face alignment on the later. How do you do in this case ? Or simply you take directly the image of database and compute the embedding then the score (because I see that in this CFP database, all images are already cropped around the faces)

Thanks"
"Dear @yingfeng, @nttstar, @yuzhichang,
I have a face data set with the following format:
```
FaceDataSet/
    ID1/
        face_imageID1_1.jpg
        face_imageID1_2.jpg
        face_imageID1_3.jpg
    ID2/
        face_imageID2_1.jpg
        face_imageID2_2.jpg
...
```
Would you please kindly guide me to how I can train your models (e.g., with ArcFace Loss) on my own data set?
"
Currently insightface does not support python3?Whether there is any plan to support later
""
"Hi,

I wanted to know what the optimal image size for high accuracy and speed for profile pictures to predict age and gender.
ex: is 150*150px image accurate enough 85% of the time?

Or a link to the paper that the model is based on?

Cheers!"
"Hi, I am trying to deploy the age and gender model using the mxnet-model-server on an ec2 instance with 8 CPUs (m5.4xlarge).

I am facing 2 issues:
1. The response time for images with size > 300px*300px is 10 times slower than that with 200px*200px. 
2. Also I expected ~ 2x the throughput with 2x cores. But i dont see any improvement.

I am trying to understand if there is something in the model that is causing the CPU to slow down with increased throughput? 

Any help appreciated. Thanks!
"
"Thanks for Sharing! @nttstar 
About the glint-challenge i am puzzled for the result.
i use the  LResNet100E-IR,ArcFace@ms1m-refine-v2 model and generate the feature file for glint test data, the result are as flows. 

1.Identification: 0.70841   TPR@FPR=1e-3
2.Verification:0.39500    TPR@FPR=1e-9

the result is lower than most of the submitted algorithms, therefore, there is any mistake or i need to train the algorithm  with deep glint training dataset?

Thanks very much!"
"### **After running below command**
`python verification.py --data-dir /home/surveillance6/Downloads/datasets/lfw --target lfw --model /home/surveillance6/Downloads/model-r100-ii/model --gpu 0`
### **I got below error**
`image_size [250, 250]
model number 1
loading /home/surveillance6/Downloads/model-r100-ii/model 0
[11:09:59] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.2.0. Attempting to upgrade...
[11:09:59] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""verification.py"", line 575, in <module>
    model.set_params(arg_params, aux_params)
  File ""/home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/module/module.py"", line 350, in set_params
    allow_extra=allow_extra)
  File ""/home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/module/module.py"", line 309, in init_params
    _impl(desc, arr, arg_params)
  File ""/home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/module/module.py"", line 297, in _impl
    cache_arr.copyto(arr)
  File ""/home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/ndarray/ndarray.py"", line 2066, in copyto
    return _internal._copyto(self, out=other)
  File ""<string>"", line 25, in _copyto
  File ""/home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/_ctypes/ndarray.py"", line 92, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [11:10:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/3rdparty/mshadow/../../src/operator/tensor/../elemwise_op_common.h:133: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [512,25088], got [512,131072]

Stack trace returned 10 entries:
[bt] (0) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x36bac2) [0x7fb37514fac2]
[bt] (1) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x36c0a8) [0x7fb3751500a8]
[bt] (2) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x548e23) [0x7fb37532ce23]
[bt] (3) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x54970b) [0x7fb37532d70b]
[bt] (4) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x2fa4692) [0x7fb377d88692]
[bt] (5) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x2fae34e) [0x7fb377d9234e]
[bt] (6) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x2ecc3ab) [0x7fb377cb03ab]
[bt] (7) /home/surveillance6/insightface-env/lib/python3.5/site-packages/mxnet/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7fb377cb096f]
[bt] (8) /home/surveillance6/insightface-env/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7fb3a74c4e20]
[bt] (9) /home/surveillance6/insightface-env/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7fb3a74c488b]'"
":~/ai/insightface/recognition$ CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100 --loss arcface --dataset emore
Traceback (most recent call last):
  File ""train.py"", line 10, in <module>
    import sklearn
  File ""/home/zds/.local/lib/python2.7/site-packages/sklearn/__init__.py"", line 64, in <module>
    from .base import clone
  File ""/home/zds/.local/lib/python2.7/site-packages/sklearn/base.py"", line 10, in <module>
    import numpy as np
  File ""/home/zds/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>
    from . import core
  File ""/home/zds/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 57, in <module>
    from . import numerictypes as nt
  File ""/home/zds/.local/lib/python2.7/site-packages/numpy/core/numerictypes.py"", line 111, in <module>
    from ._type_aliases import (
  File ""/home/zds/.local/lib/python2.7/site-packages/numpy/core/_type_aliases.py"", line 63, in <module>
    _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}
  File ""/home/zds/.local/lib/python2.7/site-packages/numpy/core/_type_aliases.py"", line 63, in <setcomp>
    _concrete_types = {v.type for k, v in _concrete_typeinfo.items()}
AttributeError: 'tuple' object has no attribute 'type'

I am following this new trainning code https://github.com/deepinsight/insightface/tree/master/recognition and got above error.  
The first trun, it told me the sklearn package is missing, and I just do the ""pip install sklearn"".  Then got above error. 
Need I install specific version numpy and sklearn?
thank you! @nttstar "
您好，请问您默认的训练epoch是多少次，训练的多长时间，谢谢。
"Thanks for sharing the code, but I am confused about one issue. Is the input image to the trained face recognition model in RGB or BGR?

The input here https://github.com/deepinsight/insightface/blob/master/deploy/test.py#L21 is RGB,
but the input here https://github.com/deepinsight/insightface/blob/master/deploy/test.py#L28 is BGR. 
Which one is correct? I.e. which color channel order was used during training? 

Ps. actually #L28 is not able to execute as the color channel is not swapped from index 2 to index 0 yet. 

Thanks in advance! "
i have already seen Alignment，but how to use face recognition，the num of landmark  is five in project
What's the relationship between ms1m-v1 and ms1m-v2? Thank you.
"this is load dataset code:
bins, issame_list = pickle.load(open(os.path.join(args.eval_db_path, db_name + '.bin'), 'rb'), encoding='bytes')

but how to create the bin file, like lfw.bin in https://github.com/deepinsight/insightface/wiki/Model-Zoo
"
"``训练脚本sh没有设置环境变量，脚本如下：

#!/usr/bin/env bash
DATA_DIR=/data/faces_emore
NETWORK=r100
JOB=scratch_r100_emore_128n
MODELDIR=""/data-8t-2/insightface/model-$NETWORK-$JOB""
mkdir -p ""$MODELDIR""
PREFIX=""$MODELDIR/model""
LOGFILE=""$MODELDIR/log""
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --data-dir $DATA_DIR --network ""$NETWORK"" --emb-size 128 --loss-type 5 --margin-a 1 --margin-m 0.3 --margin-b 0.2 --prefix ""$PREFIX"" --per-batch-size 64 --target lfw,my_asian,cfp_fp,agedb_30 --verbose 3000  2>&1  |tee ""$LOGFILE""

主硬件配置：1080TI*4、三星512G固态硬盘。
一开始训练速度在370samples/sec ,但是在第一次test verification后就掉到280samples/sec，然后训练速度就上不去了。观察到系统主存占用前后几乎没有变化，cpu负载降低10%左右，硬盘传输训练数据io速度由4MB/s下降到2.5MB/s，显卡利用率也降低了，会出现空载，多次测试都是相同结果。个人猜测是cpu拉取数据线程减少、速度变慢导致数据供给不足，导致显卡空载。不知有没有遇到相同情况的？

日志如下：
INFO:root:Epoch[0] Batch [2940]	Speed: 371.41 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [2960]	Speed: 368.46 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [2980]	Speed: 372.38 samples/sec	acc=0.000000
lr-batch-epoch: 0.1 2999 0
testing verification..
(12000, 512)
infer time 23.602758
[lfw][3000]XNorm: 15.338712
[lfw][3000]Accuracy-Flip: 0.80867+-0.01236
testing verification..
(12000, 512)
infer time 23.134312
[my_asian][3000]XNorm: 15.136430
[my_asian][3000]Accuracy-Flip: 0.79467+-0.01242
testing verification..
(14000, 512)
infer time 26.959641
[cfp_fp][3000]XNorm: 15.231380
[cfp_fp][3000]Accuracy-Flip: 0.63686+-0.01412
testing verification..
(12000, 512)
infer time 23.346582
[agedb_30][3000]XNorm: 15.036079
[agedb_30][3000]Accuracy-Flip: 0.61150+-0.01399
saving 1
INFO:root:Saved checkpoint to ""/data-8t-2/insightface/model-r100-scratch_3000wasian/model-0001.params""
[3000]Accuracy-Highest: 0.61150
INFO:root:Epoch[0] Batch [3000]	Speed: 79.48 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [3020]	Speed: 280.54 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [3040]	Speed: 276.67 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [3060]	Speed: 279.67 samples/sec	acc=0.000000"
"As my experiments, the feature's norm is highly correlated with the image's quality, high quality image has little norm and low quality image has large norm. I can not figure it out why this happen, is it because of the low quality image have lots of noise?"
"我在src目录下训练时如果作下面这些设置，就会出现训练非常慢，速度大概
Speed: 50 samples/sec这样，如果不作这些设置会快很多大概
Speed: 1122.83 samples/sec（硬件：4X1080TI，48核CPU），感觉训练的
AAC上升比较慢
export MXNET_CPU_WORKER_NTHREADS=24
export MXNET_CUDNN_AUTOTUNE_DEFAULT=0
export MXNET_ENGINE_TYPE=ThreadedEnginePerDevice


下面是刚刚开始10分钟左右的打印：
./train_mobilefacenet_step1.sh 
/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[10:42:49] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
gpu num: 4
num_layers 1
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=512, beta=1000.0, beta_freeze=0, beta_min=5.0, bn_mom=0.9, ce_loss=False, ckpt=2, color=0, ctx_num=4, cutoff=0, data_dir='/ssd1/faces_emore', easy_margin=0, emb_size=128, end_epoch=100000, fc7_lr_mult=1.0, fc7_no_bias=False, fc7_wd_mult=10.0, gamma=0.12, image_channel=3, image_h=112, image_size='112,112', image_w=112, images_filter=0, loss_type=0, lr=0.1, lr_steps='240000,360000,440000', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.1, margin_s=32.0, max_steps=140002, mom=0.9, network='y1', num_classes=85742, num_layers=1, per_batch_size=128, power=1.0, prefix='../model-y1-test/model', pretrained='', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_multiplier=1.0, version_output='GDC', version_se=0, version_unit=3, wd=4e-05)
init mobilefacenet 1
INFO:root:loading recordio /ssd1/faces_emore/train.rec...
header0 label [5822654. 5908396.]
id2range 85742
5822653
rand_mirror 1
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [240000, 360000, 440000]
call reset()
/usr/local/anaconda3/lib/python3.6/site-packages/mxnet/module/base_module.py:505: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.001953125). Is this intended?
  optimizer_params=optimizer_params)
[10:43:30] src/kvstore/././comm.h:745: only 4 out of 12 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off
[10:43:30] src/kvstore/././comm.h:754: .v..
[10:43:30] src/kvstore/././comm.h:754: v...
[10:43:30] src/kvstore/././comm.h:754: ...v
[10:43:30] src/kvstore/././comm.h:754: ..v.
[10:43:30] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:Epoch[0] Batch [0-20]	Speed: 1132.30 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [20-40]	Speed: 1056.72 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [40-60]	Speed: 1123.02 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [60-80]	Speed: 1130.13 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [80-100]	Speed: 1123.67 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [100-120]	Speed: 1127.47 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [120-140]	Speed: 1133.34 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [140-160]	Speed: 1106.65 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [160-180]	Speed: 1116.96 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [180-200]	Speed: 1122.83 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [200-220]	Speed: 1119.48 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [220-240]	Speed: 1124.47 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [240-260]	Speed: 1126.63 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [260-280]	Speed: 1112.80 samples/sec	acc=0.000293
INFO:root:Epoch[0] Batch [280-300]	Speed: 1116.77 samples/sec	acc=0.000391
INFO:root:Epoch[0] Batch [300-320]	Speed: 1112.81 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [320-340]	Speed: 1111.72 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [340-360]	Speed: 1109.62 samples/sec	acc=0.000293
INFO:root:Epoch[0] Batch [360-380]	Speed: 1099.48 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [380-400]	Speed: 1110.43 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [400-420]	Speed: 1104.34 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [420-440]	Speed: 1111.38 samples/sec	acc=0.000098
INFO:root:Epoch[0] Batch [440-460]	Speed: 1067.16 samples/sec	acc=0.000488
INFO:root:Epoch[0] Batch [460-480]	Speed: 1102.03 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [480-500]	Speed: 1096.34 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [500-520]	Speed: 1124.60 samples/sec	acc=0.000293
INFO:root:Epoch[0] Batch [520-540]	Speed: 1108.92 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [540-560]	Speed: 1114.52 samples/sec	acc=0.000391
INFO:root:Epoch[0] Batch [560-580]	Speed: 1100.79 samples/sec	acc=0.000391
INFO:root:Epoch[0] Batch [580-600]	Speed: 1102.83 samples/sec	acc=0.000391
INFO:root:Epoch[0] Batch [600-620]	Speed: 1111.57 samples/sec	acc=0.000293
INFO:root:Epoch[0] Batch [620-640]	Speed: 1113.79 samples/sec	acc=0.000684
INFO:root:Epoch[0] Batch [640-660]	Speed: 1112.91 samples/sec	acc=0.000293
INFO:root:Epoch[0] Batch [660-680]	Speed: 1105.92 samples/sec	acc=0.000488
INFO:root:Epoch[0] Batch [680-700]	Speed: 1105.13 samples/sec	acc=0.000488
INFO:root:Epoch[0] Batch [700-720]	Speed: 1095.20 samples/sec	acc=0.000391
INFO:root:Epoch[0] Batch [720-740]	Speed: 1102.56 samples/sec	acc=0.000977
INFO:root:Epoch[0] Batch [740-760]	Speed: 1113.49 samples/sec	acc=0.000684
INFO:root:Epoch[0] Batch [760-780]	Speed: 1094.19 samples/sec	acc=0.000195
INFO:root:Epoch[0] Batch [780-800]	Speed: 1078.27 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [800-820]	Speed: 1085.61 samples/sec	acc=0.001270
INFO:root:Epoch[0] Batch [820-840]	Speed: 1104.80 samples/sec	acc=0.000684
INFO:root:Epoch[0] Batch [840-860]	Speed: 1083.88 samples/sec	acc=0.000684
INFO:root:Epoch[0] Batch [860-880]	Speed: 1102.90 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [880-900]	Speed: 1098.46 samples/sec	acc=0.000879
INFO:root:Epoch[0] Batch [900-920]	Speed: 1117.42 samples/sec	acc=0.001074
INFO:root:Epoch[0] Batch [920-940]	Speed: 1105.74 samples/sec	acc=0.000781
INFO:root:Epoch[0] Batch [940-960]	Speed: 1106.28 samples/sec	acc=0.000879
INFO:root:Epoch[0] Batch [960-980]	Speed: 1099.08 samples/sec	acc=0.000684
lr-batch-epoch: 0.1 999 0
INFO:root:Epoch[0] Batch [980-1000]	Speed: 1104.30 samples/sec	acc=0.000879
INFO:root:Epoch[0] Batch [1000-1020]	Speed: 1111.57 samples/sec	acc=0.000977
INFO:root:Epoch[0] Batch [1020-1040]	Speed: 1115.43 samples/sec	acc=0.001465
INFO:root:Epoch[0] Batch [1040-1060]	Speed: 1105.48 samples/sec	acc=0.000488
INFO:root:Epoch[0] Batch [1060-1080]	Speed: 1099.90 samples/sec	acc=0.001367
INFO:root:Epoch[0] Batch [1080-1100]	Speed: 1111.54 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [1100-1120]	Speed: 1095.31 samples/sec	acc=0.000879
INFO:root:Epoch[0] Batch [1120-1140]	Speed: 1114.35 samples/sec	acc=0.001074
INFO:root:Epoch[0] Batch [1140-1160]	Speed: 1099.33 samples/sec	acc=0.001660
INFO:root:Epoch[0] Batch [1160-1180]	Speed: 1093.73 samples/sec	acc=0.001465
INFO:root:Epoch[0] Batch [1180-1200]	Speed: 1111.13 samples/sec	acc=0.000488
INFO:root:Epoch[0] Batch [1200-1220]	Speed: 1088.15 samples/sec	acc=0.000684
INFO:root:Epoch[0] Batch [1220-1240]	Speed: 1104.83 samples/sec	acc=0.001563
INFO:root:Epoch[0] Batch [1240-1260]	Speed: 1061.60 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [1260-1280]	Speed: 1124.20 samples/sec	acc=0.001074
INFO:root:Epoch[0] Batch [1280-1300]	Speed: 1112.04 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [1300-1320]	Speed: 1082.62 samples/sec	acc=0.001172
INFO:root:Epoch[0] Batch [1320-1340]	Speed: 1096.77 samples/sec	acc=0.000781
INFO:root:Epoch[0] Batch [1340-1360]	Speed: 1111.64 samples/sec	acc=0.000781
INFO:root:Epoch[0] Batch [1360-1380]	Speed: 1120.92 samples/sec	acc=0.001855
INFO:root:Epoch[0] Batch [1380-1400]	Speed: 1105.31 samples/sec	acc=0.001270
INFO:root:Epoch[0] Batch [1400-1420]	Speed: 1118.48 samples/sec	acc=0.001270
INFO:root:Epoch[0] Batch [1420-1440]	Speed: 1114.02 samples/sec	acc=0.000977
INFO:root:Epoch[0] Batch [1440-1460]	Speed: 1114.85 samples/sec	acc=0.001953
INFO:root:Epoch[0] Batch [1460-1480]	Speed: 1104.04 samples/sec	acc=0.001563
INFO:root:Epoch[0] Batch [1480-1500]	Speed: 1113.21 samples/sec	acc=0.001367
INFO:root:Epoch[0] Batch [1500-1520]	Speed: 1125.47 samples/sec	acc=0.000586
INFO:root:Epoch[0] Batch [1520-1540]	Speed: 1119.58 samples/sec	acc=0.000684
INFO:root:Epoch[0] Batch [1540-1560]	Speed: 1111.21 samples/sec	acc=0.001563
INFO:root:Epoch[0] Batch [1560-1580]	Speed: 1115.98 samples/sec	acc=0.001172
INFO:root:Epoch[0] Batch [1580-1600]	Speed: 1110.14 samples/sec	acc=0.001367
INFO:root:Epoch[0] Batch [1600-1620]	Speed: 1096.98 samples/sec	acc=0.000781
INFO:root:Epoch[0] Batch [1620-1640]	Speed: 1124.20 samples/sec	acc=0.001563
INFO:root:Epoch[0] Batch [1640-1660]	Speed: 1108.31 samples/sec	acc=0.001172
INFO:root:Epoch[0] Batch [1660-1680]	Speed: 1091.27 samples/sec	acc=0.001660
INFO:root:Epoch[0] Batch [1680-1700]	Speed: 1106.60 samples/sec	acc=0.001855
INFO:root:Epoch[0] Batch [1700-1720]	Speed: 1111.52 samples/sec	acc=0.001660
INFO:root:Epoch[0] Batch [1720-1740]	Speed: 1097.61 samples/sec	acc=0.001855
INFO:root:Epoch[0] Batch [1740-1760]	Speed: 1089.44 samples/sec	acc=0.000977
INFO:root:Epoch[0] Batch [1760-1780]	Speed: 1096.06 samples/sec	acc=0.001855
INFO:root:Epoch[0] Batch [1780-1800]	Speed: 1103.87 samples/sec	acc=0.001758
INFO:root:Epoch[0] Batch [1800-1820]	Speed: 1115.11 samples/sec	acc=0.001953
INFO:root:Epoch[0] Batch [1820-1840]	Speed: 1085.17 samples/sec	acc=0.001270
INFO:root:Epoch[0] Batch [1840-1860]	Speed: 1114.05 samples/sec	acc=0.001953
INFO:root:Epoch[0] Batch [1860-1880]	Speed: 1104.16 samples/sec	acc=0.001074
INFO:root:Epoch[0] Batch [1880-1900]	Speed: 1103.52 samples/sec	acc=0.001660
INFO:root:Epoch[0] Batch [1900-1920]	Speed: 1100.42 samples/sec	acc=0.001563
INFO:root:Epoch[0] Batch [1920-1940]	Speed: 1110.85 samples/sec	acc=0.002441
"
why num_class=85142?my train dataset set 85742
"Hi guys, nice project. 
I made a minimalistic insightface repack that works for inference out of the box.
Original code is quite bloated and hard to navigate if all you want is to do inference with pre-trained models. I ommited anything that is not related to inference and provide some additional error handling and a Jupyter Notebook usage example.
https://github.com/kiselev1189/insightface-just-works

Mind to include it into third-party reimplemetations list?"
""
""
"f2 = model.get_feature(img)
As f2 is  a 512-dimensional zero vector！！！！
what is wrong？"
can you tell me where the SDU model is?
"Running verification.py on a big size val.bin will trigger some bug in mx.nd.slice_axis,  
`_data = nd.slice_axis(data, axis=0, begin=bb-batch_size, end=bb).`

![_15470004417371](https://user-images.githubusercontent.com/1244520/51013946-58c88e00-15a0-11e9-8223-a3e79d22f8b0.png)

Has anyone encountered the same problem?"
"hello,thanks for your great work.  I want to finetune my model by train_tripet.py but I find my initial lossvalue is small. As the training continues， the number of tripetpairs is decreasing gradually but the lossvalue keep steady.I try to modify the parameters of"" --lr, --triplet-alpha, --triplet-bag-size"",but it does not work.What I should do for this problems that the lossvalue keep steady? Can you help me? The parameters is set as follow:
""Called with argument: Namespace(batch_size=180, ckpt=2, ctx_num=2, cutoff=0, data_dir='/home/dlzh/mxnet_datasets/faces_ms1m_112x112', emb_size=512, end_epoch=100000, image_channel=3, image_h=112, image_w=112, images_per_identity=30, loss_type=12, lr=0.0001, lr_steps='90000,150000,210000,270000', max_steps=0, mom=0.9, network='r50', noise_sgd=0.0, num_classes=85164, num_layers=50, per_batch_size=90, prefix='../model-r50-tripletloss/model', pretrained='/home/nyy/insightface/mxnet-airport/models,0', rand_mirror=1, target='lfw,cfp_fp,agedb_30', triplet_alpha=0.3, triplet_bag_size=5400, triplet_max_ap=0.0, use_deformable=0, verbose=10000, version_act='prelu', version_input=1, version_output='E', version_se=1, version_unit=3, wd=0.0005)
the  initial lossvalue is 
FaceNet selection
found triplets 6705
seq len 20070
INFO:root:Epoch[0] Batch [2]	Speed: 373.09 samples/sec	lossvalue=0.044742
INFO:root:Epoch[0] Batch [4]	Speed: 254.91 samples/sec	lossvalue=0.030269
INFO:root:Epoch[0] Batch [6]	Speed: 248.46 samples/sec	lossvalue=0.036187
INFO:root:Epoch[0] Batch [8]	Speed: 248.26 samples/sec	lossvalue=0.035805
INFO:root:Epoch[0] Batch [10]	Speed: 252.61 samples/sec	lossvalue=0.020587
INFO:root:Epoch[0] Batch [12]	Speed: 253.98 samples/sec	lossvalue=0.044366
INFO:root:Epoch[0] Batch [14]	Speed: 250.19 samples/sec	lossvalue=0.043796
INFO:root:Epoch[0] Batch [16]	Speed: 254.86 samples/sec	lossvalue=0.033092
INFO:root:Epoch[0] Batch [18]	Speed: 248.32 samples/sec	lossvalue=0.041055
INFO:root:Epoch[0] Batch [20]	Speed: 248.14 samples/sec	lossvalue=0.026702
INFO:root:Epoch[0] Batch [22]	Speed: 253.62 samples/sec	lossvalue=0.028108
INFO:root:Epoch[0] Batch [24]	Speed: 250.52 samples/sec	lossvalue=0.027292
INFO:root:Epoch[0] Batch [26]	Speed: 244.14 samples/sec	lossvalue=0.033994
INFO:root:Epoch[0] Batch [28]	Speed: 260.31 samples/sec	lossvalue=0.030367
INFO:root:Epoch[0] Batch [30]	Speed: 258.46 samples/sec	lossvalue=0.030359
INFO:root:Epoch[0] Batch [32]	Speed: 248.71 samples/sec	lossvalue=0.027290
INFO:root:Epoch[0] Batch [34]	Speed: 251.30 samples/sec	lossvalue=0.028097
INFO:root:Epoch[0] Batch [36]	Speed: 250.25 samples/sec	lossvalue=0.035273
INFO:root:Epoch[0] Batch [38]	Speed: 250.58 samples/sec	lossvalue=0.038749
INFO:root:Epoch[0] Batch [40]	Speed: 247.84 samples/sec	lossvalue=0.028016
INFO:root:Epoch[0] Batch [42]	Speed: 247.20 samples/sec	lossvalue=0.032609
INFO:root:Epoch[0] Batch [44]	Speed: 252.65 samples/sec	lossvalue=0.031835
INFO:root:Epoch[0] Batch [46]	Speed: 249.61 samples/sec	lossvalue=0.037089
INFO:root:Epoch[0] Batch [48]	Speed: 250.29 samples/sec	lossvalue=0.026986
INFO:root:Epoch[0] Batch [50]	Speed: 251.30 samples/sec	lossvalue=0.028959
INFO:root:Epoch[0] Batch [52]	Speed: 248.80 samples/sec	lossvalue=0.024966
INFO:root:Epoch[0] Batch [54]	Speed: 250.33 samples/sec	lossvalue=0.026383
INFO:root:Epoch[0] Batch [56]	Speed: 251.58 samples/sec	lossvalue=0.045834
INFO:root:Epoch[0] Batch [58]	Speed: 250.17 samples/sec	lossvalue=0.030616
INFO:root:Epoch[0] Batch [60]	Speed: 245.00 samples/sec	lossvalue=0.030418
INFO:root:Epoch[0] Batch [62]	Speed: 250.69 samples/sec	lossvalue=0.025132
INFO:root:Epoch[0] Batch [64]	Speed: 253.14 samples/sec	lossvalue=0.028255
INFO:root:Epoch[0] Batch [66]	Speed: 247.15 samples/sec	lossvalue=0.022813
INFO:root:Epoch[0] Batch [68]	Speed: 247.01 samples/sec	lossvalue=0.026068
INFO:root:Epoch[0] Batch [70]	Speed: 249.28 samples/sec	lossvalue=0.022546
INFO:root:Epoch[0] Batch [72]	Speed: 245.23 samples/sec	lossvalue=0.030744
INFO:root:Epoch[0] Batch [74]	Speed: 255.72 samples/sec	lossvalue=0.028281
INFO:root:Epoch[0] Batch [76]	Speed: 250.34 samples/sec	lossvalue=0.024437
INFO:root:Epoch[0] Batch [78]	Speed: 254.82 samples/sec	lossvalue=0.030507
INFO:root:Epoch[0] Batch [80]	Speed: 251.81 samples/sec	lossvalue=0.029737
INFO:root:Epoch[0] Batch [82]	Speed: 257.33 samples/sec	lossvalue=0.026239
INFO:root:Epoch[0] Batch [84]	Speed: 253.19 samples/sec	lossvalue=0.028569
INFO:root:Epoch[0] Batch [86]	Speed: 251.57 samples/sec	lossvalue=0.031699
INFO:root:Epoch[0] Batch [88]	Speed: 249.90 samples/sec	lossvalue=0.032710
INFO:root:Epoch[0] Batch [90]	Speed: 248.32 samples/sec	lossvalue=0.031815
INFO:root:Epoch[0] Batch [92]	Speed: 247.48 samples/sec	lossvalue=0.025138
INFO:root:Epoch[0] Batch [94]	Speed: 250.01 samples/sec	lossvalue=0.029476
INFO:root:Epoch[0] Batch [96]	Speed: 246.02 samples/sec	lossvalue=0.030440
INFO:root:Epoch[0] Batch [98]	Speed: 249.62 samples/sec	lossvalue=0.029164
INFO:root:Epoch[0] Batch [100]	Speed: 243.97 samples/sec	lossvalue=0.030050
INFO:root:Epoch[0] Batch [102]	Speed: 252.68 samples/sec	lossvalue=0.020547
INFO:root:Epoch[0] Batch [104]	Speed: 248.08 samples/sec	lossvalue=0.034070
INFO:root:Epoch[0] Batch [106]	Speed: 249.24 samples/sec	lossvalue=0.024224
INFO:root:Epoch[0] Batch [108]	Speed: 249.29 samples/sec	lossvalue=0.037545
INFO:root:Epoch[0] Batch [110]	Speed: 255.19 samples/sec	lossvalue=0.023431
INFO:root:Epoch[0] Train-lossvalue=0.023431
INFO:root:Epoch[0] Time cost=167.874
call reset()
eval 5400 images.. 5400
triplet time stat [0.000219, 84.838133, 2.762294, 0.0, 0.0, 0.0]
found triplets 7051
seq len 21150
INFO:root:Epoch[1] Batch [2]	Speed: 252.53 samples/sec	lossvalue=0.043291
INFO:root:Epoch[1] Batch [4]	Speed: 252.97 samples/sec	lossvalue=0.052420
INFO:root:Epoch[1] Batch [6]	Speed: 256.09 samples/sec	lossvalue=0.033868
INFO:root:Epoch[1] Batch [8]	Speed: 253.58 samples/sec	lossvalue=0.036286
INFO:root:Epoch[1] Batch [10]	Speed: 248.00 samples/sec	lossvalue=0.037000
INFO:root:Epoch[1] Batch [12]	Speed: 254.51 samples/sec	lossvalue=0.035697
INFO:root:Epoch[1] Batch [14]	Speed: 249.80 samples/sec	lossvalue=0.044685
INFO:root:Epoch[1] Batch [16]	Speed: 250.64 samples/sec	lossvalue=0.041519
INFO:root:Epoch[1] Batch [18]	Speed: 250.69 samples/sec	lossvalue=0.033191
INFO:root:Epoch[1] Batch [20]	Speed: 245.26 samples/sec	lossvalue=0.039150
INFO:root:Epoch[1] Batch [22]	Speed: 250.77 samples/sec	lossvalue=0.037820
INFO:root:Epoch[1] Batch [24]	Speed: 249.07 samples/sec	lossvalue=0.043563
INFO:root:Epoch[1] Batch [26]	Speed: 249.93 samples/sec	lossvalue=0.037720
INFO:root:Epoch[1] Batch [28]	Speed: 248.95 samples/sec	lossvalue=0.034077
INFO:root:Epoch[1] Batch [30]	Speed: 250.06 samples/sec	lossvalue=0.042245
INFO:root:Epoch[1] Batch [32]	Speed: 245.91 samples/sec	lossvalue=0.039231
INFO:root:Epoch[1] Batch [34]	Speed: 251.09 samples/sec	lossvalue=0.047329
INFO:root:Epoch[1] Batch [36]	Speed: 248.13 samples/sec	lossvalue=0.050309
INFO:root:Epoch[1] Batch [38]	Speed: 249.39 samples/sec	lossvalue=0.041643
INFO:root:Epoch[1] Batch [40]	Speed: 250.55 samples/sec	lossvalue=0.040039
INFO:root:Epoch[1] Batch [42]	Speed: 247.48 samples/sec	lossvalue=0.030463
INFO:root:Epoch[1] Batch [44]	Speed: 247.77 samples/sec	lossvalue=0.040647
INFO:root:Epoch[1] Batch [46]	Speed: 250.08 samples/sec	lossvalue=0.032770
INFO:root:Epoch[1] Batch [48]	Speed: 248.69 samples/sec	lossvalue=0.039806
INFO:root:Epoch[1] Batch [50]	Speed: 244.83 samples/sec	lossvalue=0.040060
INFO:root:Epoch[1] Batch [52]	Speed: 251.53 samples/sec	lossvalue=0.035293
INFO:root:Epoch[1] Batch [54]	Speed: 248.79 samples/sec	lossvalue=0.047740
INFO:root:Epoch[1] Batch [56]	Speed: 253.92 samples/sec	lossvalue=0.038006
INFO:root:Epoch[1] Batch [58]	Speed: 254.37 samples/sec	lossvalue=0.044417
INFO:root:Epoch[1] Batch [60]	Speed: 252.30 samples/sec	lossvalue=0.044032
INFO:root:Epoch[1] Batch [62]	Speed: 250.49 samples/sec	lossvalue=0.031793
INFO:root:Epoch[1] Batch [64]	Speed: 250.09 samples/sec	lossvalue=0.036257
INFO:root:Epoch[1] Batch [66]	Speed: 248.02 samples/sec	lossvalue=0.037493
INFO:root:Epoch[1] Batch [68]	Speed: 247.05 samples/sec	lossvalue=0.036472
INFO:root:Epoch[1] Batch [70]	Speed: 248.64 samples/sec	lossvalue=0.035894
INFO:root:Epoch[1] Batch [72]	Speed: 249.16 samples/sec	lossvalue=0.037459
INFO:root:Epoch[1] Batch [74]	Speed: 247.82 samples/sec	lossvalue=0.035527
INFO:root:Epoch[1] Batch [76]	Speed: 249.12 samples/sec	lossvalue=0.034804
INFO:root:Epoch[1] Batch [78]	Speed: 247.95 samples/sec	lossvalue=0.038208
INFO:root:Epoch[1] Batch [80]	Speed: 247.22 samples/sec	lossvalue=0.029900
INFO:root:Epoch[1] Batch [82]	Speed: 247.46 samples/sec	lossvalue=0.035726
INFO:root:Epoch[1] Batch [84]	Speed: 248.89 samples/sec	lossvalue=0.040122
......
INFO:root:Epoch[1332] Batch [12]	Speed: 253.70 samples/sec	lossvalue=0.038092
INFO:root:Epoch[1332] Batch [14]	Speed: 257.98 samples/sec	lossvalue=0.039416
INFO:root:Epoch[1332] Batch [16]	Speed: 262.12 samples/sec	lossvalue=0.035617
INFO:root:Epoch[1332] Batch [18]	Speed: 257.56 samples/sec	lossvalue=0.033298
INFO:root:Epoch[1332] Batch [20]	Speed: 249.12 samples/sec	lossvalue=0.048304
INFO:root:Epoch[1332] Batch [22]	Speed: 266.54 samples/sec	lossvalue=0.036895
INFO:root:Epoch[1332] Batch [24]	Speed: 254.36 samples/sec	lossvalue=0.042113
INFO:root:Epoch[1332] Batch [26]	Speed: 259.29 samples/sec	lossvalue=0.045670
INFO:root:Epoch[1332] Train-lossvalue=0.034580
INFO:root:Epoch[1332] Time cost=19.098
call reset()
eval 5400 images.. 1188000
triplet time stat [3.915402000000001, 43275.00405299998, 3806.9686219999994, 0.0, 0.0, 0.0]
found triplets 1506
seq len 4500
INFO:root:Epoch[1333] Batch [2]	Speed: 262.96 samples/sec	lossvalue=0.031913
INFO:root:Epoch[1333] Batch [4]	Speed: 260.65 samples/sec	lossvalue=0.032299
INFO:root:Epoch[1333] Batch [6]	Speed: 257.78 samples/sec	lossvalue=0.039333
INFO:root:Epoch[1333] Batch [8]	Speed: 258.54 samples/sec	lossvalue=0.038160
INFO:root:Epoch[1333] Batch [10]	Speed: 261.78 samples/sec	lossvalue=0.044814
INFO:root:Epoch[1333] Batch [12]	Speed: 258.56 samples/sec	lossvalue=0.030605
INFO:root:Epoch[1333] Batch [14]	Speed: 258.91 samples/sec	lossvalue=0.029005
INFO:root:Epoch[1333] Batch [16]	Speed: 257.72 samples/sec	lossvalue=0.031869
INFO:root:Epoch[1333] Batch [18]	Speed: 258.70 samples/sec	lossvalue=0.034739
INFO:root:Epoch[1333] Batch [20]	Speed: 258.75 samples/sec	lossvalue=0.036528
INFO:root:Epoch[1333] Batch [22]	Speed: 258.14 samples/sec	lossvalue=0.033395
INFO:root:Epoch[1333] Batch [24]	Speed: 276.79 samples/sec	lossvalue=0.034203
INFO:root:Epoch[1333] Train-lossvalue=0.034203
INFO:root:Epoch[1333] Time cost=16.881
call reset()
eval 5400 images.. 1193400
triplet time stat [3.9160450000000013, 43342.681199999985, 3809.9845319999995, 0.0, 0.0, 0.0]
found triplets 1291
seq len 3870
INFO:root:Epoch[1334] Batch [2]	Speed: 258.00 samples/sec	lossvalue=0.045415
INFO:root:Epoch[1334] Batch [4]	Speed: 260.46 samples/sec	lossvalue=0.030849
INFO:root:Epoch[1334] Batch [6]	Speed: 259.04 samples/sec	lossvalue=0.039335
INFO:root:Epoch[1334] Batch [8]	Speed: 261.49 samples/sec	lossvalue=0.041315
INFO:root:Epoch[1334] Batch [10]	Speed: 251.08 samples/sec	lossvalue=0.039563
INFO:root:Epoch[1334] Batch [12]	Speed: 268.61 samples/sec	lossvalue=0.031839
INFO:root:Epoch[1334] Batch [14]	Speed: 256.37 samples/sec	lossvalue=0.035969
INFO:root:Epoch[1334] Batch [16]	Speed: 254.81 samples/sec	lossvalue=0.039614
INFO:root:Epoch[1334] Batch [18]	Speed: 253.82 samples/sec	lossvalue=0.035065
INFO:root:Epoch[1334] Batch [20]	Speed: 256.22 samples/sec	lossvalue=0.037679
INFO:root:Epoch[1334] Train-lossvalue=0.037679
INFO:root:Epoch[1334] Time cost=14.260
call reset()
eval 5400 images.. 1198800
triplet time stat [3.916269000000001, 43411.275141999984, 3812.9958259999994, 0.0, 0.0, 0.0]
found triplets 1438
seq len 4230
INFO:root:Epoch[1335] Batch [2]	Speed: 261.86 samples/sec	lossvalue=0.045334
INFO:root:Epoch[1335] Batch [4]	Speed: 262.93 samples/sec	lossvalue=0.033386
INFO:root:Epoch[1335] Batch [6]	Speed: 252.48 samples/sec	lossvalue=0.046652
INFO:root:Epoch[1335] Batch [8]	Speed: 257.25 samples/sec	lossvalue=0.043248
INFO:root:Epoch[1335] Batch [10]	Speed: 263.56 samples/sec	lossvalue=0.038918
INFO:root:Epoch[1335] Batch [12]	Speed: 256.78 samples/sec	lossvalue=0.051396
INFO:root:Epoch[1335] Batch [14]	Speed: 257.46 samples/sec	lossvalue=0.037017
INFO:root:Epoch[1335] Batch [16]	Speed: 264.41 samples/sec	lossvalue=0.047573
INFO:root:Epoch[1335] Batch [18]	Speed: 257.33 samples/sec	lossvalue=0.044533
INFO:root:Epoch[1335] Batch [20]	Speed: 259.35 samples/sec	lossvalue=0.052384
INFO:root:Epoch[1335] Batch [22]	Speed: 261.46 samples/sec	lossvalue=0.047916
INFO:root:Epoch[1335] Train-lossvalue=0.047916
INFO:root:Epoch[1335] Time cost=15.572
call reset()
eval 5400 images.. 1204200
triplet time stat [3.916579000000001, 43484.417336999984, 3815.8073949999994, 0.0, 0.0, 0.0]
found triplets 2061
seq len 6120
.......""
"
"I have test the three models in the instruction on the training set ms1m, the three models as follows:
(1). Train ArcFace with LResNet100E-IR. 
(2) Train CosineFace with LResNet50E-IR. 
(3)Train Softmax with LMobileNetE.
and I found with the same training stage (same means haved trained the same dataset with same epoch),different model have different acc on training set, especially model (1),the acc on training set grow so slow.

the result as follows:

lr-batch-epoch: 0.1 11999 0
model  | acc on Lfw | acc on cfp_fp | acc on agedb_30 | Acc on training set
(1)-11999 | 98.76     | 85.73            | 90.57                   | 0.015430
(2)-11999 | 98.45     | 80.66            | 89.18                   | 0.042383
(3)-11999 | 97.71     | 83.64           | 83.03                    | 0.203125
lr-batch-epoch: 0.1 19767 3
(1)-19767 | 99.17     | 87.04           | 94.37                    | 0.041
(2)-19767 | 99.12     | 86.61           | 93.52                    | 0.096
(3)-19767 | 99.12     | 91.86           | 89.95                    | 0.56

           when batch epoch is 11999,From the result above looks like  model(1) get the base result on testing dataset, however the acc only~0.01,sames because the LResNet100E-IR  archtecture design good , only random weight and bias get a good result.
            however when batch epoch is 19767 ,model(1) seems not so good ,so why the trainging set acc on model(1) grow so slow?? Does anyone has good method to improve the acc grow speed?




 "
Is there a list of names for the people in Asian-Celeb (94K ids/2.8M images) dataset?
"Hi,

I am trying to test on the Raspberry pi 3. Compiled and installed mxnet from source with no issue.

import mxnet  is working.

When I try to run test.py in the deploy folder I have below error. An idea ? Is there any body on the run before Rasperypi?


> `python test.py 
> loading /home/pi/Projects/mxnet-3Jan/cpp-package/example/inference/model-r50-am-lfw/ 0
> [13:02:25] /home/pi/Projects/mxnet-3Jan/src/nnvm/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 2129835052 with lower version of MXNet v10500. May cause undefined behavior. Please update MXNet if you encounter any issue
> Traceback (most recent call last):
>   File ""test.py"", line 18, in <module>
>     model = face_model.FaceModel(args)
>   File ""/home/pi/insightface/deploy/face_model.py"", line 53, in __init__
>     self.model = get_model(ctx, image_size, args.model, 'fc1')
>   File ""/home/pi/insightface/deploy/face_model.py"", line 35, in get_model
>     all_layers = sym.get_internals()
>   File ""/home/pi/Projects/mxnet-3Jan/python/mxnet/symbol/symbol.py"", line 677, in get_internals
>     self.handle, ctypes.byref(handle)))
>   File ""/home/pi/Projects/mxnet-3Jan/python/mxnet/base.py"", line 252, in check_call
>     raise MXNetError(py_str(_LIB.MXGetLastError()))
> mxnet.base.MXNetError: [13:02:33] ../include/dmlc/any.h:286: Check failed: type_ != nullptr The any container is empty requested=N4nnvm13VariableParamE
> 
> Stack trace returned 7 entries:
> [bt] (0) /home/pi/Projects/mxnet-3Jan/python/mxnet/../../build/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x38) [0x6e0234f0]
> [bt] (1) /home/pi/Projects/mxnet-3Jan/python/mxnet/../../build/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x30) [0x6e023c44]
> [bt] (2) /home/pi/Projects/mxnet-3Jan/python/mxnet/../../build/libmxnet.so(void dmlc::any::check_type<nnvm::VariableParam>() const+0x184) [0x6fb970bc]
> [bt] (3) /home/pi/Projects/mxnet-3Jan/python/mxnet/../../build/libmxnet.so(nnvm::Symbol::GetInternals() const+0x590) [0x6fb91c2c]
> [bt] (4) /home/pi/Projects/mxnet-3Jan/python/mxnet/../../build/libmxnet.so(MXSymbolGetInternals+0x48) [0x6e05df4c]
> [bt] (5) /usr/lib/arm-linux-gnueabihf/libffi.so.6(ffi_call_VFP+0x54) [0x716d2dd0]
> [bt] (6) /usr/lib/arm-linux-gnueabihf/libffi.so.6(ffi_call+0x12c) [0x716d37ec]
> `"
"hello, train res50 and generate megaface feature,  question as title. ~"
"Hi all,

The new demo is very impressive. I'm interested in the face detection algorithm used in the demo. Is it mtcnn with some extra process (since original mtcnn's result is a bit jumpy), or is it some other algorithm?

And is the time it take to generate this demo longer than the length of the original video (can this detection and recognition algorithm work in real time)?"
""
"CASIA-Webface, training acc is 0.99, but verification acc is 0.60+.
我看之前有类似的issue，#478, 但我用下面代码切分train和val，这应该可以让验证集分布和训练集一样
```
tmp = random.sample(range(a, b), (b-a)//10)
train_idx += [x for x in range(a, b) if x not in tmp]
val_idx += tmp
```
模型resnet50，per-batch-size=128，loss-type=4, 其他参数都没有变过；
需要调整下margin-m吗？0.35貌似也可以让training收敛，这是过拟合？
@nttstar "
""
""
"*** Error in `python': free(): invalid pointer: 0x00007f1ed814cb50 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f1f5d6da7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f1f5d6e337a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f1f5d6e753c]
/usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorReshape+0x1852)[0x7f1f13593ce2]
/usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c)[0x7f1f44076e40]
/usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb)[0x7f1f440768ab]
/usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x48f)[0x7f1f442863df]
/usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(+0x11d82)[0x7f1f4428ad82]
python(PyEval_EvalFrameEx+0x578d)[0x4c166d]
python(PyEval_EvalCodeEx+0x306)[0x4b9b66]

i have cuda 9 installed and i use mxnet-cu90 using pip

please help me"
"请问 https://github.com/KangKangLoveCat/insightface_ncnn 可以算Third-party Re-implementation么？
谢谢。"
Help~
你好，请问训练是否使用了CELEB-500K数据集，使用的话数据是不是做了清洗，能否提供一下清洗后的信息？
"I am trying to replicate the toy example plots in order to see class center vectors on a circle using 2D features. I am currently using resnet backbone + custom head with a weight matrix (2 - n_features, 10 n_classes) with MNIST data but can't achieve very good results as I would using a larger feature size like 512. What am I am missing in order to reproduce those 2D toy plots in the paper?

Thanks"
"Hello, I got pictures of more than 200G to be my dataset. Three days before I use face2rec.py to translate them into rec files. But until now I still not finish my translation. How much time did you consume when you translate VGG data into rec file?"
"jack,

  Can you share instructions of how to run mobilenetface with TVM on RK3399?
I have a RK3399 board and TVM is installed already and like to try it.

Thanks,"
"Hello!

I am using your age-gender prediction model as baseline for my age estimation project.
I want to draw a ROC-curve for your model, but I can't understand how to proceed from fc-layer's output to array of probabilities for each class (I want p[i] to be the probability that sample X complains to class with label i).

I tried to get softmax for every age-tuple in fc-layer's output. I thought that first elements in tuples will give me the cumulative distribution function. But occasionally i found that it was a wrong way (see picture). This distribution is totally not the CDF.

Is it possible to make proceeding from fc-layer's output to array of probabilities?
Did you draw the ROC-curve for your model?
Is there any paper about your age-gender prediction implementation?
What is the physical meaning of last fc layer?

![default](https://user-images.githubusercontent.com/45030681/49937292-56aed900-fee7-11e8-9ec2-52bfebb5fd93.png)
"
"Hi everyone.
I am running alignment with 3d_i5 model. However, I'm facing 3d_i5 model not found error. Can you help me resolve it? Thanks in advance."
"Dear insightface:
  Could you tell me what you did with the lfw dataset? I have different verification accuracy rates on the lfw dataset given by facenet.
Thank you."
""
""
"hi, excellent work! 
but i find that the bin data of cfp-ff and cfp-fp is different in face_emore file and face_ms1m_112x112 file.
Maybe the alignment is different? i use the same model to test and the acc is different.
thanks."
"is there a pretrained model for finception_resnet_v2, thank you~"
"An instance of FaceImageIter with shuffle works well, however, an instance without shuffle would throw an error because label in dataset is a list rather than a number.(rec dataset is generated with face2rec.py)

When Shuffle=True, in FaceImageIter.next_sample(), 
`if not isinstance(label, numbers.Number):
                label = label[0]`
ensure label must be a number.

When shuffle=False, the clause won't be executed, `batch_label[i][:] = label` in FaceImageIter.next() would raise an exception as `operands could not be broadcast together with remapped shapes [original->remapped]: (2,) and requested shape (1,)`"
"Hi，thanks for share your work. When i used this code to get face embeddings for a large number of  images(~4w) continuously, the GPU memory usage are always increased until it‘s full, and then repeat this process. (ps. the images' shorter side are resized to a fixed value)
Is this normal? How to solve it if not.

![image](https://user-images.githubusercontent.com/41245857/49555026-35734900-f939-11e8-8311-53c64fc6ee4b.png)
![image](https://user-images.githubusercontent.com/41245857/49555039-458b2880-f939-11e8-8f45-a53c7c47e8c3.png)
"
"Hi,

Is there anybody can run the facerecognition (ie: deploy/test.py) on any Edge devices like Raspberry 3 B+ pr Hikey970 ? Any experience or benchmark ?

What would be the suggestion for Egde Device for det & recognition ?

Best
"
"Hi,thanks for your sharing work. Can you tell me what is the convergence loss when you train the network? When I trained the SE-res50 combining with the arcloss with ms1m dataset the loss convergence to 1.5.
When I trained the SE-res50 combining with the arcloss with ms1m+vgg dataset the loss convergence to 4.1. I am not sure to train convergencely. Is it normal? Is it because the ""--margin- m"" ot ""the loss type =4 "" need to step-by-step training like this ""-margin- m=0.0"",""-margin- m=0.03"", ""-margin- m=0.05"",......""-margin- m=0.5""? Thanks a lot."
估计总有人会提出这个问题，不如我问了吧。有人尝试过group normalization了吗？
"[11:07:42] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:109: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
Traceback (most recent call last):
  File ""train_softmax.py"", line 503, in <module>
    main()
  File ""train_softmax.py"", line 500, in main
    train_net(args,)
  File ""train_softmax.py"", line 494, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/wenguang.wang/anaconda3/envs/python2/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 526, in fit
    self.update()
  File ""/home/wenguang.wang/anaconda3/envs/python2/lib/python2.7/site-packages/mxnet/module/module.py"", line 664, in update
    self._kvstore, self._exec_group.param_names)
  File ""/home/wenguang.wang/anaconda3/envs/python2/lib/python2.7/site-packages/mxnet/model.py"", line 153, in _update_params_on_kvstore
    kvstore.push(name, grad_list, priority=-index)
  File ""/home/wenguang.wang/anaconda3/envs/python2/lib/python2.7/site-packages/mxnet/kvstore.py"", line 234, in push
    self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))
  File ""/home/wenguang.wang/anaconda3/envs/python2/lib/python2.7/site-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [11:08:07] src/storage/./pooled_storage_manager.h:119: cudaMalloc failed: out of memory
"
"when running insightface/gender-age/test.py,the following error happens.
It is caused by Mtcnn model failing to locate the face.Does anyone have the same problem?Appreciate your reply.Thanks a lot.

File ""/home/ndir/.local/lib/python2.7/site-packages/mxnet/module/module.py"", line 606, in forward
    new_data_shapes = tuple(i.shape for i in data_batch.data)
AttributeError: 'NoneType' object has no attribute 'data'

environment：
cuda9.0+python2.7+mxnet-cu90"
""
"Recently www.iqiyi.com released a great video person dataset called [IQIYI_VID](http://challenge.ai.iqiyi.com/detail?raceId=5afc36639689443e8f815f9e) and also launched a person search competition on it. We finally got rank 1st(with team name WitcheR) by using Arcface models.  It is a very large and real dataset and worth trying to verify your face model accuracy precisely.  

~~Our solution will open source soon here after PRCV2018 conference.~~
You can download our code [here](https://pan.baidu.com/s/1d50eJ3efwpo7aadZU2wY6Q)

Development leaderboard is now available so anyone can submit your prediction to get your mAP. Leave messages here if you have any problem."
""
""
""
Hi，how can I do if  I want to train a arcface loss model with multi-inputs？
4 P40 GPU，MS-1M，batch_size：128x4
"After got the ""faces_emore""  , I extracted all the images from it and  used the model named ""LResNet100E-IR,ArcFace@ms1m-refine-v2""  to get their embeddings . For each id ,I calculated and normalized its mean embedding , however there were some ids whose  cosine distance of mean embeddings was around 0.9(very close) . after my checking ,they were actually the same person!
![image](https://user-images.githubusercontent.com/16490080/48110550-c6e29300-e287-11e8-92e2-9c5fd206bf16.png)
"
"Hi, thanks for your noise text file. I found that Peggy_McCay_48889.png, which is not included in facescrub_noises.txt,, is not photo of Peggy_McCay. It's Aneta Corsaut. Can you update the facescrub_noise.txt?
."
"Hi,nice work. I am trying to detect  and recognize faces from a video. What changes should I make or how should I proceed to get the best possible performance. Thanks."
""
""
"i am trying to use 'test.py' to infer the original lfw images, but 'model.get_input(img)' sometimes outputs 'None', meaning **Mtcnn model fails to locate the face and crop it**. there are several hundreds of images among Ifw that Mtcnn model failed to detect the face.

any idea that i can always manage to get the cropped face? or if i just resize the original Ifw images to 112*112 without face alignment, dose this affect the insightface performance a lot?"
"In deploy/face_embedding.py, the inference code is like this,
""
      input_blob = np.expand_dims(aligned, axis=0)
      data = mx.nd.array(input_blob)
      db = mx.io.DataBatch(data=(data,))
      self.model.forward(db, is_train=False)
      _embedding = self.model.get_outputs()[0].asnumpy()
""
Now I get a lot of pictures to get embeddings, how can I do batch inference to accelerate the whole process."
""
"
![image](https://user-images.githubusercontent.com/16372054/47849157-9fb13f00-de0b-11e8-8a08-83935ba43eb6.png)
![image](https://user-images.githubusercontent.com/16372054/47849177-ab9d0100-de0b-11e8-8e1e-6b80642ff382.png)
![image](https://user-images.githubusercontent.com/16372054/47849194-b9eb1d00-de0b-11e8-94f1-994fed78a27b.png)


"
"Hi,

I am porting to inference to c++ and successfully move forwarding.

I need to understan what I need to be take steps before I send the detected faces to the model?

I saw in python : 

>   def get_input(self, face_img):
    ret = self.detector.detect_face(face_img, det_type = self.args.det)
    if ret is None:
      return None , None
    bbox, points = ret
    if bbox.shape[0]==0:
      return None
    bbox = bbox[0,0:4]
    points = points[0,:].reshape((2,5)).T
    #print(bbox)
    #print(points)
    nimg = face_preprocess.preprocess(face_img, bbox, points, image_size='112,112')
    nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)
    aligned = np.transpose(nimg, (2,0,1))
    return aligned , points

so:
1- detect face
2- nimg = face_preprocess.preprocess(face_img, bbox, points, image_size='112,112')
3- nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)
 4- aligned = np.transpose(nimg, (2,0,1))

also I found FacePreprocess.h . in the cppalign folder.

Is this FacePreprocess.h is enough for preprocessing before sending get_features . ?

"
"
![image](https://user-images.githubusercontent.com/16372054/47636844-bb5ee000-db94-11e8-8e68-8ada4f74fff2.png)
"
"I want to training mobilefacenet with image size 160x160. I have modified mobilefacenet structure, but the data size is 112x112.
How to change size of dataset in Dataset Zoo?
Thanks~"
"I am running dataset_merge.py in order to remove duplicate images from two datasets. 
I wonder if I can use any pre-trained model? 
when I download LResNet50E-IR model and put it in the folder ""../model/LResNet50E-IR""
do I need to add epoch number at the end of model path or I have to change model name in ""LResNet50E-IR, 50"".
I have tried both cases and got the following error:


loading models/model-r50-am-lfw/model-symbol 50
Traceback (most recent call last):
  File ""src/data/dataset_merge.py"", line 292, in <module>
    main(args)
  File ""src/data/dataset_merge.py"", line 102, in main
    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/model.py"", line 438, in load_checkpoint
    symbol = sym.load('%s-symbol.json' % prefix)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 2594, in load
    check_call(_LIB.MXSymbolCreateFromFile(c_str(fname), ctypes.byref(handle)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [09:43:54] src/io/local_filesys.cc:199: Check failed: allow_null  LocalFileSystem::Open ""models/model-r50-am-lfw/model-symbol-symbol.json"": No such file or directory

Thanks for providing such a great model.
"
"use 10000 pictures test  this project

strart program  memory used about 1.4g and  display card memory 2g
 
The program runs for a period of time. 

strart program  memory used about  200M and  display card memory  about 3g.

what's happened???   very confusing

"
"In this project, there is a folder named 'losses'. But in this folder, i can only find the center loss. Where can i find the source code of the insightface loss and the sphereface loss in this project."
"As noted in  README.md :

Training Data
All face images are aligned by MTCNN and cropped to 112x112:
Refined-MS1M@BaiduDrive, Refined-MS1M@GoogleDrive

Just wondering is the  Refined-MS1M a totally clean dataset？ is there any or how much noise  in it？ "
"CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir /workspace/dataset/faces_ms1m_112x112  --prefix ../model-r100 --per-batch-size 64
[14:46:53] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:130: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:Epoch[0] Batch [20]	Speed: 1195.56 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [40]	Speed: 1277.10 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [60]	Speed: 1268.39 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [80]	Speed: 1306.79 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [100]	Speed: 1279.49 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [120]	Speed: 1238.91 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [140]	Speed: 1300.48 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [160]	Speed: 1313.65 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [180]	Speed: 1318.43 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [200]	Speed: 1309.14 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [220]	Speed: 1295.77 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [240]	Speed: 1291.87 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [260]	Speed: 1291.32 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [280]	Speed: 1300.03 samples/sec	acc=0.000000"
"Hi there,

There is something wrong during the mobile face net training. I followed the set up module in ReadMe.

Training command line:
python -u train_softmax.py --network m1 --loss-type 0 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-m1-softmax

Error:

Traceback (most recent call last):
  File ""train_softmax.py"", line 503, in <module>
    main()
  File ""train_softmax.py"", line 500, in main
    train_net(args)
  File ""train_softmax.py"", line 494, in train_net
    epoch_end_callback = epoch_cb )
  File ""C:\Program Files\Anaconda3\envs\py27\lib\site-packages\mxnet\module\base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""C:\Program Files\Anaconda3\envs\py27\lib\site-packages\mxnet\module\module.py"", line 429, in bind
    state_names=self._state_names)
  File ""C:\Program Files\Anaconda3\envs\py27\lib\site-packages\mxnet\module\executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""C:\Program Files\Anaconda3\envs\py27\lib\site-packages\mxnet\module\executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""C:\Program Files\Anaconda3\envs\py27\lib\site-packages\mxnet\module\executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""C:\Program Files\Anaconda3\envs\py27\lib\site-packages\mxnet\symbol\symbol.py"", line 1528, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 3, 112, 112)
softmax_label: (1,)
[18:05:24] C:\Jenkins\workspace\mxnet-tag\mxnet\src\storage\storage.cc:119: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: no CUDA-capable device is detected

What did i miss? Thank you in advance.
"
"For few-shot learning tasks like IDCard<->Camera face verification(identification), we only have two face images for each person in most cases for training.  Under such situation, metric learning approaches can be tried such as tripletloss. 

**STEPS:**

1. Prepare insightface '.rec' dataset from your IDCard/camera face images.
2. Finetuning pretrained models with tripletloss, for example:

```
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_triplet.py --data-dir $DATA_DIR \
  --network ""$NETWORK"" --lr 0.005 --pretrained ""$PRETRAINED"" --per-batch-size 60 
```

We use GPU to do the semi-hard mining so training will be fast.

**RESULTS:**

We have a private IDCard/Camera face image dataset with 220K identities. Each person has two or more photos, one from IDCard and the others from camera. Split it as 8-2 for training(176K IDs) and testing(45K IDs).  We report top1 accuracy and TAR vs FAR for 1:N identification task(N=45K).
(Note that we do not use idcard training data in Model-A and Model-B)

|          | DESC.                                    | Rank-1               | TAR@FAR=1e-3 | TAR@FAR=1e-4 |
| -------- | ---------------------------------------- | -------------------- | ------------ | ------------ |
| Model-A1 | LResNet100E trained on ms1m-v1 with Softmax loss | 26.9%                | 0.3%         | 0.06%        |
| Model-A2 | LResNet100E trained on ms1m-v1 with ArcFace loss | 70.7%                | 17%          | 8%           |
| Model-A3 | LResNet100E trained on ms1m-v2 with ArcFace loss | 76.8%                | 21%          | 9%           |
| Model-B  | LResNet100E trained on (ms1m-v2+Glint-Asia) with ArcFace loss | 82.4%                | 33%          | 16%          |
| Model-C  | Triplet-loss finetuning on Model-B       | 95.2%(still ongoing) |  78%            |  26%            |"
"Link: [here](https://pan.baidu.com/s/1f8RyNuQd7hl2ItlV-ibBNQ)

How to use:  refer to `get_ga()` method [here](https://github.com/deepinsight/insightface/blob/master/deploy/face_model.py#L95)

Any usage, performance and accuracy issue can be posted here."
"1. Download dataset from [http://trillionpairs.deepglint.com/data](http://trillionpairs.deepglint.com/data) (after signup). `msra` is a cleaned subset of MS1M from glint while `celebrity` is the asian dataset.
2. Generate lst file by calling `src/data/glint2lst.py`. For example:
```
python glint2lst.py /data/glint_data msra,celebrity > glint.lst
```

or generate the asian dataset only by:
```
python glint2lst.py /data/glint_data celebrity > glint_cn.lst
```

3. Call face2rec2.py to generate .rec file.
4. Merge the dataset with existing one by calling `src/data/dataset_merge.py` without setting param _model_ which will combine all IDs from those two datasets.


Finally you will get a dataset contains about 180K IDs.

Use `src/eval/gen_glint.py` to prepare test feature file by using pretrained insightface model.

You can also post your private testing results here."
"A new training dataset 'insightv2'(code name _emore_) (still largely based on ms1m) is available at [baiducloud](https://pan.baidu.com/s/1S6LJZGdqcZRle1vlcMzHOQ) and [onedrive](https://sjtueducn-my.sharepoint.com/:u:/g/personal/gdshen_sjtu_edu_cn/EQQrjHqTgnFNjuNu9XC6iPcBiNUD6_kNBCKikJV9ZMtngg?e=sdBzq6) (from @gdshen ) which can achieve a better accuracy easily. I hope anybody who uses _insightface_ can post your training accuracy and detail here to show the strength of our network backbone, dataset and loss function. (Currently I will not provide pretrained models)
(more training dataset contribution is welcome, email me pls~)

The format may like below(take from one of my experiments):
1. dataset:  emore (or faces_ms1m for the old one)
2. network backbone: r50 ( res_unit=3, output=E, emb_size=512, prelu )
3. loss function: arcface(m=0.5)
4. training pipeline:  straightforward (lr drop at 100K, 140K, 160K), batch-size:512
5. Highest LFW: 99.83%;   Highest CFP_FP:  97.67;   Highest AgeDB30:  98.10"
""
"Hi, 
I want to train arcface_torch with faces_emore dataset. I currently have 13GB free RAM but when the train process starts, RAM goes up and my systems gets freeze. 

I reduced the batch-size to 2 but it did not help. What can I do? is it possible to train the model on single gpu (1080 ti 11GB RAM) and with a system with only 16 GB of RAM?

```
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:12581
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_wks702uk/none_sjd6axom
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/home/~/2TB/venv_main_38/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=12581
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_wks702uk/none_sjd6axom/attempt_0/0/error.json
Training: 2022-11-29 17:02:55,269-rank_id: 0
/home/~/2TB/venv_main_38/lib/python3.8/site-packages/torch/nn/parallel/distributed.py:1443: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
Training: 2022-11-29 17:03:18,058-: margin_list              [1.0, 0.5, 0.0]
Training: 2022-11-29 17:03:18,059-: network                  mbf
Training: 2022-11-29 17:03:18,059-: resume                   False
Training: 2022-11-29 17:03:18,059-: save_all_states          False
Training: 2022-11-29 17:03:18,059-: output                   work_dirs/ms1mv2_mbf
Training: 2022-11-29 17:03:18,059-: embedding_size           512
Training: 2022-11-29 17:03:18,059-: sample_rate              1.0
Training: 2022-11-29 17:03:18,059-: interclass_filtering_threshold0
Training: 2022-11-29 17:03:18,059-: fp16                     True
Training: 2022-11-29 17:03:18,059-: batch_size               2
Training: 2022-11-29 17:03:18,059-: optimizer                sgd
Training: 2022-11-29 17:03:18,059-: lr                       0.1
Training: 2022-11-29 17:03:18,059-: momentum                 0.9
Training: 2022-11-29 17:03:18,059-: weight_decay             0.0001
Training: 2022-11-29 17:03:18,059-: verbose                  2000
Training: 2022-11-29 17:03:18,059-: frequent                 10
Training: 2022-11-29 17:03:18,059-: dali                     False
Training: 2022-11-29 17:03:18,059-: gradient_acc             1
Training: 2022-11-29 17:03:18,060-: seed                     2048
Training: 2022-11-29 17:03:18,060-: num_workers              2
Training: 2022-11-29 17:03:18,060-: rec                      /home/~/faces_emore
Training: 2022-11-29 17:03:18,060-: num_classes              85742
Training: 2022-11-29 17:03:18,060-: num_image                5822653
Training: 2022-11-29 17:03:18,060-: num_epoch                40
Training: 2022-11-29 17:03:18,060-: warmup_epoch             0
Training: 2022-11-29 17:03:18,060-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2022-11-29 17:03:18,060-: total_batch_size         2
Training: 2022-11-29 17:03:18,060-: warmup_step              0
Training: 2022-11-29 17:03:18,060-: total_step               116453040
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
torch.Size([12000, 3, 112, 112])
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
```"
""
"Hi,

I have tried with 3 different emails but I could not receive confirmation email. My username is Datamoon.

Thanks"
"When I run the [example](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/train.py) , I find that the output shows that the learning rate keeps increasing, even greater than 1. 

**python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=""127.0.0.1"" --master_port=12581 train.py configs/myconfig.py**
```
myconfig.py：
from easydict import EasyDict as edict
config = edict()
config.margin_list = (1.0, 0.5, 0.0)
config.network = ""r50""
config.resume = False
config.output = None
config.embedding_size = 512
config.sample_rate = 1.0
config.fp16 = True
config.momentum = 0.9
config.weight_decay = 5e-4
config.batch_size = 128
config.lr = 0.1
config.verbose = 2000
config.dali = False
config.rec = ""./data/DigiFace""
config.num_classes = 100 #93431
config.num_image = 500 # 5179510
config.num_epoch = 20
config.warmup_epoch = 0
config.val_targets = ['lfw', 'cfp_fp', ""agedb_30""]
```


**Terminal output:**
```
Training: 2022-11-08 08:12:34,946-Speed 280.12 samples/sec   Loss nan   LearningRate 5.852250   Epoch: 0   Global Step: 520   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:12:39,529-Speed 279.35 samples/sec   Loss nan   LearningRate 6.110028   Epoch: 0   Global Step: 530   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:12:44,112-Speed 279.36 samples/sec   Loss nan   LearningRate 6.373361   Epoch: 0   Global Step: 540   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:12:48,695-Speed 279.31 samples/sec   Loss nan   LearningRate 6.642250   Epoch: 0   Global Step: 550   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:12:53,285-Speed 278.95 samples/sec   Loss nan   LearningRate 6.916694   Epoch: 0   Global Step: 560   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:12:57,881-Speed 278.56 samples/sec   Loss nan   LearningRate 7.196694   Epoch: 0   Global Step: 570   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:02,469-Speed 279.01 samples/sec   Loss nan   LearningRate 7.482250   Epoch: 0   Global Step: 580   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:07,067-Speed 278.44 samples/sec   Loss nan   LearningRate 7.773361   Epoch: 0   Global Step: 590   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:11,665-Speed 278.46 samples/sec   Loss nan   LearningRate 8.070028   Epoch: 0   Global Step: 600   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:16,262-Speed 278.47 samples/sec   Loss nan   LearningRate 8.372250   Epoch: 0   Global Step: 610   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:20,866-Speed 278.10 samples/sec   Loss nan   LearningRate 8.680028   Epoch: 0   Global Step: 620   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:25,464-Speed 278.43 samples/sec   Loss nan   LearningRate 8.993361   Epoch: 0   Global Step: 630   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:30,061-Speed 278.47 samples/sec   Loss nan   LearningRate 9.312250   Epoch: 0   Global Step: 640   Fp16 Grad Scale:  0   Required: -0 hours
Training: 2022-11-08 08:13:34,664-Speed 278.11 samples/sec   Loss nan   LearningRate 9.636694   Epoch: 0   Global Step: 650   Fp16 Grad Scale:  0   Required: -0 hours
```"
"So I just executed the 'run_experiment.py' but  when the program started 'subprocess.Popen' it got the error:
Permission denied: '../bin/Identification'
what can I do? I used mac os"
"请教作者两个问题：

1、如题，ViT的训练为什么没使用Dali加载数据？是会影响效果吗？

2、如果我只有单机8卡的机器，训练ViT是否使用了梯度累计对结果大概有多大影响？根据作者的实验，Single-Host GPU训练实验中，ViT-T使用了梯度累计而ViT-B没使用梯度累计，相比64卡训练差异都比较小（MFR-ALL分别为92.04→92.24和97.16→97.42），从这个结果如何看代梯度累计的影响呢？

因计算资源较少，希望从作者的实验中多看到一些结论。非常感谢！"
"python train.py configs/ms1mv2_mbf                                              
...
Training: 2022-10-27 04:59:19,503-Speed 927.28 samples/sec   Loss 28.4681   LearningRate 0.098250   Epoch: 0   Global Step: 2000   Fp16 Grad Scale: 262144   Required: 58 hours
testing verification..
(12000, 512)
infer time 33.411153999999975
Traceback (most recent call last):
  File ""train.py"", line 212, in <module>
    main(parser.parse_args())
  File ""train.py"", line 176, in main
    callback_verification(global_step, backbone)
  File ""/workspace/insightface/recognition/arcface_torch/utils/utils_callbacks.py"", line 55, in __call__
    self.ver_test(backbone, num_update)
  File ""/workspace/insightface/recognition/arcface_torch/utils/utils_callbacks.py"", line 30, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(
  File ""/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/workspace/insightface/recognition/arcface_torch/eval/verification.py"", line 272, in test
    _, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=nfolds)
  File ""/workspace/insightface/recognition/arcface_torch/eval/verification.py"", line 191, in evaluate
    val, val_std, far = calculate_val(thresholds,
  File ""/workspace/insightface/recognition/arcface_torch/eval/verification.py"", line 151, in calculate_val
    f = interpolate.interp1d(far_train, thresholds, kind='slinear')
  File ""/opt/conda/lib/python3.8/site-packages/scipy/interpolate/_interpolate.py"", line 571, in __init__
    self._spline = make_interp_spline(xx, yy, k=order,
  File ""/opt/conda/lib/python3.8/site-packages/scipy/interpolate/_bsplines.py"", line 1252, in make_interp_spline
    raise ValueError(""Expect x to not have duplicates"")
ValueError: Expect x to not have duplicates
"
之前partialFC是分步backward，现在是直接loss.backward了，这两种方式有区别吗，为啥不用分步了
Hi what distance metric used in web demo and did you used normalized vectors or did you reduced the dimensions of the embeddings while verifying? 
"我理解 partial_fc_v2里边参与计算的仍然是采样后的sub_weight，而不是原始的weight，那么优化器直接对weight不会有问题吗？

我仿写的代码报了 AttributeError: 'generator' object has no attribute 'grad'。

想请问下作者这是为什么？和pytorch的版本有关系吗？谢谢。"
"I learn the code in this repository : insightface/examples/demo_analysis.py
Then, I find the source code in python library installed via 'pip install -U insightface' : insightface/model_zoo/retinaface.py
In retinaface.py, I'm not sure the ""retinaface.detect"" function work. 
Now I know the det_10g.onnx give 9 returns, and three of them is about score, three of them is about distance, temporary I ignore the  kpss.
I don't know about the det_thresh's meaning and why feat_stride_fpn defined as [8, 16, 32]?
I think I maybe to learn about how det_10g.onnx to build can solve my problem.
I hope to get help."
"Hello @nttstar , thanks for your excellent solution of JMLR.

The network predicts both the **3D vertexs** in the world space and **2D landmarks** on the image plane, but the ground-truth only provides the 3D points (verts3d) and camera extrinsic (R_t).

So how to convert the 3D points into image plane for 2D landmark supervision?"
"Hi there,

I was going through the [`arcface_loss`](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/losses.py) implementation in this repo and had one query regarding the same.

In the `forward` method, there is this piece of code

```python
if self.easy_margin:
    final_target_logit = torch.where(
        target_logit > 0, cos_theta_m, target_logit)
else:
    final_target_logit = torch.where(
        target_logit > self.theta, cos_theta_m, target_logit - self.sinmm)
```

We need to set the `target_logit` to `cos(theta_{yi} + margin)` unconditionally right? 

In the [`arcface_paper`](https://arxiv.org/pdf/1801.07698.pdf), equation 3 in section 3.1 defines arcface loss as follows.

![image](https://user-images.githubusercontent.com/45599762/194217164-b767db30-febf-4492-b1a3-d3f1bfb03577.png)

As per this loss function the target logit is set to `cos(theta_{yi} + margin)` scaled appropriately at the end and the rest is as is. Where does the `sin_margin * margin` and the condition `target_logit > self.thera` come from?

If someone could help me understand this section, I would be very grateful to you.

Regards,
Vinayak.
"
"Hello, when I used your JMLR code (without any modification) to train on the WCPA dataset, I couldn't get the correct result, especially when the face is sideways, the effect is shown below.  I have done a complete walkthrough of your code, but have found no problems. can you give me some suggestions？Or does the code have any special requirements for the environment? Thank you very much.
"
"From _face_align.py_, the ```norm_crop``` function takes in images and landmarks. Any advice on how to go from faces provided by ```app.get(img)``` to the norm_crop function? "
Can you add (or point me to) the code/model used for the face swapping demo?  I'd like to do inference tests vs existing models like Simswap and MegaFS.  
想用自己的数据集做训练，如何制作自己的数据集呢？
"Hi,

In standard ResNet block, there is a relu at the end of the block (https://github.com/open-mmlab/mmclassification/blob/master/mmcls/models/backbones/resnet.py#L131), but you don't have that (https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/backbones/iresnet.py#L57). I tried to add relu but got a very bad result on my own dataset.

Any reason you made the change?"
"想问一下数据预处理是怎么做的，有相应的代码吗？
就是有一批人脸数据，怎么生成的训练数据和测试数据
希望能有个完整的流程
谢谢"
"Hi , thank you for sharing your work
I am trying to run the inference
but getting this error
File ""insightface\reconstruction\jmlr\inference_simple.py"", line 75, in __init__
    backbone_pth = sorted(ckpts)[-1]
IndexError: list index out of range
"
"Hi All,

Could anyone please answer this at the earliest?

Is there image name mapping between the two files ijbc_face_tid_mid.txt and ijbc_metadata_with_age.csv? The reason is, for the same image, I want alignment coordinates (from ijbc_name_5pts_score.txt which has one-one mapping with ijbc_face_tip_mid.txt) from the file ijbc_face_tid_mid.txt metadata from the ijbc_metadat_with_age.csv file. Template_id/Sighting_id isn't helping alone as multiple sighting_ids are possible.

Thanks,"
"I found that partial_fc use the polyScheduler to adjust the learning rate. And the learning rate is changed by epoch(not iterations):
```python
def get_lr(self):
        if self.last_epoch == -1:
            return [self.warmup_lr_init for _ in self.optimizer.param_groups]
        if self.last_epoch < self.warmup_steps:
            return self.get_warmup_lr()
        else:
            alpha = pow(
                1
                - float(self.last_epoch - self.warmup_steps)
                / float(self.max_steps - self.warmup_steps),
                self.power,
            )
            return [self.base_lr * alpha for _ in self.optimizer.param_groups]
```
So the learning rate should be changed with epoch. Am I right? However, I find that the learning rate is actually changed with iterations, why ?
![image](https://user-images.githubusercontent.com/26771037/187022311-9aee7cdc-3906-4756-9687-baf8d139769e.png)
"
"there are  image mask and cameras.npz and mesh in raw data ,
I just wonder how to generate these files?
@XingyuRenSJTU "
"Line188 partial_fc.py and Line139 in partial_fc_v2.py is

`_list_embeddings = AllGather(local_embeddings, *_gather_embeddings)`

Why embeddings needs gradient across multi Gpus？"
"""we attach a video with high-resolution rotating rendered mesh
results, which showcases the quality and accuracy of the texture completion and
geometric details acquisitio""

thanks for your wonderful work!
BTW., where can I find the video?"
"Is there a way to set a threshold for detection? Currently I only get bboxes with 'det_score' > 0.5.

I would like to obtain results that are under that threshold. I couldn't find any example or documentation to resolve this. I already tried passing det_score=0.02 to FaceAnalysis contructor without any luck."
""
"hi

i'm trying to export pytorch model to onnx using this [script](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/torch2onnx.py) but it doesn't work

```
 python3 torch2onnx.py backbone.pth --network r100 --output model.onnx
```

the model i'm using is `MS1MV3/backbone.pth`

this is the error

```
Traceback (most recent call last):
  File ""/usr/lib/python3.6/tarfile.py"", line 189, in nti
    n = int(s.strip() or ""0"", 8)
ValueError: invalid literal for int() with base 8: '2\nq\x03((X\x07'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.6/tarfile.py"", line 2299, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File ""/usr/lib/python3.6/tarfile.py"", line 1093, in fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  File ""/usr/lib/python3.6/tarfile.py"", line 1035, in frombuf
    chksum = nti(buf[148:156])
  File ""/usr/lib/python3.6/tarfile.py"", line 191, in nti
    raise InvalidHeaderError(""invalid header"")
tarfile.InvalidHeaderError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 555, in _load
    return legacy_load(f)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 466, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File ""/usr/lib/python3.6/tarfile.py"", line 1591, in open
    return func(name, filemode, fileobj, **kwargs)
  File ""/usr/lib/python3.6/tarfile.py"", line 1621, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File ""/usr/lib/python3.6/tarfile.py"", line 1484, in __init__
    self.firstmember = self.next()
  File ""/usr/lib/python3.6/tarfile.py"", line 2311, in next
    raise ReadError(str(e))
tarfile.ReadError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""torch2onnx.py"", line 53, in <module>
    convert_onnx(backbone_onnx, input_file, args.output, simplify=args.simplify)
  File ""torch2onnx.py"", line 14, in convert_onnx
    weight = torch.load(path_module)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 386, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File ""/usr/local/lib/python3.6/dist-packages/torch/serialization.py"", line 559, in _load
    raise RuntimeError(""{} is a zip archive (did you mean to use torch.jit.load()?)"".format(f.name))
RuntimeError: backbone.pth is a zip archive (did you mean to use torch.jit.load()?)
```

and this is my env using `FROM nvidia/cuda:11.4.0-devel-ubuntu18.04` as base image
```
asn1crypto==0.24.0
attrs==22.1.0
awscli==1.18.69
blessed==1.19.1
botocore==1.16.19
certifi==2018.1.18
chardet==3.0.4
colorama==0.3.7
cryptography==2.1.4
cycler==0.11.0
decorator==4.4.2
distro==1.7.0
docutils==0.14
easydict==1.9
enforce==0.3.4
enlighten==1.10.2
flatbuffers==2.0
idna==2.6
imageio==2.15.0
importlib-metadata==4.8.3
iniconfig==1.1.1
jmespath==0.9.3
joblib==1.1.0
keyring==10.6.0
keyrings.alt==3.0
kiwisolver==1.3.1
matplotlib==3.3.4
networkx==2.5.1
numpy==1.19.5
onnx==1.11.0
onnxruntime-gpu==1.10.0
opencv-python==4.1.0.25
packaging==21.3
Pillow==8.4.0
pluggy==1.0.0
prefixed==0.3.2
protobuf==3.19.4
py==1.11.0
pyasn1==0.4.2
pycrypto==2.6.1
PyGObject==3.26.1
pyparsing==3.0.9
pytest==7.0.1
python-dateutil==2.8.2
PyWavelets==1.1.1
pyxdg==0.25
PyYAML==6.0
requests==2.18.4
roman==2.0.0
rsa==3.4.2
s3transfer==0.3.3
scikit-build==0.11.1
scikit-image==0.15.0
scikit-learn==0.24.1
scipy==1.5.4
SecretStorage==2.3.1
six==1.11.0
terminaltables==3.1.0
threadpoolctl==3.1.0
tomli==1.2.3
torch==1.2.0
tqdm==4.32.1
typing_extensions==4.1.1
urllib3==1.22
wcwidth==0.2.5
wrapt==1.14.1
zipp==3.6.0
```

thank you"
"Hello,

I am training [Resnet34, MS1MV3, Arcface]. Since you have kindly provided the exact pretrained weights of the backbone, I only need to first freeze the pretrained backbone train the FC layer, and then finetune the whole network with the Arcface loss and my other custom losses.

During the training of the first stage (freeze pretrained backbone, train FC), I only use the [MXFaceDataset](https://github.com/deepinsight/insightface/blob/babb9a58bbc42ae4b648acdbb803159a35f53db3/recognition/arcface_torch/dataset.py#L39) but do not use dali, DistributedSampler, or DataLoaderX to wrap the  MXFaceDataset since I only have one machine. A simple torch Dataloader is used to wrap the MXFaceDataset.

During the training of the first stage (freeze pretrained backbone, train FC), I found the shuffling in the Dataloader causes far too much time (possibly because I only have hard disks and random accessing the rec file is time-consuming). Hence, I run the [rec2shufrec.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/common/rec2shufrec.py) to do a pre-shuffling so I can set the shuffle arg in Dataloader False.

To be compatible with the code in [MXFaceDataset](https://github.com/deepinsight/insightface/blob/babb9a58bbc42ae4b648acdbb803159a35f53db3/recognition/arcface_torch/dataset.py#L149), I do a bit change in the [rec2shufrec.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/common/rec2shufrec.py). To be specific, MXRecordIO is replaced with MXIndexedRecordIO to produce both idx and rec files. That's all of the change.

The training of the FC layer (93431 classes output) goes very fast and smooth. However, at the very middle of the second epoch, the loss surges. The same case happens at the middle of the third, forth, epochs, causing a strange cyclic loss pattern.

I have checked the learning rate, different optimizers, the images and labels after running the [rec2shufrec.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/common/rec2shufrec.py) shuffling. Nothing went wrong. Then I redo the [rec2shufrec.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/common/rec2shufrec.py) many times to create different training data streams, but the loss pattern persists. The cross entropy loss always surges at the very middle of each epochs. Hence I suspect something went wrong in the [rec2shufrec.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_mxnet/common/rec2shufrec.py) and the use of [MXFaceDataset](https://github.com/deepinsight/insightface/blob/babb9a58bbc42ae4b648acdbb803159a35f53db3/recognition/arcface_torch/dataset.py#L39) though I cannot identify anything suspicious in the codes. Could you please give some clue on this? Many thanks.

(40k counts one epoch, at the first epoch, LR=1e-3, after that, LR=1e-5. No cyclic LR)
![image](https://user-images.githubusercontent.com/54563183/181166034-0dfd59b7-3c4c-4489-8b44-e4a4f7a00d98.png)

"
"It seems that this open source implementation mainly uses the `mxnet` framework. But why do I call `train.py` from the `arcface_mxnet` subdirectory with the author's pre-made dataset without pre-trained model and still fail to reproduce the results shown. The current training is 9 rounds, acc=0.646, and the correct rates on the validation set are 0.9935, 0.91129, and 0.93167 respectively.
Other than that, I noticed that the data preprocessing process did not subtract the mean and did not divide by the standard deviation.
My command is:
`CUDA_VISIBLE_DEVICES='0' nohup python -u train.py --network r50 --loss cosface --dataset emore >log &`

Any buddy can give me a hand?
Thanks in advance!"
""
"![image](https://user-images.githubusercontent.com/52409244/173769699-7437b416-d88a-4e39-920a-d8fb8b6dd340.png)
arcface_torch的train.py里初始化partial fc的时候，partial fc必须放在opt的最后吗？为什么会有这样的处理
"
"Hello, 
Thanks for sharing SCRFD implementation. I am interested in looking at MobileNet-0.5GF Implementation, could you point me to where I can find it? I could not find it on GitHub.
Thanks in advance. "
"Thank you for your excellent work！
Can you share your test code on the micc florence dataset?"
"您好，我看到arcface_torch提供的wf42m_pfc02_r100.py里，图片数量是
config.num_image = 42474557

但是我自己生成的train.lst里，却有42474558条记录，和上面的差1张图片（图片是我直接解压缩出来的，没有动过）

请问下，为啥官方的数量会少一张呢？是有啥说法吗？"
"when I training arcface_torch with webface42m dataset，after several block is traind，a mxnet error is raised，such as :
 raining: 2022-05-12 19:24:28,156-Speed 1141.43 samples/sec   Loss 42.5226   LearningRate 0.0997   Epoch: 0   Global Step: 4500   Fp16 Grad Scale: 131072   Required: 283 hours
Training: 2022-05-12 19:24:30,424-Speed 1128.66 samples/sec   Loss 42.5116   LearningRate 0.0997   Epoch: 0   Global Step: 4510   Fp16 Grad Scale: 131072   Required: 283 hours
Training: 2022-05-12 19:24:32,675-Speed 1137.82 samples/sec   Loss 42.4941   LearningRate 0.0997   Epoch: 0   Global Step: 4520   Fp16 Grad Scale: 131072   Required: 282 hours
Training: 2022-05-12 19:24:34,936-Speed 1132.51 samples/sec   Loss 42.5565   LearningRate 0.0997   Epoch: 0   Global Step: 4530   Fp16 Grad Scale: 131072   Required: 282 hours
Exception in thread Thread-2:
Traceback (most recent call last):
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""/home/cgb/insightface-master/recognition/arcface_torch/dataset.py"", line 53, in run
    for item in self.generator:
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 521, in __next__
    data = self._next_data()
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1203, in _next_data
    return self._process_data(data)
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 1229, in _process_data
    data.reraise()
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/_utils.py"", line 425, in reraise
    raise self.exc_type(msg)
mxnet.base.MXNetError: Caught MXNetError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py"", line 287, in _worker_loop
    data = fetcher.fetch(index)
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/cgb/insightface-master/recognition/arcface_torch/dataset.py"", line 130, in __getitem__
    sample = mx.image.imdecode(img).asnumpy()
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/mxnet/image/image.py"", line 211, in imdecode
    return cvimdecode(buf, *args, **kwargs)
  File ""<string>"", line 38, in _cvimdecode
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/mxnet/_ctypes/ndarray.py"", line 91, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/root/anaconda3/envs/pytorch/lib/python3.7/site-packages/mxnet/base.py"", line 246, in check_call
    raise get_last_ffi_error()
mxnet.base.MXNetError: Traceback (most recent call last):
  File ""../src/io/image_io.cc"", line 189
MXNetError: Check failed: len > 0: Input cannot be an empty buffer

If i train arcface_torch with wf12m dataset, it's  no problem .
 
I don't change any training code . Only modify config file, the config file  such as ：

from easydict import EasyDict as edict

# make training faster
# our RAM is 256G
# mount -t tmpfs -o size=140G  tmpfs /train_tmp

config = edict()
config.margin_list = (1.0, 0.0, 0.4)
config.network = ""r100""
config.resume = False
config.output = None
config.embedding_size = 512
config.sample_rate = 0.2
config.fp16 = True
config.momentum = 0.9
config.weight_decay = 5e-4
config.batch_size = 128
config.lr = 0.1
config.verbose = 2000
config.dali = False

config.rec = ""/data2/dataset/webface42m""
config.num_classes = 2059906
config.num_image = 42474557
config.num_epoch = 20
config.warmup_epoch = 0
config.val_targets = ['lfw', 'cfp_fp', ""agedb_30""]

dataset error or environment error?"
"I have run the bufallo_l model on the GPU successfully, but when I run the code multi-processing in python for getting the embedding vector I receive an exception:
```
onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : CUDA error executing cudaSetDevice(GetDeviceId())
2022-05-10 13:14:52.595859223 [E:onnxruntime:Default, cuda_call.cc:118 CudaCall] CUDA failure 3: initialization error ; GPU=0 ; hostname=ttai03 ; expr=cudaSetDevice(GetDeviceId()); 
```
Can we run multi-process (parallel) program on the insightface package with GPU?"
"您好，最近我用webface42m数据集从头训练arcface模型（用的是mxnet），训练时，lossvalue初始值很低（8点几），lfw、cfp-fp,agedb-30测试集下的精度都只有0.5左右，而且长时间不提升或者提升很慢
我曾经用faces_emore数据集做过对比（把faces_emore数据集解开再重新用自己的方法制作成rec），用来做训练是可以的。lossvalue正常（初始值是48左右）

请问webface42m数据集会引起这个问题吗？还是我制作的方法有问题（我用了2种方法，一种是用mx.tools.im2rec制作，一种是face2rec2.py制作），都有相同的问题？

非常感谢！

训练开始时的log如下：
lr_steps [320000, 640000, 960000]
[07:27:52] ../src/base.cc:80: cuDNN lib mismatch: linked-against version 8201 != compiled-against version 8100.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.
/root/anaconda3/envs/mxnet_py39/lib/python3.9/site-packages/mxnet/module/base_module.py:503: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/b
atch_size/num_workers (0.25 vs. 0.00390625). Is this intended?
  self.init_optimizer(kvstore=kvstore, optimizer=optimizer,
[07:28:16] ../src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_
CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:Epoch[0] Batch [0-20]	Speed: 326.31 samples/sec	acc=0.000000	lossvalue=8.866494
INFO:root:Epoch[0] Batch [20-40]	Speed: 331.88 samples/sec	acc=0.000000	lossvalue=8.142130
INFO:root:Epoch[0] Batch [40-60]	Speed: 441.95 samples/sec	acc=0.000000	lossvalue=8.831406
INFO:root:Epoch[0] Batch [60-80]	Speed: 440.96 samples/sec	acc=0.000000	lossvalue=8.677649
INFO:root:Epoch[0] Batch [80-100]	Speed: 441.77 samples/sec	acc=0.000000	lossvalue=8.372708
INFO:root:Epoch[0] Batch [100-120]	Speed: 439.85 samples/sec	acc=0.000000	lossvalue=8.388589
INFO:root:Epoch[0] Batch [120-140]	Speed: 441.72 samples/sec	acc=0.000000	lossvalue=8.543470
INFO:root:Epoch[0] Batch [140-160]	Speed: 441.22 samples/sec	acc=0.000000	lossvalue=8.334433
INFO:root:Epoch[0] Batch [160-180]	Speed: 439.92 samples/sec	acc=0.000000	lossvalue=8.642203
INFO:root:Epoch[0] Batch [180-200]	Speed: 441.18 samples/sec	acc=0.000000	lossvalue=8.228543
INFO:root:Epoch[0] Batch [200-220]	Speed: 437.92 samples/sec	acc=0.000000	lossvalue=8.557396
INFO:root:Epoch[0] Batch [220-240]	Speed: 320.54 samples/sec	acc=0.000000	lossvalue=8.379030
INFO:root:Epoch[0] Batch [240-260]	Speed: 437.61 samples/sec	acc=0.000000	lossvalue=8.042100
INFO:root:Epoch[0] Batch [260-280]	Speed: 438.39 samples/sec	acc=0.000000	lossvalue=8.470531
INFO:root:Epoch[0] Batch [280-300]	Speed: 439.90 samples/sec	acc=0.000000	lossvalue=8.324872
"
"Hello!
Thanks for your awesome work and detailed instructions.
I followed the configuration [[wf42m_pfc02_r100.py](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/configs/wf42m_pfc02_r100.py)](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/configs/wf42m_pfc02_r100.py) to train the model and made some changes.

```python
from easydict import EasyDict as edict

# make training faster
# our RAM is 256G
# mount -t tmpfs -o size=140G  tmpfs /train_tmp

config = edict()
config.margin_list = (1.0, 0.0, 0.4)
config.network = ""r100""
config.pretrain_model_path = None
config.resume = False
config.output = None
config.embedding_size = 512
config.sample_rate = 0.2
config.fp16 = True
config.momentum = 0.9
config.weight_decay = 5e-4
config.batch_size = 128
config.lr = 0.1
config.verbose = 10000
config.dali = False

config.rec = ""/mnt/arcface_torch""
config.num_classes = 2059906
config.num_image = 42474557
config.num_epoch = 20
config.warmup_epoch = 0
config.val_targets = ['lfw', 'cfp_fp', ""agedb_30""]

config.optimizer = ""sgd""
config.save_step = 30000
config.start_step = 0
config.fc_path = None
config.opt_path = None
```

- `save_step`, `start_step`, `fc_path`, `opt_path`, `pretrain_model_path`：These parameters are designed to allow the model to continue training after the program is disconnected.
- Other parameters are unchanged.



The code is modified only according to the new parameters added above.

```python
import argparse
import logging
import os

import numpy as np
import torch
from typing import List
from torch import distributed
from torch.utils.tensorboard import SummaryWriter

from backbones import get_model
from dataset import get_dataloader
from torch.utils.data import DataLoader
from lr_scheduler import PolyScheduler
from losses import CombinedMarginLoss
from partial_fc import PartialFC, PartialFCAdamW
from utils.utils_callbacks import CallBackLogging, CallBackVerification
from utils.utils_config import get_config
from utils.utils_logging import AverageMeter, init_logging
import time


from tqdm import tqdm

assert torch.__version__ >= ""1.9.0"", ""In order to enjoy the features of the new torch, \
we have upgraded the torch to 1.9.0. torch before than 1.9.0 may not work in the future.""

try:
    world_size = int(os.environ[""WORLD_SIZE""])
    rank = int(os.environ[""RANK""])
    distributed.init_process_group(""nccl"")
except KeyError:
    world_size = 1
    rank = 0
    distributed.init_process_group(
        backend=""nccl"",
        init_method=""tcp://127.0.0.1:12584"",
        rank=rank,
        world_size=world_size,
    )


def main(args):
    seed = 2333
    seed = seed + rank
    torch.manual_seed(seed)
    np.random.seed(seed)

    torch.cuda.set_device(args.local_rank)
    cfg = get_config(args.config)

    cfg.output = os.path.join(cfg.output, time.strftime(""%Y-%m-%d-%H-%M""))

    print(cfg.output)
    os.makedirs(cfg.output, exist_ok=True)
    init_logging(rank, cfg.output)

    summary_writer = (
        SummaryWriter(log_dir=os.path.join(cfg.output, ""tensorboard""))
        if rank == 0
        else None
    )
    train_loader = get_dataloader(
        cfg.rec, local_rank=args.local_rank, batch_size=cfg.batch_size, dali=cfg.dali)
    backbone = get_model(
        cfg.network, dropout=0.0, fp16=cfg.fp16, num_features=cfg.embedding_size
    ).cuda()
    if cfg.pretrain_model_path is not None:
        backbone.load_state_dict(torch.load(cfg.pretrain_model_path))
    
    backbone = torch.nn.parallel.DistributedDataParallel(
        module=backbone, broadcast_buffers=False, device_ids=[args.local_rank], bucket_cap_mb=16, 
        find_unused_parameters=True)

    backbone.train()
    # FIXME using gradient checkpoint if there are some unused parameters will cause error
    backbone._set_static_graph()

    margin_loss = CombinedMarginLoss(
        64, 
        cfg.margin_list[0],
        cfg.margin_list[1],
        cfg.margin_list[2],
        cfg.interclass_filtering_threshold
    )
    
    if cfg.optimizer == ""sgd"":
        module_partial_fc = PartialFC(
            margin_loss, cfg.embedding_size, cfg.num_classes, 
            cfg.sample_rate, cfg.fp16)
        if cfg.fc_path is not None:
            module_partial_fc.load_state_dict(torch.load(os.path.join(cfg.fc_path, 'softmax_fc_gpu_{}_step_{}.pt'.format(rank, cfg.start_step))))
        module_partial_fc.train().cuda()
        opt = torch.optim.SGD(
            params=[{""params"": backbone.parameters()}, {""params"": module_partial_fc.parameters()}],
            lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)
        if cfg.opt_path is not None:
            opt.load_state_dict(torch.load(os.path.join(cfg.fc_path, 'opt_gpu_{}_step_{}.pt'.format(rank, cfg.start_step))))

    elif cfg.optimizer == ""adamw"":
        module_partial_fc = PartialFCAdamW(
            margin_loss, cfg.embedding_size, cfg.num_classes, 
            cfg.sample_rate, cfg.fp16)
        if cfg.opt_path is not None:
            module_partial_fc.load_state_dict(torch.load(os.path.join(cfg.fc_path, 'softmax_fc_gpu_{}_step_{}.pt'.format(rank, cfg.start_step))))
        module_partial_fc.train().cuda()
        opt = torch.optim.AdamW(
            params=[{""params"": backbone.parameters()}, {""params"": module_partial_fc.parameters()}],
            lr=cfg.lr, weight_decay=cfg.weight_decay)
        if cfg.opt_path is not None:
            opt.load_state_dict(torch.load(os.path.join(cfg.fc_path, 'opt_gpu_{}_step_{}.pt'.format(rank, cfg.start_step))))
    else:
        raise

    cfg.total_batch_size = cfg.batch_size * world_size
    cfg.warmup_step = cfg.num_image // cfg.total_batch_size * cfg.warmup_epoch
    cfg.total_step = cfg.num_image // cfg.total_batch_size * cfg.num_epoch
    lr_scheduler = PolyScheduler(
        optimizer=opt,
        base_lr=cfg.lr,
        max_steps=cfg.total_step,
        warmup_steps=cfg.warmup_step
    )

    for key, value in cfg.items():
        num_space = 25 - len(key)
        logging.info("": "" + key + "" "" * num_space + str(value))

    callback_verification = CallBackVerification(
        val_targets=cfg.val_targets, rec_prefix=cfg.rec, summary_writer=summary_writer
    )
    callback_logging = CallBackLogging(
        frequent=cfg.frequent,
        total_step=cfg.total_step,
        batch_size=cfg.batch_size,
        writer=summary_writer
    )

    loss_am = AverageMeter()
    start_epoch = 0
    global_step = 0
    amp = torch.cuda.amp.grad_scaler.GradScaler(growth_interval=100)

    for epoch in range(start_epoch, cfg.num_epoch):

        if isinstance(train_loader, DataLoader):
            train_loader.sampler.set_epoch(epoch)
        if global_step + cfg.total_step // cfg.num_epoch < cfg.start_step:
            global_step += cfg.total_step // cfg.num_epoch
            logging.info(""global step: {}"".format(global_step))
            continue
        for _, (img, local_labels) in enumerate(train_loader):
            global_step += 1
            if global_step <= cfg.start_step:
                logging.info(""global step: {}"".format(global_step))
                continue
            local_embeddings = backbone(img)
            loss: torch.Tensor = module_partial_fc(local_embeddings, local_labels, opt)

            if cfg.fp16:
                amp.scale(loss).backward()
                amp.unscale_(opt)
                torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)
                amp.step(opt)
                amp.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(backbone.parameters(), 5)
                opt.step()

            opt.zero_grad()
            lr_scheduler.step()

            with torch.no_grad():
                loss_am.update(loss.item(), 1)
                callback_logging(global_step, loss_am, epoch, cfg.fp16, lr_scheduler.get_last_lr()[0], amp)

                if global_step % cfg.verbose == 0:
                    callback_verification(global_step, backbone)
            
            if global_step % cfg.save_step == 0 and global_step > 200:

                path_pfc = os.path.join(cfg.output, ""softmax_fc_gpu_{}_step_{}.pt"".format(rank, global_step))
                torch.save(module_partial_fc.state_dict(), path_pfc)
                path_opt = os.path.join(cfg.output, ""opt_gpu_{}_step_{}.pt"".format(rank, global_step))
                torch.save(opt.state_dict(), path_opt)
                if rank == 0:
                    path_module = os.path.join(cfg.output, ""model_step_{}.pt"".format(global_step))
                    torch.save(backbone.module.state_dict(), path_module)
        
        path_pfc = os.path.join(cfg.output, ""softmax_fc_gpu_{}.pt"".format(rank))
        torch.save(module_partial_fc.state_dict(), path_pfc)
        path_opt = os.path.join(cfg.output, ""opt_gpu_{}.pt"".format(rank))
        torch.save(opt.state_dict(), path_opt)
        if rank == 0:
            path_module = os.path.join(cfg.output, ""model.pt"")
            torch.save(backbone.module.state_dict(), path_module)
        
        if cfg.dali:
            train_loader.reset()

    if rank == 0:
        path_module = os.path.join(cfg.output, ""model.pt"")
        torch.save(backbone.module.state_dict(), path_module)

        from torch2onnx import convert_onnx
        convert_onnx(backbone.module.cpu().eval(), path_module, os.path.join(cfg.output, ""model.onnx""))

    distributed.destroy_process_group()


if __name__ == ""__main__"":
    torch.backends.cudnn.benchmark = True
    parser = argparse.ArgumentParser(description=""Distributed Arcface Training in Pytorch"")
    parser.add_argument(""config"", type=str, help=""py config file"")
    parser.add_argument(""--local_rank"", type=int, default=0, help=""local_rank"")
    main(parser.parse_args())

```



But when I train from scratch, these extra parameters and code are not needed.

I tried to train a few epochs. Part of the training logs are as follows.（the first epoch）

```bash
Training: 2022-05-05 03:01:29,603-rank_id: 0
Training: 2022-05-05 03:02:23,623-: margin_list              [1.0, 0.0, 0.4]
Training: 2022-05-05 03:02:23,624-: network                  r100
Training: 2022-05-05 03:02:23,624-: resume                   False
Training: 2022-05-05 03:02:23,624-: output                   work_dirs/wf42m_pfc02_r100/2022-05-05-03-01
Training: 2022-05-05 03:02:23,624-: embedding_size           512
Training: 2022-05-05 03:02:23,624-: sample_rate              0.2
Training: 2022-05-05 03:02:23,624-: interclass_filtering_threshold0
Training: 2022-05-05 03:02:23,624-: fp16                     True
Training: 2022-05-05 03:02:23,624-: batch_size               128
Training: 2022-05-05 03:02:23,624-: optimizer                sgd
Training: 2022-05-05 03:02:23,624-: lr                       0.1
Training: 2022-05-05 03:02:23,624-: momentum                 0.9
Training: 2022-05-05 03:02:23,624-: weight_decay             0.0005
Training: 2022-05-05 03:02:23,624-: verbose                  10000
Training: 2022-05-05 03:02:23,624-: frequent                 10
Training: 2022-05-05 03:02:23,624-: dali                     False
Training: 2022-05-05 03:02:23,624-: pretrain_model_path      None
Training: 2022-05-05 03:02:23,624-: rec                      /mnt/arcface_torch
Training: 2022-05-05 03:02:23,624-: num_classes              2059906
Training: 2022-05-05 03:02:23,624-: num_image                42474557
Training: 2022-05-05 03:02:23,624-: num_epoch                20
Training: 2022-05-05 03:02:23,624-: warmup_epoch             0
Training: 2022-05-05 03:02:23,624-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2022-05-05 03:02:23,624-: save_step                30000
Training: 2022-05-05 03:02:23,624-: start_step               0
Training: 2022-05-05 03:02:23,624-: fc_path                  None
Training: 2022-05-05 03:02:23,624-: opt_path                 None
Training: 2022-05-05 03:02:23,624-: total_batch_size         256
Training: 2022-05-05 03:02:23,624-: warmup_step              0
Training: 2022-05-05 03:02:23,624-: total_step               3318320
Training: 2022-05-05 03:04:00,229-Reducer buckets have been rebuilt in this iteration.
Training: 2022-05-05 03:04:06,521-Speed 731.07 samples/sec   Loss 43.3935   LearningRate 0.1000   Epoch: 0   Global Step: 20   Fp16 Grad Scale: 16384   Required: 2244 hours
Training: 2022-05-05 03:04:10,028-Speed 730.09 samples/sec   Loss 44.7358   LearningRate 0.1000   Epoch: 0   Global Step: 30   Fp16 Grad Scale: 16384   Required: 1625 hours
Training: 2022-05-05 03:04:13,543-Speed 728.30 samples/sec   Loss 44.9578   LearningRate 0.1000   Epoch: 0   Global Step: 40   Fp16 Grad Scale: 16384   Required: 1307 hours
Training: 2022-05-05 03:04:17,060-Speed 728.08 samples/sec   Loss 44.8275   LearningRate 0.1000   Epoch: 0   Global Step: 50   Fp16 Grad Scale: 16384   Required: 1115 hours
Training: 2022-05-05 03:04:20,582-Speed 726.96 samples/sec   Loss 44.4942   LearningRate 0.1000   Epoch: 0   Global Step: 60   Fp16 Grad Scale: 16384   Required: 985 hours
Training: 2022-05-05 03:04:24,108-Speed 726.03 samples/sec   Loss 44.6929   LearningRate 0.1000   Epoch: 0   Global Step: 70   Fp16 Grad Scale: 16384   Required: 892 hours
Training: 2022-05-05 03:04:27,798-Speed 693.88 samples/sec   Loss 44.4609   LearningRate 0.1000   Epoch: 0   Global Step: 80   Fp16 Grad Scale: 16384   Required: 824 hours
Training: 2022-05-05 03:04:31,318-Speed 727.35 samples/sec   Loss 44.4769   LearningRate 0.1000   Epoch: 0   Global Step: 90   Fp16 Grad Scale: 16384   Required: 769 hours
Training: 2022-05-05 03:04:35,442-Speed 620.98 samples/sec   Loss 44.6908   LearningRate 0.1000   Epoch: 0   Global Step: 100   Fp16 Grad Scale: 16384   Required: 731 hours
Training: 2022-05-05 03:04:38,963-Speed 727.09 samples/sec   Loss 44.4759   LearningRate 0.1000   Epoch: 0   Global Step: 110   Fp16 Grad Scale: 32768   Required: 694 hours
Training: 2022-05-05 03:04:42,480-Speed 727.90 samples/sec   Loss 44.7240   LearningRate 0.1000   Epoch: 0   Global Step: 120   Fp16 Grad Scale: 32768   Required: 663 hours
Training: 2022-05-05 03:04:45,999-Speed 727.67 samples/sec   Loss 44.6801   LearningRate 0.1000   Epoch: 0   Global Step: 130   Fp16 Grad Scale: 32768   Required: 638 hours
Training: 2022-05-05 03:04:49,516-Speed 727.92 samples/sec   Loss 44.6303   LearningRate 0.1000   Epoch: 0   Global Step: 140   Fp16 Grad Scale: 32768   Required: 615 hours
Training: 2022-05-05 03:04:53,041-Speed 726.51 samples/sec   Loss 44.6319   LearningRate 0.1000   Epoch: 0   Global Step: 150   Fp16 Grad Scale: 32768   Required: 596 hours
Training: 2022-05-05 03:04:56,561-Speed 727.37 samples/sec   Loss 44.2837   LearningRate 0.1000   Epoch: 0   Global Step: 160   Fp16 Grad Scale: 32768   Required: 579 hours
Training: 2022-05-05 03:05:00,083-Speed 726.84 samples/sec   Loss 44.3776   LearningRate 0.1000   Epoch: 0   Global Step: 170   Fp16 Grad Scale: 32768   Required: 564 hours
Training: 2022-05-05 03:05:03,788-Speed 691.03 samples/sec   Loss 44.5802   LearningRate 0.1000   Epoch: 0   Global Step: 180   Fp16 Grad Scale: 32768   Required: 552 hours
Training: 2022-05-05 03:05:07,310-Speed 727.09 samples/sec   Loss 44.3005   LearningRate 0.1000   Epoch: 0   Global Step: 190   Fp16 Grad Scale: 32768   Required: 540 hours
Training: 2022-05-05 03:05:10,833-Speed 726.65 samples/sec   Loss 44.2806   LearningRate 0.1000   Epoch: 0   Global Step: 200   Fp16 Grad Scale: 32768   Required: 529 hours
Training: 2022-05-05 03:05:14,359-Speed 726.32 samples/sec   Loss 44.2002   LearningRate 0.1000   Epoch: 0   Global Step: 210   Fp16 Grad Scale: 65536   Required: 520 hours
Training: 2022-05-05 03:05:17,883-Speed 726.46 samples/sec   Loss 44.0486   LearningRate 0.1000   Epoch: 0   Global Step: 220   Fp16 Grad Scale: 65536   Required: 511 hours
Training: 2022-05-05 03:05:21,405-Speed 726.92 samples/sec   Loss 44.2318   LearningRate 0.1000   Epoch: 0   Global Step: 230   Fp16 Grad Scale: 65536   Required: 503 hours
Training: 2022-05-05 03:05:24,935-Speed 725.47 samples/sec   Loss 44.2789   LearningRate 0.1000   Epoch: 0   Global Step: 240   Fp16 Grad Scale: 65536   Required: 495 hours
Training: 2022-05-05 03:05:28,464-Speed 725.53 samples/sec   Loss 43.9702   LearningRate 0.1000   Epoch: 0   Global Step: 250   Fp16 Grad Scale: 65536   Required: 489 hours
Training: 2022-05-05 03:05:31,986-Speed 726.93 samples/sec   Loss 43.8529   LearningRate 0.1000   Epoch: 0   Global Step: 260   Fp16 Grad Scale: 65536   Required: 482 hours
Training: 2022-05-05 03:05:35,517-Speed 725.20 samples/sec   Loss 43.7125   LearningRate 0.1000   Epoch: 0   Global Step: 270   Fp16 Grad Scale: 65536   Required: 477 hours
Training: 2022-05-05 03:05:39,044-Speed 725.77 samples/sec   Loss 43.8522   LearningRate 0.1000   Epoch: 0   Global Step: 280   Fp16 Grad Scale: 65536   Required: 471 hours
Training: 2022-05-05 03:05:42,576-Speed 724.93 samples/sec   Loss 43.8913   LearningRate 0.1000   Epoch: 0   Global Step: 290   Fp16 Grad Scale: 65536   Required: 466 hours
Training: 2022-05-05 03:05:46,107-Speed 725.26 samples/sec   Loss 43.7753   LearningRate 0.1000   Epoch: 0   Global Step: 300   Fp16 Grad Scale: 65536   Required: 462 hours
Training: 2022-05-05 03:05:49,641-Speed 724.52 samples/sec   Loss 43.9401   LearningRate 0.1000   Epoch: 0   Global Step: 310   Fp16 Grad Scale: 131072   Required: 457 hours
Training: 2022-05-05 03:05:53,162-Speed 727.03 samples/sec   Loss 43.9404   LearningRate 0.1000   Epoch: 0   Global Step: 320   Fp16 Grad Scale: 131072   Required: 453 hours
Training: 2022-05-05 03:05:56,684-Speed 727.08 samples/sec   Loss 43.9225   LearningRate 0.1000   Epoch: 0   Global Step: 330   Fp16 Grad Scale: 131072   Required: 449 hours
Training: 2022-05-05 03:06:00,216-Speed 724.79 samples/sec   Loss 43.9300   LearningRate 0.1000   Epoch: 0   Global Step: 340   Fp16 Grad Scale: 131072   Required: 445 hours
Training: 2022-05-05 03:06:03,749-Speed 724.74 samples/sec   Loss 43.8761   LearningRate 0.1000   Epoch: 0   Global Step: 350   Fp16 Grad Scale: 131072   Required: 442 hours
Training: 2022-05-05 03:06:07,280-Speed 725.12 samples/sec   Loss 44.2065   LearningRate 0.1000   Epoch: 0   Global Step: 360   Fp16 Grad Scale: 131072   Required: 439 hours
Training: 2022-05-05 03:06:10,819-Speed 723.52 samples/sec   Loss 43.7650   LearningRate 0.1000   Epoch: 0   Global Step: 370   Fp16 Grad Scale: 131072   Required: 436 hours
Training: 2022-05-05 03:06:14,353-Speed 724.53 samples/sec   Loss 43.7390   LearningRate 0.1000   Epoch: 0   Global Step: 380   Fp16 Grad Scale: 131072   Required: 433 hours
Training: 2022-05-05 03:06:17,885-Speed 725.07 samples/sec   Loss 43.7029   LearningRate 0.1000   Epoch: 0   Global Step: 390   Fp16 Grad Scale: 131072   Required: 430 hours
Training: 2022-05-05 03:06:21,423-Speed 723.52 samples/sec   Loss 43.7675   LearningRate 0.1000   Epoch: 0   Global Step: 400   Fp16 Grad Scale: 131072   Required: 428 hours
Training: 2022-05-05 03:06:24,939-Speed 728.37 samples/sec   Loss 43.9076   LearningRate 0.1000   Epoch: 0   Global Step: 410   Fp16 Grad Scale: 131072   Required: 425 hours
Training: 2022-05-05 03:06:28,475-Speed 724.16 samples/sec   Loss 43.8942   LearningRate 0.1000   Epoch: 0   Global Step: 420   Fp16 Grad Scale: 131072   Required: 423 hours
Training: 2022-05-05 03:06:31,999-Speed 726.52 samples/sec   Loss 43.9265   LearningRate 0.1000   Epoch: 0   Global Step: 430   Fp16 Grad Scale: 131072   Required: 420 hours
Training: 2022-05-05 03:06:35,532-Speed 724.71 samples/sec   Loss 43.8219   LearningRate 0.1000   Epoch: 0   Global Step: 440   Fp16 Grad Scale: 131072   Required: 418 hours
Training: 2022-05-05 03:06:39,065-Speed 724.59 samples/sec   Loss 43.8077   LearningRate 0.1000   Epoch: 0   Global Step: 450   Fp16 Grad Scale: 131072   Required: 416 hours
Training: 2022-05-05 03:06:42,594-Speed 725.68 samples/sec   Loss 43.9029   LearningRate 0.1000   Epoch: 0   Global Step: 460   Fp16 Grad Scale: 131072   Required: 414 hours
Training: 2022-05-05 03:06:46,133-Speed 723.49 samples/sec   Loss 43.9024   LearningRate 0.1000   Epoch: 0   Global Step: 470   Fp16 Grad Scale: 131072   Required: 412 hours
Training: 2022-05-05 03:06:49,670-Speed 723.81 samples/sec   Loss 43.6135   LearningRate 0.1000   Epoch: 0   Global Step: 480   Fp16 Grad Scale: 131072   Required: 411 hours
Training: 2022-05-05 03:06:53,211-Speed 723.15 samples/sec   Loss 43.6867   LearningRate 0.1000   Epoch: 0   Global Step: 490   Fp16 Grad Scale: 131072   Required: 409 hours
Training: 2022-05-05 03:06:56,737-Speed 726.14 samples/sec   Loss 43.5186   LearningRate 0.1000   Epoch: 0   Global Step: 500   Fp16 Grad Scale: 131072   Required: 407 hours
Training: 2022-05-05 03:07:00,251-Speed 728.57 samples/sec   Loss 43.5245   LearningRate 0.1000   Epoch: 0   Global Step: 510   Fp16 Grad Scale: 131072   Required: 406 hours
Training: 2022-05-05 03:07:03,781-Speed 725.46 samples/sec   Loss 43.6427   LearningRate 0.1000   Epoch: 0   Global Step: 520   Fp16 Grad Scale: 131072   Required: 404 hours
Training: 2022-05-05 03:07:07,310-Speed 725.46 samples/sec   Loss 43.6788   LearningRate 0.1000   Epoch: 0   Global Step: 530   Fp16 Grad Scale: 131072   Required: 403 hours
Training: 2022-05-05 03:07:10,843-Speed 724.73 samples/sec   Loss 43.4738   LearningRate 0.1000   Epoch: 0   Global Step: 540   Fp16 Grad Scale: 131072   Required: 401 hours
Training: 2022-05-05 03:07:14,371-Speed 725.72 samples/sec   Loss 43.5328   LearningRate 0.1000   Epoch: 0   Global Step: 550   Fp16 Grad Scale: 131072   Required: 400 hours
Training: 2022-05-05 03:07:17,914-Speed 722.58 samples/sec   Loss 43.5722   LearningRate 0.1000   Epoch: 0   Global Step: 560   Fp16 Grad Scale: 131072   Required: 398 hours
Training: 2022-05-05 03:07:21,448-Speed 724.53 samples/sec   Loss 43.6039   LearningRate 0.1000   Epoch: 0   Global Step: 570   Fp16 Grad Scale: 131072   Required: 397 hours
Training: 2022-05-05 03:07:24,969-Speed 727.31 samples/sec   Loss 43.4314   LearningRate 0.1000   Epoch: 0   Global Step: 580   Fp16 Grad Scale: 131072   Required: 396 hours
Training: 2022-05-05 03:07:28,499-Speed 725.27 samples/sec   Loss 43.3965   LearningRate 0.1000   Epoch: 0   Global Step: 590   Fp16 Grad Scale: 131072   Required: 395 hours
Training: 2022-05-05 03:07:32,027-Speed 725.86 samples/sec   Loss 43.4256   LearningRate 0.1000   Epoch: 0   Global Step: 600   Fp16 Grad Scale: 131072   Required: 394 hours
Training: 2022-05-05 03:07:35,548-Speed 727.10 samples/sec   Loss 43.3327   LearningRate 0.1000   Epoch: 0   Global Step: 610   Fp16 Grad Scale: 131072   Required: 392 hours
Training: 2022-05-05 03:07:39,078-Speed 725.43 samples/sec   Loss 43.3029   LearningRate 0.1000   Epoch: 0   Global Step: 620   Fp16 Grad Scale: 131072   Required: 391 hours
Training: 2022-05-05 03:07:42,608-Speed 725.28 samples/sec   Loss 43.3096   LearningRate 0.1000   Epoch: 0   Global Step: 630   Fp16 Grad Scale: 131072   Required: 390 hours
Training: 2022-05-05 03:07:46,143-Speed 724.20 samples/sec   Loss 43.2749   LearningRate 0.1000   Epoch: 0   Global Step: 640   Fp16 Grad Scale: 131072   Required: 389 hours
Training: 2022-05-05 03:07:49,673-Speed 725.36 samples/sec   Loss 43.4988   LearningRate 0.1000   Epoch: 0   Global Step: 650   Fp16 Grad Scale: 131072   Required: 388 hours
Training: 2022-05-05 03:07:53,208-Speed 724.32 samples/sec   Loss 43.3166   LearningRate 0.1000   Epoch: 0   Global Step: 660   Fp16 Grad Scale: 131072   Required: 387 hours
Training: 2022-05-05 03:07:56,738-Speed 725.42 samples/sec   Loss 43.3468   LearningRate 0.1000   Epoch: 0   Global Step: 670   Fp16 Grad Scale: 131072   Required: 386 hours
Training: 2022-05-05 03:08:00,278-Speed 723.26 samples/sec   Loss 43.4304   LearningRate 0.1000   Epoch: 0   Global Step: 680   Fp16 Grad Scale: 131072   Required: 386 hours
Training: 2022-05-05 03:08:03,813-Speed 724.34 samples/sec   Loss 43.3743   LearningRate 0.1000   Epoch: 0   Global Step: 690   Fp16 Grad Scale: 131072   Required: 385 hours
Training: 2022-05-05 03:08:07,346-Speed 724.55 samples/sec   Loss 43.3464   LearningRate 0.1000   Epoch: 0   Global Step: 700   Fp16 Grad Scale: 131072   Required: 384 hours
Training: 2022-05-05 03:08:10,855-Speed 729.71 samples/sec   Loss 43.2136   LearningRate 0.1000   Epoch: 0   Global Step: 710   Fp16 Grad Scale: 131072   Required: 383 hours
Training: 2022-05-05 03:08:14,384-Speed 725.51 samples/sec   Loss 43.2740   LearningRate 0.1000   Epoch: 0   Global Step: 720   Fp16 Grad Scale: 131072   Required: 382 hours
Training: 2022-05-05 03:08:17,920-Speed 724.19 samples/sec   Loss 43.2941   LearningRate 0.1000   Epoch: 0   Global Step: 730   Fp16 Grad Scale: 131072   Required: 381 hours
Training: 2022-05-05 03:08:21,451-Speed 725.21 samples/sec   Loss 43.3661   LearningRate 0.1000   Epoch: 0   Global Step: 740   Fp16 Grad Scale: 131072   Required: 381 hours
Training: 2022-05-05 03:08:24,984-Speed 724.69 samples/sec   Loss 43.5143   LearningRate 0.1000   Epoch: 0   Global Step: 750   Fp16 Grad Scale: 131072   Required: 380 hours
Training: 2022-05-05 03:08:28,520-Speed 723.93 samples/sec   Loss 43.3332   LearningRate 0.1000   Epoch: 0   Global Step: 760   Fp16 Grad Scale: 131072   Required: 379 hours
Training: 2022-05-05 03:08:32,060-Speed 723.23 samples/sec   Loss 43.3508   LearningRate 0.1000   Epoch: 0   Global Step: 770   Fp16 Grad Scale: 131072   Required: 379 hours
Training: 2022-05-05 03:08:35,593-Speed 724.79 samples/sec   Loss 43.4320   LearningRate 0.1000   Epoch: 0   Global Step: 780   Fp16 Grad Scale: 131072   Required: 378 hours
Training: 2022-05-05 03:08:39,136-Speed 722.79 samples/sec   Loss 43.4134   LearningRate 0.1000   Epoch: 0   Global Step: 790   Fp16 Grad Scale: 131072   Required: 377 hours
Training: 2022-05-05 03:08:42,678-Speed 722.72 samples/sec   Loss 43.2290   LearningRate 0.1000   Epoch: 0   Global Step: 800   Fp16 Grad Scale: 131072   Required: 377 hours
Training: 2022-05-05 03:08:46,189-Speed 729.32 samples/sec   Loss 43.3066   LearningRate 0.1000   Epoch: 0   Global Step: 810   Fp16 Grad Scale: 131072   Required: 376 hours
Training: 2022-05-05 03:08:49,718-Speed 725.63 samples/sec   Loss 43.4427   LearningRate 0.1000   Epoch: 0   Global Step: 820   Fp16 Grad Scale: 131072   Required: 375 hours
Training: 2022-05-05 03:08:53,262-Speed 722.47 samples/sec   Loss 43.3619   LearningRate 0.0999   Epoch: 0   Global Step: 830   Fp16 Grad Scale: 131072   Required: 375 hours
Training: 2022-05-05 03:08:56,795-Speed 724.61 samples/sec   Loss 43.4427   LearningRate 0.0999   Epoch: 0   Global Step: 840   Fp16 Grad Scale: 131072   Required: 374 hours
Training: 2022-05-05 03:09:00,343-Speed 721.69 samples/sec   Loss 43.3795   LearningRate 0.0999   Epoch: 0   Global Step: 850   Fp16 Grad Scale: 131072   Required: 374 hours
Training: 2022-05-05 03:09:03,874-Speed 725.11 samples/sec   Loss 43.4918   LearningRate 0.0999   Epoch: 0   Global Step: 860   Fp16 Grad Scale: 131072   Required: 373 hours
Training: 2022-05-05 03:09:07,398-Speed 726.55 samples/sec   Loss 43.2607   LearningRate 0.0999   Epoch: 0   Global Step: 870   Fp16 Grad Scale: 131072   Required: 372 hours
Training: 2022-05-05 03:09:10,926-Speed 725.79 samples/sec   Loss 43.5018   LearningRate 0.0999   Epoch: 0   Global Step: 880   Fp16 Grad Scale: 131072   Required: 372 hours
Training: 2022-05-05 03:09:14,456-Speed 725.19 samples/sec   Loss 43.4363   LearningRate 0.0999   Epoch: 0   Global Step: 890   Fp16 Grad Scale: 131072   Required: 371 hours
Training: 2022-05-05 03:09:17,993-Speed 723.89 samples/sec   Loss 43.6587   LearningRate 0.0999   Epoch: 0   Global Step: 900   Fp16 Grad Scale: 131072   Required: 371 hours
Training: 2022-05-05 03:09:21,505-Speed 729.16 samples/sec   Loss 43.5048   LearningRate 0.0999   Epoch: 0   Global Step: 910   Fp16 Grad Scale: 131072   Required: 370 hours
Training: 2022-05-05 03:09:25,037-Speed 724.88 samples/sec   Loss 43.2417   LearningRate 0.0999   Epoch: 0   Global Step: 920   Fp16 Grad Scale: 131072   Required: 370 hours
Training: 2022-05-05 03:09:28,563-Speed 726.07 samples/sec   Loss 43.5033   LearningRate 0.0999   Epoch: 0   Global Step: 930   Fp16 Grad Scale: 131072   Required: 369 hours
Training: 2022-05-05 03:09:32,096-Speed 724.81 samples/sec   Loss 43.4549   LearningRate 0.0999   Epoch: 0   Global Step: 940   Fp16 Grad Scale: 131072   Required: 369 hours
Training: 2022-05-05 03:09:35,618-Speed 727.10 samples/sec   Loss 43.1973   LearningRate 0.0999   Epoch: 0   Global Step: 950   Fp16 Grad Scale: 131072   Required: 368 hours
Training: 2022-05-05 03:09:39,149-Speed 725.15 samples/sec   Loss 43.4111   LearningRate 0.0999   Epoch: 0   Global Step: 960   Fp16 Grad Scale: 131072   Required: 368 hours
Training: 2022-05-05 03:09:42,686-Speed 723.94 samples/sec   Loss 43.2899   LearningRate 0.0999   Epoch: 0   Global Step: 970   Fp16 Grad Scale: 131072   Required: 368 hours
Training: 2022-05-05 03:09:46,211-Speed 726.28 samples/sec   Loss 43.2542   LearningRate 0.0999   Epoch: 0   Global Step: 980   Fp16 Grad Scale: 131072   Required: 367 hours
Training: 2022-05-05 03:09:49,738-Speed 726.04 samples/sec   Loss 43.2698   LearningRate 0.0999   Epoch: 0   Global Step: 990   Fp16 Grad Scale: 131072   Required: 367 hours
Training: 2022-05-05 03:09:53,261-Speed 726.82 samples/sec   Loss 43.3357   LearningRate 0.0999   Epoch: 0   Global Step: 1000   Fp16 Grad Scale: 131072   Required: 366 hours
...
...
...
...
...
...
Training: 2022-05-05 04:02:38,531-Speed 654.20 samples/sec   Loss 42.4198   LearningRate 0.0994   Epoch: 0   Global Step: 9810   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:02:42,430-Speed 656.83 samples/sec   Loss 42.4915   LearningRate 0.0994   Epoch: 0   Global Step: 9820   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:02:46,392-Speed 646.49 samples/sec   Loss 42.4388   LearningRate 0.0994   Epoch: 0   Global Step: 9830   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:02:50,393-Speed 640.30 samples/sec   Loss 42.4244   LearningRate 0.0994   Epoch: 0   Global Step: 9840   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:02:54,402-Speed 638.66 samples/sec   Loss 42.3092   LearningRate 0.0994   Epoch: 0   Global Step: 9850   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:02:58,344-Speed 649.63 samples/sec   Loss 42.3126   LearningRate 0.0994   Epoch: 0   Global Step: 9860   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:02,275-Speed 651.37 samples/sec   Loss 42.4544   LearningRate 0.0994   Epoch: 0   Global Step: 9870   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:06,232-Speed 647.06 samples/sec   Loss 42.4995   LearningRate 0.0994   Epoch: 0   Global Step: 9880   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:10,153-Speed 653.10 samples/sec   Loss 42.4693   LearningRate 0.0994   Epoch: 0   Global Step: 9890   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:14,078-Speed 653.30 samples/sec   Loss 42.2807   LearningRate 0.0994   Epoch: 0   Global Step: 9900   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:18,041-Speed 646.23 samples/sec   Loss 42.4305   LearningRate 0.0994   Epoch: 0   Global Step: 9910   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:21,969-Speed 651.95 samples/sec   Loss 42.2798   LearningRate 0.0994   Epoch: 0   Global Step: 9920   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:25,908-Speed 649.98 samples/sec   Loss 42.4574   LearningRate 0.0994   Epoch: 0   Global Step: 9930   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:29,965-Speed 631.44 samples/sec   Loss 42.3743   LearningRate 0.0994   Epoch: 0   Global Step: 9940   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:33,950-Speed 642.54 samples/sec   Loss 42.4366   LearningRate 0.0994   Epoch: 0   Global Step: 9950   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:37,915-Speed 645.97 samples/sec   Loss 42.3575   LearningRate 0.0994   Epoch: 0   Global Step: 9960   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:41,852-Speed 650.47 samples/sec   Loss 42.3721   LearningRate 0.0994   Epoch: 0   Global Step: 9970   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:45,809-Speed 647.12 samples/sec   Loss 42.4875   LearningRate 0.0994   Epoch: 0   Global Step: 9980   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:49,754-Speed 649.48 samples/sec   Loss 42.6298   LearningRate 0.0994   Epoch: 0   Global Step: 9990   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:03:53,689-Speed 650.63 samples/sec   Loss 42.4385   LearningRate 0.0994   Epoch: 0   Global Step: 10000   Fp16 Grad Scale: 32768   Required: 334 hours
Training: 2022-05-05 04:04:58,211-[lfw][10000]XNorm: 33.232371
Training: 2022-05-05 04:04:58,211-[lfw][10000]Accuracy-Flip: 0.57800+-0.01941
Training: 2022-05-05 04:04:58,211-[lfw][10000]Accuracy-Highest: 0.57800
Training: 2022-05-05 04:05:59,945-[cfp_fp][10000]XNorm: 38.853677
Training: 2022-05-05 04:05:59,945-[cfp_fp][10000]Accuracy-Flip: 0.57586+-0.00978
Training: 2022-05-05 04:05:59,946-[cfp_fp][10000]Accuracy-Highest: 0.57586
Training: 2022-05-05 04:07:10,846-[agedb_30][10000]XNorm: 33.714078
Training: 2022-05-05 04:07:10,846-[agedb_30][10000]Accuracy-Flip: 0.50183+-0.00565
Training: 2022-05-05 04:07:10,847-[agedb_30][10000]Accuracy-Highest: 0.50183
...
...
...
...
...
...
0.0912   Epoch: 0   Global Step: 149970   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:16:36,587-Speed 744.49 samples/sec   Loss 31.3188   LearningRate 0.0912   Epoch: 0   Global Step: 149980   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:16:40,024-Speed 745.08 samples/sec   Loss 31.7255   LearningRate 0.0912   Epoch: 0   Global Step: 149990   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:16:43,455-Speed 746.23 samples/sec   Loss 31.6172   LearningRate 0.0912   Epoch: 0   Global Step: 150000   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:17:17,979-[lfw][150000]XNorm: 23.721737
Training: 2022-05-05 20:17:17,981-[lfw][150000]Accuracy-Flip: 0.95317+-0.01102
Training: 2022-05-05 20:17:17,981-[lfw][150000]Accuracy-Highest: 0.95317
Training: 2022-05-05 20:17:57,559-[cfp_fp][150000]XNorm: 22.461578
Training: 2022-05-05 20:17:57,559-[cfp_fp][150000]Accuracy-Flip: 0.75986+-0.01316
Training: 2022-05-05 20:17:57,559-[cfp_fp][150000]Accuracy-Highest: 0.75986
Training: 2022-05-05 20:18:31,206-[agedb_30][150000]XNorm: 23.346873
Training: 2022-05-05 20:18:31,207-[agedb_30][150000]Accuracy-Flip: 0.77467+-0.01928
Training: 2022-05-05 20:18:31,207-[agedb_30][150000]Accuracy-Highest: 0.77467
Training: 2022-05-05 20:18:39,020-Speed 22.15 samples/sec   Loss 31.3819   LearningRate 0.0912   Epoch: 0   Global Step: 150010   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:18:42,425-Speed 751.99 samples/sec   Loss 31.5926   LearningRate 0.0912   Epoch: 0   Global Step: 150020   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:18:45,836-Speed 750.62 samples/sec   Loss 31.8748   LearningRate 0.0912   Epoch: 0   Global Step: 150030   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:18:49,255-Speed 748.88 samples/sec   Loss 31.8023   LearningRate 0.0912   Epoch: 0   Global Step: 150040   Fp16 Grad Scale: 65536   Required: 364 hours
Training: 2022-05-05 20:18:52,673-Speed 749.26 samples/sec   Loss 31.5251   LearningRate 0.0912   Epoch: 0   Global Step: 150050   Fp16 Grad Scale: 65536   Required: 364 hours
...
...
...
...
...
...
Training: 2022-05-05 21:51:17,584-Speed 743.88 samples/sec   Loss 31.3707   LearningRate 0.0903   Epoch: 0   Global Step: 165850   Fp16 Grad Scale: 131072   Required: 357 hours
Training: 2022-05-05 21:51:21,009-Speed 747.59 samples/sec   Loss 31.5126   LearningRate 0.0903   Epoch: 0   Global Step: 165860   Fp16 Grad Scale: 65536   Required: 357 hours
Training: 2022-05-05 21:51:24,447-Speed 744.78 samples/sec   Loss 31.5628   LearningRate 0.0903   Epoch: 0   Global Step: 165870   Fp16 Grad Scale: 65536   Required: 357 hours
Training: 2022-05-05 21:51:27,885-Speed 744.87 samples/sec   Loss 31.5398   LearningRate 0.0903   Epoch: 0   Global Step: 165880   Fp16 Grad Scale: 65536   Required: 357 hours
Training: 2022-05-05 21:51:31,317-Speed 746.07 samples/sec   Loss 31.5160   LearningRate 0.0903   Epoch: 0   Global Step: 165890   Fp16 Grad Scale: 65536   Required: 357 hours
Training: 2022-05-05 21:51:34,758-Speed 744.11 samples/sec   Loss 31.5816   LearningRate 0.0903   Epoch: 0   Global Step: 165900   Fp16 Grad Scale: 65536   Required: 357 hours
Training: 2022-05-05 21:51:38,484-Speed 687.12 samples/sec   Loss 31.4126   LearningRate 0.0903   Epoch: 0   Global Step: 165910   Fp16 Grad Scale: 65536   Required: 357 hours

```



I compared this result with the [[official logs(wf42m_pfc02_r100/training.log)](https://raw.githubusercontent.com/anxiangsir/insightface_arcface_log/master/wf42m_pfc02_r100/training.log)](https://raw.githubusercontent.com/anxiangsir/insightface_arcface_log/master/wf42m_pfc02_r100/training.log), and found that my results were much worse.

- My loss only dropped to 30 in the first epoch but the official one can drop to 14. Meanwhile, my results are much worse on the three test sets(""lfw"", ""cfp_fp"", ""agedb_30"")



My hardware environment is 2 GPUs(RTX 3090, 24G), I think maybe there are some setup issues due to the difference in the number of GPUs. 

How to correctly train the model arcface in Webface?



Thanks!"
What is the loss function of buffalo_l? is that arcface?
您好，我在训练glint360数据集的时候使用8卡V100 32G显存的一台服务器，batch_size设置为512的时候，第一张显卡显存会爆，提示RuntimeError: CUDA out of memory，但是剩余7张显卡都没有用满32G显存，请问下这个问题怎么解决呢？期待您的回复，十分感谢！
"Hi Team, thanks for making such a wonderful open-source repo. 
I am trying to test the retina face but while running `make` command I am facing below error.

```
cd rcnn/cython/; python setup.py build_ext --inplace; rm -rf build; cd ../../
Skipping GPU_NMS
running build_ext
skipping 'bbox.c' Cython extension (up-to-date)
skipping 'anchors.c' Cython extension (up-to-date)
skipping 'cpu_nms.c' Cython extension (up-to-date)
building 'bbox' extension
creating build
creating build/temp.linux-x86_64-3.7
x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/akshaysharma/Documents/face_match/venv/lib/python3.7/site-packages/numpy/core/include -I/home/akshaysharma/Documents/face_match/venv/include -I/usr/include/python3.7m -c bbox.c -o build/temp.linux-x86_64-3.7/bbox.o -Wno-cpp -Wno-unused-function
bbox.c:6:10: fatal error: Python.h: No such file or directory
 #include ""Python.h""
          ^~~~~~~~~~
compilation terminated.
error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
cd rcnn/pycocotools/; python setup.py build_ext --inplace; rm -rf build; cd ../../
running build_ext
building '_mask' extension
creating build
creating build/temp.linux-x86_64-3.7
x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/akshaysharma/Documents/face_match/venv/lib/python3.7/site-packages/numpy/core/include -I/home/akshaysharma/Documents/face_match/venv/include -I/usr/include/python3.7m -c _mask.c -o build/temp.linux-x86_64-3.7/_mask.o -Wno-cpp -Wno-unused-function -std=c99
_mask.c:36:10: fatal error: Python.h: No such file or directory
 #include ""Python.h""
          ^~~~~~~~~~
compilation terminated.
error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
```
I have tried various things but nothing works. Any suggestion/solution?
Thanks

System: Ubuntu"
"Partial FC: Training 10 Million Identities on a Single Machine
What does the formula 3 and 2 means in the formula?"
"Killing Two Birds with One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC
what is means?"
""
""
""
"backbone = torch.nn.parallel.DistributedDataParallel(
        module=backbone, broadcast_buffers=False, device_ids=[args.local_rank], bucket_cap_mb=16, 
        find_unused_parameters=True)

backbone._set_static_graph()"
"im trying to train insightFace on CASIA - Webface, and i tryed to train arcface and cosface from scratch. Due my limit system, i just set batch size to 64. My result when i training arcface, cosface on LFW are 99,3 and 99,4 respectively, not like result in the paper.

could anyone give me some advice on how to train arcface, learning rate...?

Configs  for arc/cosface are same, like this:
## hyper params
weight_decay:  5e-4
logits_scale: 30
logits_margin: 0.5 ( 0.35 for cosface)
drop_ratio: 0.5 
##-------------------
#optimizer
optimizer: SGD
momentum: 0.9
base_lr : 0.01
lr_steps: [20000, 28000] 

Thanks in advance!"
""
"I have used pytorch3d==0.4.0.
And I trained and tested using your data and scripts.
But the result seems not so good.
training result blew
Detailed_3dmm_400.obj  
<img width=""454"" alt=""image"" src=""https://user-images.githubusercontent.com/12846142/159428765-54ae61fa-03d3-428e-ba09-1f6a5c9b42a7.png"">

and eval result blew
eval_013.png
<img width=""370"" alt=""image"" src=""https://user-images.githubusercontent.com/12846142/159429093-c76fcbf8-bdca-47bb-96ac-70178487a60a.png"">

could you provide some tips for training?
"
"In reconstruction/PBIDR  paert,
running ""bash script/data_process.sh""  seems wrong.
creating images in data/image and data/mask folder.  all images are whilte"
"Hello,
https://github.com/deepinsight/insightface/blob/478aafb4fc66030e07fb46143e8e069f85e68147/detection/scrfd/tools/scrfd.py#L26
It seems that the postprocess limits input-size in times of 64. I meet error when detecting img in size (640, 481). So how can I evalute widerface in origin size(README has shown the results of origin size). In addition, I test wideface validation with this code(https://github.com/deepinsight/insightface/blob/478aafb4fc66030e07fb46143e8e069f85e68147/detection/scrfd/tools/scrfd.py) in VGA(640, 480) resolution but get low score than noted in README. I wonder if any TTA(such as flip) do you use in testing process.
Looking forward to your answer. Thanks."
"Hi,

@nttstar @yingfeng 
Is there any r200 model or training example ? 


Best"
"How to use [Glint360K](https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc#4-download) dataset to eval model ?
How can I follow the step to eval ?
https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch"
There are several algorithm evaluated on the dataset **IFRT**. Can you show the developers the link or the location of the IFRT dataset?  Thanks!
"i have download the dataset from https://www.face-benchmark.org/download.html, but I do not find the webface-12M dataset list. what should i do to get the webface-12M? "
"I downloaded the model :ms1mv3_arcface_r50_fp16, for eval lfw, get results:
![image](https://user-images.githubusercontent.com/35617526/154442144-ef31a6bc-11f8-472e-9204-30d70b620ea9.png)
but, This result is the result of 10 fold cross validation.The threshold is set to1.46, and I calculated that the result on the whole lfw verification set is very poor? as follows: acc only 74.8%
![image](https://user-images.githubusercontent.com/35617526/154443475-0622e0dc-80e4-4317-ad99-57feb3d60fd7.png)
why? Is it normal?

"
"Were the face extracted and aligned before training? If so, do we have to follow that for inference too?"
I could not find the WebFace42M-PartialFC-0.2 R100 weights that I saw newly added to the performance table. Am I missing something or the weights are not published yet?
"hi @nttstar @ppwwyyxx 
Thanks for your valuable work. I would like to know more about your verification method (/recognition/arcface_mxnet/verification.py)
Is there any paper or tutor about how it works?
"
"Good Afternoon, 

I have a current setup of two machines in the same network, each of them presenting a RTX3090 GPU, and despite intializing a process in each of the machines (Checked with nvidia-smi), and the master machine printing ""Training YYYY-MM-DD HH:MM:SS,MS-rank_id: 0"" both processes seem to freeze (power usage decay on GPUs) and later on the following error is raised in the master machine:

```
python_env/lib64/python3.6/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated and will be removed in future. Use torchrun.                                                                                                                
Note that --use_env is set by default in torchrun.                                                                                                                          
If your script expects `--local_rank` argument to be set, please                                                                                                             
change it to read from `os.environ['LOCAL_RANK']` instead. See                                                                                                               
https://pytorch.org/docs/stable/distributed.html#launch-utility for                                                                                                          
further instructions                                                                                                                                                         
                                                                                                                                                                             
  FutureWarning,                                                                                                                                                            
Traceback (most recent call last):                                                                                                                                           
  File ""train.py"", line 141, in <module>                                                                                                                                     
    main(parser.parse_args())                                                                                                                                                
  File ""train.py"", line 59, in main                                                                                                                                          
    module=backbone, broadcast_buffers=False, device_ids=[local_rank])                                                                                                       
  File ""/home/nunes/python_env/lib64/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 578, in __init__                                                        
    dist._verify_model_across_ranks(self.process_group, parameters)                                                                                                          
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:957, unhandled system error, NCCL version 21.0.3                                            
ncclSystemError: System call (socket, malloc, munmap, etc) failed.                                                                                                           
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 123541) of binary: /home/nunes/python_env/bin/python
```

Both machines present the same NCCL (21.0.3) and Driver Versions (510.47.03).
(Fun fact, swapping the ranks and the master machine, the error still pop on the same machine, implying the problem is with such machine.)

These are my running configurations:

Master (Machine 1) - Rank 0
```bash
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=1 --master_addr=""192.168.0.207"" --master_port=1234 train.py configs/test_02
ps -ef | grep ""train"" | grep -v grep | awk '{print ""kill -9 ""$2}' | sh
```

Machine 2 - Rank 1
```bash
CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 --nnodes=2 --node_rank=0 --master_addr=""192.168.0.207"" --master_port=1234 train.py configs/test_02
ps -ef | grep ""train"" | grep -v grep | awk '{print ""kill -9 ""$2}' | sh
``` 

Any ideas what might be the cause of this problem?

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Edit:

Okay, so I enabled NCCL Debug with `export NCCL_DEBUG=INFO` and seems like it is a NCCL connection problem according to this output:

```
FutureWarning,
gr3090:166652:166652 [0] NCCL INFO Bootstrap : Using enp6s0:192.168.0.136<0>
gr3090:166652:166652 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
gr3090:166652:166652 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
gr3090:166652:166652 [0] NCCL INFO NET/Socket : Using [0]enp6s0:192.168.0.136<0>
gr3090:166652:166652 [0] NCCL INFO Using network Socket
gr3090:166652:166738 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
gr3090:166652:166738 [0] NCCL INFO Channel 00 : 0[1000] -> 1[9000] [receive] via NET/Socket/0
gr3090:166652:166738 [0] NCCL INFO Channel 01 : 0[1000] -> 1[9000] [receive] via NET/Socket/0
gr3090:166652:166738 [0] NCCL INFO Channel 00 : 1[9000] -> 0[1000] [send] via NET/Socket/0
gr3090:166652:166738 [0] NCCL INFO Channel 01 : 1[9000] -> 0[1000] [send] via NET/Socket/0
gr3090:166652:166738 [0] NCCL INFO Call to connect returned Connection timed out, retrying
gr3090:166652:166738 [0] NCCL INFO Call to connect returned Connection timed out, retrying

gr3090:166652:166738 [0] include/socket.h:409 NCCL WARN Net : Connect to 192.168.122.1<49891> failed : Connection timed out
gr3090:166652:166738 [0] NCCL INFO transport/net_socket.cc:316 -> 2
gr3090:166652:166738 [0] NCCL INFO include/net.h:21 -> 2
gr3090:166652:166738 [0] NCCL INFO transport/net.cc:210 -> 2
gr3090:166652:166738 [0] NCCL INFO transport.cc:111 -> 2
gr3090:166652:166738 [0] NCCL INFO init.cc:778 -> 2
gr3090:166652:166738 [0] NCCL INFO init.cc:904 -> 2
gr3090:166652:166738 [0] NCCL INFO group.cc:72 -> 2 [Async thread]
Traceback (most recent call last):
  File ""train.py"", line 141, in <module>
    main(parser.parse_args())
  File ""train.py"", line 59, in main
    module=backbone, broadcast_buffers=False, device_ids=[local_rank])
  File ""/home/nunes/penv/lib64/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 578, in __init__
    dist._verify_model_across_ranks(self.process_group, parameters)
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:957, unhandled system error, NCCL version 21.0.3
ncclSystemError: System call (socket, malloc, munmap, etc) failed.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 166652) of binary: /home/nunes/penv/bin/python
```"
"Hi,

Thank you for your continuous hard work on insightface. Are you guys going to release the model for `WebFace42M-PartialFC-0.2` ? I tried looking in the model zoo but I couldn't find them"
"i am trying to run the simple python-package, folder but am getting : ModuleNotFoundError: No module named 'insightface.data'
what might be the issue?

Package                Version
---------------------- -----------
albumentations         1.1.0
certifi                2021.5.30
chardet                3.0.4
charset-normalizer     2.0.11
colorama               0.4.4
cycler                 0.11.0
Cython                 0.29.27
decorator              4.4.2
easydict               1.9
flatbuffers            2.0
graphviz               0.8.4
idna                   2.6
imageio                2.14.1
insightface            0.2.1
joblib                 1.1.0
kiwisolver             1.3.1
matplotlib             3.3.4
mxnet                  1.7.0.post2
networkx               2.5.1
numpy                  1.16.6
onnx                   1.10.2
onnxruntime            1.10.0
opencv-python          4.5.5.62
opencv-python-headless 4.5.5.62
Pillow                 8.4.0
pip                    21.2.2
protobuf               3.19.4
pyparsing              3.0.7
python-dateutil        2.8.2
PyWavelets             1.1.1
PyYAML                 6.0
qudida                 0.0.4
requests               2.18.4
scikit-image           0.17.2
scikit-learn           0.24.2
scipy                  1.5.4
setuptools             58.0.4
six                    1.16.0
threadpoolctl          3.1.0
tifffile               2020.9.3
tqdm                   4.62.3
typing_extensions      4.0.1
urllib3                1.22
wheel                  0.37.1
wincertstore           0.2
"
There is no `cfg.resume` used in `train.py` and `partial_fc.py` does not have code to load saved `.pt` files.
"In `train.py` loss is not being passed as an argument, so default CosFace loss is used even the config file shows different loss name.

Line 54 add **cfg.loss**
```
module_partial_fc = PartialFC(
        cfg.embedding_size, 
        cfg.num_classes, 
        cfg.sample_rate, 
        cfg.fp16,
        cfg.loss
    )
```
"
"In Arcface class, self.easy_margin is not defined.  Variable easy_margin must be passed as input during initialization
"
"Hi

I'm looking to do some unsupervised face clustering with the face vectors produced by insightface. I have trained on a custom dataset (ms1m + 10,000 of my own images) and have tried clustering with both HDBSCAN and DBSCAN (using cosine distance as a metric), but seem to have much lower performance than dlib + chinese whisper clustering.

I have trained insightface with embedding_size = 256 to reduce complexity (hdbscan performs poorly in high dimensions), but am still getting poor results (0 clusters, 100s of clusters when there should be 10)

Has anyone tried training insightface to perform face clustering? If so, are there any steps that I can do to improve the performance?"
"Dear @nttstar ,
Thanks for sharing your excellent work.
I am interested in training `SCRFD` for multi-classes datasets, for example `{""person"", ""head"", ""face""}`.
I refer to your change from training face to person: https://github.com/deepinsight/insightface/blob/f1e4b1291a388b0721463bdb7ec50b50bc7bd684/detection/scrfd/configs/scrfd_crowdhuman/scrfd_crowdhuman_2.5g_bnkps.py#L144
and I set configs to my 3-class {""person"", ""head"", ""face""} detection dataset:

```
        scale_mode=2,
        anchor_generator=dict(
            type='AnchorGenerator',
            ratios=[1.0, 2.0],
            scales = [1,2,3],
            base_sizes = [8, 16, 32, 64, 128],
            strides=[8, 16, 32, 64, 128]),
```

and I observe that the `person AP` is very small. It is unusual. Normally, `person AP` is larger than that of face. 
![Screen Shot 2022-01-18 at 16 21 36](https://user-images.githubusercontent.com/7928673/149914101-01d32d2c-262b-4e6f-a048-d2e693c613a9.png)

Do you have any suggestion?
Thanks for your time."
"[Here](https://github.com/deepinsight/insightface/tree/master/python-package) is clearly stated that pre-trained models weights are not under an MIT license.
So under what license they are published?
 
Would be it ok if I modify the models and publish them on Github attributing this repo for base weights?
```
The code of InsightFace Python Library is released under the MIT License. There is no limitation for both academic and commercial usage.

The pretrained models we provided with this library are available for non-commercial research purposes only, including both auto-downloading models and manual-downloading models.
```"
"Hi, I came across this
https://github.com/deepinsight/insightface/tree/master/alignment/synthetics

afaik it is the first open source implementation of a face alignment model train on that synthetic dataset. Thanks for sharing. 

Wonder if there is any plan to share the pre-trained model?"
I have downloaded and extracted glint360 dataset. Now I have 7 files: glint360k_00 to glint360k_06. How can I further extract the images? I want to access the individual images.
"![image](https://user-images.githubusercontent.com/30692249/147542350-b1ced342-4f21-4580-b425-609ad13b3ecd.png)
随机边缘遮挡这里存在bug，应该如下，否则大概率产生全黑的图像
![image](https://user-images.githubusercontent.com/30692249/147542421-442c02a9-97a1-4212-af3e-300eb21eb442.png)
"
"I download CASIA-webface dataset from the dataset wiki in this project and convert face images to record file via im2rec.py.
According to your paper, max step is set to 32,000, the start lr is 0.1 and lr decay septs are 20,000 and 28,000. Batch size is 128*4. 
The training environment is V100 *4. network is resnet50. training dataset is casia-webface. val target is lfw and so on.
But the training loss always turned to be nan after few epochs. Is there any suggestion?
 @nttstar "
Is it possible to use this library for training retinanet and other models in pytorch? Are there copies of the mxnet model weights/code in pytorch?
"Hello to every one,
Every time i run quick example of insightface in this [link](https://github.com/deepinsight/insightface/tree/master/python-package).
I get the following error : 
<pre>
app = FaceAnalysis()
File ""F:\Programming\python36\lib\site-packages\insightface\app\face_analysis.py"", line 31, in __init__
model = model_zoo.get_model(onnx_file, **kwargs)
File ""F:\Programming\python36\lib\site-packages\insightface\model_zoo\model_zoo.py"", line 70, in get_model
model = router.get_model(providers=kwargs.get('providers'), provider_options=kwargs.get('provider_options'))
File ""F:\Programming\python36\lib\site-packages\insightface\model_zoo\model_zoo.py"", line 26, in get_model
session = onnxruntime.InferenceSession(self.onnx_file, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'provider_options'
</pre>
Can anyone help me?"
"在insightface/recognition/arcface_paddle/README_cn.md里面
解析数据的时候是：python tools/mx_recordio_2_images.py --root_dir ms1m-retinaface-t1/ --output_dir MS1M_v3/
启动训练的时候配置的参数是    
     --dataset MS1M_v2 \
     --data_dir MS1M_v2/ \
这个是不是写错了？
而且这里训的是人脸识别 arcface，为什么解析数据贴到是retinaface"
"Traceback (most recent call last):
  File ""train.py"", line 141, in <module>
    main(parser.parse_args())
  File ""train.py"", line 110, in main
    features = F.normalize(backbone(img))
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/nn/parallel/distributed.py"", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/bahy/GIABAO/insightface_1/recognition/arcface_torch/backbones/iresnet.py"", line 152, in forward
    x = self.fc(x.float() if self.fp16 else x)
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py"", line 93, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/nn/functional.py"", line 1690, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: mat1 dim 1 must match mat2 dim 0
Traceback (most recent call last):
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/distributed/launch.py"", line 260, in <module>
    main()
  File ""/home/bahy/anaconda3/envs/fs_pytorch/lib/python3.6/site-packages/torch/distributed/launch.py"", line 256, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/home/bahy/anaconda3/envs/fs_pytorch/bin/python', '-u', 'train.py', '--local_rank=0', 'configs/ms1mv3_r50']' returned non-zero exit status 1.

Hi i run trainning and i have this error  
Can anyone help me :("
"How to run insightface with gpu on jetson nano?
- python 3.6
- CUDA Version 10.2.89
- insightface 0.5
- onnx 1.10.2
- onnxruntime 1.10.0
- onnxruntime-gpu 1.8.0"
""
"Hi, is there any trick to set max_norm=5 in [the following line](https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/train.py#L120)
Thanks!"
"```
from insightface.app import FaceAnalysis
import insightface
app = FaceAnalysis(allowed_modules=[""detection""])
detector = insightface.model_zoo.get_model('buffalo_l/det_10g.onnx', download=True)
print(detector.__class__)
```
> insightface.model_zoo.retinaface.RetinaFace"
""
"This is not a bug report nor feature request, at best it can be seen as a documentation request.

Which model available for the python package gives currently the best accuracy for face detection? (It seems to be RetinaFace-R50 right?)
If we allow to be 5% below the best accuracy, which model has the highest prediction speed? (ie. could you add infer (ms) to the RetinaFace table? It looks like SCRFD_10G gives a good tradeoff?)

I am looking at: https://github.com/deepinsight/insightface/tree/master/model_zoo#2-face-detection-models and to me it's not clear which models use PyTorch. "
"I run file scrfd_person.py , it's show
`Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}`
how to fix it?"
"I try to do inference on your pretrained model following: https://github.com/deepinsight/insightface/blob/master/examples/demo_analysis.py
Here's my code:
```
import insightface
from cv2 import cv2

app = insightface.app.FaceAnalysis()
app.prepare(0, det_size=640)
image = cv2.imread(""my-picture.jpg"")
faces = app.get(image)
assert len(faces)==6
rimg = app.draw_on(image, faces)
cv2.imwrite(""output.jpg"", rimg)
```

Error: 
```
Traceback (most recent call last):
  File ""E:\Work work\Python\Work\atin\FaceRecog\main.py"", line 7, in <module>
    faces = app.get(image)
  File ""E:\Anaconda\envs\insightface\lib\site-packages\insightface\app\face_analysis.py"", line 59, in get
    bboxes, kpss = self.det_model.detect(img,
  File ""E:\Anaconda\envs\insightface\lib\site-packages\insightface\model_zoo\retinaface.py"", line 212, in detect
    model_ratio = float(input_size[1]) / input_size[0]
TypeError: 'int' object is not subscriptable
find model: .insightface\models\buffalo_l\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5
set det-size: 640
```
"
""
"Hi, thanks for your great repo and implementation of arcface_torch.
I am training resnet18 using webface dataset, after keyboard interrupt，the resume training turns to start training from epoch 0.
Here is the changed part I revise to make it work(I have one GPU, so I only tested it on one GPU)
the changed lines are marked with changed
main.py
```python

import argparse
import logging
import os

import torch
import torch.distributed as dist
import torch.nn.functional as F
import torch.utils.data.distributed
from torch.nn.utils import clip_grad_norm_

import losses
from backbones import get_model
from dataset import MXFaceDataset, SyntheticDataset, DataLoaderX
from partial_fc import PartialFC
from utils.utils_amp import MaxClipGradScaler
# changed
from utils.utils_callbacks import CallBackVerification, CallBackLogging, CallBackLoggingResume, CallBackModelCheckpoint, CallBackModelCheckpointResume
from utils.utils_config import get_config
from utils.utils_logging import AverageMeter, init_logging


def main(args):
    cfg = get_config(args.config)
    try:
        world_size = int(os.environ['WORLD_SIZE'])
        rank = int(os.environ['RANK'])
        dist.init_process_group('nccl') 
    except KeyError:
        world_size = 1
        rank = 0
        dist.init_process_group(backend='nccl', init_method=""tcp://127.0.0.1:12584"", rank=rank, world_size=world_size)

    local_rank = args.local_rank
    torch.cuda.set_device(local_rank)
    os.makedirs(cfg.output, exist_ok=True)
    init_logging(rank, cfg.output)

    if cfg.rec == ""synthetic"":
        train_set = SyntheticDataset(local_rank=local_rank)
    else:
        train_set = MXFaceDataset(root_dir=cfg.rec, local_rank=local_rank)
    num_image = len(train_set)

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set, shuffle=True)
    train_loader = DataLoaderX(
        local_rank=local_rank, dataset=train_set, batch_size=cfg.batch_size,
        sampler=train_sampler, num_workers=2, pin_memory=True, drop_last=True)
    backbone = get_model(cfg.network, dropout=0.0, fp16=cfg.fp16, num_features=cfg.embedding_size).to(local_rank)
    
    total_batch_size = cfg.batch_size * world_size
    cfg.warmup_step = num_image // total_batch_size * cfg.warmup_epoch
    cfg.total_step = num_image // total_batch_size * cfg.num_epoch

    def lr_step_func(current_step):
        cfg.decay_step = [x * num_image // total_batch_size for x in cfg.decay_epoch]
        if current_step < cfg.warmup_step: 
            return current_step / cfg.warmup_step
        else:
            return 0.1 ** len([m for m in cfg.decay_step if m <= current_step])
    
    # changed
    if cfg.resume:
        try:   
            backbone_pth = os.path.join(cfg.output, ""savedckpt.pth"")
            savedckpt = torch.load(backbone_pth, map_location=torch.device(local_rank))
            start_epoch = savedckpt['epoch'] + 1
            global_step = int(num_image/cfg.batch_size) * (savedckpt['epoch']+1) + 1 
            backbone.load_state_dict(savedckpt['backbone'].module.state_dict())
            if rank == 0:
                logging.info(""backbone resume successfully!"")
        except (FileNotFoundError, KeyError, IndexError, RuntimeError):
            if rank == 0:
                logging.info(""resume fail, backbone init successfully!"")
    else:
        start_epoch = 0
        global_step = 0

    backbone = torch.nn.parallel.DistributedDataParallel(
        module=backbone, broadcast_buffers=False, device_ids=[local_rank])
    backbone.train()
    margin_softmax = losses.get_loss(cfg.loss)
    module_partial_fc = PartialFC(
        rank=rank, local_rank=local_rank, world_size=world_size, resume=cfg.resume,
        batch_size=cfg.batch_size, margin_softmax=margin_softmax, num_classes=cfg.num_classes,
        sample_rate=cfg.sample_rate, embedding_size=cfg.embedding_size, prefix=cfg.output)

    opt_backbone = torch.optim.SGD(
        params=[{'params': backbone.parameters()}],
        lr=cfg.lr / 512 * cfg.batch_size * world_size,
        momentum=0.9, weight_decay=cfg.weight_decay)
    opt_pfc = torch.optim.SGD(
        params=[{'params': module_partial_fc.parameters()}],
        lr=cfg.lr / 512 * cfg.batch_size * world_size,
        momentum=0.9, weight_decay=cfg.weight_decay)

    scheduler_backbone = torch.optim.lr_scheduler.LambdaLR(
        optimizer=opt_backbone, lr_lambda=lr_step_func)
    scheduler_pfc = torch.optim.lr_scheduler.LambdaLR(
        optimizer=opt_pfc, lr_lambda=lr_step_func)

    for key, value in cfg.items():
        num_space = 25 - len(key)
        logging.info("": "" + key + "" "" * num_space + str(value))

    val_target = cfg.val_targets 
    callback_verification = CallBackVerification(2000, rank, val_target, cfg.rec)
    # changed
    callback_logging = CallBackLoggingResume(50, rank, cfg.total_step, global_step, cfg.batch_size, world_size, None)
    callback_checkpoint = CallBackModelCheckpointResume(rank, cfg.output)

    loss = AverageMeter()
    grad_amp = MaxClipGradScaler(cfg.batch_size, 128 * cfg.batch_size, growth_interval=100) if cfg.fp16 else None
    for epoch in range(start_epoch, cfg.num_epoch):
        train_sampler.set_epoch(epoch)
        for step, (img, label) in enumerate(train_loader):
            global_step += 1
            features = F.normalize(backbone(img))
            x_grad, loss_v = module_partial_fc.forward_backward(label, features, opt_pfc)
            if cfg.fp16:
                features.backward(grad_amp.scale(x_grad))
                grad_amp.unscale_(opt_backbone)
                clip_grad_norm_(backbone.parameters(), max_norm=5, norm_type=2)
                grad_amp.step(opt_backbone)
                grad_amp.update()
            else:
                features.backward(x_grad)
                clip_grad_norm_(backbone.parameters(), max_norm=5, norm_type=2)
                opt_backbone.step()

            opt_pfc.step()
            module_partial_fc.update()
            opt_backbone.zero_grad()
            opt_pfc.zero_grad()
            loss.update(loss_v, 1)
            callback_logging(global_step, loss, epoch, cfg.fp16, scheduler_backbone.get_last_lr()[0], grad_amp)
            callback_verification(global_step, backbone)
            scheduler_backbone.step()
            scheduler_pfc.step()
        callback_checkpoint(global_step, epoch, backbone, module_partial_fc)
    dist.destroy_process_group()


if __name__ == ""__main__"":
    torch.backends.cudnn.benchmark = True
    parser = argparse.ArgumentParser(description='PyTorch ArcFace Training')
    parser.add_argument('config', type=str, help='py config file')
    parser.add_argument('--local_rank', type=int, default=0, help='local_rank')
    main(parser.parse_args())

```

in utils_callback.py, I changed the following two:(only add one or two lines)
CallBackLoggingResume
```python
class CallBackLoggingResume(object):
    def __init__(self, frequent, rank, total_step, start_step, batch_size, world_size, writer=None):
        self.frequent: int = frequent
        self.rank: int = rank
        self.time_start = time.time()
        # added
        self.start_step: int = start_step
        self.total_step: int = total_step
        self.batch_size: int = batch_size
        self.world_size: int = world_size
        self.writer = writer

        self.init = False
        self.tic = 0

    def __call__(self,
                 global_step: int,
                 loss: AverageMeter,
                 epoch: int,
                 fp16: bool,
                 learning_rate: float,
                 grad_scaler: torch.cuda.amp.GradScaler):
        if self.rank == 0 and global_step > 0 and global_step % self.frequent == 0:
            if self.init:
                try:
                    speed: float = self.frequent * self.batch_size / (time.time() - self.tic)
                    speed_total = speed * self.world_size
                except ZeroDivisionError:
                    speed_total = float('inf')

                time_now = (time.time() - self.time_start) / 3600 
                # changed
                time_total = time_now / ((global_step-self.start_step + 1) / self.total_step)
                time_for_end = time_total - time_now

                if self.writer is not None:
                    self.writer.add_scalar('time_for_end', time_for_end, global_step)
                    self.writer.add_scalar('learning_rate', learning_rate, global_step)
                    self.writer.add_scalar('loss', loss.avg, global_step)
                if fp16:
                    msg = ""Speed %.2f samples/sec   Loss %.4f   LearningRate %.4f   Epoch: %d   Global Step: %d   "" \
                          ""Fp16 Grad Scale: %2.f   Required: %1.f hours"" % (
                              speed_total, loss.avg, learning_rate, epoch, global_step,
                              grad_scaler.get_scale(), time_for_end
                          )
                else:
                    msg = ""Speed %.2f samples/sec   Loss %.4f   LearningRate %.4f   Epoch: %d   Global Step: %d   "" \
                          ""Required: %1.f hours"" % (
                              speed_total, loss.avg, learning_rate, epoch, global_step, time_for_end
                          )
                logging.info(msg)
                loss.reset()
                self.tic = time.time()
            else:
                self.init = True
                self.tic = time.time()
```
and  CallBackModelCheckpointResume
```python
class CallBackModelCheckpointResume(object):
    def __init__(self, rank, output=""./""):
        self.rank: int = rank
        self.output: str = output

    def __call__(self, global_step, epoch, backbone, partial_fc):
        if global_step > 100 and self.rank == 0:
            path_module = os.path.join(self.output, ""savedckpt.pth"")
            # changed
            state = {'epoch': epoch, 'backbone': backbone}
            torch.save(state, path_module)
            logging.info(""Pytorch Model Saved in '{}'"".format(path_module))

        if global_step > 100 and partial_fc is not None:
            partial_fc.save_params()
```



"
"mxnet 1.8
partical fc 0.1

I reimplement the partial fc code in my code, but the training speed decresed from 580 --> 270 samples/s.  

I git clone the code from insightface , the same thing occurs.

I use num_work=2 in my code and dose not use modify the code cloned from insightface .

"
"```
import cv2
import numpy as np
import insightface
from insightface.app import FaceAnalysis
from insightface.data import get_image as ins_get_image

app = FaceAnalysis()
app.prepare(ctx_id=0, det_size=(640, 640))
img = ins_get_image('t1')
faces = app.get(img)
rimg = app.draw_on(img, faces)
cv2.imwrite(""./t1_output.jpg"", rimg)
```


---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-3-91a2923a61d1> in <module>
      3 import insightface
      4 from insightface.app import FaceAnalysis
----> 5 from insightface.data import get_image as ins_get_image
      6 
      7 app = FaceAnalysis()

ModuleNotFoundError: No module named 'insightface.data'"
"Hi! I have noticed that r50 Webface600k model is mentioned twice in model zoo readme, with different reported accuracy.

First is the `w600k_r50` model from `buffalo_l` and `buffalo_m` [packages](https://github.com/deepinsight/insightface/tree/master/model_zoo#recognition-accuracy-of-python-library-model-packs) with reported MR-All accuracy 91.25 and IJB-C(E4) accuracy 97.25, which seems to be higher than accuracy reported for Glint360K r100 model.

Second is `WebFace600K` model from IResNet-50 [block](https://github.com/deepinsight/insightface/tree/master/model_zoo#list-of-models-by-iresnet-50-and-different-training-datasets) with reported MR-All accuracy 90.566 and IJB-C(E4) accuracy 97.120.

I have checked outputs of both models, they are different for same face, so models seems to be different indeed, but is it true that r50 model is more accurate than you previous best r100 model?"
"SCRFD 2.5g bnkps onnx model is perfect. But 2.5g(only bbox) onnx model have a problem. 2.5g onnx model have İnstance normalization but 2.5g_bnkps is not have this layers. I didint convert  2.5g onnx model to tensorrt. 
Fist model is 2.5g_bnkps
![Screenshot from 2021-11-05 17-30-31](https://user-images.githubusercontent.com/44634165/140526901-024658dc-2c5b-4987-9fdd-a1dfa6925e13.png)
Second Model is 2.5g(only_bbox)
![Screenshot from 2021-11-05 17-29-54](https://user-images.githubusercontent.com/44634165/140526910-41407cd2-f11a-48ad-82a6-97ebc1a42b23.png)

 "
"Hi, I am new to such project, I want to train a big classification model with partial_fc , in my code my batch is a list of images, which is the reason I can not use DataloaderX. 
1. Can I use normal dataloader to alternate DataloaderX?
2.  Could you pleaser give me some intuition about DataloaderX?

非常感谢"
"![image](https://user-images.githubusercontent.com/47652945/139006630-2d58309a-ab0a-40ce-97fa-b304d293b79f.png)
"
"![image](https://user-images.githubusercontent.com/47652945/138863307-897fe7d0-a564-4107-baa7-b23d9d56c5a7.png)
我看这里提供的都是resnet50模型比较大,我想放在手机上运行不合适"
"Hello!
Do you support setting the image-size (ex. to 64x64) in training script in Insightface-Pytorch?

Thanks ~"
Can some please help me what is iResnet? What is the difference between resnet and iresnet? where can find details about iResnet?
"Hi! I have noticed you mentioned this dataset for many recently released pretained models, but when I try to google it I can only find WebFace260k dataset.

Is it a typo, or this is you internal unreleased dataset?"
"RuntimeError: simple_bind error. Arguments: 
data: (32, 1, 130, 112)
Error in operator pre_fc1: Shape inconsistent, Provided = [128,512], inferred shape=(128,1536)"
"Hello,
I am currently using your pretrained model: ms1mv2_arcface_r50_fp16 according this link: https://onedrive.live.com/?authkey=%21AFZjr283nwZHqbA&id=4A83B6B633B029CC%215583&cid=4A83B6B633B029CC 
I try to inference on LFW and TAR@FAR=1e-3 is only about 22.7% while in the arcface paper this number is over 90%
Because I run the file inference.py to test so I think it's not because of the code
Can you tell me whether the model in above link is your best model or just an average model?
Thank you very much!
"
"```
def test(data_set, backbone, batch_size, nfolds=10):
    print('testing verification..')
    data_list = data_set[0]
    issame_list = data_set[1]
    embeddings_list = []
    time_consumed = 0.0
    for i in range(len(data_list)):
        data = data_list[i]
        embeddings = None
        ba = 0
        while ba < data.shape[0]:
            bb = min(ba + batch_size, data.shape[0])
            count = bb - ba
            _data = data[bb - batch_size: bb]
            time0 = datetime.datetime.now()
            img = ((_data / 255) - 0.5) / 0.5
            **net_out: torch.Tensor = backbone(img)**
            _embeddings = net_out.detach().cpu().numpy()
            time_now = datetime.datetime.now()
            diff = time_now - time0
            time_consumed += diff.total_seconds()
            if embeddings is None:
                embeddings = np.zeros((data.shape[0], _embeddings.shape[1]))
            embeddings[ba:bb, :] = _embeddings[(batch_size - count):, :]
            ba = bb
        embeddings_list.append(embeddings)
 I got  such error:
Traceback (most recent call last):
  File ""/home/yl/PycharmProjects/sumail/insightface/recognition/arcface_torch/eval/verification.py"", line 400, in <module>
    acc1, std1, acc2, std2, xnorm, embeddings_list = tes( ver_list[i], model, args.batch_size, args.nfolds)
  File ""/home/yl/anaconda3/envs/insightface/lib/python3.6/site-packages/torch/autograd/grad_mode.py"", line 49, in decorate_no_grad
    return func(*args, **kwargs)
  File ""/home/yl/PycharmProjects/sumail/insightface/recognition/arcface_torch/eval/verification.py"", line 244, in tes
    net_out: torch.Tensor = backbone(img)
TypeError: 'Module' object is not callable
"
"Hi, Thanks for your nice work!

I am going to fine-tune the ArcFace-Torch by using the checkpoints download from BaiduYun(e8pw):
link: https://pan.baidu.com/share/init?surl=CL-l4zWqsI1oDuEEYVhj-g
file path: arcface_torch/ms1mv3_arcface_r50_fp16/

I notice that the MS1M-RetinaFace training dataset's ""num_classes"" is 93431. However, the num_classes in ""weight"" loaded from the files ""rank_0_softmax_weigh.pt""~""rank_0_softmax_weigh.pt"" are 11398, 11398, 11398, 11398, 11397, 11397, 11397, 11397, respectively. And the sum of them is 91180, which is not 93431.

Thus, when I set ""resume=True"", the ""weight"" and ""weight_mom"" can not resume successfully. And then they are initialized randomly.

I wonder what is wrong with this?
Is it ok when I only load the pre-trained ""backbone"" but don't load the ""weight"" and ""weight_mom""?

I will be very happy if you can kindly reply to me."
"There are two pictures and they include the same person, but one picture rotation exceeds 90 degrees by rotating taking in Mobile, and another is normal.
then call the `detector.detect` to detect the person, then call `model.get_input` and `model.get_feature` to generate the feature for these two pictures. 
after two features are generated, then compare them, they are not matched, but they are the same person.

And it looks like if I use another rotating picture to generate a feature, and it is similar to the current rotating picture.

Can you help with this? thanks"
"Hi,
first of all thank you for providing the code, models and preprocessed images.

I downloaded the MS1M-ArcFace (85K ids/5.8M images) [5,7] from [here](https://github.com/deepinsight/insightface/tree/master/recognition/_datasets_) and extracted the images from the bin file using this [code](https://github.com/wujiyang/Face_Pytorch/blob/master/utils/load_images_from_bin.py).

Now when I compare the images that I aligned with the current face alignment (based on landmarks detected by MTCNN) and the ones I extracted, they look slightly different. I even tried the face alignment code from [2018](https://github.com/deepinsight/insightface/blob/60bb5829b1d76bfcec7930ce61c41dde26413279/src/common/face_preprocess.py), but I cannot reproduce the same alignment.

(Mine)
![Abel_Pacheco_0001](https://user-images.githubusercontent.com/38668954/133396842-0737ca1d-3208-4b30-bb6d-32da3925e510.jpg)
(Yours)
![0](https://user-images.githubusercontent.com/38668954/133396854-f5c20b1f-8c8d-4ed5-a44d-5017009addc1.jpg)

Can you provide me with more information on which alignment you used at the time? I don't think it's because of MTCNN and the landmarks, because at least the timestamp of the files says they are from 2018, so before RetinaFace.

My problem is that I have models that have been trained with this alignment, but the performance drops sharply when I evaluate them with the current alignment.

Thanks in advance!
Best,
Marco"
"Thank you for your work.
but why are your reported retinaface accuracy so low in hard subset?
<img width=""647"" alt=""37858330911fd89c9012bb77c98e7b7"" src=""https://user-images.githubusercontent.com/59432975/132636751-47355af9-42f1-4f9f-87ba-9eda0c6a5b66.png"">
"
"recognition onnx inference example for single image:
https://github.com/eeric/Face_recognition_cnn/tree/main/code/insightface/onnx_inference"
"Hello and thanks for great works,
My first question was while examining the pytorch implementation of Arcface, I saw the r2060 backbone in the table. But I could not find the weights of this model in the link. Haven't you posted yet or did I miss something?

![Screenshot from 2021-09-08 11-40-49](https://user-images.githubusercontent.com/53911245/132479994-91c066f5-c0c2-47db-9207-2972a6e76b95.png)
My second question is, I couldn't see any alignment process while inference. Did you use a different alignment template or is the old template still up to date? (Here I am assuming that you have trained pytorch from scratch and have not made any conversions from mxnet)
![Screenshot from 2021-09-08 12-05-14](https://user-images.githubusercontent.com/53911245/132480814-695053a0-3e5c-4b09-825c-cb778d30e72d.png)"
"Dear Sir,

When I trained the model with CASIA with batch size 256, the previous loss is around 5, the next loss becomes Nan. 
 "
the dataset link given in the  repo isnt working 
"when I ran the demo/image_demo.py file with the model SCRFD_2.5G_KPS ,I can not get five face keypoints. But while I convert the model to onnx and use tools/scrfd.py, I can get five face keypoints. Why? Thanks."
"想請問一下你們訓練時是用甚麼GPU配置
我用了一張Nvidia 3090去訓練,但是batch size只能去到256 這正常嗎
我訓練一個epoch要約40小時這也正常嗎
另外想問一下是不是因為缺少了rank0 （以我理解是softmax層）的參數,所以一開始的Loss有40左右
1萬個Global Step之後下降到20多
因為GPU能力有限所以我直接Warm Up Epoch調零

可以分享一下你們training的log嗎 我想參考一下你們loss的下降趨勢

感謝各位帥哥 大神!!!"
"I want to use this ""insightface/recognition/arcface_torch/inference.py"" to get feats from image, Do I need to perform face detection and face alignment first？Can you show me the usage?

我想用这段代码—“insightface/recognition/arcface_torch/inference.py”从图像提取特征，我是否事先要进行人脸检测和人脸对齐的操作呢？我希望有人能写个简单的例子说明下。"
"Hi,

I got this error on image_demo.py:

Traceback (most recent call last):
  File ""image_demo.py"", line 3, in <module>
    from mmdet.apis import inference_detector, init_detector, show_result_pyplot
  File ""/dados/mmdet/apis/__init__.py"", line 1, in <module>
    from .inference import (async_inference_detector, inference_detector,
  File ""/dados/mmdet/apis/inference.py"", line 7, in <module>
    from mmcv.ops import RoIPool
  File ""/opt/conda/lib/python3.6/site-packages/mmcv/ops/__init__.py"", line 2, in <module>
    from .bbox import bbox_overlaps
  File ""/opt/conda/lib/python3.6/site-packages/mmcv/ops/bbox.py"", line 4, in <module>
    ext_module = ext_loader.load_ext('_ext', ['bbox_overlaps'])
  File ""/opt/conda/lib/python3.6/site-packages/mmcv/utils/ext_loader.py"", line 13, in load_ext
    ext = importlib.import_module('mmcv.' + name)
  File ""/opt/conda/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: /opt/conda/lib/python3.6/site-packages/mmcv/_ext.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda12device_countEv

>>> torch.__version__
'1.7.0a0+8deb4fe'

>>> torch.version.cuda
'11.0'

>>> torch.cuda.is_available()
True

mmcv-full 1.3.11
"
"
class FaceAnalysis:
    def __init__(self, name=DEFAULT_MP_NAME, root='~/.insightface', allowed_modules=None):
        onnxruntime.set_default_logger_severity(3)
        self.models = {}
        self.model_dir = ensure_available('models', name, root=root)
        onnx_files = glob.glob(osp.join(self.model_dir, '*.onnx'))
        onnx_files = sorted(onnx_files)


没看到文档，代码中也没有注释"
"RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.


may because that multi input to model"
"For example, the freezing model can have a good training effect. Is there any research?"
""
" I had modified base on arcface_torch project with one gpu, please reference.

https://github.com/eeric/Face_recognition_cnn/tree/main/code/insightface/arcface_torch"
"Hi, I tried to download MS1MV2 with R100 backbone on MXnet, but it is uploaded on Bdrive which is in chinese language and I can't download it. could you please upload that in google drive? thanks in advance"
""
"Hi I tried to download [RetinaFace Pretrained Models on Dropbox](https://github.com/deepinsight/insightface/tree/master/detection/retinaface#retinaface-pretrained-models), however, the link doesn't work. Can you check it?"
"数据集：WIDERFACE
标注：README.md提供的annotations
预训练模型：README.md提供的RetinaFace-MobileNet0.25

CUDA_VISIBLE_DEVICES='0,1' python3 -u train.py --prefix ./model/mnet025 --network mnet
RPNAcc都有0.983以上，训练出来的模型好像随机性很强，有时候可以检测人脸，有时候完全没办法检测，有没谁碰到这种情况？帮忙指点一下，谢谢。"
"Hi, 
I'm using partial fc training on mxnet. 
When sample rate is set to 0.1, an AssertionError is raised by line 321 of memory_module.py [(link)](https://github.com/deepinsight/insightface/blob/337c8abda9a9930ac05f9562921510171935afd0/recognition/partial_fc/mxnet/memory_module.py#L321) as below:
```
assert memory_bank.num_local == memory_bank.num_sample, ""pass""
```
I found that these two parameters are defined in line 106 and 107 of train_memory.py [(link)](https://github.com/deepinsight/insightface/blob/337c8abda9a9930ac05f9562921510171935afd0/recognition/partial_fc/mxnet/train_memory.py#L106) as below:
```
num_local = (config.num_classes + size - 1) // size
num_sample = int(num_local * config.sample_ratio)
``` 
Apparently they are not equal if sample_ratio is set to 0.1. 
Is it not supported now or is there something I haven't noticed?
Thank you.

"
对人脸算法不是很了解，但知道人脸算法有自己的loss function，请问是用什么向量相似度函数来度量embedding结果
""
"训练一段时间，就不再动了，查看GPU有一个卡进程没了，试了好几次都这样
![image](https://user-images.githubusercontent.com/25198185/126742247-84c710e3-66bd-46e6-8e04-5d650c55f953.png)
"
"I use the Insightface library from Pypi (https://pypi.org/project/insightface/) and there is a severe memory leak of the CPU RAM, over 40 GB until I stopped it (not the GPU memory, it is fine, stable), 
it happens when I use the detector as follows:
```
import insightface
import cv2
import time

model = insightface.app.FaceAnalysis()

# It happens only when using GPU !!!
ctx_id = 0

image_path = ""my-face-image.jpg""
image = cv2.imread(image_path)

model.prepare(ctx_id = ctx_id, det_thresh=0.3, det_size=[416, 416])

detector =  model.models[""detection""]

for i in range(100000):
    start_t = time.time()
    bboxes, landmarks = detector.detect(image)
    end_t = time.time()
    print('Time detect is {}'.format(end_t - start_t))

print(f'DONE')
```
**My setup is (inside docker):**
- Docker Base Image - nvidia/cuda:11.0.3-cudnn8-devel-ubuntu18.04
- Nvidia Driver - 465.27
- python - 3.6.9
- insightface==0.3.8
- mxnet==1.8.0.post0
- mxnet-cu110==2.0.0a0
- numpy==1.18.5
- onnx==1.9.0
- onnxruntime-gpu==1.8.1

"
"The models download link give us pdiparams but the code is using pdparams.
Can you export pdparams for us to do inference?"
"I run cmd:
CUDA_VISIBLE_DEVICES=0,1 python3 -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=""0.0.0.0"" --master_port=12584 train.py configs/ms1mv3_r50
but I got the above errors, I check used port but I didn't any port like that work"
"
"
"Hello!
Is it possible somehow to send several images in one time to the net and get vectors for all of them? https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/inference.py
For example, `feat = net((img1, img2)).numpy()`
"
"Hi authors,
Thanks for your useful project,
I want to test my pretrain model on IJBB-IJBC dataset, but it is not available on DatasetZoo.
Can you share link to download?
Many thanks,
"
"When I changed embeding_size in config to 128 for training, I found a bug. After debugging, I found that num_features at line 129 of insightface/recognition/arcface_torch/backbones/mobilefacenet.py should be changed to num_features=num_features. Otherwise, the embedding_size set in config cannot be passed in."
"When I changed embeding_size in config to 128 for training, I found a bug. After debugging, I found that num_features at line 129 of insightface/recognition/arcface_torch/backbones/mobilefacenet.py should be changed to num_features=num_features. Otherwise, the embedding_size set in config cannot be passed in."
"HI  I'm analyzing the figures related to the face-detector
,but there is something I don't understand, so I'm posting it.

I am comparing the performance of SCRFD 
with retinaface and Tinaface, but it came out too low than the number in SCRFD paper.

Below are the performance figures for each paper.

What am I missing?

## SCRFD paper
![image](https://user-images.githubusercontent.com/11434363/124897675-7ebc3580-e019-11eb-94eb-e059f488030b.png)

## Tinaface paper
![image](https://user-images.githubusercontent.com/11434363/124897840-a4493f00-e019-11eb-8072-d7500b5879e1.png)

## Retinaface paper
![image](https://user-images.githubusercontent.com/11434363/124897909-b1662e00-e019-11eb-95f2-d3311f253d48.png)

"
"Hello!
I'm trying to run insightface/recognition/arcface_torch/inference.py with my jpg picture.
`python inference.py --weight ms1mv3_arcface_r18_fp16/backbone.pth --network r18 --img test.jpg`
Getting 
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x32768 and 25088x512)
What am i doing wrong?"
"When I clone the current version (0.3.6), and install it by `python setup.py install`. When I try to `import insightface` after installation is finished, I got this error message:

> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/mikeshi/test/insightface/venv/lib/python3.7/site-packages/insightface-0.3.6-py3.7-linux-x86_64.egg/insightface/__init__.py"", line 18, in <module>
>   File ""/home/mikeshi/test/insightface/venv/lib/python3.7/site-packages/insightface-0.3.6-py3.7-linux-x86_64.egg/insightface/app/__init__.py"", line 2, in <module>
>   File ""/home/mikeshi/test/insightface/venv/lib/python3.7/site-packages/insightface-0.3.6-py3.7-linux-x86_64.egg/insightface/app/mask_renderer.py"", line 8, in <module>
>   File ""/home/mikeshi/test/insightface/venv/lib/python3.7/site-packages/insightface-0.3.6-py3.7-linux-x86_64.egg/insightface/thirdparty/face3d/__init__.py"", line 3, in <module>
>   File ""/home/mikeshi/test/insightface/venv/lib/python3.7/site-packages/insightface-0.3.6-py3.7-linux-x86_64.egg/insightface/thirdparty/face3d/mesh/__init__.py"", line 9, in <module>
> ModuleNotFoundError: No module named 'insightface.thirdparty.face3d.mesh.cython'

And I also tried to run `python setup.py install` in `thirdparty/face3d/mesh/cython` directory, and the building is success, but the name of installed package is `mesh_core_cython`, I can `import mesh_core_cython` successfully."
"https://github.com/deepinsight/insightface/blob/2669ad984fa1d828373de3ab3381f7a65910837d/python-package/insightface/thirdparty/face3d/mesh/cython/setup.py#L16

When I try to install 0.3.6, I got an error, the reason is that there is no `mesh_core_cython.pyx` file in the package downloaded from pypi.org.
"
"https://github.com/deepinsight/insightface/blob/54611b420771e89289dcf53a65c71e0777bedf05/python-package/insightface/app/face_analysis.py#L29

User cannot specify the root of models directory, because the `root` parameter does not be passed to `ensure_available` function."
"https://github.com/deepinsight/insightface/blob/54611b420771e89289dcf53a65c71e0777bedf05/python-package/insightface/model_zoo/arcface_onnx.py#L71

Even `import numpy as np` is added at the begining of the file, but `from np.linalg import norm` still does not work, I think it should be `from numpy.linalg import norm`"
"I use the paras as these,but after 10000 epochs, the losses are still big.
network.mnet = edict()
network.mnet.pretrained = 'model/mobilenet_0_25'
network.mnet.FIXED_PARAMS = ['^stage1', '^.*upsampling']
network.mnet.BATCH_IMAGES = 16
network.mnet.HEAD_FILTER_NUM = 64
network.mnet.CONTEXT_FILTER_RATIO = 1
network.mnet.PIXEL_MEANS = np.array([0.0, 0.0, 0.0])
network.mnet.PIXEL_STDS = np.array([1.0, 1.0, 1.0])
network.mnet.PIXEL_SCALE = 1.0
network.mnet.pretrained_epoch = 0
network.mnet.max_feat_channel = 8888
network.mnet.COLOR_MODE = 1
network.mnet.USE_CROP = True
network.mnet.RPN_ANCHOR_CFG = RAC_SSH
network.mnet.LAYER_FIX = True
network.mnet.LANDMARK_LR_MULT = 2.5
network.mnet.end_epoch = 10000
**network.mnet.lr_step = '55,68,80'
network.mnet.lr = 0.01**
Are the lr and lr_step set properly?"
"Training: 2021-07-03 00:08:28,141-Reducer buckets have been rebuilt in this iteration.
Training: 2021-07-03 00:09:37,089-Speed 1473.00 samples/sec   Loss 48.4599   Epoch: 0   Global Step: 100   Required: 99 hours
Training: 2021-07-03 00:10:10,158-Speed 1548.32 samples/sec   Loss 47.0618   Epoch: 0   Global Step: 150   Required: 87 hours
Training: 2021-07-03 00:10:44,123-Speed 1507.46 samples/sec   Loss 45.0560   Epoch: 0   Global Step: 200   Required: 81 hours
Training: 2021-07-03 00:11:17,872-Speed 1517.13 samples/sec   Loss 43.4365   Epoch: 0   Global Step: 250   Required: 77 hours
Training: 2021-07-03 00:11:51,112-Speed 1540.35 samples/sec   Loss 41.9377   Epoch: 0   Global Step: 300   Required: 75 hours
Training: 2021-07-03 00:12:24,561-Speed 1530.72 samples/sec   Loss 41.0083   Epoch: 0   Global Step: 350   Required: 73 hours
Training: 2021-07-03 00:12:57,934-Speed 1534.23 samples/sec   Loss 40.2119   Epoch: 0   Global Step: 400   Required: 71 hours
Training: 2021-07-03 00:13:54,513-Speed 904.93 samples/sec   Loss 39.6560   Epoch: 0   Global Step: 450   Required: 75 hours
Training: 2021-07-03 00:15:40,041-Speed 485.18 samples/sec   Loss 39.0769   Epoch: 0   Global Step: 500   Required: 87 hours
Training: 2021-07-03 00:17:25,048-Speed 487.59 samples/sec   Loss 38.5804   Epoch: 0   Global Step: 550   Required: 97 hours
Training: 2021-07-03 00:19:09,988-Speed 487.90 samples/sec   Loss 38.2579   Epoch: 0   Global Step: 600   Required: 105 hours
Training: 2021-07-03 00:20:54,885-Speed 488.10 samples/sec   Loss 37.8968   Epoch: 0   Global Step: 650   Required: 112 hours
Training: 2021-07-03 00:22:38,605-Speed 493.64 samples/sec   Loss 37.6360   Epoch: 0   Global Step: 700   Required: 117 hours
Training: 2021-07-03 00:24:22,966-Speed 490.60 samples/sec   Loss 37.3774   Epoch: 0   Global Step: 750   Required: 122 hours
Training: 2021-07-03 00:26:03,244-Speed 510.58 samples/sec   Loss 37.1269   Epoch: 0   Global Step: 800   Required: 126 hours"
"Hi @nttstar ,  as far as I know, this is the first time I've seen an e-version ResNet. Compared with ResNetV1d, it replaces the first 3x3 maxpool with 2x2. Did you observe any accuracy gain from that?  Thank you!
https://github.com/deepinsight/insightface/blob/54b97b0c23fdbd2083c91e54526a407037399958/detection/scrfd/mmdet/models/backbones/resnet.py#L702"
"Traceback (most recent call last):
  File ""train.py"", line 503, in <module>
    main()
  File ""train.py"", line 499, in main
    lr_step=args.lr_step)
  File ""train.py"", line 392, in train_net
    num_epoch=end_epoch)
  File ""/opt/mxnet/python/mxnet/module/base_module.py"", line 552, in fit
    if 'lr_scheduler' in optimizer_params.keys():
AttributeError: 'tuple' object has no attribute 'keys'
"
"https://github.com/deepinsight/insightface/blob/996d844e12ed315b44935131c1f25bc4aef4e15e/recognition/arcface_torch/partial_fc.py#L158

Hi guys, I recheck your piplene and have a question: why do you multiplicate gradients by world size after backward? It means gradients in backbone will be world_size times bigger and dont looks like correct. In my toy example gradients equivalent without multiplication"
""
"Hi all! Wanted to give retinaface a try but the Dropbox download link seems to be broken! Could you update it?
[https://www.dropbox.com/s/53ftnlarhyrpkg2/retinaface-R50.zip?dl=0](https://www.dropbox.com/s/53ftnlarhyrpkg2/retinaface-R50.zip?dl=0)

Thanks for your help!

"
""
"the label url onedrive could't open.

https://1drv.ms/u/s!AswpsDO2toNKrU5PF53RsjTWACnc?e=jbQHnr
"
"The Baidu&Onedrive link are not available, Could someone provide the model or refresh the link?"
"Hello @nttstar ,  I am puzzled by below line in `get_flops.py`.  Why flops is multiplied by 0.75？

https://github.com/deepinsight/insightface/blob/e1300d32774da859b2f6c1538168c29f6cb8fb4c/detection/scrfd/tools/get_flops.py#L62"
"Hello @nttstar ,  
Thanks for the great work which promotes the face analysis community a lot.
From the SCRFD project, I've noticed the hard-set mAP of SCRFD_2.5G_KPS and SCRFD_10G_KPS model is lower than their no-kps version ([table here](https://github.com/deepinsight/insightface/tree/master/detection/scrfd#pretrained-models)). I'm curious how do you comprehend this phenomena. Thank you!

![图片](https://user-images.githubusercontent.com/18748036/122890798-97b8bb80-d376-11eb-9327-16e294927ba9.png)
"
"I have a problem when I run the executable with the command:
python -u train.py --network r100 --loss arcface --dataset emore
-----------------------------------------------------------------------
gpu num: 4
prefix ./models\r100-arcface-emore\model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=512, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'is_shuffled_rec': False, 'fp16': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'r100', 'num_workers': 1, 'batch_size': 512, 'per_batch_size': 128}
0 1 E 3 prelu False
Network FLOPs: 24.2G
loading: ../datasets/faces_emore\train.rec False
INFO:root:loading recordio ../datasets/faces_emore\train.rec...
Traceback (most recent call last):
  File ""train.py"", line 485, in <module>
    main()
  File ""train.py"", line 481, in main
    train_net(args)
  File ""train.py"", line 342, in train_net
    train_dataiter = get_face_image_iter(config, data_shape, path_imgrec)
  File ""E:\DataTraining\insightface\insightface\recognition\ArcFace\image_iter.py"", line 334, in get_face_image_iter
    images_filter=cfg.data_images_filter,
  File ""E:\DataTraining\insightface\insightface\recognition\ArcFace\image_iter.py"", line 45, in __init__
    'r')  # pylint: disable=redefined-variable-type
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\recordio.py"", line 245, in __init__
    super(MXIndexedRecordIO, self).__init__(uri, flag)
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\recordio.py"", line 71, in __init__
    self.open()
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\recordio.py"", line 248, in open
    super(MXIndexedRecordIO, self).open()
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\recordio.py"", line 79, in open
    check_call(_LIB.MXRecordIOReaderCreate(self.uri, ctypes.byref(self.handle)))
  File ""C:\Users\nhatd\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [16:22:13] C:\Jenkins\workspace\mxnet-tag\mxnet\3rdparty\dmlc-core\src\io\local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open ""../datasets/faces_emore\train.rec"": No such file or directory
-----------------------------------------------------------------------
i use python version 3.6 and my cuda version 10.1. I install the library mxnet-1.7.0.post2
 and version mxnet-cu101.
 What is the problem I have here? What to do to fix the error. I need help, thank you.
"
I would like to take features from the earlier layers of the model. Is there a function that allows me to pull features from different layers of the model? I
"Hello.

I've got some strange problem right there. Here is the simple code (without imports though)

```
cap = cv2.VideoCapture(0)

detector = insightface.app.FaceAnalysis(name=""antelope"")

detector.prepare(ctx_id=0)

while cap.isOpened():
    ret, img = cap.read()
    if not ret:
        break
    faces = detector.get(img)
```

And on the detector.get line i'm receiving following error:
`raceback (most recent call last):
  File ""/home/daddywesker/anaconda3/envs/tf/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py"", line 188, in run
    return self._sess.run(output_names, input_feed, run_options)
onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running FusedConv node. Name:'Conv_0_Relu_2' Status Message: CUDNN error executing cudnnFindConvolutionForwardAlgorithmEx( s_.handle, s_.x_tensor, s_.x_data, s_.w_desc, s_.w_data, s_.conv_desc, s_.y_tensor, s_.y_data, 1, &algo_count, &perf, algo_search_workspace.get(), max_ws_size)
python-BaseException`

I've tried to remove conda environment and reinstall insightface once again. Both ways - via conda install and pip install. Nothing changes. Cuda 11.3, RTX 3070 Laptop, Linux Mint 20. Will be glad to hear any advices. I've tried to install onnx both ways too - via pip and conda.
Cudnn installe using tar installation from official nvidia guide."
"Hi, I have read some of your codes and noticed that `deploy/test.py` with the model named 'antelope' is a merge of all your models together, I was wondering if is there separate .onnx files for your models or if they are still going to be still running on Mxnet?
since from what I gathered for using models separately for instance `Retinaface`,  the code still imports Mxnet."
can't access dropbox link
"I have two questions:
1. Could you tell me the pipeline of face alignment?
2. Did you apply that pipeline for evaluation datasets (AgeDB30, CFP-FP, ...)?

Thank you in advance"
"Thank u for you repository very useful
I 'am trying to run face detection with SCRFD , it work very well on (640, 640) image
But i get this error when run with another image shape (300, 300)
```
File ""D:\FTechAI\fid-face\SCRFD.py"", line 101, in distance2bbox
    x1 = points[:, 0] - distance[:, 0]
ValueError: operands could not be broadcast together with shapes (2738,) (2888,)
```
I run it via scrfd.py after convert model to onnx 
Do i need do something to run this model on another image shape?"
"Hello.

I wanted to try this Image_infer example of getting landmarks from image. I've installed mxnet and insightface and tried to launch a code. Unfortunately, on the 85th line https://github.com/deepinsight/insightface/blob/c61d3cd208a603dfa4a338bd743b320ce3e94730/alignment/coordinateReg/image_infer.py#L85 i'm getting None as detector. As i understand, new model_zoo.get_model looking only for onnx models but old retinaface is not onnx and that is why get_model returns None. So, am i right that Image_infer example only works on lesser versions of insightface? Or there is some solution?"
"运行指令：
```
export CUDA_VISIBLE_DEVICES='0,4'
export HOROVOD_GPU_ALLREDUCE=NCCL
export HOROVOD_GPU_ALLGATHER=NCCL
export HOROVOD_GPU_BROADCAST=NCLL
export MXNET_CPU_WORKER_NTHREADS=3

mpirun -np 2 \
-hostfile hosts/host_8 --allow-run-as-root \
-bind-to none -map-by slot \
-x LD_LIBRARY_PATH -x PATH \
-mca pml ob1 -mca btl ^openib \
-mca btl_tcp_if_include eth0 \
-x OMP_NUM_THREADS=2 \
python3 train_memory.py \
--dataset glint360k_dh_merge \
--loss cosface \
--network r100 \
```
运行时2个进程都占用了显卡0，如果np设置超过2则会超过显存。请问如何去使1个进程占1个显卡呢？"
"用arcface_torch里的train.py脚本能正常进行训练，但是加载样本的速率每隔400step都会由500多 samples/sec下降到90多 samples/sec ，请问是什么原因导致的？
![image](https://user-images.githubusercontent.com/50003218/121130216-ad02f580-c860-11eb-9809-96891c8d684b.png)

> "
"![image](https://user-images.githubusercontent.com/37058972/121108992-aadf6d80-c845-11eb-9cd5-b0688fd3c33a.png)
![image](https://user-images.githubusercontent.com/37058972/121109019-b763c600-c845-11eb-9993-c541fbd5fbbe.png)

1. I want to do alignment after detection image
    the rotation value is 0. Is there a way to get the rotation value?
```
def transform(data, center, output_size, scale, rotation):
    scale_ratio = scale
    rot = float(rotation) * np.pi / 180.0
    #translation = (output_size/2-center[0]*scale_ratio, output_size/2-center[1]*scale_ratio)
    t1 = trans.SimilarityTransform(scale=scale_ratio)
    cx = center[0] * scale_ratio
    cy = center[1] * scale_ratio
    t2 = trans.SimilarityTransform(translation=(-1 * cx, -1 * cy))
    t3 = trans.SimilarityTransform(rotation=rot)
    t4 = trans.SimilarityTransform(translation=(output_size / 2,
                                                output_size / 2))
    t = t1 + t2 + t3 + t4
    M = t.params[0:2]
    cropped = cv2.warpAffine(data,
                             M, (output_size, output_size),
                             borderValue=0.0)
    return cropped, M
```
2. How can I move on to the next step in image_infer.py?
    (detector(retinaface) -> alignment(similiaritytransform) -> landmark(mobilenet 106) -> arcface)"
"I ran the IJB_11.py but instead of using all the images, I only considered the images that belong to images and frames (not video). When I run IJB_11.py I get the following error:
```
Traceback (most recent call last):
  File ""IJB_11.py"", line 347, in <module>
    score = verification(template_norm_feats, unique_templates, p1, p2)
  File ""IJB_11.py"", line 191, in verification
    feat1 = template_norm_feats[template2id[p1[s]]]
IndexError: index 170001 is out of bounds for axis 0 with size 28937

```

How do I fix this?"
"Hello, does anybody share mirror repository or links to Mobilefacenet pretrained model inside the Model zoo? I cannot download due to:
1. Baidu is inaccessible
2. Dropbox return 404

thank you!"
"Thanks for sharing great code.

I have a question.  In README.md mobile family chart, SCRFD-0.5GF use Depth-wise Conv.
But in scrfd_500m_bnkps.py code, it used mobilenetv1  not depth-wise conv. 

Although mobilenetv1 contained depth-wise conv, Strictly speaking, aren't you using mobilenetv1 instead of depth-wise conv?

Then it should be changed to Mobilenetv1, not Depth-wise Conv.

Thanks"
"Why I use insightface subface R100 run:2(s) / 1 image, arcface run 0.8(s) / image ? 
 + model size subfaceR100 same size model arcfaceR100"
"Hi,
I sometimes receive up-side-down aligned face by using this code https://github.com/deepinsight/insightface/blob/master/recognition/tools/cpp-align/FacePreprocess.h to align face.

Here is some sample:
![0](https://user-images.githubusercontent.com/35292523/120663102-f1367480-c4b3-11eb-99bb-a1747392af71.jpg)
![1](https://user-images.githubusercontent.com/35292523/120663148-fabfdc80-c4b3-11eb-89c9-da87f4b433d8.jpg)

Align code in python works perfectly. I don't know what wrong with c++ version.
Can you pls help to fix?

Thanks,
"
"Your code 'arcface_torch' shows that the backbones are not standard ResNets.
Such as the ReLU is changed to PReLU, the 101-layers model dosen't use the BottleBlock but the BasicBlock.
Could you explain why?
Or could you please provide the related paper of this ResNet-like backbone?"
"Your code 'arcface_torch' shows that the backbones are not standard ResNets.
Such as the ReLU is changed to PReLU, the 101-layers model dosen't use the BottleBlock but the BasicBlock.
Could you explain why?
Or could you please provide the related paper of this ResNet-like backbone?"
"I'm trying to train with using the arcface_torch/train.py script but with embedding_size=256 in the config.

When doing this, I get: 

```
Traceback (most recent call last):
  File ""train.py"", line 138, in <module>
    main(args_)
  File ""train.py"", line 105, in main
    x_grad, loss_v = module_partial_fc.forward_backward(label, features, opt_pfc)
  File ""/home/ubuntu/insightface/recognition/arcface_torch/partial_fc.py"", line 118, in forward_backward
    dist.all_gather(list(total_features.chunk(self.world_size, dim=0)), features.data)
  File ""/home/ubuntu/insightface/.venv/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py"", line 1863, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
RuntimeError: All tensor operands to scatter/gather must have the same number of elements
```
But it works when embedding_size=512

Any ideas on how to reduce the embedding size with the Pytorch training model?"
I'm unable to figure out how this would be done using the csvs that NIST provides with IJB-C.
"In 'insightface/recognition/arcface_torch/config.py', the 'config.batch_size' is set as '64'.
But in your paper, you set the batch size to 512 and trained models on four GPUs.
Did you mean each single GPU accepts the mini-batch_size of 512?
Or maybe the total four GPUs accept the effective-batch_size of 512, where effective-batch_size = config.batch_size * WORLD_SIZE?"
""
"训练scrfd时，config用scrfd_10g_bnkps.py，报错如下：
`mmdet/models/losses/iou_loss.py"", line 365, in forward
    return (pred * weight).sum()  # 0
RuntimeError: The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 1`
请问这怎么解决呀？感谢 @nttstar "
"It is bit struggle to collect dataset.  Can you please correct those links?
Can I get the link of drive link MS1M-Arcface 85k?"
"I was using arcface_r100_v1 model in my project and insightface version 0.1.5. 
The model used to be downloaded automatically but that download link is now expired. 

http://insightface.ai/files/models/arcface_r100_v1.zip

This link is expired, and the latest package of insightface also does not download the model.

Where are these models hosted now ?"
"Hi, I'm modifying the detection code for rotation augmentation.

There's something I don't understand while looking at the code.
Getting Acc and Loss while training the model is not what I thought.

<img width=""457"" alt=""Screen Shot 2021-05-17 at 4 53 15 PM"" src=""https://user-images.githubusercontent.com/55076107/118452684-5c045380-b731-11eb-95c7-da66d256b37f.png"">

the update() part of kinda EvalMetric classes, just use preds list for label, not labels list.

1) Why not use labels list but use preds list?
2) What is preds list?


Extra Question)
I'm trying to modify the code for rotation augmentation.
We modified gt_cls from 0,1 to 0, 1, 2, 3, 4 for rotating 90 degrees to have each label, and the model is also being modified.

3) This process is not as easy as I thought, and I wonder if you know how to do rotation augmentation easily?

I'd appreciate it if you'd give me an answer.



 
"
"
face detection scrfd output does not have 5 points on the face. How can I get those 5 points?"
"As we know, mxnet pred is fit in single-thread, and I have found a muti-thread mxnet version 1.6.0. 
Does it works for muti-thread retinaface?"
"在做dataset_merge的时候，我用1个10个id的dataset和glint360k合并，最后得到416058的id数。在运行代码时有片段：
```
processing id 416030
processing id 416040
0 416048
```

是不是意味着glint360k的id数目其实是416048而不是360232"
"Hi, thank you for the good work.

I downloaded the most up-to-date model(https://github.com/deepinsight/insightface/tree/master/challenges/IFRT)
When i tested this on LFW, a performance is low. The validation rate was 4%.


I extracted the embedding with the method below.

ort_session = onnxruntime.InferenceSession(rec_name)
input_name = ort_session.get_inputs()[0].name #'data'
outputs = ort_session.get_outputs()[0].name #'fc1'

img = cv2.imread('image path')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img = np.transpose(img, (2, 0, 1))
img = np.expand_dims(img, axis=0)
img = img.astype(np.float32)

emb = ort_session.run([outputs], input_feed={input_name: img})

"
"hi,

I read the paper and you mentioned that the margin of cosface is setting to 0.35.

is this also same with arcface-torch (https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch)?"
"在arcface_pytorch项目中，我把backbone的lr设置位0了，为什么backbone还是会有变化？(在测试集上测试出来的精度不同)

![Snipaste_2021-05-10_10-05-10](https://user-images.githubusercontent.com/20027988/117597004-3ac2c680-b177-11eb-8ede-5600093a283c.png)
"
"![image](https://user-images.githubusercontent.com/62049652/117533315-b52c0300-b01e-11eb-9f4b-64b5232ae1d6.png)
为什么一开始有1000+图片/Sec，而后面越来越小"
"I want to use dataset ms1m-retinaface-t1 to train arcface_torch,but I donot know how to do it ,please help me"
"Please kindly share the ""MS1MV2-Arcface"" training set link.
Thank you"
Solved
"Dev Env: Windows 10
Dev Language: C++ 
Dev Compiler: VS2019
1.When I run Retinaface detection in gpu, it costs nearly 40s to start while using cpu is fast. Is it normal?
2. Why I run Retinaface in RTX2080 at 60ms and run Retinaface in  i9-10920X at 135ms?

"
""
"Hi,

As described here: https://github.com/deepinsight/insightface/blob/master/recognition/arcface_torch/docs/eval.md"
"loading ./assets_mask/if1k3d68 0
[20:12:12] ../src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.6.0. Attempting to upgrade...
[20:12:12] ../src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""mask_renderer.py"", line 96, in <module>
    tool = MaskRenderer('./assets_mask')
  File ""mask_renderer.py"", line 28, in __init__
    self.if3d68_handler = Handler(osp.join(model_dir, 'if1k3d68'), 0, 192, ctx_id=0)
  File ""/home/yuange/code/SelfServer/DeepInsight/insightface/recognition/tools/image_3d68.py"", line 74, in __init__
    ('data', (1, 3, image_size[0], image_size[1]))])
  File ""/home/yuange/program/anaconda3/envs/pytorch_1.6.0/lib/python3.6/site-packages/mxnet/module/module.py"", line 443, in bind
    for x in self._exec_group.param_arrays
  File ""/home/yuange/program/anaconda3/envs/pytorch_1.6.0/lib/python3.6/site-packages/mxnet/module/module.py"", line 443, in <listcomp>
    for x in self._exec_group.param_arrays
  File ""/home/yuange/program/anaconda3/envs/pytorch_1.6.0/lib/python3.6/site-packages/mxnet/ndarray/utils.py"", line 67, in zeros
    return _zeros_ndarray(shape, ctx, dtype, **kwargs)
  File ""/home/yuange/program/anaconda3/envs/pytorch_1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py"", line 4752, in zeros
    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, **kwargs)
  File ""<string>"", line 39, in _zeros
  File ""mxnet/cython/ndarray.pyx"", line 162, in mxnet._cy3.ndarray._imperative_invoke
TypeError: _imperative_invoke() takes exactly 5 positional arguments (7 given)"
"Hi,

I found out the training steps of one epoch is 16650 (https://raw.githubusercontent.com/anxiangsir/insightface_arcface_log/master/glint360k_cosface_r18_fp16_0.1/training.log).

this means that the total number of images in one epoch is 

16650 * 8 * 64 = 8524800

but what i got the total length is 17091584

did you resize the dataset when training?"
"hi , @anxiangsir 
i have reproduced your work , by training glint1600W dataset , and it achieved a good result. However , i have some question about the strategy you use for the learning rate schedule . 
as we can see from the code.

```
...
config.batch_size = 64
config.lr = 0.1  # batch size is 512
...
...
    opt_pfc = torch.optim.SGD(
        params=[{'params': module_partial_fc.parameters()}],
        lr=cfg.lr / 512 * cfg.batch_size * world_size,
        momentum=0.9, weight_decay=cfg.weight_decay)
...
...
def lr_step_func(epoch):
        return ((epoch + 1) / (4 + 1)) ** 2 if epoch < config.warmup_epoch else 0.1 ** len(
            [m for m in [8, 12, 15, 18] if m - 1 <= epoch])
...
...
    scheduler_pfc = torch.optim.lr_scheduler.LambdaLR(
        optimizer=opt_pfc, lr_lambda=cfg.lr_func)
...
```
the values for cfg.lr and optimizer.param_groups['lr] will be like below:

- epoch                                                0~6
- cfg.lr                                                  1.0
- optimizer.param_groups['lr]               0.025

- epoch                                                7~11
- cfg.lr                                                  0.1
- optimizer.param_groups['lr]               0.0025

- epoch                                                12~14
- cfg.lr                                                  0.01
- optimizer.param_groups['lr]               0.00025

and finally my question comes:
       why do you use "" lr=cfg.lr / 512 * cfg.batch_size * world_size "" , why do you divide 512 and multiply batch_size , multiply world_size later ? 
       any learning rate schedule theory basis for distributed training  ? 
       is my question properly described ?
       waiting for your reply, thanks .

"
"Hi, 
could you please tell me which dataset have you been used in arcface_torch(speed benchmark part)
![image](https://user-images.githubusercontent.com/32949741/115505670-66efd380-a2ac-11eb-98d0-4f9ff58f974e.png)
"
"Hi, thinks for sharing your nice project! I want to know if some pre-trained models of sub-center ArcFace can be accessed. I have seen the Sub-center ArcFace folder but can't find any pre-trained models of sub-center ArcFace in the Model zoo. Looking forward to your reply!"
"  optimizer.state[self.sub_weight]['momentum_buffer'] = self.sub_weight_mom

what does momentum buffer mean ? why do we need it ?

what does following statement do ?
optimizer.state.pop(optimizer.param_groups[-1]['params'][0], None)"
I have downloaded 30GB faces_glint.zip from model-zoo. Initially I got error in extracting the zip but I was extract it with the help of tar. Now I have train.rec file and I want to see some sample images from the dataset. Can anyone please help me with method to load/read images from train.rec? 
"why following is done in ArcFace (As given in sudo algo in research paper)

fc7 = fc7 + mx.sym.broadcast mul (one hot, mx.sym.expand dims (marginal target logit - original target logit, 1))"
"What does fp16 stand for ? in arcface_pytorch

is it for fartial fully connected ? If yes i can see optimiser for partial full connected updates weight even if config.fp16 = False

Any help would be appriciated.
Thanks.
"
"hi, i am using arcface torch for large scalｅ　ｄａｔａｓｅｔｓ,　I use you provide weights for fine tuning,  same dataset（ｇｌｉｎｔ３６０ｋ），start epoch 19, loss value  rise from 0.9 to 8, and not decline always.why? what  i need to notice"
"Hello,
I'm currently experimenting with robust face recognition algorithms like Subcenter-Arcface. Can anyone, who has the Celeb-500k dataset, share it via torrents? 
Thanks so much in advance!"
"Hi there, i found an interesting issue in `glint360k` dataset.
According to your code in [unpack_glint360k.py](https://raw.githubusercontent.com/deepinsight/insightface/master/recognition/partial_fc/unpack_glint360k.py), indices within `[17091658., 17451890.]` stores `identity`, and `[1., 17091657.]` stores `image`. But it seems that label information stored in `identity` **occationally** doesn't match with labels in `image`.

For example, when i use
```python
s = imgrec.read_idx(17444270)
header, _ = mx.recordio.unpack(s)
print(header.label.astype(int))
```
i get `[16880352 16880388]`, which i think it means images with idx in `range(16880352, 16880388)` belongs to a **single** person, but actually they're NOT!

Here's the code:
```python
# this belongs to a man
s = imgrec.read_idx(16880352)
image_header, image_bytes = mx.recordio.unpack(s)

encoded_array = np.frombuffer(image_bytes, np.uint8)
img = cv2.imdecode(encoded_array, cv2.IMREAD_ANYCOLOR)

print(image_header.label.astype(int))
plt.imshow(img[..., ::-1])

# this belongs to a woman
s = imgrec.read_idx(16880353)
image_header, image_bytes = mx.recordio.unpack(s)

encoded_array = np.frombuffer(image_bytes, np.uint8)
img = cv2.imdecode(encoded_array, cv2.IMREAD_ANYCOLOR)

print(image_header.label.astype(int))
plt.imshow(img[..., ::-1])
```

This happens often to latter indexed identities, which I think is misleading. Since indices within `[0., 17091657.]` already provided adequate information, why appending it with those confusion data?"
"I want to train arcface model based on documentation on GitHub, currently I use Google Colaboratory as my environment.

I have uploaded my source code to Google drive as shown below:
![image](https://user-images.githubusercontent.com/19206896/112744979-7a5d9680-8fad-11eb-9832-439b10b00724.png)

uploaded dataset face_emore
![image](https://user-images.githubusercontent.com/19206896/112745375-a4648800-8fb0-11eb-9ca7-8939343072e5.png)

also copy sample_config.py to config.py

added the line config.fp16 = False config.py

installed 
cuda 10.1
mxnet-cu101

running train as documented:
CUDA_VISIBLE_DEVICES='0' python train.py --network r100 --loss arcface --dataset emore

the result have been error:
NameError: name 'config' is not defined

![image](https://user-images.githubusercontent.com/19206896/112745424-f2798b80-8fb0-11eb-97e7-2e360921937f.png)

anyone can help me for solving this problem?"
"I'm new to Insightface. When I run the first Train ""ArcFace with LResNet100E-IR"" example, I got:

NameError: name 'config' is not defined

Anyone knows why?

=====================================================================
%CUDA_VISIBLE_DEVICES='0,1,2,3' python3 -u train.py --network r50 --loss cosface --dataset emore


gpu num: 4
prefix ./models/r50-cosface-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=512, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='cosface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r50', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'fp16': False, 'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'is_shuffled_rec': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.0, 'loss_m3': 0.35, 'net_name': 'fresnet', 'num_layers': 50, 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'cosface', 'network': 'r50', 'num_workers': 1, 'batch_size': 512, 'per_batch_size': 128}
0 1 E 3 prelu False
Network FLOPs: 12.6G
loading: ../datasets/faces_emore/train.rec False
Traceback (most recent call last):
  File ""train.py"", line 484, in <module>
    main()
  File ""train.py"", line 480, in main
    train_net(args)
  File ""train.py"", line 341, in train_net
    train_dataiter = get_face_image_iter(config, data_shape, path_imgrec)
  File ""/home/akwok/TVM/insightface/recognition/ArcFace/image_iter.py"", line 330, in get_face_image_iter
    rand_mirror=config.data_rand_mirror,
NameError: name 'config' is not defined
"
"i have two machines and trained the arcface in distributed way . but errors occurred at my second machine then the master node blocked . the error is below:

```
Traceback (most recent call last):
  File ""train.py"", line 141, in <module>
    main(args_)
  File ""train.py"", line 63, in main
    dist.broadcast(ps, 0)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 842, in broadcast
    work = _default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:518, unhandled system error, NCCL version 2.4.8
```

any ideas ? @anxiangsir
"
"Hello! Your project insightface/recognition/ArcFace/sample_config.py None of them config.fp16 This parameter。

The error is as follows ：
    if config.fp16:
AttributeError: 'EasyDict' object has no attribute 'fp16'
"
"Hi,

I tried to train glint360k with arcface-torch.

but the gradscale keeps getting zero and the loss keeps unchanged in each iteration.

any idea?"
"version:pytorch 1.8.0
model: rank_0_softmax_weight_mom.pt

File ""convert_onnx.py"", line 15, in convert_onnx
    model.load_state_dict(weight)
  File ""/root/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1196, in load_state_dict
    state_dict = state_dict.copy()
AttributeError: 'Tensor' object has no attribute 'copy'
"
"I am trying to convert the [Insightface arcface LResNet100E-IR,ArcFace@ms1m-refine-v2 mxnet model](https://github.com/deepinsight/insightface/wiki/Model-Zoo#31-lresnet100e-irarcfacems1m-refine-v2) to work with onnx. 

Based on [this](https://github.com/apache/incubator-mxnet/issues/14589#issuecomment-479057849) issue, it looks like mxnet only supports up to onnx v1.3.0. I am therefore using the following library verions:
```
mxnet==1.7.0.post1
onnx==1.3.0
onnxruntime==1.6.0
```

It looks like out of the box the arcface model is not support by onnx, but I came across [this](https://github.com/deepinsight/insightface/issues/1350#issue-757229024) issue which which [links a script](https://github.com/linghu8812/tensorrt_inference/blob/master/arcface/export_onnx.py) for properly converting the arcface model to onnx.  

Running the script converts the model from mxnet format to onnx format successfully, however the output is no longer correct. 
Here is my mxnet script I use for running inference (using a pre-aligned face chip):
```
#############################
# Inference with mxnet
#############################
import numpy as np
import cv2
import mxnet as mx
from collections import namedtuple

import pkg_resources
print(""mxnet version:"", pkg_resources.get_distribution(""mxnet"").version)

Batch = namedtuple('Batch', ['data'])
def getEmbedding(face_chip):
    face_chip = np.swapaxes(face_chip, 0, 2)
    face_chip = np.swapaxes(face_chip, 1, 2)
    face_chip = face_chip[np.newaxis, :]
    array = mx.nd.array(face_chip)
    mod.forward(Batch([array]))
    out = mod.get_outputs()[0].asnumpy()
    return out


prefix = ""./models/insightface/model""
sym, arg, aux = mx.model.load_checkpoint(prefix, 0)
ctx = mx.cpu()
mod = mx.mod.Module(symbol=sym, context=ctx, label_names=None)
mod.bind(for_training=False, data_shapes=[('data', (1,3,112,112))])
mod.set_params(arg, aux)

img = cv2.imread(""face_chip.jpg"")

img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)


res = getEmbedding(img)
print(res[0][0])

```
I print the first element of the feature vector, which is: `2.216425`

Now here is my script for running inference with onnxruntime:
```
#############################
# Inference with onnx runtime
#############################
import json
import numpy as np
import cv2
import onnx
import onnxruntime
from onnx import numpy_helper

import pkg_resources
print(""onnx-runtime version:"", pkg_resources.get_distribution(""onnxruntime"").version)
print(""onnx version:"", pkg_resources.get_distribution(""onnx"").version)


model=""./models/insightface/arcface_r100.onnx""
 
#Preprocess the image
img = cv2.imread(""face_chip.jpg"")

img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img.resize((1, 3, 112, 112))
data = json.dumps({'data': img.tolist()})
data = np.array(json.loads(data)['data']).astype('float32')
 
session = onnxruntime.InferenceSession(model, None)
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
 
result = session.run([output_name], {input_name: data})
print(result[0][0][0])
```
Again, I print the first element of the feature vector, which is now: `-0.19618489`.

This is clearly not correct. What have I done wrong? Has the conversion failed? 

"
"Hello!
I have ran into a problem when training the network.
After running few batches, it stops with call_reset and starts the next batch.
![IMG_4087](https://user-images.githubusercontent.com/55081401/109399003-67b95880-7940-11eb-9000-02f302f5cb7d.JPG)


Please help me with it.
"
"Hi everyone :

I want to use the pretrained model weights of partial_fc_glint360k_r100 from 16_backbone.pth, then finetuning the backbone with my own dataset. 

The strange problem is that:  I can load model weights from 16_backbone.pth, but the initial loss is the same as that of not using pretrained model weights, in other words,  loading pretrained model weights does not make any sense.

There may be some problem with my code, so I paste my code here, can anyone give me some suggestions ?

Core code (partial_fc.py):

`

def main(local_rank):

    """"""  
    主函数

    Args:
        local_rank: list[integer], rank of the process on the local machine, e.g., [0, 1, 2, 3] if there are 4 GPUs in this machine

    Comments:  local_rank vs rank
                    |    Node1  |   Node2    |
        ____________| p1 |  p2  |  p3  |  p4 |
        local_rank  | 0  |   1  |  0   |   1 |
        rank        | 0  |   1  |  2   |   4 |
    """"""

    """""" 设置一些与分布式训练相关的参数 """"""
    dist.init_process_group(backend='nccl', init_method='env://')
    cfg.local_rank = local_rank
    torch.cuda.set_device(local_rank)
    cfg.rank = dist.get_rank()
    cfg.world_size = dist.get_world_size()
    # debug
    print(f'cfg.local_rank:')
    print(cfg.local_rank)


    """""" 构建训练集的 data loader """"""
    trainset = MXFaceDataset(
        root_dir=cfg.rec, 
        local_rank=local_rank
    )
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        trainset, shuffle=True)
    train_loader = DataLoaderX(local_rank=local_rank,
                               dataset=trainset,
                               batch_size=cfg.batch_size,
                               sampler=train_sampler,
                               num_workers=0,
                               pin_memory=True,
                               drop_last=False)


    """""" 构建网络 backbone """"""
    backbone = backbones.iresnet100(pretrained=False).to(local_rank)
    backbone.train()


    """""" 是否使用预训练的模型权重 """"""
    if cfg.use_pretrained:

        backbone.load_state_dict(torch.load(cfg.pretrained_model_path))
        # show message
        logger.info(f'\n Loaded pretrained model weights from {cfg.pretrained_model_path} \n')


    # Broadcast init parameters
    for ps in backbone.parameters():
        dist.broadcast(ps, 0)

    """""" 
    做两件事情：
        1. 将 backbone 复制多份，每个设备一份；
        2. 将 input data 划分多份，每个设备一份
    """"""
    backbone = torch.nn.parallel.DistributedDataParallel(
        module=backbone,
        broadcast_buffers=False,
        device_ids=[cfg.local_rank])
    backbone.train()


    # Memory classifer
    dist_sample_classifer = DistSampleClassifier(
        rank=dist.get_rank(),
        local_rank=local_rank,
        world_size=cfg.world_size)

`
Other codes are not modified.

Any suggestions would be appreciated !"
"Hi, @jiankangdeng could you please give me a link to download meta data used in ijbc? The links in VGG2 is only left with ijbb.
Thanks in advance."
"when I run recognition/ArcFace/train.py, error happened,saying ""No module named 'config' "",and I can't find any config.py in the whole project except ""recognition/partial_fc/pytorch/config.py"",did the author miss config.py or just indeed not providing it? can anyone tell where to find the config.py?thanks a lot !"
想用原图做些实验，不知道方不方便提供glint360k未对齐的原图，或者对齐为400*400的图片，非常感谢啊！
"I use model face detection R-50, you report is:
         - WiderFace validation mAP: Easy 96.5, Medium 95.6, Hard 90.4
I test in WiderFace in validation and results:
         - Easy   Val AP: 0.9031890567440208
         - Medium Val AP: 0.8786441818813505 
         - Hard   Val AP: 0.6533242254169304
You can show hyperparamater in face detection for me. Thanks.
"
retinaface人脸检测，将检测并裁减出的人脸送入Lresnet特征提取器并于人脸库进行比对（计算欧式距离），发现人脸比对结果错误率很高。请问这是特征提取器的问题吗？
how to do the experiment of megaface verification locally?
"How to package their own data into the program inside the format for training, or what way can these face data merge? Thank you very much for such an excellent project and I hope you can answer this question."
"Are these both same models? 

Is there any trained model available for Glint360K dataset?"
"Dear @nttstar and colleagues, 

I have prepared a custom face dataset similar to LFW with 600 identities and 12K images for my research, and able to generate the dataset based on the predefined format.

I am able to run a training session based on the dataset with defaults network: RsNet100. the training accuracy quickly climbing to 100%, and the verification accuracy increases up to 0.15688 (15.6%) in epoch 12

The following are the snapshot of the training session result:

```
call reset()
2021-01-25 13:38:28,608 root        : INFO     Epoch[12] Batch [0-20]	Speed: 67.70 samples/sec	acc=1.000000	lossvalue=0.000000
2021-01-25 13:38:38,010 root        : INFO     Epoch[12] Batch [20-40]	Speed: 68.10 samples/sec	acc=1.000000	lossvalue=0.000000
2021-01-25 13:38:47,437 root        : INFO     Epoch[12] Batch [40-60]	Speed: 67.91 samples/sec	acc=1.000000	lossvalue=0.000000
lr-batch-epoch: 0.1 75 12
testing verification..
(5606, 512)
infer time 51.81577400000001
[ilfw][4000]XNorm: 18.111734
[ilfw][4000]Accuracy-Flip: 0.11371+-0.10493
testing verification..
(5322, 512)
infer time 48.87921500000002
[ilfw-test][4000]XNorm: 18.014994
[ilfw-test][4000]Accuracy-Flip: 0.15668+-0.06784
saving 1
2021-01-25 13:40:40,766 root        : INFO     Saved checkpoint to ""./models/r100-arcface-emore/model-0001.params""
[4000] Accuracy-Highest: 0.15668
2021-01-25 13:40:42,840 root        : INFO     Epoch[12] Batch [60-80]	Speed: 5.55 samples/sec	acc=1.000000	lossvalue=0.000000
2021-01-25 13:40:52,221 root        : INFO     Epoch[12] Batch [80-100]	Speed: 68.24 samples/sec	acc=1.000000	lossvalue=0.000000
2021-01-25 13:41:01,634 root        : INFO     Epoch[12] Batch [100-120]	Speed: 68.01 samples/sec	acc=1.000000	lossvalue=0.000000
```

The verification accuracy never increased ever since.

The dataset is being prepared by the following process:

a. Collect data from google images, resized to 112x112, and aligned
b. Remove duplicates
c. Add an augmented face masks
d. Split dataset of persons into training, validation, and test (80:10:10). Persons in training sets never used in validation, vice versa.
e. Generate a .lst file based on the training dataset
f. Based on the generated .lst file, create .rec and .idx file
g. Generate matcher and unmatched pairs file based on validation and test sets
h. generate bin file based on pairs file

My questions are:
1. Am I making any mistakes in data preparation steps?
2. How many identities are required as a minimum to achieve good results?

Thank you for your kind help and support.

Best Regards,

Wirianto Widjaya





"
"For example, the results on IJB-B, IJB-C using the R100/R50 models trained my ms1m-arcface."
"I am very confused with datasets in Dataset Zoo. 

For example:
 - Is MS1M ArcFace, MS1M IBUG and MS1M RetinaFace created by refining from MS1M dataset?
 - What is the difference between MS1M ArcFace, MS1M IBUG and MS1M RetinaFace?
 
Is there any document about datasets in Dataset Zoo?"
"Hello, I'm doing face super-resolution reconstruction recently based on glint360k dataset，and I've noticed that you've published the pre trained model 16backbone.pth.I wonder to know whether this model can be used to evaluate the similarity between reconstructed face and origin face, and what is the output of this model? Looking forward to your reply~"
"Hello, jia guo. 
This work is very good and it has helped me a lot, but I encountered a problem: in the 512-D Feature Embedding section, you have said: 

> Put the model under $INSIGHTFACE_ROOT/models/. For example, $INSIGHTFACE_ROOT/models/model-r100-ii. 

However, there are two problems, 
1) In deploy/test.py, I need to enter a (model_prefix, epoch) as the input of --model, which seems to contradict your document  /models/model-r100-ii. 
2) I cannot find a list of these pre-trained models similar to (model_prefix, epoch) elsewhere. 
Is this ?  
https://github.com/deepinsight/insightface/wiki/Model-Zoo  (**3. Face Recognition models. section?**)

Hope to get your help, thank you!"
"After I unzipped the Glint360K Dataset, wrote the image path and label into a file 'train.list' by 'unpack_glint360k.py', I found some images in same ID had been written twice in the 'train.list' file. Is there any duplicated images information or mistake in 'train.rec' file?

File 'train.list' was written by follow codes.
'

      for identity in seq_identity:

            s = imgrec.read_idx(identity)
            header, _ = mx.recordio.unpack(s)
            imgid = 0
            for _idx in range(int(header.label[0]), int(header.label[1])):
                s = imgrec.read_idx(_idx)
                _header, _img = mx.recordio.unpack(s)
                label = int(_header.label[0])
                class_path = os.path.join(args.output, ""%d"" % label)   
                image_path = os.path.join(class_path, ""%d.jpg"" % imgid)  
                if not os.path.exists(class_path):
                    os.makedirs(class_path)

                _img = mx.image.imdecode(_img).asnumpy()[:, :, ::-1]  # to bgr
                cv2.imwrite(image_path, _img)
                with open("" ./train.list"",'a') as f:
                    line = []
                    line = '%s\t' % image_path
                    line += '%d\n' % label
                    f.write(line)
                imgid += 1
                imgsum += 1`
"
Does glint 360k database has any profile face images?
"Hi friends,

Allow me to share the data preparation script to build custom face dataset that are collected from Google Image for your Insightface Project:

https://github.com/wwidjaya/insightface-lfw-data-preparation

Please feel free to comment and provide feedback.

Warm Regards,

Wirianto Widjaya"
"Hi,
    I use glint360K to train pytorch partial_fc, if the embedding_size=512, everything work fine.
    But if I changed embedding_size=128 , model name is iresnet34, the code can't converge, loss will drop to small  and nan quickly in one epoch.
    I use three GPU to train. batch_size=256, sample_rate=0.1

    Do I need change something if I changed embedding_size?

Thanks
meixitu"
"Would you please provide a mean landmark data for 106 lmrks? I want to use it to caculate tranform mat , and do face alignment .
Just like mean landmarks for 68 points.:
```
mean_face_x = np.array([
0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483, 0.799124,
0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127, 0.36688, 0.426036,
0.490127, 0.554217, 0.613373, 0.121737, 0.187122, 0.265825, 0.334606, 0.260918,
0.182743, 0.645647, 0.714428, 0.793132, 0.858516, 0.79751, 0.719335, 0.254149,
0.340985, 0.428858, 0.490127, 0.551395, 0.639268, 0.726104, 0.642159, 0.556721,
0.490127, 0.423532, 0.338094, 0.290379, 0.428096, 0.490127, 0.552157, 0.689874,
0.553364, 0.490127, 0.42689 ])

mean_face_y = np.array([
0.106454, 0.038915, 0.0187482, 0.0344891, 0.0773906, 0.0773906, 0.0344891,
0.0187482, 0.038915, 0.106454, 0.203352, 0.307009, 0.409805, 0.515625, 0.587326,
0.609345, 0.628106, 0.609345, 0.587326, 0.216423, 0.178758, 0.179852, 0.231733,
0.245099, 0.244077, 0.231733, 0.179852, 0.178758, 0.216423, 0.244077, 0.245099,
0.780233, 0.745405, 0.727388, 0.742578, 0.727388, 0.745405, 0.780233, 0.864805,
0.902192, 0.909281, 0.902192, 0.864805, 0.784792, 0.778746, 0.785343, 0.778746,
0.784792, 0.824182, 0.831803, 0.824182 ])
```"
"When training the model using partial_fc (mxnet) code in float16,i found the model can't be stored in fp32. it should be storing automatically for FP32.how to solve it?"
"作者你好，我在自测的IJBC测试集上的指标跟论文中有很大差异(test1,test2,test4)，这可能是数据清洗和裁剪导致的，为了保持数据一致，能否提供下IJBC和IJBB的测试集链接呢，不胜感激~"
"I have followed the training steps in sub-center arcface and recognized that the model don't have any key = 'fc7_%d_weight'. That lead to the error in below picture. Could anyone fix it?
![image](https://user-images.githubusercontent.com/54897327/102183904-31dc0a80-3ee1-11eb-8a41-9468224aaf49.png)
"
""
"I have sovled this problem,

if anyone wanna load checkpoint file,and continue train, maybe  this information will help you.

-  **`step 1`**  change config.sh like this:
```
export CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7'
export HOROVOD_GPU_ALLREDUCE=NCCL
export HOROVOD_GPU_ALLGATHER=NCCL
export HOROVOD_GPU_BROADCAST=NCLL
export MXNET_CPU_WORKER_NTHREADS=3

PYTHON_EXEC=/home/liuyang/anaconda3/bin/python
${PYTHON_EXEC} train_memory.py \
--dataset glint360k_8GPU \
--loss cosface \
--network r100 \
--models-root ./models/glint360k_8GPU_r100FC_1.0_fp16_cosface \
--pretrain True
```
-  **`step 2`**  chang ` train_memory.py` like this:
```
def parse_args():
    parser = argparse.ArgumentParser(description='Train parall face network')
    #general
    parser.add_argument('--dataset', default='emore', help='dataset config')
    parser.add_argument('--network', default='r100', help='network config')
    parser.add_argument('--loss', default='cosface', help='loss config')
    parser.add_argument('--pretrain', default='False', help='pretrain or not')

    args, rest = parser.parse_known_args()
    default.generate_config(args.loss, args.dataset, args.network)
    parser.add_argument('--models-root', default=""./test"", help='root directory to save model.')
    args = parser.parse_args()
    return args
def train_net():
     ........
    train_module = SampleDistributeModule(
        pretrain=args.pretrain,
        prefix = prefix_dir,
        symbol=esym,
        fc7_model=fc7_model,
        memory_bank=memory_bank,
        memory_optimizer=memory_bank_optimizer)

```
-  **`step 3`**  chang `memory_bank.py` like this
```
class MemoryBank(object):
    def __init__(self,pretrain,num_sample, num_local, rank, local_rank, embedding_size, prefix, gpu=True):
        self.num_sample = num_sample
        self.num_local = num_local
        self.rank = rank
        self.embedding_size = embedding_size
        self.gpu = gpu
        self.prefix = prefix

        if self.gpu:
            context = mx.gpu(local_rank)
        else:
            context = mx.cpu()

        # In order to apply update, weight and momentum should be storage.
        #print(""num_local :"",self.num_local)
        self.weight = nd.random_normal(
            loc=0, scale=0.01, shape=(self.num_local, self.embedding_size),
            ctx=context)
        self.weight_mom = nd.zeros_like(self.weight)
        #print(""init weight type"", self.weight)
        if pretrain == 'True':
            print(""load center.... "", pretrain)
         self.load()
        self.weight_index_sampler = WeightIndexSampler(num_sample, num_local,  rank)
       def load(self):
        print(""center param: "",os.path.join(self.prefix, ""%d_centers.param"" % self.rank))
        self.weight = nd.load(os.path.join(self.prefix, ""%d_centers.param"" % self.rank))[0]
        self.weight_mom = nd.load(os.path.join(self.prefix,""%d_centers_mom.param"" % self.rank))[0]


```
- **`step 4`**
```
    def init_params(self, initializer, arg_params=None, aux_params=None,
                    allow_missing=False, force_init=False, allow_extra=False):
        if self.pretrain == 'True':
            _, arg_params, aux_params = mx.module.module.load_checkpoint(prefix=os.path.join(self.prefix,""model_average""),epoch=0)
            allow_missing = True
            allow_extra = True
        # backbone
        self.backbone_module.init_params(
            initializer=initializer, arg_params=arg_params,
            aux_params=aux_params, allow_missing=allow_missing,
            force_init=force_init, allow_extra=allow_extra)
```

_Originally posted by @fucker007 in https://github.com/deepinsight/insightface/issues/1283#issuecomment-740564556_"
"Currently the [alignment/](https://github.com/deepinsight/insightface/tree/d95aeb8029d9478815f2dcf986b3a76f83d93ad7/alignment) part actually presents the landmark detection, rather than the face alignment given the image and landmarks as input.

So is there any codes associated to the face alignment ? "
"Hello all,

I was wondering if anyone has successfully do so, and I am specifically talking about arcface_v100 and MobileFaceNet.

I was able to import params with mxnet, and export with onnx format. I think mxnet currently does not support exporting to onnx with dynamic shapes. So I have to manually modify the onnx graph to take dynamic input. And this is where I got the error: Shuffle]: **at most one dimension may be inferred**. I did some searches and it seems that error was related to BatchNormalization.

I was wondering if anyone has been successfully converting the model to onnx with dynamic shapes or if there is workaround. Any ideas would be helpful. Thanks a lot.

My environment:
onnx: 1.3.0
mxnet: 1.6.0"
"`run.sh`  but  I get a error like this, I have tried mxnet 1.6.0, mxnet-cu101, but it is not work .the `horovodrun --check`  like this .

```
Horovod v0.19.2:

Available Frameworks:
    [X] TensorFlow
    [X] PyTorch
    [ ] MXNet

Available Controllers:
    [X] MPI
    [X] Gloo

Available Tensor Operations:
    [X] NCCL
    [ ] DDL
    [ ] CCL
    [X] MPI
    [X] Gloo
```
- my cuda version is 10.02 . so , Is my cuda version is wrong ???

when I `run.sh ` , the problem like this .
```

[ps-SYS-4028GR-TR:13182] Warning: could not find environment variable ""LD_LIBRARY_PATH""
Traceback (most recent call last):
  File ""train_memory.py"", line 14, in <module>
    import horovod.mxnet as hvd
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/mxnet/__init__.py"", line 23, in <module>
    __file__, 'mpi_lib')
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/common/util.py"", line 56, in check_extension
    'Horovod with %s=1 to debug the build error.' % (ext_name, ext_env_var))
ImportError: Extension horovod.mxnet has not been built.  If this is not expected, reinstall Horovod with HOROVOD_WITH_MXNET=1 to debug the build error.
Traceback (most recent call last):
  File ""train_memory.py"", line 14, in <module>
    import horovod.mxnet as hvd
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/mxnet/__init__.py"", line 23, in <module>
    __file__, 'mpi_lib')
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/common/util.py"", line 56, in check_extension
    'Horovod with %s=1 to debug the build error.' % (ext_name, ext_env_var))
ImportError: Extension horovod.mxnet has not been built.  If this is not expected, reinstall Horovod with HOROVOD_WITH_MXNET=1 to debug the build error.
Traceback (most recent call last):
  File ""train_memory.py"", line 14, in <module>
    import horovod.mxnet as hvd
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/mxnet/__init__.py"", line 23, in <module>
    __file__, 'mpi_lib')
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/common/util.py"", line 56, in check_extension
    'Horovod with %s=1 to debug the build error.' % (ext_name, ext_env_var))
ImportError: Extension horovod.mxnet has not been built.  If this is not expected, reinstall Horovod with HOROVOD_WITH_MXNET=1 to debug the build error.
Traceback (most recent call last):
  File ""train_memory.py"", line 14, in <module>
    import horovod.mxnet as hvd
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/mxnet/__init__.py"", line 23, in <module>
    __file__, 'mpi_lib')
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/common/util.py"", line 56, in check_extension
    'Horovod with %s=1 to debug the build error.' % (ext_name, ext_env_var))
ImportError: Extension horovod.mxnet has not been built.  If this is not expected, reinstall Horovod with HOROVOD_WITH_MXNET=1 to debug the build error.
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
Traceback (most recent call last):
  File ""train_memory.py"", line 14, in <module>
    import horovod.mxnet as hvd
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/mxnet/__init__.py"", line 23, in <module>
    __file__, 'mpi_lib')
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/common/util.py"", line 56, in check_extension
    'Horovod with %s=1 to debug the build error.' % (ext_name, ext_env_var))
ImportError: Extension horovod.mxnet has not been built.  If this is not expected, reinstall Horovod with HOROVOD_WITH_MXNET=1 to debug the build error.
Traceback (most recent call last):
  File ""train_memory.py"", line 14, in <module>
    import horovod.mxnet as hvd
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/mxnet/__init__.py"", line 23, in <module>
    __file__, 'mpi_lib')
  File ""/home/liuyang/anaconda3/envs/mxnet_partial/lib/python3.6/site-packages/horovod/common/util.py"", line 56, in check_extension
    'Horovod with %s=1 to debug the build error.' % (ext_name, ext_env_var))
ImportError: Extension horovod.mxnet has not been built.  If this is not expected, reinstall Horovod with HOROVOD_WITH_MXNET=1 to debug the build error.
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[35340,1],1]
  Exit code:    1
```"
"Hi all,
I am currently experiencing some issue running the model in onnx.

I used MobileFaceNet (from the pretrained models), and converted to onnx model, but when I tried to load model to infer at runtime, I got the following error: Type Error: Type parameter (T) bound to different types (tensor(double) and tensor(float) in node (conv_1_conv2d).

I did a bit research, and it looks like onnx file is not generating correctly. Could someone share a correct way to export the onnx model? Thanks in advance

My environment:
mxnet           == 1.6.0
onnx             == 1.7.0
onnxruntime == 1.5.2"
I trained model on Glint360K dataset. The performance on IJBC at 1e-4 is very good (about 97.3) but very poor at 1e-6 (about 85) in comparsion with MS1M (about 90). Any reason for the result? Thanks
Could you please share raw Glint360K (without crop and align)? Thanks
"Hi, can you provide links of IJB_B and IJB_C database, which has been processed to 112 x 112?"
use gluoncv 0.8 but it occurs AttributeError: module 'gluoncv.nn.sampler' has no attribute 'SplitSortedBucketSampler'
"A LResNet100E-IR model trained from scratch on the custom dataset is quite slower (30% slower) while inferencing on C++ code. After removing the last layer the architecture and the weight file size are the exact same for the trained and the pre-trained model, but there is a significant difference in inference speeds. I wanted to know if this is because of the models being optimized by TVM or something else."
"few days before now torrent worked, but yesterday and today not....."
"Do you have release the R100 model for dataset Glint360k ?
thanks
https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc"
"could you provide the google drive link?
Thanks a lot
https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc"
"Hi,
I have a quick question concerning lines 127-131 in /recognition/partial_fc/pytorch/partial_fc.py

                max_fc = torch.max(logits, dim=1, keepdim=True)[0]
                dist.all_reduce(max_fc, dist.ReduceOp.MAX)
                # Calculate exp(logits) and all-reduce
                logits_exp = torch.exp(logits - max_fc)

I did not find any hint in the paper. If I see it correctly, for each sample you subtract the largest logit score across devices, i.e. all logit scores are shifted so that the values are <= 0. Afaik, the cross-entropy loss is invariant to constant shifts, so I assume the motivation must have something to do with stability of numerical values when calling the exponential function, or am I missing out on something?"
"We cannot download baidu link, so please help us to provide Dropbox or Drive link? Thanks"
"So I downloaded and extracted glint360k and now I found that lfw.bin from glint360k is not the same as what I got from [ICCV 2019 workshop](https://ibug.doc.ic.ac.uk/resources/lightweight-face-recognition-challenge-workshop/) . I then inspect the images in the bin files and found that images from glint360's lfw.bin are aligned pretty badly comparing to the one from ICCV 2019. Here are some of the images:

lfw.bin from glint360k:
![glint2](https://user-images.githubusercontent.com/37909282/98338952-89af6800-203d-11eb-8fe0-390b1058b362.png)
![glint1](https://user-images.githubusercontent.com/37909282/98339033-acda1780-203d-11eb-8b08-1503e0ded48c.png)
![glint1](https://user-images.githubusercontent.com/37909282/98339385-35f14e80-203e-11eb-98b5-faec4323f84b.png)

lfw.bin from ICCV2019 workshop:
![ins2](https://user-images.githubusercontent.com/37909282/98338969-95029380-203d-11eb-8be7-8dd09cd6b78d.png)
![ins1](https://user-images.githubusercontent.com/37909282/98339051-b2cff880-203d-11eb-98fa-dbc45169cbab.png)
![ins1](https://user-images.githubusercontent.com/37909282/98339388-37bb1200-203e-11eb-9c55-e991a6839e57.png)
"
"`unpack_glint360.py` only needs `train.rec` and `train.idx`

what are `agedb_30.bin  calfw.bin cfp_ff.bin cfp_fp.bin cplfw.bin lfw.bin vgg2_fp.bin `for?
and how to read them?"
我用horovodrun -np 2 -H localhost:2 bash config.sh 去跑，几个钟一直没反应，这是在加载数据还是跑不起来？
"Could you comment on the main reason for the large difference in accuracy on east asian between InsightFace-Private with ArcFace (91.67) vs all other datasets/methods for example: Glint360k with PartialFC (63.91)?

R100 | Glint360k | PartialFC(r=0.1) | 90.45 | 94.60 | 93.96 | 63.91 | 88.23

R180 | InsightFace-Private | ArcFace | 94.45 | 96.98 | 96.02 | 91.67 | 96.26

It is a huge difference. Can it be explained by more/better data on east asians?




"
"I have downloaded the dataset via magnet link with qBittorent and it's been downloaded 99.99% (20MB left) for 4-5 days now and I checked the md5 hashes are as follows:

`cf7433cbb915ac422230ba33176f4625  glint360k_00`
`589a5ea3ab59f283d2b5dd3242bc027a  glint360k_01`
`12724f5b5ea380a94daf38dc2d3d01a8  glint360k_02`
`cd7f008579dbed9c5af4d1275915d95e  glint360k_03`
`64666b324911b47334cc824f5f836d4c  glint360k_04`
`1c98a1744e68aa8c3873628b5f8aa847  glint360k_05`
`c3ae1dcbecea360d2ec2a43a7b6f1d94  glint360k_06`

My hashes from `glint360k_02` and `glint360k_05` are not the same as provided hashes. Does anyone experience the same issue or have any solution? thanks."
"Nice work for partial FC and Glint360K, is there any identity overlap between Glint360k and Asian-Celeb?"
Everybody，how do face cluster for unlabeled face image？ths.
"Hi, first I want to say thanks for your sharing and effort to make glint360 dataset available.
We've downloaded the torrent and got 7 files, the first :
glint360k/glint360k_00 is gzip type file.
When we try to open it using the command you gave or others we get error, problem with EOF.

The rest are data files, are those files suppose to be used directly with ""unpack_glint360k.py""? or open in a different way?

Thanks a lot.




"
"We are learning models using arcface.

However, the results of the IJBC evaluation are drastically lowered in the 1e-05 1e-06 section.

What did we do wrong?

![image](https://user-images.githubusercontent.com/11434363/97841400-1948d400-1d29-11eb-9c8d-131ef3f126a7.png)
"
"File ""/anaconda3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 842, in broadcast
    work = _default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1595629395347/work/torch/lib/c10d/../c10d/NCCLUtils.hpp:82, invalid argument, NCCL version 2.4.8
##### pytorch1.6, cuda10.2 #####
gpu 0 can run normally, others report error.
"
"Can you share the download link of MS1MV3 used in the paper ""Sub-center ArcFace: Boosting Face Recognition by Large-scale Noisy Web Faces""? Thank you."
你好，请问有106关键点对应的序号吗，尤其是两眼中心对应的序号。
"After training IResnet-100 with Glint360k, I did validation of IJB-B and IJB-C.

TPR@FPR=1e-4 of IJB-C was 97.10% (closed to your Partial-FC paper result = 97.30%). 

But TPR@FPR=1e-6 of IJB-C was too low(82%) compared to other models(92.0%) trained with different datasets (ex: MS1MV2)

You know, the result of TPR(@FPR=1e-4 of IJB-B & IJB-C) has been mainly addressed in recent papers.

What do you think of which result is more relevant in ""NIST FRVT 1:1 rank"" between TPR@FPR=1e-4 and TPR@FPR=1e-6?

Could you share your opinion about this?

Thanks for reading."
can we download the original unaligned face images ?   thanks !
"Hello,
I want to test Arcface model that trained on pose specific alignment templates. But I can not find models in repository. Can you help me?"
"When I use train the model use ""horovodrun -np 8 -H localhost:8 bash config.sh""
I got the log as follows:
[1,5]<stderr>:[8311b4012d34:153937] Read -1, expected 131072, errno = 1
[1,7]<stderr>:[8311b4012d34:153939] Read -1, expected 131072, errno = 1
[1,3]<stderr>:[8311b4012d34:153934] Read -1, expected 131072, errno = 1
[1,6]<stderr>:[8311b4012d34:153938] Read -1, expected 131072, errno = 1
[1,4]<stderr>:[8311b4012d34:153933] Read -1, expected 131072, errno = 1
[1,7]<stderr>:[8311b4012d34:153939] Read -1, expected 131072, errno = 1
[1,5]<stderr>:[8311b4012d34:153937] Read -1, expected 131072, errno = 1
[1,6]<stderr>:[8311b4012d34:153938] Read -1, expected 131072, errno = 1
[1,7]<stderr>:[8311b4012d34:153939] Read -1, expected 131072, errno = 1

It seems something wrong, anybody can help me solve the problem?"
"I want to train my own model on 10 Million Identities. At the beginning of training, speed is 10000 samples/sec. In a few minutes, speed is 300 samples/sec. Why?
GPU: V100 * 8"
"Could you please explain the meaning of the rescale parameter in your sgd implementation?
https://github.com/deepinsight/insightface/blob/master/recognition/partial_fc/pytorch/sgd.py#L42

You set it to the world_size in the training here https://github.com/deepinsight/insightface/blob/master/recognition/partial_fc/pytorch/partial_fc.py#L82
and it seems like it affects gradients in both the backbone and the head."
能否提供glint360k每个分包的md5值。这样能更方便定位哪个分包下载有问题
"Dear @nttstar, @anxiangsir 

Would it be possible for you to share the training logs for the models evaluated in your new benchmark, i.e. r124, r180 and r100 in the new glint-celeb dataset?

Keep up the great work.
Thanks in advance"
"Before unpacking rec file, the number of images (described in train.idx) was 17,507,700

But after unpacking rec file with your unpacking code, the number of images becomes 17,091,657

Is 17 million real # of images of Glint360k?

◈ Note that I downloaded Glint360k using baidu."
"When I  run test.py in RetinaFace Anti Cov Face Detector, I get ""ModuleNotFoundError: No module named 'rcnn.cython'"" error.  How can I solve it ?

![Screenshot from 2020-10-13 21-31-35](https://user-images.githubusercontent.com/15729405/95901509-2151cb80-0d9c-11eb-9043-22adc030b235.png)
"
"Hi , do you have plan to release Glint360k? If not, what is Glint360k dataset? Is Glint360k = Celeb +DeepGlint? Why not uses MS1M? Thanks"
"I have an overflow of memory when I use train face recognition with multi-thread in use SubFace-Center with num-worker=1.
```Traceback (most recent call last):
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
web_1  |     self.run()
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/threading.py"", line 864, in run
web_1  |     self._target(*self._args, **self._kwargs)
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/io/io.py"", line 399, in prefetch_func
web_1  |     self.next_batch[i] = self.iters[i].next()
web_1  |   File ""arc_face_model_training/recognition/SubCenter_ArcFace/image_iter.py"", line 193, in arc_face_model_training.recognition.SubCenter_ArcFace.image_iter.FaceImageIter.next
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/ndarray/utils.py"", line 103, in empty
web_1  |     return _empty_ndarray(shape, ctx, dtype)
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py"", line 3955, in empty
web_1  |     return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py"", line 141, in _new_alloc_handle
web_1  |     ctypes.byref(hdl)))
web_1  |   File ""/usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/base.py"", line 253, in check_call
web_1  |     raise MXNetError(py_str(_LIB.MXGetLastError()))
web_1  | mxnet.base.MXNetError: [07:41:06] src/storage/./cpu_device_storage.h:75: Failed to allocate CPU Memory
web_1  | Stack trace:
web_1  |   [bt] (0) /usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x4b04cb) [0x7efe1a9b24cb]
web_1  |   [bt] (1) /usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e61e61) [0x7efe1d363e61]
web_1  |   [bt] (2) /usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2e6af0f) [0x7efe1d36cf0f]
web_1  |   [bt] (3) /usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/libmxnet.so(mxnet::NDArray::NDArray(mxnet::TShape const&, mxnet::Context, bool, int)+0x5d0) [0x7efe1ca336d0]
web_1  |   [bt] (4) /usr/local/envs/insightface/lib/python3.6/site-packages/mxnet/libmxnet.so(MXNDArrayCreateEx+0x22d) [0x7efe1ca3419d]
web_1  |   [bt] (5) /usr/local/envs/insightface/lib/python3.6/lib-dynload/../../libffi.so.7(+0x69dd) [0x7efeeeb379dd]
web_1  |   [bt] (6) /usr/local/envs/insightface/lib/python3.6/lib-dynload/../../libffi.so.7(+0x6067) [0x7efeeeb37067]
web_1  |   [bt] (7) /usr/local/envs/insightface/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7efeee8bcede]
web_1  |   [bt] (8) /usr/local/envs/insightface/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x13915) [0x7efeee8bd915]``` 


Can anyone help me fix it? Thanks."
I do not want to use the pretrained model 
"I want to train gender-age model to new dataset.
Dataset: http://afad-dataset.github.io/"
"When I run Evaluation/IJB/IJB_1N.py or Evaluation/IJB/IJB_11.py this error occurs:
`No module named 'embedding'.`

I think there may be an inconsistency or I have missed some files because at:
https://github.com/deepinsight/insightface/blob/master/Evaluation/IJB/IJB_11.py#L19
a path should be added to the system.
There should be a ""recognition"" folder.
"
"@nttstar 

After converting the models to TVM. 
How to get the actual coordinates in a particular image via python ? 

Thanks."
已经实验过，可以提升0.6-0.8个百分点。
"在face_align.py中用到的参考坐标是:
arcface_src = np.array([
  [38.2946, 51.6963],
  [73.5318, 51.5014],
  [56.0252, 71.7366],
  [41.5493, 92.3655],
  [70.7299, 92.2041] ], dtype=np.float32 )
但在face_preprocess.py中的参考坐标是:
src = np.array([
      [30.2946, 51.6963],
      [65.5318, 51.5014],
      [48.0252, 71.7366],
      [33.5493, 92.3655],
      [62.7299, 92.2041] ], dtype=np.float32 )

请问训练数据集中进行对齐时所用的参考坐标是哪一个? 前向时人脸对齐的参考坐标是否需要与训练时保持一致, 不同的参考坐标对性能的影响大吗? 感谢~"
感谢分享，请问一下，106点的人脸关键点方法论文出处是哪篇文章？期待回复。
"I have to begin with apologize with starting this, because i'm not experienced in working with such tools and CUDA, but I really want to try and I can't do it for like 3 month because of the following issue:
**OSError: libcudart.so.10.0: cannot open shared object file: No such file or directory** when i start any of .py programs inside the folder.
I had installed cuda-10.2 at first, then i thought that it wouldn't work with mxnet101, so I installed cuda-10.1 too and add all of their pathes to library_path. When I had cuda10-2 this error looked like OSError: libcudart.so.**10.1**: cannot open shared object file: No such file or directory
but now, when I installed 10.1, I still have the same issue, but with libcudart.so.10.0. 
I have troubles with installing cuda-10.0, but I also think that it shouldn't work like this, so maybe I failed somewhere, but I can't find where.
I use ubuntu through VirtualBox, so maybe this could be the reason too, but I didn't find out any thoughts about this here.
There is an issue #1022 and i don't know if anyone solved this yet. 
Much love
"
"用LResnet50IR,训练集是MS1MV2，lfw=99.68，cfp_fp=97.47，agedb_30=97.73，没有对应的参考系可以参照，不确定这样是否收敛了"
"```
CUDA_VISIBLE_DEVICES='0' python3 -u train.py --network vargfacenet --loss arcface --dataset retina
usage: train.py [-h] [--data-dir DATA_DIR] [--prefix PREFIX]
                [--pretrained PRETRAINED] [--ckpt CKPT] [--network NETWORK]
                [--version-se VERSION_SE] [--version-input VERSION_INPUT]
                [--version-output VERSION_OUTPUT]
                [--version-unit VERSION_UNIT] [--version-act VERSION_ACT]
                [--end-epoch END_EPOCH] [--noise-sgd NOISE_SGD] [--lr LR]
                [--wd WD] [--mom MOM] [--emb-size EMB_SIZE]
                [--per-batch-size PER_BATCH_SIZE] [--margin-m MARGIN_M]
                [--margin-s MARGIN_S] [--margin-a MARGIN_A]
                [--margin-b MARGIN_B] [--easy-margin EASY_MARGIN]
                [--margin-verbose MARGIN_VERBOSE]
                [--logits-verbose LOGITS_VERBOSE]
                [--c2c-threshold C2C_THRESHOLD] [--c2c-mode C2C_MODE]
                [--output-c2c OUTPUT_C2C] [--train-limit TRAIN_LIMIT]
                [--margin MARGIN] [--beta BETA] [--beta-min BETA_MIN]
                [--beta-freeze BETA_FREEZE] [--gamma GAMMA] [--power POWER]
                [--scale SCALE] [--center-alpha CENTER_ALPHA]
                [--center-scale CENTER_SCALE]
                [--images-per-identity IMAGES_PER_IDENTITY]
                [--triplet-bag-size TRIPLET_BAG_SIZE]
                [--triplet-alpha TRIPLET_ALPHA]
                [--triplet-max-ap TRIPLET_MAX_AP] [--verbose VERBOSE]
                [--loss-type LOSS_TYPE] [--incay INCAY]
                [--use-deformable USE_DEFORMABLE] [--rand-mirror RAND_MIRROR]
                [--cutoff CUTOFF] [--patch PATCH] [--lr-steps LR_STEPS]
                [--max-steps MAX_STEPS] [--target TARGET]
train.py: error: argument --loss-type: invalid int value: 'arcface'

```"
"Hello,

I was trying to download from the provided links for SubCenter Arcface and I am getting an extraction code wrong error. Is there another link to use?

Thanks for all your great work!"
https://github.com/WIKI2020/FacePose_pytorch
"i downloaded all the files in the link (https://pan.baidu.com/s/1oer0p4_mcOrs4cfdeWfbFg) provided by the repo owner.

**--- for IJBB:**

i ran the ```IJBB_Evaluation_MS1MV2.ipynb``` without modification, but the result is:

+-------------------------------------------+--------+--------+--------+--------+--------+--------+
|                  Methods                  | 1e-06  | 1e-05  | 0.0001 | 0.001  |  0.01  |  0.1   |
+-------------------------------------------+--------+--------+--------+--------+--------+--------+
| MS1MV2-ResNet100-ArcFace-TestMode(N0D1F2) | 0.1015 | 0.2333 | 0.4314 | 0.6743 | 0.8566 | 0.9440 |
| MS1MV2-ResNet100-ArcFace-TestMode(N1D1F2) | 0.1877 | 0.3847 | 0.6430 | 0.8425 | 0.9127 | 0.9500 |
+-------------------------------------------+--------+--------+--------+--------+--------+--------+

even i tried different ways for many times:
1, test on differnent machine 
2, change numpy version as mentioned in [issue470](https://github.com/deepinsight/insightface/issues/470), 
3, change python version
...

i still can not get the fancy result as show in the original notebook:

| MS1MV2-ResNet100-ArcFace-TestMode(N0D1F2) | 0.4089 | 0.8995 | 0.9463 | 0.9642 | 0.9761 | 0.9867 |
| MS1MV2-ResNet100-ArcFace-TestMode(N1D1F2) | 0.4281 | 0.9082 | 0.9490 | 0.9647 | 0.9767 | 0.9866 |

also, **--- for IJBC:** 
i ran ```IJBC_Evaluation_MS1MV2.ipynb``` and only got:
+-------------------------------------------+--------+--------+--------+--------+--------+--------+
|                  Methods                  | 1e-06  | 1e-05  | 0.0001 | 0.001  |  0.01  |  0.1   |
+-------------------------------------------+--------+--------+--------+--------+--------+--------+
| MS1MV2-ResNet100-ArcFace-TestMode(N0D0F0) | 0.1639 | 0.2839 | 0.4581 | 0.6822 | 0.8632 | 0.9500 |
| MS1MV2-ResNet100-ArcFace-TestMode(N1D1F1) | 0.2027 | 0.3830 | 0.6461 | 0.8564 | 0.9256 | 0.9589 |
+-------------------------------------------+--------+--------+--------+--------+--------+--------+

far away from the presented results by the repo owner, too.
it stuck me for many days, could you please help me ....?
@yingfeng @nttstar @ppwwyyxx @jiankangdeng @szad670401 


my environment configuration:
```
Package                            Version
---------------------------------- ----------------------
absl-py                            0.6.1
adium-theme-ubuntu                 0.3.4
apptools                           4.4.0
astor                              0.7.1
backports-abc                      0.5
backports.shutil-get-terminal-size 1.0.0
backports.weakref                  1.0.post1
bleach                             2.1.3
ccsm                               0.9.11.3
certifi                            2018.1.18
chardet                            3.0.4
compizconfig-python                0.9.11.3
configobj                          5.0.6
configparser                       3.5.0
cycler                             0.9.0
Cython                             0.23.4
decorator                          4.3.0
easydict                           1.7
entrypoints                        0.2.3
enum34                             1.1.6
funcsigs                           1.0.2
functools32                        3.2.3.post2
future                             0.17.1
futures                            3.2.0
gast                               0.2.0
graphviz                           0.8.1
grpcio                             1.16.1
h5py                               2.7.1
html5lib                           1.0.1
idna                               2.6
iotop                              0.6
ipykernel                          4.8.2
ipython                            5.7.0
ipython-genutils                   0.2.0
ipywidgets                         7.2.1
Jinja2                             2.10
jsonschema                         2.6.0
jupyter                            1.0.0
jupyter-client                     5.2.3
jupyter-console                    5.2.0
jupyter-core                       4.4.0
Keras-Applications                 1.0.6
Keras-Preprocessing                1.0.5
leveldb                            0.194
lmdb                               0.94
Markdown                           3.0.1
MarkupSafe                         1.0
matplotlib                         1.5.1
menpo                              0.8.1
missingno                          0.3.8
mistune                            0.8.3
mock                               2.0.0
mxnet-cu80                         1.3.1
mxnet-cu90                         1.3.0
nbconvert                          5.3.1
nbformat                           4.4.0
networkx                           2.1
nose                               1.3.7
notebook                           5.5.0
numpy                              1.13.1
opencv-python                      3.3.0.10
pandas                             0.21.0
pandocfilters                      1.4.2
pathlib                            1.0
pathlib2                           2.3.2
pbr                                5.1.1
pexpect                            4.5.0
pickleshare                        0.7.4
Pillow                             3.1.2
pip                                20.2.2
prettytable                        0.7.2
prompt-toolkit                     1.0.15
protobuf                           3.6.1
ptyprocess                         0.5.2
pycurl                             7.43.0
pydot                              1.0.29
pyface                             5.1.0
pygal                              2.4.0
Pygments                           2.2.0
pygobject                          3.20.0
pyparsing                          2.0.3
Pyste                              0.9.10
python-apt                         1.1.0b1+ubuntu0.16.4.8
python-dateutil                    2.7.3
python-gflags                      3.1.2
pytz                               2014.10
PyYAML                             3.11
pyzmq                              17.0.0
qtconsole                          4.3.1
requests                           2.23.0
scandir                            1.7
scikit-image                       0.10.1
scikit-learn                       0.20.3
scipy                              0.19.1
seaborn                            0.8.1
Send2Trash                         1.5.0
setuptools                         39.1.0
Shapely                            1.5.13
simplegeneric                      0.8.1
singledispatch                     3.4.0.3
six                                1.11.0
sklearn                            0.0
tensorboard                        1.12.0
termcolor                          1.1.0
terminado                          0.8.1
testpath                           0.3.1
tornado                            5.0.2
traitlets                          4.3.2
traits                             4.6.0
traitsui                           5.1.0
unity-lens-photos                  1.0
urllib3                            1.22
virtualenv                         15.1.0
vispy                              0.5.1
wcwidth                            0.1.7
webencodings                       0.5.1
Werkzeug                           0.14.1
wheel                              0.29.0
widgetsnbextension                 3.2.1
```"
may I ask how to generate the heatmap label data in alignment task？ 
"【I'm using mxnet version 1.1.0.】

src/eval/validation.py:def test():L229:db = mx.io.DataBatch(data=(_data,), label=(_label,)):
https://github.com/deepinsight/insightface/blob/836db1b89677b566fe80575e6b694079b3c6f299/src/eval/verification.py#L229

Here, label is also specified in creating data for feature extraction.
I didn't feel it was necessary to specify a label. 
（In model.bind, label_shapes is not specified.）
①Can someone please tell me why I need to specify label in the model run for feature extraction?

Furthermore, the error occurs under these conditions.
[Batch when loading a model! = Batch at inference]

```
Traceback (most recent call last): |   |  
File ""verification_on_memory.py"", line 169, in <module>
main(parse_arguments(sys.argv[1:]))
File ""verification_on_memory.py"", line 92, in main
learned_embeddings = mxnet_util.get_reps(learned_data, model, args.batch_size_1)
File ""C:\libai\yamada\src\variable_batch\error_fix\symbol\mxnet_util.py"", line 246, in get_reps
model.forward(db, is_train=False)
File ""C:\Users\mitsu\Anaconda3\envs\yamada_bookai\lib\site-packages\mxnet\module\module.py"", line 611, in forward
for i, j in zip(self._label_shapes, data_batch.label)]
TypeError: zip argument #1 must support iteration
```

②Let me know how to solve the error

Any input would be appreciated！


"
"The repository has received many improvements, will the python package receive these improvements?"
"Could you tell me where to find the module named""insightface""?"
"Dataset: Deepglint
My dataset is DeepGlint, and I use warm up in my training schedular. 
num_workers = 1, num_ctx = 4, num_classes = 180125
loss_s = 64, loss_m1 = 1.0, loss_m2 = 0.35, loss_m3 = 0.25

BATCH: 20, CELOSS: 53, learning rate: 1e-5
...
BATCH: 280, CELOSS: 56, learning rate: 8e-5
...
BATCH: 540, CELOSS: 64, learning rate: 1e-4
...

any one know why?"
"Thanks. Thanks to you, I used 'test.py' usefully.

You are now about to run test_widerface.py. There were a lot of errors, but I'm still working on it.

But there's a big problem.

The 9th line of the 'test_rcnn.py' file is the problem.

Line 9: 'from ..core.loader import TestLoader'

But there is no 'TestLoader' in 'loader.py.'


Help me figure this out................."
resolved.
"python -u train.py --network r100 --loss arcface --dataset emore
Traceback (most recent call last):
  File ""train.py"", line 23, in <module>
    import fresnet
ModuleNotFoundError: No module named 'fresnet'

***************************************************

should have requirements.txt for dependencies.
"
"I'm trying to run training on **Colab** to check the training process, but it is getting cancelled while the training with default configs on Casia-webface dataset

**default config.py is used, I only added this part for dataset:**

dataset.casia = edict()
dataset.casia.dataset = 'casia'
dataset.casia.dataset_path = '/content/drive/My Drive/$CASIA_WEBFACE_ROOT'
dataset.casia.num_classes = 10572
dataset.casia.image_shape = (112,112,3)


**Command that I'm running**: *CUDA_VISIBLE_DEVICES='0' python -u train.py --network r100 --loss arcface --dataset casia*


**Output from the shell:**

gpu num: 1
prefix ./models/r100-arcface-casia/model
image_size [112, 112]
num_classes 10572
Called with argument: Namespace(batch_size=128, ckpt=3, ctx_num=1, dataset='casia', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'E', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'fresnet', 'num_layers': 100, 'dataset': 'casia', 'dataset_path': '/content/drive/My Drive/Deep Learning/Datasets/casia-webface', 'num_classes': 10572, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'r100', 'num_workers': 1, 'batch_size': 128, 'per_batch_size': 128}
0 1 E 3 prelu False
Network FLOPs: 24.2G
INFO:root:loading recordio /content/drive/My Drive/Deep Learning/Datasets/casia-webface/train.rec...
header0 label [490624. 501196.]
id2range 10572
490623
rand_mirror True
tcmalloc: large alloc 1806336000 bytes == 0xa650000 @  0x7efd7b731b6b 0x7efd7b751379 0x7efd36eaae2a 0x7efd36eb3f0f 0x7efd3657a6d0 0x7efd3657b19d 0x7efd7320adae 0x7efd7320a71f 0x7efd7341e5c4 0x7efd7341ec33 0x5a9eec 0x50a783 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d
tcmalloc: large alloc 1806336000 bytes == 0x760f8000 @  0x7efd7b731b6b 0x7efd7b751379 0x7efd36eaae2a 0x7efd36eb3f0f 0x7efd3657a6d0 0x7efd3657b19d 0x7efd7320adae 0x7efd7320a71f 0x7efd7341e5c4 0x7efd7341ec33 0x5a9eec 0x50a783 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
tcmalloc: large alloc 2107392000 bytes == 0xe3366000 @  0x7efd7b731b6b 0x7efd7b751379 0x7efd36eaae2a 0x7efd36eb3f0f 0x7efd3657a6d0 0x7efd3657b19d 0x7efd7320adae 0x7efd7320a71f 0x7efd7341e5c4 0x7efd7341ec33 0x5a9eec 0x50a783 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d
tcmalloc: large alloc 2107392000 bytes == 0x160d2a000 @  0x7efd7b731b6b 0x7efd7b751379 0x7efd36eaae2a 0x7efd36eb3f0f 0x7efd3657a6d0 0x7efd3657b19d 0x7efd7320adae 0x7efd7320a71f 0x7efd7341e5c4 0x7efd7341ec33 0x5a9eec 0x50a783 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
tcmalloc: large alloc 1806336000 bytes == 0x1df2b0000 @  0x7efd7b731b6b 0x7efd7b751379 0x7efd36eaae2a 0x7efd36eb3f0f 0x7efd3657a6d0 0x7efd3657b19d 0x7efd7320adae 0x7efd7320a71f 0x7efd7341e5c4 0x7efd7341ec33 0x5a9eec 0x50a783 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
**call reset()**
**^C**"
"Hi
I'm struggling to download  https://pan.baidu.com/s/10m5GmtNV5snynDrq3KqIdg (code: lqvv)
Would it be possible to upload to another host ?

thanks"
训练的权重做预训练的模型的时候，loss还是从最大开始，test的时候有精度输出。大家有遇到这个问题吗
"from rcnn.config import config, default, generate_config
rcnn.config应该改成sample_config!"
"I did test.py in RetinaFace then I got this problem
>python test.py

>   File ""test.py"", line 7, in <module>
    from retinaface import RetinaFace
  File ""C:\Users\Admin\Desktop\AI\AI Face\insightface-master\RetinaFace\retinaface.py"", line 13, in <module>
    from rcnn.processing.bbox_transform import clip_boxes
  File ""C:\Users\Admin\Desktop\AI\AI Face\insightface-master\RetinaFace\rcnn\processing\bbox_transform.py"", line 2, in <module>
    from ..cython.bbox import bbox_overlaps_cython
ModuleNotFoundError: No module named 'rcnn.cython.bbox'

I found that I have to ""make"" first! So I ran this command in cython folder

>python setup.py build_ext --inplace 

But I got this Error

>Traceback (most recent call last):
  File ""setup.py"", line 168, in <module>
    cmdclass={'build_ext': custom_build_ext},
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python37\lib\site-packages\setuptools\__init__.py"", line 145, in setup
    return distutils.core.setup(**attrs)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python37\lib\distutils\core.py"", line 148, in setup
    dist.run_commands()
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python37\lib\distutils\dist.py"", line 966, in run_commands
    self.run_command(cmd)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python37\lib\distutils\dist.py"", line 985, in run_command
    cmd_obj.run()
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python37\lib\site-packages\Cython\Distutils\old_build_ext.py"", line 186, in run
    _build_ext.build_ext.run(self)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python37\lib\distutils\command\build_ext.py"", line 340, in run
    self.build_extensions()
  File ""setup.py"", line 115, in build_extensions
    customize_compiler_for_nvcc(self.compiler)
  File ""setup.py"", line 88, in customize_compiler_for_nvcc
    default_compiler_so = self.compiler_so
AttributeError: 'MSVCCompiler' object has no attribute 'compiler_so'

How can I solve this problem? Please help me!"
"Hi author, 

I would like to ask you about this processing step. In the function `get_input`, you use the for loop to loop over the channel and standardize the pixel values with mean and std. 

https://github.com/deepinsight/insightface/blob/master/RetinaFace/retinaface.py#L218

Can I replace this code with these steps
```python
im_tensor = im / 255.0
im_tensor = im_tensor.transpose(2,0,1)
im_tensor = np.expand_dims(new_frame_crop, axis=0)
```

Furthermore, if I don't want the standardizing step,  is this code 
```python
im_tensor = im.transpose(2,0,1)
im_tensor = np.expand_dims(new_frame_crop, axis=0)
```
will be equivalent to 
```python
for i in range(3):
     im_tensor[0, i, :, :] = im[:, :, 2 - i]
```

Thank you in advance."
"I am trying to create rec file using custom image. I am getting index error at 0 while training.
How to insert header while creating rec file?"
我想写一个circle loss来试试看，但是如果按照mxnet的方式来写，我想的是直接MakeLoss(circle_loss)，但是却不知道如何写入这个get_symbol中，可能还是我没有完全理解透这个函数。所以想请教一下，如果我目前只写出了公式的前提下，该如何加入到这个get_symbol中可行？
"**deploy/test.py**  uses the MTCNN for face detction and landmark which is quite outdated and fails in a lot of tests.

Is there a way to switch to **Retinaface detection**? How to change the face detection to Retinaface in deploy/test.py?"
"Retinaface by default is using network='net3' but if you change it to use net5 then it gives an error, 

ValueError: operands could not be broadcast together with shapes

according to the comments in the code, net5 represents the retinaface network.

Is there any solutions for it?"
"In the ArcFace paper you report results on models trained on VGGFace2, but they are not in the model zoo. Would it be possible to upload it somewhere, preferably on DropBox (or anywhere but Baidu actually) and post the link here? Thanks a lot."
"Thanks deepinsight team for the great work! I have a problem about finetuning model. 

The ArcLoss introduces a margin to help the embedding model to perform better. However, I already have a pretrained embedding model, and a tiny dataset.  So I should only train the ArcLoss head (except for the embedding model) to recognition the identities of this tiny dataset. In my opinion, I should set `m = 0` so that the output of ArcLoss head is confidence of each identity.

But after I train the arcloss head in this way, the predicted result of arcloss head is much more unstable compared to facenet (based on softmax) and cosface.

Do I have something wrong? And how do I use arcloss to predict the confidence? Thanks a lot."
"python convert_onnx.py --prefix /run/media/insightface/RetinaFaceAntiCov/model/mnet_cov2 --output-onnx mnet_cov2.onnx
mxnet version: 1.5.1
onnx version: 1.2.1
input-shape: (1, 3, 112, 112)
[11:37:12] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.5.0. Attempting to upgrade...
[11:37:12] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""convert_onnx.py"", line 30, in <module>
    converted_model_path = onnx_mxnet.export_model(sym_file, params_file, [input_shape], np.float32, args.output_onnx)
  File ""/run/media/virtualenv/maxnet_py3/lib/python3.6/site-packages/mxnet/contrib/onnx/mx2onnx/export_model.py"", line 83, in export_model
    verbose=verbose)
  File ""/run/media//virtualenv/maxnet_py3/lib/python3.6/site-packages/mxnet/contrib/onnx/mx2onnx/export_onnx.py"", line 253, in create_onnx_graph_proto
    idx=idx
  File ""/run/media/virtualenv/maxnet_py3/lib/python3.6/site-packages/mxnet/contrib/onnx/mx2onnx/export_onnx.py"", line 90, in convert_layer
    raise AttributeError(""No conversion function registered for op type %s yet."" % op)
AttributeError: No conversion function registered for op type SoftmaxActivation yet."
""
"Hello
I want to evalute in CP-LEW and CA-LFW dataset . Where  is the .bin document of CP-LEW and CA-LFW datasets ?
"
"install insightface(python2.7), but when install network,require python 3.5+?
base on the readme, installed python2.7, then install insightface, but why insightface dependency a  component(networkx) which need python3.5+?

**Error Stack:**

  Using cached https://mirrors.aliyun.com/pypi/packages/ea/0b/189cd3c19faf362ff2df5f301456c6cf8571ef6684644cfdfdbff293825c/cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)
Collecting networkx>=1.8
  Using cached https://mirrors.aliyun.com/pypi/packages/bf/63/7b579dd3b1c49ce6b7fd8f6f864038f255201410905dd183cf7f4a3845cf/networkx-2.4.tar.gz (1.5 MB)
    ERROR: Command errored out with exit status 1:
     command: /usr/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-9DXqvi/networkx/setup.py'""'""'; __file__='""'""'/tmp/pip-install-9DXqvi/networkx/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base /tmp/pip-pip-egg-info-kUvbVY
         cwd: /tmp/pip-install-9DXqvi/networkx/
    Complete output (6 lines):
    NetworkX 2.3+ requires Python 3.5 or later (2.7 detected).
    
    For Python 2.7, please install version 2.2 using:
    
    $ pip install 'networkx==2.2'"
"Really appreciate the brilliant work of insightface.
But I got some problems when training.
It does not print loss value.

Here is my called argument:
```bash
Called with argument: Namespace(batch_size=1024, beta=1000.0, beta_freeze=0, beta_min=5.0, bn_mom=0.9, ce_loss=False, ckpt=2, color=0, ctx_num=4, cutoff=0, data_dir='./train_res', easy_margin=0, emb_size=128, end_epoch=100000, fc7_lr_mult=1.0, fc7_no_bias=False, fc7_wd_mult=10.0, gamma=0.12, image_channel=3, image_h=112, image_size='112,112', image_w=112, images_filter=0, loss_type=4, lr=0.001, lr_steps='20000,30000,40000', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=128.0, max_steps=100000, mom=0.9, network='y1', num_classes=40000, num_layers=1, per_batch_size=256, power=1.0, prefix='models/id_arcface/id_arcface', pretrained='/models/th_arcface_id/th_arcface_id,0007', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='val', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_multiplier=1.0, version_output='E', version_se=0, version_unit=3, wd=4e-05)
```
As above, the key argument is
`batch_size=1024, ce_loss=False, loss_type=4, network='y1'`
I can't see why it doesn't print lossvalue

Here is part of the training log:
```bash
call reset()
INFO:root:Epoch[10] Batch [0-20]        Speed: 441.49 samples/sec       acc=0.938709
INFO:root:Epoch[10] Batch [20-40]       Speed: 437.72 samples/sec       acc=0.933838
INFO:root:Epoch[10] Batch [40-60]       Speed: 440.15 samples/sec       acc=0.931934
INFO:root:Epoch[10] Train-acc=0.932805
INFO:root:Epoch[10] Time cost=180.892
call reset()
INFO:root:Epoch[11] Batch [0-20]        Speed: 444.41 samples/sec       acc=0.937360
INFO:root:Epoch[11] Batch [20-40]       Speed: 441.98 samples/sec       acc=0.937793
INFO:root:Epoch[11] Batch [40-60]       Speed: 441.07 samples/sec       acc=0.931982
INFO:root:Epoch[11] Train-acc=0.934733
INFO:root:Epoch[11] Time cost=180.304
```
I check the [issue](https://github.com/deepinsight/insightface/issues/783#issuecomment-535346115)
His argument has `--version-output GDC`
What does this argument means?
When I try this `--version-output GDC` from my former argument `--version-output E`
There is an error 
```bash
mxnet.base.MXNetError: [04:51:33] /home/travis/build/dmlc/mxnet-distro/mxnet-build/3rdparty/mshadow/../../src/operator/tensor/../elemwise_op_common.h:135: Check failed: assign(&dattr, vec.at(i)) Incompatible attr in node  at 0-th output: expected [128,25088], got [128,512]
```
But when it's  `--version-output E`, everything goes fine.
So how to print lossvalue? Need some help."
"NFO:root:Epoch[0] Batch [20-40]	Speed: 0.99 samples/sec	acc=0.000000	
INFO:root:Epoch[0] Batch [40-60]	Speed: 0.95 samples/sec	acc=0.000000	
INFO:root:Epoch[0] Batch [60-80]	Speed: 0.94 samples/sec	acc=0.006250	
INFO:root:Epoch[0] Batch [80-100]	Speed: 0.95 samples/sec	acc=0.006250	
INFO:root:Epoch[0] Batch [100-120]	Speed: 0.96 samples/sec	acc=0.006250	
INFO:root:Epoch[0] Batch [120-140]	Speed: 0.94 samples/sec	acc=0.006250	
INFO:root:Epoch[0] Batch [140-160]	Speed: 0.95 samples/sec	acc=0.000000	
INFO:root:Epoch[0] Batch [160-180]	Speed: 0.96 samples/sec	acc=0.000000	
INFO:root:Epoch[0] Batch [180-200]	Speed: 0.98 samples/sec	acc=0.000000	
INFO:root:Epoch[0] Batch [200-220]	Speed: 1.00 samples/sec	acc=0.000000	

I want print the lossvalue , but I can't find how to do it."
""
"while using alignment code i am not able to figure out why all the output is all zero after this step
`warped = cv2.warpAffine(img,M, (image_size, image_size), borderValue = 0.0)`

# File link
[https://github.com/deepinsight/insightface/blob/master/common/face_align.py](url)

# Input details of the function
img is a cropped face
img = (265,266,3)
image_size= 112
M = [[ 4.39405972e-01 -7.26766376e-02 -1.64471346e+02]
         [ 7.26766376e-02  4.39405972e-01 -1.09747717e+02]]"
"I am trying to convert the face preprocess for c++ using opencv. Can someone tell me equivalent code for below step using opencv?

`aligned = np.transpose(nimg, (2,0,1))`

I already have the aligned face image in RGB format (i.e. nimg). Also, what is the need for this transpose in the first place? Does it rotate the image?"
"Hi,

MXNet nightly builds have been moved from PyPI to S3 (https://github.com/pypa/pypi-support/issues/243). If this project requires the nightly builds for CI validation, please make sure that your CI pipeline now takes the nightly builds from https://dist.mxnet.io/python. Furthermore, as MXNet [1.7](https://github.com/apache/incubator-mxnet/issues/16864) and [2.0](https://github.com/apache/incubator-mxnet/issues/16167) are currently in progress simultaneously, we are releasing nightly builds for both 1.x and 2.x. Take `mxnet-cu102` for example:

If you need the nightly builds for 1.x:
```
pip install --pre ""mxnet-cu102<2"" -f https://dist.mxnet.io/python
```

If you need the nightly builds for 2.x:
```
pip install --pre ""mxnet-cu102>=2"" -f https://dist.mxnet.io/python
```

Feel free to close this issue if the necessary changes have been made, or If the project doesn't rely on MXNet nightly builds. Thanks."
"I was trying to train retina face with mnet, which was said to be 'lightweight'. But the generated model was too large:

```
-rw-r--r--. 1 root root 224M Mar  5 17:24 retina-0000.params
-rw-r--r--. 1 root root 340K Mar  5 17:24 retina-symbol.json

```

so any idea about this?"
"你好。
我们用MXNet的C++接口去调用R50模型并得到模型的耗时。发现我们采用MXNet-1.3.1训练的R50模型和你们所开放的R50模型在CPU上的耗时相差10倍。
| 模型             | MXNet版本 | CPU上平均耗时 | model-symbol.json.attrs.mxnet_version |
| ---------------- | --------- | --------- | --------- |
| model-r50-am-lfw | 0.12.1    | 0.5s      | 1201 |
| our-r50          | 1.3.1     | 1.6s      | 10301 |

我们比对两个模型的`model-symbol.json`，发现model-r50-am-lfw的是`""attrs"": {""mxnet_version"": [""int"", 1201]}`， 而our-r50的是`""attrs"": {""mxnet_version"": [""int"", 10301]}`. 除此之外两个模型的`model-symbol.json`完全一致。
请问这是由于训练模型的MXNet版本不同引起的吗？或者这种情况该如何解决？
谢谢。"
Solved.
"Hi all,

I know a few people asked this earlier but none of there solution solves my problem.

The .lst file contains 5880026 lines, which means my dataset contains 5880026 images. And the line's format in .lst file is '1 image_path id' since all images are aligned. ID starts from 0 and all images belong to the same person are listed together. And I run the code use python2.7, not python3.

The time count during the execution is around 0.3s per 1000 images before it counts to 5880000 images. After that, the image count still grows until 5967000 and the time count is around 0.05, which is much less than before.

Any idea why the count is more than actual image count?

Many thanks.
"
"According to `src/align/align_celeb.py` and the clean list provided in #19 , original MSCeleb dataset can be extracted and aligned. Names of aligned images still follow the clean list. However, the file structure and file name in faces_emore is quite different. Could you please tell me how to correspond images in faces_emore to original dataset? Thanks!"
"I follow the Third-party Re-implementation [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch/blob/master/prepare_data.py) to unpack the faces_emore dataset. As shown below, BGR Images can be seen after unpack. I thought they might be RGB. Why is that? Is the training using BGR instead of RGB?
![image](https://user-images.githubusercontent.com/25877709/74717085-b7b00a80-526a-11ea-86b8-95014dd1c64f.png)




"
Kindly provide alternate links for download.
"@nttstar @jiankangdeng 
I modified the config file for training a resnet 152 on ms1m dataset, but the resulting symbol is not correct. the params file is almost the same size as a resnet 100.

Here is the config I used, can someone help me identifying what went wrong?

```
config = edict()

config.bn_mom = 0.9
config.workspace = 256
config.emb_size = 512
config.ckpt_embedding = True
config.net_se = 0
config.net_act = 'prelu'
config.net_unit = 3
config.net_input = 1
config.net_blocks = [1,4,6,2]
config.net_output = 'E'
config.net_multiplier = 1.0
config.val_targets = ['lfw', 'cfp_fp', 'agedb_30']
config.ce_loss = True
config.fc7_lr_mult = 1.0
config.fc7_wd_mult = 1.0
config.fc7_no_bias = False
config.max_steps = 0
config.data_rand_mirror = True
config.data_cutoff = False
config.data_color = 0
config.data_images_filter = 0
config.count_flops = True
config.memonger = False #not work now


# network settings
network = edict()

network.r100 = edict()
network.r100.net_name = 'fresnet'
network.r100.num_layers = 100

network.r152 = edict()
network.r152.net_name = 'fresnet'
network.r152.num_layers = 152

dataset = edict()

dataset.merged = edict()

dataset.emore = edict()
dataset.emore.dataset = 'emore'
dataset.emore.dataset_path = '/media/ssd/fr-data/faces_emore'
dataset.emore.num_classes = 85742
dataset.emore.image_shape = (112,112,3)
dataset.emore.val_targets = ['lfw', 'cfp_fp', 'cfp_ff', 'agedb_30']

loss = edict()
loss.softmax = edict()
loss.softmax.loss_name = 'softmax'

loss.nsoftmax = edict()
loss.nsoftmax.loss_name = 'margin_softmax'
loss.nsoftmax.loss_s = 64.0
loss.nsoftmax.loss_m1 = 1.0
loss.nsoftmax.loss_m2 = 0.0
loss.nsoftmax.loss_m3 = 0.0

loss.arcface = edict()
loss.arcface.loss_name = 'margin_softmax'
loss.arcface.loss_s = 64.0
loss.arcface.loss_m1 = 1.0
loss.arcface.loss_m2 = 0.5
loss.arcface.loss_m3 = 0.0

loss.cosface = edict()
loss.cosface.loss_name = 'margin_softmax'
loss.cosface.loss_s = 64.0
loss.cosface.loss_m1 = 1.0
loss.cosface.loss_m2 = 0.0
loss.cosface.loss_m3 = 0.35

loss.combined = edict()
loss.combined.loss_name = 'margin_softmax'
loss.combined.loss_s = 64.0
loss.combined.loss_m1 = 1.0
loss.combined.loss_m2 = 0.3
loss.combined.loss_m3 = 0.2

loss.triplet = edict()
loss.triplet.loss_name = 'triplet'
loss.triplet.images_per_identity = 5
loss.triplet.triplet_alpha = 0.3
loss.triplet.triplet_bag_size = 7200
loss.triplet.triplet_max_ap = 0.0
loss.triplet.per_batch_size = 60
loss.triplet.lr = 0.05

loss.atriplet = edict()
loss.atriplet.loss_name = 'atriplet'
loss.atriplet.images_per_identity = 5
loss.atriplet.triplet_alpha = 0.35
loss.atriplet.triplet_bag_size = 7200
loss.atriplet.triplet_max_ap = 0.0
loss.atriplet.per_batch_size = 60
loss.atriplet.lr = 0.05

# default settings
default = edict()

# default network
default.network = 'r152'
default.pretrained = ''
default.pretrained_epoch = 1
# default dataset
default.dataset = 'emore'
default.loss = 'arcface'
default.frequent = 20
default.verbose = 2000
default.kvstore = 'device'

default.end_epoch = 10000
default.lr = 0.1
default.wd = 0.0005
default.mom = 0.9
default.per_batch_size = 64
default.ckpt = 3
default.lr_steps = '100000,160000,220000'
default.models_root = './models'


def generate_config(_network, _dataset, _loss):
    for k, v in loss[_loss].items():
      config[k] = v
      if k in default:
        default[k] = v
    for k, v in network[_network].items():
      config[k] = v
      if k in default:
        default[k] = v
    for k, v in dataset[_dataset].items():
      config[k] = v
      if k in default:
        default[k] = v
    config.loss = _loss
    config.network = _network
    config.dataset = _dataset
    config.num_workers = 1
    if 'DMLC_NUM_WORKER' in os.environ:
      config.num_workers = int(os.environ['DMLC_NUM_WORKER'])
```"
What exactly is the use_se parameter in residual units functions in fresnet.py? @nttstar 
Did you use SE-NET in your r50 model?
"if one person has n face images. After they through the net, we can achieve n-512 vectors, but we should choose one vector to represent a person(have many face images). so we use the **center feature vector** represent the person. is right? But how do you achieve the **center feature vector**.
is it achieved by the average of n-512 vectors???
hope your replay!"
"Hi,
    @yingfeng , there is no config file in RetinaFace project"
想基于insightface大模型进行小模型的蒸馏训练，采用最基本的KD方法，请问我应该取fc1层的输出还是fc7层的输出作为logits呢？
"In the ArcFace article, the _s_ scaling factor is applied to all the logits after the margin has been added on the logit that corresponds to the target class.

In the code here, `recognition/train_parall.py` seems to conform to the description in the article but I see an unexplained variation in `recognition/train.py`... Is there an explanation for the variant in `recognition/train.py`?

This version seems consistent with the article (the `config.loss_s` factor is applied at the end on all the logits):

https://github.com/deepinsight/insightface/blob/b1c747e4d58dba0dd011df4cd813ff70d828449c/recognition/train_parall.py#L100-L114


But this version is weird and unexplained - the logit of the target class is _divided_ by `config.loss_s`, then the margin is applied, then the modified logit is multiplied by `config.loss_s`, while the other logits are not affected by any scaling:

https://github.com/deepinsight/insightface/blob/4a4b8d03fec981912fdef5b3232a37a827cbeed6/recognition/train.py#L91-L105

"
Is mxnet-cu80 ok？
"Since I am modifying the source code,I tracked it to 
the 10th line in load_data.py:
`imdb = eval(dataset_name)(image_set_name, root_path, dataset_path)`

I can't find which library the eval is from and why it has two pairs of brackets?"
"您好：
虽然很多人问了这个问题，但是我还是不是很明白
out_list = [mx.symbol.BlockGrad(embedding)]这句话的含义

我认为embedding代表从图像到特征图的过程，如果这部分的梯度被切断，回传的梯度为0
那么该部分就无法从loss中获取回传信息，那里面的参数都不会变化，如何起到训练的作用？如何使该embedding朝着好的方向优化？
换句话说，embedding里的参数是根据什么优化的？

期待您的回答，非常感谢~"
"Hello Developer,

I found some ValueError: zero-size array to reduction operation maximum which has no identity issues that derive from numpy version 1.17.4, when executing verification.py

OS: Ubuntu 19.04 64 bits 5.0.0-36-generic

Python 3.7.3

pip3 list
```
Package                       Version    
----------------------------- -----------
absl-py                       0.8.1      
alabaster                     0.7.12     
alembic                       1.3.1      
aniso8601                     8.0.0      
apturl                        0.5.2      
asn1crypto                    0.24.0     
astor                         0.8.0      
astroid                       2.3.3      
attrs                         19.3.0     
Babel                         2.7.0      
backcall                      0.1.0      
bleach                        3.1.0      
blinker                       1.4        
Brlapi                        0.6.7      
certifi                       2018.8.24  
chardet                       3.0.4      
Click                         7.0        
cloudpickle                   1.2.2      
command-not-found             0.3        
cryptography                  2.3        
cupshelpers                   1.0        
cycler                        0.10.0     
dataset                       1.1.2      
decorator                     4.4.1      
defer                         1.0.6      
defusedxml                    0.6.0      
distro                        1.3.0      
distro-info                   0.21ubuntu2
dlib                          19.18.99   
docutils                      0.15.2     
easydict                      1.9        
entrypoints                   0.3        
facenet-pytorch               0.5.0      
Flask                         1.1.1      
flask-restplus                0.13.0     
gast                          0.2.2      
google-pasta                  0.1.8      
graphviz                      0.8.4      
grpcio                        1.25.0     
h5py                          2.10.0     
httplib2                      0.11.3     
idna                          2.6        
imageio                       2.6.1      
imagesize                     1.1.0      
importlib-metadata            0.23       
imutils                       0.5.3      
ipykernel                     5.1.3      
ipython                       7.9.0      
ipython-genutils              0.2.0      
isort                         4.3.21     
itsdangerous                  1.1.0      
jedi                          0.15.1     
Jinja2                        2.10.3     
joblib                        0.14.0     
jsonschema                    3.2.0      
jupyter-client                5.3.4      
jupyter-core                  4.6.1      
Keras                         2.3.1      
Keras-Applications            1.0.8      
Keras-Preprocessing           1.1.0      
keyring                       17.1.1     
keyrings.alt                  3.1.1      
kiwisolver                    1.1.0      
language-selector             0.1        
launchpadlib                  1.10.6     
lazr.restfulclient            0.14.2     
lazr.uri                      1.0.3      
lazy-object-proxy             1.4.3      
louis                         3.8.0      
macaroonbakery                1.2.1      
Mako                          1.0.7      
Markdown                      3.1.1      
MarkupSafe                    1.1.0      
matplotlib                    3.1.1      
mccabe                        0.6.1      
mistune                       0.8.4      
more-itertools                7.2.0      
mpi4py                        3.0.3      
mxnet-mkl                     1.5.1.post0
nbconvert                     5.6.1      
nbformat                      4.4.0      
netifaces                     0.10.4     
networkx                      2.4        
numpy                         1.17.4     
numpydoc                      0.9.1      
oauthlib                      2.1.0      
olefile                       0.46       
opencv-python                 4.1.1.26   
opt-einsum                    3.1.0      
packaging                     19.2       
pandas                        0.25.3     
pandocfilters                 1.4.2      
parso                         0.5.1      
pbr                           5.4.3      
pexpect                       4.6.0      
pickleshare                   0.7.5      
Pillow                        5.4.1      
pip                           19.3.1     
prompt-toolkit                2.0.10     
protobuf                      3.6.1      
psutil                        5.6.5      
pycairo                       1.16.2     
pycodestyle                   2.5.0      
pycrypto                      2.6.1      
pycups                        1.9.73     
pyflakes                      2.1.1      
Pygments                      2.4.2      
PyGObject                     3.32.0     
PyJWT                         1.7.0      
pylint                        2.4.4      
pymacaroons                   0.13.0     
PyNaCl                        1.3.0      
pyparsing                     2.4.5      
PyQt5                         5.12.3     
PyQt5-sip                     12.7.0     
PyQtWebEngine                 5.12.1     
pyRFC3339                     1.1        
pyrsistent                    0.15.5     
python-apt                    1.8.4      
python-dateutil               2.7.3      
python-debian                 0.1.34     
python-editor                 1.0.4      
pytz                          2018.9     
PyWavelets                    1.1.1      
pyxdg                         0.25       
PyYAML                        3.13       
pyzmq                         18.1.1     
QtAwesome                     0.6.0      
qtconsole                     4.6.0      
QtPy                          1.9.0      
reportlab                     3.5.18     
requests                      2.22.0     
requests-unixsocket           0.1.5      
rope                          0.14.0     
scikit-image                  0.16.2     
scikit-learn                  0.21.3     
scipy                         1.3.2      
seaborn                       0.9.0      
SecretStorage                 2.3.1      
setuptools                    41.6.0     
simplejson                    3.16.0     
six                           1.12.0     
snowballstemmer               2.0.0      
Sphinx                        2.2.1      
sphinxcontrib-applehelp       1.0.1      
sphinxcontrib-devhelp         1.0.1      
sphinxcontrib-htmlhelp        1.0.2      
sphinxcontrib-jsmath          1.0.1      
sphinxcontrib-qthelp          1.0.2      
sphinxcontrib-serializinghtml 1.1.3      
spyder                        3.3.6      
spyder-kernels                0.5.2      
SQLAlchemy                    1.3.11     
system-service                0.3        
systemd-python                234        
tensorboard                   1.15.0     
tensorflow                    1.15.0     
tensorflow-estimator          1.15.1     
termcolor                     1.1.0      
testpath                      0.4.4      
testresources                 2.0.1      
torch                         1.3.1+cpu  
torchvision                   0.4.2+cpu  
tornado                       6.0.3      
tqdm                          4.39.0     
traitlets                     4.3.3      
typed-ast                     1.4.0      
ubuntu-advantage-tools        19.2       
ubuntu-drivers-common         0.0.0      
ufw                           0.36       
unattended-upgrades           0.1        
urllib3                       1.24.1     
usb-creator                   0.3.3      
uWSGI                         2.0.18     
wadllib                       1.3.3      
wcwidth                       0.1.7      
webencodings                  0.5.1      
Werkzeug                      0.16.0     
wheel                         0.32.3     
wrapt                         1.11.2     
wurlitzer                     2.0.0      
xkit                          0.0.0      
zipp                          0.6.0 
``` 

Dataset: faces_ms1m-refine-v2_112x112.zip

Model: model-r100-arcface-ms1m-refine-v2.zip

Log:

```
python3 verification.py --data-dir=../../datasets/faces_emore --model=../../models --gpu=0 --batch-size=1 --target=lfw
image_size [112, 112]
model number 0
model loading time 5e-06
loading..  lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
Traceback (most recent call last):
  File ""verification.py"", line 588, in <module>
    print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results)))
  File ""<__array_function__ internals>"", line 6, in amax
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 2621, in amax
    keepdims=keepdims, initial=initial, where=where)
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 90, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
```

```
python3 verification.py --data-dir=../../datasets/faces_emore --model=../../models --gpu=0 --batch-size=1 --target=cfp_ff
image_size [112, 112]
model number 0
model loading time 6e-06
loading..  cfp_ff
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
Traceback (most recent call last):
  File ""verification.py"", line 588, in <module>
    print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results)))
  File ""<__array_function__ internals>"", line 6, in amax
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 2621, in amax
    keepdims=keepdims, initial=initial, where=where)
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 90, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
```

```
python3 verification.py --data-dir=../../datasets/faces_emore --model=../../models --gpu=0 --batch-size=1 --target=cfp_fp
image_size [112, 112]
model number 0
model loading time 5e-06
loading..  cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
Traceback (most recent call last):
  File ""verification.py"", line 588, in <module>
    print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results)))
  File ""<__array_function__ internals>"", line 6, in amax
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 2621, in amax
    keepdims=keepdims, initial=initial, where=where)
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 90, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
```

```
python3 verification.py --data-dir=../../datasets/faces_emore --model=../../models --gpu=0 --batch-size=1 --target=agedb_30
image_size [112, 112]
model number 0
model loading time 5e-06
loading..  agedb_30
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
Traceback (most recent call last):
  File ""verification.py"", line 588, in <module>
    print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results)))
  File ""<__array_function__ internals>"", line 6, in amax
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 2621, in amax
    keepdims=keepdims, initial=initial, where=where)
  File ""/home/dragon/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 90, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
```

Thanks
"
"您好，很感谢您的代码
我想用您的代码 训练自己的数据，看不懂数据的标注。我想请问数据标注的格式
#图片名称
{}x y w h （人脸框）}， {x1,y1,f1}， {x2,y2,f2}， {x3,y3,f3}， {x4,y4,f4}，val
其中xn,yn应该是代表landmark的坐标，fn的是代表点是否可见么
最后的浮点数val的含义是什么？
有些标注是负1的，是不是代码这个人脸的landmark不可见"
""
"Hello Developer,

I encountered an CPU memory error issue when I have executed the src/eval/verification.py to verify the pre-trained ""model-r100-arcface-ms1m-refine-v2"" model with ""faces_ms1m-refine-v2_112x112"" dataset.

OS: Ubuntu 19.04 64 bits 5.0.0-36-generic

Python 3.7.3

pip3 list
```
Package                       Version    
----------------------------- -----------
absl-py                       0.8.1      
alabaster                     0.7.12     
alembic                       1.3.1      
aniso8601                     8.0.0      
apturl                        0.5.2      
asn1crypto                    0.24.0     
astor                         0.8.0      
astroid                       2.3.3      
attrs                         19.3.0     
Babel                         2.7.0      
backcall                      0.1.0      
bleach                        3.1.0      
blinker                       1.4        
Brlapi                        0.6.7      
certifi                       2018.8.24  
chardet                       3.0.4      
Click                         7.0        
cloudpickle                   1.2.2      
command-not-found             0.3        
cryptography                  2.3        
cupshelpers                   1.0        
cycler                        0.10.0     
dataset                       1.1.2      
decorator                     4.4.1      
defer                         1.0.6      
defusedxml                    0.6.0      
distro                        1.3.0      
distro-info                   0.21ubuntu2
docutils                      0.15.2     
easydict                      1.9        
entrypoints                   0.3        
facenet-pytorch               0.5.0      
Flask                         1.1.1      
flask-restplus                0.13.0     
gast                          0.2.2      
google-pasta                  0.1.8      
graphviz                      0.8.4      
grpcio                        1.25.0     
h5py                          2.10.0     
httplib2                      0.11.3     
idna                          2.6        
imagesize                     1.1.0      
importlib-metadata            0.23       
imutils                       0.5.3      
ipykernel                     5.1.3      
ipython                       7.9.0      
ipython-genutils              0.2.0      
isort                         4.3.21     
itsdangerous                  1.1.0      
jedi                          0.15.1     
Jinja2                        2.10.3     
joblib                        0.14.0     
jsonschema                    3.2.0      
jupyter-client                5.3.4      
jupyter-core                  4.6.1      
Keras                         2.3.1      
Keras-Applications            1.0.8      
Keras-Preprocessing           1.1.0      
keyring                       17.1.1     
keyrings.alt                  3.1.1      
kiwisolver                    1.1.0      
language-selector             0.1        
launchpadlib                  1.10.6     
lazr.restfulclient            0.14.2     
lazr.uri                      1.0.3      
lazy-object-proxy             1.4.3      
louis                         3.8.0      
macaroonbakery                1.2.1      
Mako                          1.0.7      
Markdown                      3.1.1      
MarkupSafe                    1.1.0      
matplotlib                    3.1.1      
mccabe                        0.6.1      
mistune                       0.8.4      
more-itertools                7.2.0      
mpi4py                        3.0.3      
mxnet-mkl                     1.5.1.post0
nbconvert                     5.6.1      
nbformat                      4.4.0      
netifaces                     0.10.4     
numpy                         1.17.4     
numpydoc                      0.9.1      
oauthlib                      2.1.0      
olefile                       0.46       
opencv-python                 4.1.1.26   
opt-einsum                    3.1.0      
packaging                     19.2       
pandas                        0.25.3     
pandocfilters                 1.4.2      
parso                         0.5.1      
pbr                           5.4.3      
pexpect                       4.6.0      
pickleshare                   0.7.5      
Pillow                        5.4.1      
pip                           19.3.1     
prompt-toolkit                2.0.10     
protobuf                      3.6.1      
psutil                        5.6.5      
pycairo                       1.16.2     
pycodestyle                   2.5.0      
pycrypto                      2.6.1      
pycups                        1.9.73     
pyflakes                      2.1.1      
Pygments                      2.4.2      
PyGObject                     3.32.0     
PyJWT                         1.7.0      
pylint                        2.4.4      
pymacaroons                   0.13.0     
PyNaCl                        1.3.0      
pyparsing                     2.4.5      
PyQt5                         5.12.3     
PyQt5-sip                     12.7.0     
PyQtWebEngine                 5.12.1     
pyRFC3339                     1.1        
pyrsistent                    0.15.5     
python-apt                    1.8.4      
python-dateutil               2.7.3      
python-debian                 0.1.34     
python-editor                 1.0.4      
pytz                          2018.9     
pyxdg                         0.25       
PyYAML                        3.13       
pyzmq                         18.1.1     
QtAwesome                     0.6.0      
qtconsole                     4.6.0      
QtPy                          1.9.0      
reportlab                     3.5.18     
requests                      2.21.0     
requests-unixsocket           0.1.5      
rope                          0.14.0     
scikit-learn                  0.21.3     
scipy                         1.3.2      
seaborn                       0.9.0      
SecretStorage                 2.3.1      
setuptools                    41.6.0     
simplejson                    3.16.0     
six                           1.12.0     
snowballstemmer               2.0.0      
Sphinx                        2.2.1      
sphinxcontrib-applehelp       1.0.1      
sphinxcontrib-devhelp         1.0.1      
sphinxcontrib-htmlhelp        1.0.2      
sphinxcontrib-jsmath          1.0.1      
sphinxcontrib-qthelp          1.0.2      
sphinxcontrib-serializinghtml 1.1.3      
spyder                        3.3.6      
spyder-kernels                0.5.2      
SQLAlchemy                    1.3.11     
system-service                0.3        
systemd-python                234        
tensorboard                   1.15.0     
tensorflow                    1.15.0     
tensorflow-estimator          1.15.1     
termcolor                     1.1.0      
testpath                      0.4.4      
testresources                 2.0.1      
torch                         1.3.1+cpu  
torchvision                   0.4.2+cpu  
tornado                       6.0.3      
traitlets                     4.3.3      
typed-ast                     1.4.0      
ubuntu-advantage-tools        19.2       
ubuntu-drivers-common         0.0.0      
ufw                           0.36       
unattended-upgrades           0.1        
urllib3                       1.24.1     
usb-creator                   0.3.3      
uWSGI                         2.0.18     
wadllib                       1.3.3      
wcwidth                       0.1.7      
webencodings                  0.5.1      
Werkzeug                      0.16.0     
wheel                         0.32.3     
wrapt                         1.11.2     
wurlitzer                     2.0.0      
xkit                          0.0.0      
zipp                          0.6.0      
```
Dataset: faces_ms1m-refine-v2_112x112.zip

Model: model-r100-arcface-ms1m-refine-v2.zip

Log:

```
python3 verification.py --data-dir=../../datasets/faces_emore --model=../../models --gpu=0 --batch-size=1
image_size [112, 112]
model number 0
model loading time 7e-06
loading..  lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
loading..  cfp_ff
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
loading..  cfp_fp
Traceback (most recent call last):
  File ""verification.py"", line 574, in <module>
    data_set = load_bin(path, image_size)
  File ""verification.py"", line 188, in load_bin
    data = nd.empty((len(issame_list)*2, 3, image_size[0], image_size[1]))
  File ""/home/dragon/.local/lib/python3.7/site-packages/mxnet/ndarray/utils.py"", line 103, in empty
    return _empty_ndarray(shape, ctx, dtype)
  File ""/home/dragon/.local/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py"", line 3955, in empty
    return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
  File ""/home/dragon/.local/lib/python3.7/site-packages/mxnet/ndarray/ndarray.py"", line 141, in _new_alloc_handle
    ctypes.byref(hdl)))
  File ""/home/dragon/.local/lib/python3.7/site-packages/mxnet/base.py"", line 253, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [15:05:59] src/storage/./cpu_device_storage.h:75: Failed to allocate CPU Memory
Stack trace:
  [bt] (0) /home/dragon/.local/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x25b3db) [0x7f129f62a3db]
  [bt] (1) /home/dragon/.local/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cb1811) [0x7f12a2080811]
  [bt] (2) /home/dragon/.local/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2cb7c1f) [0x7f12a2086c1f]
  [bt] (3) /home/dragon/.local/lib/python3.7/site-packages/mxnet/libmxnet.so(mxnet::NDArray::NDArray(mxnet::TShape const&, mxnet::Context, bool, int)+0x673) [0x7f129f63a7f3]
  [bt] (4) /home/dragon/.local/lib/python3.7/site-packages/mxnet/libmxnet.so(MXNDArrayCreateEx+0x236) [0x7f12a1766a96]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f12ccdab81e]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x22f) [0x7f12ccdab1ef]
  [bt] (7) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2b4) [0x7f12ce0a4ef4]
  [bt] (8) /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12563) [0x7f12ce0a5563]
```

I hope someone able to point the issue or provide a solution.

Thank you.



"
""
"I try to retrain vargfacenet as in the LFR Challenge, however the training is showing infinite loss immediately.
Can someone please help?
For the information, I train with smaller batch_size and with only one GPU, but I doubt if that is the problem.
Log:
```
CUDA_VISIBLE_DEVICES='0' python -u train.py --network vargfacenet --loss arcface --dataset retina
gpu num: 1
prefix ./models/vargfacenet-arcface-retina/model
image_size [112, 112]
num_classes 93431
Called with argument: Namespace(batch_size=32, ckpt=3, ctx_num=1, dataset='retina', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='vargfacenet', per_batch_size=32, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 512, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_blocks': [1, 4, 6, 2], 'net_output': 'J', 'net_multiplier': 1.25, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'count_flops': True, 'memonger': False, 'loss_name': 'margin_softmax', 'loss_s': 64.0, 'loss_m1': 1.0, 'loss_m2': 0.5, 'loss_m3': 0.0, 'net_name': 'vargfacenet', 'dataset': 'retina', 'dataset_path': '../datasets/ms1m-retinaface-t1', 'num_classes': 93431, 'image_shape': [112, 112, 3], 'loss': 'arcface', 'network': 'vargfacenet', 'num_workers': 1, 'batch_size': 32, 'per_batch_size': 32}
Network FLOPs: 1.0G
INFO:root:loading recordio ../datasets/ms1m-retinaface-t1/train.rec...
header0 label [5179511. 5272942.]
id2range 93431
5179510
rand_mirror True
[13:21:19] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()
/home/vdx/csenv/lib/python3.7/site-packages/mxnet/module/base_module.py:504: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.03125). Is this intended?
  optimizer_params=optimizer_params)
[13:22:01] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (set the environment variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:Epoch[0] Batch [0-20]	Speed: 62.45 samples/sec	acc=0.000000	lossvalue=nan
INFO:root:Epoch[0] Batch [20-40]	Speed: 61.56 samples/sec	acc=0.000000	lossvalue=nan
INFO:root:Epoch[0] Batch [40-60]	Speed: 58.71 samples/sec	acc=0.000000	lossvalue=nan
INFO:root:Epoch[0] Batch [60-80]	Speed: 44.77 samples/sec	acc=0.000000	lossvalue=nan
INFO:root:Epoch[0] Batch [80-100]	Speed: 74.33 samples/sec	acc=0.000000	lossvalue=nan
INFO:root:Epoch[0] Batch [100-120]	Speed: 26.56 samples/sec	acc=0.000000	lossvalue=nan
```"
"Is there a pre-trained model available for use with the implementation of ""Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment"" available at https://github.com/deepinsight/insightface/tree/master/alignment ?

Thank you"
"first, i read yours code and find landmarks loss method is the same as box center point, 
so in yolov3 ， i calculation the  landmarks loss the same as box center point too.
but  the reult face box detection is right, landmarks is too bad. 

Can landmarks regression only use alike ssd anchors get better result?"
"I found a license description as below:

The training data containing the annotation (and the models trained with these data) are available for non-commercial research purposes only.

If we use VGG2 in Dataset Zoo for model training, are the obtained models available for commercial use?
The original VGG2  is under a Creative Commons license."
"Hi,
When I try to get results on multiple checkpoints say 10 using verification.py, each forward call of the checkpoint creates a new 1.2GB allocation on the GPU RAM and eventually the GPU RAM is used completely and a out of memory error is displayed.
How to solve this?"
"hi, i'm trying to understand your age and gender modle and I saw there is BS * 101 output for your modle, could you explain more on the your algorithm? Thank you"
"Hello everybody, I intend to use this awesome work on Nvidia Jetson Nano (arm64), is this okay?

Btw, what is the best strategy to use this work so that it provides the fastest inference speed but also acceptable accuracy? Because the faces I intended to recognize are nearly frontal, purple-colored at night (captured with Waveshare IMX219-160IR night vision camera).

Did anyone have prior experience with Jetson devices or have any related benchmark? Please give me some advices on this. Thank you a lot!!"
"I want to use mobilefacenet as a classification model instead of an embedding model, but when I run the compile script as follows, pay attention that the checkpoint is not trimmed after the embedding layer but kept the last fully-connetcted layer, I just want get the prob of which person one face belongs to .
```
import numpy as np
import argparse
import nnvm.compiler
import nnvm.testing
import tvm
from tvm.contrib import graph_runtime
import mxnet as mx
from mxnet import ndarray as nd

prefix,epoch = ""/app/model/model-mobilefacenets/model-y1-softmax"", 0
sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
image_size = (112, 112)
opt_level = 3

shape_dict = {'data': (1, 3, *image_size)}
target = tvm.target.create(""cuda"")
# ""target"" means your target platform you want to compile.

#target = tvm.target.create(""llvm -mcpu=broadwell"")
nnvm_sym, nnvm_params = nnvm.frontend.from_mxnet(sym, arg_params, aux_params)
```
the error messages is as follows:
OpNotImplemented: Operator BlockGrad is not supported in frontend MXNet.

What can I do to deal with this error? Please help me out, thank you very much.
@yingfeng @nttstar @ppwwyyxx @kernel8liang @jiankangdeng "
"Hi @nttstar !

I've read your code in reconition/parall_module_local_v1.py. What confused me was that why you conduct `fc7_outs[i] = nd.broadcast_sub(fc7_outs[i]`, _max when you implement back propagation."
" I had tired to add code to visualize loss, but it seemed to be called only once. I was not skilled with mxnet,could you help me?
https://github.com/YangYangGirl/insightface/blob/282a57cd0714fd220a7df2f636c153f1697760b7/recognition/train.py#L236

![image](https://user-images.githubusercontent.com/32426369/66675148-48605380-ec97-11e9-9bad-7f8d9413025f.png)
"
"I'm just trying to understand what's behind the ""feature embedding"" term/function:
1. If when the deep ANN is training it's goal is to become a good face feature extractor, then it should not embed any face feature in itself, OK ? Then where the features to be embedded go ?

2. If when training, the goal is to ""embed"" all face features **in** ANN weights (and have say 10k outputs for 10k persons to classify),  then the ""**embedding**"" already happened during training (which requires lot of passes and samples). So how then the ""feature embedding"" which takes only 17 mS (as written in the .md file) works ?  E.t. where does it embed features ?"
"Hi all,

I've been trying to train insightface from scratch for smaller input image size, 64x64. The sturcture is r50 and dataset is emore.

I first trained on my local workstation with batch size 128. The validation accuracy on agedb_30 is 0.96. Validation accuracy is 0.4, loss is around 6. And I tested it on my private dataset, had an accuracy of 0.996.

And then I trained it on a remote server using the same setting except batch size, this time I set it to 512 since the server has a better GPU. I also trained it using a single card. The validation accuracy on agedb_30 was 0.974, which is a little better as expected. Validation accuracy was 0.67, loss was 3. However, when I tested this model on my private dataset, the accuracy was only 0.96, 3.6% lower than the previous model.

The test codes are exactly the same. One possible reason might be the different cuda driver version I have locally and remotely (396 vs 418). And when running on remote server, I used docker for convenience. But I don't know how to eliminate this possibility except training again remotely using batch size 128.

Any idea what might caused this problem?

Thanks."
"@yingfeng 
Hello, Could you upload the datasets of gender-age?"
"Hi, you mentioned in the post https://github.com/deepinsight/insightface/issues/556#issuecomment-466807654
that the ijbc_face_tid_mid.txt is from [vgg_face2]( https://github.com/ox-vgg/vgg_face2/tree/master/standard_evaluation).
But the file vgg_face2 they provide is based on IJBB.
could you also share your ijbc related files?
Thank you."
I want function get_feature with all GPU (my GPU have 2 core). please?
"When I try to train RetinaFace with the following command:  `CUDA_VISIBLE_DEVICES='0' python -u train.py --prefix ./model/retina --network resnet` it trains all the way to whatever epoch I have set, and then crashes during ""save_model()"" with the message:

```
terminated without active exception
Aborted (core dumped)
```
Any idea as to why?

More details:
GPU: Nvidia 2080TI
Starting weight: ResNet50 training weights
I also just used the default values in config.py"
"Sorry, I note that in file `insightface/recognition/train.py`, line 89 - 104
https://github.com/deepinsight/insightface/blob/3fb3f12c3c516482c6810b06e95dbcc7c0d4c177/recognition/train.py#L89-L104

In my understanding,
```python
zy = mx.sym.pick(fc7, gt_label, axis=1)  # pick s*cos(theta_{y_i})
cos_t = zy/s    # cos(theta_{y_i})
t = mx.sym.arccos(cos_t)  # theta_{y_i}
if config.loss_m1!=1.0:
    t = t*config.loss_m1  # m_1 * theta_{y_i}
if config.loss_m2>0.0:
    t = t+config.loss_m2  # m_1 * theta_{y_i} + m_2
body = mx.sym.cos(t)  # cos(m_1 * theta_{y_i} + m_2)
if config.loss_m3>0.0:
    body = body - config.loss_m3  # cos(m_1 * theta_{y_i} + m_2) - m_3
new_zy = body*s  # s * cos(m_1 * theta_{y_i} + m_2) - m_3
diff = new_zy - zy  # s * cos(m_1 * theta_{y_i} + m_2) - m_3 - s * cos(theta_{y_i})
diff = mx.sym.expand_dims(diff, 1)
gt_one_hot = mx.sym.one_hot(gt_label, depth = config.num_classes, on_value = 1.0, off_value = 0.0)  # label one hot
body = mx.sym.broadcast_mul(gt_one_hot, diff)
fc7 = fc7+body  # keep elements which j != y_i
```

Then we can get
**fc7_ij = s * cos(theta_ij) if j != gt_i**
**fc7_ij = s * cos(m_1 * theta_{theta_ij} + m_2) - m_3 - s * cos(theta_ij) if j == gt_i**

This is so different from [Eq4](https://arxiv.org/pdf/1801.07698.pdf).

I know some implementations use **cos(x)cos(t) - sin(x)sin(t)** instead of **cos(x+t)**, so that they need **new_zy = cos(x)cos(t)** and **zy = sin(x)sin(t)**. If **arccos** is used, I think **new_zy** and **diff** are not needed anymore. Maybe you forget to delete it...

Or,  is there something wrong about my understanding?"
"maybe one face image per one person?
Please let me know"
"When I use face2rec.py all goes well, but in the end I have this error below:
`Traceback (most recent call last):
  File ""face2rec2.py"", line 253, in <module>
    image_encode(args, i, item, q_out)
  File ""face2rec2.py"", line 105, in image_encode
    s = mx.recordio.pack(header, '')
  File ""/home/song/miniconda3/lib/python3.7/site-packages/mxnet/recordio.py"", line 393, in pack
    s = label.tostring() + s
TypeError: can't concat str to bytes
`
It seems like there is something in the last line of .lst file
( I seperate the .lst file into 2 files, error persists.)
Hi someone could help to explain this error?"
"It looks like in order to convert the model over to TVM to see the increases in performance advertised on the benchmark page, the model needs to be [autotuned](https://docs.tvm.ai/tutorials/autotvm/tune_relay_x86.html#sphx-glr-tutorials-autotvm-tune-relay-x86-py). Without autotuning, the [speed of inference](https://github.com/deepinsight/insightface/issues/897#issue-490561769) with TVM is roughly that of MXNET. Sample output from the conversion step. `Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 3, 112, 112, 'float32'), (64, 3, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.` This indicates that autotuning in necessary. 

Can you share the script used for autotuning?
The tutorial posted [here](https://github.com/deepinsight/insightface/wiki/Tutorial:-Deploy-Face-Recognition-Model-via-TVM) is incomplete without it.  
 "
"Hi,
I'm not very familiar with mxnet and I have a problem with gender-age model from this repo. I noticed that it predicts different results depends on some third side code. I tried to make a minimal [example](https://gist.github.com/RomanSteinberg/f9405fa02f3fdf98d3c7e24550c5336b). So, it loads model, takes first operation called ‘_minus_scalar0’, which should subtract 127.5 from the input matrix. But it is not! If you look at the [screenshot](http://i.imgur.com/2lvfoeC.png), you can see that it subtracts 127.0 only. 

**Important.** If I comment line 8, the code works correctly and predicts the correct age if I specify `fc1` output. I don’t understand why. I reproduced this issue on 3 computers from 4.  

1. What symbol operation corresponds to `_minus_scalar`? I checked `broadcast_minus` and `broadcast_sub`, these are not `_minus_scalar` operations.  
2. How can I fix incorrect subtraction?

"
"as i trained the retinaface with resnet152 with config as below ,

network.resnet.pretrained = 'model/resnet-152'
network.resnet.pretrained_epoch = 0
network.resnet.lr_step = '1,2,3,4,5,55,68,80'
network.resnet.lr = 0.001
network.resnet.PIXEL_MEANS = np.array([0.0, 0.0, 0.0])
network.resnet.PIXEL_STDS = np.array([1.0, 1.0, 1.0])
network.resnet.PIXEL_SCALE = 1.0
network.resnet.FIXED_PARAMS = ['^stage1', '^.*upsampling']
network.resnet.BATCH_IMAGES = 2
network.resnet.HEAD_FILTER_NUM = 256
network.resnet.CONTEXT_FILTER_RATIO = 1
#network.resnet.USE_DCN = 2
network.resnet.USE_DCN = 0
network.resnet.RPN_BATCH_SIZE = 256
network.resnet.RPN_ANCHOR_CFG = RAC_RETINA

After trained 10 epoch , i test got error below , plz help to check the issue 
Traceback (most recent call last):
  File ""test.py"", line 38, in <module>
    faces, landmarks = detector.detect(img, thresh, scales=scales, do_flip=flip)
  File ""/home/ai/big_disk/GearRoundSpace/SOTA/insightface/RetinaFace/retinaface.py"", line 296, in detect
    proposals = proposals[order, :]
IndexError: index 768 is out of bounds for axis 0 with size 768
"
"I am trying to convert an MXNET model to TVM in order to improve the inference speed.  I am able to convert it successfully, however I do not experience the improvements in speed which are advertised on [this page](https://github.com/deepinsight/insightface/wiki/TVM-Benchmark)

I have followed the tutorial [here](https://github.com/deepinsight/insightface/wiki/Tutorial:-Deploy-Face-Recognition-Model-via-TVM), but I will go through the steps I took.

I first downloaded the Insightface model `LResNet100E-IR,ArcFace@ms1m-refine-v2` which can be found [here](https://github.com/deepinsight/insightface/wiki/Model-Zoo#31-lresnet100e-irarcfacems1m-refine-v2). Note that I am using the same model from the TVM benchmark.

Next, I use the following python script to convert the model to the TVM compatible models
Note that when I run the command `llc --version` I get the following output (which is why I set the target to `skylake`)
```
LLVM (http://llvm.org/):
  LLVM version 6.0.0
  
  Optimized build.
  Default target: x86_64-pc-linux-gnu
  Host CPU: skylake

```
Python conversion script
```
from tvm.contrib import graph_runtime
import mxnet as mx
from mxnet import ndarray as nd
import nnvm.compiler
import nnvm.testing
import tvm

prefix,epoch = ""/home/nchafni/Cyrus/models/faceDetection/Insightface/model-r100-ii/model"",0
sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
opt_level = 3

shape_dict = {'data': (1, 3, 112, 112)}
target = tvm.target.create(""llvm -mcpu=skylake"")
#target = tvm.target.intel_graphics()
nnvm_sym, nnvm_params = nnvm.frontend.from_mxnet(sym, arg_params, aux_params)
with nnvm.compiler.build_config(opt_level=opt_level):
   graph, lib, params = nnvm.compiler.build(nnvm_sym, target, shape_dict, params=nnvm_params)
lib.export_library(""./deploy_lib.so"")
print('lib export succeefully')
with open(""./deploy_graph.json"", ""w"") as fo:
   fo.write(graph.json())
with open(""./deploy_param.params"", ""wb"") as fo:
   fo.write(nnvm.compiler.save_param_dict(params))
``` 

When I run the script, I get the following warning messages:
```
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 3, 112, 112, 'float32'), (64, 3, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 64, 112, 112, 'float32'), (64, 64, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 64, 112, 112, 'float32'), (64, 64, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 64, 112, 112, 'float32'), (64, 64, 1, 1, 'float32'), (2, 2), (0, 0), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 64, 56, 56, 'float32'), (128, 64, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 128, 56, 56, 'float32'), (128, 128, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 128, 28, 28, 'float32'), (256, 128, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 256, 28, 28, 'float32'), (256, 256, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 256, 14, 14, 'float32'), (512, 256, 3, 3, 'float32'), (1, 1), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
Cannot find config for target=llvm -mcpu=skylake, workload=('conv2d', (1, 512, 14, 14, 'float32'), (512, 512, 3, 3, 'float32'), (2, 2), (1, 1), (1, 1), 'NCHW', 'float32'). A fallback configuration is used, which may bring great performance regression.
WARNING:autotvm:Cannot find config for target=llvm -mcpu=skylake, workload=('dense', (1, 25088, 'float32'), (512, 25088, 'float32'), (512, 'float32'), 0). A fallback configuration is used, which may bring great performance regression.
lib export succeefully

```
but it ultimately exports the models successfully. 

Next, I import the the converted models and `deploy_lib.so` into my C++ project. I am using the following code. The majority of the code is taken from the example on [this page](https://github.com/deepinsight/insightface/wiki/Tutorial:-Deploy-Face-Recognition-Model-via-TVM)

```
#include <chrono>
#include <iostream>
#include <fstream>
#include ""opencv2/opencv.hpp""
#include ""tvm/runtime/module.h""
#include ""tvm/runtime/registry.h""
#include ""tvm/runtime/packed_func.h""

typedef std::chrono::high_resolution_clock Clock;

class FR_MFN_Deploy{
private:
    void * handle;

public:
    FR_MFN_Deploy(std::string modelFolder)
    {
        tvm::runtime::Module mod_syslib = tvm::runtime::Module::LoadFromFile(""/home/nchafni/Cyrus/tvm_test/lib/deploy_lib.so"");
        //load graph
        std::string modelPath = modelFolder + ""/deploy_graph.json"";
        std::ifstream json_in(modelPath);
        std::string json_data((std::istreambuf_iterator<char>(json_in)), std::istreambuf_iterator<char>());
        json_in.close();
        int device_type = kDLCPU;
        int device_id = 0;
        // get global function module for graph runtime
        tvm::runtime::Module mod = (*tvm::runtime::Registry::Get(""tvm.graph_runtime.create""))(json_data, mod_syslib, device_type, device_id);
        this->handle = new tvm::runtime::Module(mod);
        //load param
        std::ifstream params_in(modelFolder + ""/deploy_param.params"", std::ios::binary);
        std::string params_data((std::istreambuf_iterator<char>(params_in)), std::istreambuf_iterator<char>());
        params_in.close();
        TVMByteArray params_arr;
        params_arr.data = params_data.c_str();
        params_arr.size = params_data.length();
        tvm::runtime::PackedFunc load_params = mod.GetFunction(""load_params"");
        load_params(params_arr);
    }


    cv::Mat forward(cv::Mat inputImageAligned)
    {
        //mobilefacnet preprocess has been written in graph.
        cv::Mat tensor = cv::dnn::blobFromImage(inputImageAligned,1.0,cv::Size(112,112),cv::Scalar(0,0,0),true);
        //convert uint8 to float32 and convert to RGB via opencv dnn function
        DLTensor* input;
        constexpr int dtype_code = kDLFloat;
        constexpr int dtype_bits = 32;
        constexpr int dtype_lanes = 1;
        constexpr int device_type = kDLCPU;
        constexpr int device_id = 0;
        constexpr int in_ndim = 4;
        const int64_t in_shape[in_ndim] = {1, 3, 112, 112};
        TVMArrayAlloc(in_shape, in_ndim, dtype_code, dtype_bits, dtype_lanes, device_type, device_id, &input);//
        TVMArrayCopyFromBytes(input,tensor.data,112*3*112*4);
        tvm::runtime::Module* mod = (tvm::runtime::Module*)handle;
        tvm::runtime::PackedFunc set_input = mod->GetFunction(""set_input"");
        set_input(""data"", input);
        tvm::runtime::PackedFunc run = mod->GetFunction(""run"");
        run();
        tvm::runtime::PackedFunc get_output = mod->GetFunction(""get_output"");
        tvm::runtime::NDArray res = get_output(0);
        cv::Mat vector(512,1,CV_32F);
        memcpy(vector.data,res->data,512*4);
        cv::Mat _l2;
        cv::multiply(vector,vector,_l2);
        float l2 =  cv::sqrt(cv::sum(_l2).val[0]);
        vector = vector / l2;
        TVMArrayFree(input);
        return vector;
    }

};

inline float CosineDistance(const cv::Mat &v1,const cv::Mat &v2){
    return static_cast<float>(v1.dot(v2));
}


cv::Mat getTemplate(const std::string& imagePath, FR_MFN_Deploy& deploy) {
    cv::Mat data = cv::imread(imagePath);
    auto time_1 = Clock::now();
    cv::Mat out = deploy.forward(data);
    auto time_2 = Clock::now();
    std::cout << std::to_string(std::chrono::duration_cast<std::chrono::milliseconds>(time_2 - time_1).count()) << std::endl;
    return out;
}


int main() {
    std::cout << ""Loading the model"" << std::endl;
    FR_MFN_Deploy deploy(""../models"");
    std::cout << ""Loaded model"" << std::endl;

    // Different People
//    std::vector<std::string> imagePaths = {
//            ""../images/chip17.jpg"",
//            ""../images/chip18.jpg"",
//            ""../images/chip19.jpg"",
//            ""../images/chip20.jpg"",
//            ""../images/chip21.jpg"",
//            ""../images/chip22.jpg"",
//            ""../images/chip23.jpg"",
//    };

// Same person
    std::vector<std::string> imagePaths = {
            ""../images/chip1.jpg"",
            ""../images/chip2.jpg"",
            ""../images/chip3.jpg"",
            ""../images/chip4.jpg"",
            ""../images/chip5.jpg"",
            ""../images/chip6.jpg"",
            ""../images/chip7.jpg"",
            ""../images/chip8.jpg"",
            ""../images/chip9.jpg"",
            ""../images/chip10.jpg"",
            ""../images/chip11.jpg"",
            ""../images/chip12.jpg"",
            ""../images/chip13.jpg"",
            ""../images/chip14.jpg"",
            ""../images/chip15.jpg"",
            ""../images/chip16.jpg"",
    };

    std::vector<cv::Mat> res;
    std::vector<float> scoresVec;

    for (const auto& path: imagePaths) {
        res.emplace_back(getTemplate(path, deploy));
    }

    for (size_t i = 0; i < res.size(); i++) {
        for (size_t k = i + 1; k < res.size(); k++) {
            auto score = CosineDistance(res[i],res[k]);
            if (score < 0) {
                score = 0;
            }
            scoresVec.emplace_back(score);
        }
    }

    double total = 0;
    for (int i = 0; i < scoresVec.size(); ++i) {
        total +=  scoresVec[i];
        std::cout << scoresVec[i] << std::endl;
    }

    std::cout << ""Total score: "" << total << ""\n"";

    return 0;
}
```
Note that the images I am provided are pre-aligned and cropped to 112x112.

On average, the inference takes 360ms, which is roughly the same time it takes to perform inference using MXNET (C++, MKLDNN). I was expecting to see a significant decrease in inference time. 

I am not sure if the issue has to do with the warnings during the conversion? I followed the conversion tutorial exactly and the tutorial did not mention needing to fine tune the model or anything.

Here is the output of `cat /proc/cpuinfo` to understand what hardward I am running the benchmark on:

```
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 158
model name	: Intel(R) Core(TM) i5-7500T CPU @ 2.70GHz
stepping	: 9
microcode	: 0xb4
cpu MHz		: 1407.008
cache size	: 6144 KB
physical id	: 0
siblings	: 4
core id		: 0
cpu cores	: 4
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 22
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d
bugs		: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs
bogomips	: 5424.00
clflush size	: 64
cache_alignment	: 64
address sizes	: 39 bits physical, 48 bits virtual
power management:

```

"
"please add 

from __future__ import division
import mxnet as mx
import numpy as np
import mxnet.ndarray as nd
import cv2

in your face_detection.py

or you will get an issue when you change scaling call in your tutorial example when you want to resize the image cause cv2 is missing.

bboxes, landmark = model.detect(img, threshold=0.5, scale=0.5)

"
"I am trying to build and run the C++ code in the example [here](https://github.com/deepinsight/insightface/wiki/Tutorial:-Deploy-Face-Recognition-Model-via-TVM)

I followed the installation instructions for TVM found [here](https://docs.tvm.ai/install/from_source.html#build-the-shared-library)  with `USE_OPENCL=OFF`

I then used the following script to generate the `deploy_lib.so`
```
from tvm.contrib import graph_runtime
import mxnet as mx
from mxnet import ndarray as nd
import nnvm.compiler
import nnvm.testing
import tvm

prefix,epoch = ""/home/Cyrus/models/model"",0
sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
opt_level = 3

shape_dict = {'data': (1, 3, 112, 112)}
target = tvm.target.intel_graphics()
nnvm_sym, nnvm_params = nnvm.frontend.from_mxnet(sym, arg_params, aux_params)
with nnvm.compiler.build_config(opt_level=opt_level):
   graph, lib, params = nnvm.compiler.build(nnvm_sym, target, shape_dict, params=nnvm_params)
lib.export_library(""./deploy_lib.so"")
print('lib export succeefully')
with open(""./deploy_graph.json"", ""w"") as fo:
   fo.write(graph.json())
with open(""./deploy_param.params"", ""wb"") as fo:
   fo.write(nnvm.compiler.save_param_dict(params))
```

When I try to run my inference program, I get the following error:

```
terminate called after throwing an instance of 'dmlc::Error'
  what():  [16:20:27] /home/Cyrus/tvm_test/src/runtime/module_util.cc:55: Check failed: f != nullptr: Loader of opencl(module.loadbinary_opencl) is not presented.
Stack trace:
  [bt] (0) ./tvm_test(+0xc024) [0x556ea2400024]
  [bt] (1) ./tvm_test(+0x1503a) [0x556ea240903a]
  [bt] (2) ./tvm_test(+0x3406b) [0x556ea242806b]
  [bt] (3) ./tvm_test(+0x1ab99) [0x556ea240eb99]
  [bt] (4) ./tvm_test(+0x23456) [0x556ea2417456]
  [bt] (5) ./tvm_test(+0xe444) [0x556ea2402444]
  [bt] (6) ./tvm_test(+0x3abbc) [0x556ea242ebbc]
  [bt] (7) ./tvm_test(+0x15c75) [0x556ea2409c75]
  [bt] (8) ./tvm_test(+0xcf91) [0x556ea2400f91]

```

Any ideas??"
"**Original Model** 
```
  0.00675622 -0.03742538  0.03577848  0.02893462 -0.01584588 -0.04866453
 -0.0352581  -0.00489045  0.03216138 -0.01399782 -0.00362552 -0.0554513
 -0.00752576 -0.00390024  0.01482633 -0.06620109  0.00105063 -0.03775996
  0.04578774 -0.09448475 -0.01194919 -0.14657265 -0.03667854 -0.03659046
  0.01852678 -0.02289687  0.01898221  0.01794297  0.0443676  -0.02609266
  0.01293959 -0.016729    0.0356557   0.03135339  0.04697475  0.03509054
  0.00060206 -0.03371359 -0.05095973  0.09035986 -0.0168414   0.02548007
  0.00863866 -0.02226089  0.0316763   0.02461711 -0.03980781  0.04869832
 -0.02577006  0.00397647  0.01082991 -0.00130232  0.04165735 -0.01785829
  0.05406979 -0.00309397  0.031048   -0.02412992  0.03556775 -0.01318556
 -0.02777418 -0.00191084  0.0048639   0.0204445   0.05174106  0.05434688
  0.06496373  0.00842059 -0.00334796 -0.06135342 -0.10769684 -0.03259984
  0.07277102 -0.03202721  0.00457122  0.02115323  0.00655404  0.03531669
 -0.00801192  0.03510019  0.01583081 -0.02665322  0.0804356  -0.03705338
  0.07928468 -0.02539046  0.00220785 -0.02071189 -0.11317256  0.04057368
  0.01673253 -0.0547691  -0.06760184 -0.06985002  0.04183917  0.00375162
  0.03828076 -0.01464206  0.07674873  0.09327153 -0.04846806 -0.04711819
```

**TVM**
```
  2.47226888e-03 -1.45394250e-03  7.21026212e-04 -9.32182302e-04
  1.98681862e-03  1.74708781e-03  2.61754962e-03  1.95533037e-03
  3.35488221e-05 -1.87860138e-03 -2.83959974e-03  5.03507210e-03
 -9.38448822e-04  1.41981128e-03  4.81366820e-04 -1.24042900e-03
  1.76508096e-03  1.37172383e-03 -2.21819477e-03  2.71358690e-03
 -1.43596891e-03  2.21575916e-04  6.03470195e-04 -7.25693753e-05
  2.32125283e-03 -9.95105947e-04  3.01290140e-03 -1.72401298e-04
  1.73006870e-03 -1.34458137e-03  1.98192173e-03 -7.34730624e-04
 -1.54764415e-03 -1.06481064e-04  2.71032768e-04  1.13921613e-03
  2.88313674e-03  3.02833994e-03  3.61993280e-03  4.69215098e-04
 -1.86557882e-04 -3.41875944e-03 -6.00113068e-03 -1.81653874e-03
  4.05497896e-03 -1.78463489e-03  2.54718907e-04  1.17870606e-03
  3.65209067e-04  1.96793559e-03 -4.46444727e-04  1.95587077e-03
  8.82135297e-04 -1.48518407e-03  4.48206766e-03 -2.06470513e-03
  4.41793120e-03 -1.41481869e-03  1.23027290e-04 -1.15411659e-03
 -6.30625105e-03  2.26086471e-03  9.32377530e-04 -3.05187027e-03
 -3.76694114e-03 -3.89221078e-03  2.33138097e-03  2.09050195e-04
  2.13309960e-03 -8.15887935e-04  4.27662535e-03  5.19731594e-03
 -2.70075654e-03 -2.62554362e-03 -4.90247784e-03  1.11286048e-04
```

I am getting quite different results with TVM and looks like it is missing lot of matches...."
"Hi @nttstar,

This is more a question than an issue.

Thanks for open your code to the community. Noticed the new package, thanks a lot for that also.

Are the models included in the package the ones from insightface model zoo of or the lightweight face recognition challenge?

Thanks,
Miguel "
"I made my own dataset and suffering from the accuracy loss..
I have tried changing lr,
number of classes of dataset,
number of images  of a person in dataset,
training loss f, 
aligning method..

any tips will be greatly appreciated"
"执行此语句后
`CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore`
提示错误：
`raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [14:09:27] src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open ""home/cl/insightface/datasets/faces_emore/train.rec"": No such file or directory
`
但是train.rec文件的目录检查过没有问题，为什么会报错呢？"
"execute 
`CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore`
Error prompted
`raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [14:09:27] src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open ""home/cl/insightface/datasets/faces_emore/train.rec"": No such file or directory
`
But the file exists in this path
"
"shouldn't it be ```range(1, int(header.label[1]))```
```
imgrec = mx.recordio.MXIndexedRecordIO(idx_path=args.idx_path, uri=args.bin_path, flag='r')
s = imgrec.read_idx(0)
header, _ = mx.recordio.unpack(s)
imgidx = list(range(1, int(header.label[0])))
```
?"
"@nttstar the function get_feature in gen_megaface.py,you mean to use both rgb and bgr data to geneate features of megaface and finnally add together. here I don't know why need to extract features with rgb and bgr in the same  image?
  data = mx.nd.zeros(shape = (count*2, 3, imgs[0].shape[0], imgs[0].shape[1]))
  for idx, img in enumerate(imgs):
      img = img[:,:,::-1] #to rgb
      img = np.transpose( img, (2,0,1) )
      for flipid in [0,1]:
          _img = np.copy(img)
          if flipid==1:
              _img = _img[:,:,::-1]
          _img = nd.array(_img)
      data[count*flipid+idx] = _img
anyone who reply it will be thanks!"
"I am trying to align image with different size (182x182) but I am not getting good results. The alignment looks good if I pass the default image_size (112x112). So, I am wondering if the
`

      src = np.array([
      [30.2946, 51.6963],
      [65.5318, 51.5014],
      [48.0252, 71.7366],
      [33.5493, 92.3655],
      [62.7299, 92.2041] ], dtype=np.float32 )

    if image_size[1]==112:
      src[:,0] += 8.0`
matrix given is for 112x112 images only, "
"I can't detect 900 faces as the papers said on the world largest selfie, instead I can just detect 575 faces. Should I try multi-scale test on world largest selfie?"
"I followed the instruction in 4.8 of retinaface paper, I trained a mobilenet0.25 backbone with  a 7*7 convolution and stride=4 , and train my retinaface model based my backbone, but can just get AP 72.3%
More strangely, I got a worse result(AP 67%) when I tile dense anchor on P3 P4 and P5 "
"I'm running RetinaFace/test.py (with default settings + the pretrained model) and it found 153 faces in a picture of 6 faces. Turned out that it's drawing multiple bounding boxes on the same face. What may be going wrong? 
<img width=""600"" alt=""Screen Shot 2562-08-05 at 23 28 50"" src=""https://user-images.githubusercontent.com/7446985/62481084-a6764380-b7db-11e9-8c5f-b2d918cd1e7f.png"">
"
"我用adam在vggface2上训练，逐渐衰减lr到0.001，模型的loss衰减到1.45几乎不再变化了（8631分类的training acc在81%。）感觉很迷惑，这是遇到极值点了吗。我应该怎样避免呢。


还是说这个时候模型的性能已经很好了，对图像对进行test能达到比较高的精度（>90%）？"
"what module should I install?

already installed nms, helper btw"
"Hi, @nttstar , this project is great and I have some questions to ask you.

If the two photos in the comparison are abnormal faces, such as very blurred faces, non-faces, and extreme side faces, their feature vectors will have a similar degree of similarity such as 70% or more (I guess because they don’t already have the characteristics of normal faces, so the inferred features are not comparable to the features of normal faces, but the feature vectors between non-face images will be similar). 

In the production environment, if the bottom library has unclean photos, and the front-end camera also accidentally pushes the undetected non-face photo, an error comparison will occur. In addition to ensuring the cleanliness of the database and improving the accuracy of camera face detection, what are the options for optimizing the face recognition model to avoid this problem?

Thank you! Looking forward to your favourable reply."
"i want to train in my own dataset,but how can i get the landmarks ?
and ,just a question ,what is the landmarks?
roi = {
              'image': image_path,
              'stream': stream,
              'height': imsize[1],
              'width': imsize[0],
              'boxes': boxes,
              'landmarks': landmarks,
              'blur': blur,
              'gt_classes': gt_classes,
              'gt_overlaps': overlaps,
              'max_classes': overlaps.argmax(axis=1),
              'max_overlaps': overlaps.max(axis=1),
              'flipped': False,
            }"
"This line is resulting in a segmentation fault. 
` memcpy(v.data,res->data,512*4);`

I have made changes according to the GPU and 512D vector. Is this a bug ?

```
        cv::Mat forward(cv::Mat inputImageAligned)
        {
            //mobilefacnet preprocess has been written in graph.
            cv::Mat tensor = cv::dnn::blobFromImage(inputImageAligned,1.0,cv::Size(112,112),cv::Scalar(0,0,0),true);
            //convert uint8 to float32 and convert to RGB via opencv dnn function
            DLTensor* input;
            constexpr int dtype_code = kDLFloat;
            constexpr int dtype_bits = 32;
            constexpr int dtype_lanes = 1;
            constexpr int device_type = kDLGPU;
            constexpr int device_id = 0;
            constexpr int in_ndim = 4;
            const int64_t in_shape[in_ndim] = {1, 3, 112, 112};
            TVMArrayAlloc(in_shape, in_ndim, dtype_code, dtype_bits, dtype_lanes, device_type, device_id, &input);//
            TVMArrayCopyFromBytes(input,tensor.data,112*3*112*4);
            tvm::runtime::Module* mod = (tvm::runtime::Module*)handle;
            tvm::runtime::PackedFunc set_input = mod->GetFunction(""set_input"");
            set_input(""data"", input);
            tvm::runtime::PackedFunc run = mod->GetFunction(""run"");
            run();
            tvm::runtime::PackedFunc get_output = mod->GetFunction(""get_output"");
            tvm::runtime::NDArray res = get_output(0);

            cv::Mat v(512,1,CV_32F);
            memcpy(v.data,res->data,512*4);

            cv::Mat _l2;
            // normlize
            cv::multiply(v,v,_l2);
            float l2 =  cv::sqrt(cv::sum(_l2).val[0]);
            v = v / l2;

            TVMArrayFree(input);
            return v;
        }
```"
why is 102300 how to get it?
""
"CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --prefix ./model/mnet --network mnet
Traceback (most recent call last):
  File ""train.py"", line 14, in <module>
    from rcnn.symbol import *
  File ""/opt/RetinaFace/rcnn/symbol/__init__.py"", line 2, in <module>
    from .symbol_mnet import *
  File ""/opt/RetinaFace/rcnn/symbol/symbol_mnet.py"", line 9, in <module>
    from symbol_common import get_sym_train
ModuleNotFoundError: No module named 'symbol_common'"
"Dose anyone run the `test.py` based on the released mode **RetinaFace-R50**  ([baidu cloud](https://pan.baidu.com/s/1C6nKq122gJxRhb37vK0_LQ) or [dropbox](https://www.dropbox.com/s/53ftnlarhyrpkg2/retinaface-R50.zip?dl=0))

I ran this `test.py` and got the final results like follows:
```
[3222] recall 1.0 (32, 71) all: 0.858467975701
[3223] recall 1.0 (1, 2) all: 0.858471543076
[3224] recall 0.9473684210526315 (19, 46) all: 0.858514095684
[3225] recall 1.0 (4, 11) all: 0.858528352268
```
It shows the final over all result is about 0.858, which doesn't match with the reported :
```
WiderFace validation mAP: Easy 96.5, Medium 95.6, Hard 90.4.
```
Actually, it's belower than even the Hard result: 90.4 
Please note me if there is something I'm wrong.
"
""
相似的，在src目录下，还看到其他模型的训练。请问使用中有什么需要注意的吗
看代码没有这个功能，每个有谁知道咋搞吗
"@nttstar Hello,
I am trying to get two or more faces from one image and extract landmarks and print number of faces with mtcnn in `deploy/mtcnn_detector.py` when I am printing `print(total_boxes)` on [ line no](https://github.com/deepinsight/insightface/blob/master/deploy/mtcnn_detector.py#L513)   I am getting the output as `[[265.23931277 257.99892117 529.48574665 602.69352765   0.99998236]
 [554.60383058 311.6666975  579.33545327 341.22184038   0.97458839]]
` if there are two faces I am getting the same output 
I am running `deploy/test.py` how can I get two faces from one image"
"Hi Insightface team, thank you for sharing the code. I have tried to develop a recoginition demo using KNN. But I found the results is not good as I have done with face_recoginition of Dlib. The distance between faces and those in the model trained by archface is alway higher than face_recognition lib (mean that less confidence). Could you pls help me to review if I did smt wrong ? 
Here is the code I use to train : 

    `
    X = []
    y = []
    parser = argparse.ArgumentParser(description='face model test')
    parser.add_argument('--image-size', default='112,112', help='')
    parser.add_argument('--model', default='./deploy/model-r100-ii/model,0', help='path to load model.')
    parser.add_argument('--ga-model', default='./deploy/gamodel-r50/model,0', help='path to load model.')
    parser.add_argument('--gpu', default=0, type=int, help='gpu id')
    parser.add_argument('--det', default=0, type=int,
                        help='mtcnn option, 1 means using R+O, 0 means detect from begining')
    parser.add_argument('--flip', default=0, type=int, help='whether do lr flip aug')
    parser.add_argument('--threshold', default=1.24, type=float, help='ver dist threshold')
    args = parser.parse_args()
    model = face_model.FaceModel(args)
    today = date.today().strftime(""%d%m%Y"")

    # Loop through each person in the training set
    i = 0;
    for class_dir in os.listdir(train_dir):
        i += 1
        print(str(i) + '------------------' + class_dir)
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue

        # Loop through each training image for the current person
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            img = cv2.imread(img_path)
            img = model.get_input(img)
            print('Running inference... %s', img_path)
            if  img is None or len(img) == 0 :
                 print('Cannot calculate ', img_path)
                 continue
             start = time.time()
             _face_description = model.get_feature(img)
             print(""Face extract took {} seconds."".format(time.time() - start))
             X.append(_face_description)
             y.append(class_dir)
    clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    clf.fit(X, y)
    return clf
`
and the code to recognize is following: 

      ```

       video_capture = cv2.VideoCapture(video_path + filename)
       ret, frame = video_capture.read()
       img = self.face_describer.get_input(frame)
       _face_description = self.face_describer.get_feature(img)
       _face_description = np.expand_dims(_face_description, axis=0)
       # Use the KNN model to find the best matches for the test face
       closest_distances = clf.kneighbors(_face_description, n_neighbors=5)
       are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(closest_distances[0]))]
       # Predict classes and remove classifications that aren't within the threshold
       return [(closest_distances[0][0][0], pred, loc) if rec else (closest_distances[0][0][0], ""unknown"", loc) for pred, loc, rec in zip(clf.predict(_face_description), X_face_locations, are_matches)]
```

Thanks. "
"I found this github thread from the MXNet repository where they talk about a similar issue : [https://github.com/apache/incubator-mxnet/issues/8801](https://github.com/apache/incubator-mxnet/issues/8801).

So maybe try adding this import at the start of `retinaface.py` : `from rcnn.PY_OP import rpn_fpn_ohem3`.

Hope this helps.

_Originally posted by @levotrea in https://github.com/deepinsight/insightface/issues/770#issuecomment-508521365_"
"Hi, I'm a student who studies CV, deeplearning
Your work is so great and we study your work at university
I'm trying to open verification.py but directory error occurs..
This tries to access to your jenkins folder but it says that there's no such directory or files
The file i'm trying to open is ../model/softmax-symbol.json
It would be very thankful when you could answer.. "
"I run the ""test.py"" with replacing trained model, I find the error: ""Cannot find custom operator rpn_fpn_ohem3 "". 
Anyone can explain these?"
"There are only the 1:1 verification test scripts under the /Evaluation/IJB folder, I wonder whether the IJB-C 1:N identification test code will be released?

Thank you!"
""
"src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)

And setting the MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 is not working, any clues?"
There are multiple syntax errors in the codebase.  Why is there no automated testing on this repo?
"Hello, thanks for your awesome code.

I'm trying to change the input size of training RetinaFace from square(640x640) to rectangle(320x480 for example) using resnet-50 pretrained model.

I changed the code of **get_sym_conv** function in **rcnn/symbol/symbol_common.py**

from
```
def get_sym_conv(data, sym):
    all_layers = sym.get_internals()

    isize = 640
    _, out_shape, _ = all_layers.infer_shape(data = (1,3,isize,isize))
...
   stride = isize//shape[2]
```
to
```
...
    isize = (320, 480)
    _, out_shape, _ = all_layers.infer_shape(data = (1,3,isize[0],isize[1]))
...
   stride = isize[0]//shape[2]
```
and **get_resnet_conv** function in **rcnn/symbol/symbol_resnet.py**

from
```
def get_resnet_conv(data, sym):
    all_layers = sym.get_internals()
    isize = 640
    _, out_shape, _ = all_layers.infer_shape(data = (1,3,isize,isize))
...
      if c1 is None and shape[2]==isize//16:
...
      if c2 is None and shape[2]==isize//32:
...
      if shape[2]==isize//32:
...
```

to
```
def get_resnet_conv(data, sym):
    all_layers = sym.get_internals()
    isize = (320, 480)
    _, out_shape, _ = all_layers.infer_shape(data = (1,3,isize[0],isize[1]))
...
      if c1 is None and shape[2]==isize[0]//16:
...
      if c2 is None and shape[2]==isize[0]//32:
...
      if shape[2]==isize[0]//32:
...
```
It works when I change the training param **config.SCALES** to (320, 320)

Besides above modification, I was wondering if there is other codes that I need to change?

I tried to change **max_data_shape** param in train_maskrcnn.py, train_rcnn.py, train_rpn.py, test_rcnn.py, test_rpn.py, but I have no idea how to change it. So I just change **config.SCALES** in training param.

What is the relationship between config.SCALES and my input size of network?

How to determine my config.SCALES? For my example, (320, 320) or (480, 480) or other?"
"### Hello guys,
### After everything is set well, I start to run the demo test.py. But met this type of error:
[32, 16, 8] {'32': {'ALLOWED_BORDER': 9999, 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'SCALES': (32, 16)}, '8': {'ALLOWED_BORDER': 9999, 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'SCALES': (2, 1)}, '16': {'ALLOWED_BORDER': 9999, 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'SCALES': (8, 4)}}
[14:02:42] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:02:42] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
means [0. 0. 0.]
use_landmarks False
sym size: 1
Traceback (most recent call last):
  File ""./insightface-master/RetinaFace/test.py"", line 15, in <module>
    detector = RetinaFace('./model/imagenet-resnet-152/resnet-152', 0, gpuid, 'net3')
  File ""./insightface-master/RetinaFace/retinaface.py"", line 139, in __init__
    self.model.set_params(arg_params, aux_params)
  File ""~/.virtualenvs/mxnet_py2/local/lib/python2.7/site-packages/mxnet/module/module.py"", line 350, in set_params
    allow_extra=allow_extra)
  File ""~/.virtualenvs/mxnet_py2/local/lib/python2.7/site-packages/mxnet/module/module.py"", line 309, in init_params
    _impl(desc, arr, arg_params)
  File ""~/.virtualenvs/mxnet_py2/local/lib/python2.7/site-packages/mxnet/module/module.py"", line 300, in _impl
    raise RuntimeError(""%s is not presented"" % name)
RuntimeError: softmax_label is not presented


### I found solutions on some webpages like:
Adding `arg_params['softmax_label'] = mx.nd.array([0])` before `self.model.set_params(arg_params, aux_params)`


### After doing that another error occurs:
[32, 16, 8] {'32': {'ALLOWED_BORDER': 9999, 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'SCALES': (32, 16)}, '8': {'ALLOWED_BORDER': 9999, 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'SCALES': (2, 1)}, '16': {'ALLOWED_BORDER': 9999, 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'SCALES': (8, 4)}}
[14:03:23] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[14:03:23] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
means [0. 0. 0.]
use_landmarks False
sym size: 1
(1104, 736, 3)
('im_scale', 1.391304347826087)
[14:03:24] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
Traceback (most recent call last):
  File ""/home/zhaocy/chenyang/insightface_chenyang/insightface-master/RetinaFace/test.py"", line 37, in <module>
    faces, landmarks = detector.detect(img, thresh, scales=scales, do_flip=flip)
  File ""/home/zhaocy/chenyang/insightface_chenyang/insightface-master/RetinaFace/retinaface.py"", line 229, in detect
    scores = scores[:, self._num_anchors['stride%s'%s]:, :, :]
IndexError: too many indices for array

### I found that the shape of scores is 1*11586 at that point which may lead this error.
### Anyone can explain these?"
"When I run `/gender-age/test.py` I get an error  
```
Traceback (most recent call last):
  File ""/home/zbh/Projects/insightface/gender-age/test.py"", line 1, in <module>
    import face_model
  File ""/home/zbh/Projects/insightface/gender-age/face_model.py"", line 21, in <module>
    import face_preprocess
  File ""/home/zbh/Projects/insightface/gender-age/../src/common/face_preprocess.py"", line 4, in <module>
    from skimage import transform as trans
  File ""/home/zbh/Anaconda3/envs/age/lib/python2.7/site-packages/skimage/__init__.py"", line 167, in <module>
    from .util.dtype import (img_as_float32,
  File ""/home/zbh/Anaconda3/envs/age/lib/python2.7/site-packages/skimage/util/__init__.py"", line 6, in <module>
    from .apply_parallel import apply_parallel
  File ""/home/zbh/Anaconda3/envs/age/lib/python2.7/site-packages/skimage/util/apply_parallel.py"", line 8, in <module>
    import dask.array as da
  File ""/home/zbh/Anaconda3/envs/age/lib/python2.7/site-packages/dask/array/__init__.py"", line 5, in <module>
    from .core import (Array, block, concatenate, stack, from_array, store,
  File ""/home/zbh/Anaconda3/envs/age/lib/python2.7/site-packages/dask/array/core.py"", line 29, in <module>
    from . import chunk
  File ""/home/zbh/Anaconda3/envs/age/lib/python2.7/site-packages/dask/array/chunk.py"", line 73, in <module>
    nancumprod = npcompat.nancumprod
AttributeError: 'module' object has no attribute 'nancumprod'
```

This is my specific version in the virtual environment.    
```
# Name                    Version                   Build  Channel
backports                 1.0                        py_2    anaconda
backports.functools_lru_cache 1.5                        py_2    anaconda
backports_abc             0.5                        py_0    anaconda
blas                      1.0                         mkl    defaults
ca-certificates           2019.6.16            hecc5488_0    conda-forge
cairo                     1.14.12              h8948797_3    defaults
certifi                   2019.3.9                 py27_0    conda-forge
chardet                   3.0.4                    pypi_0    pypi
cloudpickle               1.1.1                      py_0    anaconda
cudatoolkit               9.0                  h13b8566_0    defaults
cycler                    0.10.0                   py27_0    anaconda
cytoolz                   0.9.0.1          py27h14c3975_1    anaconda
dask-core                 1.2.2                      py_0    anaconda
dbus                      1.13.6               h746ee38_0    anaconda
decorator                 4.4.0                    py27_1    anaconda
easydict                  1.9                        py_0    conda-forge
expat                     2.2.6                he6710b0_0    anaconda
fontconfig                2.13.0               h9420a91_0    defaults
freetype                  2.9.1                h8a8886c_1    defaults
functools32               3.2.3.2                  py27_1    anaconda
futures                   3.2.0                    py27_0    anaconda
glib                      2.56.2               hd408876_0    defaults
gst-plugins-base          1.14.0               hbbd80ab_1    anaconda
gstreamer                 1.14.0               hb453b48_1    anaconda
harfbuzz                  0.9.39                        1    defaults
hdf5                      1.8.17                        2    defaults
icu                       58.2                 h9c2bf20_1    defaults
idna                      2.8                      pypi_0    pypi
imageio                   2.5.0                    py27_0    anaconda
intel-openmp              2019.4                      243    defaults
jpeg                      9b                   h024ee3a_2    defaults
kiwisolver                1.1.0            py27he6710b0_0    anaconda
libedit                   3.1.20181209         hc058e9b_0    defaults
libffi                    3.2.1                hd88cf55_4    defaults
libgcc-ng                 9.1.0                hdf63c60_0    defaults
libgfortran-ng            7.3.0                hdf63c60_0    defaults
libpng                    1.6.37               hbc83047_0    defaults
libstdcxx-ng              9.1.0                hdf63c60_0    defaults
libtiff                   4.0.10               h2733197_2    defaults
libuuid                   1.0.3                h1bed415_2    defaults
libxcb                    1.13                 h1bed415_1    defaults
libxml2                   2.9.9                he19cac6_0    defaults
matplotlib                2.2.3            py27hb69df0a_0    anaconda
matplotlib-base           2.2.3            py27h60b886d_1    conda-forge
mkl                       2019.4                      243    defaults
mxnet-cu90                1.4.1                    pypi_0    pypi
ncurses                   6.1                  he6710b0_1    defaults
networkx                  2.2                      py27_1    anaconda
numpy                     1.14.6                   pypi_0    pypi
olefile                   0.46                     py27_0    anaconda
opencv3                   3.2.0               np111py27_0    menpo
openssl                   1.1.1b               h14c3975_1    conda-forge
pcre                      8.43                 he6710b0_0    defaults
pillow                    6.0.0            py27h34e0f95_0    anaconda
pip                       19.1.1                   py27_0    defaults
pixman                    0.38.0               h7b6447c_0    defaults
pyparsing                 2.4.0                      py_0    anaconda
pyqt                      5.9.2            py27h22d08a2_1    anaconda
python                    2.7.16               h9bab390_0    defaults
python-dateutil           2.8.0                    py27_0    anaconda
python-graphviz           0.8.4                    pypi_0    pypi
pytz                      2019.1                     py_0    anaconda
pywavelets                1.0.3            py27hdd07704_1    anaconda
qt                        5.9.7                h5867ecd_1    anaconda
readline                  7.0                  h7b6447c_5    defaults
requests                  2.22.0                   pypi_0    pypi
scikit-image              0.14.2           py27hf484d3e_1    conda-forge
scikit-learn              0.20.3           py27hd81dba3_0    defaults
scipy                     1.2.1            py27h7c811a0_0    defaults
setuptools                41.0.1                   py27_0    defaults
singledispatch            3.4.0.3                  py27_0    anaconda
sip                       4.19.13          py27he6710b0_0    anaconda
six                       1.12.0                   py27_0    anaconda
sqlite                    3.28.0               h7b6447c_0    defaults
subprocess32              3.5.4            py27h7b6447c_0    anaconda
tk                        8.6.9             hed695b0_1002    conda-forge
toolz                     0.9.0                    py27_0    anaconda
tornado                   5.1.1            py27h7b6447c_0    anaconda
urllib3                   1.25.3                   pypi_0    pypi
wheel                     0.33.4                   py27_0    defaults
xz                        5.2.4                h14c3975_4    defaults
zlib                      1.2.11               h7b6447c_3    defaults
zstd                      1.3.7                h0b5b093_0    defaults
```

Does anyone know the version that makes it run?

@nttstar  look forward to your reply"
"I tryed to train mobilefacenet with [mobilenetv3](https://github.com/xwu6614555/MobileNetV3-Mxnet), but got very worse result as bellow:

INFO:root:Epoch[0] Batch [0-20]	Speed: 1383.73 samples/sec	acc=0.000000	lossvalue=45.870661
INFO:root:Epoch[0] Batch [20-40]	Speed: 1478.24 samples/sec	acc=0.000000	lossvalue=45.791344
INFO:root:Epoch[0] Batch [40-60]	Speed: 1529.15 samples/sec	acc=0.000000	lossvalue=45.693285
INFO:root:Epoch[0] Batch [60-80]	Speed: 1486.48 samples/sec	acc=0.000000	lossvalue=45.641450
INFO:root:Epoch[0] Batch [80-100]	Speed: 1319.35 samples/sec	acc=0.000000	lossvalue=45.618647
INFO:root:Epoch[0] Batch [100-120]	Speed: 1443.19 samples/sec	acc=0.000000	lossvalue=45.513809
INFO:root:Epoch[0] Batch [120-140]	Speed: 1423.21 samples/sec	acc=0.000000	lossvalue=45.537901
INFO:root:Epoch[0] Batch [140-160]	Speed: 1412.56 samples/sec	acc=0.000000	lossvalue=45.422717
INFO:root:Epoch[0] Batch [160-180]	Speed: 1368.57 samples/sec	acc=0.000000	lossvalue=45.396835
INFO:root:Epoch[0] Batch [180-200]	Speed: 1403.32 samples/sec	acc=0.000000	lossvalue=45.292634
INFO:root:Epoch[0] Batch [200-220]	Speed: 1396.86 samples/sec	acc=0.000000	lossvalue=45.342935
INFO:root:Epoch[0] Batch [220-240]	Speed: 1398.27 samples/sec	acc=0.000000	lossvalue=45.262554
INFO:root:Epoch[0] Batch [240-260]	Speed: 1414.64 samples/sec	acc=0.000000	lossvalue=45.187489
INFO:root:Epoch[0] Batch [260-280]	Speed: 1410.47 samples/sec	acc=0.000000	lossvalue=45.190232
INFO:root:Epoch[0] Batch [280-300]	Speed: 1422.95 samples/sec	acc=0.000000	lossvalue=45.187069
INFO:root:Epoch[0] Batch [300-320]	Speed: 1353.78 samples/sec	acc=0.000000	lossvalue=45.118768
INFO:root:Epoch[0] Batch [320-340]	Speed: 1449.97 samples/sec	acc=0.000000	lossvalue=45.086011
INFO:root:Epoch[0] Batch [340-360]	Speed: 1469.77 samples/sec	acc=0.000000	lossvalue=45.082504
INFO:root:Epoch[0] Batch [360-380]	Speed: 1439.81 samples/sec	acc=0.000000	lossvalue=45.032984
INFO:root:Epoch[0] Batch [380-400]	Speed: 1420.65 samples/sec	acc=0.000000	lossvalue=44.992775
INFO:root:Epoch[0] Batch [400-420]	Speed: 1443.83 samples/sec	acc=0.000000	lossvalue=44.951703
INFO:root:Epoch[0] Batch [420-440]	Speed: 1448.53 samples/sec	acc=0.000000	lossvalue=44.939882
INFO:root:Epoch[0] Batch [440-460]	Speed: 1398.47 samples/sec	acc=0.000000	lossvalue=44.914947
INFO:root:Epoch[0] Batch [460-480]	Speed: 1302.95 samples/sec	acc=0.000000	lossvalue=44.818283
INFO:root:Epoch[0] Batch [480-500]	Speed: 1448.03 samples/sec	acc=0.000000	lossvalue=44.843665
INFO:root:Epoch[0] Batch [500-520]	Speed: 1446.03 samples/sec	acc=0.000000	lossvalue=44.770267
INFO:root:Epoch[0] Batch [520-540]	Speed: 1407.06 samples/sec	acc=0.000000	lossvalue=44.787757
INFO:root:Epoch[0] Batch [540-560]	Speed: 1410.15 samples/sec	acc=0.000000	lossvalue=44.759510
INFO:root:Epoch[0] Batch [560-580]	Speed: 1414.18 samples/sec	acc=0.000000	lossvalue=44.713844
INFO:root:Epoch[0] Batch [580-600]	Speed: 1397.30 samples/sec	acc=0.000000	lossvalue=44.694967
INFO:root:Epoch[0] Batch [600-620]	Speed: 1432.31 samples/sec	acc=0.000000	lossvalue=44.672683
INFO:root:Epoch[0] Batch [620-640]	Speed: 1428.93 samples/sec	acc=0.000000	lossvalue=44.670642
INFO:root:Epoch[0] Batch [640-660]	Speed: 1398.66 samples/sec	acc=0.000000	lossvalue=44.618105
INFO:root:Epoch[0] Batch [660-680]	Speed: 1450.98 samples/sec	acc=0.000000	lossvalue=44.593677
INFO:root:Epoch[0] Batch [680-700]	Speed: 1445.77 samples/sec	acc=0.000000	lossvalue=44.566131
INFO:root:Epoch[0] Batch [700-720]	Speed: 1443.75 samples/sec	acc=0.000000	lossvalue=44.497654
INFO:root:Epoch[0] Batch [720-740]	Speed: 1429.67 samples/sec	acc=0.000000	lossvalue=44.470155
INFO:root:Epoch[0] Batch [740-760]	Speed: 1442.88 samples/sec	acc=0.000000	lossvalue=44.450265
INFO:root:Epoch[0] Batch [760-780]	Speed: 1444.47 samples/sec	acc=0.000000	lossvalue=44.425858
INFO:root:Epoch[0] Batch [780-800]	Speed: 1438.24 samples/sec	acc=0.000000	lossvalue=44.400564
INFO:root:Epoch[0] Batch [800-820]	Speed: 1435.24 samples/sec	acc=0.000000	lossvalue=44.400841
INFO:root:Epoch[0] Batch [820-840]	Speed: 1419.75 samples/sec	acc=0.000000	lossvalue=44.402317
INFO:root:Epoch[0] Batch [840-860]	Speed: 1436.97 samples/sec	acc=0.000000	lossvalue=44.341323
INFO:root:Epoch[0] Batch [860-880]	Speed: 1414.36 samples/sec	acc=0.000000	lossvalue=44.396828
INFO:root:Epoch[0] Batch [880-900]	Speed: 1430.75 samples/sec	acc=0.000000	lossvalue=44.332490
INFO:root:Epoch[0] Batch [900-920]	Speed: 1417.48 samples/sec	acc=0.000000	lossvalue=44.267582
INFO:root:Epoch[0] Batch [920-940]	Speed: 1432.93 samples/sec	acc=0.000000	lossvalue=44.277461
INFO:root:Epoch[0] Batch [940-960]	Speed: 1415.93 samples/sec	acc=0.000000	lossvalue=44.248853
INFO:root:Epoch[0] Batch [960-980]	Speed: 1417.95 samples/sec	acc=0.000000	lossvalue=44.262166
lr-batch-epoch: 0.01 999 0
INFO:root:Epoch[0] Batch [980-1000]	Speed: 1331.41 samples/sec	acc=0.000000	lossvalue=44.207125
INFO:root:Epoch[0] Batch [1000-1020]	Speed: 1421.38 samples/sec	acc=0.000000	lossvalue=44.183258
INFO:root:Epoch[0] Batch [1020-1040]	Speed: 1436.71 samples/sec	acc=0.000000	lossvalue=44.153931
INFO:root:Epoch[0] Batch [1040-1060]	Speed: 1440.20 samples/sec	acc=0.000000	lossvalue=44.121399
INFO:root:Epoch[0] Batch [1060-1080]	Speed: 1409.65 samples/sec	acc=0.000000	lossvalue=44.097820
INFO:root:Epoch[0] Batch [1080-1100]	Speed: 1393.39 samples/sec	acc=0.000000	lossvalue=44.112846
INFO:root:Epoch[0] Batch [1100-1120]	Speed: 1451.69 samples/sec	acc=0.000000	lossvalue=44.098074
INFO:root:Epoch[0] Batch [1120-1140]	Speed: 1444.11 samples/sec	acc=0.000000	lossvalue=44.076516
INFO:root:Epoch[0] Batch [1140-1160]	Speed: 1482.61 samples/sec	acc=0.000000	lossvalue=43.990565
INFO:root:Epoch[0] Batch [1160-1180]	Speed: 1465.92 samples/sec	acc=0.000000	lossvalue=44.019992
INFO:root:Epoch[0] Batch [1180-1200]	Speed: 1442.89 samples/sec	acc=0.000000	lossvalue=44.019296
INFO:root:Epoch[0] Batch [1200-1220]	Speed: 1450.66 samples/sec	acc=0.000000	lossvalue=43.938944
INFO:root:Epoch[0] Batch [1220-1240]	Speed: 1453.08 samples/sec	acc=0.000000	lossvalue=43.946642
INFO:root:Epoch[0] Batch [1240-1260]	Speed: 1416.95 samples/sec	acc=0.000000	lossvalue=43.960813
INFO:root:Epoch[0] Batch [1260-1280]	Speed: 1438.48 samples/sec	acc=0.000000	lossvalue=43.953425
INFO:root:Epoch[0] Batch [1280-1300]	Speed: 1449.19 samples/sec	acc=0.000000	lossvalue=43.938370
INFO:root:Epoch[0] Batch [1300-1320]	Speed: 1434.35 samples/sec	acc=0.000000	lossvalue=43.881000
INFO:root:Epoch[0] Batch [1320-1340]	Speed: 1431.79 samples/sec	acc=0.000000	lossvalue=43.851503
INFO:root:Epoch[0] Batch [1340-1360]	Speed: 1440.55 samples/sec	acc=0.000000	lossvalue=43.858247
INFO:root:Epoch[0] Batch [1360-1380]	Speed: 1429.69 samples/sec	acc=0.000000	lossvalue=43.866306
INFO:root:Epoch[0] Batch [1380-1400]	Speed: 1431.58 samples/sec	acc=0.000000	lossvalue=43.823151
INFO:root:Epoch[0] Batch [1400-1420]	Speed: 1469.46 samples/sec	acc=0.000000	lossvalue=43.824927
INFO:root:Epoch[0] Batch [1420-1440]	Speed: 1457.58 samples/sec	acc=0.000000	lossvalue=43.790226
INFO:root:Epoch[0] Batch [1440-1460]	Speed: 1438.75 samples/sec	acc=0.000000	lossvalue=43.781603
INFO:root:Epoch[0] Batch [1460-1480]	Speed: 1426.49 samples/sec	acc=0.000000	lossvalue=43.761189
INFO:root:Epoch[0] Batch [1480-1500]	Speed: 1436.99 samples/sec	acc=0.000000	lossvalue=43.762128
INFO:root:Epoch[0] Batch [1500-1520]	Speed: 1441.87 samples/sec	acc=0.000000	lossvalue=43.748407
INFO:root:Epoch[0] Batch [1520-1540]	Speed: 1429.04 samples/sec	acc=0.000000	lossvalue=43.719254
INFO:root:Epoch[0] Batch [1540-1560]	Speed: 1428.21 samples/sec	acc=0.000000	lossvalue=43.722699
INFO:root:Epoch[0] Batch [1560-1580]	Speed: 1336.43 samples/sec	acc=0.000000	lossvalue=43.743402
INFO:root:Epoch[0] Batch [1580-1600]	Speed: 1458.24 samples/sec	acc=0.000000	lossvalue=43.732825
INFO:root:Epoch[0] Batch [1600-1620]	Speed: 1424.19 samples/sec	acc=0.000000	lossvalue=43.661644
INFO:root:Epoch[0] Batch [1620-1640]	Speed: 1449.55 samples/sec	acc=0.000000	lossvalue=43.642130
INFO:root:Epoch[0] Batch [1640-1660]	Speed: 1485.67 samples/sec	acc=0.000000	lossvalue=43.653942
INFO:root:Epoch[0] Batch [1660-1680]	Speed: 1438.97 samples/sec	acc=0.000000	lossvalue=43.629233
INFO:root:Epoch[0] Batch [1680-1700]	Speed: 1462.65 samples/sec	acc=0.000000	lossvalue=43.580198
INFO:root:Epoch[0] Batch [1700-1720]	Speed: 1416.80 samples/sec	acc=0.000000	lossvalue=43.591219
INFO:root:Epoch[0] Batch [1720-1740]	Speed: 1450.70 samples/sec	acc=0.000000	lossvalue=43.565578
INFO:root:Epoch[0] Batch [1740-1760]	Speed: 1443.97 samples/sec	acc=0.000000	lossvalue=43.544191
INFO:root:Epoch[0] Batch [1760-1780]	Speed: 1431.65 samples/sec	acc=0.000000	lossvalue=43.568670
INFO:root:Epoch[0] Batch [1780-1800]	Speed: 1459.77 samples/sec	acc=0.000000	lossvalue=43.550752
INFO:root:Epoch[0] Batch [1800-1820]	Speed: 1444.76 samples/sec	acc=0.000000	lossvalue=43.517789
INFO:root:Epoch[0] Batch [1820-1840]	Speed: 1446.74 samples/sec	acc=0.000000	lossvalue=43.470806
INFO:root:Epoch[0] Batch [1840-1860]	Speed: 1446.13 samples/sec	acc=0.000000	lossvalue=43.482472
INFO:root:Epoch[0] Batch [1860-1880]	Speed: 1437.97 samples/sec	acc=0.000000	lossvalue=43.475338
INFO:root:Epoch[0] Batch [1880-1900]	Speed: 1465.48 samples/sec	acc=0.000000	lossvalue=43.460360
INFO:root:Epoch[0] Batch [1900-1920]	Speed: 1432.71 samples/sec	acc=0.000000	lossvalue=43.460346
INFO:root:Epoch[0] Batch [1920-1940]	Speed: 1393.56 samples/sec	acc=0.000000	lossvalue=43.419805
INFO:root:Epoch[0] Batch [1940-1960]	Speed: 1433.58 samples/sec	acc=0.000000	lossvalue=43.436627
INFO:root:Epoch[0] Batch [1960-1980]	Speed: 1412.13 samples/sec	acc=0.000000	lossvalue=43.436714
lr-batch-epoch: 0.01 1999 0
testing verification..
(12000, 512)
infer time 4.767922
[agedb_30][2000]XNorm: 5.186779
[agedb_30][2000]Accuracy-Flip: 0.56133+-0.01785
testing verification..
(12000, 512)
infer time 4.882558
[calfw][2000]XNorm: 5.326550
[calfw][2000]Accuracy-Flip: 0.50433+-0.01352
testing verification..
(14000, 512)
infer time 5.835709
[cfp_ff][2000]XNorm: 24.351639
[cfp_ff][2000]Accuracy-Flip: 0.64257+-0.02092
testing verification..
(14000, 512)
infer time 6.510527
[cfp_fp][2000]XNorm: 21.560500
[cfp_fp][2000]Accuracy-Flip: 0.60300+-0.02718
testing verification..
(12000, 512)
infer time 5.20483
[cplfw][2000]XNorm: 8.356117
[cplfw][2000]Accuracy-Flip: 0.50817+-0.01463
testing verification..
(12000, 512)
infer time 5.349741
[lfw][2000]XNorm: 3.610347
[lfw][2000]Accuracy-Flip: 0.52283+-0.02500
saving 1
INFO:root:Saved checkpoint to ""./models/m3-arcface-emore/model-0001.params""
[2000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [1980-2000]	Speed: 170.20 samples/sec	acc=0.000000	lossvalue=43.404378
INFO:root:Epoch[0] Batch [2000-2020]	Speed: 1397.33 samples/sec	acc=0.000000	lossvalue=43.363487
INFO:root:Epoch[0] Batch [2020-2040]	Speed: 1397.93 samples/sec	acc=0.000000	lossvalue=43.367393
INFO:root:Epoch[0] Batch [2040-2060]	Speed: 1376.78 samples/sec	acc=0.000000	lossvalue=43.385328
INFO:root:Epoch[0] Batch [2060-2080]	Speed: 1391.53 samples/sec	acc=0.000000	lossvalue=43.348934
INFO:root:Epoch[0] Batch [2080-2100]	Speed: 1403.09 samples/sec	acc=0.000000	lossvalue=43.293539
INFO:root:Epoch[0] Batch [2100-2120]	Speed: 1359.96 samples/sec	acc=0.000000	lossvalue=43.275719
INFO:root:Epoch[0] Batch [2120-2140]	Speed: 1335.08 samples/sec	acc=0.000000	lossvalue=43.333215
INFO:root:Epoch[0] Batch [2140-2160]	Speed: 1447.66 samples/sec	acc=0.000000	lossvalue=43.351860
INFO:root:Epoch[0] Batch [2160-2180]	Speed: 1438.23 samples/sec	acc=0.000000	lossvalue=43.268195
INFO:root:Epoch[0] Batch [2180-2200]	Speed: 1302.31 samples/sec	acc=0.000000	lossvalue=43.260030
INFO:root:Epoch[0] Batch [2200-2220]	Speed: 1428.12 samples/sec	acc=0.000000	lossvalue=43.272679
INFO:root:Epoch[0] Batch [2220-2240]	Speed: 1384.74 samples/sec	acc=0.000000	lossvalue=43.243468
INFO:root:Epoch[0] Batch [2240-2260]	Speed: 1449.10 samples/sec	acc=0.000000	lossvalue=43.249479
INFO:root:Epoch[0] Batch [2260-2280]	Speed: 1418.01 samples/sec	acc=0.000000	lossvalue=43.220001
INFO:root:Epoch[0] Batch [2280-2300]	Speed: 1416.75 samples/sec	acc=0.000000	lossvalue=43.239038
INFO:root:Epoch[0] Batch [2300-2320]	Speed: 1452.63 samples/sec	acc=0.000000	lossvalue=43.182532
INFO:root:Epoch[0] Batch [2320-2340]	Speed: 1425.29 samples/sec	acc=0.000000	lossvalue=43.180995
INFO:root:Epoch[0] Batch [2340-2360]	Speed: 1440.57 samples/sec	acc=0.000000	lossvalue=43.188975
INFO:root:Epoch[0] Batch [2360-2380]	Speed: 1434.21 samples/sec	acc=0.000000	lossvalue=43.196234
INFO:root:Epoch[0] Batch [2380-2400]	Speed: 1423.04 samples/sec	acc=0.000000	lossvalue=43.177007
INFO:root:Epoch[0] Batch [2400-2420]	Speed: 1407.31 samples/sec	acc=0.000000	lossvalue=43.145509
INFO:root:Epoch[0] Batch [2420-2440]	Speed: 1417.49 samples/sec	acc=0.000000	lossvalue=43.158761
INFO:root:Epoch[0] Batch [2440-2460]	Speed: 1426.18 samples/sec	acc=0.000000	lossvalue=43.141473
INFO:root:Epoch[0] Batch [2460-2480]	Speed: 1435.73 samples/sec	acc=0.000000	lossvalue=43.126241
INFO:root:Epoch[0] Batch [2480-2500]	Speed: 1432.84 samples/sec	acc=0.000000	lossvalue=43.093768
INFO:root:Epoch[0] Batch [2500-2520]	Speed: 1422.08 samples/sec	acc=0.000000	lossvalue=43.118095
INFO:root:Epoch[0] Batch [2520-2540]	Speed: 1454.36 samples/sec	acc=0.000000	lossvalue=43.107464
INFO:root:Epoch[0] Batch [2540-2560]	Speed: 1429.15 samples/sec	acc=0.000000	lossvalue=43.098206
INFO:root:Epoch[0] Batch [2560-2580]	Speed: 1425.54 samples/sec	acc=0.000000	lossvalue=43.083347
INFO:root:Epoch[0] Batch [2580-2600]	Speed: 1455.07 samples/sec	acc=0.000000	lossvalue=43.080341
INFO:root:Epoch[0] Batch [2600-2620]	Speed: 1451.78 samples/sec	acc=0.000000	lossvalue=43.062382
INFO:root:Epoch[0] Batch [2620-2640]	Speed: 1443.80 samples/sec	acc=0.000000	lossvalue=43.049131
INFO:root:Epoch[0] Batch [2640-2660]	Speed: 1406.09 samples/sec	acc=0.000000	lossvalue=43.024845
INFO:root:Epoch[0] Batch [2660-2680]	Speed: 1452.47 samples/sec	acc=0.000000	lossvalue=43.027570
INFO:root:Epoch[0] Batch [2680-2700]	Speed: 1407.33 samples/sec	acc=0.000000	lossvalue=43.035068
INFO:root:Epoch[0] Batch [2700-2720]	Speed: 1433.38 samples/sec	acc=0.000000	lossvalue=43.018409
INFO:root:Epoch[0] Batch [2720-2740]	Speed: 1445.41 samples/sec	acc=0.000000	lossvalue=43.000950
INFO:root:Epoch[0] Batch [2740-2760]	Speed: 1476.70 samples/sec	acc=0.000000	lossvalue=43.001233
INFO:root:Epoch[0] Batch [2760-2780]	Speed: 1418.75 samples/sec	acc=0.000000	lossvalue=42.993690
INFO:root:Epoch[0] Batch [2780-2800]	Speed: 1439.07 samples/sec	acc=0.000000	lossvalue=42.977153
INFO:root:Epoch[0] Batch [2800-2820]	Speed: 1406.74 samples/sec	acc=0.000000	lossvalue=42.964536
INFO:root:Epoch[0] Batch [2820-2840]	Speed: 1308.39 samples/sec	acc=0.000000	lossvalue=42.955763
INFO:root:Epoch[0] Batch [2840-2860]	Speed: 1418.14 samples/sec	acc=0.000000	lossvalue=42.965248
INFO:root:Epoch[0] Batch [2860-2880]	Speed: 1420.41 samples/sec	acc=0.000000	lossvalue=42.951956
INFO:root:Epoch[0] Batch [2880-2900]	Speed: 1416.45 samples/sec	acc=0.000000	lossvalue=42.900782
INFO:root:Epoch[0] Batch [2900-2920]	Speed: 1437.10 samples/sec	acc=0.000000	lossvalue=42.941202
INFO:root:Epoch[0] Batch [2920-2940]	Speed: 1427.38 samples/sec	acc=0.000000	lossvalue=42.905976
INFO:root:Epoch[0] Batch [2940-2960]	Speed: 1423.14 samples/sec	acc=0.000000	lossvalue=42.923396
INFO:root:Epoch[0] Batch [2960-2980]	Speed: 1432.20 samples/sec	acc=0.000000	lossvalue=42.925973
lr-batch-epoch: 0.01 2999 0
INFO:root:Epoch[0] Batch [2980-3000]	Speed: 1417.04 samples/sec	acc=0.000000	lossvalue=42.919900
INFO:root:Epoch[0] Batch [3000-3020]	Speed: 1436.58 samples/sec	acc=0.000000	lossvalue=42.874327
INFO:root:Epoch[0] Batch [3020-3040]	Speed: 1416.77 samples/sec	acc=0.000000	lossvalue=42.899130
INFO:root:Epoch[0] Batch [3040-3060]	Speed: 1426.80 samples/sec	acc=0.000000	lossvalue=42.854612
INFO:root:Epoch[0] Batch [3060-3080]	Speed: 1407.18 samples/sec	acc=0.000000	lossvalue=42.905168
INFO:root:Epoch[0] Batch [3080-3100]	Speed: 1414.27 samples/sec	acc=0.000000	lossvalue=42.866004
INFO:root:Epoch[0] Batch [3100-3120]	Speed: 1429.14 samples/sec	acc=0.000000	lossvalue=42.863892
INFO:root:Epoch[0] Batch [3120-3140]	Speed: 1426.98 samples/sec	acc=0.000000	lossvalue=42.844164
INFO:root:Epoch[0] Batch [3140-3160]	Speed: 1429.76 samples/sec	acc=0.000000	lossvalue=42.820837
INFO:root:Epoch[0] Batch [3160-3180]	Speed: 1434.44 samples/sec	acc=0.000000	lossvalue=42.846137
INFO:root:Epoch[0] Batch [3180-3200]	Speed: 1427.84 samples/sec	acc=0.000000	lossvalue=42.812166
INFO:root:Epoch[0] Batch [3200-3220]	Speed: 1436.46 samples/sec	acc=0.000000	lossvalue=42.801479
INFO:root:Epoch[0] Batch [3220-3240]	Speed: 1444.55 samples/sec	acc=0.000000	lossvalue=42.782662
INFO:root:Epoch[0] Batch [3240-3260]	Speed: 1418.60 samples/sec	acc=0.000000	lossvalue=42.834113
INFO:root:Epoch[0] Batch [3260-3280]	Speed: 1423.13 samples/sec	acc=0.000000	lossvalue=42.797234
INFO:root:Epoch[0] Batch [3280-3300]	Speed: 1453.12 samples/sec	acc=0.000000	lossvalue=42.787975
INFO:root:Epoch[0] Batch [3300-3320]	Speed: 1449.47 samples/sec	acc=0.000000	lossvalue=42.788614
INFO:root:Epoch[0] Batch [3320-3340]	Speed: 1428.52 samples/sec	acc=0.000000	lossvalue=42.770773
INFO:root:Epoch[0] Batch [3340-3360]	Speed: 1427.01 samples/sec	acc=0.000000	lossvalue=42.791105
INFO:root:Epoch[0] Batch [3360-3380]	Speed: 1414.08 samples/sec	acc=0.000000	lossvalue=42.767718
INFO:root:Epoch[0] Batch [3380-3400]	Speed: 1426.79 samples/sec	acc=0.000000	lossvalue=42.723938
INFO:root:Epoch[0] Batch [3400-3420]	Speed: 1428.09 samples/sec	acc=0.000000	lossvalue=42.720111
INFO:root:Epoch[0] Batch [3420-3440]	Speed: 1426.12 samples/sec	acc=0.000000	lossvalue=42.728325
INFO:root:Epoch[0] Batch [3440-3460]	Speed: 1321.42 samples/sec	acc=0.000000	lossvalue=42.722214
INFO:root:Epoch[0] Batch [3460-3480]	Speed: 1430.87 samples/sec	acc=0.000000	lossvalue=42.717883
INFO:root:Epoch[0] Batch [3480-3500]	Speed: 1423.54 samples/sec	acc=0.000000	lossvalue=42.723384
INFO:root:Epoch[0] Batch [3500-3520]	Speed: 1442.60 samples/sec	acc=0.000000	lossvalue=42.691322
INFO:root:Epoch[0] Batch [3520-3540]	Speed: 1427.28 samples/sec	acc=0.000000	lossvalue=42.682272
INFO:root:Epoch[0] Batch [3540-3560]	Speed: 1432.55 samples/sec	acc=0.000000	lossvalue=42.702149
INFO:root:Epoch[0] Batch [3560-3580]	Speed: 1425.38 samples/sec	acc=0.000000	lossvalue=42.702578
INFO:root:Epoch[0] Batch [3580-3600]	Speed: 1437.43 samples/sec	acc=0.000000	lossvalue=42.698026
INFO:root:Epoch[0] Batch [3600-3620]	Speed: 1400.61 samples/sec	acc=0.000000	lossvalue=42.677364
INFO:root:Epoch[0] Batch [3620-3640]	Speed: 1442.46 samples/sec	acc=0.000000	lossvalue=42.661396
INFO:root:Epoch[0] Batch [3640-3660]	Speed: 1437.35 samples/sec	acc=0.000000	lossvalue=42.672045
INFO:root:Epoch[0] Batch [3660-3680]	Speed: 1434.27 samples/sec	acc=0.000000	lossvalue=42.653893
INFO:root:Epoch[0] Batch [3680-3700]	Speed: 1416.24 samples/sec	acc=0.000000	lossvalue=42.647855
INFO:root:Epoch[0] Batch [3700-3720]	Speed: 1434.07 samples/sec	acc=0.000000	lossvalue=42.636478
INFO:root:Epoch[0] Batch [3720-3740]	Speed: 1451.74 samples/sec	acc=0.000000	lossvalue=42.657539
INFO:root:Epoch[0] Batch [3740-3760]	Speed: 1410.02 samples/sec	acc=0.000000	lossvalue=42.635114
INFO:root:Epoch[0] Batch [3760-3780]	Speed: 1439.91 samples/sec	acc=0.000000	lossvalue=42.629244
INFO:root:Epoch[0] Batch [3780-3800]	Speed: 1418.50 samples/sec	acc=0.000000	lossvalue=42.636886
INFO:root:Epoch[0] Batch [3800-3820]	Speed: 1404.63 samples/sec	acc=0.000000	lossvalue=42.611131
INFO:root:Epoch[0] Batch [3820-3840]	Speed: 1428.15 samples/sec	acc=0.000000	lossvalue=42.616557
INFO:root:Epoch[0] Batch [3840-3860]	Speed: 1406.44 samples/sec	acc=0.000000	lossvalue=42.608893
INFO:root:Epoch[0] Batch [3860-3880]	Speed: 1448.66 samples/sec	acc=0.000000	lossvalue=42.591905
INFO:root:Epoch[0] Batch [3880-3900]	Speed: 1437.10 samples/sec	acc=0.000000	lossvalue=42.607033
INFO:root:Epoch[0] Batch [3900-3920]	Speed: 1434.37 samples/sec	acc=0.000000	lossvalue=42.613058
INFO:root:Epoch[0] Batch [3920-3940]	Speed: 1468.91 samples/sec	acc=0.000000	lossvalue=42.591647
INFO:root:Epoch[0] Batch [3940-3960]	Speed: 1430.64 samples/sec	acc=0.000000	lossvalue=42.566539
INFO:root:Epoch[0] Batch [3960-3980]	Speed: 1442.96 samples/sec	acc=0.000000	lossvalue=42.585085
lr-batch-epoch: 0.01 3999 0
testing verification..
(12000, 512)
infer time 5.147187
[agedb_30][4000]XNorm: 2.132027
[agedb_30][4000]Accuracy-Flip: 0.51183+-0.01613
testing verification..
(12000, 512)
infer time 5.289282
[calfw][4000]XNorm: 3.467386
[calfw][4000]Accuracy-Flip: 0.50417+-0.01009
testing verification..
(14000, 512)
infer time 5.898726
[cfp_ff][4000]XNorm: 30.462818
[cfp_ff][4000]Accuracy-Flip: 0.66143+-0.02125
testing verification..
(14000, 512)
infer time 5.747373
[cfp_fp][4000]XNorm: 24.980808
[cfp_fp][4000]Accuracy-Flip: 0.61129+-0.02457
testing verification..
(12000, 512)
infer time 4.717469
[cplfw][4000]XNorm: 6.243488
[cplfw][4000]Accuracy-Flip: 0.50100+-0.00854
testing verification..
(12000, 512)
infer time 5.074133
[lfw][4000]XNorm: 2.288231
[lfw][4000]Accuracy-Flip: 0.49783+-0.01726
[4000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [3980-4000]	Speed: 175.71 samples/sec	acc=0.000000	lossvalue=42.550051
INFO:root:Epoch[0] Batch [4000-4020]	Speed: 1393.38 samples/sec	acc=0.000000	lossvalue=42.574287
INFO:root:Epoch[0] Batch [4020-4040]	Speed: 1423.98 samples/sec	acc=0.000000	lossvalue=42.553419
INFO:root:Epoch[0] Batch [4040-4060]	Speed: 1394.03 samples/sec	acc=0.000000	lossvalue=42.549029
INFO:root:Epoch[0] Batch [4060-4080]	Speed: 1423.38 samples/sec	acc=0.000000	lossvalue=42.536241
INFO:root:Epoch[0] Batch [4080-4100]	Speed: 1474.36 samples/sec	acc=0.000000	lossvalue=42.540261
INFO:root:Epoch[0] Batch [4100-4120]	Speed: 1302.93 samples/sec	acc=0.000000	lossvalue=42.547042
INFO:root:Epoch[0] Batch [4120-4140]	Speed: 1429.10 samples/sec	acc=0.000000	lossvalue=42.525436
INFO:root:Epoch[0] Batch [4140-4160]	Speed: 1427.53 samples/sec	acc=0.000000	lossvalue=42.532377
INFO:root:Epoch[0] Batch [4160-4180]	Speed: 1383.11 samples/sec	acc=0.000000	lossvalue=42.500257
INFO:root:Epoch[0] Batch [4180-4200]	Speed: 1432.20 samples/sec	acc=0.000000	lossvalue=42.516144
INFO:root:Epoch[0] Batch [4200-4220]	Speed: 1420.97 samples/sec	acc=0.000000	lossvalue=42.509694
INFO:root:Epoch[0] Batch [4220-4240]	Speed: 1421.21 samples/sec	acc=0.000000	lossvalue=42.520906
INFO:root:Epoch[0] Batch [4240-4260]	Speed: 1432.03 samples/sec	acc=0.000000	lossvalue=42.508407
INFO:root:Epoch[0] Batch [4260-4280]	Speed: 1442.09 samples/sec	acc=0.000000	lossvalue=42.490495
INFO:root:Epoch[0] Batch [4280-4300]	Speed: 1439.79 samples/sec	acc=0.000000	lossvalue=42.510910
INFO:root:Epoch[0] Batch [4300-4320]	Speed: 1427.76 samples/sec	acc=0.000000	lossvalue=42.469509
INFO:root:Epoch[0] Batch [4320-4340]	Speed: 1443.76 samples/sec	acc=0.000000	lossvalue=42.482943
INFO:root:Epoch[0] Batch [4340-4360]	Speed: 1442.26 samples/sec	acc=0.000000	lossvalue=42.456209
INFO:root:Epoch[0] Batch [4360-4380]	Speed: 1433.37 samples/sec	acc=0.000000	lossvalue=42.468422
INFO:root:Epoch[0] Batch [4380-4400]	Speed: 1436.84 samples/sec	acc=0.000000	lossvalue=42.462881
INFO:root:Epoch[0] Batch [4400-4420]	Speed: 1446.74 samples/sec	acc=0.000000	lossvalue=42.482299
INFO:root:Epoch[0] Batch [4420-4440]	Speed: 1467.64 samples/sec	acc=0.000000	lossvalue=42.459066
INFO:root:Epoch[0] Batch [4440-4460]	Speed: 1461.00 samples/sec	acc=0.000000	lossvalue=42.462655
INFO:root:Epoch[0] Batch [4460-4480]	Speed: 1415.87 samples/sec	acc=0.000000	lossvalue=42.460793
INFO:root:Epoch[0] Batch [4480-4500]	Speed: 1410.21 samples/sec	acc=0.000000	lossvalue=42.469575
INFO:root:Epoch[0] Batch [4500-4520]	Speed: 1430.43 samples/sec	acc=0.000000	lossvalue=42.429813
INFO:root:Epoch[0] Batch [4520-4540]	Speed: 1437.68 samples/sec	acc=0.000000	lossvalue=42.460910
INFO:root:Epoch[0] Batch [4540-4560]	Speed: 1415.48 samples/sec	acc=0.000000	lossvalue=42.440271
INFO:root:Epoch[0] Batch [4560-4580]	Speed: 1444.00 samples/sec	acc=0.000000	lossvalue=42.425996
INFO:root:Epoch[0] Batch [4580-4600]	Speed: 1445.10 samples/sec	acc=0.000000	lossvalue=42.444479
INFO:root:Epoch[0] Batch [4600-4620]	Speed: 1429.36 samples/sec	acc=0.000000	lossvalue=42.441484
INFO:root:Epoch[0] Batch [4620-4640]	Speed: 1441.15 samples/sec	acc=0.000000	lossvalue=42.407234
INFO:root:Epoch[0] Batch [4640-4660]	Speed: 1455.54 samples/sec	acc=0.000000	lossvalue=42.419878
INFO:root:Epoch[0] Batch [4660-4680]	Speed: 1428.00 samples/sec	acc=0.000000	lossvalue=42.431764
INFO:root:Epoch[0] Batch [4680-4700]	Speed: 1422.00 samples/sec	acc=0.000000	lossvalue=42.408870
INFO:root:Epoch[0] Batch [4700-4720]	Speed: 1419.59 samples/sec	acc=0.000000	lossvalue=42.403528
INFO:root:Epoch[0] Batch [4720-4740]	Speed: 1422.40 samples/sec	acc=0.000000	lossvalue=42.374300
INFO:root:Epoch[0] Batch [4740-4760]	Speed: 1328.92 samples/sec	acc=0.000000	lossvalue=42.394442
INFO:root:Epoch[0] Batch [4760-4780]	Speed: 1454.65 samples/sec	acc=0.000000	lossvalue=42.384118
INFO:root:Epoch[0] Batch [4780-4800]	Speed: 1428.79 samples/sec	acc=0.000000	lossvalue=42.393840
INFO:root:Epoch[0] Batch [4800-4820]	Speed: 1432.17 samples/sec	acc=0.000000	lossvalue=42.386306
INFO:root:Epoch[0] Batch [4820-4840]	Speed: 1430.07 samples/sec	acc=0.000000	lossvalue=42.375308
INFO:root:Epoch[0] Batch [4840-4860]	Speed: 1424.42 samples/sec	acc=0.000000	lossvalue=42.365072
INFO:root:Epoch[0] Batch [4860-4880]	Speed: 1433.53 samples/sec	acc=0.000000	lossvalue=42.370385
INFO:root:Epoch[0] Batch [4880-4900]	Speed: 1423.51 samples/sec	acc=0.000000	lossvalue=42.375198
INFO:root:Epoch[0] Batch [4900-4920]	Speed: 1428.65 samples/sec	acc=0.000000	lossvalue=42.384885
INFO:root:Epoch[0] Batch [4920-4940]	Speed: 1375.04 samples/sec	acc=0.000000	lossvalue=42.364101
INFO:root:Epoch[0] Batch [4940-4960]	Speed: 1418.07 samples/sec	acc=0.000000	lossvalue=42.331564
INFO:root:Epoch[0] Batch [4960-4980]	Speed: 1411.97 samples/sec	acc=0.000000	lossvalue=42.336429
lr-batch-epoch: 0.01 4999 0
INFO:root:Epoch[0] Batch [4980-5000]	Speed: 1383.41 samples/sec	acc=0.000000	lossvalue=42.342953
INFO:root:Epoch[0] Batch [5000-5020]	Speed: 1418.45 samples/sec	acc=0.000000	lossvalue=42.353621
INFO:root:Epoch[0] Batch [5020-5040]	Speed: 1447.26 samples/sec	acc=0.000000	lossvalue=42.346309
INFO:root:Epoch[0] Batch [5040-5060]	Speed: 1432.50 samples/sec	acc=0.000000	lossvalue=42.341695
INFO:root:Epoch[0] Batch [5060-5080]	Speed: 1440.10 samples/sec	acc=0.000000	lossvalue=42.317080
INFO:root:Epoch[0] Batch [5080-5100]	Speed: 1408.21 samples/sec	acc=0.000000	lossvalue=42.341804
INFO:root:Epoch[0] Batch [5100-5120]	Speed: 1439.04 samples/sec	acc=0.000000	lossvalue=42.332448
INFO:root:Epoch[0] Batch [5120-5140]	Speed: 1437.54 samples/sec	acc=0.000000	lossvalue=42.317316
INFO:root:Epoch[0] Batch [5140-5160]	Speed: 1448.88 samples/sec	acc=0.000000	lossvalue=42.329248
INFO:root:Epoch[0] Batch [5160-5180]	Speed: 1424.91 samples/sec	acc=0.000000	lossvalue=42.299067
INFO:root:Epoch[0] Batch [5180-5200]	Speed: 1415.99 samples/sec	acc=0.000000	lossvalue=42.319560
INFO:root:Epoch[0] Batch [5200-5220]	Speed: 1422.27 samples/sec	acc=0.000000	lossvalue=42.309711
INFO:root:Epoch[0] Batch [5220-5240]	Speed: 1427.98 samples/sec	acc=0.000000	lossvalue=42.298055
INFO:root:Epoch[0] Batch [5240-5260]	Speed: 1440.10 samples/sec	acc=0.000000	lossvalue=42.315811
INFO:root:Epoch[0] Batch [5260-5280]	Speed: 1437.36 samples/sec	acc=0.000000	lossvalue=42.300466
INFO:root:Epoch[0] Batch [5280-5300]	Speed: 1412.81 samples/sec	acc=0.000000	lossvalue=42.294637
INFO:root:Epoch[0] Batch [5300-5320]	Speed: 1393.58 samples/sec	acc=0.000000	lossvalue=42.288443
INFO:root:Epoch[0] Batch [5320-5340]	Speed: 1440.76 samples/sec	acc=0.000000	lossvalue=42.292585
INFO:root:Epoch[0] Batch [5340-5360]	Speed: 1420.52 samples/sec	acc=0.000000	lossvalue=42.291715
INFO:root:Epoch[0] Batch [5360-5380]	Speed: 1458.70 samples/sec	acc=0.000000	lossvalue=42.288405
INFO:root:Epoch[0] Batch [5380-5400]	Speed: 1438.32 samples/sec	acc=0.000000	lossvalue=42.278516
INFO:root:Epoch[0] Batch [5400-5420]	Speed: 1439.55 samples/sec	acc=0.000000	lossvalue=42.273524
INFO:root:Epoch[0] Batch [5420-5440]	Speed: 1303.37 samples/sec	acc=0.000000	lossvalue=42.284446
INFO:root:Epoch[0] Batch [5440-5460]	Speed: 1450.37 samples/sec	acc=0.000000	lossvalue=42.264119
INFO:root:Epoch[0] Batch [5460-5480]	Speed: 1416.76 samples/sec	acc=0.000000	lossvalue=42.270426
INFO:root:Epoch[0] Batch [5480-5500]	Speed: 1444.06 samples/sec	acc=0.000000	lossvalue=42.281589
INFO:root:Epoch[0] Batch [5500-5520]	Speed: 1461.23 samples/sec	acc=0.000000	lossvalue=42.248047
INFO:root:Epoch[0] Batch [5520-5540]	Speed: 1419.07 samples/sec	acc=0.000000	lossvalue=42.268784
INFO:root:Epoch[0] Batch [5540-5560]	Speed: 1425.21 samples/sec	acc=0.000000	lossvalue=42.271940
INFO:root:Epoch[0] Batch [5560-5580]	Speed: 1421.12 samples/sec	acc=0.000000	lossvalue=42.264134
INFO:root:Epoch[0] Batch [5580-5600]	Speed: 1430.31 samples/sec	acc=0.000000	lossvalue=42.274274
INFO:root:Epoch[0] Batch [5600-5620]	Speed: 1429.46 samples/sec	acc=0.000000	lossvalue=42.264286
INFO:root:Epoch[0] Batch [5620-5640]	Speed: 1420.31 samples/sec	acc=0.000000	lossvalue=42.248168
INFO:root:Epoch[0] Batch [5640-5660]	Speed: 1445.08 samples/sec	acc=0.000000	lossvalue=42.239975
INFO:root:Epoch[0] Batch [5660-5680]	Speed: 1424.62 samples/sec	acc=0.000000	lossvalue=42.237265
INFO:root:Epoch[0] Batch [5680-5700]	Speed: 1432.27 samples/sec	acc=0.000000	lossvalue=42.241098
INFO:root:Epoch[0] Batch [5700-5720]	Speed: 1400.27 samples/sec	acc=0.000000	lossvalue=42.227811
INFO:root:Epoch[0] Batch [5720-5740]	Speed: 1416.32 samples/sec	acc=0.000000	lossvalue=42.244482
INFO:root:Epoch[0] Batch [5740-5760]	Speed: 1406.91 samples/sec	acc=0.000000	lossvalue=42.220995
INFO:root:Epoch[0] Batch [5760-5780]	Speed: 1413.25 samples/sec	acc=0.000000	lossvalue=42.223339
INFO:root:Epoch[0] Batch [5780-5800]	Speed: 1433.31 samples/sec	acc=0.000000	lossvalue=42.230837
INFO:root:Epoch[0] Batch [5800-5820]	Speed: 1419.77 samples/sec	acc=0.000000	lossvalue=42.208267
INFO:root:Epoch[0] Batch [5820-5840]	Speed: 1434.48 samples/sec	acc=0.000000	lossvalue=42.216342
INFO:root:Epoch[0] Batch [5840-5860]	Speed: 1445.68 samples/sec	acc=0.000000	lossvalue=42.207249
INFO:root:Epoch[0] Batch [5860-5880]	Speed: 1412.29 samples/sec	acc=0.000000	lossvalue=42.231215
INFO:root:Epoch[0] Batch [5880-5900]	Speed: 1489.47 samples/sec	acc=0.000000	lossvalue=42.206901
INFO:root:Epoch[0] Batch [5900-5920]	Speed: 1431.93 samples/sec	acc=0.000000	lossvalue=42.200964
INFO:root:Epoch[0] Batch [5920-5940]	Speed: 1412.74 samples/sec	acc=0.000000	lossvalue=42.202295
INFO:root:Epoch[0] Batch [5940-5960]	Speed: 1399.04 samples/sec	acc=0.000000	lossvalue=42.190144
INFO:root:Epoch[0] Batch [5960-5980]	Speed: 1423.30 samples/sec	acc=0.000000	lossvalue=42.184068
lr-batch-epoch: 0.01 5999 0
testing verification..
(12000, 512)
infer time 5.024747
[agedb_30][6000]XNorm: 1.423212
[agedb_30][6000]Accuracy-Flip: 0.50133+-0.00323
testing verification..
(12000, 512)
infer time 5.24066
[calfw][6000]XNorm: 2.490709
[calfw][6000]Accuracy-Flip: 0.50133+-0.00702
testing verification..
(14000, 512)
infer time 5.529946
[cfp_ff][6000]XNorm: 24.229985
[cfp_ff][6000]Accuracy-Flip: 0.65329+-0.01836
testing verification..
(14000, 512)
infer time 6.060891
[cfp_fp][6000]XNorm: 19.544963
[cfp_fp][6000]Accuracy-Flip: 0.59671+-0.02189
testing verification..
(12000, 512)
infer time 5.416972
[cplfw][6000]XNorm: 5.137669
[cplfw][6000]Accuracy-Flip: 0.50150+-0.00529
testing verification..
(12000, 512)
infer time 4.913089
[lfw][6000]XNorm: 1.665831
[lfw][6000]Accuracy-Flip: 0.50117+-0.01229
[6000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [5980-6000]	Speed: 171.20 samples/sec	acc=0.000000	lossvalue=42.196841
INFO:root:Epoch[0] Batch [6000-6020]	Speed: 1382.05 samples/sec	acc=0.000000	lossvalue=42.192054
INFO:root:Epoch[0] Batch [6020-6040]	Speed: 1452.38 samples/sec	acc=0.000000	lossvalue=42.191133
INFO:root:Epoch[0] Batch [6040-6060]	Speed: 1376.12 samples/sec	acc=0.000000	lossvalue=42.189343
INFO:root:Epoch[0] Batch [6060-6080]	Speed: 1399.28 samples/sec	acc=0.000000	lossvalue=42.204668
INFO:root:Epoch[0] Batch [6080-6100]	Speed: 1326.18 samples/sec	acc=0.000000	lossvalue=42.176307
INFO:root:Epoch[0] Batch [6100-6120]	Speed: 1462.26 samples/sec	acc=0.000000	lossvalue=42.181716
INFO:root:Epoch[0] Batch [6120-6140]	Speed: 1434.85 samples/sec	acc=0.000000	lossvalue=42.196211
INFO:root:Epoch[0] Batch [6140-6160]	Speed: 1455.70 samples/sec	acc=0.000000	lossvalue=42.194986
INFO:root:Epoch[0] Batch [6160-6180]	Speed: 1426.39 samples/sec	acc=0.000000	lossvalue=42.171385
INFO:root:Epoch[0] Batch [6180-6200]	Speed: 1453.21 samples/sec	acc=0.000000	lossvalue=42.172093
INFO:root:Epoch[0] Batch [6200-6220]	Speed: 1408.12 samples/sec	acc=0.000000	lossvalue=42.170577
INFO:root:Epoch[0] Batch [6220-6240]	Speed: 1417.95 samples/sec	acc=0.000000	lossvalue=42.170736
INFO:root:Epoch[0] Batch [6240-6260]	Speed: 1447.37 samples/sec	acc=0.000000	lossvalue=42.161931
INFO:root:Epoch[0] Batch [6260-6280]	Speed: 1444.26 samples/sec	acc=0.000000	lossvalue=42.145989
INFO:root:Epoch[0] Batch [6280-6300]	Speed: 1437.50 samples/sec	acc=0.000000	lossvalue=42.140790
INFO:root:Epoch[0] Batch [6300-6320]	Speed: 1441.20 samples/sec	acc=0.000000	lossvalue=42.174975
INFO:root:Epoch[0] Batch [6320-6340]	Speed: 1441.32 samples/sec	acc=0.000000	lossvalue=42.161075
INFO:root:Epoch[0] Batch [6340-6360]	Speed: 1453.63 samples/sec	acc=0.000000	lossvalue=42.162563
INFO:root:Epoch[0] Batch [6360-6380]	Speed: 1398.34 samples/sec	acc=0.000000	lossvalue=42.150429
INFO:root:Epoch[0] Batch [6380-6400]	Speed: 1378.18 samples/sec	acc=0.000000	lossvalue=42.140183
INFO:root:Epoch[0] Batch [6400-6420]	Speed: 1433.25 samples/sec	acc=0.000000	lossvalue=42.154503
INFO:root:Epoch[0] Batch [6420-6440]	Speed: 1433.36 samples/sec	acc=0.000000	lossvalue=42.139214
INFO:root:Epoch[0] Batch [6440-6460]	Speed: 1450.16 samples/sec	acc=0.000000	lossvalue=42.139873
INFO:root:Epoch[0] Batch [6460-6480]	Speed: 1441.93 samples/sec	acc=0.000000	lossvalue=42.146629
INFO:root:Epoch[0] Batch [6480-6500]	Speed: 1439.58 samples/sec	acc=0.000000	lossvalue=42.147348
INFO:root:Epoch[0] Batch [6500-6520]	Speed: 1438.32 samples/sec	acc=0.000000	lossvalue=42.136663
INFO:root:Epoch[0] Batch [6520-6540]	Speed: 1450.35 samples/sec	acc=0.000000	lossvalue=42.134079
INFO:root:Epoch[0] Batch [6540-6560]	Speed: 1426.14 samples/sec	acc=0.000000	lossvalue=42.124281
INFO:root:Epoch[0] Batch [6560-6580]	Speed: 1409.75 samples/sec	acc=0.000000	lossvalue=42.134493
INFO:root:Epoch[0] Batch [6580-6600]	Speed: 1412.57 samples/sec	acc=0.000000	lossvalue=42.128955
INFO:root:Epoch[0] Batch [6600-6620]	Speed: 1416.73 samples/sec	acc=0.000000	lossvalue=42.117061
INFO:root:Epoch[0] Batch [6620-6640]	Speed: 1362.93 samples/sec	acc=0.000000	lossvalue=42.129056
INFO:root:Epoch[0] Batch [6640-6660]	Speed: 1398.75 samples/sec	acc=0.000000	lossvalue=42.124024
INFO:root:Epoch[0] Batch [6660-6680]	Speed: 1404.41 samples/sec	acc=0.000000	lossvalue=42.118924
INFO:root:Epoch[0] Batch [6680-6700]	Speed: 1365.18 samples/sec	acc=0.000000	lossvalue=42.114451
INFO:root:Epoch[0] Batch [6700-6720]	Speed: 1424.69 samples/sec	acc=0.000000	lossvalue=42.115270
INFO:root:Epoch[0] Batch [6720-6740]	Speed: 1421.34 samples/sec	acc=0.000000	lossvalue=42.112736
INFO:root:Epoch[0] Batch [6740-6760]	Speed: 1276.32 samples/sec	acc=0.000000	lossvalue=42.098629
INFO:root:Epoch[0] Batch [6760-6780]	Speed: 1403.91 samples/sec	acc=0.000000	lossvalue=42.105654
INFO:root:Epoch[0] Batch [6780-6800]	Speed: 1427.84 samples/sec	acc=0.000000	lossvalue=42.103990
INFO:root:Epoch[0] Batch [6800-6820]	Speed: 1462.64 samples/sec	acc=0.000000	lossvalue=42.106331
INFO:root:Epoch[0] Batch [6820-6840]	Speed: 1406.41 samples/sec	acc=0.000000	lossvalue=42.113586
INFO:root:Epoch[0] Batch [6840-6860]	Speed: 1377.89 samples/sec	acc=0.000000	lossvalue=42.103058
INFO:root:Epoch[0] Batch [6860-6880]	Speed: 1355.97 samples/sec	acc=0.000000	lossvalue=42.112700
INFO:root:Epoch[0] Batch [6880-6900]	Speed: 1325.65 samples/sec	acc=0.000000	lossvalue=42.097591
INFO:root:Epoch[0] Batch [6900-6920]	Speed: 1411.97 samples/sec	acc=0.000000	lossvalue=42.098501
INFO:root:Epoch[0] Batch [6920-6940]	Speed: 1423.06 samples/sec	acc=0.000000	lossvalue=42.089276
INFO:root:Epoch[0] Batch [6940-6960]	Speed: 1427.09 samples/sec	acc=0.000000	lossvalue=42.080213
INFO:root:Epoch[0] Batch [6960-6980]	Speed: 1432.16 samples/sec	acc=0.000000	lossvalue=42.089613
lr-batch-epoch: 0.01 6999 0
INFO:root:Epoch[0] Batch [6980-7000]	Speed: 1400.62 samples/sec	acc=0.000000	lossvalue=42.088815
INFO:root:Epoch[0] Batch [7000-7020]	Speed: 1439.06 samples/sec	acc=0.000000	lossvalue=42.087424
INFO:root:Epoch[0] Batch [7020-7040]	Speed: 1440.75 samples/sec	acc=0.000000	lossvalue=42.067699
INFO:root:Epoch[0] Batch [7040-7060]	Speed: 1429.19 samples/sec	acc=0.000000	lossvalue=42.094941
INFO:root:Epoch[0] Batch [7060-7080]	Speed: 1444.85 samples/sec	acc=0.000000	lossvalue=42.085171
INFO:root:Epoch[0] Batch [7080-7100]	Speed: 1424.62 samples/sec	acc=0.000000	lossvalue=42.083311
INFO:root:Epoch[0] Batch [7100-7120]	Speed: 1450.79 samples/sec	acc=0.000000	lossvalue=42.086273
INFO:root:Epoch[0] Batch [7120-7140]	Speed: 1427.95 samples/sec	acc=0.000000	lossvalue=42.081083
INFO:root:Epoch[0] Batch [7140-7160]	Speed: 1420.23 samples/sec	acc=0.000000	lossvalue=42.083924
INFO:root:Epoch[0] Batch [7160-7180]	Speed: 1427.76 samples/sec	acc=0.000000	lossvalue=42.082238
INFO:root:Epoch[0] Batch [7180-7200]	Speed: 1439.70 samples/sec	acc=0.000000	lossvalue=42.081499
INFO:root:Epoch[0] Batch [7200-7220]	Speed: 1431.10 samples/sec	acc=0.000000	lossvalue=42.065490
INFO:root:Epoch[0] Batch [7220-7240]	Speed: 1443.44 samples/sec	acc=0.000000	lossvalue=42.083921
INFO:root:Epoch[0] Batch [7240-7260]	Speed: 1420.57 samples/sec	acc=0.000000	lossvalue=42.065946
INFO:root:Epoch[0] Batch [7260-7280]	Speed: 1406.95 samples/sec	acc=0.000000	lossvalue=42.072768
INFO:root:Epoch[0] Batch [7280-7300]	Speed: 1415.92 samples/sec	acc=0.000000	lossvalue=42.065378
INFO:root:Epoch[0] Batch [7300-7320]	Speed: 1415.89 samples/sec	acc=0.000000	lossvalue=42.062989
INFO:root:Epoch[0] Batch [7320-7340]	Speed: 1419.06 samples/sec	acc=0.000000	lossvalue=42.064022
INFO:root:Epoch[0] Batch [7340-7360]	Speed: 1422.13 samples/sec	acc=0.000000	lossvalue=42.051758
INFO:root:Epoch[0] Batch [7360-7380]	Speed: 1427.27 samples/sec	acc=0.000000	lossvalue=42.051090
INFO:root:Epoch[0] Batch [7380-7400]	Speed: 1425.62 samples/sec	acc=0.000000	lossvalue=42.050246
INFO:root:Epoch[0] Batch [7400-7420]	Speed: 1305.61 samples/sec	acc=0.000000	lossvalue=42.062048
INFO:root:Epoch[0] Batch [7420-7440]	Speed: 1435.37 samples/sec	acc=0.000000	lossvalue=42.061152
INFO:root:Epoch[0] Batch [7440-7460]	Speed: 1410.85 samples/sec	acc=0.000000	lossvalue=42.054612
INFO:root:Epoch[0] Batch [7460-7480]	Speed: 1425.80 samples/sec	acc=0.000000	lossvalue=42.047012
INFO:root:Epoch[0] Batch [7480-7500]	Speed: 1452.10 samples/sec	acc=0.000000	lossvalue=42.059667
INFO:root:Epoch[0] Batch [7500-7520]	Speed: 1425.86 samples/sec	acc=0.000000	lossvalue=42.026051
INFO:root:Epoch[0] Batch [7520-7540]	Speed: 1431.03 samples/sec	acc=0.000000	lossvalue=42.045337
INFO:root:Epoch[0] Batch [7540-7560]	Speed: 1443.19 samples/sec	acc=0.000000	lossvalue=42.038288
INFO:root:Epoch[0] Batch [7560-7580]	Speed: 1435.81 samples/sec	acc=0.000000	lossvalue=42.030659
INFO:root:Epoch[0] Batch [7580-7600]	Speed: 1448.49 samples/sec	acc=0.000000	lossvalue=42.039278
INFO:root:Epoch[0] Batch [7600-7620]	Speed: 1422.49 samples/sec	acc=0.000000	lossvalue=42.040024
INFO:root:Epoch[0] Batch [7620-7640]	Speed: 1420.79 samples/sec	acc=0.000000	lossvalue=42.033022
INFO:root:Epoch[0] Batch [7640-7660]	Speed: 1426.24 samples/sec	acc=0.000000	lossvalue=42.032648
INFO:root:Epoch[0] Batch [7660-7680]	Speed: 1427.59 samples/sec	acc=0.000000	lossvalue=42.040214
INFO:root:Epoch[0] Batch [7680-7700]	Speed: 1436.16 samples/sec	acc=0.000000	lossvalue=42.040748
INFO:root:Epoch[0] Batch [7700-7720]	Speed: 1447.00 samples/sec	acc=0.000000	lossvalue=42.030937
INFO:root:Epoch[0] Batch [7720-7740]	Speed: 1427.96 samples/sec	acc=0.000000	lossvalue=42.029444
INFO:root:Epoch[0] Batch [7740-7760]	Speed: 1433.70 samples/sec	acc=0.000000	lossvalue=42.039563
INFO:root:Epoch[0] Batch [7760-7780]	Speed: 1425.40 samples/sec	acc=0.000000	lossvalue=42.038318
INFO:root:Epoch[0] Batch [7780-7800]	Speed: 1433.88 samples/sec	acc=0.000000	lossvalue=42.022294
INFO:root:Epoch[0] Batch [7800-7820]	Speed: 1425.88 samples/sec	acc=0.000000	lossvalue=42.020742
INFO:root:Epoch[0] Batch [7820-7840]	Speed: 1430.61 samples/sec	acc=0.000000	lossvalue=42.027658
INFO:root:Epoch[0] Batch [7840-7860]	Speed: 1433.35 samples/sec	acc=0.000000	lossvalue=42.034516
INFO:root:Epoch[0] Batch [7860-7880]	Speed: 1421.92 samples/sec	acc=0.000000	lossvalue=42.010682
INFO:root:Epoch[0] Batch [7880-7900]	Speed: 1409.32 samples/sec	acc=0.000000	lossvalue=42.016182
INFO:root:Epoch[0] Batch [7900-7920]	Speed: 1424.05 samples/sec	acc=0.000000	lossvalue=42.024273
INFO:root:Epoch[0] Batch [7920-7940]	Speed: 1416.98 samples/sec	acc=0.000000	lossvalue=42.014298
INFO:root:Epoch[0] Batch [7940-7960]	Speed: 1411.33 samples/sec	acc=0.000000	lossvalue=42.031139
INFO:root:Epoch[0] Batch [7960-7980]	Speed: 1382.00 samples/sec	acc=0.000000	lossvalue=42.025514
lr-batch-epoch: 0.01 7999 0
testing verification..
(12000, 512)
infer time 4.986866
[agedb_30][8000]XNorm: 1.027080
[agedb_30][8000]Accuracy-Flip: 0.50067+-0.00186
testing verification..
(12000, 512)
infer time 4.825368
[calfw][8000]XNorm: 1.632914
[calfw][8000]Accuracy-Flip: 0.50217+-0.00454
testing verification..
(14000, 512)
infer time 5.582666
[cfp_ff][8000]XNorm: 16.623992
[cfp_ff][8000]Accuracy-Flip: 0.60443+-0.01392
testing verification..
(14000, 512)
infer time 5.578
[cfp_fp][8000]XNorm: 13.326273
[cfp_fp][8000]Accuracy-Flip: 0.56386+-0.02096
testing verification..
(12000, 512)
infer time 4.811976
[cplfw][8000]XNorm: 3.476282
[cplfw][8000]Accuracy-Flip: 0.49917+-0.00664
testing verification..
(12000, 512)
infer time 4.507722
[lfw][8000]XNorm: 1.115357
[lfw][8000]Accuracy-Flip: 0.50383+-0.00817
[8000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [7980-8000]	Speed: 182.05 samples/sec	acc=0.000000	lossvalue=42.002839
INFO:root:Epoch[0] Batch [8000-8020]	Speed: 1343.79 samples/sec	acc=0.000000	lossvalue=42.023796
INFO:root:Epoch[0] Batch [8020-8040]	Speed: 1391.27 samples/sec	acc=0.000000	lossvalue=42.021088
INFO:root:Epoch[0] Batch [8040-8060]	Speed: 1406.34 samples/sec	acc=0.000000	lossvalue=42.018094
INFO:root:Epoch[0] Batch [8060-8080]	Speed: 1416.11 samples/sec	acc=0.000000	lossvalue=42.013804
INFO:root:Epoch[0] Batch [8080-8100]	Speed: 1414.52 samples/sec	acc=0.000000	lossvalue=42.021646
INFO:root:Epoch[0] Batch [8100-8120]	Speed: 1410.66 samples/sec	acc=0.000000	lossvalue=42.005379
INFO:root:Epoch[0] Batch [8120-8140]	Speed: 1335.76 samples/sec	acc=0.000000	lossvalue=42.003079
INFO:root:Epoch[0] Batch [8140-8160]	Speed: 1399.16 samples/sec	acc=0.000000	lossvalue=42.000332
INFO:root:Epoch[0] Batch [8160-8180]	Speed: 1414.74 samples/sec	acc=0.000000	lossvalue=42.009885
INFO:root:Epoch[0] Batch [8180-8200]	Speed: 1415.45 samples/sec	acc=0.000000	lossvalue=42.003193
INFO:root:Epoch[0] Batch [8200-8220]	Speed: 1432.53 samples/sec	acc=0.000000	lossvalue=41.998407
INFO:root:Epoch[0] Batch [8220-8240]	Speed: 1407.10 samples/sec	acc=0.000000	lossvalue=42.007038
INFO:root:Epoch[0] Batch [8240-8260]	Speed: 1438.36 samples/sec	acc=0.000000	lossvalue=42.010870
INFO:root:Epoch[0] Batch [8260-8280]	Speed: 1432.62 samples/sec	acc=0.000000	lossvalue=41.998699
INFO:root:Epoch[0] Batch [8280-8300]	Speed: 1418.51 samples/sec	acc=0.000000	lossvalue=41.996247
INFO:root:Epoch[0] Batch [8300-8320]	Speed: 1432.23 samples/sec	acc=0.000000	lossvalue=41.997199
INFO:root:Epoch[0] Batch [8320-8340]	Speed: 1463.53 samples/sec	acc=0.000000	lossvalue=41.997478
INFO:root:Epoch[0] Batch [8340-8360]	Speed: 1436.30 samples/sec	acc=0.000000	lossvalue=42.006742
INFO:root:Epoch[0] Batch [8360-8380]	Speed: 1439.45 samples/sec	acc=0.000000	lossvalue=41.987122
INFO:root:Epoch[0] Batch [8380-8400]	Speed: 1414.07 samples/sec	acc=0.000000	lossvalue=41.988386
INFO:root:Epoch[0] Batch [8400-8420]	Speed: 1425.56 samples/sec	acc=0.000000	lossvalue=41.987975
INFO:root:Epoch[0] Batch [8420-8440]	Speed: 1429.08 samples/sec	acc=0.000000	lossvalue=41.991180
INFO:root:Epoch[0] Batch [8440-8460]	Speed: 1430.65 samples/sec	acc=0.000000	lossvalue=42.001754
INFO:root:Epoch[0] Batch [8460-8480]	Speed: 1435.57 samples/sec	acc=0.000000	lossvalue=41.991315
INFO:root:Epoch[0] Batch [8480-8500]	Speed: 1432.98 samples/sec	acc=0.000000	lossvalue=41.982089
INFO:root:Epoch[0] Batch [8500-8520]	Speed: 1424.44 samples/sec	acc=0.000000	lossvalue=41.990520
INFO:root:Epoch[0] Batch [8520-8540]	Speed: 1431.28 samples/sec	acc=0.000000	lossvalue=41.995154
INFO:root:Epoch[0] Batch [8540-8560]	Speed: 1426.16 samples/sec	acc=0.000000	lossvalue=41.987347
INFO:root:Epoch[0] Batch [8560-8580]	Speed: 1426.23 samples/sec	acc=0.000000	lossvalue=41.980704
INFO:root:Epoch[0] Batch [8580-8600]	Speed: 1428.86 samples/sec	acc=0.000000	lossvalue=41.987747
INFO:root:Epoch[0] Batch [8600-8620]	Speed: 1438.03 samples/sec	acc=0.000000	lossvalue=41.981680
INFO:root:Epoch[0] Batch [8620-8640]	Speed: 1434.76 samples/sec	acc=0.000000	lossvalue=41.995870
INFO:root:Epoch[0] Batch [8640-8660]	Speed: 1395.83 samples/sec	acc=0.000000	lossvalue=41.980906
INFO:root:Epoch[0] Batch [8660-8680]	Speed: 1436.40 samples/sec	acc=0.000000	lossvalue=41.985622
INFO:root:Epoch[0] Batch [8680-8700]	Speed: 1427.66 samples/sec	acc=0.000000	lossvalue=41.980081
INFO:root:Epoch[0] Batch [8700-8720]	Speed: 1433.52 samples/sec	acc=0.000000	lossvalue=41.981155
INFO:root:Epoch[0] Batch [8720-8740]	Speed: 1407.18 samples/sec	acc=0.000000	lossvalue=41.984562
INFO:root:Epoch[0] Batch [8740-8760]	Speed: 1436.53 samples/sec	acc=0.000000	lossvalue=41.978110
INFO:root:Epoch[0] Batch [8760-8780]	Speed: 1411.15 samples/sec	acc=0.000000	lossvalue=41.976267
INFO:root:Epoch[0] Batch [8780-8800]	Speed: 1425.90 samples/sec	acc=0.000000	lossvalue=41.980106
INFO:root:Epoch[0] Batch [8800-8820]	Speed: 1316.30 samples/sec	acc=0.000000	lossvalue=41.981414
INFO:root:Epoch[0] Batch [8820-8840]	Speed: 1412.42 samples/sec	acc=0.000000	lossvalue=41.977699
INFO:root:Epoch[0] Batch [8840-8860]	Speed: 1441.54 samples/sec	acc=0.000000	lossvalue=41.966282
INFO:root:Epoch[0] Batch [8860-8880]	Speed: 1441.60 samples/sec	acc=0.000000	lossvalue=41.969540
INFO:root:Epoch[0] Batch [8880-8900]	Speed: 1410.46 samples/sec	acc=0.000000	lossvalue=41.974941
INFO:root:Epoch[0] Batch [8900-8920]	Speed: 1416.21 samples/sec	acc=0.000000	lossvalue=41.972329
INFO:root:Epoch[0] Batch [8920-8940]	Speed: 1423.41 samples/sec	acc=0.000000	lossvalue=41.982397
INFO:root:Epoch[0] Batch [8940-8960]	Speed: 1433.85 samples/sec	acc=0.000000	lossvalue=41.960446
INFO:root:Epoch[0] Batch [8960-8980]	Speed: 1419.24 samples/sec	acc=0.000000	lossvalue=41.965259
lr-batch-epoch: 0.01 8999 0
INFO:root:Epoch[0] Batch [8980-9000]	Speed: 1429.51 samples/sec	acc=0.000000	lossvalue=41.963624
INFO:root:Epoch[0] Batch [9000-9020]	Speed: 1421.19 samples/sec	acc=0.000000	lossvalue=41.975398
INFO:root:Epoch[0] Batch [9020-9040]	Speed: 1434.94 samples/sec	acc=0.000000	lossvalue=41.967802
INFO:root:Epoch[0] Batch [9040-9060]	Speed: 1441.51 samples/sec	acc=0.000000	lossvalue=41.958901
INFO:root:Epoch[0] Batch [9060-9080]	Speed: 1404.33 samples/sec	acc=0.000000	lossvalue=41.964572
INFO:root:Epoch[0] Batch [9080-9100]	Speed: 1446.24 samples/sec	acc=0.000000	lossvalue=41.959388
INFO:root:Epoch[0] Batch [9100-9120]	Speed: 1422.98 samples/sec	acc=0.000000	lossvalue=41.962714
INFO:root:Epoch[0] Batch [9120-9140]	Speed: 1414.02 samples/sec	acc=0.000000	lossvalue=41.957292
INFO:root:Epoch[0] Batch [9140-9160]	Speed: 1442.95 samples/sec	acc=0.000000	lossvalue=41.947298
INFO:root:Epoch[0] Batch [9160-9180]	Speed: 1428.41 samples/sec	acc=0.000000	lossvalue=41.957963
INFO:root:Epoch[0] Batch [9180-9200]	Speed: 1423.07 samples/sec	acc=0.000000	lossvalue=41.949533
INFO:root:Epoch[0] Batch [9200-9220]	Speed: 1425.92 samples/sec	acc=0.000000	lossvalue=41.955536
INFO:root:Epoch[0] Batch [9220-9240]	Speed: 1425.39 samples/sec	acc=0.000000	lossvalue=41.955172
INFO:root:Epoch[0] Batch [9240-9260]	Speed: 1422.23 samples/sec	acc=0.000000	lossvalue=41.948323
INFO:root:Epoch[0] Batch [9260-9280]	Speed: 1440.76 samples/sec	acc=0.000000	lossvalue=41.963384
INFO:root:Epoch[0] Batch [9280-9300]	Speed: 1426.57 samples/sec	acc=0.000000	lossvalue=41.951702
INFO:root:Epoch[0] Batch [9300-9320]	Speed: 1433.89 samples/sec	acc=0.000000	lossvalue=41.956513
INFO:root:Epoch[0] Batch [9320-9340]	Speed: 1425.13 samples/sec	acc=0.000000	lossvalue=41.953564
INFO:root:Epoch[0] Batch [9340-9360]	Speed: 1370.22 samples/sec	acc=0.000000	lossvalue=41.964537
INFO:root:Epoch[0] Batch [9360-9380]	Speed: 1365.07 samples/sec	acc=0.000000	lossvalue=41.946557
INFO:root:Epoch[0] Batch [9380-9400]	Speed: 1390.25 samples/sec	acc=0.000000	lossvalue=41.948429

(eog:35032): EOG-WARNING **: Failed to open file '/home/rongy/.cache/thumbnails/normal/e0f3196ef3a20c0b96b730474c7231ee.png': No such file or directory
INFO:root:Epoch[0] Batch [9400-9420]	Speed: 1423.46 samples/sec	acc=0.000000	lossvalue=41.953710
INFO:root:Epoch[0] Batch [9420-9440]	Speed: 1409.61 samples/sec	acc=0.000000	lossvalue=41.943771
INFO:root:Epoch[0] Batch [9440-9460]	Speed: 1404.73 samples/sec	acc=0.000000	lossvalue=41.947741
INFO:root:Epoch[0] Batch [9460-9480]	Speed: 1410.33 samples/sec	acc=0.000000	lossvalue=41.959211
INFO:root:Epoch[0] Batch [9480-9500]	Speed: 1436.10 samples/sec	acc=0.000000	lossvalue=41.964881
INFO:root:Epoch[0] Batch [9500-9520]	Speed: 1284.75 samples/sec	acc=0.000000	lossvalue=41.940122
INFO:root:Epoch[0] Batch [9520-9540]	Speed: 1416.94 samples/sec	acc=0.000000	lossvalue=41.946654
INFO:root:Epoch[0] Batch [9540-9560]	Speed: 1409.22 samples/sec	acc=0.000000	lossvalue=41.953679
INFO:root:Epoch[0] Batch [9560-9580]	Speed: 1430.54 samples/sec	acc=0.000000	lossvalue=41.946638
INFO:root:Epoch[0] Batch [9580-9600]	Speed: 1421.10 samples/sec	acc=0.000000	lossvalue=41.945858
INFO:root:Epoch[0] Batch [9600-9620]	Speed: 1457.82 samples/sec	acc=0.000000	lossvalue=41.945140
INFO:root:Epoch[0] Batch [9620-9640]	Speed: 1428.59 samples/sec	acc=0.000000	lossvalue=41.937235
INFO:root:Epoch[0] Batch [9640-9660]	Speed: 1420.10 samples/sec	acc=0.000000	lossvalue=41.946729
INFO:root:Epoch[0] Batch [9660-9680]	Speed: 1377.31 samples/sec	acc=0.000000	lossvalue=41.943734
INFO:root:Epoch[0] Batch [9680-9700]	Speed: 1460.06 samples/sec	acc=0.000000	lossvalue=41.942536
INFO:root:Epoch[0] Batch [9700-9720]	Speed: 1453.38 samples/sec	acc=0.000000	lossvalue=41.944737
INFO:root:Epoch[0] Batch [9720-9740]	Speed: 1440.01 samples/sec	acc=0.000000	lossvalue=41.938507
INFO:root:Epoch[0] Batch [9740-9760]	Speed: 1427.13 samples/sec	acc=0.000000	lossvalue=41.932979
INFO:root:Epoch[0] Batch [9760-9780]	Speed: 1446.75 samples/sec	acc=0.000000	lossvalue=41.941228
INFO:root:Epoch[0] Batch [9780-9800]	Speed: 1423.12 samples/sec	acc=0.000000	lossvalue=41.939507
INFO:root:Epoch[0] Batch [9800-9820]	Speed: 1456.51 samples/sec	acc=0.000000	lossvalue=41.925296
INFO:root:Epoch[0] Batch [9820-9840]	Speed: 1456.65 samples/sec	acc=0.000000	lossvalue=41.934135
INFO:root:Epoch[0] Batch [9840-9860]	Speed: 1428.55 samples/sec	acc=0.000000	lossvalue=41.936381
INFO:root:Epoch[0] Batch [9860-9880]	Speed: 1419.47 samples/sec	acc=0.000000	lossvalue=41.939161
INFO:root:Epoch[0] Batch [9880-9900]	Speed: 1462.09 samples/sec	acc=0.000000	lossvalue=41.936261
INFO:root:Epoch[0] Batch [9900-9920]	Speed: 1442.82 samples/sec	acc=0.000000	lossvalue=41.929150
INFO:root:Epoch[0] Batch [9920-9940]	Speed: 1444.01 samples/sec	acc=0.000000	lossvalue=41.932265
INFO:root:Epoch[0] Batch [9940-9960]	Speed: 1419.85 samples/sec	acc=0.000000	lossvalue=41.929162
INFO:root:Epoch[0] Batch [9960-9980]	Speed: 1436.27 samples/sec	acc=0.000000	lossvalue=41.931037
lr-batch-epoch: 0.01 9999 0
testing verification..
(12000, 512)
infer time 4.598723
[agedb_30][10000]XNorm: 0.798147
[agedb_30][10000]Accuracy-Flip: 0.49950+-0.00150
testing verification..
(12000, 512)
infer time 4.744518
[calfw][10000]XNorm: 1.327497
[calfw][10000]Accuracy-Flip: 0.50333+-0.00325
testing verification..
(14000, 512)
infer time 5.152224
[cfp_ff][10000]XNorm: 18.945766
[cfp_ff][10000]Accuracy-Flip: 0.58400+-0.01557
testing verification..
(14000, 512)
infer time 5.904056
[cfp_fp][10000]XNorm: 14.005116
[cfp_fp][10000]Accuracy-Flip: 0.54114+-0.01367
testing verification..
(12000, 512)
infer time 5.123472
[cplfw][10000]XNorm: 3.457998
[cplfw][10000]Accuracy-Flip: 0.50133+-0.00799
testing verification..
(12000, 512)
infer time 4.997448
[lfw][10000]XNorm: 0.884298
[lfw][10000]Accuracy-Flip: 0.50283+-0.00753
[10000]Accuracy-Highest: 0.52283
INFO:root:Epoch[0] Batch [9980-10000]	Speed: 179.89 samples/sec	acc=0.000000	lossvalue=41.932265
INFO:root:Epoch[0] Batch [10000-10020]	Speed: 1352.16 samples/sec	acc=0.000000	lossvalue=41.930890
INFO:root:Epoch[0] Batch [10020-10040]	Speed: 1403.24 samples/sec	acc=0.000000	lossvalue=41.936999
INFO:root:Epoch[0] Batch [10040-10060]	Speed: 1352.08 samples/sec	acc=0.000000	lossvalue=41.921464
INFO:root:Epoch[0] Batch [10060-10080]	Speed: 1213.13 samples/sec	acc=0.000000	lossvalue=41.930905
INFO:root:Epoch[0] Batch [10080-10100]	Speed: 1293.58 samples/sec	acc=0.000000	lossvalue=41.927600
INFO:root:Epoch[0] Batch [10100-10120]	Speed: 1389.86 samples/sec	acc=0.000000	lossvalue=41.936483
INFO:root:Epoch[0] Batch [10120-10140]	Speed: 1375.93 samples/sec	acc=0.000000	lossvalue=41.926200
INFO:root:Epoch[0] Batch [10140-10160]	Speed: 1390.30 samples/sec	acc=0.000000	lossvalue=41.931243
^CTraceback (most recent call last):
  File ""train.py"", line 447, in <module>
    
  File ""train.py"", line 444, in main
    
  File ""train.py"", line 439, in train_net
    
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 539, in fit
    self.update_metric(eval_metric, data_batch.label)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/module/module.py"", line 775, in update_metric
    self._exec_group.update_metric(eval_metric, labels, pre_sliced)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 639, in update_metric
    eval_metric.update_dict(labels_, preds)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/metric.py"", line 304, in update_dict
    metric.update_dict(labels, preds)
  File ""/home/rongy/.local/lib/python2.7/site-packages/mxnet/metric.py"", line 132, in update_dict
"
"I have used deploy folder for generating the embeddings.
CPU and GPU times are similar, ~300 ms and 200ms respectively.
Details:
4*Tesla P100 GPUs
ctx = mx.gpu()
cuda-100-devel docker image with mxnet-cu100 (installed with pip)
python version 3.6"
"GTX 1080
mobilenet0.25
I run test.py with the image (640*640）and found that the detect time is 120ms.
But I found the forward time is near 0.3ms. It's confused me. Is there any tricks to speed up detect time.?

"
""
"the offered pretrained r34 model is 134M, but the finetune model base on the pretrained model is about 340M, why this happened?
or how to generate a small size model?"
"insightface's age gender model maybe the best model for asian in open source.
here is a new age gender model,comparing with insightface's:
https://www.youtube.com/watch?v=Bec7roJJHzY&feature=youtu.be"
"Hi,

I would like to run this repo on mobile phone ?
Is this possible ?

Thanks."
"  Why the default threshold is 0.02?
  When I change it to 0.5 and run it to test R50 model, the mAP of Hard subset is just 84.9 which is far below 90.4(the author said)"
"hi, when i type make to build cxx tools, got an error 'cythonbbox_overlaps_cython.pxd' not found,
the stacktraces is:

![810A0A3551EB01B40DC01C4A6F3F43F0411790461_IMAGE](https://user-images.githubusercontent.com/10165921/58956701-8ae9bf00-87d1-11e9-8f6f-90db8b1703e4.jpg)

hope someone can help me 
"
"CUDA_VISIBLE_DEVICES='0,1,2,3' python3 -u train.py --network r10
0 --loss arcface --dataset emore
gpu num: 4
prefix ./models/r100-arcface-emore/model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=512, ckpt=3, ctx_num=4, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='arcface', lr=0.1, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='r100', per_batch_size=128, pretrained='', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'net_act': 'prelu', 'data_rand_mirror': True, 'loss_m3': 0.0, 'fc7_no_bias': False, 'net_output': 'E', 'network': 'r100', 'image_shape': [112, 112, 3], 'num_layers': 100, 'dataset_path': '../datasets/faces_emore', 'data_color': 0, 'num_classes': 85742, 'data_cutoff': False, 'batch_size': 512, 'net_unit': 3, 'emb_size': 512, 'bn_mom': 0.9, 'fc7_lr_mult': 1.0, 'count_flops': True, 'ckpt_embedding': True, 'net_input': 1, 'loss_m2': 0.5, 'dataset': 'emore', 'fc7_wd_mult': 1.0, 'loss_m1': 1.0, 'net_multiplier': 1.0, 'net_se': 0, 'data_images_filter': 0, 'loss': 'arcface', 'net_blocks': [1, 4, 6, 2], 'workspace': 256, 'loss_s': 64.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'memonger': False, 'num_workers': 1, 'net_name': 'fresnet', 'ce_loss': True, 'per_batch_size': 128, 'loss_name': 'margin_softmax', 'max_steps': 0}
0 1 E 3 prelu False
Network FLOPs: 24.2G
INFO:root:loading recordio ../datasets/faces_emore/train.rec...
header0 label [5822654. 5908396.]
id2range 85742
5822653
rand_mirror True
[19:39:08] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
Traceback (most recent call last):
  File ""train.py"", line 377, in <module>
    main()
  File ""train.py"", line 374, in main
    train_net(args)
  File ""train.py"", line 264, in train_net
    data_set = verification.load_bin(path, image_size)
  File ""eval/verification.py"", line 186, in load_bin
    data = nd.empty((len(issame_list)*2, 3, image_size[0], image_size[1]))
  File ""/home/gabit/face/face/lib/python3.5/site-packages/mxnet/ndarray/utils.py"", line 103, in empty
    return _empty_ndarray(shape, ctx, dtype)
  File ""/home/gabit/face/face/lib/python3.5/site-packages/mxnet/ndarray/ndarray.py"", line 3877, in empty
    return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
  File ""/home/gabit/face/face/lib/python3.5/site-packages/mxnet/ndarray/ndarray.py"", line 140, in _new_alloc_handle
    ctypes.byref(hdl)))
  File ""/home/gabit/face/face/lib/python3.5/site-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [19:39:09] src/storage/./cpu_device_storage.h:75: Failed to allocate CPU Memory

Stack trace returned 10 entries:
[bt] (0) /home/gabit/face/face/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x40b29a) [0x7ff4d069429a]
[bt] (1) /home/gabit/face/face/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x40b8b1) [0x7ff4d06948b1]
[bt] (2) /home/gabit/face/face/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x34239a1) [0x7ff4d36ac9a1]
[bt] (3) /home/gabit/face/face/lib/python3.5/site-packages/mxnet/libmxnet.so(+0x342bc1f) [0x7ff4d36b4c1f]
[bt] (4) /home/gabit/face/face/lib/python3.5/site-packages/mxnet/libmxnet.so(mxnet::NDArray::NDArray(nnvm::TShape const&, mxnet::Context, bool, int)+0x816) [0x7ff4d2ec96a6]
[bt] (5) /home/gabit/face/face/lib/python3.5/site-packages/mxnet/libmxnet.so(MXNDArrayCreateEx+0x158) [0x7ff4d2ec9f18]
[bt] (6) /home/gabit/face/face/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7ff5238a6e20]
[bt] (7) /home/gabit/face/face/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7ff5238a688b]
[bt] (8) /home/gabit/face/face/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7ff5238a101a]
[bt] (9) /home/gabit/face/face/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x9fcb) [0x7ff523894fcb]"
""
Are you using multiple models for FRVT??
""
"This problem is when I run ${DIR}/recognition/train.py
After two days of  train, the Fmobilefacenet model is stil have very low accuracy.
The accuracy has been kept at around 40% when the EPOCH exceeds 7,The accuracy has not changed for more than a day.
The initial learning rate I set is 0.01,(the other parameters are defaule value, I used the pre_train model of model-y1-test2.the dataset is faces_emore)but now it has been trained more than 80W times.the accuracy has been 0.4 since 20W times.
I would like to ask if I set some parameters wrong,do I need to stop training and run begin or wait?
could anyone give me some advice?



this is the result:
INFO:root:Epoch[19] Batch [12700-12720]	Speed: 962.42 samples/sec	acc=0.428906	lossvalue=6.046267
INFO:root:Epoch[19] Batch [12720-12740]	Speed: 963.09 samples/sec	acc=0.427344	lossvalue=5.902352
INFO:root:Epoch[19] Batch [12740-12760]	Speed: 964.05 samples/sec	acc=0.405469	lossvalue=6.006512
INFO:root:Epoch[19] Batch [12760-12780]	Speed: 963.62 samples/sec	acc=0.417578	lossvalue=5.868161
INFO:root:Epoch[19] Batch [12780-12800]	Speed: 964.76 samples/sec	acc=0.417187	lossvalue=5.869784
INFO:root:Epoch[19] Batch [12800-12820]	Speed: 963.46 samples/sec	acc=0.411719	lossvalue=5.908864
INFO:root:Epoch[19] Batch [12820-12840]	Speed: 964.05 samples/sec	acc=0.421875	lossvalue=5.642534
INFO:root:Epoch[19] Batch [12840-12860]	Speed: 962.02 samples/sec	acc=0.414844	lossvalue=5.834743
INFO:root:Epoch[19] Batch [12860-12880]	Speed: 962.71 samples/sec	acc=0.444141	lossvalue=5.611831
INFO:root:Epoch[19] Batch [12880-12900]	Speed: 962.17 samples/sec	acc=0.423828	lossvalue=5.923916
INFO:root:Epoch[19] Batch [12900-12920]	Speed: 960.92 samples/sec	acc=0.426563	lossvalue=6.021607
INFO:root:Epoch[19] Batch [12920-12940]	Speed: 961.78 samples/sec	acc=0.428906	lossvalue=5.919426
INFO:root:Epoch[19] Batch [12940-12960]	Speed: 963.17 samples/sec	acc=0.445703	lossvalue=5.512040
INFO:root:Epoch[19] Batch [12960-12980]	Speed: 964.21 samples/sec	acc=0.428906	lossvalue=5.967726
INFO:root:Epoch[19] Batch [12980-13000]	Speed: 962.44 samples/sec	acc=0.425781	lossvalue=5.983425
INFO:root:Epoch[19] Batch [13000-13020]	Speed: 965.38 samples/sec	acc=0.411719	lossvalue=6.059293
INFO:root:Epoch[19] Batch [13020-13040]	Speed: 965.50 samples/sec	acc=0.444531	lossvalue=5.542191
INFO:root:Epoch[19] Batch [13040-13060]	Speed: 962.32 samples/sec	acc=0.425391	lossvalue=5.960517
INFO:root:Epoch[19] Batch [13060-13080]	Speed: 961.57 samples/sec	acc=0.433203	lossvalue=5.703861
INFO:root:Epoch[19] Batch [13080-13100]	Speed: 962.33 samples/sec	acc=0.427734	lossvalue=5.952133
INFO:root:Epoch[19] Batch [13100-13120]	Speed: 961.56 samples/sec	acc=0.429297	lossvalue=5.969763
INFO:root:Epoch[19] Batch [13120-13140]	Speed: 961.73 samples/sec	acc=0.429297	lossvalue=5.904568
INFO:root:Epoch[19] Batch [13140-13160]	Speed: 960.38 samples/sec	acc=0.430469	lossvalue=5.789618
INFO:root:Epoch[19] Batch [13160-13180]	Speed: 961.28 samples/sec	acc=0.413281	lossvalue=6.073223
INFO:root:Epoch[19] Batch [13180-13200]	Speed: 896.69 samples/sec	acc=0.440234	lossvalue=5.846615
INFO:root:Epoch[19] Batch [13200-13220]	Speed: 963.65 samples/sec	acc=0.420703	lossvalue=5.787654
INFO:root:Epoch[19] Batch [13220-13240]	Speed: 963.00 samples/sec	acc=0.432031	lossvalue=5.570357
INFO:root:Epoch[19] Batch [13240-13260]	Speed: 963.06 samples/sec	acc=0.429688	lossvalue=5.887249
INFO:root:Epoch[19] Batch [13260-13280]	Speed: 959.40 samples/sec	acc=0.421875	lossvalue=5.919816
INFO:root:Epoch[19] Batch [13280-13300]	Speed: 963.92 samples/sec	acc=0.419531	lossvalue=5.864249
INFO:root:Epoch[19] Batch [13300-13320]	Speed: 962.32 samples/sec	acc=0.426563	lossvalue=5.908086
INFO:root:Epoch[19] Batch [13320-13340]	Speed: 960.50 samples/sec	acc=0.425391	lossvalue=5.949596
INFO:root:Epoch[19] Batch [13340-13360]	Speed: 965.47 samples/sec	acc=0.419531	lossvalue=5.906333
INFO:root:Epoch[19] Batch [13360-13380]	Speed: 962.36 samples/sec	acc=0.402734	lossvalue=6.195996
INFO:root:Epoch[19] Batch [13380-13400]	Speed: 960.74 samples/sec	acc=0.419531	lossvalue=6.070377
INFO:root:Epoch[19] Batch [13400-13420]	Speed: 960.01 samples/sec	acc=0.424219	lossvalue=5.838836
INFO:root:Epoch[19] Batch [13420-13440]	Speed: 960.15 samples/sec	acc=0.417187	lossvalue=6.094613
INFO:root:Epoch[19] Batch [13440-13460]	Speed: 962.03 samples/sec	acc=0.421094	lossvalue=5.768937
INFO:root:Epoch[19] Batch [13460-13480]	Speed: 964.95 samples/sec	acc=0.424609	lossvalue=5.965929
INFO:root:Epoch[19] Batch [13480-13500]	Speed: 963.47 samples/sec	acc=0.425781	lossvalue=5.824329
INFO:root:Epoch[19] Batch [13500-13520]	Speed: 962.11 samples/sec	acc=0.415234	lossvalue=6.084083
INFO:root:Epoch[19] Batch [13520-13540]	Speed: 959.87 samples/sec	acc=0.414062	lossvalue=5.965085
INFO:root:Epoch[19] Batch [13540-13560]	Speed: 961.37 samples/sec	acc=0.403516	lossvalue=5.991571
INFO:root:Epoch[19] Batch [13560-13580]	Speed: 965.02 samples/sec	acc=0.423047	lossvalue=5.960075
INFO:root:Epoch[19] Batch [13580-13600]	Speed: 963.58 samples/sec	acc=0.430078	lossvalue=5.924636
INFO:root:Epoch[19] Batch [13600-13620]	Speed: 961.85 samples/sec	acc=0.417578	lossvalue=5.853987
INFO:root:Epoch[19] Batch [13620-13640]	Speed: 960.35 samples/sec	acc=0.423438	lossvalue=5.940636
INFO:root:Epoch[19] Batch [13640-13660]	Speed: 963.53 samples/sec	acc=0.442188	lossvalue=5.928348
INFO:root:Epoch[19] Batch [13660-13680]	Speed: 962.81 samples/sec	acc=0.441797	lossvalue=5.847528
INFO:root:Epoch[19] Batch [13680-13700]	Speed: 958.40 samples/sec	acc=0.426563	lossvalue=5.718157
lr-batch-epoch: 1e-05 13708 19
testing verification..
(12000, 128)
infer time 5.692380000000002
[lfw][878000]XNorm: 11.473797
[lfw][878000]Accuracy-Flip: 0.99450+-0.00422
testing verification..
(14000, 128)
infer time 6.6363949999999985
[cfp_fp][878000]XNorm: 9.717314
[cfp_fp][878000]Accuracy-Flip: 0.93700+-0.01510
testing verification..
(12000, 128)
infer time 5.682969999999998
[agedb_30][878000]XNorm: 11.312865
[agedb_30][878000]Accuracy-Flip: 0.95650+-0.01063
[878000]Accuracy-Highest: 0.95933
INFO:root:Epoch[19] Batch [13700-13720]	Speed: 95.56 samples/sec	acc=0.423047	lossvalue=6.061180
INFO:root:Epoch[19] Batch [13720-13740]	Speed: 963.14 samples/sec	acc=0.410938	lossvalue=5.823772
INFO:root:Epoch[19] Batch [13740-13760]	Speed: 961.95 samples/sec	acc=0.413672	lossvalue=6.148740
INFO:root:Epoch[19] Batch [13760-13780]	Speed: 962.54 samples/sec	acc=0.416406	lossvalue=6.151298
INFO:root:Epoch[19] Batch [13780-13800]	Speed: 961.03 samples/sec	acc=0.422266	lossvalue=5.950473
INFO:root:Epoch[19] Batch [13800-13820]	Speed: 963.65 samples/sec	acc=0.430078	lossvalue=5.755674
INFO:root:Epoch[19] Batch [13820-13840]	Speed: 961.82 samples/sec	acc=0.432812	lossvalue=5.677666
INFO:root:Epoch[19] Batch [13840-13860]	Speed: 964.20 samples/sec	acc=0.417969	lossvalue=6.021457
INFO:root:Epoch[19] Batch [13860-13880]	Speed: 961.87 samples/sec	acc=0.420312	lossvalue=6.127388
INFO:root:Epoch[19] Batch [13880-13900]	Speed: 964.52 samples/sec	acc=0.415234	lossvalue=5.956756
INFO:root:Epoch[19] Batch [13900-13920]	Speed: 962.92 samples/sec	acc=0.418750	lossvalue=5.817868
INFO:root:Epoch[19] Batch [13920-13940]	Speed: 962.17 samples/sec	acc=0.433984	lossvalue=5.877563
INFO:root:Epoch[19] Batch [13940-13960]	Speed: 962.69 samples/sec	acc=0.419922	lossvalue=6.022497
INFO:root:Epoch[19] Batch [13960-13980]	Speed: 961.04 samples/sec	acc=0.435156	lossvalue=5.622451
INFO:root:Epoch[19] Batch [13980-14000]	Speed: 960.89 samples/sec	acc=0.436719	lossvalue=5.697604



I used GTX 2080 ,cpu i7-7800."
"Hi ,
Thanks for your wonderful work.
I tried to use your pre-trained Retinaface-R50 model to run inference.

When I run inference for megaface dataset, it always has below infomation:
Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)

This test slow down the inference so much, I set the MXNET_CUDNN_AUTOTUNE_DEFAULT=0 in environment, but it is not disabled.

I use mxnet-cu10.0 1.3.0 version, Is anyone can help me?


"
"TITAN X
mobilenet0.25
I run test.py and calc faces, landmarks = detector.detect(img, thresh, scales=scales, do_flip=flip) time

img:640*640  26ms
img:1920*1080  126ms

is this right?
in paper 1920*1080 time is 6.1ms!!!"
""
"Hi, @nttstar,
Can I confirm the following information with you?
MS1MV1 is MS1M-IBUG?
MS1MV2 is MS1M-ArcFace?

Thank you in advance."
"I have prepared my pairs.txt.  like:
name                                                       picID1    picID2
0a5611aa-f909-451d-ad37-bcfe0aaadf0d	7	7
0a5611aa-f909-451d-ad37-bcfe0aaadf0d	9	2
name1                                                        picID1     name2                                                   picID2
0ab60387-a2bf-4ce1-878b-c77f671cdf56	113	0a226558-f0f4-406c-9d7f-1d7fc6aa0ef7	7     3
0a39a7cf-84a3-4bd3-84cf-83154dcbf658	1	0a422ae9-8df6-4edc-bb67-a7c9ff5d9ac9	0
0adf3bd8-f17b-4516-9477-7494f6845fb4	11	0a422ae9-8df6-4edc-bb67-a7c9ff5d9ac9	1

Is this pairs.txt format correct?

And then:
I use lfw2pack.py to create .bin file. 

But when I train the netword, i get this error:
Traceback (most recent call last):
  File ""train_softmax.py"", line 601, in <module>
    main()
  File ""train_softmax.py"", line 598, in main
    train_net(args)
  File ""train_softmax.py"", line 592, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/heisai/.local/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 560, in fit
    callback(batch_end_params)
  File ""train_softmax.py"", line 530, in _batch_callback
    acc_list = ver_test(mbatch)
  File ""train_softmax.py"", line 489, in ver_test
    acc1, std1, acc2, std2, xnorm, embeddings_list = verification.test(ver_list[i], model, args.batch_size, 10, None, None)
  File ""eval/verification.py"", line 282, in test
    _, _, accuracy, val, val_std, far = evaluate(embeddings, issame_list, nrof_folds=nfolds)
  File ""eval/verification.py"", line 181, in evaluate
    np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)
  File ""eval/verification.py"", line 152, in calculate_val
    val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])
  **File ""eval/verification.py"", line 169, in calculate_val_far
    far = float(false_accept) / float(n_diff)
ZeroDivisionError: float division by zero**

"
"I have got 88.3% ( for 1E-6 ) for ResNet-18IR with SpatialTransformer instead of 5points alignment, MobileFaceNet is training now, but about 89% is expected, also about 89.5% was mentioned in the one of the issues here. So, did anyone reproduce MobileFaceNet paper? Thanks!"
"Thanks for the amazing work, could you please provide the mobilenet pretrained models, as saying in your paper."
"进入Evaluation/Megaface根据README下载并解压缩到./data中。
修改了model路径，运行run.sh。
开始生成特征并remove_noise。
然后在运行run_experience.py时候，卡在0.99800然后程序就不动了，有人遇见过吗。"
Can you provide the mobilenetv2 x0.25 model.thank you.
""
"我使用4个1080Ti,每个batch60，那么学习率和学习率下降steps应该设置为多少好呢？
之前我设置的学习率为3e-3，训练了40多个epoch，cfp_fp的精确度在94左右，再也上不去了，我怀疑落入了局部极小。
谢谢各位"
"when i trained the retinaface ,the question occurred"
"Can someone clarify if both datasets can be blindly merge using dataset_merge.oy, i.e. by setting model to None?

Cheers"
"I used ArcFace to train ResNet-18IR nets with (1) cleaned MS-Celeb dataset, (2) the same R-18 net but with VGG2 dataset. Now I see that MegaFace results are completely different, MS Celeb model is about 15% better. Also VGG2 trained model demonstrate very good results for VGG2 test set, but MS-Celeb model fails for VGG2 test set. As it looks for me MS-Celeb and VGG2 sets are different in some parameters. 
I didn't see that behavior for CenterLoss, MSCeleb trained model is better for Megaface (it's clear why), but the same model is also better for VGG2 test set.
Did anyone see something similar? Thanks!"
I can't find any dataset donwload link on [official website](https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/).     And the mapping file about celebrity names to images is which I need. Would anyone share me the file Top1M_MidList.Name.tsv. Sincerely hope for your useful reply.
"Hi,
Is it possible to share RetinaFace-R50 somewhere else?"
"我在LFW上跑了一次测试，1：1和1：N的分值交差较大

1：1上限分值在98.8，集中分布在80到98.8之间
1：N（不同人）上限分值在88.2，集中分布在50到88.2

测试基数是3006人，从结果来看，80到88.2为交差严重的分值，也就是说，如果以3006为基数做1：N比对情况，为达到不误识别，需要将预值至少设为88.2，如果从88.2来算，通过率只有(3006-1079)/3006=64.105123087159015302727877578177%

[人脸识别算法测试报告.xlsx](https://github.com/deepinsight/insightface/files/3110336/default.xlsx)
"
"I follow the tutorial and run with `CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train.py --network r100 --loss arcface --dataset emore`. But it encountered the loss NAN problem.

![image](https://user-images.githubusercontent.com/20432947/56454068-3784f400-637e-11e9-859c-dff9c34120c2.png)
"
"Hi all,

When using the caffemodel in insightface/deploy/mtcnn-model directory, the output of the caffemodel is quite different from the mxnet model.

I'm wondering what tool is used to generate the caffemodel or it is trained directly from caffe? And what might have cause the different output?

Thanks"
"我使用model zoo中提供的mobilefacenet模型转换到ncnn中，在手机端测试（小米8，高通845），cpu 4线程跑的速度需要40ms左右。
mobilefacenet的作者在论文中给的性能指标是在高通820处理器，cpu，4线程可以跑到24ms。
感觉速度差异太大了， 没发现问题在哪里，@nttstar 大佬这个是为啥啊？谢谢"
I want to test data in celebA with your model. So I have to prepare the datasets with same rules. Could you share the cropped CelebA? Or tell me how to crop after detecting with MTCNN.
How can i use the pretrained model to get the same performance on FGNet megaface challenges?
Code does not allow to be run in multi threaded mode....can you share the reason the reason why ?
"这是后面全部的输出，请问有人碰到过这种情况吗？
Done matching! Score matrix size: 3530 1000000
Saving to ../../mx_results/otherFiles/facescrub_megaface_r100ii_1000000_1.bin
Computing test results with 1000000 images for set 1
Loaded 3530 probes spanning 80 classes
Loading from ../../mx_results/otherFiles/facescrub_facescrub_r100ii.bin
Probe score matrix size: 3530 3530
distractor score matrix size: 3530 1000000
Done loading. Time to compute some stats!
/home/asd/Install/insightface/Evaluation/Megaface
"
"Both datasets could be downloaded from this repo ( thanks a lot! ) and form DeepGlint for example, but it contains already aligned faces. Did anyone see not-aligned versions? I have found MS-Celeb, but no success for Asian-Celeb. Thanks!"
""
"self.repeat = 3000000.0/(self.images_per_identity*len(self.id2range))
AttributeError: 'FaceImageIter' object has no attribute 'id2range'"
"Hi,

I am testing using train_triplet to finetune a pre-trained model (LResNet100E-IR,ArcFace@ms1m-refine-v2), however I got an error ` Incompatible attr in node _minus1 at 1-th input: expected [3,512], got [4,512]`.
Does anyone have an idea about how to fix it?

Many thanks

Full error log:
```
use cpu
num_layers 100
image_size [112, 112]
num_classes 1000
Called with argument: Namespace(batch_size=10, ckpt=3, ctx_num=1, cutoff=0, data_dir='/images/training_data_insight', emb_size=512, end_epoch=100000, image_channel=3, image_h=112, image_w=112, images_per_identity=5, loss_type=1, lr=0.005, lr_steps='', max_steps=0, mom=0.9, network='r100', noise_sgd=0.0, num_classes=1000, num_layers=100, per_batch_size=10, prefix='../model/model', pretrained='./models/face-reco-model-r100-ii/model,0', rand_mirror=1, target='lfw,cfp_fp,agedb_30', triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
loading ['./models/face-reco-model-r100-ii/model', '0']
[14:27:08] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v1.2.0. Attempting to upgrade...
[14:27:08] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
INFO:root:loading recordio /images/zaw-photos-1000users-output/insight/training_data_insight/train.rec...
header0 label [19997. 20997.]
id2range 1000
19996
rand_mirror 1
5 2 600
triplet_seq 4994
lr_steps [1000000000]
Traceback (most recent call last):
  File ""src/train_triplet.py"", line 401, in <module>
    main()
  File ""src/train_triplet.py"", line 398, in main
    train_net(args)
  File ""src/train_triplet.py"", line 392, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 498, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 279, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 375, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 662, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 1528, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (10, 3, 112, 112)
softmax_label: (10,)
Error in operator _minus1: [14:27:09] /home/travis/build/dmlc/mxnet-distro/mxnet-build/3rdparty/mshadow/../../src/operator/tensor/../elemwise_op_common.h:133: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node _minus1 at 1-th input: expected [3,512], got [4,512]
```"
"Calculating euclidean distance over millions of vectors is increasing delay in output. From a single second to now over 20 seconds for 1 million data. Is there any algorithm that can reduce the computation time to a couple of seconds for this amount of data ?

Thanks."
Can you share which dataset was used for creating pretrained models ?
"Rencently I've download the raw AgeDB30 and I want use different method to allign them and test my models' performance on it.  Since I don't know your strategy to generate face pairs, I can't compare the performance from MTCNN and my allignment method. Could you please share the pair list?"
"Platform information:

- Python 2.7
- MXNet 1.4

Hi, I met the following error when I ran the `train.py` under `insightface/recognition`. I am sure I use the **recommended dataset MS1M-ArcFace** from the  [baidu source](https://pan.baidu.com/s/1S6LJZGdqcZRle1vlcMzHOQ).

> Traceback (most recent call last):
  File ""train.py"", line 366, in <module>
    main()
  File ""train.py"", line 363, in main
    train_net(args)
  File ""train.py"", line 231, in train_net
    images_filter        = config.data_images_filter,
  File ""/home/liuxx/work/insightface/recognition/image_iter.py"", line 42, in __init__
    header, _ = recordio.unpack(s)
  File ""/home/liuxx/anaconda2/envs/mxnet/lib/python2.7/site-packages/mxnet/recordio.py"", line 416, in unpack
    header = IRHeader(*struct.unpack(_IR_FORMAT, s[:_IR_SIZE]))
TypeError: 'NoneType' object has no attribute '__getitem__'

Then I manually load the dataset:
`>>> imgrec = mx.recordio.MXIndexedRecordIO('datasets/faces_emore/train.idx', 'datasets/faces_emore/train.rec', 'r')`
 `>>> s = imgrec.read_idx(0)`
`>>>s is None`
`True`

The result of `s is None` is **True**. Hence is there something wrong with the dataset?  
"
"## Currently support

- [x] ARM Rockchip RK3288 (GPU)
- [x] ARM Rockchip RK3399 (GPU)
- [x] ARM Samsung 7420 (GPU)

https://github.com/SharpAI/DeepCamera
"
"It was 6 Sep 2018.
Where can I find it?

---

It will be made open source soon.

_Originally posted by @nttstar in https://github.com/deepinsight/insightface/issues/353#issuecomment-418725610_"
"Hi, how i can train model on CPU?"
"```
import numpy as np
import nnvm.compiler
import nnvm.testing
import tvm
from tvm.contrib import graph_runtime
import mxnet as mx
from mxnet import ndarray as nd

ctx = tvm.gpu()
# load the module back.
data_shape = (1,3,112,112)
loaded_json = open(""./model_gpu_tvm_mobile.json"").read()
loaded_lib = tvm.module.load(""./model_gpu_tvm_mobile.so"")
loaded_params = bytearray(open(""./model_gpu_tvm_mobile.params"", ""rb"").read())

input_data = tvm.nd.array(np.random.uniform(size=data_shape).astype(""float32""))

module = graph_runtime.create(loaded_json, loaded_lib, ctx)
module.load_params(loaded_params)

# Tiny benchmark test.
import time
for i in range(100):
   t0 = time.time()
   module.run(data=input_data)
   print(time.time() - t0)
```

```
python3 mobile_model_gpu.py 
Traceback (most recent call last):
  File ""mobile_model_gpu.py"", line 25, in <module>
    module.run(data=input_data)
  File ""/opt/github/tvm/python/tvm/contrib/graph_runtime.py"", line 151, in run
    self._run()
  File ""/opt/github/tvm/python/tvm/_ffi/_ctypes/function.py"", line 185, in __call__
    ctypes.byref(ret_val), ctypes.byref(ret_tcode)))
  File ""/opt/github/tvm/python/tvm/_ffi/base.py"", line 71, in check_call
    raise TVMError(py_str(_LIB.TVMGetLastError()))
tvm._ffi.base.TVMError: [12:44:07] /opt/github/tvm/src/runtime/module_util.cc:53: Check failed: ret == 0 (-1 vs. 0) Assert fail: (dev_type == 1), device_type need to be 1

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/libtvm.so(+0x73859d) [0x7f6f0f47359d]
[bt] (1) /usr/local/lib/libtvm.so(+0xe746f3) [0x7f6f0fbaf6f3]
[bt] (2) /usr/local/lib/libtvm.so(+0xeb3767) [0x7f6f0fbee767]
[bt] (3) /usr/local/lib/libtvm.so(+0xeb1b17) [0x7f6f0fbecb17]
[bt] (4) /usr/local/lib/libtvm.so(TVMFuncCall+0x5e) [0x7f6f0fb991ee]
[bt] (5) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c) [0x7f6f1499ce20]
[bt] (6) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb) [0x7f6f1499c88b]
[bt] (7) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a) [0x7f6f1499701a]
[bt] (8) /usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x9fcb) [0x7f6f1498afcb]
[bt] (9) python3(PyObject_Call+0x47) [0x5c20e7]

```"
I've noticed that resuming with pre-trained weights is disabled in the code. Is this intended to be so or for some other reasons? Thanks!
"Currently I am trying the train_parallel.py and it works very well with commodity gpus. But during training it only outputs the speed, can you add more info to screen, such as acc and loss? thanks"
"Hi, do you have any suggestions on the next problem about facial landmarks:
While training sdu(nadam,lr=0.00025), this is the loss on validation test:
![image](https://user-images.githubusercontent.com/48060406/53564775-d2c0d080-3b68-11e9-8c6e-5c2cda496ac0.png)
Different model on the same training data was fine
Also, while training, lossvalue=nan starts to appear from epoch,depended on lr"
"Just a quick question, according to the paper, ResNet50 trained on CASIA  got 99.08% on LFW.

May I ask the performance of the following combination on LFW?
1. ResNet50 model with BN-Dropout-FC-BN output, trained on MS1MV2, using cross entropy loss only
2. ResNet50 model with BN-Dropout-FC-BN output, trained on MS1MV2, using cross entropy loss + center loss

Since I got lower acc. after adding center loss.... that would be great if I can confirm that is normal.

Thanks in advance!"
"我把这行代码` softmax = mx.symbol.SoftmaxOutput(data=fc7, label = gt_label, name='softmax', normalization='valid')`替换成`focalloss = mx.symbol.Custom(data=fc7, op_type='FocalLoss', labels = gt_label, name='focalloss', alpha=0.25, gamma=2)` 然后换了损伤函数,可是它提示在backword 的时候label越界了,为了方便测试我随便设置了batch_size = 16,num_classes = 717, 然后我就输出了一下label和pred_label,
发现label超过717了,但是我用softmaxloss output的时候label也超过717了,但是没有问题,正常运行,请问是怎么回事啊?
![default](https://user-images.githubusercontent.com/26423386/53401409-b565f800-39ea-11e9-8679-67547451fc8a.png)
![default](https://user-images.githubusercontent.com/26423386/53401562-0118a180-39eb-11e9-8a98-0b5bf9ba57be.png)

"
"I've downloaded face_emore dataset and unpacked it, but it's all in binary.

Any idea how to transfer it back to image file? Thanks."
"Hi,

First of all you guys are doing excellent work. Many thanks for this great work.

I was looking specifically for IJBC evaluation and that brought me to your repository.

I see some information here https://github.com/deepinsight/insightface/tree/master/Evaluation/IJB

and specifically looked at ijbc_face_tid_mid.txt which contains 469,375 entries (corresponds to 469,375 images). I got the specification and protocol files from here http://nigos.nist.gov:8080/facechallenges/IJB-C_Protocols_update.tar.gz which has bunch of CSV and meta data files. However, I could not find ijbc_face_tid_mid.txt or ijbc_name_5pts_score.txt in that bundle.

The images are to be dowloaded separately though after filling a form at NIST but your repository also provides the images (the loose_crop folder).

I am bit puzzled and would appreciate if you could help answer these questions -

a) Did you create/curate ijbc_face_tid_mid.txt and ijbc_name_5pts_score.txt files ?
b) The images in loose_crop folder are the ones to be obtained after filling the form at NIST web site or these images were collected by you.

Many thanks




"
""
"Hello guys,

I try to get only the face feature vectors but when I import the project in Pycharm IDE, I have problem with import face_preprocess, detect_face in /home/galip/PycharmProjects/insightface/src/api/face_model.py. I tried both Python 2.7.12 and 3.5.2.

If I use, /home/galip/PycharmProjects/insightface/deploy/test.py I have problem with import face_model.
In /home/galip/PycharmProjects/insightface/deploy/face_model.py I can not import mtcnn_detector and face_preprocess again.

Could you please tell me how did you get the face vectors? How can I import them?
I put the model-0000.params and model-symbol.json under /model.

Regards."
"```
gpu num: 1
prefix ./models\y1-triplet-emore\model
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=60, ckpt=3, ctx_num=1, dataset='emore', frequent=20, image_channel=3, kvstore='device', loss='triplet', lr=0.005, lr_steps='100000,160000,220000', models_root='./models', mom=0.9, network='y1', per_batch_size=60, pretrained='models/y1-arcface-emore/model', pretrained_epoch=1, rescale_threshold=0, verbose=2000, wd=0.0005) {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 128, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_output': 'GDC', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'loss_name': 'triplet', 'images_per_identity': 5, 'triplet_alpha': 0.3, 'triplet_bag_size': 7200, 'triplet_max_ap': 0.0, 'per_batch_size': 60, 'lr': 0.05, 'net_name': 'fmobilefacenet', 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'triplet', 'network': 'y1', 'num_workers': 1}
loading models/y1-arcface-emore/model 1
in_network {'bn_mom': 0.9, 'workspace': 256, 'emb_size': 128, 'ckpt_embedding': True, 'net_se': 0, 'net_act': 'prelu', 'net_unit': 3, 'net_input': 1, 'net_output': 'GDC', 'net_multiplier': 1.0, 'val_targets': ['lfw', 'cfp_fp', 'agedb_30'], 'ce_loss': True, 'fc7_lr_mult': 1.0, 'fc7_wd_mult': 1.0, 'fc7_no_bias': False, 'max_steps': 0, 'data_rand_mirror': True, 'data_cutoff': False, 'data_color': 0, 'data_images_filter': 0, 'loss_name': 'triplet', 'images_per_identity': 5, 'triplet_alpha': 0.3, 'triplet_bag_size': 7200, 'triplet_max_ap': 0.0, 'per_batch_size': 60, 'lr': 0.05, 'net_name': 'fmobilefacenet', 'dataset': 'emore', 'dataset_path': '../datasets/faces_emore', 'num_classes': 85742, 'image_shape': [112, 112, 3], 'loss': 'triplet', 'network': 'y1', 'num_workers': 1}
INFO:root:loading recordio ../datasets/faces_emore\train.rec...
header0 label [ 5822654.  5908396.]
id2range 85742
5822653
rand_mirror True
5 12 6
triplet_seq 427507
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000, 3, 112, 112)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000, 3, 112, 112)
ver agedb_30
lr_steps [100000, 160000, 220000]
call reset()
eval 7200 images.. 0
triplet time stat [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""C:\Users\**\AppData\Local\Programs\Python\Python36\lib\threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""C:\Users\**\AppData\Local\Programs\Python\Python36\lib\threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\**\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\io.py"", line 401, in prefetch_func
    self.next_batch[i] = self.iters[i].next()
  File ""D:\projects\python\insightface\recognition\triplet_image_iter.py"", line 480, in next
    self.reset()
  File ""D:\projects\python\insightface\recognition\triplet_image_iter.py"", line 391, in reset
    self.select_triplets()
  File ""D:\projects\python\insightface\recognition\triplet_image_iter.py"", line 254, in select_triplets
    self.mx_model.forward(db, is_train=False)
  File ""C:\Users\**\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\module.py"", line 591, in forward
    assert self.binded and self.params_initialized
AssertionError

C:\Users\**\AppData\Local\Programs\Python\Python36\lib\site-packages\mxnet\module\base_module.py:504: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.016666666666666666). Is this intended?
  optimizer_params=optimizer_params)

```"
"Dear @yingfeng & @nttstar,
Thank you for your nice work. I have one question about the released face recognition pre-trained models on the [Model Zoo](https://github.com/deepinsight/insightface/wiki/Model-Zoo) section. Your pre-trained models trained on which data set? (i.e., MegaFace or LFW or MS1M)
"
""
"I am running the age and gender model with the default inference code.

If I call the model repeatedly; I get the error (intermittently) -> 'NoneType' object has no attribute '__getitem__' during inference.

Any idea why this is happening? 
```
 def inference(self, model_input):
        """"""
        Internal inference methods for MXNet. Run forward computation and
        return output.

        :param model_input: list of NDArray
            Preprocessed inputs in NDArray format.
        :return: list of NDArray
            Inference output.
        """"""
        if self.error is not None:
            return None

        # Check input shape
        check_input_shape(model_input, self.signature)
        model_input = [item.as_in_context(self.mxnet_ctx) for item in model_input]
        print(""model_input --"",model_input)
        self.mx_model.forward(DataBatch(model_input))

        model_input = self.mx_model.get_outputs()
        print(""model_input -- "", model_input)
        # by pass lazy evaluation get_outputs either returns a list of nd arrays
        # a list of list of NDArray
        for d in model_input:
            if isinstance(d, list):
                for n in model_input:
                    if isinstance(n, mx.ndarray.ndarray.NDArray):
                        n.wait_to_read()
            elif isinstance(d, mx.ndarray.ndarray.NDArray):
                d.wait_to_read()
        return model_input
```"
"when I changed as below , I got this error, is there anybody can help me to solve this error?
`loss.combined = edict()
loss.combined.loss_name = 'combined'
loss.combined.loss_s = 64.0
loss.combined.loss_m1 = 1.0
loss.combined.loss_m2 = 0.3
loss.combined.loss_m3 = 0.2

default.loss = 'combined'
`"
"Thank you for share.

I have used this insightface code and pretrained params([LResNet100E-IR,ArcFace@ms1m-refine-v2](https://github.com/deepinsight/insightface/wiki/Model-Zoo)) you share to fine-tune on [DeepGlint](https://github.com/deepinsight/insightface/wiki/Dataset-Zoo),
but the training acc is stayed at about 0.63, stoping increase.

I use 
--per-batch-size 50 on 3 GPU
--emb-size 512
gpu num: 3
num_layers 100
image_size [112, 112]
num_classes 180855
Called with argument: Namespace(batch_size=150, beta=1000.0, beta_freeze=0, beta_min=5.0, bn_mom=0.9, ce_loss=False, ckpt=2, color=0, ctx_num=3, cutoff=0, data_dir='../data/faces_glint', easy_margin=0, emb_size=512, end_epoch=100000, fc7_lr_mult=1.0, fc7_no_bias=False, fc7_wd_mult=1.0, gamma=0.12, image_channel=3, image_h=112, image_size='112,112', image_w=112, images_filter=0, loss_type=4, lr=5e-06, lr_steps='50000,100000,150000,200000', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, max_steps=0, mom=0.9, network='r100', num_classes=180855, num_layers=100, per_batch_size=50, power=1.0, prefix='../ckpt/glint_faces_back2/glint_faces', pretrained='../ckpt/glint_faces_back2/glint_faces,46', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_multiplier=1.0, version_output='E', version_se=0, version_unit=3, wd=1e-05)
loading ['../ckpt/glint_faces_back2/glint_faces', '46']
[19:14:22] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
init resnet 100
0 1 E 3 prelu

I tryed decreased learning rate(even to 1e-5), decreased weight decay(wd, to 5e-6), but the training acc is still stay at about 0.63?

I wonder whether the clsss num 180855 is too large for the embedding(512) to map to?

So can you help me check it, thank you very much. "
""
"你好，首先感谢您开源的代码。
我的问题比较大：
在使用softmax训练时，在vgg数据集上我的acc可以做到从0增大，ce-loss从40开始减小，我认为这非常好
当使用我自己的数据集时，ce-loss初始值在8-11左右（我认为这个loss值很小，不是很正常）当跑完大概12000个batch后，acc依然为0，ce-loss始终保持在5-8左右震荡。
我的数据集大小为57733类*210张图片，其中大多数图片为增强的图片。
我想请问我的ce-loss是否正常，或者针对我的数据集应该做参数的调整？
非常感谢您！
![image](https://user-images.githubusercontent.com/44387104/51320883-32bd5500-1a9c-11e9-957e-d691ad7614ba.png)
上面是我的loss图"
"Hi all, I hope someone tried this. I have trained ResNet-18 with Softmax, Centerloss and finally ArcFace. VGG2 was used as training data, no alignment step has been done because we use another face detection approach. VGG2 test results are the following: 68% for SoftMax, about 89% for CenterLoss ( more than 20% boost) and surprisingly only 70% for ArcFace. I guess this caused because alignment step is missed and because of some reasons this step is extremely important for ArcFace and not so important for CenterLoss. But it's just an assumption. Does anyone have experience in training with no aligned images?
Thank you!"
"In the dataset zoo ,the train-cele dataset size is 30G ，but the first release in #256 is 90G ，is there any difference?"
"May I ask the settings of training iterations:
- In `train_softmax.py`, I think `mbatch = param.epoch*batch_iteration_per_epoch + param.nbatch`. 
- For softmax loss, `lr_steps=[40000,60000,80000]` convert to epoch, means to decay approximately at `[5.3, 8.1, 10.7]`. I do not know whether I calculate correctly: `40000*512/(3.8*10**6)=5.3`. Here 5.3 rather than 5, means the model decays at middle of 5-th epoch. 
- For other angular margin-based losses, since they are harder, `lr_steps=[100000,140000,160000]`. Converting to epoch, they decay at `[13.4, 18.6,21.55]`. 
- However, I observe that in the log, the model decays at epoch=[6.6,10.6, 13.3] and finishes at epoch 14.9. 

Could you help me check whether my understanding is correct? And may I ask whether the softmax loss can achieve better performance, given longer training time as other margin-based loss?

Thank you very much for this great work!  
"
"hello, thanks for your great work. I want to finetune my model by triplet loss,but I have difficulty in trainning network. Can you tell me how to print lossvalue by train.py or trian_triplet.py?Thanks a lot."
"hi thank you for your nice project

as i noticed theres a bar next to the bbox 
what is the meaning of it ?"
"Hi,  I have a question that I want to use train model with two loss，how can I set different loss-weights for different loss-types like caffe?"
"i.e. the emore use the same transformation or standard 5 landmarks with celebrity and msra?
Thanks."
"Hi,
align_insight.py (https://github.com/deepinsight/insightface/blob/master/src/align/align_insight.py) does mention VGGFACE2, but it has no alignment steps. 

Clearly, the code only detect faces but not use the landmarks.

So given the bounding boxes and landmarks, where can I find the alignment process of VGGFACE2?"
"hello, in your code, I have seen several line codes like this:
      if mbatch<=args.beta_freeze:
        _beta = args.beta
      else:
        move = max(0, mbatch-args.beta_freeze)
        _beta = max(args.beta_min, args.beta*math.pow(1+args.gamma*move, -1.0*args.power))
      os.environ['BETA'] = str(_beta)
And I discover that the same parameter beta has also appear on Lsoftmax, but I don't know what their usage, Could you please help me solve this problem? thank you very much."
"Is there any body tested inference mxnet+nnpack ? any improvement ?

is it worth to try for improve inference speed?"
"I have tested the pretrained LResNet50E-IR mxnet model on my own dataset and it works very well. However, when I use the same mtcnn and alignment method, then feed the aligned image to the pretrained caffe model. The embeddings looks completely different compared with the embeddings of the same image from the mxnet model. And the distance between different people's images is not much larger than the same people's images, and sometimes it's even smaller.

I'm wondering if there is any difference between those two models, maybe they need different input channel order, different normalization method? And another question, is the transformed caffe model supposed to have the exact same output as the mxnet model?

Does anyone has anyone idea what may have caused this?
Thanks"
"In the readme file, download links to aligned and cropped ms1m and vgg2 are provided.

I'm wondering if those 112x112 aligned images given are used directly to train the model or not. I think during inference, after mtcnn procedure, a warp process (using original image and 5 landmark points) is necessary for the model to have a good performance. But the training set description never mentioned this and says any mtcnn will work. So I'm not sure if those data are warped or not.

Can anyone explain this to me.
Much appreciated."
"given the landmark

```
[[35.916126 43.42934 ]
 [86.2129   49.016266]
 [55.2765   71.023384]
 [32.937046 87.26657 ]
 [72.59505  91.47491 ]]
```

and the python code

```
tform = trans.SimilarityTransform()
M=tform.estimate(dst, src)
print (M)
```

the matrix is

```
[[ 0.80088555  0.12822719  1.92989341]
 [-0.12822719  0.80088555 24.34255532]
 [ 0.          0.          1.        ]]
```

but in c++ code (from https://github.com/deepinsight/insightface/blob/master/cpp-align/FacePreprocess.h)

```
  float v1[5][2] = {
      {30.2946f, 51.6963f},
      {65.5318f, 51.5014f},
      {48.0252f, 71.7366f},
      {33.5493f, 92.3655f},
      {62.7299f, 92.2041f}};

  cv::Mat src(5,2,CV_32FC1, v1);

  memcpy(src.data, v1, 2 * 5 * sizeof(float));

  float v2[5][2] = 
  {{35.916126, 43.42934 },
   {86.2129 , 49.016266},
   {55.2765 , 71.023384},
   {32.937046,87.26657 },
   {72.59505, 91.47491 }};

  cv::Mat dst(5,2,CV_32FC1, v2);

  memcpy(dst.data, v2, 2 * 5 * sizeof(float));
  cv::Mat m = FacePreprocess::similarTransform(src, dst);

  std::cout << m << std::endl;

```

what I got is 

```
[1.1905804, -0.19061996, 13.114243;
  0.19061995, 1.1905805, -26.316315;
  0, 0, 1]
```

and apparently the image alignment result is wrong in c++.
 
Any idea?"
"Since the batch size is always 1 when testing megaface (https://github.com/deepinsight/insightface/blob/master/src/megaface/gen_megaface.py), is there any reason for setting such small batch size? "
"I try to train my own dataset, it comes such problem.
![screenshot from 2018-12-18 09-14-05](https://user-images.githubusercontent.com/22907560/50126119-8aeb1680-02a6-11e9-9a66-ecf1e6e4deba.png)
Anybody knows why?"
could you tell me where is facenet  ?
"Hi, @nttstar . Very nice work!  I am a newbie for face recognition.  I've fine-tuned my own dataset with pre-trained models. Now I want to get the rank1 accuracy of my model on the validation set, but I only found accuracy and VR@FAR in src/eval/verification.py. So how to get the result of  rank-1 by your code?
I look forward to your reply, thank you very much."
"Dear insightface: 
I have tried python2/python3 and mxnet-cu80/mxnet-cu90. But I cannot reproduce the results in <https://github.com/deepinsight/insightface/blob/master/Evaluation/IJB/IJBC_Evaluation_MS1MV2.ipynb>. 

After running through the notebook, my ROC curve is somewhat strange: 
![image](https://user-images.githubusercontent.com/14788650/49707221-63bc9580-fc65-11e8-8e75-236b217e9725.png)
![image](https://user-images.githubusercontent.com/14788650/49707234-70d98480-fc65-11e8-952d-a658a3155633.png)

If I directly use the downloaded results file (*.npy), rather than the file generated by myself, the ROC curve is normal. 
![image](https://user-images.githubusercontent.com/14788650/49707351-e9d8dc00-fc65-11e8-9a61-6c20a537e0f0.png)

I also investigate the features at images level, the cluster pattern can be observed, which means that the extracted features of the starting 128 images are correct.
![image](https://user-images.githubusercontent.com/14788650/49707467-5653db00-fc66-11e8-80cf-d36099cde518.png)

When reproducing, 
I modify `similarity_score = np.sum(feat1 * feat2, -1)` to `similarity_score = np.sum(feat1 * feat2, -1).flatten()`. For python2, I do not change anything else; for python3, I modifies  
`img_input_feats = img_feats[:,0:img_feats.shape[1]/2]` to `img_input_feats = img_feats[:,0:img_feats.shape[1]//2]`. 

Could you help me find out the problem? 
Thanks a lot! 
"
"Hi,

How to get the confidence value of age and gender"
"@nttstar 
the final layer of the model you provided in the link is only the embedding layer. Looks like it can bot be used directly to predict the age and gender. 
Would you mind to post the original model that we can try?
Or where to get the train data for this to train?

Best
"
"Evaluation/IJB/ 中的代码能提供下国内的下载地址吗, 或者把tar包分割下，dropbox上下载经常断 @nttstar "
""
"As I see, the intra loss is update in the code, and for which I know that it is to constrained the same class's cos distance as little as possible. But the Softmax layer also do the same thing. I am curious of the effect of the intra loss. Does anyone use that loss to do experiment?"
"Dear authors, thanks a lot for your amazing work.
I have one question about your backbone network, it's related to bias term in convolution layer.
I noticed that you set ""no_bias=True"" in all the backbone networks, but generally bias term has been widely adopted, do you have any special considerations in your work?"
"train command :  --network y1  --emb-size 512     model size:225,333KB
after remove the fc layer params by deploy/model_slim.py, the model size reduce to 53,849KB

train command :  --network y1  --emb-size 128     model size:16,209KB
after remove the fc layer params by deploy/model_slim.py, the model size reduce to 16,209KB, which is still much bigger than 4M

What can I do now? Thanks!
"
"Hello, at the beginning of using two gpus, the training speed of per_batch_size=256 can reach 800 samples/ SEC, but when the computer is restarted, the training speed of four gpus, per_batch_size=256, is reduced to less than 200 samples/ SEC, the GPU call rate is sometimes 100%, sometimes 0%, resulting in the slow training now, do you know what is the reason?"
""
"Hi, u really did a great work.  I have a question about SphereFace in figure3. Are the axises  adverse?
For example, give a point（theta1=30, theta2=0） in vertical axis, it should be classified to class2, but in figure, it is classified to class1. The same question also exist in ArcFace.
Suppose you can read Chinese.
![image](https://user-images.githubusercontent.com/9246739/48893791-19bd6c80-ee7c-11e8-8a3c-03e298dfbcea.png)
"
"hello,I have trained SE-Res50 with ms1m dataset, but I want to finetune the model of "" SE-Res50"" with the ""ms1m+vgg2"" dataset. I try to merge ms1m dataset and vgg2 dataset some times by the function of ""dataset_merge.py"", but i cannot merge datasets successfully.Can you help me or share your dataset of ""ms1m+vgg2""?Thanks very a lot.
What's more, I merged our datasets by the function of ""dataset_merge.py"", the error appeared as following:""Traceback (most recent call last):
  File ""/home/algorithm-5/insightface/src/data/dataset_merge.py"", line 293, in <module>
    main(args)
  File ""/home/algorithm-5/insightface/src/data/dataset_merge.py"", line 128, in main
    assert header.flag>0
AssertionError""
  What is meanings of ""  for ds_id in xrange(len(rec_list)):
    id_list = []
    imgrec = rec_list[ds_id]
    s = imgrec.read_idx(0)
    header, _ = mx.recordio.unpack(s)
    assert header.flag>0
    print('header0 label', header.label)
    header0 = (int(header.label[0]), int(header.label[1]))
    #assert(header.flag==1)
    imgidx = range(1, int(header.label[0]))
    id2range = {}
    seq_identity = range(int(header.label[0]), int(header.label[1]))""?
Thanks!"
I need all the contents.
"Thank you for once again.I have trained se-res50 with ms1m dataset, then I want to train vgg dataset and  treat  the model of ""se-res50-ms1m"" as pretrained model ,but the error appear. How to I should modify the code and the error is:
""   File ""/usr/lib/python2.7/site-packages/mxnet/module/module.py"", line 297, in _impl
    cache_arr.copyto(arr)
  File ""/usr/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 2066, in copyto
    return _internal._copyto(self, out=other)
  File ""<string>"", line 25, in _copyto
  File ""/usr/lib/python2.7/site-packages/mxnet/_ctypes/ndarray.py"", line 92, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/usr/lib/python2.7/site-packages/mxnet/base.py"", line 252, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [13:54:26] /home/travis/build/dmlc/mxnet-distro/mxnet-build/3rdparty/mshadow/../../src/operator/tensor/../elemwise_op_common.h:133: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [85164,512], got [8631,512]

Stack trace returned 10 entries:
[bt] (0) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x36bac2) [0x7f6bd78d2ac2]
[bt] (1) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x36c0a8) [0x7f6bd78d30a8]
[bt] (2) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x548e23) [0x7f6bd7aafe23]
[bt] (3) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x54970b) [0x7f6bd7ab070b]
[bt] (4) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2fa4692) [0x7f6bda50b692]
[bt] (5) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2fae34e) [0x7f6bda51534e]
[bt] (6) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2ecc3ab) [0x7f6bda4333ab]
[bt] (7) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f6bda43396f]
[bt] (8) /lib64/libffi.so.6(ffi_call_unix64+0x4c) [0x7f6c07219dcc]
[bt] (9) /lib64/libffi.so.6(ffi_call+0x1f5) [0x7f6c072196f5]
"""
"作者您好！
我按照您的步骤和代码，
利用Model Zoo 中的“ 3.2 LResNet50E-IR,ArcFace@ms1m-refine-v1（model-r50-am-lfw）” 预训练模型，
利用Dataset Zoo中的“ MS1M-IBUG (85K ids/3.8M images) [5,6]（faces_ms1m_112x112）” 数据集，
运行命令参数为：
CUDA_VISIBLE_DEVICES='0,1' python -u /data/src/train_softmax.py --network r50 --per-batch-size 64 --loss-type 4 --ckpt 2  --margin-m 0.5 --pretrained '/data/ckpt/model-r50-am-lfw/model,0' --data-dir /data/datasets/faces_ms1m_112x112 --prefix /data/ckpt/arcface/arcface-r50-s
进行训练。
但是我在日志中打印的acc的结果，在跑完3Epoch后：
lr-batch-epoch: 0.1 10824 3
testing verification..
(12000, 512)
infer time 17.748569
[lfw][100000]XNorm: 21.851122
[lfw][100000]Accuracy-Flip: 0.97883+-0.00573
testing verification..
(14000, 512)
infer time 20.685041
[cfp_fp][100000]XNorm: 17.821963
[cfp_fp][100000]Accuracy-Flip: 0.75800+-0.02343
testing verification..
(12000, 512)
infer time 17.510608
[agedb_30][100000]XNorm: 21.715151
[agedb_30][100000]Accuracy-Flip: 0.89000+-0.02486
saving 50
[100000]Accuracy-Highest: 0.90500

### acc on lfw 基本在0.97 左右徘徊。
### acc on cfp_fp 基本在0.75 左右徘徊。
### acc on agedb_30 基本在0.89 左右徘徊。

请问一下，是不是我那一不对？还是有哪些细节没看到导致无法复现：
Mode Zoo 中 3.2中的table的 Performance:
Method | LFW(%) | CFP-FP(%) | AgeDB-30(%) | MegaFace(%)
Ours     | 99.80      | 92.74          | 97.76              | 97.64

希望得到您的回复！
谢谢！
"
"I just want to know anyone who has try to use MTCNN net to train? the nets in insight-master don't contain the MTCNN!
"
"hi,thanks for your sharing.I want to set ""ls-steps"" value by myself and trained res50 with ms1m dataset but the error appeared in running.the command lines settings and the error are respectively as following:
""CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --network r50 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112 --per-batch-size 8 ---lr-steps [80000,150000,200000] --prefix ../model-r50""

""ver agedb_30
Traceback (most recent call last):
  File ""train_softmax.py"", line 502, in <module>
    main()
  File ""train_softmax.py"", line 499, in main
    train_net(args)
  File ""train_softmax.py"", line 427, in train_net
    lr_steps = [int(x) for x in args.lr_steps.split(',')]
ValueError: invalid literal for int() with base 10: '[80000'
""
Can you help me? Thanks a lot."
""
"Hi, Sorry to bother you. I encountered the following problem when merging of the datasets by ""dataset_merge.py"".Now I have downloaded of the .rec and .idx files of ms1m and vgg datasets,meanwhile the model of ""model-r50-am-lfw"" have been downloaded,but i cannot merge datasets successfully.Can you help me?Thanks very a lot.
Commanding lines setting as following:
python dataset_merge.py --include /datasets/faces_ms1m_112x112,/datasets/faces_vgg_112x112 --output /datasets/ms1m+vgg --model ./model-r50-am-lfw/model,0 --param1 0.7 --gpu 0 --batch-size 32"
""
""
"- Gender-age fine-tuned model is about 120M. How to reduce size of the model size?
- How to save the best model during training automatically?"
"when i unzip the data,'bad CRC 53fbb04f  (should be c18790d6)' occured. Why?"
"Hi, 
How to do face color normalization during training? I did not find the corresponding codes in the class of `FaceImageIter`. The Arcface paper mentioned as ""each pixel (ranged between [0, 255]) in RGB images is normalised by subtracting 127.5 then divided by 128"". 

Does anyone know the answer? 

Thank you.
"
"Hi all,

I am trying to apply this repository to a server side face register and recognition service. I have tried to detect faces and generate embeddings for all the photos in a directory (contains 300 photos) and it worked fine. However, when I attach it to local server code, it can only do a single face detection and embedding generation. Once a second detection is called, it raises an error.

my configuration is CUDA 9.0, mxnet 1.3.0, cudnn7, python2.7

Detailed error messages here:

File ""server.py"", line 118, in login
    login_res, message = face_verification(file_path, regis_path, username)
  File ""server.py"", line 14, in face_verification
    result, data = server_function.verify(embedding_dir, photo_dir, login_id)
  File ""/home/wenbin/project/mxnet_faceID/server_function.py"", line 88, in verify
    img_tmp = model.get_input(image)
  File ""/home/wenbin/project/mxnet_faceID/face_model.py"", line 71, in get_input
    ret = self.detector.detect_face(face_img, det_type = self.args.det)
  File ""/home/wenbin/project/mxnet_faceID/mtcnn_detector.py"", line 493, in detect_face
    output = self.LNet.predict(input_buf)
  File ""/home/wenbin/.local/lib/python2.7/site-packages/mxnet/model.py"", line 717, in predict
    o_list.append(o_nd[0:real_size].asnumpy())
  File ""/home/wenbin/.local/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 1894, in asnumpy
    ctypes.c_size_t(data.size)))
  File ""/home/wenbin/.local/lib/python2.7/site-packages/mxnet/base.py"", line 210, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
MXNetError: [11:00:54] src/operator/nn/./cudnn/cudnn_convolution-inl.h:156: Check failed: e == CUDNN_STATUS_SUCCESS (7 vs. 0) cuDNN: CUDNN_STATUS_MAPPING_ERROR

Stack trace returned 10 entries:
[bt] (0) /home/wenbin/mxnet/lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f1372ee4dcb]
[bt] (1) /home/wenbin/mxnet/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f1372ee5938]
[bt] (2) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::op::CuDNNConvolutionOp<float>::Forward(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x389) [0x7f1377346829]
[bt] (3) /home/wenbin/mxnet/lib/libmxnet.so(void mxnet::op::ConvolutionCompute<mshadow::gpu>(nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0xbfc) [0x7f137733bbec]
[bt] (4) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::exec::FComputeExecutor::Run(mxnet::RunContext, bool)+0x59) [0x7f13754883f9]
[bt] (5) /home/wenbin/mxnet/lib/libmxnet.so(+0x317c8d3) [0x7f13754348d3]
[bt] (6) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x8e5) [0x7f1375a92185]
[bt] (7) /home/wenbin/mxnet/lib/libmxnet.so(void mxnet::engine::ThreadedEnginePerDevice::GPUWorker<(dmlc::ConcurrentQueueType)0>(mxnet::Context, bool, mxnet::engine::ThreadedEnginePerDevice::ThreadWorkerBlock<(dmlc::ConcurrentQueueType)0>*, std::shared_ptr<dmlc::ManualEvent> const&)+0xeb) [0x7f1375aa931b]
[bt] (8) /home/wenbin/mxnet/lib/libmxnet.so(std::_Function_handler<void (std::shared_ptr<dmlc::ManualEvent>), mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)::{lambda()#3}::operator()() const::{lambda(std::shared_ptr<dmlc::ManualEvent>)#1}>::_M_invoke(std::_Any_data const&, std::shared_ptr<dmlc::ManualEvent>&&)+0x4e) [0x7f1375aa958e]
[bt] (9) /home/wenbin/mxnet/lib/libmxnet.so(std::thread::_Impl<std::_Bind_simple<std::function<void (std::shared_ptr<dmlc::ManualEvent>)> (std::shared_ptr<dmlc::ManualEvent>)> >::_M_run()+0x4a) [0x7f1375a9178a]

[11:00:54] src/resource.cc:262: Ignore CUDA Error [11:00:54] src/storage/./pooled_storage_manager.h:85: CUDA: an illegal memory access was encountered

Stack trace returned 10 entries:
[bt] (0) /home/wenbin/mxnet/lib/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x5b) [0x7f1372ee4dcb]
[bt] (1) /home/wenbin/mxnet/lib/libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x28) [0x7f1372ee5938]
[bt] (2) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::storage::GPUPooledStorageManager::DirectFreeNoLock(mxnet::Storage::Handle)+0x95) [0x7f1375ab5815]
[bt] (3) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::storage::GPUPooledStorageManager::DirectFree(mxnet::Storage::Handle)+0x3d) [0x7f1375ab81bd]
[bt] (4) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::StorageImpl::DirectFree(mxnet::Storage::Handle)+0x68) [0x7f1375ab1418]
[bt] (5) /home/wenbin/mxnet/lib/libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::resource::ResourceManagerImpl::ResourceTempSpace::~ResourceTempSpace()::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext&&)+0xff) [0x7f1375b8090f]
[bt] (6) /home/wenbin/mxnet/lib/libmxnet.so(+0x37dfe01) [0x7f1375a97e01]
[bt] (7) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::engine::ThreadedEngine::ExecuteOprBlock(mxnet::RunContext, mxnet::engine::OprBlock*)+0x8e5) [0x7f1375a92185]
[bt] (8) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::engine::ThreadedEnginePerDevice::PushToExecute(mxnet::engine::OprBlock*, bool)+0x65) [0x7f1375aad085]
[bt] (9) /home/wenbin/mxnet/lib/libmxnet.so(mxnet::engine::ThreadedEngine::PushAsync(std::function<void (mxnet::RunContext, mxnet::engine::CallbackOnComplete)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, mxnet::FnProperty, int, char const*, bool)+0x1b0) [0x7f1375a98400]

Any help would be appreciated!"
"Hi guys..
seriously thanks for your efforts and thanks for sharing.. 

but please guys.. the code is a mess..
no oop and design patterns at all..
please try to refactor the all project"
"I had a strange behaviour with some pictures.

With mxnet 1.3.x  below face give error in model.get_input(img).

but mxnet 1.2.1.post1  model.get_input(img) is working without any problem.

Do we need to use specific version of the mxnet . ?

![barry_alvarez_0001](https://user-images.githubusercontent.com/17505439/47608212-c437a080-da32-11e8-8bc6-edfb3149a631.jpg)
"
"I try to train a model with r50 on 'faces_emore' dataset and set --emb-size=1024;
firstly, I train a new model with softmax, and secondly, I finetune it with arcloss; but got a problem like:
mxnet.base.MXNetError: [09:58:00] src/operator/nn/../tensor/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [1024,512], got [1024,25088]

How to fix the problem?
thank you!"
"{
	""cmc"" : 
	[
		[ 0, 275, 1000000 ],
		[ 0.9857323169708252, 0.9957365989685059, 1.0 ]
	],
	""roc"" : 
	[
		
		[
			0.0,
			9.999999939225290e-09,
			1.999999987845058e-08,
			5.999999785899490e-08,
			2.799999947455944e-07,
			0.0006921099848113954,
			1.0
		],
		
		[
			0.8836455345153809,
			0.9026387333869934,
			0.9641355276107788,
			0.9759168028831482,
			0.9859926700592041,
			0.9960035085678101,
			1.0
		]
	],
	""traditional_cmc"" : 
	[
		[ 0, 182, 1000079 ],
		[ 0.9860869646072388, 0.9961445331573486, 0.9999974966049194 ]
	]
}
how can i get VR@FAR10^6(because no FAR1e-6 in roc block)?
thx

_Originally posted by @cavalleria in https://github.com/deepinsight/insightface/issues/251#issuecomment-408306183_"
"Could you share  the raw data set of IQIYI_VID since the download is shut down? Thanks.
"
"Hi,
      I want to download your refined dataset from google drive because in china BaiduYun is so slow. But the google drive URL is not available!
      
![image](https://user-images.githubusercontent.com/26196355/47295474-4ba39f00-d642-11e8-8b93-94ac9c9c5f47.png)
Best wishes,
Minda."
"I use mtcnn to detect face and face_preprocess.py to align, then use your pre-trained model 34e to do verification. But the accuracy I got is only 95%. Your readme.md mentioned that the accuracy should be 99%. Can you please tell me the parameters you set when you do the verification test, such as verification threshold, det_factor, det_minisize, det_threshold and det_type? Please help me. 
Best wishes!"
"Hello,
In your ReadMe.md, in the section ""Pretrained Models"", you say that ""You can use $INSIGHTFACE/src/eval/verification.py to test all the pre-trained models.""

When I look inside this verification.py, it uses this code to compute the score:
diff = np.subtract(embeddings1, embeddings2)
dist = np.sum(np.square(diff),1)

This score is not adapted to SphereFace, CosineFace, ArcFace. For these loss functions, we must use embeddings1.dot_product(embeddings2).

Looking at the header of the verification.py, I see that it is written by David Sandberg - the author of facenet (https://github.com/davidsandberg/facenet). In facenet, I remember that they use TripletLoss and then the score = sum_of_square_of_difference makes sense..

In one of pre-trained model (https://pan.baidu.com/s/1ENjcACInLfBGHZ8e7Fc-XA), I see that the file name is arcface50-caffe (the term ""arcface"" rings me a bell :) ). Then I guess if I use this trained model, I have to use dot_product to compute the distance between 2 embeddings.

Am I wrong ?

Thanks !"
"Yeah, I developed a small FR system based on Insightface. But lately, for few times when I run the system to recognize self-collected images, the system encounter an error. It seems there is something wrong with the input images, especially for those side faces or low-resolution faces. Is that normal for Insightface ? 
 Come and help me ,who is just a sophomore. Thanks a lot !!!

Below are the console information of my python project:
Traceback (most recent call last):
  File ""E:/xiang/Moon/face_platform/gui/recognize_faces.py"", line 37, in <module>
    form.append(model.get_feature(model.get_input(image)))
  File ""E:\xiang\Moon\face_platform\Insightface\face_model.py"", line 114, in get_feature
    self.model.forward(db, is_train=False)
  File ""D:\Python352\lib\site-packages\mxnet\module\module.py"", line 610, in forward
    self.reshape(new_dshape, new_lshape)
  File ""D:\Python352\lib\site-packages\mxnet\module\module.py"", line 471, in reshape
    self._exec_group.reshape(self._data_shapes, self._label_shapes)
  File ""D:\Python352\lib\site-packages\mxnet\module\executor_group.py"", line 382, in reshape
    self.bind_exec(data_shapes, label_shapes, reshape=True)
  File ""D:\Python352\lib\site-packages\mxnet\module\executor_group.py"", line 358, in bind_exec
    allow_up_sizing=True, **dict(data_shapes_i + label_shapes_i))
  File ""D:\Python352\lib\site-packages\mxnet\executor.py"", line 402, in reshape
    arg_shapes, _, aux_shapes = self._symbol.infer_shape(**kwargs)
  File ""D:\Python352\lib\site-packages\mxnet\symbol\symbol.py"", line 990, in infer_shape
infer_shape error. Arguments:
  data: (1,)
    res = self._infer_shape_impl(False, *args, **kwargs)
  File ""D:\Python352\lib\site-packages\mxnet\symbol\symbol.py"", line 1120, in _infer_shape_impl
    ctypes.byref(complete)))
  File ""D:\Python352\lib\site-packages\mxnet\base.py"", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: Error in operator conv_1_conv2d: [19:18:09] C:\Jenkins\workspace\mxnet-tag\mxnet\src\operator\nn\convolution.cc:143: Check failed: dshp.ndim() == 4U (1 vs. 4) Input data should be 4D in batch-num_filter-y-x

Process finished with exit code 1
"
"Hi,

We have small budget to use:

We need to use model from C++ and extract futures like in test.py

Is there anyone who expert on the both C++ and mxnet to spare a few hours?

Best
"
"I see ./alignment and ./src/align both, and  lots of align_xxx.py files. 
If I need use my own dataset to align and get feature, which method can I use?
Thanks."
"Hi,

First of all thanks for you effort and such a good work.

Our base is mainly c++ and can we use the your pre-trained  model-r50-am-lfw model from c++ ?

such as 👍 

1 - how we can load it to c++  (saw some mxnet examples but looks complicated)
2 - we just want to extract futures and compar from c++

an the MOST importantly :  is it POSSIBLE :)

best"
"您好！我运行SSH/test.py，出现一下错误：
Traceback (most recent call last):
  File ""test.py"", line 6, in <module>
    from ssh_detector import SSHDetector
  File ""/home/fll/work/Face recognition project/Age_Estimation/insightface/SSH/ssh_detector.py"", line 10, in <module>
    from rcnn.processing.bbox_transform import nonlinear_pred, clip_boxes
  File ""/home/fll/work/Face recognition project/Age_Estimation/insightface/SSH/rcnn/processing/bbox_transform.py"", line 2, in <module>
    from ..cython.bbox import bbox_overlaps_cython
ImportError: No module named bbox

如何解决呢，谢谢！
"
"我将glint数据集进行清洗,得到了rec和idx文件, 然后想要将该数据集和其他数据集(已经有train.rec和train.idx)进行合并.
调用dataset_merge.py,报错:

File ""dataset_merge.py"", line 292, in <module>
    main(args)
  File ""dataset_merge.py"", line 125, in main
    s = imgrec.read_idx(0)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/recordio.py"", line 264, in read_idx
    self.seek(idx)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/recordio.py"", line 226, in seek
    pos = ctypes.c_size_t(self.idx[idx])
KeyError: 0

意思是我在glint数据集上生成的rec文件,没有0这个idx.
请问是我调用face2rec2.py出错还是其他问题?

万分感谢!"
"I have a mxnet version implementation of SSR-Net for age and gender Estimation,train on megaage-asia,model size just 170kb. https://github.com/wayen820/gender_age_estimation_mxnet"
"I found cfp_fp.bin and cfp_ff.bin in emore different from the previous version in MSv1. They get higher scores at one of my models than the previous cfp. What is changed therein, please?"
"Hi 

when I tried recognition . test.py I couldt find the 3d_I5-symbol.json model.

where can I download?"
"Hi 

Is there  c++ version feature extract and prediction ?

how we can use mxnet c++ with your model both feature extract and prediction?"
"I have build mxnet manually by the following steps:

```
git clone --recursive https://github.com/apache/incubator-mxnet
cd incubator-mxnet
make -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1 USE_GPERFTOOLS=0 USE_JEMALLOC=0 USE_OPENCV=1
sudo pip3 install -e.
```

But after running a problem for face embedding generation using SSH and Embedding model, I have found the following error 

*** Error in `/usr/bin/python3.5': free(): invalid pointer: 0x00007f3db820ede0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f3eb1f6a7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f3eb1f7337a]
/lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f3eb1f7753c]
/hdd/face-detection/mxnet/python/mxnet/libmxnet.so(MXExecutorReshape+0x1e02)[0x7f3dec8708c2]
/usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call_unix64+0x4c)[0x7f3eaea27e20]
/usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(ffi_call+0x2eb)[0x7f3eaea2788b]
/usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(_ctypes_callproc+0x49a)[0x7f3eaea2201a]
/usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so(+0x9fcb)[0x7f3eaea15fcb]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4ec6)[0x53bba6]
/usr/bin/python3.5(PyEval_EvalCodeEx+0x88a)[0x5416ea]
/usr/bin/python3.5[0x4ebe37]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x252b)[0x53920b]
/usr/bin/python3.5[0x540199]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x50b2)[0x53bd92]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5[0x540199]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x50b2)[0x53bd92]
/usr/bin/python3.5[0x540199]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x50b2)[0x53bd92]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5[0x540199]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x50b2)[0x53bd92]
/usr/bin/python3.5(PyEval_EvalCodeEx+0x13b)[0x540f9b]
/usr/bin/python3.5[0x4ebe37]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x252b)[0x53920b]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalCodeEx+0x13b)[0x540f9b]
/usr/bin/python3.5[0x4ebd23]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5[0x4fb9ce]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5[0x574b36]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4ec6)[0x53bba6]
/usr/bin/python3.5[0x4ed3f5]
/usr/bin/python3.5(PyEval_EvalFrameEx+0xab1)[0x537791]
/usr/bin/python3.5[0x5406df]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x54f0)[0x53c1d0]
/usr/bin/python3.5[0x5406df]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x54f0)[0x53c1d0]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalCodeEx+0x13b)[0x540f9b]
/usr/bin/python3.5[0x4ebd23]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5[0x4fb9ce]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5[0x574999]
/usr/bin/python3.5[0x57f03c]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4ec6)[0x53bba6]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
/usr/bin/python3.5(PyEval_EvalCodeEx+0x13b)[0x540f9b]
/usr/bin/python3.5[0x4ebe37]
/usr/bin/python3.5(PyObject_Call+0x47)[0x5c1797]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x252b)[0x53920b]
/usr/bin/python3.5(PyEval_EvalFrameEx+0x4b04)[0x53b7e4]
======= Memory map: ========
00400000-007a9000 r-xp 00000000 08:02 788763                             /usr/bin/python3.5
009a9000-009ab000 r--p 003a9000 08:02 788763                             /usr/bin/python3.5
009ab000-00a42000 rw-p 003ab000 08:02 788763                             /usr/bin/python3.5
00a42000-00a73000 rw-p 00000000 00:00 0 
01521000-0ffe0000 rw-p 00000000 00:00 0                                  [heap]
10000000-10001000 rw-s 00000000 00:06 440                                /dev/nvidia0
10001000-10002000 rw-s 00000000 00:06 440                                /dev/nvidia0
10002000-10003000 rw-s 00000000 00:06 440                                /dev/nvidia0
10003000-10004000 rw-s 00000000 00:06 440                                /dev/nvidia0
10004000-10005000 rw-s 00000000 00:06 440                                /dev/nvidia0
10005000-10006000 rw-s 00000000 00:06 440                                /dev/nvidia0
10006000-10007000 rw-s 00000000 00:06 440                                /dev/nvidia0
10007000-10008000 rw-s 00000000 00:06 440                                /dev/nvidia0
10008000-10009000 rw-s 00000000 00:06 440                                /dev/nvidia0
10009000-1000a000 rw-s 00000000 00:06 440                                /dev/nvidia0
1000a000-1000b000 rw-s 00000000 00:06 440                                /dev/nvidia0
1000b000-1000c000 rw-s 00000000 00:06 440                                /dev/nvidia0
1000c000-1000d000 rw-s 00000000 00:06 440                                /dev/nvidia0
1000d000-1000e000 rw-s 00000000 00:06 440                                /dev/nvidia0
1000e000-1000f000 rw-s 00000000 00:06 440                                /dev/nvidia0
1000f000-10010000 rw-s 00000000 00:06 440                                /dev/nvidia0
10010000-20000000 ---p 00000000 00:00 0 
200000000-200200000 rw-s 00000000 00:06 439                              /dev/nvidiactl
200200000-200400000 ---p 00000000 00:00 0 
200400000-200404000 rw-s 00000000 00:06 439                              /dev/nvidiactl
200404000-200600000 ---p 00000000 00:00 0 
200600000-200a00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
200a00000-201600000 ---p 00000000 00:00 0 
201600000-201604000 rw-s 00000000 00:06 439                              /dev/nvidiactl
201604000-201800000 ---p 00000000 00:00 0 
201800000-201c00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
201c00000-202800000 ---p 00000000 00:00 0 
202800000-202804000 rw-s 00000000 00:06 439                              /dev/nvidiactl
202804000-202a00000 ---p 00000000 00:00 0 
202a00000-202e00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
202e00000-203a00000 ---p 00000000 00:00 0 
203a00000-203a04000 rw-s 00000000 00:06 439                              /dev/nvidiactl
203a04000-203c00000 ---p 00000000 00:00 0 
203c00000-204000000 rw-s 00000000 00:06 439                              /dev/nvidiactl
204000000-204c00000 ---p 00000000 00:00 0 
204c00000-204c04000 rw-s 00000000 00:06 439                              /dev/nvidiactl
204c04000-204e00000 ---p 00000000 00:00 0 
204e00000-205200000 rw-s 00000000 00:06 439                              /dev/nvidiactl
205200000-205e00000 ---p 00000000 00:00 0 
205e00000-205e04000 rw-s 00000000 00:06 439                              /dev/nvidiactl
205e04000-206000000 ---p 00000000 00:00 0 
206000000-206400000 rw-s 00000000 00:06 439                              /dev/nvidiactl
206400000-207000000 ---p 00000000 00:00 0 
207000000-207004000 rw-s 00000000 00:06 439                              /dev/nvidiactl
207004000-207200000 ---p 00000000 00:00 0 
207200000-207600000 rw-s 00000000 00:06 439                              /dev/nvidiactl
207600000-208200000 ---p 00000000 00:00 0 
208200000-208204000 rw-s 00000000 00:06 439                              /dev/nvidiactl
208204000-208400000 ---p 00000000 00:00 0 
208400000-208800000 rw-s 00000000 00:06 439                              /dev/nvidiactl
208800000-209400000 ---p 00000000 00:00 0 
209400000-209404000 rw-s 00000000 00:06 439                              /dev/nvidiactl
209404000-209600000 ---p 00000000 00:00 0 
209600000-209a00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
209a00000-209a04000 rw-s 00000000 00:06 439                              /dev/nvidiactl
209a04000-209c00000 ---p 00000000 00:00 0 
209c00000-20a000000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20a000000-20a004000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20a004000-20a200000 ---p 00000000 00:00 0 
20a200000-20a600000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20a600000-20a604000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20a604000-20a800000 ---p 00000000 00:00 0 
20a800000-20ac00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20ac00000-20ac04000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20ac04000-20ae00000 ---p 00000000 00:00 0 
20ae00000-20b200000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20b200000-20b204000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20b204000-20b400000 ---p 00000000 00:00 0 
20b400000-20b800000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20b800000-20b804000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20b804000-20ba00000 ---p 00000000 00:00 0 
20ba00000-20be00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20be00000-20be04000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20be04000-20c000000 ---p 00000000 00:00 0 
20c000000-20c400000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20c400000-20c600000 ---p 00000000 00:00 0 
20c600000-20c800000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20c800000-20ca00000 rw-s 00000000 00:06 439                              /dev/nvidiactl
20ca00000-500200000 ---p 00000000 00:00 0 
10000000000-10404400000 ---p 00000000 00:00 0 
10404400000-10404600000 rw-s 00000000 00:05 1233141                      /dev/zero (deleted)
10404600000-10404800000 rw-s 00000000 00:06 439                          /dev/nvidiactl
10404800000-10404a00000 rw-s 00000000 00:05 1233142                      /dev/zero (deleted)
10404a00000-10404c00000 rw-s 00000000 00:06 439                          /dev/nvidiactl
10404c00000-10404ed6000 rw-s 00000000 00:06 439                          /dev/nvidiactl
10404ed6000-1040f000000 ---p 00000000 00:00 0 
1040f000000-1040f200000 rw-s 00000000 00:05 1235302                      /dev/zero (deleted)
1040f200000-1040f600000 ---p 00000000 00:00 0 
1040f600000-1040f800000 rw-s 00000000 00:05 1235303                      /dev/zero (deleted)
1040f800000-1047b000000 ---p 00000000 00:00 0 
1047b200000-10497a00000 ---p 00000000 00:00 0 
7f3cd4000000-7f3cd654a000 rw-p 00000000 00:00 0 
7f3cd654a000-7f3cd8000000 ---p 00000000 00:00 0 
7f3cdc000000-7f3cdd77c000 rw-p 00000000 00:00 0 
7f3cdd77c000-7f3ce0000000 ---p 00000000 00:00 0 
7f3ce0000000-7f3ce0021000 rw-p 00000000 00:00 0 
7f3ce0021000-7f3ce4000000 ---p 00000000 00:00 0 
7f3ce4000000-7f3ce7eaf000 rw-p 00000000 00:00 0 
7f3ce7eaf000-7f3ce8000000 ---p 00000000 00:00 0 
7f3ce8000000-7f3cebf8e000 rw-p 00000000 00:00 0 
7f3cebf8e000-7f3cec000000 ---p 00000000 00:00 0 
7f3cec000000-7f3ceff8f000 rw-p 00000000 00:00 0 
7f3ceff8f000-7f3cf0000000 ---p 00000000 00:00 0 
7f3cf1eb7000-7f3cf1eb8000 ---p 00000000 00:00 0 
7f3cf1eb8000-7f3cf26b8000 rw-p 00000000 00:00 0 
7f3cf26b8000-7f3cf26c3000 r-xp 00000000 08:02 7209056                    /lib/x86_64-linux-gnu/libnss_files-2.23.so
7f3cf26c3000-7f3cf28c2000 ---p 0000b000 08:02 7209056                    /lib/x86_64-linux-gnu/libnss_files-2.23.so
7f3cf28c2000-7f3cf28c3000 r--p 0000a000 08:02 7209056                    /lib/x86_64-linux-gnu/libnss_files-2.23.so
7f3cf28c3000-7f3cf28c4000 rw-p 0000b000 08:02 7209056                    /lib/x86_64-linux-gnu/libnss_files-2.23.so
7f3cf28c4000-7f3cf28ca000 rw-p 00000000 00:00 0 
7f3cf28ca000-7f3cf28cb000 ---p 00000000 00:00 0 
7f3cf28cb000-7f3cf61d9000 rw-p 00000000 00:00 0 
7f3cf620e000-7f3d04000000 rw-p 00000000 00:00 0 
7f3d04000000-7f3d078d8000 rw-p 00000000 00:00 0 
7f3d078d8000-7f3d08000000 ---p 00000000 00:00 0 
7f3d08000000-7f3d0bef5000 rw-p 00000000 00:00 0 
7f3d0bef5000-7f3d0c000000 ---p 00000000 00:00 0 
7f3d0c000000-7f3d0ff1c000 rw-p 00000000 00:00 0 
7f3d0ff1c000-7f3d10000000 ---p 00000000 00:00 0 
7f3d10000000-7f3d10022000 rw-p 00000000 00:00 0 
7f3d10022000-7f3d14000000 ---p 00000000 00:00 0 
7f3d1404d000-7f3d18000000 rw-p 00000000 00:00 0 
7f3d18000000-7f3d18022000 rw-p 00000000 00:00 0 
7f3d18022000-7f3d1c000000 ---p 00000000 00:00 0 
7f3d1c0b9000-7f3d1ceff000 rw-p 00000000 00:00 0 
7f3d1cffa000-7f3d20000000 rw-p 00000000 00:00 0 
7f3d20000000-7f3d23ee3000 rw-p 00000000 00:00 0 
7f3d23ee3000-7f3d24000000 ---p 00000000 00:00 0 
7f3d24000000-7f3d27d08000 rw-p 00000000 00:00 0 
7f3d27d08000-7f3d28000000 ---p 00000000 00:00 0 
7f3d28038000-7f3d2c000000 rw-p 00000000 00:00 0 
7f3d2c000000-7f3d2fdd2000 rw-p 00000000 00:00 0 
7f3d2fdd2000-7f3d30000000 ---p 00000000 00:00 0 
7f3d30000000-7f3d31ea1000 rw-p 00000000 00:00 0 
7f3d31ea1000-7f3d34000000 ---p 00000000 00:00 0 
7f3d34000000-7f3d3bfff000 rw-p 00000000 00:00 0 
7f3d3bfff000-7f3d3c000000 ---p 00000000 00:00 0 
7f3d3c000000-7f3d3ff06000 rw-p 00000000 00:00 0 
7f3d3ff06000-7f3d40000000 ---p 00000000 00:00 0 
7f3d40000000-7f3d43ef8000 rw-p 00000000 00:00 0 
7f3d43ef8000-7f3d44000000 ---p 00000000 00:00 0 
7f3d44000000-7f3d47ffc000 rw-p 00000000 00:00 0 
7f3d47ffc000-7f3d48000000 ---p 00000000 00:00 0 
7f3d48000000-7f3d4bfff000 rw-p 00000000 00:00 0 
7f3d4bfff000-7f3d4c000000 ---p 00000000 00:00 0 
7f3d4c000000-7f3d4ffbc000 rw-p 00000000 00:00 0 
7f3d4ffbc000-7f3d50000000 ---p 00000000 00:00 0 
7f3d50000000-7f3d53fd4000 rw-p 00000000 00:00 0 
7f3d53fd4000-7f3d54000000 ---p 00000000 00:00 0 
7f3d54000000-7f3d5bffe000 rw-p 00000000 00:00 0 
7f3d5bffe000-7f3d5c000000 ---p 00000000 00:00 0 
7f3d5c000000-7f3d5c17b000 rw-p 00000000 00:00 0 
7f3d5c17b000-7f3d60000000 ---p 00000000 00:00 0 
7f3d60075000-7f3d61ffc000 rw-p 00000000 00:00 0 
7f3d61ffc000-7f3d61ffd000 ---p 00000000 00:00 0 
7f3d61ffd000-7f3d627fd000 rw-p 00000000 00:00 0 
7f3d627fd000-7f3d627fe000 ---p 00000000 00:00 0 
7f3d627fe000-7f3d62ffe000 rw-p 00000000 00:00 0 
7f3d62ffe000-7f3d62fff000 ---p 00000000 00:00 0 
7f3d62fff000-7f3d637ff000 rw-p 00000000 00:00 0 
7f3d637ff000-7f3d63800000 ---p 00000000 00:00 0 
7f3d63800000-7f3d64000000 rw-p 00000000 00:00 0 
7f3d64000000-7f3d67f66000 rw-p 00000000 00:00 0 
7f3d67f66000-7f3d68000000 ---p 00000000 00:00 0 
7f3d6801c000-7f3d685bd000 rw-p 00000000 00:00 0 
7f3d685bd000-7f3d685be000 ---p 00000000 00:00 0 
7f3d685be000-7f3d68ebe000 rw-p 00000000 00:00 0 
7f3d68ebe000-7f3d68ebf000 ---p 00000000 00:00 0 
7f3d68ebf000-7f3d69ab0000 rw-p 00000000 00:00 0 
7f3d69ab0000-7f3d69ab1000 ---p 00000000 00:00 0 
7f3d69ab1000-7f3d6a2b1000 rw-p 00000000 00:00 0 
7f3d6a2b1000-7f3d6a2b2000 ---p 00000000 00:00 0 
7f3d6a2b2000-7f3d90000000 rw-p 00000000 00:00 0 
7f3d90000000-7f3d90021000 rw-p 00000000 00:00 0 
7f3d90021000-7f3d94000000 ---p 00000000 00:00 0 
7f3d940ac000-7f3d98000000 rw-p 00000000 00:00 0 
7f3d98000000-7f3d98021000 rw-p 00000000 00:00 0 
7f3d98021000-7f3d9c000000 ---p 00000000 00:00 0 
7f3d9c08b000-7f3d9cffe000 rw-p 00000000 00:00 0 
7f3d9cffe000-7f3d9cfff000 ---p 00000000 00:00 0 
7f3d9cfff000-7f3d9d7ff000 rw-p 00000000 00:00 0 
7f3d9d7ff000-7f3d9d800000 ---p 00000000 00:00 0 
7f3d9d800000-7f3d9e000000 rw-p 00000000 00:00 0 
7f3d9e000000-7f3da4000000 ---p 00000000 00:00 0 
7f3da4000000-7f3da4022000 rw-p 00000000 00:00 0 
7f3da4022000-7f3da8000000 ---p 00000000 00:00 0 
7f3da8000000-7f3da8022000 rw-p 00000000 00:00 0 
7f3da8022000-7f3dac000000 ---p 00000000 00:00 0 
7f3dac000000-7f3dac022000 rw-p 00000000 00:00 0 
7f3dac022000-7f3db0000000 ---p 00000000 00:00 0 
7f3db00f7000-7f3db01f7000 rw-p 00000000 00:00 0 
7f3db01f7000-7f3db01f8000 ---p 00000000 00:00 0 
7f3db01f8000-7f3db09f8000 rw-p 00000000 00:00 0 
7f3db0aed000-7f3db37ff000 rw-p 00000000 00:00 0 
7f3db37ff000-7f3db3800000 ---p 00000000 00:00 0 
7f3db3800000-7f3db4000000 rw-p 00000000 00:00 0 
7f3db4000000-7f3db4022000 rw-p 00000000 00:00 0 
7f3db4022000-7f3db8000000 ---p 00000000 00:00 0 
7f3db80d8000-7f3db81d8000 rw-p 00000000 00:00 0 
7f3db81d9000-7f3db875b000 rw-p 00000000 00:00 0 
7f3db875b000-7f3db875c000 ---p 00000000 00:00 0 
7f3db875c000-7f3db8f5c000 rw-p 00000000 00:00 0 
7f3db8f5c000-7f3db8fa6000 r-xp 00000000 08:02 5532229                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/bilateral_cy.cpython-35m-x86_64-linux-gnu.so
7f3db8fa6000-7f3db91a6000 ---p 0004a000 08:02 5532229                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/bilateral_cy.cpython-35m-x86_64-linux-gnu.so
7f3db91a6000-7f3db91aa000 rw-p 0004a000 08:02 5532229                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/bilateral_cy.cpython-35m-x86_64-linux-gnu.so
7f3db91aa000-7f3db91ab000 rw-p 00000000 00:00 0 
7f3db91ab000-7f3db9234000 r-xp 00000000 08:02 5532234                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/percentile_cy.cpython-35m-x86_64-linux-gnu.so
7f3db9234000-7f3db9433000 ---p 00089000 08:02 5532234                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/percentile_cy.cpython-35m-x86_64-linux-gnu.so
7f3db9433000-7f3db9439000 rw-p 00088000 08:02 5532234                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/percentile_cy.cpython-35m-x86_64-linux-gnu.so
7f3db9439000-7f3db943a000 rw-p 00000000 00:00 0 
7f3db943a000-7f3db948d000 r-xp 00000000 08:02 5532236                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/core_cy.cpython-35m-x86_64-linux-gnu.so
7f3db948d000-7f3db968c000 ---p 00053000 08:02 5532236                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/core_cy.cpython-35m-x86_64-linux-gnu.so
7f3db968c000-7f3db968f000 rw-p 00052000 08:02 5532236                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/core_cy.cpython-35m-x86_64-linux-gnu.so
7f3db968f000-7f3db9690000 rw-p 00000000 00:00 0 
7f3db9690000-7f3db977b000 r-xp 00000000 08:02 5532233                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/generic_cy.cpython-35m-x86_64-linux-gnu.so
7f3db977b000-7f3db997b000 ---p 000eb000 08:02 5532233                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/generic_cy.cpython-35m-x86_64-linux-gnu.so
7f3db997b000-7f3db9982000 rw-p 000eb000 08:02 5532233                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/filters/rank/generic_cy.cpython-35m-x86_64-linux-gnu.so
7f3db9982000-7f3db9983000 rw-p 00000000 00:00 0 
7f3db9983000-7f3db99b1000 r-xp 00000000 08:02 5532161                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_seam_carving.cpython-35m-x86_64-linux-gnu.so
7f3db99b1000-7f3db9bb1000 ---p 0002e000 08:02 5532161                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_seam_carving.cpython-35m-x86_64-linux-gnu.so
7f3db9bb1000-7f3db9bb4000 rw-p 0002e000 08:02 5532161                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_seam_carving.cpython-35m-x86_64-linux-gnu.so
7f3db9bb4000-7f3db9bb5000 rw-p 00000000 00:00 0 
7f3db9bb5000-7f3db9bdf000 r-xp 00000000 08:02 5532171                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_radon_transform.cpython-35m-x86_64-linux-gnu.so
7f3db9bdf000-7f3db9ddf000 ---p 0002a000 08:02 5532171                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_radon_transform.cpython-35m-x86_64-linux-gnu.so
7f3db9ddf000-7f3db9de2000 rw-p 0002a000 08:02 5532171                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_radon_transform.cpython-35m-x86_64-linux-gnu.so
7f3db9de2000-7f3db9de3000 rw-p 00000000 00:00 0 
7f3db9de3000-7f3db9e0e000 r-xp 00000000 08:02 5532173                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_warps_cy.cpython-35m-x86_64-linux-gnu.so
7f3db9e0e000-7f3dba00e000 ---p 0002b000 08:02 5532173                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_warps_cy.cpython-35m-x86_64-linux-gnu.so
7f3dba00e000-7f3dba012000 rw-p 0002b000 08:02 5532173                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_warps_cy.cpython-35m-x86_64-linux-gnu.so
7f3dba012000-7f3dba05f000 r-xp 00000000 08:02 5531886                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/draw/_draw.cpython-35m-x86_64-linux-gnu.so
7f3dba05f000-7f3dba25f000 ---p 0004d000 08:02 5531886                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/draw/_draw.cpython-35m-x86_64-linux-gnu.so
7f3dba25f000-7f3dba265000 rw-p 0004d000 08:02 5531886                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/draw/_draw.cpython-35m-x86_64-linux-gnu.so
7f3dba265000-7f3dba266000 rw-p 00000000 00:00 0 
7f3dba266000-7f3dba2a6000 r-xp 00000000 08:02 5532167                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_hough_transform.cpython-35m-x86_64-linux-gnu.so
7f3dba2a6000-7f3dba4a5000 ---p 00040000 08:02 5532167                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_hough_transform.cpython-35m-x86_64-linux-gnu.so
7f3dba4a5000-7f3dba4ab000 rw-p 0003f000 08:02 5532167                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/transform/_hough_transform.cpython-35m-x86_64-linux-gnu.so
7f3dba4ab000-7f3dba4ac000 rw-p 00000000 00:00 0 
7f3dba4ac000-7f3dba4d7000 r-xp 00000000 08:02 5532395                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_pnpoly.cpython-35m-x86_64-linux-gnu.so
7f3dba4d7000-7f3dba6d7000 ---p 0002b000 08:02 5532395                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_pnpoly.cpython-35m-x86_64-linux-gnu.so
7f3dba6d7000-7f3dba6da000 rw-p 0002b000 08:02 5532395                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_pnpoly.cpython-35m-x86_64-linux-gnu.so
7f3dba6da000-7f3dba6db000 rw-p 00000000 00:00 0 
7f3dba6db000-7f3dba701000 r-xp 00000000 08:02 5532398                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_moments_cy.cpython-35m-x86_64-linux-gnu.so
7f3dba701000-7f3dba900000 ---p 00026000 08:02 5532398                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_moments_cy.cpython-35m-x86_64-linux-gnu.so
7f3dba900000-7f3dba903000 rw-p 00025000 08:02 5532398                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_moments_cy.cpython-35m-x86_64-linux-gnu.so
7f3dba903000-7f3dba944000 rw-p 00000000 00:00 0 
7f3dba944000-7f3dba947000 r-xp 00000000 08:02 1056153                    /usr/lib/python3.5/lib-dynload/_multiprocessing.cpython-35m-x86_64-linux-gnu.so
7f3dba947000-7f3dbab46000 ---p 00003000 08:02 1056153                    /usr/lib/python3.5/lib-dynload/_multiprocessing.cpython-35m-x86_64-linux-gnu.so
7f3dbab46000-7f3dbab47000 r--p 00002000 08:02 1056153                    /usr/lib/python3.5/lib-dynload/_multiprocessing.cpython-35m-x86_64-linux-gnu.so
7f3dbab47000-7f3dbab48000 rw-p 00003000 08:02 1056153                    /usr/lib/python3.5/lib-dynload/_multiprocessing.cpython-35m-x86_64-linux-gnu.so
7f3dbab48000-7f3dbabc8000 rw-p 00000000 00:00 0 
7f3dbabc8000-7f3dbac51000 r-xp 00000000 08:02 5511918                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/eigen/arpack/_arpack.cpython-35m-x86_64-linux-gnu.so
7f3dbac51000-7f3dbae51000 ---p 00089000 08:02 5511918                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/eigen/arpack/_arpack.cpython-35m-x86_64-linux-gnu.so
7f3dbae51000-7f3dbae5c000 rw-p 00089000 08:02 5511918                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/eigen/arpack/_arpack.cpython-35m-x86_64-linux-gnu.so
7f3dbae5c000-7f3dbae5e000 rw-p 00000000 00:00 0 
7f3dbae5e000-7f3dbae62000 rw-p 0017c000 08:02 5511918                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/eigen/arpack/_arpack.cpython-35m-x86_64-linux-gnu.so
7f3dbae62000-7f3dbaea2000 rw-p 00000000 00:00 0 
7f3dbaea2000-7f3dbaeef000 r-xp 00000000 08:02 5511890                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/dsolve/_superlu.cpython-35m-x86_64-linux-gnu.so
7f3dbaeef000-7f3dbb0ee000 ---p 0004d000 08:02 5511890                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/dsolve/_superlu.cpython-35m-x86_64-linux-gnu.so
7f3dbb0ee000-7f3dbb0f0000 rw-p 0004c000 08:02 5511890                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/dsolve/_superlu.cpython-35m-x86_64-linux-gnu.so
7f3dbb0f0000-7f3dbb0f4000 rw-p 00196000 08:02 5511890                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/dsolve/_superlu.cpython-35m-x86_64-linux-gnu.so
7f3dbb0f4000-7f3dbb126000 r-xp 00000000 08:02 5511944                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/isolve/_iterative.cpython-35m-x86_64-linux-gnu.so
7f3dbb126000-7f3dbb326000 ---p 00032000 08:02 5511944                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/isolve/_iterative.cpython-35m-x86_64-linux-gnu.so
7f3dbb326000-7f3dbb32d000 rw-p 00032000 08:02 5511944                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/isolve/_iterative.cpython-35m-x86_64-linux-gnu.so
7f3dbb32d000-7f3dbb32f000 rw-p 00000000 00:00 0 
7f3dbb32f000-7f3dbb333000 rw-p 000a5000 08:02 5511944                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/sparse/linalg/isolve/_iterative.cpython-35m-x86_64-linux-gnu.so
7f3dbb333000-7f3dbb373000 rw-p 00000000 00:00 0 
7f3dbb373000-7f3dbb385000 r-xp 00000000 08:02 5528796                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/murmurhash.cpython-35m-x86_64-linux-gnu.so
7f3dbb385000-7f3dbb584000 ---p 00012000 08:02 5528796                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/murmurhash.cpython-35m-x86_64-linux-gnu.so
7f3dbb584000-7f3dbb586000 rw-p 00011000 08:02 5528796                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/murmurhash.cpython-35m-x86_64-linux-gnu.so
7f3dbb586000-7f3dbb589000 r-xp 00000000 08:02 5528553                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/__check_build/_check_build.cpython-35m-x86_64-linux-gnu.so
7f3dbb589000-7f3dbb788000 ---p 00003000 08:02 5528553                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/__check_build/_check_build.cpython-35m-x86_64-linux-gnu.so
7f3dbb788000-7f3dbb789000 rw-p 00002000 08:02 5528553                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/__check_build/_check_build.cpython-35m-x86_64-linux-gnu.so
7f3dbb789000-7f3dbbc49000 rw-p 00000000 00:00 0 
7f3dbbc49000-7f3dbbc4a000 ---p 00000000 00:00 0 
7f3dbbc4a000-7f3dbc44a000 rw-p 00000000 00:00 0 
7f3dbc44a000-7f3dbc44b000 ---p 00000000 00:00 0 
7f3dbc44b000-7f3dbcc4b000 rw-p 00000000 00:00 0 
7f3dbcc4b000-7f3dbcc4c000 ---p 00000000 00:00 0 
7f3dbcc4c000-7f3dbd44c000 rw-p 00000000 00:00 0 
7f3dbd44c000-7f3dbd44d000 ---p 00000000 00:00 0 
7f3dbd44d000-7f3dbdc4d000 rw-p 00000000 00:00 0 
7f3dbdc4d000-7f3dbdc4e000 ---p 00000000 00:00 0 
7f3dbdc4e000-7f3dbe44e000 rw-p 00000000 00:00 0 
7f3dbe44e000-7f3dbe44f000 ---p 00000000 00:00 0 
7f3dbe44f000-7f3dbec4f000 rw-p 00000000 00:00 0 
7f3dbec4f000-7f3dbec50000 ---p 00000000 00:00 0 
7f3dbec50000-7f3dbf450000 rw-p 00000000 00:00 0 
7f3dbf450000-7f3dc7450000 rw-p 00000000 00:00 0 
7f3dc7489000-7f3dc7549000 rw-p 00000000 00:00 0 
7f3dc7549000-7f3dc755f000 r-xp 00000000 08:02 5532403                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_ccomp.cpython-35m-x86_64-linux-gnu.so
7f3dc755f000-7f3dc775e000 ---p 00016000 08:02 5532403                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_ccomp.cpython-35m-x86_64-linux-gnu.so
7f3dc775e000-7f3dc7760000 rw-p 00015000 08:02 5532403                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_ccomp.cpython-35m-x86_64-linux-gnu.so
7f3dc7760000-7f3dc77c6000 r-xp 00000000 08:02 5532393                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_marching_cubes_classic_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc77c6000-7f3dc79c5000 ---p 00066000 08:02 5532393                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_marching_cubes_classic_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc79c5000-7f3dc79c9000 rw-p 00065000 08:02 5532393                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_marching_cubes_classic_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc79c9000-7f3dc79ca000 rw-p 00000000 00:00 0 
7f3dc79ca000-7f3dc7a0d000 r-xp 00000000 08:02 5532397                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_marching_cubes_lewiner_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc7a0d000-7f3dc7c0d000 ---p 00043000 08:02 5532397                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_marching_cubes_lewiner_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc7c0d000-7f3dc7c12000 rw-p 00043000 08:02 5532397                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_marching_cubes_lewiner_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc7c12000-7f3dc7c13000 rw-p 00000000 00:00 0 
7f3dc7c13000-7f3dc7c3a000 r-xp 00000000 08:02 5532405                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_find_contours_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc7c3a000-7f3dc7e39000 ---p 00027000 08:02 5532405                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_find_contours_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc7e39000-7f3dc7e3d000 rw-p 00026000 08:02 5532405                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/measure/_find_contours_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc7e3d000-7f3dc7e7c000 r-xp 00000000 08:02 5532040                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_nl_means_denoising.cpython-35m-x86_64-linux-gnu.so
7f3dc7e7c000-7f3dc807c000 ---p 0003f000 08:02 5532040                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_nl_means_denoising.cpython-35m-x86_64-linux-gnu.so
7f3dc807c000-7f3dc8081000 rw-p 0003f000 08:02 5532040                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_nl_means_denoising.cpython-35m-x86_64-linux-gnu.so
7f3dc8081000-7f3dc8082000 rw-p 00000000 00:00 0 
7f3dc8082000-7f3dc80d8000 r-xp 00000000 08:02 5530372                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_swt.cpython-35m-x86_64-linux-gnu.so
7f3dc80d8000-7f3dc82d7000 ---p 00056000 08:02 5530372                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_swt.cpython-35m-x86_64-linux-gnu.so
7f3dc82d7000-7f3dc82dc000 rw-p 00055000 08:02 5530372                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_swt.cpython-35m-x86_64-linux-gnu.so
7f3dc82dc000-7f3dc82dd000 rw-p 00000000 00:00 0 
7f3dc82dd000-7f3dc8320000 r-xp 00000000 08:02 5530371                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_cwt.cpython-35m-x86_64-linux-gnu.so
7f3dc8320000-7f3dc851f000 ---p 00043000 08:02 5530371                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_cwt.cpython-35m-x86_64-linux-gnu.so
7f3dc851f000-7f3dc8524000 rw-p 00042000 08:02 5530371                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_cwt.cpython-35m-x86_64-linux-gnu.so
7f3dc8524000-7f3dc8593000 r-xp 00000000 08:02 5530373                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_dwt.cpython-35m-x86_64-linux-gnu.so
7f3dc8593000-7f3dc8792000 ---p 0006f000 08:02 5530373                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_dwt.cpython-35m-x86_64-linux-gnu.so
7f3dc8792000-7f3dc8797000 rw-p 0006e000 08:02 5530373                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_dwt.cpython-35m-x86_64-linux-gnu.so
7f3dc8797000-7f3dc8798000 rw-p 00000000 00:00 0 
7f3dc8798000-7f3dc880d000 r-xp 00000000 08:02 5530370                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_pywt.cpython-35m-x86_64-linux-gnu.so
7f3dc880d000-7f3dc8a0c000 ---p 00075000 08:02 5530370                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_pywt.cpython-35m-x86_64-linux-gnu.so
7f3dc8a0c000-7f3dc8a16000 rw-p 00074000 08:02 5530370                    /home/socian-ai/.local/lib/python3.5/site-packages/pywt/_extensions/_pywt.cpython-35m-x86_64-linux-gnu.so
7f3dc8a16000-7f3dc8a18000 rw-p 00000000 00:00 0 
7f3dc8a18000-7f3dc8a4e000 r-xp 00000000 08:02 5532041                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_denoise_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc8a4e000-7f3dc8c4e000 ---p 00036000 08:02 5532041                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_denoise_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc8c4e000-7f3dc8c52000 rw-p 00036000 08:02 5532041                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_denoise_cy.cpython-35m-x86_64-linux-gnu.so
7f3dc8c52000-7f3dc8c53000 rw-p 00000000 00:00 0 
7f3dc8c53000-7f3dcac53000 rw-p 00000000 00:00 0 
7f3dcac54000-7f3dcadd4000 rw-p 00000000 00:00 0 
7f3dcadd4000-7f3dcadff000 r-xp 00000000 08:02 5532038                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_3d.cpython-35m-x86_64-linux-gnu.so
7f3dcadff000-7f3dcafff000 ---p 0002b000 08:02 5532038                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_3d.cpython-35m-x86_64-linux-gnu.so
7f3dcafff000-7f3dcb002000 rw-p 0002b000 08:02 5532038                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_3d.cpython-35m-x86_64-linux-gnu.so
7f3dcb002000-7f3dcb02a000 r-xp 00000000 08:02 5532036                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_2d.cpython-35m-x86_64-linux-gnu.so
7f3dcb02a000-7f3dcb22a000 ---p 00028000 08:02 5532036                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_2d.cpython-35m-x86_64-linux-gnu.so
7f3dcb22a000-7f3dcb22d000 rw-p 00028000 08:02 5532036                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_2d.cpython-35m-x86_64-linux-gnu.so
7f3dcb22d000-7f3dcb251000 r-xp 00000000 08:02 5532043                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_1d.cpython-35m-x86_64-linux-gnu.so
7f3dcb251000-7f3dcb451000 ---p 00024000 08:02 5532043                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_1d.cpython-35m-x86_64-linux-gnu.so
7f3dcb451000-7f3dcb454000 rw-p 00024000 08:02 5532043                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/restoration/_unwrap_1d.cpython-35m-x86_64-linux-gnu.so
7f3dcb454000-7f3dcd454000 rw-p 00000000 00:00 0 
7f3dcd48b000-7f3dcd5cb000 rw-p 00000000 00:00 0 
7f3dcd5cb000-7f3dcd5fb000 r-xp 00000000 08:02 5512286                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_peak_finding_utils.cpython-35m-x86_64-linux-gnu.so
7f3dcd5fb000-7f3dcd7fb000 ---p 00030000 08:02 5512286                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_peak_finding_utils.cpython-35m-x86_64-linux-gnu.so
7f3dcd7fb000-7f3dcd7ff000 rw-p 00030000 08:02 5512286                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_peak_finding_utils.cpython-35m-x86_64-linux-gnu.so
7f3dcd7ff000-7f3dcd800000 rw-p 00000000 00:00 0 
7f3dcd800000-7f3dcd809000 r-xp 00000000 08:02 5512284                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_spectral.cpython-35m-x86_64-linux-gnu.so
7f3dcd809000-7f3dcda08000 ---p 00009000 08:02 5512284                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_spectral.cpython-35m-x86_64-linux-gnu.so
7f3dcda08000-7f3dcda0a000 rw-p 00008000 08:02 5512284                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_spectral.cpython-35m-x86_64-linux-gnu.so
7f3dcda0a000-7f3dcda4a000 rw-p 00000000 00:00 0 
7f3dcda4a000-7f3dcda54000 r-xp 00000000 08:02 5512281                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/spline.cpython-35m-x86_64-linux-gnu.so
7f3dcda54000-7f3dcdc53000 ---p 0000a000 08:02 5512281                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/spline.cpython-35m-x86_64-linux-gnu.so
7f3dcdc53000-7f3dcdc55000 rw-p 00009000 08:02 5512281                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/spline.cpython-35m-x86_64-linux-gnu.so
7f3dcdc55000-7f3dd9c55000 rw-p 00000000 00:00 0 
7f3dd9c72000-7f3dd9cb2000 rw-p 00000000 00:00 0 
7f3dd9cb2000-7f3dd9ce9000 r-xp 00000000 08:02 5512283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_upfirdn_apply.cpython-35m-x86_64-linux-gnu.so
7f3dd9ce9000-7f3dd9ee8000 ---p 00037000 08:02 5512283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_upfirdn_apply.cpython-35m-x86_64-linux-gnu.so
7f3dd9ee8000-7f3dd9eec000 rw-p 00036000 08:02 5512283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_upfirdn_apply.cpython-35m-x86_64-linux-gnu.so
7f3dd9eec000-7f3dd9eed000 rw-p 00000000 00:00 0 
7f3dd9eed000-7f3dd9f14000 r-xp 00000000 08:02 5512274                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_max_len_seq_inner.cpython-35m-x86_64-linux-gnu.so
7f3dd9f14000-7f3dda113000 ---p 00027000 08:02 5512274                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_max_len_seq_inner.cpython-35m-x86_64-linux-gnu.so
7f3dda113000-7f3dda116000 rw-p 00026000 08:02 5512274                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/_max_len_seq_inner.cpython-35m-x86_64-linux-gnu.so
7f3dda116000-7f3dda117000 rw-p 00000000 00:00 0 
7f3dda117000-7f3dda129000 r-xp 00000000 08:02 5512278                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/sigtools.cpython-35m-x86_64-linux-gnu.so
7f3dda129000-7f3dda328000 ---p 00012000 08:02 5512278                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/sigtools.cpython-35m-x86_64-linux-gnu.so
7f3dda328000-7f3dda329000 rw-p 00011000 08:02 5512278                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/signal/sigtools.cpython-35m-x86_64-linux-gnu.so
7f3dda329000-7f3dda3aa000 rw-p 00000000 00:00 0 
7f3dda3aa000-7f3dda3e6000 r-xp 00000000 08:02 5530482                    /home/socian-ai/.local/lib/python3.5/site-packages/kiwisolver.cpython-35m-x86_64-linux-gnu.so
7f3dda3e6000-7f3dda5e5000 ---p 0003c000 08:02 5530482                    /home/socian-ai/.local/lib/python3.5/site-packages/kiwisolver.cpython-35m-x86_64-linux-gnu.so
7f3dda5e5000-7f3dda5e8000 rw-p 0003b000 08:02 5530482                    /home/socian-ai/.local/lib/python3.5/site-packages/kiwisolver.cpython-35m-x86_64-linux-gnu.so
7f3dda5e8000-7f3dda6a8000 rw-p 00000000 00:00 0 
7f3dda6a8000-7f3dda704000 r-xp 00000000 08:02 5530541                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_qhull.cpython-35m-x86_64-linux-gnu.so
7f3dda704000-7f3dda903000 ---p 0005c000 08:02 5530541                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_qhull.cpython-35m-x86_64-linux-gnu.so
7f3dda903000-7f3dda905000 rw-p 0005b000 08:02 5530541                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_qhull.cpython-35m-x86_64-linux-gnu.so
7f3dda905000-7f3dda907000 rw-p 00000000 00:00 0 
7f3dda907000-7f3dda925000 r-xp 00000000 08:02 5530535                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_tri.cpython-35m-x86_64-linux-gnu.so
7f3dda925000-7f3ddab25000 ---p 0001e000 08:02 5530535                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_tri.cpython-35m-x86_64-linux-gnu.so
7f3ddab25000-7f3ddab26000 rw-p 0001e000 08:02 5530535                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_tri.cpython-35m-x86_64-linux-gnu.so
7f3ddab26000-7f3ddaba7000 rw-p 00000000 00:00 0 
7f3ddaba7000-7f3ddabe1000 r-xp 00000000 08:02 5530568                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_image.cpython-35m-x86_64-linux-gnu.so
7f3ddabe1000-7f3ddade1000 ---p 0003a000 08:02 5530568                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_image.cpython-35m-x86_64-linux-gnu.so
7f3ddade1000-7f3ddade2000 rw-p 0003a000 08:02 5530568                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_image.cpython-35m-x86_64-linux-gnu.so
7f3ddade2000-7f3ddaf23000 rw-p 00000000 00:00 0 
7f3ddaf23000-7f3ddaf62000 r-xp 00000000 08:02 5531134                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/.libs/libpng16-cfdb1654.so.16.21.0
7f3ddaf62000-7f3ddb161000 ---p 0003f000 08:02 5531134                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/.libs/libpng16-cfdb1654.so.16.21.0
7f3ddb161000-7f3ddb162000 rw-p 0003e000 08:02 5531134                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/.libs/libpng16-cfdb1654.so.16.21.0
7f3ddb162000-7f3ddb166000 rw-p 00040000 08:02 5531134                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/.libs/libpng16-cfdb1654.so.16.21.0
7f3ddb166000-7f3ddb16f000 r-xp 00000000 08:02 5530567                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_png.cpython-35m-x86_64-linux-gnu.so
7f3ddb16f000-7f3ddb36e000 ---p 00009000 08:02 5530567                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_png.cpython-35m-x86_64-linux-gnu.so
7f3ddb36e000-7f3ddb36f000 rw-p 00008000 08:02 5530567                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_png.cpython-35m-x86_64-linux-gnu.so
7f3ddb36f000-7f3ddb371000 rw-p 0000a000 08:02 5530567                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_png.cpython-35m-x86_64-linux-gnu.so
7f3ddb371000-7f3ddb471000 rw-p 00000000 00:00 0 
7f3ddb471000-7f3ddb545000 r-xp 00000000 08:02 5530521                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/ft2font.cpython-35m-x86_64-linux-gnu.so
7f3ddb545000-7f3ddb744000 ---p 000d4000 08:02 5530521                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/ft2font.cpython-35m-x86_64-linux-gnu.so
7f3ddb744000-7f3ddb74b000 rw-p 000d3000 08:02 5530521                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/ft2font.cpython-35m-x86_64-linux-gnu.so
7f3ddb74b000-7f3ddb7cc000 rw-p 00000000 00:00 0 
7f3ddb7cc000-7f3ddb7e2000 r-xp 00000000 08:02 5530540                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_contour.cpython-35m-x86_64-linux-gnu.so
7f3ddb7e2000-7f3ddb9e2000 ---p 00016000 08:02 5530540                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_contour.cpython-35m-x86_64-linux-gnu.so
7f3ddb9e2000-7f3ddb9e3000 rw-p 00016000 08:02 5530540                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_contour.cpython-35m-x86_64-linux-gnu.so
7f3ddb9e3000-7f3ddbb64000 rw-p 00000000 00:00 0 
7f3ddbb64000-7f3ddbb91000 r-xp 00000000 08:02 5530573                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_path.cpython-35m-x86_64-linux-gnu.so
7f3ddbb91000-7f3ddbd91000 ---p 0002d000 08:02 5530573                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_path.cpython-35m-x86_64-linux-gnu.so
7f3ddbd91000-7f3ddbd92000 rw-p 0002d000 08:02 5530573                    /home/socian-ai/.local/lib/python3.5/site-packages/matplotlib/_path.cpython-35m-x86_64-linux-gnu.so
7f3ddbd92000-7f3ddbfd3000 rw-p 00000000 00:00 0 
7f3ddbfd3000-7f3ddbfd8000 r-xp 00000000 08:02 5532640                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/external/tifffile/_tifffile.cpython-35m-x86_64-linux-gnu.so
7f3ddbfd8000-7f3ddc1d7000 ---p 00005000 08:02 5532640                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/external/tifffile/_tifffile.cpython-35m-x86_64-linux-gnu.so
7f3ddc1d7000-7f3ddc1d8000 rw-p 00004000 08:02 5532640                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/external/tifffile/_tifffile.cpython-35m-x86_64-linux-gnu.so
7f3ddc1d8000-7f3ddc2d8000 rw-p 00000000 00:00 0 
7f3ddc2d8000-7f3ddc2ec000 r-xp 00000000 08:02 5512240                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/convolve.cpython-35m-x86_64-linux-gnu.so
7f3ddc2ec000-7f3ddc4eb000 ---p 00014000 08:02 5512240                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/convolve.cpython-35m-x86_64-linux-gnu.so
7f3ddc4eb000-7f3ddc4ed000 rw-p 00013000 08:02 5512240                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/convolve.cpython-35m-x86_64-linux-gnu.so
7f3ddc4ed000-7f3ddc4ef000 rw-p 0003c000 08:02 5512240                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/convolve.cpython-35m-x86_64-linux-gnu.so
7f3ddc4ef000-7f3ddc543000 r-xp 00000000 08:02 5512243                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/_fftpack.cpython-35m-x86_64-linux-gnu.so
7f3ddc543000-7f3ddc743000 ---p 00054000 08:02 5512243                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/_fftpack.cpython-35m-x86_64-linux-gnu.so
7f3ddc743000-7f3ddc74a000 rw-p 00054000 08:02 5512243                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/_fftpack.cpython-35m-x86_64-linux-gnu.so
7f3ddc74a000-7f3ddc74c000 rw-p 000df000 08:02 5512243                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/fftpack/_fftpack.cpython-35m-x86_64-linux-gnu.so
7f3ddc74c000-7f3ddc84c000 rw-p 00000000 00:00 0 
7f3ddc84c000-7f3ddc84f000 r-xp 00000000 08:02 5532612                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/_shared/geometry.cpython-35m-x86_64-linux-gnu.so
7f3ddc84f000-7f3ddca4e000 ---p 00003000 08:02 5532612                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/_shared/geometry.cpython-35m-x86_64-linux-gnu.so
7f3ddca4e000-7f3ddca4f000 rw-p 00002000 08:02 5532612                    /home/socian-ai/.local/lib/python3.5/site-packages/skimage/_shared/geometry.cpython-35m-x86_64-linux-gnu.so
7f3ddca4f000-7f3ddca5a000 r-xp 00000000 08:02 5528464                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/decomposition/_online_lda.cpython-35m-x86_64-linux-gnu.so
7f3ddca5a000-7f3ddcc59000 ---p 0000b000 08:02 5528464                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/decomposition/_online_lda.cpython-35m-x86_64-linux-gnu.so
7f3ddcc59000-7f3ddcc5b000 rw-p 0000a000 08:02 5528464                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/decomposition/_online_lda.cpython-35m-x86_64-linux-gnu.so
7f3ddcc5b000-7f3de2c5b000 rw-p 00000000 00:00 0 
7f3de2c6e000-7f3de2d6e000 rw-p 00000000 00:00 0 
7f3de2d6e000-7f3de2dda000 r-xp 00000000 08:02 5528512                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/kd_tree.cpython-35m-x86_64-linux-gnu.so
7f3de2dda000-7f3de2fd9000 ---p 0006c000 08:02 5528512                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/kd_tree.cpython-35m-x86_64-linux-gnu.so
7f3de2fd9000-7f3de2fe1000 rw-p 0006b000 08:02 5528512                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/kd_tree.cpython-35m-x86_64-linux-gnu.so
7f3de2fe1000-7f3de2fe2000 rw-p 00000000 00:00 0 
7f3de2fe2000-7f3de2fe6000 r-xp 00000000 08:02 5528516                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/typedefs.cpython-35m-x86_64-linux-gnu.so
7f3de2fe6000-7f3de31e6000 ---p 00004000 08:02 5528516                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/typedefs.cpython-35m-x86_64-linux-gnu.so
7f3de31e6000-7f3de31e7000 rw-p 00004000 08:02 5528516                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/typedefs.cpython-35m-x86_64-linux-gnu.so
7f3de31e7000-7f3de322b000 r-xp 00000000 08:02 5528519                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/dist_metrics.cpython-35m-x86_64-linux-gnu.so
7f3de322b000-7f3de342b000 ---p 00044000 08:02 5528519                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/dist_metrics.cpython-35m-x86_64-linux-gnu.so
7f3de342b000-7f3de3432000 rw-p 00044000 08:02 5528519                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/dist_metrics.cpython-35m-x86_64-linux-gnu.so
7f3de3432000-7f3de3433000 rw-p 00000000 00:00 0 
7f3de3433000-7f3de34a0000 r-xp 00000000 08:02 5528505                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/ball_tree.cpython-35m-x86_64-linux-gnu.so
7f3de34a0000-7f3de369f000 ---p 0006d000 08:02 5528505                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/ball_tree.cpython-35m-x86_64-linux-gnu.so
7f3de369f000-7f3de36a7000 rw-p 0006c000 08:02 5528505                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/neighbors/ball_tree.cpython-35m-x86_64-linux-gnu.so
7f3de36a7000-7f3de36a8000 rw-p 00000000 00:00 0 
7f3de36a8000-7f3de36e9000 r-xp 00000000 08:02 5528894                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/libsvm_sparse.cpython-35m-x86_64-linux-gnu.so
7f3de36e9000-7f3de38e9000 ---p 00041000 08:02 5528894                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/libsvm_sparse.cpython-35m-x86_64-linux-gnu.so
7f3de38e9000-7f3de38ec000 rw-p 00041000 08:02 5528894                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/libsvm_sparse.cpython-35m-x86_64-linux-gnu.so
7f3de38ec000-7f3de3906000 r-xp 00000000 08:02 5528893                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/liblinear.cpython-35m-x86_64-linux-gnu.so
7f3de3906000-7f3de3b06000 ---p 0001a000 08:02 5528893                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/liblinear.cpython-35m-x86_64-linux-gnu.so
7f3de3b06000-7f3de3b08000 rw-p 0001a000 08:02 5528893                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/liblinear.cpython-35m-x86_64-linux-gnu.so
7f3de3b08000-7f3de3b4e000 r-xp 00000000 08:02 5528895                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/libsvm.cpython-35m-x86_64-linux-gnu.so
7f3de3b4e000-7f3de3d4d000 ---p 00046000 08:02 5528895                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/libsvm.cpython-35m-x86_64-linux-gnu.so
7f3de3d4d000-7f3de3d51000 rw-p 00045000 08:02 5528895                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/svm/libsvm.cpython-35m-x86_64-linux-gnu.so
7f3de3d51000-7f3de3d52000 rw-p 00000000 00:00 0 
7f3de3d52000-7f3de3d69000 r-xp 00000000 08:02 5528186                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/sag_fast.cpython-35m-x86_64-linux-gnu.so
7f3de3d69000-7f3de3f68000 ---p 00017000 08:02 5528186                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/sag_fast.cpython-35m-x86_64-linux-gnu.so
7f3de3f68000-7f3de3f6b000 rw-p 00016000 08:02 5528186                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/sag_fast.cpython-35m-x86_64-linux-gnu.so
7f3de3f6b000-7f3de3f76000 r-xp 00000000 08:02 5528811                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/weight_vector.cpython-35m-x86_64-linux-gnu.so
7f3de3f76000-7f3de4175000 ---p 0000b000 08:02 5528811                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/weight_vector.cpython-35m-x86_64-linux-gnu.so
7f3de4175000-7f3de4177000 rw-p 0000a000 08:02 5528811                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/weight_vector.cpython-35m-x86_64-linux-gnu.so
7f3de4177000-7f3de41bf000 r-xp 00000000 08:02 5528171                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/sgd_fast.cpython-35m-x86_64-linux-gnu.so
7f3de41bf000-7f3de43be000 ---p 00048000 08:02 5528171                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/sgd_fast.cpython-35m-x86_64-linux-gnu.so
7f3de43be000-7f3de43c5000 rw-p 00047000 08:02 5528171                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/sgd_fast.cpython-35m-x86_64-linux-gnu.so
7f3de43c5000-7f3de43c6000 rw-p 00000000 00:00 0 
7f3de43c6000-7f3de4424000 r-xp 00000000 08:02 5528179                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/cd_fast.cpython-35m-x86_64-linux-gnu.so
7f3de4424000-7f3de4624000 ---p 0005e000 08:02 5528179                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/cd_fast.cpython-35m-x86_64-linux-gnu.so
7f3de4624000-7f3de4629000 rw-p 0005e000 08:02 5528179                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/linear_model/cd_fast.cpython-35m-x86_64-linux-gnu.so
7f3de4629000-7f3de462a000 rw-p 00000000 00:00 0 
7f3de462a000-7f3de463c000 r-xp 00000000 08:02 5528804                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/_random.cpython-35m-x86_64-linux-gnu.so
7f3de463c000-7f3de483c000 ---p 00012000 08:02 5528804                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/_random.cpython-35m-x86_64-linux-gnu.so
7f3de483c000-7f3de483f000 rw-p 00012000 08:02 5528804                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/_random.cpython-35m-x86_64-linux-gnu.so
7f3de483f000-7f3de4849000 r-xp 00000000 08:02 5528792                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/arrayfuncs.cpython-35m-x86_64-linux-gnu.so
7f3de4849000-7f3de4a48000 ---p 0000a000 08:02 5528792                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/arrayfuncs.cpython-35m-x86_64-linux-gnu.so
7f3de4a48000-7f3de4a4a000 rw-p 00009000 08:02 5528792                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/arrayfuncs.cpython-35m-x86_64-linux-gnu.so
7f3de4a4a000-7f3de4a5d000 r-xp 00000000 08:02 5528795                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/seq_dataset.cpython-35m-x86_64-linux-gnu.so
7f3de4a5d000-7f3de4c5d000 ---p 00013000 08:02 5528795                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/seq_dataset.cpython-35m-x86_64-linux-gnu.so
7f3de4c5d000-7f3de4c5f000 rw-p 00013000 08:02 5528795                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/seq_dataset.cpython-35m-x86_64-linux-gnu.so
7f3de4c5f000-7f3de4c9d000 r-xp 00000000 08:02 789824                     /usr/lib/x86_64-linux-gnu/libquadmath.so.0.0.0
7f3de4c9d000-7f3de4e9c000 ---p 0003e000 08:02 789824                     /usr/lib/x86_64-linux-gnu/libquadmath.so.0.0.0
7f3de4e9c000-7f3de4e9d000 r--p 0003d000 08:02 789824                     /usr/lib/x86_64-linux-gnu/libquadmath.so.0.0.0
7f3de4e9d000-7f3de4e9e000 rw-p 0003e000 08:02 789824                     /usr/lib/x86_64-linux-gnu/libquadmath.so.0.0.0
7f3de4e9e000-7f3de4fc7000 r-xp 00000000 08:02 791239                     /usr/lib/x86_64-linux-gnu/libgfortran.so.3.0.0
7f3de4fc7000-7f3de51c6000 ---p 00129000 08:02 791239                     /usr/lib/x86_64-linux-gnu/libgfortran.so.3.0.0
7f3de51c6000-7f3de51c7000 r--p 00128000 08:02 791239                     /usr/lib/x86_64-linux-gnu/libgfortran.so.3.0.0
7f3de51c7000-7f3de51c9000 rw-p 00129000 08:02 791239                     /usr/lib/x86_64-linux-gnu/libgfortran.so.3.0.0
7f3de51c9000-7f3de65f3000 r-xp 00000000 08:02 4459771                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so.9.0.176
7f3de65f3000-7f3de67f2000 ---p 0142a000 08:02 4459771                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so.9.0.176
7f3de67f2000-7f3de699c000 rw-p 01429000 08:02 4459771                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libnvrtc.so.9.0.176
7f3de699c000-7f3de6a14000 rw-p 00000000 00:00 0 
7f3de6a14000-7f3de8878000 r-xp 00000000 08:02 791451                     /usr/lib/libopenblasp-r0.2.18.so
7f3de8878000-7f3de8a77000 ---p 01e64000 08:02 791451                     /usr/lib/libopenblasp-r0.2.18.so
7f3de8a77000-7f3de8a7d000 r--p 01e63000 08:02 791451                     /usr/lib/libopenblasp-r0.2.18.so
7f3de8a7d000-7f3de8a8f000 rw-p 01e69000 08:02 791451                     /usr/lib/libopenblasp-r0.2.18.so
7f3de8a8f000-7f3de8aa8000 rw-p 00000000 00:00 0 
7f3de8aa8000-7f3dfd4f5000 r-xp 00000000 08:11 49941659                   /hdd/face-detection/mxnet/python/mxnet/libmxnet.so
7f3dfd4f5000-7f3dfd6f4000 ---p 14a4d000 08:11 49941659                   /hdd/face-detection/mxnet/python/mxnet/libmxnet.so
7f3dfd6f4000-7f3dfd72f000 r--p 14a4c000 08:11 49941659                   /hdd/face-detection/mxnet/python/mxnet/libmxnet.so
7f3dfd72f000-7f3dfd757000 rw-p 14a87000 08:11 49941659                   /hdd/face-detection/mxnet/python/mxnet/libmxnet.so
7f3dfd757000-7f3dfe0a1000 rw-p 00000000 00:00 0 
7f3dfe0a1000-7f3dfe0ea000 r-xp 00000000 08:02 5512128                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so
7f3dfe0ea000-7f3dfe2ea000 ---p 00049000 08:02 5512128                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so
7f3dfe2ea000-7f3dfe2ee000 rw-p 00049000 08:02 5512128                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so
7f3dfe2ee000-7f3dfe2ef000 rw-p 00000000 00:00 0 
7f3dfe2ef000-7f3dfe319000 r-xp 00000000 08:02 5512182                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_hausdorff.cpython-35m-x86_64-linux-gnu.so
7f3dfe319000-7f3dfe518000 ---p 0002a000 08:02 5512182                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_hausdorff.cpython-35m-x86_64-linux-gnu.so
7f3dfe518000-7f3dfe51c000 rw-p 00029000 08:02 5512182                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_hausdorff.cpython-35m-x86_64-linux-gnu.so
7f3dfe51c000-7f3dfe531000 r-xp 00000000 08:02 5512181                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_distance_wrap.cpython-35m-x86_64-linux-gnu.so
7f3dfe531000-7f3dfe731000 ---p 00015000 08:02 5512181                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_distance_wrap.cpython-35m-x86_64-linux-gnu.so
7f3dfe731000-7f3dfe732000 rw-p 00015000 08:02 5512181                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_distance_wrap.cpython-35m-x86_64-linux-gnu.so
7f3dfe732000-7f3dfe75b000 r-xp 00000000 08:02 5512183                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_voronoi.cpython-35m-x86_64-linux-gnu.so
7f3dfe75b000-7f3dfe95b000 ---p 00029000 08:02 5512183                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_voronoi.cpython-35m-x86_64-linux-gnu.so
7f3dfe95b000-7f3dfe95e000 rw-p 00029000 08:02 5512183                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/_voronoi.cpython-35m-x86_64-linux-gnu.so
7f3dfe95e000-7f3dfe95f000 rw-p 00000000 00:00 0 
7f3dfe95f000-7f3dfe968000 r-xp 00000000 08:02 5511102                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/_lib/messagestream.cpython-35m-x86_64-linux-gnu.so
7f3dfe968000-7f3dfeb68000 ---p 00009000 08:02 5511102                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/_lib/messagestream.cpython-35m-x86_64-linux-gnu.so
7f3dfeb68000-7f3dfeb69000 rw-p 00009000 08:02 5511102                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/_lib/messagestream.cpython-35m-x86_64-linux-gnu.so
7f3dfeb69000-7f3dfec3b000 r-xp 00000000 08:02 5512184                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/qhull.cpython-35m-x86_64-linux-gnu.so
7f3dfec3b000-7f3dfee3a000 ---p 000d2000 08:02 5512184                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/qhull.cpython-35m-x86_64-linux-gnu.so
7f3dfee3a000-7f3dfee43000 rw-p 000d1000 08:02 5512184                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/qhull.cpython-35m-x86_64-linux-gnu.so
7f3dfee43000-7f3dfee45000 rw-p 00000000 00:00 0 
7f3dfee45000-7f3dfee4b000 rw-p 003e1000 08:02 5512184                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/qhull.cpython-35m-x86_64-linux-gnu.so
7f3dfee4b000-7f3dfeee0000 r-xp 00000000 08:02 5512174                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/ckdtree.cpython-35m-x86_64-linux-gnu.so
7f3dfeee0000-7f3dff0df000 ---p 00095000 08:02 5512174                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/ckdtree.cpython-35m-x86_64-linux-gnu.so
7f3dff0df000-7f3dff0e8000 rw-p 00094000 08:02 5512174                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/spatial/ckdtree.cpython-35m-x86_64-linux-gnu.so
7f3dff0e8000-7f3dff139000 r-xp 00000000 08:02 5511801                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/interpnd.cpython-35m-x86_64-linux-gnu.so
7f3dff139000-7f3dff339000 ---p 00051000 08:02 5511801                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/interpnd.cpython-35m-x86_64-linux-gnu.so
7f3dff339000-7f3dff33e000 rw-p 00051000 08:02 5511801                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/interpnd.cpython-35m-x86_64-linux-gnu.so
7f3dff33e000-7f3dff33f000 rw-p 00000000 00:00 0 
7f3dff33f000-7f3dff38d000 r-xp 00000000 08:02 5511803                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_ppoly.cpython-35m-x86_64-linux-gnu.so
7f3dff38d000-7f3dff58d000 ---p 0004e000 08:02 5511803                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_ppoly.cpython-35m-x86_64-linux-gnu.so
7f3dff58d000-7f3dff593000 rw-p 0004e000 08:02 5511803                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_ppoly.cpython-35m-x86_64-linux-gnu.so
7f3dff593000-7f3dff594000 rw-p 00000000 00:00 0 
7f3dff594000-7f3dff598000 rw-p 0019f000 08:02 5511803                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_ppoly.cpython-35m-x86_64-linux-gnu.so
7f3dff598000-7f3dff5d4000 r-xp 00000000 08:02 5511806                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_bspl.cpython-35m-x86_64-linux-gnu.so
7f3dff5d4000-7f3dff7d3000 ---p 0003c000 08:02 5511806                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_bspl.cpython-35m-x86_64-linux-gnu.so
7f3dff7d3000-7f3dff7d9000 rw-p 0003b000 08:02 5511806                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_bspl.cpython-35m-x86_64-linux-gnu.so
7f3dff7d9000-7f3dff7da000 rw-p 00000000 00:00 0 
7f3dff7da000-7f3dff7dd000 rw-p 0013f000 08:02 5511806                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_bspl.cpython-35m-x86_64-linux-gnu.so
7f3dff7dd000-7f3dff83c000 r-xp 00000000 08:02 5511808                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/dfitpack.cpython-35m-x86_64-linux-gnu.so
7f3dff83c000-7f3dffa3c000 ---p 0005f000 08:02 5511808                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/dfitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffa3c000-7f3dffa43000 rw-p 0005f000 08:02 5511808                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/dfitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffa43000-7f3dffa45000 rw-p 00112000 08:02 5511808                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/dfitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffa45000-7f3dffa79000 r-xp 00000000 08:02 5511800                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_fitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffa79000-7f3dffc79000 ---p 00034000 08:02 5511800                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_fitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffc79000-7f3dffc7a000 rw-p 00034000 08:02 5511800                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_fitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffc7a000-7f3dffc7c000 rw-p 00091000 08:02 5511800                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/interpolate/_fitpack.cpython-35m-x86_64-linux-gnu.so
7f3dffc7c000-7f3dffc8d000 r-xp 00000000 08:02 5511406                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ellip_harm_2.cpython-35m-x86_64-linux-gnu.so
7f3dffc8d000-7f3dffe8d000 ---p 00011000 08:02 5511406                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ellip_harm_2.cpython-35m-x86_64-linux-gnu.so
7f3dffe8d000-7f3dffe8f000 rw-p 00011000 08:02 5511406                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ellip_harm_2.cpython-35m-x86_64-linux-gnu.so
7f3dffe8f000-7f3dffe93000 rw-p 0005a000 08:02 5511406                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ellip_harm_2.cpython-35m-x86_64-linux-gnu.so
7f3dffe93000-7f3dffe99000 r-xp 00000000 08:02 5511398                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_comb.cpython-35m-x86_64-linux-gnu.so
7f3dffe99000-7f3e00099000 ---p 00006000 08:02 5511398                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_comb.cpython-35m-x86_64-linux-gnu.so
7f3e00099000-7f3e0009a000 rw-p 00006000 08:02 5511398                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_comb.cpython-35m-x86_64-linux-gnu.so
7f3e0009a000-7f3e0014f000 r-xp 00000000 08:02 5511401                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/specfun.cpython-35m-x86_64-linux-gnu.so
7f3e0014f000-7f3e0034f000 ---p 000b5000 08:02 5511401                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/specfun.cpython-35m-x86_64-linux-gnu.so
7f3e0034f000-7f3e00356000 rw-p 000b5000 08:02 5511401                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/specfun.cpython-35m-x86_64-linux-gnu.so
7f3e00356000-7f3e00358000 rw-p 0019d000 08:02 5511401                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/specfun.cpython-35m-x86_64-linux-gnu.so
7f3e00358000-7f3e00374000 r-xp 00000000 08:02 5511407                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs_cxx.cpython-35m-x86_64-linux-gnu.so
7f3e00374000-7f3e00574000 ---p 0001c000 08:02 5511407                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs_cxx.cpython-35m-x86_64-linux-gnu.so
7f3e00574000-7f3e00575000 rw-p 0001c000 08:02 5511407                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs_cxx.cpython-35m-x86_64-linux-gnu.so
7f3e00575000-7f3e00576000 rw-p 00000000 00:00 0 
7f3e00576000-7f3e006f5000 r-xp 00000000 08:02 5511395                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs.cpython-35m-x86_64-linux-gnu.so
7f3e006f5000-7f3e008f4000 ---p 0017f000 08:02 5511395                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs.cpython-35m-x86_64-linux-gnu.so
7f3e008f4000-7f3e008fd000 rw-p 0017e000 08:02 5511395                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs.cpython-35m-x86_64-linux-gnu.so
7f3e008fd000-7f3e00905000 rw-p 00000000 00:00 0 
7f3e00905000-7f3e0090a000 rw-p 004b5000 08:02 5511395                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/special/_ufuncs.cpython-35m-x86_64-linux-gnu.so
7f3e0090a000-7f3e00925000 r-xp 00000000 08:02 5512133                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so
7f3e00925000-7f3e00b24000 ---p 0001b000 08:02 5512133                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so
7f3e00b24000-7f3e00b25000 rw-p 0001a000 08:02 5512133                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so
7f3e00b25000-7f3e00bb0000 r-xp 00000000 08:02 5511283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
7f3e00bb0000-7f3e00daf000 ---p 0008b000 08:02 5511283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
7f3e00daf000-7f3e00db3000 rw-p 0008a000 08:02 5511283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
7f3e00db3000-7f3e00dc4000 rw-p 001eb000 08:02 5511283                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so
7f3e00dc4000-7f3e00dfe000 r-xp 00000000 08:02 5511267                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
7f3e00dfe000-7f3e00ffe000 ---p 0003a000 08:02 5511267                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
7f3e00ffe000-7f3e01002000 rw-p 0003a000 08:02 5511267                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
7f3e01002000-7f3e01009000 rw-p 00133000 08:02 5511267                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so
7f3e01009000-7f3e01051000 r-xp 00000000 08:02 5511257                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so
7f3e01051000-7f3e01250000 ---p 00048000 08:02 5511257                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so
7f3e01250000-7f3e01256000 rw-p 00047000 08:02 5511257                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so
7f3e01256000-7f3e01257000 rw-p 00000000 00:00 0 
7f3e01257000-7f3e0128b000 r-xp 00000000 08:02 5511272                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so
7f3e0128b000-7f3e0148b000 ---p 00034000 08:02 5511272                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so
7f3e0148b000-7f3e0148f000 rw-p 00034000 08:02 5511272                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so
7f3e0148f000-7f3e01490000 rw-p 00000000 00:00 0 
7f3e01490000-7f3e05490000 rw-p 00000000 00:00 0 
7f3e05490000-7f3e05577000 r-xp 00000000 08:02 5511249                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
7f3e05577000-7f3e05777000 ---p 000e7000 08:02 5511249                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
7f3e05777000-7f3e057cc000 rw-p 000e7000 08:02 5511249                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
7f3e057cc000-7f3e057d4000 rw-p 00335000 08:02 5511249                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so
7f3e057d4000-7f3e0b7d4000 rw-p 00000000 00:00 0 
7f3e0b7d4000-7f3e0b7d5000 ---p 00000000 00:00 0 
7f3e0b7d5000-7f3e0bfd5000 rw-p 00000000 00:00 0 
7f3e0bfd5000-7f3e0bfd6000 ---p 00000000 00:00 0 
7f3e0bfd6000-7f3e0c7d6000 rw-p 00000000 00:00 0 
7f3e0c7d6000-7f3e0c7d7000 ---p 00000000 00:00 0 
7f3e0c7d7000-7f3e0cfd7000 rw-p 00000000 00:00 0 
7f3e0cfd7000-7f3e0cfd8000 ---p 00000000 00:00 0 
7f3e0cfd8000-7f3e0d7d8000 rw-p 00000000 00:00 0 
7f3e0d7d8000-7f3e137d8000 rw-p 00000000 00:00 0 
7f3e13803000-7f3e13983000 rw-p 00000000 00:00 0 
7f3e13983000-7f3e139b8000 r-xp 00000000 08:02 5527935                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/metrics/pairwise_fast.cpython-35m-x86_64-linux-gnu.so
7f3e139b8000-7f3e13bb7000 ---p 00035000 08:02 5527935                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/metrics/pairwise_fast.cpython-35m-x86_64-linux-gnu.so
7f3e13bb7000-7f3e13bbb000 rw-p 00034000 08:02 5527935                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/metrics/pairwise_fast.cpython-35m-x86_64-linux-gnu.so
7f3e13bbb000-7f3e13bbc000 rw-p 00000000 00:00 0 
7f3e13bbc000-7f3e13bc0000 r-xp 00000000 08:02 5528810                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/lgamma.cpython-35m-x86_64-linux-gnu.so
7f3e13bc0000-7f3e13dbf000 ---p 00004000 08:02 5528810                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/lgamma.cpython-35m-x86_64-linux-gnu.so
7f3e13dbf000-7f3e13dc0000 rw-p 00003000 08:02 5528810                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/lgamma.cpython-35m-x86_64-linux-gnu.so
7f3e13dc0000-7f3e13dd7000 r-xp 00000000 08:02 5527943                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/metrics/cluster/expected_mutual_info_fast.cpython-35m-x86_64-linux-gnu.so
7f3e13dd7000-7f3e13fd7000 ---p 00017000 08:02 5527943                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/metrics/cluster/expected_mutual_info_fast.cpython-35m-x86_64-linux-gnu.so
7f3e13fd7000-7f3e13fd9000 rw-p 00017000 08:02 5527943                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/metrics/cluster/expected_mutual_info_fast.cpython-35m-x86_64-linux-gnu.so
7f3e13fd9000-7f3e17fd9000 rw-p 00000000 00:00 0 
7f3e17fec000-7f3e1806c000 rw-p 00000000 00:00 0 
7f3e1806c000-7f3e18087000 r-xp 00000000 08:02 5511550                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/streams.cpython-35m-x86_64-linux-gnu.so
7f3e18087000-7f3e18287000 ---p 0001b000 08:02 5511550                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/streams.cpython-35m-x86_64-linux-gnu.so
7f3e18287000-7f3e18289000 rw-p 0001b000 08:02 5511550                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/streams.cpython-35m-x86_64-linux-gnu.so
7f3e18289000-7f3e1828a000 rw-p 00000000 00:00 0 
7f3e1828a000-7f3e182b8000 r-xp 00000000 08:02 5511547                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/mio5_utils.cpython-35m-x86_64-linux-gnu.so
7f3e182b8000-7f3e184b8000 ---p 0002e000 08:02 5511547                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/mio5_utils.cpython-35m-x86_64-linux-gnu.so
7f3e184b8000-7f3e184bc000 rw-p 0002e000 08:02 5511547                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/mio5_utils.cpython-35m-x86_64-linux-gnu.so
7f3e184bc000-7f3e184fd000 rw-p 00000000 00:00 0 
7f3e184fd000-7f3e18505000 r-xp 00000000 08:02 5511551                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/mio_utils.cpython-35m-x86_64-linux-gnu.so
7f3e18505000-7f3e18705000 ---p 00008000 08:02 5511551                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/mio_utils.cpython-35m-x86_64-linux-gnu.so
7f3e18705000-7f3e18706000 rw-p 00008000 08:02 5511551                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/io/matlab/mio_utils.cpython-35m-x86_64-linux-gnu.so
7f3e18706000-7f3e1872c000 r-xp 00000000 08:02 5528456                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/decomposition/cdnmf_fast.cpython-35m-x86_64-linux-gnu.so
7f3e1872c000-7f3e1892b000 ---p 00026000 08:02 5528456                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/decomposition/cdnmf_fast.cpython-35m-x86_64-linux-gnu.so
7f3e1892b000-7f3e1892e000 rw-p 00025000 08:02 5528456                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/decomposition/cdnmf_fast.cpython-35m-x86_64-linux-gnu.so
7f3e1892e000-7f3e1892f000 rw-p 00000000 00:00 0 
7f3e1892f000-7f3e189bb000 r-xp 00000000 08:02 5528808                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/sparsefuncs_fast.cpython-35m-x86_64-linux-gnu.so
7f3e189bb000-7f3e18bba000 ---p 0008c000 08:02 5528808                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/sparsefuncs_fast.cpython-35m-x86_64-linux-gnu.so
7f3e18bba000-7f3e18bc0000 rw-p 0008b000 08:02 5528808                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/sparsefuncs_fast.cpython-35m-x86_64-linux-gnu.so
7f3e18bc0000-7f3e18bc1000 rw-p 00000000 00:00 0 
7f3e18bc1000-7f3e18bc9000 r-xp 00000000 08:02 5528791                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/_logistic_sigmoid.cpython-35m-x86_64-linux-gnu.so
7f3e18bc9000-7f3e18dc9000 ---p 00008000 08:02 5528791                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/_logistic_sigmoid.cpython-35m-x86_64-linux-gnu.so
7f3e18dc9000-7f3e18dca000 rw-p 00008000 08:02 5528791                    /home/socian-ai/.local/lib/python3.5/site-packages/sklearn/utils/_logistic_sigmoid.cpython-35m-x86_64-linux-gnu.so
7f3e18dca000-7f3e18dda000 r-xp 00000000 08:11 76156199                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/gpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e18dda000-7f3e18fd9000 ---p 00010000 08:11 76156199                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/gpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e18fd9000-7f3e18fda000 r--p 0000f000 08:11 76156199                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/gpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e18fda000-7f3e18fdb000 rw-p 00010000 08:11 76156199                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/gpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e18fdb000-7f3e1afdb000 rw-p 00000000 00:00 0 
7f3e1afdb000-7f3e1afdc000 ---p 00000000 00:00 0 
7f3e1afdc000-7f3e1b7dc000 rw-p 00000000 00:00 0 
7f3e1b7dc000-7f3e1d7dc000 rw-p 00000000 00:00 0 
7f3e1d7df000-7f3e1d81f000 rw-p 00000000 00:00 0 
7f3e1d81f000-7f3e1d82b000 r-xp 00000000 08:11 76156198                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/cpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e1d82b000-7f3e1da2a000 ---p 0000c000 08:11 76156198                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/cpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e1da2a000-7f3e1da2b000 r--p 0000b000 08:11 76156198                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/cpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e1da2b000-7f3e1da2c000 rw-p 0000c000 08:11 76156198                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/cpu_nms.cpython-35m-x86_64-linux-gnu.so
7f3e1da2c000-7f3e1da35000 r-xp 00000000 08:11 76156197                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/anchors.cpython-35m-x86_64-linux-gnu.so
7f3e1da35000-7f3e1dc34000 ---p 00009000 08:11 76156197                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/anchors.cpython-35m-x86_64-linux-gnu.so
7f3e1dc34000-7f3e1dc35000 r--p 00008000 08:11 76156197                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/anchors.cpython-35m-x86_64-linux-gnu.so
7f3e1dc35000-7f3e1dc36000 rw-p 00009000 08:11 76156197                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/anchors.cpython-35m-x86_64-linux-gnu.so
7f3e1dc36000-7f3e1dc3e000 r-xp 00000000 08:11 76156196                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/bbox.cpython-35m-x86_64-linux-gnu.so
7f3e1dc3e000-7f3e1de3d000 ---p 00008000 08:11 76156196                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/bbox.cpython-35m-x86_64-linux-gnu.so
7f3e1de3d000-7f3e1de3e000 r--p 00007000 08:11 76156196                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/bbox.cpython-35m-x86_64-linux-gnu.so
7f3e1de3e000-7f3e1de3f000 rw-p 00008000 08:11 76156196                   /hdd/face-detection/face-detection/socian-face-detection/insightface/SSH/rcnn/cython/bbox.cpython-35m-x86_64-linux-gnu.so
7f3e1de3f000-7f3e1de50000 r-xp 00000000 08:02 5511164                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/mvn.cpython-35m-x86_64-linux-gnu.so
7f3e1de50000-7f3e1e04f000 ---p 00011000 08:02 5511164                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/mvn.cpython-35m-x86_64-linux-gnu.so
7f3e1e04f000-7f3e1e051000 rw-p 00010000 08:02 5511164                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/mvn.cpython-35m-x86_64-linux-gnu.so
7f3e1e051000-7f3e1e148000 rw-p 00000000 00:00 0 
7f3e1e148000-7f3e1e14a000 rw-p 00031000 08:02 5511164                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/mvn.cpython-35m-x86_64-linux-gnu.so
7f3e1e14a000-7f3e1e154000 r-xp 00000000 08:02 5511165                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/statlib.cpython-35m-x86_64-linux-gnu.so
7f3e1e154000-7f3e1e353000 ---p 0000a000 08:02 5511165                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/statlib.cpython-35m-x86_64-linux-gnu.so
7f3e1e353000-7f3e1e355000 rw-p 00009000 08:02 5511165                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/statlib.cpython-35m-x86_64-linux-gnu.so
7f3e1e355000-7f3e1e357000 rw-p 00026000 08:02 5511165                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/statlib.cpython-35m-x86_64-linux-gnu.so
7f3e1e357000-7f3e1e3bb000 r-xp 00000000 08:02 5511163                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/_stats.cpython-35m-x86_64-linux-gnu.so
7f3e1e3bb000-7f3e1e5bb000 ---p 00064000 08:02 5511163                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/_stats.cpython-35m-x86_64-linux-gnu.so
7f3e1e5bb000-7f3e1e5c0000 rw-p 00064000 08:02 5511163                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/stats/_stats.cpython-35m-x86_64-linux-gnu.so
7f3e1e5c0000-7f3e1e5c1000 rw-p 00000000 00:00 0 
7f3e1e5c1000-7f3e1e5da000 r-xp 00000000 08:02 5512072                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/integrate/lsoda.cpython-35m-x86_64-linux-gnu.so
7f3e1e5da000-7f3e1e7d9000 ---p 00019000 08:02 5512072                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/integrate/lsoda.cpython-35m-x86_64-linux-gnu.so
7f3e1e7d9000-7f3e1e7db000 rw-p 00018000 08:02 5512072                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/integrate/lsoda.cpython-35m-x86_64-linux-gnu.so
7f3e1e7db000-7f3e1e7de000 rw-p 00048000 08:02 5512072                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/integrate/lsoda.cpython-35m-x86_64-linux-gnu.so
7f3e1e7de000-7f3e247de000 rw-p 00000000 00:00 0 
7f3e247de000-7f3e26af2000 r-xp 00000000 08:02 5512069                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/.libs/libopenblasp-r0-39a31c03.2.18.so
7f3e26af2000-7f3e26cf1000 ---p 02314000 08:02 5512069                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/.libs/libopenblasp-r0-39a31c03.2.18.so
7f3e26cf1000-7f3e26d10000 rw-p 02313000 08:02 5512069                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/.libs/libopenblasp-r0-39a31c03.2.18.so
7f3e26d10000-7f3e26d73000 rw-p 00000000 00:00 0 
7f3e26d73000-7f3e26e09000 rw-p 02425000 08:02 5512069                    /home/socian-ai/.local/lib/python3.5/site-packages/scipy/.libs/libopenblasp-r0-39a31c03.2.18.so
7f3e26e09000-7f3e26e20000 r-xp 00000000 08:02 5516432                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so
7f3e26e20000-7f3e27020000 ---p 00017000 08:02 5516432                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so
7f3e27020000-7f3e27026000 rw-p 00017000 08:02 5516432                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so
7f3e27026000-7f3e2702f000 r-xp 00000000 08:02 5516445                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
7f3e2702f000-7f3e2722f000 ---p 00009000 08:02 5516445                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
7f3e2722f000-7f3e27230000 rw-p 00009000 08:02 5516445                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
7f3e27230000-7f3e27233000 rw-p 0000b000 08:02 5516445                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so
7f3e27233000-7f3e272a4000 r-xp 00000000 08:02 5516428                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
7f3e272a4000-7f3e274a3000 ---p 00071000 08:02 5516428                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
7f3e274a3000-7f3e274ae000 rw-p 00070000 08:02 5516428                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
7f3e274ae000-7f3e274af000 rw-p 00000000 00:00 0 
7f3e274af000-7f3e274b3000 rw-p 0007b000 08:02 5516428                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so
7f3e274b3000-7f3e274e5000 r-xp 00000000 08:02 5516447                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
7f3e274e5000-7f3e276e4000 ---p 00032000 08:02 5516447                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
7f3e276e4000-7f3e276e6000 rw-p 00031000 08:02 5516447                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
7f3e276e6000-7f3e276ed000 rw-p 00034000 08:02 5516447                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so
7f3e276ed000-7f3e2770c000 r-xp 00000000 08:02 5516435                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so
7f3e2770c000-7f3e2790b000 ---p 0001f000 08:02 5516435                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so
7f3e2790b000-7f3e27912000 rw-p 0001e000 08:02 5516435                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so
7f3e27912000-7f3e2791f000 r-xp 00000000 08:02 5516426                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
7f3e2791f000-7f3e27b1e000 ---p 0000d000 08:02 5516426                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
7f3e27b1e000-7f3e27b20000 rw-p 0000c000 08:02 5516426                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
7f3e27b20000-7f3e27b21000 rw-p 00000000 00:00 0 
7f3e27b21000-7f3e27b24000 rw-p 0000f000 08:02 5516426                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so
7f3e27b24000-7f3e27b36000 r-xp 00000000 08:02 5516451                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
7f3e27b36000-7f3e27d36000 ---p 00012000 08:02 5516451                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
7f3e27d36000-7f3e27d37000 rw-p 00012000 08:02 5516451                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
7f3e27d37000-7f3e27d3a000 rw-p 00014000 08:02 5516451                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so
7f3e27d3a000-7f3e27d7a000 rw-p 00000000 00:00 0 
7f3e27d7a000-7f3e27d82000 r-xp 00000000 08:02 5516536                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libaec-2147abcd.so.0.0.4
7f3e27d82000-7f3e27f81000 ---p 00008000 08:02 5516536                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libaec-2147abcd.so.0.0.4
7f3e27f81000-7f3e27f82000 rw-p 00007000 08:02 5516536                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libaec-2147abcd.so.0.0.4
7f3e27f82000-7f3e27f83000 rw-p 00009000 08:02 5516536                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libaec-2147abcd.so.0.0.4
7f3e27f83000-7f3e27f85000 r-xp 00000000 08:02 5516535                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libsz-1c7dd0cf.so.2.0.1
7f3e27f85000-7f3e28184000 ---p 00002000 08:02 5516535                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libsz-1c7dd0cf.so.2.0.1
7f3e28184000-7f3e28187000 rw-p 00001000 08:02 5516535                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libsz-1c7dd0cf.so.2.0.1
7f3e28187000-7f3e281a8000 r-xp 00000000 08:02 5516537                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5_hl-d68fbc5b.so.100.1.0
7f3e281a8000-7f3e283a8000 ---p 00021000 08:02 5516537                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5_hl-d68fbc5b.so.100.1.0
7f3e283a8000-7f3e283a9000 rw-p 00021000 08:02 5516537                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5_hl-d68fbc5b.so.100.1.0
7f3e283a9000-7f3e283aa000 rw-p 00000000 00:00 0 
7f3e283aa000-7f3e283b4000 rw-p 00023000 08:02 5516537                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5_hl-d68fbc5b.so.100.1.0
7f3e283b4000-7f3e28751000 r-xp 00000000 08:02 5516539                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5-b253f31d.so.101.1.0
7f3e28751000-7f3e28951000 ---p 0039d000 08:02 5516539                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5-b253f31d.so.101.1.0
7f3e28951000-7f3e28963000 rw-p 0039d000 08:02 5516539                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5-b253f31d.so.101.1.0
7f3e28963000-7f3e28965000 rw-p 00000000 00:00 0 
7f3e28965000-7f3e289a3000 rw-p 003af000 08:02 5516539                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/.libs/libhdf5-b253f31d.so.101.1.0
7f3e289a3000-7f3e289ae000 r-xp 00000000 08:02 5516439                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
7f3e289ae000-7f3e28bae000 ---p 0000b000 08:02 5516439                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
7f3e28bae000-7f3e28baf000 rw-p 0000b000 08:02 5516439                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
7f3e28baf000-7f3e28bb2000 rw-p 0000d000 08:02 5516439                    /home/socian-ai/.local/lib/python3.5/site-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so
7f3e28bb2000-7f3e299b2000 rw-p 00000000 00:00 0 
7f3e299b2000-7f3e299c6000 r-xp 00000000 08:02 5525049                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/framework/fast_tensor_util.so
7f3e299c6000-7f3e29bc5000 ---p 00014000 08:02 5525049                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/framework/fast_tensor_util.so
7f3e29bc5000-7f3e29bc6000 r--p 00013000 08:02 5525049                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/framework/fast_tensor_util.so
7f3e29bc6000-7f3e29bc8000 rw-p 00014000 08:02 5525049                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/framework/fast_tensor_util.so
7f3e29bc8000-7f3e29c88000 rw-p 00000000 00:00 0 
7f3e29c88000-7f3e29c8c000 r-xp 00000000 08:02 1056152                    /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
7f3e29c8c000-7f3e29e8b000 ---p 00004000 08:02 1056152                    /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
7f3e29e8b000-7f3e29e8c000 r--p 00003000 08:02 1056152                    /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
7f3e29e8c000-7f3e29e8e000 rw-p 00004000 08:02 1056152                    /usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so
7f3e29e8e000-7f3e29e95000 r-xp 00000000 08:02 1056141                    /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
7f3e29e95000-7f3e2a095000 ---p 00007000 08:02 1056141                    /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
7f3e2a095000-7f3e2a096000 r--p 00007000 08:02 1056141                    /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
7f3e2a096000-7f3e2a098000 rw-p 00008000 08:02 1056141                    /usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so
7f3e2a098000-7f3e2a1d8000 rw-p 00000000 00:00 0 
7f3e2a1d8000-7f3e2a3ee000 r-xp 00000000 08:02 5517692                    /home/socian-ai/.local/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so
7f3e2a3ee000-7f3e2a5ed000 ---p 00216000 08:02 5517692                    /home/socian-ai/.local/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so
7f3e2a5ed000-7f3e2a5fe000 rw-p 00215000 08:02 5517692                    /home/socian-ai/.local/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so
7f3e2a5fe000-7f3e2a5ff000 rw-p 00000000 00:00 0 
7f3e2a5ff000-7f3e2a600000 r-xp 00000000 08:02 5517538                    /home/socian-ai/.local/lib/python3.5/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so
7f3e2a600000-7f3e2a7ff000 ---p 00001000 08:02 5517538                    /home/socian-ai/.local/lib/python3.5/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so
7f3e2a7ff000-7f3e2a800000 rw-p 00000000 08:02 5517538                    /home/socian-ai/.local/lib/python3.5/site-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so
7f3e2a800000-7f3e2aa00000 rw-p 00000000 00:00 0 
7f3e2aa31000-7f3e2aa74000 r-xp 00000000 08:02 988164                     /usr/lib/nvidia-384/libnvidia-fatbinaryloader.so.384.130
7f3e2aa74000-7f3e2ac73000 ---p 00043000 08:02 988164                     /usr/lib/nvidia-384/libnvidia-fatbinaryloader.so.384.130
7f3e2ac73000-7f3e2ac7e000 rw-p 00042000 08:02 988164                     /usr/lib/nvidia-384/libnvidia-fatbinaryloader.so.384.130
7f3e2ac7e000-7f3e2ac83000 rw-p 00000000 00:00 0 
7f3e2ac83000-7f3e2d10c000 r-xp 00000000 08:02 4459730                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0.176
7f3e2d10c000-7f3e2d30b000 ---p 02489000 08:02 4459730                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0.176
7f3e2d30b000-7f3e2e6dd000 rw-p 02488000 08:02 4459730                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0.176
7f3e2e6dd000-7f3e2ebe7000 rw-p 00000000 00:00 0 
7f3e2ebe7000-7f3e36a15000 r-xp 00000000 08:02 4459718                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0.176
7f3e36a15000-7f3e36c15000 ---p 07e2e000 08:02 4459718                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0.176
7f3e36c15000-7f3e36c24000 rw-p 07e2e000 08:02 4459718                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0.176
7f3e36c24000-7f3e36c88000 rw-p 00000000 00:00 0 
7f3e36c88000-7f3e47e82000 r-xp 00000000 08:02 4459884                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5
7f3e47e82000-7f3e48082000 ---p 111fa000 08:02 4459884                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5
7f3e48082000-7f3e480d5000 rw-p 111fa000 08:02 4459884                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.0.5
7f3e480d5000-7f3e4811f000 rw-p 00000000 00:00 0 
7f3e4811f000-7f3e48c3f000 r-xp 00000000 08:02 786455                     /usr/lib/x86_64-linux-gnu/libcuda.so.384.130
7f3e48c3f000-7f3e48e3e000 ---p 00b20000 08:02 786455                     /usr/lib/x86_64-linux-gnu/libcuda.so.384.130
7f3e48e3e000-7f3e48f8f000 rw-p 00b1f000 08:02 786455                     /usr/lib/x86_64-linux-gnu/libcuda.so.384.130
7f3e48f8f000-7f3e48f9d000 rw-p 00000000 00:00 0 
7f3e48f9d000-7f3e48fbe000 r-xp 00000000 08:02 786703                     /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f3e48fbe000-7f3e491bd000 ---p 00021000 08:02 786703                     /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f3e491bd000-7f3e491be000 r--p 00020000 08:02 786703                     /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f3e491be000-7f3e491bf000 rw-p 00021000 08:02 786703                     /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
7f3e491bf000-7f3e49228000 r-xp 00000000 08:02 4459578                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
7f3e49228000-7f3e49427000 ---p 00069000 08:02 4459578                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
7f3e49427000-7f3e4942b000 rw-p 00068000 08:02 4459578                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
7f3e4942b000-7f3e4942c000 rw-p 00000000 00:00 0 
7f3e4942c000-7f3e4dddb000 r-xp 00000000 08:02 4459753                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0.176
7f3e4dddb000-7f3e4dfdb000 ---p 049af000 08:02 4459753                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0.176
7f3e4dfdb000-7f3e4e015000 rw-p 049af000 08:02 4459753                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcusolver.so.9.0.176
7f3e4e015000-7f3e4e027000 rw-p 00000000 00:00 0 
7f3e4e027000-7f3e51218000 r-xp 00000000 08:02 4459704                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0.176
7f3e51218000-7f3e51417000 ---p 031f1000 08:02 4459704                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0.176
7f3e51417000-7f3e5144e000 rw-p 031f0000 08:02 4459704                    /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0.176
7f3e5144e000-7f3e5145d000 rw-p 00000000 00:00 0 
7f3e5145d000-7f3e521b5000 r-xp 00000000 08:02 5518363                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so
7f3e521b5000-7f3e523b5000 ---p 00d58000 08:02 5518363                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so
7f3e523b5000-7f3e523fe000 r--p 00d58000 08:02 5518363                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so
7f3e523fe000-7f3e52401000 rw-p 00da1000 08:02 5518363                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/libtensorflow_framework.so
7f3e52401000-7f3e5240b000 rw-p 00000000 00:00 0 
7f3e5240b000-7f3e7ce50000 r-xp 00000000 08:02 5524395                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
7f3e7ce50000-7f3e7d04f000 ---p 2aa45000 08:02 5524395                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
7f3e7d04f000-7f3e7d1d0000 r--p 2aa44000 08:02 5524395                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
7f3e7d1d0000-7f3e7d1df000 rw-p 2abc5000 08:02 5524395                    /home/socian-ai/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
7f3e7d1df000-7f3e7d3ff000 rw-p 00000000 00:00 0 
7f3e7d400000-7f3e7d5c0000 rw-p 00000000 00:00 0 
7f3e7d5c0000-7f3e7d5c5000 r-xp 00000000 08:02 794358                     /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0
7f3e7d5c5000-7f3e7d7c4000 ---p 00005000 08:02 794358                     /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0
7f3e7d7c4000-7f3e7d7c5000 r--p 00004000 08:02 794358                     /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0
7f3e7d7c5000-7f3e7d7c6000 rw-p 00005000 08:02 794358                     /usr/lib/x86_64-linux-gnu/libXdmcp.so.6.0.0
7f3e7d7c6000-7f3e7d7c8000 r-xp 00000000 08:02 794347                     /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0
7f3e7d7c8000-7f3e7d9c8000 ---p 00002000 08:02 794347                     /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0
7f3e7d9c8000-7f3e7d9c9000 r--p 00002000 08:02 794347                     /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0
7f3e7d9c9000-7f3e7d9ca000 rw-p 00003000 08:02 794347                     /usr/lib/x86_64-linux-gnu/libXau.so.6.0.0
7f3e7d9ca000-7f3e7d9eb000 r-xp 00000000 08:02 795765                     /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0
7f3e7d9eb000-7f3e7dbea000 ---p 00021000 08:02 795765                     /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0
7f3e7dbea000-7f3e7dbeb000 r--p 00020000 08:02 795765                     /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0
7f3e7dbeb000-7f3e7dbec000 rw-p 00021000 08:02 795765                     /usr/lib/x86_64-linux-gnu/libxcb.so.1.1.0
7f3e7dbec000-7f3e7dc5a000 r-xp 00000000 08:02 7213657                    /lib/x86_64-linux-gnu/libpcre.so.3.13.2
7f3e7dc5a000-7f3e7de5a000 ---p 0006e000 08:02 7213657                    /lib/x86_64-linux-gnu/libpcre.so.3.13.2
7f3e7de5a000-7f3e7de5b000 r--p 0006e000 08:02 7213657                    /lib/x86_64-linux-gnu/libpcre.so.3.13.2
7f3e7de5b000-7f3e7de5c000 rw-p 0006f000 08:02 7213657                    /lib/x86_64-linux-gnu/libpcre.so.3.13.2
7f3e7de5c000-7f3e7df91000 r-xp 00000000 08:02 788328                     /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0
7f3e7df91000-7f3e7e191000 ---p 00135000 08:02 788328                     /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0
7f3e7e191000-7f3e7e192000 r--p 00135000 08:02 788328                     /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0
7f3e7e192000-7f3e7e196000 rw-p 00136000 08:02 788328                     /usr/lib/x86_64-linux-gnu/libX11.so.6.3.0
7f3e7e196000-7f3e7e1a7000 r-xp 00000000 08:02 794360                     /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0
7f3e7e1a7000-7f3e7e3a6000 ---p 00011000 08:02 794360                     /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0
7f3e7e3a6000-7f3e7e3a7000 r--p 00010000 08:02 794360                     /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0
7f3e7e3a7000-7f3e7e3a8000 rw-p 00011000 08:02 794360                     /usr/lib/x86_64-linux-gnu/libXext.so.6.4.0
7f3e7e3a8000-7f3e7e3b1000 r-xp 00000000 08:02 794382                     /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0
7f3e7e3b1000-7f3e7e5b0000 ---p 00009000 08:02 794382                     /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0
7f3e7e5b0000-7f3e7e5b1000 r--p 00008000 08:02 794382                     /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0
7f3e7e5b1000-7f3e7e5b2000 rw-p 00009000 08:02 794382                     /usr/lib/x86_64-linux-gnu/libXrender.so.1.3.0
7f3e7e5b2000-7f3e7e5c8000 r-xp 00000000 08:02 794211                     /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0
7f3e7e5c8000-7f3e7e7c7000 ---p 00016000 08:02 794211                     /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0
7f3e7e7c7000-7f3e7e7c8000 r--p 00015000 08:02 794211                     /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0
7f3e7e7c8000-7f3e7e7c9000 rw-p 00016000 08:02 794211                     /usr/lib/x86_64-linux-gnu/libICE.so.6.3.0
7f3e7e7c9000-7f3e7e7cc000 rw-p 00000000 00:00 0 
7f3e7e7cc000-7f3e7e7d3000 r-xp 00000000 08:02 794333                     /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1
7f3e7e7d3000-7f3e7e9d2000 ---p 00007000 08:02 794333                     /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1
7f3e7e9d2000-7f3e7e9d3000 r--p 00006000 08:02 794333                     /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1
7f3e7e9d3000-7f3e7e9d4000 rw-p 00007000 08:02 794333                     /usr/lib/x86_64-linux-gnu/libSM.so.6.0.1
7f3e7e9d4000-7f3e7eae3000 r-xp 00000000 08:02 7216254                    /lib/x86_64-linux-gnu/libglib-2.0.so.0.4800.2
7f3e7eae3000-7f3e7ece2000 ---p 0010f000 08:02 7216254                    /lib/x86_64-linux-gnu/libglib-2.0.so.0.4800.2
7f3e7ece2000-7f3e7ece3000 r--p 0010e000 08:02 7216254                    /lib/x86_64-linux-gnu/libglib-2.0.so.0.4800.2
7f3e7ece3000-7f3e7ece4000 rw-p 0010f000 08:02 7216254                    /lib/x86_64-linux-gnu/libglib-2.0.so.0.4800.2
7f3e7ece4000-7f3e7ece5000 rw-p 00000000 00:00 0 
7f3e7ece5000-7f3e7ece6000 r-xp 00000000 08:02 790307                     /usr/lib/x86_64-linux-gnu/libgthread-2.0.so.0.4800.2
7f3e7ece6000-7f3e7eee5000 ---p 00001000 08:02 790307                     /usr/lib/x86_64-linux-gnu/libgthread-2.0.so.0.4800.2
7f3e7eee5000-7f3e7eee6000 r--p 00000000 08:02 790307                     /usr/lib/x86_64-linux-gnu/libgthread-2.0.so.0.4800.2
7f3e7eee6000-7f3e7eee7000 rw-p 00001000 08:02 790307                     /usr/lib/x86_64-linux-gnu/libgthread-2.0.so.0.4800.2
7f3e7eee7000-7f3e7f1ca000 r-xp 00000000 08:02 5516304                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0
7f3e7f1ca000-7f3e7f3ca000 ---p 002e3000 08:02 5516304                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0
7f3e7f3ca000-7f3e7f3cc000 rw-p 002e3000 08:02 5516304                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0
7f3e7f3cc000-7f3e7f3d3000 rw-p 00000000 00:00 0 
7f3e7f3d3000-7f3e7f3d4000 rw-p 00313000 08:02 5516304                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libvpx-81a43c0a.so.5.0.0
7f3e7f3d4000-7f3e7f3ee000 r-xp 00000000 08:02 5516308                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100
7f3e7f3ee000-7f3e7f5ee000 ---p 0001a000 08:02 5516308                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100
7f3e7f5ee000-7f3e7f5f0000 rw-p 0001a000 08:02 5516308                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100
7f3e7f5f0000-7f3e7f5f2000 rw-p 0001d000 08:02 5516308                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswresample-b4363bfa.so.3.2.100
7f3e7f5f2000-7f3e7f608000 r-xp 00000000 08:02 7213566                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f3e7f608000-7f3e7f807000 ---p 00016000 08:02 7213566                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f3e7f807000-7f3e7f808000 rw-p 00015000 08:02 7213566                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f3e7f808000-7f3e7f97a000 r-xp 00000000 08:02 788482                     /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21
7f3e7f97a000-7f3e7fb7a000 ---p 00172000 08:02 788482                     /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21
7f3e7fb7a000-7f3e7fb84000 r--p 00172000 08:02 788482                     /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21
7f3e7fb84000-7f3e7fb86000 rw-p 0017c000 08:02 788482                     /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.21
7f3e7fb86000-7f3e7fb8a000 rw-p 00000000 00:00 0 
7f3e7fb8a000-7f3e7fb91000 r-xp 00000000 08:02 7213546                    /lib/x86_64-linux-gnu/librt-2.23.so
7f3e7fb91000-7f3e7fd90000 ---p 00007000 08:02 7213546                    /lib/x86_64-linux-gnu/librt-2.23.so
7f3e7fd90000-7f3e7fd91000 r--p 00006000 08:02 7213546                    /lib/x86_64-linux-gnu/librt-2.23.so
7f3e7fd91000-7f3e7fd92000 rw-p 00007000 08:02 7213546                    /lib/x86_64-linux-gnu/librt-2.23.so
7f3e7fd92000-7f3e8008e000 r-xp 00000000 08:02 5516301                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7
7f3e8008e000-7f3e8009e000 ---p 002fc000 08:02 5516301                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7
7f3e8009e000-7f3e800ca000 rw-p 0030c000 08:02 5516301                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7
7f3e800ca000-7f3e8028d000 ---p 00338000 08:02 5516301                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7
7f3e8028d000-7f3e8029d000 rw-p 002fb000 08:02 5516301                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtCore-9549151f.so.4.8.7
7f3e8029d000-7f3e8029e000 rw-p 00000000 00:00 0 
7f3e8029e000-7f3e802c2000 r-xp 00000000 08:02 5516302                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7
7f3e802c2000-7f3e804c2000 ---p 00024000 08:02 5516302                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7
7f3e804c2000-7f3e804cb000 rw-p 00024000 08:02 5516302                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtTest-1183da5d.so.4.8.7
7f3e804cb000-7f3e80fd3000 r-xp 00000000 08:02 5516306                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7
7f3e80fd3000-7f3e811d3000 ---p 00b08000 08:02 5516306                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7
7f3e811d3000-7f3e8122c000 rw-p 00b08000 08:02 5516306                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7
7f3e8122c000-7f3e8122e000 rw-p 00000000 00:00 0 
7f3e8122e000-7f3e8135c000 rw-p 00b61000 08:02 5516306                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libQtGui-6d0f14dd.so.4.8.7
7f3e8135c000-7f3e813db000 r-xp 00000000 08:02 5516300                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100
7f3e813db000-7f3e815da000 ---p 0007f000 08:02 5516300                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100
7f3e815da000-7f3e815dc000 rw-p 0007e000 08:02 5516300                    /home/socian-ai/.local/lib/python3.5/site-packages/cv2/.libs/libswscale-15b3fdc6.so.5.2.100
7f3e815dc000-7f3e815e4000 rw-p 00000000 00:00 0 
"
请问下主页链接的InsightFace_TF 里实现的arcloss 是否有了解过  我发现哪里的实现和这里的似乎有很多不一样？？
" i am using the --ce_loss option to get the loss output while doing mobilefacenet trainging
but the output loss results  are always nan

did anyone meet with the same problem?
how should i fix this?

(i'm using batchsize 128 and training from scratch)"
"I am trying to train the alignment model. The train.py file link  
https://github.com/deepinsight/insightface/blob/master/alignment/train.py 

but I am not clear about the train dataset . which dataset is used and what exactly means by 'train.rec'

```
  train_iter = FaceSegIter(path_imgrec = os.path.join(args.data_dir, 'train.rec'),
      batch_size = args.batch_size,
      per_batch_size = args.per_batch_size,
      aug_level = 1,
      use_coherent = args.use_coherent,
      args = args,
)
```"
"Hi @nttstar ,
How should I use your code (or files to read and modify) for finding similarity between any 2 images(not faces)"
"How to use gray image and make the input shape be '1, 1, 112, 112', is this possible?"
"What's the difference between given ""faescrub_images_112x112.tar"" and the facescrub aligned by ""align_facescrub.py"" ?
It's well-known that the detection and alignment largely affect the face recognition performance, and there should be a 1% top-1 identification performance gap on Megaface (the given faescrub_images_112x112.tar is better).
Without knowing this detail, it is quite hurting the re-producible of the Arcface paper, could you help share it ? Thanks in advance !"
"What's the difference between given ""faescrub_images_112x112.tar""  and the facescrub aligned by ""align_facescrub.py"" ?
It's well-known that the detection and alignment largely affect the face recognition performance, and there should be a 1% top-1 identification performance gap (the given faescrub_images_112x112.tar is better).
Without knowing this detail, it is quite hurting the re-producible of the Arcface paper, could you help share it ? Thanks in advance !"
"     Hello, the url of Refined-MS1M@GoogleDrive is  lost, the BaiduDrive is too slow. Could you give me a new url of GoogleDrive? Thank you so much!"
""
"When I use the arcface loss with gluon code, there is something wrong with the convergence.(s=64,m=0.5)
And the cross entropy loss does not drops when it is about 20."
"Any reference paper to the face attribute (gender, age) model (https://www.dropbox.com/s/2xq8mcao6z14e3u/gamodel-r50.zip?dl=0)?"
"I trained with Resnet 100 with new cleaned MS data @ 4GPU @128 batchsize/GPU, lr=0.1 at first, from the loss it looks weird to me:
1. the sudden large noise in the loss (the left red circle)
2. the loss is rising (in the right red circle)

Anyone can give me some suggestions?
![11](https://user-images.githubusercontent.com/15931069/45975149-6b2eb400-c011-11e8-96cc-4aaa73d45444.png)
"
"how to output the face feature vector?
ref:
https://github.com/ageitgey/face_recognition/blob/master/examples/face_distance.py
Key code:
known_obama_image = face_recognition.load_image_file(""obama.jpg"")
obama_face_encoding = face_recognition.face_encodings(known_obama_image)[0]
the ""obama_face_encoding""  variable is the numpy vector.
And how to get the face_encoding vector from insightface api.
please show the demo code. thanks.
"
"是否可以训练模型，能够提取出一些结构化特征：
戴眼镜、侧脸、表情、化妆、年龄段、戴帽子、胡子等等。"
"As for a M:N face recognition task, my current method is to compare the calculated features with the features of the test datasets one by one, which is very time consuming if M and N is large. Is there any accelerating algorithms that do not need to compare features one by one? How to make this process faster? "
nasnet out of memory
"Hi,

I know that cmc outputs ranks and scores.

but I run two models and got two different ranks.

for example,

in `cmc_facescrub_megaface_resnet18_112x112_100_1.json`, I got

```
	""cmc"" : 
	[
		[ 0, 1, 2, 3, 4, 5, 7, 9, 11, 14, 17, 21, 26, 32, 41, 54, 74, 100 ],
		
		[
			0.7638087868690491,
			0.8032987713813782,
			0.8251949548721313,
			0.8412851095199585,
			0.8535284996032715,
			0.8639559149742126,
			0.8812437057495117,
			0.8939622640609741,
			0.9058541655540466,
			0.9195685982704163,
			0.9295989274978638,
			0.9410156607627869,
			0.952159047126770,
			0.9628793001174927,
			0.9738143682479858,
			0.9844045042991638,
			0.9947472810745239,
			1.0
		]
	]
```

in `cmc_facescrub_megaface_resnet20_112x112_100_1.json`, I got

```
	""cmc"" : 
	[
		[ 0, 1, 2, 3, 5, 7, 9, 11, 13, 16, 19, 23, 28, 35, 43, 54, 71, 90, 100 ],
		
		[
			0.7660283446311951,
			0.7958394885063171,
			0.8119166493415833,
			0.8239842653274536,
			0.8400874733924866,
			0.8548563718795776,
			0.8678092360496521,
			0.8797271251678467,
			0.8903498053550720,
			0.9029837250709534,
			0.9138927459716797,
			0.9261426329612732,
			0.9374357461929321,
			0.9487678408622742,
			0.9589608907699585,
			0.9698048830032349,
			0.9801931977272034,
			0.9904252886772156,
			1.0
		]
	]
```

as you can see, one of the rank  is [ 0, 1, 2, 3, 4, 5, 7, 9, 11, 14, 17, 21, 26, 32, 41, 54, 74, 100 ] and the other one is [ 0, 1, 2, 3, 5, 7, 9, 11, 13, 16, 19, 23, 28, 35, 43, 54, 71, 90, 100 ] are different.

So it's hard for me to compare both ranks in the same picture.

any idea?
"
"LResNet50E-IR is trained on VGGFACE2, but you don't show the accuracy on megaface.

How about the accuracy on megaface with LResNet50E-IR?"
"Since the training .rec data is not large, how can I put them into RAM?"
How did you transform the MxNet model into caffe model ?  Would you please publish the transform code to us?
Just wondering which one is better for insight face models?
"I want to print the both the accuracy and loss，the accuracy is normal, but loss value is very very small , why?"
我写了两版。每次得到的特征都不一样。 不知道是预处理问题还是 没用对mxnet c++ api
"Hi,

When doing alignment, how do you determine the src parameter (https://github.com/deepinsight/insightface/blob/master/src/common/face_preprocess.py#L61)?

Is it ok to apply that in any face datasets?"
"`# -*- coding: utf-8 -*-
from multiprocessing import Process,Queue
import time,random,os
import cv2
import mxnet as mx
from mtcnn_detector import MtcnnDetector
cap = cv2.VideoCapture(0)

detector = MtcnnDetector(model_folder='mtcnn-model', ctx=mx.cpu(0), num_worker = 1 , accurate_landmark = False)'

def consumer(queue):
    while True: 
        frame=queue.get()
        results = detector.detect_face(frame)
        print('queue.size()=={}'.format(queue.qsize()))

def producer(queue):
    while True:
        ret_flag, frame = cap.read()
        queue.put(frame)

if __name__ == '__main__':
    q=Queue()
    p1=Process(target=producer,args=(q,))
    c1=Process(target=consumer,args=(q,))

    p1.start()
    c1.start()'

the output is:
queue.size()==6
queue.size()==11
queue.size()==17
queue.size()==22
queue.size()==27
queue.size()==31
queue.size()==36
queue.size()==41
queue.size()==46
queue.size()==51
queue.size()==55
queue.size()==60
queue.size()==65
queue.size()==70

the size not reduce, it seems get stuck when running mtcnn.

"
"    elif args.loss_type == 4: # arc face
        s = args.margin_s
        m = args.margin_m
        assert s > 0.0
        assert m >= 0.0
        assert m < (math.pi / 2)

        # start to compute the cos(theta)
        _weight = mx.symbol.L2Normalization(_weight, mode='instance')
        nembedding = mx.symbol.L2Normalization(embedding, mode='instance', name='fc1n') * s
        fc7 = mx.sym.FullyConnected(data=nembedding, weight=_weight, no_bias=True, num_hidden=args.num_classes,
                                    name='fc7')
        zy = mx.sym.pick(fc7, gt_label, axis=1)
        cos_t = zy / s
        # end to compute the cos(theta)

        cos_m = math.cos(m)
        sin_m = math.sin(m)
        **mm = math.sin(math.pi - m) * m**
        # threshold = 0.0
        threshold = math.cos(math.pi - m)
        if args.easy_margin:
            cond = mx.symbol.Activation(data=cos_t, act_type='relu')
        else:
            cond_v = cos_t - threshold
            cond = mx.symbol.Activation(data=cond_v, act_type='relu')
        # compute cos(theta + m)
        body = cos_t * cos_t
        body = 1.0 - body
        sin_t = mx.sym.sqrt(body)
        new_zy = cos_t * cos_m
        b = sin_t * sin_m
        new_zy = new_zy - b
        # new_zy is cos(theta + m) = cos(theta)*cos(m) - sin(theta)*sin(m)
        new_zy = new_zy * s
        if args.easy_margin:
            zy_keep = zy
        else:
            **zy_keep = zy - s * mm**
---------------------------------------------
1) In the above arc loss implementation, what's the motivation of doing ""zy_keep = zy - s * mm"" ?
2) How does ""mm = math.sin(math.pi - m) * m"" derived ?"
"
![1234](https://user-images.githubusercontent.com/20920889/45276640-0abe5380-b4f6-11e8-993b-cd403265dfbe.png)
请访问我的github项目https://github.com/zuoqing1988/ZQCNN-v0.0

"
"In  verification.py

under the function:
def test(data_set, mx_model, batch_size, nfolds=10, data_extra=None, label_shape=None):
""
**embeddings = embeddings_list[0] + embeddings_list[1]**
""
What's the motivation for the above code to add two embeddings ?"
"I have the same question with @bruinxiong. Emore only have hundreds more identities, but two million more images, can you please tell me how you setup this dataset?
"
I want to modify the MobileFaceNet  to make it faster in some device to meet the requirement of FPS. Is there any modification suggestion?
"I  follow the steps to change glint data to train.rec and train.idx, but when i run train_softmax.py, it has this erro   s = self.imgrec.read_idx(0)  KeyError: 0, the idx file is attached! thanks!
![2018-09-06 10-43-02](https://user-images.githubusercontent.com/26209785/45131949-b5b2d280-b1c1-11e8-8b7a-4abc0295f630.png)
"
"My system have 2 ""GTX 1080 (8 GB)"" , which model of yours can be fitted in for training. 
"
"Hi
I want to finetune your pretrained model (r50) with embedding size 128. Starting point is your train_softmax.py:
`CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --emb-size=128 --pretrained '../models/model-r50-am-lfw/model,0' --network r50 --loss-type 0 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112  --prefix ../models/model-r50-am-lfw/`

this results in:
`mxnet.base.MXNetError: [08:51:39] src/operator/nn/../tensor/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [512], got [128]`

I have tried to remove the last fc layer (fc1), then add fc7 as done in your code, then freeze all layers except fc7. Still the dimensions don't match.

Any advise on how to do this? Thanks."
"I am trying to use triplet loss to finetune mobilefacenet, but the acc on lfw,cfp,agedb all decrease after use it"
"@nttstar First thanks for your great work.

When i want to try _/recognition/test.py_ the alignment model is missing and i can not find any model provided by your repository (there is no model in Model-Zoo)

Do i have to train it myself? If so, what dataset and what script i have to use?
Would be awesome if you could provide your pretrained model, so i can run your test.py script.

Greetings."
"I am training with arcface. The entire training set has at least 2 photos each, with 12w people and a total of 6 million photos. Lr: 0.1, WD: 0.0005. Save a model every time you train an epoch and test it. It is now found that the training accuracy rises steadily, but the test test accuracy fluctuates greatly. What is the reason for this, and what parameters are generally tried to adjust when encountering this phenomenon?
@nttstar  

lfw_112x112:86.38+2.03
lfw_112x112:50.00+0.00
lfw_112x112:50.00+0.00
lfw_112x112:96.68+0.85
lfw_112x112:98.75+0.55
lfw_112x112:50.00+0.00
lfw_112x112:98.93+0.48
lfw_112x112:94.72+0.95
lfw_112x112:98.80+0.53
lfw_112x112:97.95+0.50
lfw_112x112:97.58+0.85
lfw_112x112:50.00+0.00
lfw_112x112:50.00+0.00
lfw_112x112:50.00+0.00"
"mxnet.base.MXNetError: [16:42:12] src/operator/nn/../tensor/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [512,25088], got [512,61952]

python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir  /zqzn/LLface_rec/ --prefix ../model-r100 --per-batch-size 28 --pretrained ../models/model-r100-ii/model,0

finetune imagesize is 171*171

why? help me!"
"I use the pretrained model https://github.com/deepinsight/insightface/wiki/Model-Zoo#31-lresnet100e-irarcfacems1m-refine-v2 to finetune on the same ms1m-refine-v2 dataset(https://github.com/deepinsight/insightface/wiki/Dataset-Zoo#ms1m-refine-v2recommended)
The accuracy start from 0, is this normal?
The command line is as follows
```
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_emore  --prefix ../model-r100 --per-batch-size 64  --pretrained ../pre_trained_models/model-r100-ii/model,0
```
Thanks"
"@nttstar 
The author of the MobileNet-V2 has updated the paper. The line-79 and line-80  are different  with the new paper. Thankyou!
![a111](https://user-images.githubusercontent.com/19170430/44788533-e183f600-abcc-11e8-865c-eb7ae0eb6932.png)

"
"As code [here](https://github.com/deepinsight/insightface/blob/master/src/train_softmax.py#L219-L236):
```
    threshold = math.cos(math.pi-m)
    if args.easy_margin:
      cond = mx.symbol.Activation(data=cos_t, act_type='relu')
    else:
      cond_v = cos_t - threshold
      cond = mx.symbol.Activation(data=cond_v, act_type='relu')
    body = cos_t*cos_t
    body = 1.0-body
    sin_t = mx.sym.sqrt(body)
    new_zy = cos_t*cos_m
    b = sin_t*sin_m
    new_zy = new_zy - b
    new_zy = new_zy*s
    if args.easy_margin:
      zy_keep = zy
    else:
      zy_keep = zy - s*mm
    new_zy = mx.sym.where(cond, new_zy, zy_keep)
```
Some codes are meant to make sure the theta+m is in  [0, pi]. However, when in the combine loss codes, this constrain is not done. I am not very sure why in combine loss this constrain can be deleted."
"spatial_norm=body*body 
维度应该是[minibatch,7,7,512]？
后面
spatial_norm=mx.sym.sum(data=spatial_norm, axis=1, keepdims=True)
axis=1是什么意思，输出维度是否变成[minibatch,1,7,512] 不应该是对宽高都计算吗？
还有后面的spatial_attention_inverse=mx.symbol.tile(spatial_div_inverse, reps=(1,filters_in,1,1))   也不是很了解，能否讲解下 "
""
"Hi @nttstar  @yingfeng  @yuzhichang  @zhangxu19830126  @kernel8liang 
I want to use my own dataset for finetuning the pre-trained age-gender model(gamodel-r50). And I am creating a directory 'all_ages' in the same directory in which face2rec.py(src/data/) is. Then I have 100 sub-directories in this directory data corresponding to 0-99 ages, which further contains images in it accordingly. But face2rec.py function doesn't contain code or make_list function (even if I pass --list True to parser). The other option I am left with is to use im2rec.py from mxnet

But the format of .lst file that im2rec.py generates doesn't matches with the format that parse_lst_line (face_preprocess.py) function processes.


```
def parse_lst_line(line):
  vec = line.strip().split(""\t"")
  assert len(vec)>=3
  aligned = int(vec[0])
  image_path = vec[1]
  label = int(vec[2])
  bbox = None
  landmark = None
  #print(vec)
  if len(vec)>3:
    bbox = np.zeros( (4,), dtype=np.int32)
    for i in xrange(3,7):
      bbox[i-3] = int(float(vec[i]))
    landmark = None
    if len(vec)>7:
      _l = []
      for i in xrange(7,17):
        _l.append(float(vec[i]))
      landmark = np.array(_l).reshape( (2,5) ).T
  #print(aligned)
  return image_path, label, bbox, landmark, aligned
```
It seems like aligned accepts a number(probably 0 or 1). Hence, I guess the format of a line in .lst file will be:

`aligned    img_path     age    bbox_x1     bbox_y1    bbox_x2    bbox_y2    p1    p2    p3    p4    p5    p6    p7    p8    p9    p10 `
Am I correct? If not, then please provide the right way to do it.

Also, after creating .lst file, should I use face2rec.py as it is to create .rec file
"
"Hi,

I saw gamma of the batch normalization after the fully connection is fixed during training.

https://github.com/deepinsight/insightface/blob/master/src/symbols/symbol_utils.py#L33

I don't understand why you did this.

Any math behind that?

thank you"
The Asian face datasets released are of poor quality and have a number of error samples
"Hi, i run https://github.com/deepinsight/insightface/blob/master/SSH/test.py with model(https://pan.baidu.com/s/1AoNAIq2pD1H58xuzoOIrIw).  But I get Error. How can i fix it?
-----------------------------------------------------------------------------------------------------------------
 src/nnvm/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 10300 with lower version of MXNet v10201. May cause undefined behavior. Please update MXNet if you encounter any issue
[10:39:40] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
Traceback (most recent call last):
  File ""test.py"", line 12, in <module>
    detector = SSHDetector('./model/sshb', 0)
  File ""/home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/ssh_detector.py"", line 39, in __init__
    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
  File ""/home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/model.py"", line 420, in load_checkpoint
    save_dict = nd.load('%s-%04d.params' % (prefix, epoch))
  File ""/home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/ndarray/utils.py"", line 175, in load
    ctypes.byref(names)))
  File ""/home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/base.py"", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [10:39:40] src/ndarray/ndarray.cc:1814: Check failed: fi->Read(data) Invalid NDArray file format

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x30756a) [0x7f0e55dfd56a]
[bt] (1) /home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x307b91) [0x7f0e55dfdb91]
[bt] (2) /home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x263e054) [0x7f0e58134054]
[bt] (3) /home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/local/lib/python2.7/site-packages/mxnet/libmxnet.so(MXNDArrayLoad+0x263) [0x7f0e57ef4d33]
[bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f0e73344e40]
[bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7f0e733448ab]
[bt] (6) /home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x48f) [0x7f0e735543df]
[bt] (7) /home/ubuntu/facenet_gitlab/PythonDev/project/insight_face/insightface/SSH/venv/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(+0x11d82) [0x7f0e73558d82]
[bt] (8) python(PyEval_EvalFrameEx+0x578f) [0x4c15bf]
[bt] (9) python(PyEval_EvalCodeEx+0x306) [0x4b9ab6]

"
"Using the mtcnn implementation from facenet, the system finds a face in the attached image:

![item](https://user-images.githubusercontent.com/40426/44222161-f0849600-a140-11e8-8380-2c4a5756964a.png)
 

The mtcnn implementation included under deploy, however, does not find the face.  Specifically, it is filtered after the first step by the second network."
"Hello
Unfortunately both google drive links to the MS1M and VGG2 datasets are invalid. Any chance you could update them?"
"Hi,

Can you provide the deploy prototxt for the caffe model (LResNet50E-IR)?"
Create requirements.txt file
"Such as agedb_30.bin cfp_ff.bin cfp_fp.bin lfw.bin
Now I have a batch of my own training data,I want to convert them to bin format. What should I do?"
"  File ""train_triplet.py"", line 233, in train_net
    spherenet.init_weights(sym, data_shape_dict, args.num_layers)
  File ""symbols/spherenet.py"", line 71, in init_weights
    arg_shape_dict = dict(zip(arg_name, arg_shape))
TypeError: zip argument #2 must support iteration

用spherenet训练softmax没有问题，用triplet训练r34也没有问题"
你好，在Model Zoo中的年龄性别那里说是在Asian dataset上训练的，请问是之前说到的glint的Asian dataset吗，但是我下载下来似乎没有看到有年龄性别的标签啊？
is there a c++ version of face alignment? thank you
""
"I know that [overlapping identities/images have been removed for facescrub](https://github.com/deepinsight/insightface/issues/24), but does the [emore](https://github.com/deepinsight/insightface/issues/215) dataset have any overlap with LFW, CFP, or AgeDB?

Thank you!"
"![image](https://user-images.githubusercontent.com/35397905/44035347-d91a9c36-9f41-11e8-873a-8ed28e7b5bb6.png)
How to solve this problem? It's the problem of mxnet editon?
thank you! @nttstar "
"https://github.com/deepinsight/insightface/blob/8ee8244d45ab2821e52d73637b8d17b60ce475ef/src/data/agedb2pack.py#L79

@nttstar  Can you share the file '04_FINAL_protocol_30_years.mat' with me, I couldn't find it on the official website. Thank you!"
"in combined margin loss, if margin_m isn't 0, new_cos = cos(acos(cos_t) - margin_m) instead of cos_t * cos(margin_m) - sin_t * sin(margin_m) like arcface loss? thanks!"
"我的对齐代码如下：
import os
import numpy as np
import math
import cv2
import os.path
import scipy.io as scio
from skimage import transform as trans

def to_rgb(img):
    w, h = img.shape
    ret = np.empty((w, h, 3), dtype=np.uint8)
    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img
    return ret

def preprocess(img, bbox=None, landmark=None, **kwargs):
    M = None
    str_image_size = kwargs.get('image_size', '')
    #image_size = int(str_image_size)
    image_size = str_image_size    
    src = np.array([
      [38.2946, 51.6963],
      [73.5318, 51.5014],
      [56.0252, 71.7366],
      [41.5493, 92.3655],
      [70.7299, 92.2041] ], dtype=np.float32 )
    dst = landmark.astype(np.float32）

    tform = trans.SimilarityTransform()
    tform.estimate(dst, src)
    M = tform.params[0:2,:]
   
    if M is None:
        if bbox is None: #use center crop
            det = np.zeros(4, dtype=np.int32)
            det[0] = int(img.shape[1]*0.0625)
            det[1] = int(img.shape[0]*0.0625)
            det[2] = img.shape[1] - det[0]
            det[3] = img.shape[0] - det[1]
        else:
            det = bbox
        margin = kwargs.get('margin', 44)
        bb = np.zeros(4, dtype=np.int32)
        bb[0] = np.maximum(det[0]-margin/2, 0)
        bb[1] = np.maximum(det[1]-margin/2, 0)
        bb[2] = np.minimum(det[2]+margin/2, img.shape[1])
        bb[3] = np.minimum(det[3]+margin/2, img.shape[0])
        ret = img[bb[1]:bb[3],bb[0]:bb[2],:]
        ret = cv2.resize(ret, (image_size[0], image_size[1]))
        return ret
    else: 
        warped = cv2.warpAffine(img,M,(image_size[0],image_size[1]), borderValue = 0.0)
        bgr = warped[...,::-1]        
        return bgr

if __name__ == ""__main__"":
    f = open('/data2/lmy/lfw_ori_2.txt','r')
    points_prob = scio.loadmat(""/data2/lmy/InsightFace/MTCNNv2/landmark_lfw.mat"")
    bbox_prob = scio.loadmat(""/data2/lmy/InsightFace/MTCNNv2/bbox_lfw.mat"")
    m = f.readlines()
    i = 0
    for a in m:
        path = str(a[0:-1])
        print(path)
        img_or = cv2.imread(path)        
        if img_or.ndim == 2:
            img_or = to_rgb(img_or)
            img_or = img_or[:,:,0:3]                
        temp = points_prob['landmark'][i]
        temp = np.resize(temp,(1,10))
        points = temp
        print(points)
        points_5_2 = np.zeros((5,2))
        #print(points[0,2])
        for j in range(5):
            points_5_2[j,0] = points[0,j]
            points_5_2[j,1] = points[0,j+5]
        print(points_5_2)
        #print(bbox_prob['bbox'])
        points_5_2 = np.float32(points_5_2)
        temp = bbox_prob['bbox'][i]
        temp = np.resize(temp,(1,4))
        bbox_prob_2 =temp
        #print(bbox_prob_2)
        i = i+1
        crop_imgs = preprocess(img_or, bbox=bbox_prob_2, landmark = points_5_2, image_size=[112,112])
        element = a.replace('\r\n','').split(""/"") 
        save_path = ""/data2/lmy/mtcnn_lfw/""+str(element[-2])
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        save_name = save_path + '/' + str(element[-1][0:-1])
        cv2.imwrite(save_name,crop_imgs)"
"The dataset I used contains unaligned photos of about 50k people collected from several Chinese social media platforms. I used mtcnn to detect the faces, align_megaface.py to align and face2rec2.py to generate the .idx and .rec files. Then I called train_softmax.py with arguments r50, loss type 2, margin-m 0.35 and max-step 400000. When the training ended the classification accuracy was only about 0.70 and the lfw accuracy was only about 98.2%.
I found that some of the faces are deformed or blurred by photoshop and some of them have bad resolutions. Does these two factors negatively affect the overall performance of the algorithm? Is it possible to improve the accuracy just by adjusting parameters like margin and lr?

Thanks in advance for any help."
在windows 下 准备跑ssh 模型，遇到这个问题
我使用的是sphereface中的matlab程序代码进行的裁剪的lfw图片（112X112），也是使用他的matlab测试代码进行的测试（使用你release的caffe模型），但是效果不到99%，请问是因为我使用的matlab，而你使用的是python的原因么？
"您好，今天在做mxnet C++版本实现人脸识别的时候，发现一个有趣的问题。
按我对人脸识别流程的理解，训练本质其实是分类，训练生成的模型最后一层输出维度应该是num_classes，就是类别数目；要找到任一张图片的embedding特征，应该在交叉熵或者其他形式的损失之前的一层去查看，这个层的输出一般来讲是512维的。
但是我今天用C++版本进行前向过程的时候发现，在没有做中间某一层的特征提取的时候，模型最后一层的输出是512维！这个我感到很有意思！
是您在保存模型的时候已经做了处理，把模型保存到了交叉熵前一层吗？
而且我在阅读您的源码的时候，确实发现在某一特定模型在调用get_symbol()函数时，确实是将实参emb_size传给了形参num_classes，这里我也不是特别了然。

 @nttstar 盼 望您的解答，多谢！"
"RT。此repo提供的MTCNN对卡通图片误检率特别高，但原版MTCNN并不会这样，不知是对MTCNN做了何种处理？关键的是，检测到的框也并非卡通人物的人脸区域。但用repo给出的emore训练模型，就必须用此repo的MTCNN对齐才能得到好的结果。。
"
不知道谁有将 该库移植到手机端的经验，多谢了
"I want to use another face detection algorithm instead of MTCNN. As MTCNN accuracy isn't very good for detecting some faces. So, I have used my own implemented Face Detection Algorithm which it's accuracy is better than MTCNN. However, I have faced a new problem. The problem is that I can't get appropriate results with integration of the new face detection method via your face embedding model. I think the problem related to the face alignment method. As your example (in `./deploy/test.py`) you have used the MTCNN face detector and then by using its landmark detection you have aligned the face image and then pass it to the face embedding model (_e.g.,_ _LResNet50E-IR_). Unfortunately, my face detector just detect faces and doesen't have any face landmark localization method. Would you please kindly help me to guide me how I can integrate this new face detection method (without landmark localization) into your face embedding models?
It is worth nothing that I can integrate it into the face embedding model (i.e., without face alignment) but the results were bad.
 "
"hi,i run https://github.com/deepinsight/insightface/blob/master/SSH/test.py with model https://pan.baidu.com/s/1wjgc227L-pPhdwunk948DA ,every image(size about 400px*500px) inference cost about 12s by GPU , I confuse about the code why warmup cost many time but run second time become quickly

 for i in xrange(t-1): #warmup
      faces = detector.detect(img)
timea = datetime.datetime.now()
faces = detector.detect(img, threshold=0.5)
timeb = datetime.datetime.now()"
""
"Dear @nttstar,
I have used the MXNet version 1.2.1. When using your pre-trained models the following messages are shown:
```
[10:38:11] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[10:38:11] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
```
Would you please help me to remove above messages? 
"
"
![qq 20180803172230](https://user-images.githubusercontent.com/34421038/43635499-d78ae98c-9741-11e8-81ae-392c15f0b78c.png)
"
我没有P40*4这么多的GPU，我只有一张P100。请问大家都在什么配置下训练了多久呢？
"Thanks for your great job for face recognition!

I want to try the model from scratch, however, the link of the cleaned dataset is invalid now.

It would be great if you can update it. :)"
""
"请问一下，有人训过triplit loss吗？为啥感觉完全不收敛啊。虽说是online-hard-negtive-mining,但是总得有个整体的趋势吧？感觉一直不降啊，有啥好办法吗？"
"I use insightface to retrain **mobilefacenet** , then get new params.
But when I transfer it into **NCNN** and validate the **infer** result ,it's **quite different** from the infer result by **MXNET.**
I found ""**fix_gamma**"" param in the last bn layer of json file is the key factor.
“fix_gamma” is set to true in the original json file of mobilefacenet，when I change it into false in infer stage, the infer result out by MXNET becomes same with NCNN.
After reading MXNET code ,I guess this param should be set to false when doing infer.
But in insightface code train_softmax.py, the verification func ver_test , this param seems doesnot change to false, thus the verification result is correct?

I just started to learn MXNET, not quite familiar with both MXNET and insightface,maybe made some misunderstanding about this.
Bow~"
"1、说明中需要安装的包都安装了，
2、model-r34-amf文件夹放在了insightface/models中
3、在anaconda prompt中进入insightface/deploy目录，执行python test.py --model model-0000.params
失败，提示如下：
File ""test.py"", line 18, in <module>
    model = face_model.FaceModel(args)
  File ""H:\0insightfacedata\mxnet\mxnet_base_cu80\insightface\deploy\face_model.py"", line 53, in __init__
    self.model = get_model(ctx, image_size, args.model, 'fc1')
  File ""H:\0insightfacedata\mxnet\mxnet_base_cu80\insightface\deploy\face_model.py"", line 30, in get_model
    assert len(_vec)==2
AssertionError

问题：
1、执行的命令如何写，这个命令肯定不对
2、有人在windows中偿试吗？结果怎么样？正常来说，这个也应该是可以在windows中测试的吧，
3、谢谢大家！3333"
"
#### enviroment


$ uname -a
Linux i-c0tujza1 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux

NVIDIA Tesla P100 GPU

$ lspci -v -s `lspci | grep -i NVIDIA | awk '{ print $1 }'`
00:09.0 3D controller: NVIDIA Corporation Device 15f7 (rev a1)
        Subsystem: NVIDIA Corporation Device 11da
        Physical Slot: 9
        Flags: bus master, fast devsel, latency 0, IRQ 10
        Memory at fb000000 (32-bit, non-prefetchable) [size=16M]
        Memory at 400000000 (64-bit, prefetchable) [size=16G]
        Memory at 800000000 (64-bit, prefetchable) [size=32M]
        Capabilities: <access denied>
        Kernel driver in use: nvidia
        Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia


$ python --version

Python 2.7.12

$cat /usr/local/cuda/include/cudnn.h | grep ""#define CUDNN_MAJOR"" -A 1

    #define CUDNN_MAJOR 7
    #define CUDNN_MINOR 1

CUDA9.1+cuDNN7.1.3

$ cat /usr/local/cuda/version.txt

CUDA Version 9.1.85
CUDA Patch Version 9.1.85.1
CUDA Patch Version 9.1.85.2
CUDA Patch Version 9.1.85.3

$ pip install mxnet-cu91 -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com

note :  try mxnet the same error.

#### Run

follow the README.md, 

$ python  train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir /data/datasets/faces_emore/  --prefix ../model-r100

Illegal instruction (core dumped)

$ dmesg

[ 4338.396112] traps: python[487] trap invalid opcode ip:7f426ecb3389 sp:7ffce7bc3410 error:0 in libmxnet.so[7f426e7a9000+26bf4000]

"
"I download the lfw and cfp.bin provided by you. I found the dataset contains two list of data. For lfw, each list shape is (12000,3,112,112). the number of labels is 6000. 
i.e. why the data_list contains two (12000,3,112,112) shape data? Thank you.
```
def test(data_set, mx_model, batch_size, nfolds=10, data_extra = None, label_shape = None):
  print('testing verification..')
  data_list = data_set[0]
...
  for i in xrange( len(data_list) ):
    data = data_list[i]
```"
"First, thankyou for sharing the code. I have trained the project, and get the accuracy of 95% at the training dataset-ms1m. In order to achieve the goal of Face Recognition, I build a database of 20 peoples.  However, I get the number of distance is 0 always, when I give any one image to obtain the nearest person.
Next, I will show the training log, and the test log. In the test log, I will show the top 2 distances.
I'm looking forward to someone to answer， thankyou.

1. 

**INFO:root:Epoch[1] Batch [12000]	Speed: 312.45 samples/sec	acc=0.955898	lossvalue=0.000000
INFO:root:Epoch[1] Batch [13000]	Speed: 312.53 samples/sec	acc=0.955641	lossvalue=0.000001
INFO:root:Epoch[1] Batch [14000]	Speed: 312.49 samples/sec	acc=0.955789	lossvalue=0.000000
INFO:root:Epoch[1] Batch [15000]	Speed: 312.53 samples/sec	acc=0.956078	lossvalue=0.000000
INFO:root:Epoch[1] Batch [16000]	Speed: 312.51 samples/sec	acc=0.956133	lossvalue=0.000000
INFO:root:Epoch[1] Batch [17000]	Speed: 312.48 samples/sec	acc=0.955438	lossvalue=0.000000**

2. 
**deal with 43393/43399('distance ', [0.0, 0.0])
 deal with 43394/43399('distance ', [0.0, 0.0])
deal with 43395/43399('distance ', [0.0, 0.0])
deal with 43396/43399('distance ', [0.0, 0.0])
deal with 43397/43399('distance ', [0.0, 0.0])
deal with 43398/43399('distance ', [0.0, 0.0])
deal with 43399/43399('distance ', [0.0, 0.0])**

"
"Dear @nttstar,
Thank you for your nice work. 
As you have mentioned in `~/deploy/test.py` if one wants to do face verification task, then can use the distance between two feature vectors & a pre-defined threshold (_e.g.,_ 1.24). Now, my question is that, is it better to design a really verification network for this task? 
I mean a kind of _Siamese verification network_ which gets two input face images, and by using the difference of feature vectors, then learn _same person_ or _different person_ categories?  "
what is the link for downloading model-r34-amf ?/
"I fined tune r50 model with glint data all with triplet loss or arcface loss. The training log showed 99.8% acc in LFW, 94.50% in CFP and 98.10% in AgeDB, but when I submit the testdata feature of glint data, the result is only 20%, much lower than r50 (without fine-tuned) 48%, anyone knows why? Or is there anybody who submit results fine-tuned in glint training data?"
"Using deploy/test.py it would be possible to estimate gender and age, how can I estimate the mood using your code?
Do I need to write my own classifier using the extracted feature vector or it is already possible? (pretrained model can give me an estimation)"
"Try to test SSH detector it gives 👍 

> SSH . ImportError: No module named bbox


python 2.7 and added the . sys.path.

any idea?"
"As I review your code about mobilefacenet, I find the repeat time of first bottleneck is 4 in your code, but the repeat time in the original paper is 5.
![_75f1f04a-09d0-43b0-ac20-1f846ababfca](https://user-images.githubusercontent.com/34436077/43252213-8e1a7298-90f4-11e8-9104-8858ebf3efd4.png)
![mobilefacenet](https://user-images.githubusercontent.com/34436077/43252293-b5334346-90f4-11e8-9812-c40503023df1.jpg)
I'm not sure if it is my misunderstanding."
"INFO:root:Epoch[2] Batch [8880] Speed: 111.85 samples/sec       acc=0.066016

这个acc是怎么计算的？挑一些训练的样本然后计算verification的正确度？"
"lr-batch-epoch: 0.1 3999 0
testing verification..
(12000L, 512L)
infer time 82.392
[lfw][4000]XNorm: 16.636743
[lfw][4000]Accuracy-Flip: 0.85000+-0.01972
testing verification..
(14000L, 512L)
infer time 95.22
[cfp_fp][4000]XNorm: 15.990457
[cfp_fp][4000]Accuracy-Flip: 0.64200+-0.01316
testing verification..
(12000L, 512L)
infer time 80.785
[agedb_30][4000]XNorm: 15.559318
[agedb_30][4000]Accuracy-Flip: 0.63983+-0.01315
saving 2
[4000]Accuracy-Highest: 0.63983
INFO:root:Saved checkpoint to ""../model-r100_CombineFace/model-0002.params""
INFO:root:Epoch[0] Batch [4096]	Speed: 37.06 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [4224]	Speed: 103.41 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [4352]	Speed: 103.40 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [4480]	Speed: 103.60 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [4608]	Speed: 103.63 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [4736]	Speed: 103.42 samples/sec	acc=0.000000
作者你好，请问作者，训练集精度一直都是0.000，这样是正常的吗，验证集上精度是一直在上升的"
"what is 'item.flag' meaning in src/data/face2rec.py ，and in the function 'image_encode' ,there is  
s = mx.recordio.pack(header, '')   
there is no image data"
"training a model with 700k id and 5000k image ，r50 model，image_size 112*112 ,  using eight 1080ti ,even per-batch-size=1 out of memory ，is this normal?
i was doubt that if more id ,need more memory"
"Hi,

We would like to see c++ prediction speed and some batch test.

Is this possible to use model-r50-am-lfw model in C++ ? We just need following in the C++ : 

```
model = face_model.FaceModel(args) . // loading models in c++
img = model.get_input(img)
f2 = model.get_feature(img) // get features
```

rest can be done through vector?

Any idea, direction and help appreciated.
"
"I've tried to change the real path of nvvc and lib64, and I've removed default_compiler_so, but still can't be installed. Is SSH really unavailable on Windows? Did anyone else meet or solve the problem? Thank you!"
Do you test the mobilenetFaceNet on MegaFace or Asian training dataset(from glint)? How about the perfomance ? 
"hi!
After downloaded the database of 112x112 aligned faces from BaiduNetdisk, I could not unzip it and it says that I need 743PB to do it...Can you help me fix the problem? Many thanks."
"I try to finetune   LResNet50E-IR model using glint Asian dataset, I run the following command:  
CUDA_VISIBLE_DEVICES='0,1,2' python -u train_softmax.py \
                                --network r50 \
                                --loss-type 0 \
                                --lr 0.005 \
                                --per-batch-size 64 \
                                --data-dir /mntML/dongbin/datasets/faces_asian_112x112 \
                                --pretrained ../models/model-r50-am-lfw/model,0000 \
                                --prefix ""$PREFIX""
I use mxnet_cu90, and I use 1080Ti x 3，the speed is so slow, the log is as follows:

gpu num: 3
num_layers 50
image_size [112, 112]
num_classes 93979
Called with argument: Namespace(batch_size=192, beta=1000.0, beta_freeze=0, beta_min=5.0, bn_mom=0.9, ckpt=1, ctx_num=3, cutoff=0, data_dir='/mntML/dongbin/datasets/faces_asian_112x112', easy_margin=0, emb_size=512, end_epoch=100000, fc7_wd_mult=1.0, gamma=0.12, image_channel=3, image_h=112, image_w=112, loss_type=0, lr=0.005, lr_steps='', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, max_steps=0, mom=0.9, network='r50', num_classes=93979, num_layers=50, per_batch_size=64, power=1.0, prefix='../model-r50-asian/model-asian', pretrained='../models/model-r50-am-lfw/model,0000', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
loading ['../models/model-r50-am-lfw/model', '0000']
[07:58:36] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
init resnet 50
0 1 E 3 prelu
INFO:root:loading recordio /mntML/dongbin/datasets/faces_asian_112x112/train.rec...
header0 label [2830147. 2924126.]
id2range 93979
2830146
rand_mirror 1
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [106666, 160000, 213333]
call reset()
/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:490: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.333333333333 vs. 0.00520833333333). Is this intended?
  optimizer_params=optimizer_params)
INFO:root:Epoch[0] Batch [20]	Speed: 29.62 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [40]	Speed: 36.43 samples/sec	acc=0.006510
INFO:root:Epoch[0] Batch [60]	Speed: 41.90 samples/sec	acc=0.040104
INFO:root:Epoch[0] Batch [80]	Speed: 55.39 samples/sec	acc=0.102344
INFO:root:Epoch[0] Batch [100]	Speed: 76.50 samples/sec	acc=0.155469
INFO:root:Epoch[0] Batch [120]	Speed: 70.54 samples/sec	acc=0.182031
INFO:root:Epoch[0] Batch [140]	Speed: 72.65 samples/sec	acc=0.203125  


Anybody knows why?"
I check your code and you used lsoftmax which only allows integer m
"`sym = all_layers['fc1_output']  &nbsp;
dellist = [] &nbsp; 

for k,v in arg_params.items():
  if k.startswith('fc7'):
    dellist.append(k)
print('The contents of ',dellist)`
I try print dellist tuple and it shows this after running : 

> loading ../models/model-r34-amf/model/model 0
> The contents of []

It seems the contents of dellist are not being modified, so do you have any idea for a fix or it works for the other pre trained models only?"
"1. When i look through the face2rec2.py and glint2lst.py, I cannot find the step of detecting and crop face by mtcnn. So is the rec file could train dictectly?
2. when I packing the glint dataset to bin, there is not a standard way to build the bin file. your way suit to the lfw and cfp dataset which is special. So could you help provide py file generating bin from images?

Thank you very much.
@nttstar "
"Traceback (most recent call last):
  File ""E:/0-project/1-facerecognition/1-code/insightface-master/insightface-master/gluon/verification.py"", line 365, in <module>
testing verification..
    acc1, std1, acc2, std2, xnorm, embeddings_list = test(ver_list[i], model, ctx, args.batch_size, args.nfolds)
  File ""E:/0-project/1-facerecognition/1-code/insightface-master/insightface-master/gluon/verification.py"", line 229, in test
    z = net.feature(x)
AttributeError: 'Module' object has no attribute 'feature'"
"As mentioned in the paper {ArcFace: Additive Angular Margin Loss for Deep Face Recognition}, it said that "" During testing, the score is computed by the Cosine Distance of two feature vectors."" Anyway, I found that the Euclidian distance is employed in verfication.py, namely,  "" diff = np.subtract(embeddings1, embeddings2), dist = np.sum(np.square(diff),1)"". Why? 

If replaced with cosine-distance,  I found that poorer accuracy is obtained for the pretrained models. "
""
"where is the SSH model **e2ef** in the : 

insightface/SSH/test.py

`detector = SSHDetector('./model/e2ef', 0)
`"
"1、CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network m1 --loss-type 12 --lr 0.005 --mom 0.0 --per-batch-size 150 --data-dir ../datasets/faces_ms1m_112x112 --pretrained ../model-m1-softmax,50 --prefix ../model-m1-triplet
其实train_softmax.py 没有tripletloss，tripletloss在train.py中。
2、对照和其他的参数设置，作者把初始学习率：0.1->0.005，mom：0.9->0
3、特有参数：triplet-bag-size（defaut=3600），triplet-alpha（default=0.3），triplet-max-ap（default=0）.**在这里，我想问问作者，triplet-bag-size 是否大点好？**（本人设置值=4200）
目前我发现我的训练日志中发现反复调用：call reset（如下），这是否正常？？
**call reset()**
eval 4200 images.. 4200
triplet time stat [0.000102, 14.305246, 1.949595, 0.0, 0.0, 0.0]
found triplets 2071
seq len 6150
**INFO:root:Epoch[0] Batch [10]	Speed: 122.12 samples/sec	lossvalue=0.174548**
INFO:root:Epoch[0] Batch [12]	Speed: 949.29 samples/sec	lossvalue=0.064859
INFO:root:Epoch[0] Batch [14]	Speed: 642.70 samples/sec	lossvalue=0.065269
INFO:root:Epoch[0] Batch [16]	Speed: 652.19 samples/sec	lossvalue=0.059076
INFO:root:Epoch[0] Batch [18]	Speed: 645.43 samples/sec	lossvalue=0.068372
**call reset()**
eval 4200 images.. 8400
triplet time stat [0.000167, 20.816745, 3.800071, 0.0, 0.0, 0.0]
found triplets 2016
seq len 6000
**INFO:root:Epoch[0] Batch [20]	Speed: 115.45 samples/sec	lossvalue=0.173918**
INFO:root:Epoch[0] Batch [22]	Speed: 948.28 samples/sec	lossvalue=0.070200
INFO:root:Epoch[0] Batch [24]	Speed: 639.35 samples/sec	lossvalue=0.068926
INFO:root:Epoch[0] Batch [26]	Speed: 644.25 samples/sec	lossvalue=0.066985
INFO:root:Epoch[0] Batch [28]	Speed: 643.97 samples/sec	lossvalue=0.062959
**call reset()**
eval 4200 images.. 12600
triplet time stat [0.00022899999999999998, 27.907889, 5.54027, 0.0, 0.0, 0.0]
found triplets 1873
seq len 5550
**INFO:root:Epoch[0] Batch [30]	Speed: 124.86 samples/sec	lossvalue=0.185821**
INFO:root:Epoch[0] Batch [32]	Speed: 931.34 samples/sec	lossvalue=0.058671
INFO:root:Epoch[0] Batch [34]	Speed: 648.38 samples/sec	lossvalue=0.066276
INFO:root:Epoch[0] Batch [36]	Speed: 638.66 samples/sec	lossvalue=0.058656
INFO:root:Epoch[0] Batch [38]	Speed: 636.64 samples/sec	lossvalue=0.067309
call reset()
4、我发现每一个bag中的第一个batch的lossvalue总要比后面的大不少，这是否正常？？
**训练具体参数**
Called with argument: Namespace(batch_size=600, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_mode=-10, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.785196082309279, ctx_num=4, cutoff=0, data_dir='/db/train', easy_margin=0, emb_size=256, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=96, images_per_identity=5, incay=0.0, logits_verbose=0, loss_type=12, lr=0.005, lr_steps='', margin=4, margin_a=0.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.0, network='r50', noise_sgd=0.0, num_classes=105916, num_layers=50, output_c2c=0, patch='0_0_96_112_0', per_batch_size=150, per_identities=30, power=1.0, prefix='./arcface/7/model-r50', pretrained='/home/isag/insightface/arcface/4/model-r50,11', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', train_limit=0, triplet_alpha=0.3, triplet_bag_size=4200, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_act='prelu', version_input=1, version_output='K', version_se=0, version_unit=3, wd=0.0005)
loading ['/home/isag/insightface/arcface/4/model-r50', '11']"
"Thanks for sharing your code.
I want to ask some question:
 1, you  cropped and aligned MS1M ,VGG2 for training . do you have given parameters which you use to crop and align ?
	detector.extract_image_chips(img, points, desired_size, padding)
	which padding do you use for MS1M,VGG2 ?
2. In folder src/align/  , you use diffrent alignments for diffrent datasets ?
	What did you base on to choose these parameter for the data?
3. if i use my custom data, which padding should  i use? 
	detector.extract_image_chips(img, points, desired_size, padding)
who can help me? Thanks very much . "
"`Traceback (most recent call last):
  File ""test.py"", line 18, in <module>
    model = face_model.FaceModel(args)
  File ""/home/anshuman/insightface/deploy/face_model.py"", line 53, in __init__
    self.model = get_model(ctx, image_size, args.model, 'fc1')
  File ""/home/anshuman/insightface/deploy/face_model.py"", line 30, in get_model
    assert len(_vec)==2
AssertionError
`
Here is the snippet where the error is occurring
`self.model = None
  self.ga_model = None
    if len(args.model)>0:
      self.model = get_model(ctx, image_size, args.model, 'fc1')`
I had donwnloaded the models from mentioned  #226 and the readme.md, however it is raising this exception. Does anyone know how to fix it?
I am running Python 3.6."
"I have trained my own model and with my own different datasets and networks for different cases. The test result shows that it performs best when m = 0.5 which is consistent with the outcome in your paper (testing on lfw).  Sure experiments give us the result. But isn't m larger, the condition is more strict, and the result will be better? What's the reason behind?"
"I had this error.
`/home/anshuman/anaconda3/lib/python3.6/site-packages/skimage/__init__.py:60: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[08:04:50] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[08:04:50] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
/home/anshuman/anaconda3/lib/python3.6/site-packages/mxnet/model.py:928: DeprecationWarning: mxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.
  **kwargs)
[08:04:50] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[08:04:50] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[08:04:50] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[08:04:50] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[08:04:50] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[08:04:50] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""test.py"", line 23, in <module>
    gender, age = model.get_ga(img)
  File ""/home/anshuman/Documents/Repos/insightface/deploy/face_model.py"", line 103, in get_ga
    self.ga_model.forward(db, is_train=False)
AttributeError: 'NoneType' object has no attribute 'forward'`

Did anyone else have this error? So basically the  line 
`    self.ga_model.forward(db, is_train=False) ` 
in face_model.py doesn't have any attribute forward, so does anyone know how to fix this?"
Help needed by sophomore in CQUPT ~
"Hi there,

I find that many filters in conv0_weight have near-zero weights. For example, in the r50 model I trained, 34 out of the 64 filters have abs sum of weights < 1e-5, and the weakest filter have abs sum of weight < 1e-14.

The trained model behaves well though, reaching > 99.8 on lfw and >98.0 on agedb and cfp_fp, and reaching TPR>80%@FAR<1e-8 on one of my own testing dataset.

But problem happens when I try to convert the mxnet model to my own format for serving purpose. I have a light weighted inference library which does not support bn layers, so I have to merge the bn layers into the conv layers.  Since conv0 have many weak filters, and the following bn0 and stage1_unit1_bn1 try to normalize the very weak responses, using very small gamma and moving_var,  the order of computation matters. It is ok to merge bn0 into conv0, but it is problematic when merging stage1_unit1_bn1 into stage1_unit1_conv1, significant numerical errors will be produced after merging stage1_unit1_bn1 into stage1_unit1_conv1.

My problem can be bypassed by introducing the bn layer into my inference library so that the first bn in each residual block is left as it is. But I think these nearly dead neurons are worthy of further investigation.






"
"1. cmd:  python run_experiment.py  -p ../templatelists/facescrub_uncropped_features_list.json ../megaface/MegaFace_Features/ ../megaface/FaceScrub_Features/  _mxsphereface20c_112x112.bin ./results
2. then i got the error:
  File ""run_experiment.py"", line 134, in <module>
    main()
  File ""run_experiment.py"", line 103, in main
    [IDENTIFICATION_EXE, model, ""path"", probe_feature_list, probe_feature_list, probe_score_filename])
  File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__
    errread, errwrite)
  File ""/usr/lib/python2.7/subprocess.py"", line 1343, in _execute_child
    raise child_exception
OSError: [Errno 2] No such file or directory
3 .  i cant find IDENTIFICATION_EXE in devkit ....maybe ~~~do you have some idea? ~~~"
您好，非常感谢您将程序开源。现在我在合并celebrity和msrc数据集时遇到这么一个问题：关于msrc数据集的.rec和.idx文件我已经下载得到，celebrity数据集我也使用face2rec2.py转化得到了.rec和.idx文件，使用的模型下载了model-r50-am-lfw，但是我没能使用dataset_merge.py将这两个数据集合并下来。请您指导一下，万分感谢！！！
"![42222141-d519be74-7f06-11e8-9a68-d4535013702e](https://user-images.githubusercontent.com/3853150/42269333-752d7ca8-7fb0-11e8-80b1-7413a9b8c2a3.png)
When i train a classification network in multi-GPU style, the gpu0 memory is larger than others. Is a test network occupy the gpu0 memory. In that circumstances, how can i remove the test network ?"
"For example,how to adjust m when training with VGG Face instead of MS-1m?"
"hi, have you tried vgg16 with arcface loss? what's your accuracy on lfw? i train the vgg16 with arcface loss training only on the vggface2 dataset,  the loss can't converge and it stop at about 6. I checked the code and without arcface loss it works, but there are some problems while adding arcface loss.i am so confused about it. Adam and momentum got the same result. Please help me .Many thanks."
"有哪位大佬可以给出这两个的具体使用方法，比如test的时候
'--model', default='', help='path to load model.'
'--ga-model', default='', help='path to load model.'
两个参数，哪个是使用训练好的模型路径，那另外一个model路径是什么，是指的mtcnn-dodel路径地下的文件吗？"
"I know that image rec is fast to load, but I need to load training images from image list file.

Is this currently supported ? If not, will this be supported in the future ?

Thanks! "
"`
    anchor = mx.symbol.slice_axis(nembedding, axis=0, begin=0, end=args.per_batch_size//3)
    positive = mx.symbol.slice_axis(nembedding, axis=0, begin=args.per_batch_size//3, end=2*args.per_batch_size//3)
    negative = mx.symbol.slice_axis(nembedding, axis=0, begin=2*args.per_batch_size//3, end=args.per_batch_size)`

As your code in train.py, should I construct my data in each batch by the format that the first 1/3 are anchor samples, then positive samples, and the last 1/3 are negative samples? @nttstar "
"I am training resnet152 with arc loss, with the loss decreasing, lfw, agedb accuracy still very low compare to resnet101"
"1、使用face2rec2.py生成自己的rec文件，recordio.unpack解析该文件时发现header有两个值，而解析train.rec时，header只有一个值。
2、tripletloss微调时，data.py中select_triplets里的“embeddings = sklearn.preprocessing.normalize(embeddings)”出现错误“ValueError: Input contains NaN, infinity or a value too large for dtype('float64')”，不知道是不是rec文件解析的原因。
3、train.py中没有facemoubilenet，是不是意味着tripletloss不支持facemobilenet？"
"我利用mxnet\trunk\tools\im2rec.py生成lst文件
再利用这lst文件，通过insightface\trunk\src\data\face2rec2.py 生成rec文件
是否要修改这些地方，insightface\trunk\src\train_softmax.py读取数据。
1、insightface\trunk\src\common\face_preprocess.py
def parse_lst_line(line):
  vec = line.strip().split(""\t"")
  assert len(vec)>=3
  aligned = int(vec[0])
  image_path = vec[1]
  label = int(vec[2])
  bbox = None
  landmark = None
。。。
修改为：
def parse_lst_line(line):
  vec = line.strip().split(""\t"")
  assert len(vec)>=3
  aligned = True#int(vec[0])
  image_path = vec[2]
  label = int(float(vec[1]))
  bbox = None
  landmark = None

2、insightface\trunk\src\data\face2rec2.py

def read_list(path_in):
    path_ = path_in[:-4]        #获取文件所在目录
    with open(path_in) as fin:
        identities = []
        last = [-1, -1]
        _id = 1
        while True:
            line = fin.readline()
            if not line:
                break
            item = edict()
            item.flag = 0
            item.image_path, label, item.bbox, item.landmark, item.aligned = face_preprocess.parse_lst_line(line)
            item.image_path = os.path.join(path_, item.image_path)             #图片路径
            if not os.path.exists( item.image_path):
                continue
            if not item.aligned and item.landmark is None:
              #print('ignore line', line)
              continue
            item.id = _id
            item.label = [label, item.aligned]  => item.label = label        #label
  "
"hello,there. recently i'm trying to fine-tune the triplet loss for occulsion face, which pretrained model should i use, can u give me some advice? (LResNet50E-IR or LResNet34E-IR ?)"
"out_list = [mx.symbol.BlockGrad(embedding)]
Can you tell me why you use the 'mx.symbol.blockGrad()'?
"
""
"In the paper, you use batchsize = 512 and 8 GPUs.

So does it mean (1). the REAL batchsize = 8 * 512 = 4096? 

or it means (2). each GPU use batch 64 and 64 * 8 = 512?

which one is true? 
"
"Hi 

I am training with the following command:
```CUDA_VISIBLE_DEVICES='0,1' python -u train_softmax.py --network y1 --loss-type 4 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-ms1m-arcface```

I am using the default learning params however the accuracy stays zero:
```call reset()
[21:47:58] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
INFO:root:Epoch[0] Batch [20]	Speed: 329.02 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [40]	Speed: 333.24 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [60]	Speed: 333.42 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [80]	Speed: 333.21 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [100]	Speed: 333.03 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [120]	Speed: 333.32 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [140]	Speed: 333.22 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [160]	Speed: 333.32 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [180]	Speed: 333.24 samples/sec	acc=0.000000

```"
"Hi,

I would like to test  LResNet50E-IR@BaiduDrive,  which is shown %99.80 accuracy in the readme.

I tried to use deploy / test.py but it gives error.

What is the basic baby step to use this LResNet50E-IR@BaiduDrive model to test to images similarity ? I could help other beginners ..

thx"
"I have a database and wanna make it as verification sets during training network.Could you tell me how to make it  .bin file? just as lfw.bin,  cfp_ff.bin, cfp_fp.bin, agedb_30.bin.THANKS!"
"I follow your steps，but i meet this problem ,can anybody give me some solutions.  

Traceback (most recent call last):
  File ""train_softmax.py"", line 485, in <module>
    main()
  File ""train_softmax.py"", line 482, in main
    train_net(args)
  File ""train_softmax.py"", line 476, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 512, in fit
    self.update()
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 651, in update
    self._kvstore, self._exec_group.param_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/model.py"", line 134, in _update_params_on_kvstore
    kvstore.push(name, grad_list, priority=-index)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/kvstore.py"", line 232, in push
    self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [11:41:11] src/storage/./pooled_storage_manager.h:108: cudaMalloc failed: out of memory



"
"when i use test.py,meet this error ,I try many mxnet Version ,but not reolve .who can ask me the reason,Thanks!
python deploy/test.py --model models/model-r34-amf/model,0

File ""deploy/test.py"", line 29, in <module>
    f1 = model.get_feature(img)
  File ""/home/wcy/face/insightface/deploy/face_model.py"", line 92, in get_feature
    self.model.forward(db, is_train=False)
  File ""/usr/lib/python2.7/site-packages/mxnet-1.2.0-py2.7.egg/mxnet/module/module.py"", line 610, in forward
    self.reshape(new_dshape, new_lshape)
  File ""/usr/lib/python2.7/site-packages/mxnet-1.2.0-py2.7.egg/mxnet/module/module.py"", line 471, in reshape
    self._exec_group.reshape(self._data_shapes, self._label_shapes)
  File ""/usr/lib/python2.7/site-packages/mxnet-1.2.0-py2.7.egg/mxnet/module/executor_group.py"", line 382, in reshape
    self.bind_exec(data_shapes, label_shapes, reshape=True)
  File ""/usr/lib/python2.7/site-packages/mxnet-1.2.0-py2.7.egg/mxnet/module/executor_group.py"", line 358, in bind_exec
    allow_up_sizing=True, **dict(data_shapes_i + label_shapes_i))
  File ""/usr/lib/python2.7/site-packages/mxnet-1.2.0-py2.7.egg/mxnet/executor.py"", line 430, in reshape
    ""If this is intended, set partial_shaping=True to suppress this warning."")
AssertionError: Shape of unspecified array arg:conv0_weight changed. This can cause the new executor to not share parameters with the old one. Please check for error in network.If this is intended, set partial_shaping=True to suppress this warning."
"I tried using the LSoftmax files but got the errors (can be seen below):

```
perator/lsoftmax.o
src/operator/lsoftmax.cc:30:3: error: stray '\302' in program
   <title>insightface/lsoftmax.cc at master · deepinsight/insightface · GitHub</title>
   ^
src/operator/lsoftmax.cc:30:3: error: stray '\267' in program
src/operator/lsoftmax.cc:30:3: error: stray '\302' in program
src/operator/lsoftmax.cc:30:3: error: stray '\267' in program
src/operator/lsoftmax.cc:159:10: warning: missing terminating ' character
     <!-- '""` --><!-- </textarea></xmp> --></option></form><form class=""js-site-search-form"" data-scope-type=""Repository"" data-scope-id=""102057483"" data-scoped-search-url=""/deepinsight/insightface/search"" data-unscoped-search-url=""/search"" action=""/deepinsight/insightface/search"" accept-charset=""UTF-8"" method=""get""><input name=""utf8"" type=""hidden"" value=""&#x2713;"" />
          ^
src/operator/lsoftmax.cc:159:5: error: missing terminating ' character
     <!-- '""` --><!-- </textarea></xmp> --></option></form><form class=""js-site-search-form"" data-scope-type=""Repository"" data-scope-id=""102057483"" data-scoped-search-url=""/deepinsight/insightface/search"" data-unscoped-search-url=""/search"" action=""/deepinsight/insightface/search"" accept-charset=""UTF-8"" method=""get""><input name=""utf8"" type=""hidden"" value=""&#x2713;"" />
     ^
src/operator/lsoftmax.cc:506:69: error: stray '#' in program
         <td id=""LC7"" class=""blob-code blob-code-inner js-file-line"">#<span class=""pl-k"">include</span> <span class=""pl-s""><span class=""pl-pds"">&quot;</span>./lsoftmax-inl.h<span class=""pl-pds"">&quot;</span></span></td>
                                                                     ^
src/operator/lsoftmax.cc:812:10: warning: missing terminating ' character
     <!-- '""` --><!-- </textarea></xmp> --></option></form><form class=""js-jump-to-line-form"" action="""" accept-charset=""UTF-8"" method=""get""><input name=""utf8"" type=""hidden"" value=""&#x2713;"" />
          ^
src/operator/lsoftmax.cc:812:5: error: missing terminating ' character
     <!-- '""` --><!-- </textarea></xmp> --></option></form><form class=""js-jump-to-line-form"" action="""" accept-charset=""UTF-8"" method=""get""><input name=""utf8"" type=""hidden"" value=""&#x2713;"" />
     ^
src/operator/lsoftmax.cc:864:5: error: stray '\342' in program
     You can’t perform that action at this time.
     ^
src/operator/lsoftmax.cc:864:5: error: stray '\200' in program
src/operator/lsoftmax.cc:864:5: error: stray '\231' in program
src/operator/lsoftmax.cc:7:1: error: expected unqualified-id before '<' token
 <!DOCTYPE html>
 ^
src/operator/lsoftmax.cc:506:150: error: expected unqualified-id before '<' token
         <td id=""LC7"" class=""blob-code blob-code-inner js-file-line"">#<span class=""pl-k"">include</span> <span class=""pl-s""><span class=""pl-pds"">&quot;</span>./lsoftmax-inl.h<span class=""pl-pds"">&quot;</span></span></td>
                                                                                                                                                      ^
src/operator/lsoftmax.cc:506:200: error: expected unqualified-id before '<' token
         <td id=""LC7"" class=""blob-code blob-code-inner js-file-line"">#<span class=""pl-k"">include</span> <span class=""pl-s""><span class=""pl-pds"">&quot;</span>./lsoftmax-inl.h<span class=""pl-pds"">&quot;</span></span></td>
                                                                                                                                                                                                        ^
Makefile:431: recipe for target 'build/src/operator/lsoftmax.o' failed
make: *** [build/src/operator/lsoftmax.o] Error 1
```
It would be helpful if anyone can tell me about the error."
"@nttstar 

Hi

First of all thanks for your support in the issues section. I had a question regarding the possibility to batch multiple images for faster inference:

**Is it possible to get the embedding of many images in one go instead of repeatedly calling the `get_feature` method?**

My inference time for just one cropped image is nearly 20ms per image but I was wondering if this time could be reduced for batches of images.

I wrote a code to initialize the Model with variable `batch_size`s (I used 10, 25 and 50) and fed `self.model.forward(db)` with a list of cropped and aligned images (between 100 or a 1000 images) but two strange things happened:

1. The inference time increased when passing `n` images to it compared to when I called `get_feature` repeatedly `n` times.

2. No matter how much I changed the `batch_size` (from 1 to 50), the inference time stayed near 30 ms per image.

I'm more familiar with Tensorflow than MXNet and I was wondering if I'm doing something wrong with these parts of the code:

```
data = mx.nd.array(aligned_face_images)
db = mx.io.DataBatch(data=(data,))
self.model.forward(db)
```"
"I generated my own data using lfw2pack.py, and image size is 160x160.
But when I run verification.py, it popped out this error...

""expected [3,160,160], got [3,112,112]""

Traceback (most recent call last):
  File ""verification.py"", line 566, in <module>
    data_set = load_bin(path, image_size)
  File ""verification.py"", line 196, in load_bin
    data_list[flip][i][:] = img
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.py"", line 437, in __setitem__
    self._set_nd_basic_indexing(key, value)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.py"", line 691, in _set_nd_basic_indexing
    value.copyto(self)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/ndarray/ndarray.py"", line 1876, in copyto
    return _internal._copyto(self, out=other)
  File ""<string>"", line 25, in _copyto
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/_ctypes/ndarray.py"", line 92, in _imperative_invoke
    ctypes.byref(out_stypes)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 146, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:54:06] src/operator/nn/./../tensor/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node  at 0-th output: expected [3,160,160], got [3,112,112]"
"How do you get `Verification` or `VR@FAR10^6` score on megaface?
I run command like:
```shell
python run_experiment.py -s 1000000 \
    path_megaface_feature path_facescrub_feature _arcface.bin path_results/
```
And I got output with pretrained-R50E (cleaned)
```log
Done matching! Score matrix size: 3530 1000000
Saving to /home/data/insight/megaface/raw_eval/r50/results_cm/otherFiles/facescrub_megaface_r50_112x112_1000000_1.bin
Computing test results with 1000000 images for set 1
Loaded 3530 probes spanning 80 classes
Loading from /home/data/insight/megaface/raw_eval/r50/results_cm/otherFiles/facescrub_facescrub_r50_112x112.bin
Probe score matrix size: 3530 3530
distractor score matrix size: 3530 1000000
Done loading. Time to compute some stats!
Finding top distractors!
Done sorting distractor scores
Making gallery!
Done Making Gallery!
Allocating ranks (1000080)
Rank 1: 0.965815
```
I carefully check all output log, and I couldn't find anything seems to be `Verification` score. 
Is there any other command that can produce  `Verification` or `VR@FAR10^6` score?
@nttstar @bruinxiong or anyone who is willing to offer help.
Thank you.
- megaface devkit [readme.txt](http://megaface.cs.washington.edu/participate/content/readme.txt)"
"I am using the arcface loss to train the resnet34. I set the margin parameter m  to 0.5, and set the learning rate as suggested in the paper. But my network can't converge well. Are there any modifications sholud I do to fit my input?"
"hello everyone,
while training on my dataset, the saved model size of mobilefacenet is only 9.6M rather than 48M (training on the provided faces_ms1m_112x112). I only change the ms1m to my dataset,  anyone konws where is wrong? "
"I have trained the model according to  the tutorial, now it is the epoch 9/batch 18000, it seems that no temp model generated"
"In the ArcFace code 

> mm = math.sin(math.pi-m)*m

 line present.
Could you explain what is the meaning/purpose of using mm? It seems that it lowers logit of ground truth class when angle between weight and sample + margin is bigger than pi (cosine function out of monotonic range and we can't apply margin directly), but why it takes this particular form?
Also why is it valid to multiply something in sine space (and actually why sin?) and margin which is in angular space?
Thanks for helping!


"
"train data preprocess： 
                _data = self.imdecode(s)
......................................
                if self.nd_mean is not None:
                    _data = _data.astype('float32')
                    _data -= self.nd_mean
                    _data *= 0.0078125


test data preprocess： 
def load_bin(path, image_size):
  ..............
   img = mx.image.imdecode(_bin)
    img = nd.transpose(img, axes=(2, 0, 1))

Why training data is to subtract the mean and multiply by 0.0078, but why not tesing data?"
"I Fine-tune  'model-r50-am-lfw' on 'faces_emore' datasets,and got a new model v1, but when i try to finetune v1 on my own datasets ,it got a problem like this. 
Incompatible attr in node  at 0-th output: expected [85742,512], got [12700,512]
i also can finet-une  'model-r50-am-lfw' on my own datasets ,it works.
 How to save the model correctly? or How to finetune on my own datasets ? 
thank you!"
"I ran the r50 and the saved model is 570MBytes. I think it saves all variables and unnecessary parts like data pre-processing. 
I just want to save the forward architecture and weights only and used for calculating the embedding. Any way to achieve that? 
I'm new in Mxnet. I read the docs and I didn't find such functions. 
@nttstar I found the pre-trained model you provided for r50 is only ~400Mbytes. Which function you used to save the model?"
"we can see the following function。

def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):
    assert(embeddings1.shape[0] == embeddings2.shape[0])
    assert(embeddings1.shape[1] == embeddings2.shape[1])
    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])
    nrof_thresholds = len(thresholds)
    k_fold = LFold(n_splits=nrof_folds, shuffle=False)
    
    val = np.zeros(nrof_folds)
    far = np.zeros(nrof_folds)
    
    diff = np.subtract(embeddings1, embeddings2)
    dist = np.sum(np.square(diff),1)
    indices = np.arange(nrof_pairs)
    
    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):
      
        # Find the threshold that gives FAR = far_target
        far_train = np.zeros(nrof_thresholds)
        for threshold_idx, threshold in enumerate(thresholds):
            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])
        if np.max(far_train)>=far_target:
            f = interpolate.interp1d(far_train, thresholds, kind='slinear')
            threshold = f(far_target)
        else:
            threshold = 0.0
    
        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])
  
    val_mean = np.mean(val)
    far_mean = np.mean(far)
    val_std = np.std(val)
    return val_mean, val_std, far_mean

waiting for your reply!"
"Hi, with the megaface development kit,  we can get the cmc and roc.
I am curious about the thresholds used in the roc test.
Do you have any idea?"
"mxnet.base.MXNetError: [17:18:55] src/io/local_filesys.cc:166: Check failed: allow_null  LocalFileSystem::Open ""/medical_data/Datasets/insightface/models/model-r50-am-lfw-symbol.json"": No such file or directory

I run verification.py like this, but got an error says cannot find this file model-r50-am-lfw-symbol.json, but seems like it is a wrong file path.

sudo python3 -u verification.py --gpu 0 --data-dir /medical_data/Datasets/insightface/datasets/faces_ms1m_112x112 --model /medical_data/Datasets/insightface/models/model-r50-am-lfw,0 --target agedb_30 --batch-size 128
"
"Did your released model ""LResNet50E-IR"" use the triplet loss to fine-tune?"
"Hello，I want to use your training set (MS-Celeb-1m).Due to I need filter out some identities,can you give me the original id？（I find the training set label from 1~8.5k）"
could you please explain the meaning of output_c2c?
"Are the COCO loss, Marginal loss and Large Margin softmax loss functions complete in train.py? Plus center loss would be completed in near future?"
"I just tried the trained model. If the LFW is not aligned, the acc is really low, 93% in my case. But if it's aligned with 'align_lfw.py', the acc could be high as 99.8. The align is necessary but why the model relies on the alignment so much? And also, it depends on the exact alignment method (maybe the exact settings)? This makes the model not so robust. 
Is that because the training data is also aligned this way? anyway to improve? "
"Hello @nttstar

I didn't found exact problem in current issues. Could you help me solve the issue please?
My GPU is 950M with 4GB dedicated memory. I use cuda-9.1 with mxnet-cu91 module.
``` bash 
$ CUDA_VISIBLE_DEVICES='0' python2 -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_emore --prefix ../model 
gpu num: 1
num_layers 100
image_size [112, 112]
num_classes 85742
Called with argument: Namespace(batch_size=128, beta=1000.0, beta_freeze=0, beta_min=5.0, bn_mom=0.9, ckpt=1, ctx_num=1, cutoff=0, data_dir='../datasets/faces_emore', easy_margin=0, emb_size=512, end_epoch=100000, fc7_wd_mult=1.0, gamma=0.12, image_channel=3, image_h=112, image_w=112, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, max_steps=0, mom=0.9, network='r100', num_classes=85742, num_layers=100, per_batch_size=128, power=1.0, prefix='../model', pretrained='', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 100
0 1 E 3 prelu
INFO:root:loading recordio ../datasets/faces_emore/train.rec...
header0 label [5822654. 5908396.]
id2range 85742
5822653
rand_mirror 1
[16:41:23] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [400000, 560000, 640000]
Traceback (most recent call last):
  File ""train_softmax.py"", line 485, in <module>
    main()
  File ""train_softmax.py"", line 482, in main
    train_net(args)
  File ""train_softmax.py"", line 476, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 484, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/lib/python2.7/site-packages/mxnet/module/module.py"", line 430, in bind
    state_names=self._state_names)
  File ""/usr/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 265, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 361, in bind_exec
    shared_group))
  File ""/usr/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 639, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 1519, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
[16:41:41] src/storage/storage.cc:119: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: unknown error

Stack trace returned 10 entries:
[bt] (0) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x31a18a) [0x7f32ed68618a]
[bt] (1) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x31a7b1) [0x7f32ed6867b1]
[bt] (2) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x29e36e3) [0x7f32efd4f6e3]
[bt] (3) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x29e65df) [0x7f32efd525df]
[bt] (4) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x29e7a9f) [0x7f32efd53a9f]
[bt] (5) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24a13c3) [0x7f32ef80d3c3]
[bt] (6) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24a153d) [0x7f32ef80d53d]
[bt] (7) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24adfef) [0x7f32ef819fef]
[bt] (8) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24b4bd0) [0x7f32ef820bd0]
[bt] (9) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24c1232) [0x7f32ef82d232]
```

Thanks a lot."
"I noticed the following code:

```python3
  tform = trans.SimilarityTransform()
  tform.estimate(dst, src)
  M = tform.params[0:2,:]
  #M = cv2.estimateRigidTransform( dst.reshape(1,5,2), src.reshape(1,5,2), False)
```
I looks like originally the transform M is estimated with opencv estimateRigidTransform, it may return None for bad data, is this  the **only reason** to switch to skimage.transform.SimilarityTransform()?"
embedding = sklearn.preprocessing.normalize(embedding).flatten() 
"An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.


what's wrong?how to deal with it?
I have install mxnet twice,but it's no useful."
"Hi all,
I tried to train according to the tutorial, but after loading the data, cuda broken with hints:
""no capable cuda device"", it is very strange, because I always train the yolo networks without any error before, so I checked with gpu-z, the cuda/opencl had been disabled! I googled for each solution but still failed, I have to re-install the windows and cuda, then everything is fine, but if I install mxnet and train insightface, then the same nightmare happened, I have to re-install windows again:(
anyone with the same experience? plz help..."
"Have you ever done ablation analysis on the scaling parameter s?
In other words, what do we rely on to set s? the number of IDs? or the distribution of dataset?"
"I have read the implementation of angular margin in your code. It seems there are some details not mentioned in the paper.

Let t be the angle between a feature vector and the weight vector of its groundtruth class. The value of t is in [0, pi]. Let m be the margin parameter. m must be < pi/2 and >= 0. Let s be the scaling factor.

If no margin is applied, the score of the groundtruth class input to the softmax layer is s * cos(t).
The angular margin is then implemented as follows:
if t + m < pi, i.e. cos(t) > cos(pi - m), the score of the groundtruth class is changed to s * cos(t+m).
if t + m >= pi: If easy_margin is true, the score of the groundtruth class is not modified. Otherwise the score of the  groundtruth class is changed to s * (cos(t) - m*sin(pi-m)).

So for very hard instances (t + m >= pi), the code actually falls back to cosine margin with margin=m*sin(pi-m). This actually creates a discontinuity point on the target logit curve. 

The paper says:
    the target logit curve is not monotonic decreasing  when theta in [0, 180] ... the increasing interval of ArcFace is almost never reached during training. Therefore we do not need to deal with this explicitly.

But actually you have dealt with this explicitly in the code.


"
"在train_softmax.py中好像没有tripletloss， 在train.py中有tripletloss。但你的例子中写的是tripletloss
(4). Fine-turn the above Softmax model with Triplet loss.
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network m1 --loss-type 12 --lr 0.005 --mom 0.0 --per-batch-size 150 --data-dir ../datasets/faces_ms1m_112x112 --pretrained ../model-m1-softmax,50 --prefix ../model-m1-triplet
train.py是不是可以使用？"
"Hello author, I am very happy to learn from your open source InsightFace project. 
I recently found that the project has updated the function of recognizing age and gender. 
This is also a function that I am very much looking forward to and I would like to express my thanks again.

I have a question that I saw in deploy/test.py
```
parser.add_argument('--model', default='', help='path to load model.')
parser.add_argument('--ga-model', default='', help='path to load model.')
```
In previous versions, the model parameter of test.py was the r34 model. 
Why was the model parameter empty in test.py this time?
And this time the ga-model has been added. 
It seems that I can't find the download of the ga-model model. 
I do not know if it is convenient for you to provide a download of ga-model?

Looking forward to your reply.
Good luck."
" As you say, the emore dataset is still largely based on ms1m, do you add new identity besides ms1m? Can you public the names? Thanks a lot.
 best "
"@nttstar   As arch loss is s.cos(t+m)=s(cos_t.cos_m-sin_t.sin_m). However, I'm not able to understand the codes about the arch loss: Why the cost_t needs to devided by s first? see the bold line below.  


  elif args.loss_type==4:
    s = args.margin_s
    m = args.margin_m
    assert s>0.0
    assert m>=0.0
    assert m<(math.pi/2)
    _weight = mx.symbol.L2Normalization(_weight, mode='instance')
    nembedding = mx.symbol.L2Normalization(embedding, mode='instance', name='fc1n')*s
    fc7 = mx.sym.FullyConnected(data=nembedding, weight = _weight, no_bias = True, num_hidden=args.num_classes, name='fc7')
    zy = mx.sym.pick(fc7, gt_label, axis=1)
    **cos_t = zy/s**
    cos_m = math.cos(m)
    sin_m = math.sin(m)
    mm = math.sin(math.pi-m)*m
    #threshold = 0.0
    threshold = math.cos(math.pi-m)
    if args.easy_margin:
      cond = mx.symbol.Activation(data=cos_t, act_type='relu')
    else:
      cond_v = cos_t - threshold
      cond = mx.symbol.Activation(data=cond_v, act_type='relu')
    body = cos_t*cos_t
    body = 1.0-body
    sin_t = mx.sym.sqrt(body)
    new_zy = cos_t*cos_m
    b = sin_t*sin_m
    new_zy = new_zy - b
    new_zy = new_zy*s
    if args.easy_margin:
      zy_keep = zy
    else:
      zy_keep = zy - s*mm
    new_zy = mx.sym.where(cond, new_zy, zy_keep)
    diff = new_zy - zy
    diff = mx.sym.expand_dims(diff, 1)
    gt_one_hot = mx.sym.one_hot(gt_label, depth = args.num_classes, on_value = 1.0, off_value = 0.0)
    body = mx.sym.broadcast_mul(gt_one_hot, diff)
    fc7 = fc7+body"
"Hi,

How we can test pretrained models on CPU ?

it gaves:

RuntimeError: simple_bind error. Arguments:
data: (1, 3L, 68L, 68L)
[23:39:56] src/storage/storage.cc:118: Compile with USE_CUDA=1 to enable GPU usage
"
solved
"Hi, 
I'd like to report a problem when I use insightface
I used cv2.imread() to read 46,000 jpg file, only 3 has been read successfully with the following code.
But if I import cv2 before face_model, I can read all the pictures. I checked face_model.py, connot find the reason. (not because of absolute_import)

```

import face_model
import cv2
import os

rootdir = '/home/xxx/imagedir'
listf = os.listdir(rootdir)
a = 0
for i in range(0,len(listf)):
        path = os.path.join(rootdir,listf[i])
        if os.path.isfile(path):
                img = cv2.imread(path)
                if img is None:
                        print(path)
                        continue
                else:
                        a  += 1
print(a)
```
"
"it'll be easy for us to use the dataset and update it in the future. 

Best"
"I  downloaded the resnet-34 model and I followed the instructions about the ""Test on MegaFace"":
1. Align all face images of facescrub dataset and megaface distractors
2. Generate feature files for both facescrub and megaface images.
3. Remove Megaface noises which generates new feature files.

Next, I ran the MegFace's test script ""run_experiment.py"" with the following command:
```
python run_experiment.py -p ../templatelists/facescrub_features_list.json ../../MegaFace_Features_cm ../../FaceScrub_Features_cm _arcface.bin ../../results
```

Finlly, I got the result files, and  I opened the file ""cmc_facescrub_megaface_arcface_1000000_1.json""  and I saw the following (part of the file content) : 
```
	""cmc"" : 
	[
		[ 0, 2, 16, 425, 444333, 1000000 ],
		
		[
			0.9542288780212402,
			0.9660166501998901,
			0.9761315584182739,
			0.9861488938331604,
			0.9961531758308411,
			1.0
		]
	],
```
While the rank1@10^6 accuracy is  95.4% , but I see the proposed accuracy is 96.7%.
What I tested is the model provided by the user, but why  there is a difference between these two results?
I use the cropped facescrub set to test the result, which is a subset of FaceScrub including 80 identities, is the method I used to test wrong? 
I check the official website of megface, there is a dataset named “FaceScrub cropped + json zip ” in which the images are all aligned and cropped, should I use this dataset to test? But I think the test results should be the same. 
Plese help me and give me a little guidance. Thanks!

"
"when I run  python deploy/ test.py,
I have not  model 
model-r50-am-age "
"Hi, recently I'm very confused about the format of verification sets, such as lfw.bin. Specially, whats the meaning of Ture or False in lfw.bin???
`bins, issame_list = pickle.load(open(path, 'rb'))`
Looking forward to your reply, and thank you for your resources very much!"
"The shuffling will cause slow IO because the file reader is out-of-order. This problem will occur even in the SSD disk.
https://github.com/deepinsight/insightface/blob/d37bb397f01256c83312675ca7a2cb3170b77ddc/src/image_iter.py#L93

I think remove shuffling or add a PrefetchingIter on the origin iter will speed up IO.
```
    data_train = FaceImageIter(...)
    data_val = FaceImageIter(...)
    data_train = mx.io.PrefetchingIter(data_train)
    data_val = mx.io.PrefetchingIter(data_val)
```"
"I trained 50 layer network with loss type 4 and my prediction time almost 3-4 second per image but the pre trained model (""Refined-MS1M@BaiduDrive"") only 1.5-2 second. I compared the model size network layer,  ""json"" file and layer name . All are same in both model. Please advice me, where is my mistake.

"
"Hi, where is your errata list? I tried to find, but all in vain.
I have many additions and even corrections to your list.
For example

Adrienne Barbeau 4246 is actually Susan Delfino

while Zooey Deschanel_20937 is actually Zooey Deschanel

and so on... I have many differences with your noises file

![zooey deschanel_20937](https://user-images.githubusercontent.com/27707369/40043109-2b7c350a-582c-11e8-9214-4033d10c9d07.jpg)
"
"Where are the architecture files (fresnet, finception_resnet_v2) imported in [train_softmax.py](https://github.com/deepinsight/insightface/blob/895ee7e3ae81efda22f8a4c3c9a1022915b9aff7/src/train_softmax.py#L22)?

Why is not present in the repo? "
"i have run the lfw2pack.py to generate my own eval datasets.but the accuracy is very low,so i make the lfw.bin again,named lfw112jpg.bin. and i find the accuracy is also two low when contrast your lfw.bin.
here is the accuracy.
testing verification..
(12000, 512)
infer time 11.238896
[lfw][4000]XNorm: 22.697246
[lfw][4000]Accuracy-Flip: 0.92200+-0.01503
testing verification..
(12000, 512)
infer time 11.237962
[lfw112jpg][4000]XNorm: 22.694899
[lfw112jpg][4000]Accuracy-Flip: 0.72300+-0.01709

i don't know how to solve this?can this have any data process i ignored？
@nttstar 
"
"Hello. I have some questions about facemobilenet.
I have trained the mobilefacenet.
According to MobileFaceNet Paper, the mobilenet model size is only 4M. 
However, My mobilefacenet model size is about 172M.
Do you know why?? 

My train code is below.
CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --network y1 --loss-type 0 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-y1-softmax
"
"hi everyone,
The model size of mobilefacenet I trained is about 46M，which is much bigger than size in the original paper(4M). Do you know where is wrong?"
"Traceback (most recent call last):
  File ""train_softmax.py"", line 485, in <module>
    main()
  File ""train_softmax.py"", line 482, in main
    train_net(args)
  File ""train_softmax.py"", line 334, in train_net
    sym, arg_params, aux_params = get_symbol(args, arg_params, aux_params)
  File ""train_softmax.py"", line 268, in get_symbol
    softmax = mx.symbol.SoftmaxOutput(data=fc7, label = gt_label, name='softmax', normalization='valid')
UnboundLocalError: local variable 'fc7' referenced before assignment

My network is ""y1"".
I read the code in train_softmax.py, there is no ""loss_type==12"" !? Of course there is no fc7"
"I have only two GPU of 1070ti, so my command is `CUDA_VISIBLE_DEVICES='0,1' python -u train_softmax.py --network y1 --loss-type 4 --margin-s 128 --margin-m 0.5 --per-batch-size 182 --emb-size 128 --ckpt 2 --data-dir ../datasets/faces_ms1m_112x112 --wd 0.00004 --fc7-wd-mult 10.0 --prefix ../mobilenet/model-mobilefacenet-182`
My final accuracy is lfw: 0.98967,     agedb_30: 0.911,     cfp_fp: 0.87471.  And the speed of training is about 900sample/sec.
Is the accuracy normal or too low?"
""
Why not combining the `ms1m` datasets and `vggface2` together to train the model?
"hi,
I have gone through the repository and found that the license type used in this repository is ""MIT"".
1.Is the model file also covered under MIT license?
2.With your knowledge, any idea about the licensing of datasets you used?"
" I can't run this test ,got stuck .


root@tegra-ubuntu:/home/nvidia/zhuzhipeng/insightface/deploy# python test.py
loading ../models/model-r34-amf/model 0
[07:28:23] src/nnvm/legacy_json_util.cc:209: Loading symbol saved by previous version v0.12.1. Attempting to upgrade...
[07:28:23] src/nnvm/legacy_json_util.cc:217: Symbol successfully upgraded!
[07:28:23] src/engine/engine.cc:55: MXNet start using engine: NaiveEngine
Traceback (most recent call last):
  File ""test.py"", line 18, in <module>
    model = face_embedding.FaceModel(args)
  File ""/home/nvidia/zhuzhipeng/insightface/deploy/face_embedding.py"", line 52, in __init__
    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])
  File ""/home/nvidia/zhuzhipeng/mxnetTX2/incubator-mxnet/python/mxnet/module/module.py"", line 430, in bind
    state_names=self._state_names)
  File ""/home/nvidia/zhuzhipeng/mxnetTX2/incubator-mxnet/python/mxnet/module/executor_group.py"", line 265, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/home/nvidia/zhuzhipeng/mxnetTX2/incubator-mxnet/python/mxnet/module/executor_group.py"", line 361, in bind_exec
    shared_group))
  File ""/home/nvidia/zhuzhipeng/mxnetTX2/incubator-mxnet/python/mxnet/module/executor_group.py"", line 639, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/home/nvidia/zhuzhipeng/mxnetTX2/incubator-mxnet/python/mxnet/symbol/symbol.py"", line 1524, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 3, 112, 112)
[07:28:27] /home/nvidia/zhuzhipeng/mxnetTX2/incubator-mxnet/3rdparty/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:110: Check failed: err == cudaSuccess (48 vs. 0) Name: MapPlanKernel ErrStr:no kernel image is available for execution on the device

"
"Traceback (most recent call last):
  File ""verification.py"", line 43, in <module>
    import mxnet as mx
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/__init__.py"", line 25, in <module>
    from . import engine
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/engine.py"", line 23, in <module>
    from .base import _LIB, check_call
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 111, in <module>
    _LIB = _load_lib()
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/base.py"", line 103, in _load_lib
    lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_LOCAL)
  File ""/usr/lib/python2.7/ctypes/__init__.py"", line 362, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: libcudart.so.8.0: cannot open shared object file: No such file or directory


This is my error. However, I install cuda 9.1 , not cuda 8.0. So how can I change cuda path or code in this git source?


"
"I had trained enough epoch and I wanted to manually stop it (the default epoch is 100000! I don't have so much time of my life), so I press ctrl+c, but where is my model?
I can't find it! The command I used to train is: `CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py 
--network y1 --loss-type 4 --margin-s 128 --margin-m 0.5 --per-batch-size 128 --emb-size 128 --data-dir ../datasets/faces_ms1m_112x112 --wd 0.00004 --fc7-wd-mult 10.0 --prefix ../model-mobilefacenet-128`
But I can't find any directory or file name like model-mobilefacenet-128. So I look into train_softmax.py and find the code: `
prefix = args.prefix
prefix_dir = os.path.dirname(prefix)
if not os.path.exists(prefix_dir):
   os.makedirs(prefix_dir)`
 in line 284
then I print the prefix_dir, and it's just "".."" !!!!
Why is that? The version of python I use is python2.7."
"It's great project !!!
But when I read the src code and paper , I have a question. 
When loss_type==4 and  (cos_t - threshold ) < 0, why zy_keep = zy - s×mm? I am wondering how the ""zy - s×mm"" outcome, because I don't find this part in the paper 《ArcFace: additive Angular Margin Loss for Deep Face Recognition 》， Thx~"
"Hi @nttstar , It is a great project!
I was confused why add arccos in the end of the angulat triplet loss?Is it possible just used the cos ap and cos an to compute the loss?"
"can you give me those codes about     # make_list(args) in the face2rec2.py,I want to reduce training samples，please！"
"@nttstar , your work is nice.Ask a question. how to save values of loss  when train model(Train ArcFace with LResNet100E-IR.)? "
Is it should be trained from finetuning?
"(1) when train loss type 2 with sphereface net,  there will be a shape error, as follows; but use other nets, it's ok. The error occurs in the sphereface_net.init_weights when infer_shape, but I can't figure it out 

> src/operator/nn/./../tensor/../elemwise_op_common.h:123: Check failed: assign(&dattr, (*vec)[i]) Incompatible attr in node _minus0 at 1-th input: expected [128,85164], got [85164]

(2) I am new to mxnet from pytorch and  how to debug mxnet conveniently?

any help will be appreciate.
"
"hi, thank you for your so nice project!
I want to train and verify on my own dataset, but your readme just show how to make train set by face2rec2.py, I want to verify on my own dataset instead of lfw.bin, can you tell us how to make verification set, which file?
thank you!"
"
Hi,
I'm trying to use the pre-trained models on my own images and noticed that the face detection section is not implemented.
Since its necessary to forward the model on the cropped faces only i wonder what face detector should i use. Do you have any tips for what will be the best face detector to use?
I saw that the datasets on the site contain already cropped faces. what detector did you use to create them?

Thanks,
Ariel."
"Coulkd you please tell me what c2c stand for? Thank U

@nttstar "
"Hi, Could you please provide the train.lst for both vgg and ms1m? cuz I want to combine these two dataset.
Thank U!

@nttstar "
"testing verification..
(12000, 128)
infer time 30.116359
[lfw][6000]XNorm: 38.367005
[lfw][6000]Accuracy-Flip: 0.50000+-0.00000
testing verification..
(14000, 128)
infer time 35.065952
[cfp_fp][6000]XNorm: 38.365932
[cfp_fp][6000]Accuracy-Flip: 0.50000+-0.00000
testing verification..
(12000, 128)
infer time 30.366434
[agedb_30][6000]XNorm: 38.366582
[agedb_30][6000]Accuracy-Flip: 0.50000+-0.00000
[6000]Accuracy-Highest: 0.51533

the train script is 
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network y1 --loss-type 4 --margin-s 128 --margin-m 0.5 --per-batch-size 128 --emb-size 128 --data-dir ../datasets/faces_ms1m_112x112 --wd 0.00004 --fc7-wd-mult 10.0 --prefix ../model-mobilefacenet-128"
"What is fc7_wd_mult used for, Why?

@nttstar "
"My acc is always like this  ""acc=0.144043"", but the LFW Accuracy-Flip is already 0.99433+-0.00403, I don't change the code, is there anything wrong with my environment? Thanks!"
"HI i'm trying to use the pretrained  MXnet models on the ms1m datasset , both available on site, but without sucess.
the pretrained model i downloaded,""model-r50-am-lfw"", had only two files in it, ""model-0000.params"" and ""model-symbol.json"" but the verification.py script seem to not work when i feed it with the following command:  
`python -u verification.py --gpu 0 --data-dir '<INSIGHTFACE_ROOT>/datasets/faces_ms1m_112x112'  -- <INSIGHTFACE_ROOT>/datasets/model-r50-am-lfw'   --target agedb_30 --batch-size 128
`
im getting 

> image_size [112, 112]
model number 0
model loading time 2.1e-05
loading..  lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
Traceback (most recent call last):
  File ""verification.py"", line 580, in <module>
    print('Max of [%s] is %1.5f' % (ver_name_list[i], np.max(results)))
  File ""/home/ariel-e/.local/lib/python2.7/site-packages/numpy/core/fromnumeric.py"", line 2272, in amax
    out=out, **kwargs)
  File ""/home/ariel-e/.local/lib/python2.7/site-packages/numpy/core/_methods.py"", line 26, in _amax
    return umr_maximum(a, axis, None, out, keepdims)
ValueError: zero-size array to reduction operation maximum which has no 

> 

I't seems like the script itself is not good and expecting to get other files in the model prameter then the model dir.

Did you overcheck this script is working with the supllied pretrained model and datsets?

I'm using:
Cuda: 7.5
cudnn: 5.1
GPU: GeForce GTX 1070
System: Ubuntu
![36717978-d0c3c1e2-1ba8-11e8-90c6-7b0b2bb214f7](https://user-images.githubusercontent.com/32613612/39093507-6049760e-4629-11e8-934a-4cd6a07943df.PNG)

Thanks,
Ariel
"
"When I used the align_facescrub.py to handle the original Facescrub, I found some of the cropped images are badly cropped, the faces are not at center and even no face in image at all. I checked the code and found that the problem is that some bounding boxes provided by Facescrub in facescrub_actors.txt and facescrub_actresses.txt are wrong. The align_facescrub.py uses that bounding boxes information to calculate IOU. As wrong bounding boxes have low IOU with faces detected by MTCNN, the face detection fails. I fixed wrong bounding boxes in facescrub_actors.txt and facescrub_actresses.txt , and here is the correction I made: 

Not center, bad crop:

Image                           |    original bounding boxes |     corrected bounding boxes
actresses:
Erin_Cummings_17432        (278,131,427,280)                    (175,60,325,250)
Erin_Cummings_17279        (208,162,429,383)                   (150,50,340,325)
Morena_Baccarin_3599      (50,477,831,1258)                    (75,50,575,700)
Andrea_Bowen_9073          (105,89,237,221)                       (160,40,260,160)
Brianna_Brown_10442        (105,38,237,170)                       (50,30,270,300) 
actors:
Mark_Wahlberg_40368      (103,87,229,213)                        (125,40,325,350)
Robert_Duvall_49660        (177,181,562,566)                       (70,35,325,410)

No face:
actresses:
Fran_Drescher_22942      (434,169,544,279)                      (365,125,445,240)
Joanna_Kerns_40134        (183,102,290,209)                      (350,125,540,400)
actors:
Shia_LaBeouf_53180        (351,118,665,432)                        (210,10,400,250)
Robert_Duvall_49687        (283,94,435,246)                       (200,30, 310,200)

By doing this, you do not have to change the code in align_facescrub.py in order to detect the missing faces, so that ensures you are using same codes to deal with megaface distraction images.

#167 mentioned failed to replicate the results in megaface, using the align_facescrub.py

#144 also mentioned the differences between images in aurthor's facescrub_images_112x112.tar, and images generated by the align_facescrub.py

This may help you get all the faces. But I still find that, after this correction, the faces generated by align_facescrub.py are not exactly the same as images in facescrub_images_112x112.tar . The face images in facescrub_images_112x112.tar cropped a smaller region centered in face, so after resize, the face looks bigger. Is there any extra preprocessing you did to generate face images provided in facescrub_images_112x112.tar ?
"
"1. add losstype=8 //centerloss.
problem: 
mxnet.base.MXNetError: [13:38:41] src/io/image_io.cc:186: Check failed: inputs[00
].ctx().dev_mask() == Context::kCPU (2 vs. 1) Only supports cpu input"
"I first copy ""operator"" folder of insightface to “3rdparty” folder of mxnet, then compile mxnet successfully. At last I bind python with mxnet. 
What do I miss?
"
"Hi,

Are they pre-trained model to test with different face domains ? 

Best"
"I have used the MTCNN to generate the aligned images and trained the models. Now I want to test the model on megaface. When I follow your instructions ""python -u gen_megaface.py"", I find a few path parameters confuse me. Could you help me?

What does the ""lst"" file store? How can I generate it?

Thanks

```
  #megaface_lst = ""/raid5data/dplearn/faceinsight_align_megaface.lst""
  megaface_lst = ""/raid5data/dplearn/megaface/megaface_mtcnn_112x112/lst""
  #facescrub_lst = ""/raid5data/dplearn/faceinsight_align_facescrub.lst""
  facescrub_lst = ""/raid5data/dplearn/megaface/facescrubr/small_lst""
  if args.fsall>0:
    facescrub_lst = ""/raid5data/dplearn/megaface/facescrubr/lst""
```"
"@nttstar, Thanks for sharing this nice work. I am trying to train with Inception-ResNet-v2, howerer the GPU memory consumes so much.  Could  you please tell me the details about training Inception-ResNet-V2, such as GPU version 、GPU numbers、batch_size per GPU."
"Thanks for sharing your code.
I have read sphere face , cos(mx) need bp, however,  there isn't bp for cos(m+x) in your code. Why did not  you write back propagation."
Could you provide the visualization of architecture of LResNet100E_IR? I wanna check some details because I found there were modifications to backbone ResNet100 except for those mentioned in the paper when I run the TF version.
"I tested a 112 by 112 image get 512 features using insightface source  in the same machine.

Caffe LResNet50E-IR is about 0.8 - 0.9 sec base on c++.
Mxnet LResNet50E-IR is about 0.3 - 0.4 sec base on python.

Do you know why? I think c++ caffe should be faster than python."
"I checked the Facescrub dataset for Megaface Challenge and found two extra noisy images after using facescrub_noises.txt provided by the author:

Christian_Bale_12006.png
Pamela_Anderson_2474.png

These two images are not real person. One is a toy figure and the other one is a painting. 




"
"what's the format in cfp_frontal_paris.mat and 04_FINAL_protocol_30_years.mat? 
I want to re-generation these bin files, could you share these matlab files and related txt files?
thanks!"
"百度网盘里给的模型和verification代码里用的名字似乎不同，没有办法用逗号分隔，net是空的。
The pretrained model “model-r50-am-lfw” can't split by "","" the array net is empty."
"Hi,
I noticed you use skimage.transform rather than cv2.estimateRigidTransform to do face alignment. Does it mean skimage performs better on this task?
I would like to deploy with C++, which means OpenCV is more accessible using C++. What is the problem if alignment with opencv?"
""
"Thanks for your contributions in the community of face recognition. 
As discussed in the #19, I also want to use the clean list for my own alignment method. I extract the images from the original tsv file and name them by the rule ""mid/searchid.jpg"". But how do I attach label for each image? It seems that MID is not reliable for the unique person label. For example, the directory of MID m.0107_f contains different people. What's more, other clean lists constructed by other institutions also indicates that MID does not map to the person label in a one-to-one manner. "
"Dear Author

The `test.py' code in the deploy package calculates the Distance and Similarity of 2 faces. But is there a proper threshold to dismiss two faces or accept them as from the same person?

For example DLib's network is trained for a `0.6` radius and if the L2 distance of the faces is more than that, then the faces are not from the same person.

Thanks"
"I have tried many times and only got 96.58 on megaface, does anyone else have replicated this result? In my case, the first 10 number of Adam_Brody_420.png's feature is [ 0.0424159 ,  0.05080357,  0.00801507, -0.04773382,  0.00920391, 0.03691193, -0.01137877, -0.02552283,  0.01239266,  0.00478121], can u tell me whether it is right or not? Thank you! @nttstar "
"Hey, I recently found an interesting 3D alignment repo in github, it's base on [""Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network""](https://arxiv.org/abs/1803.07835).
The repo's name is : [PRNet]( https://github.com/YadiraF/PRNet) .
I had tried it, and it's really fast. I hope it can helps you guys to build the 3D recognition.
Close it when you see it, thank you."
"There is one command line in README.md as below:

------------------------------
(4). Fine-turn the above Softmax model with Triplet loss.
        CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network m1 --loss-type 12 --lr 0.005 --mom 0.0 --per-batch-size 150 --data-dir ../datasets/faces_ms1m_112x112 --pretrained ../model-m1-softmax,50 --prefix ../model-m1-triplet
------------------------------

I cannot find triplet loss definition in src/train_softmax.py but find ""elif args.loss_type==12: #triplet loss"" in src/train.py. Should the python file be train.py instead of train_softmax.py in README.md for triplet loss?"
"使用自己的数据集fine-tuning模型，mxnet使用官网提供的im2rec.py只能生成.lst和.rec,.idx文件，无property文件。
但是执行README.md给出的命令：
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network m1 --loss-type 12 --lr 0.005 --mom 0.0 --per-batch-size 150 --data-dir ../datasets/faces_ms1m_112x112 --pretrained ../model-m1-softmax,50 --prefix ../model-m1-triplet

提示[Errno 2]No such file or directory:""./property""，不知道这个property文件如何生成，还请回复，多谢。
"
"@nttstar Sorry to bother you. I guess there must exist something I have missed. Could you help me?


In your implementations, we need to train for 10000 epoches(line 93 of train_softmax.py). 

`
parser.add_argument('--end-epoch', type=int, default=100000, help='training epoch size.')
`


But it seems that you mentioned that you train for 20w iterations rather than epoches in your paper !

Training for 10w epoches over 2000w images....... Crazy!!!!!!!

I have trained your methods with 4XP100 GPUs for 7 days and only finished 46 epoches   ......................

Each epoch takes about 10000 iterations as I use smaller batch size=64 on each GPU.


So 10w / 50 = 2000 x 7 days ..... 140000 days????


Here I provide some training log,


```
INFO:root:Epoch[46] Batch [4360]        Speed: 61.02 samples/sec        acc=0.925391
INFO:root:Epoch[46] Batch [4380]        Speed: 317.33 samples/sec       acc=0.923633
INFO:root:Epoch[46] Batch [4400]        Speed: 317.54 samples/sec       acc=0.924609
INFO:root:Epoch[46] Batch [4420]        Speed: 317.96 samples/sec       acc=0.922656
INFO:root:Epoch[46] Batch [4440]        Speed: 316.52 samples/sec       acc=0.928906
INFO:root:Epoch[46] Batch [4460]        Speed: 318.25 samples/sec       acc=0.923633
INFO:root:Epoch[46] Batch [4480]        Speed: 317.42 samples/sec       acc=0.924023
INFO:root:Epoch[46] Batch [4500]        Speed: 316.43 samples/sec       acc=0.922656
INFO:root:Epoch[46] Batch [4520]        Speed: 316.35 samples/sec       acc=0.924219
INFO:root:Epoch[46] Batch [4540]        Speed: 317.87 samples/sec       acc=0.927734
INFO:root:Epoch[46] Batch [4560]        Speed: 319.68 samples/sec       acc=0.931055
INFO:root:Epoch[46] Batch [4580]        Speed: 315.75 samples/sec       acc=0.926953
INFO:root:Epoch[46] Batch [4600]        Speed: 318.45 samples/sec       acc=0.923242
```


"
"Hi
I use the pretrained model(resnet-34),Continue to train on the resnet34 model, why the initial accuracy is still 0"
"Hi ，
I train a resnet-18 network in a single GPU,but the lfw accuracy is always about 97% ,the batch_size is 128, 
what should I do next ?,thank you"
"How can I get ""spherefacei-s60-p0_15_96_112_0-symbol.json"" in gen_megaface.py?
`
parser.add_argument('--model', type=str, help='', default='../model/spherefacei-s60-p0_15_96_112_0,95')
`
run any script or download it somewhere?"
I am asking this because I hope that the method can be used to clean other dataset. 
"Hi,
I'm trying to replicate 99,7% accuracy of r50 model against original LFW images (not using pre-processed aligned face chips encoded into a binary file).

What I found is the resulting accuracy hardly depends on alignment. 
I used default MTCNN face and facial landmarks detector (MtcnnDetector.detect_face()) with default parameters (minsize=20, threshold=[0.6,0.7,0.8], factor=0.709) instead of mentioned within demo scripts (MtcnnDetector.detect_face_limited()). This way I've reached accuracy 97,5%.
Next I used pre-detected lfw_landmarks obtained from SphereFace project (https://github.com/clcarwin/sphereface_pytorch/tree/master/data) without running MTCNN detector. And this allowed me to reach accuracy 99,7%. 

So, I guess, you've used also pre-detected landmarks.

The question is how have you obtained those landmarks? Which MTCNN detector have you used (there are many implementations of it)? And which detector parameters have you used?

I would really like to replicate reported accuracy on LFW without using any pre-detections but running the whole pipeline.

Thanks"
"I can just get 93% training acc after 90 epochs when train resnet50 with arc loss，could you tell me your final training acc while get 98% in megaface test？
Thank U
@nttstar "
"what is the property file in dataset? I just have .idx and .rec files, how to create property file? Thanks."
"Due to hardware resource limitations，I want to train the resnet-18 ,Can I achieve the same result？"
I saw that in your code but have no idea what it is? Can you explain?
"Sorry, I don't understand threshold theta angle ( theta angle < pi - m)
i think threshold (theta angle < pi - m/2)
thank you for answer!"
"Hi, It is a great work! And thank u for releasing this amazing project.
I want to use some data augmentation tricks to train my data, like brightness or contract augmentation, I know how to do it in mx.io.ImageRecIter(), But Could you tell me How to do it in FaceImageIter?"
Can you provide other trained models? 
""
" I use PYTHON 3.6.3 ,when i run your code,i get the error:   File ""eval/verification.py"", line 186, in load_bin     bins, issame_list = pickle.load(open(path, 'rb')) UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128)
I have tried many method ,but can't solve it.....should i change the python version to 2.7 or is there any other method,,,,please help"
"I would like to know what kind of problems this loss function can also solve, in addition to face recognition.
Thanks."
"According to the numbers in the paper, it seems that models trained on VGG2 have higher accuracy on CFP-FP than models trained on MS1M, while the relation is reversed for AgeDB-30. For example:
Table 5, SE-LResNet50E-IR, softmax@VGG2, using WD=5e-4, 
Accuracy on CFP-FP=96.82%, accuracy on AgeDB-30=93.83%

Table 7, LResNet100E-IR@MS1M, ArcFace m=0.5:
Accuracy on CFP-FP=94.04, accuracy on AgeDB-30=98.08%.

Does this mean that there is something alike between VGG2 and CFP-FP, such that a model trained on VGG2 already knows something about CFP-FP and performs better than that trained on MS1M? What characteristics of VGG2 may cause this phenomenon? Identity overlap or that VGG2 contains much more varied pose than MS1M?

"
"First of all, thanks for the great job.

After reading the fresnet.py, I see that the difference between residual_unit_v1_L(for 112x112) and residual_unit_v1(for 224x224) is only in the stride in conv1 or conv3. So I am a little curious about the difference. Will this setting make a big difference?"
"I run the project in a centos7 environment, without GPU, and the Runtime error: simple_bind error.Arguments.bugs. data: 1, 3, 112, 112). Do I need to cut the size ahead of time when loading the custom image?.???"
""
"in facenet，author had done experiment proved that embedding size setting to 128 is the best size，and bigger embedding size will lead to model size bigger，bigger size model mean that latency, RAM ,etc. all will be increase. so, why insightface embedding size is 512, and why don't use 128 or smaller？look forward to you guide, thanks."
"<img width=""943"" alt=""wx20180330-110802 2x"" src=""https://user-images.githubusercontent.com/9246255/38122937-dadd728e-340a-11e8-871c-84e356edb076.png"">
I'm confused about line 206 in the screenshot: variable zy is already cos_t, why it still needed divide by a feature scale value?"
"I was training on one computer yesterday, and everything goes fine: 121 samples/sec for 1 gpu, and 242 samples/sec for 2 gpus. Today I want to use another computer, so I directly moved the hard disk on which the OS is installed to the other computer, and then the training speed stucked at 100 samples/sec no matter how many gpus used or how the MXNET_CPU_WORKER_NTHREADS is set. Then I moved the hard disk back to the first computer, and it still stucked at 100 samples/sec! So now I badly need help and advice so that I would not have to reinstall the operating system. p.s. I have tried reinstalled mxnet several times, but it did not work.

Following are my computers:
computer 1:
CPU: E5-1650v3 (6 cores 3.5 GHz)
GPU: Titan X (Maxwell) * 2
Memory: 64 GB DDR4-2133
computer 2:
CPU: E5-2683v3*2 (14 cores 2.0GHz)
GPU: Titan X (Pascal) * 4
Memory: 256 GB DDR4-2133

OS: Ubuntu 16.04
MXNet: mxnet-cu80 20180326


See from the system monitor, there is only 1 python processing running, although I have set MXNET_CPU_WORKER_NTHREADS to 24. The gpu utilization fluctuates between 0% and 99%, and the cpu usage is no higher than 5%.

I also run image net training on the same computer, using the gluon interface of mxnet, and set the number of workers in data loader to 8. Then I can see 9 (1 master + 8 workers) python processes running from the system monitor, and the gpu utilization is high all the time.

So I guess somehow the insightface training failed to open MXNET_CPU_WORKER_NTHREADS workers for processing the data, which limits the overall throughput. Then the question is how I can fix this problem except reinstalling the operating system? I have tried reinstalling mxnet several times, but it does not work.


"
"when I use the pretrained caffe-r34-amf model to test megaface datasets,the Rank 1: 0.232038

Probe score matrix size: 3530 3530
distractor score matrix size: 3530 1000000
Done loading. Time to compute some stats!
Finding top distractors!
Done sorting distractor scores
Making gallery!
Done Making Gallery!
Allocating ranks (1000080)
Rank 1: 0.232038

the code of get feature is:

def do_flip(data):
  for idx in xrange(data.shape[0]):
    data[idx,:,:] = np.fliplr(data[idx,:,:])

class Resnet_caffe(object):
    def __init__(self,deploy_net=deploy_net,caffe_model=caffe_model):
        caffe.set_device(0)
        caffe.set_mode_gpu()
        self.net = caffe.Net(deploy_net,caffe_model,caffe.TEST)
    
    def get_feature(self,img):
        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        img = cv2.resize(img,(112,112))
        #img = img/127.5 -1
        img = np.transpose(img,(2,0,1))

        embedding = None
        for flipid in [0,1]:
            _img = np.copy(img)
            if flipid==1:
                do_flip(_img)
            #self.net.blobs['data'].data[0,...] = _img  #np.expand_dims(_img,axis=0)
            self.net.blobs['data'].data[...] = np.expand_dims(_img,axis=0)
            out = self.net.forward()
            _embedding = self.net.blobs['fc1'].data[0].flatten()
            if embedding is None:
                embedding = _embedding
            else:
                embedding +=_embedding
        _norm = np.linalg.norm(embedding)
        embedding /= _norm

        return embedding

I don't know why? can you give me some advice?"
"Would you release the final Megaface test set  as a .rec file and .lst file？
Thank U
 
@nttstar "
"The gradient is blocked from back propagating to the input embedding, is there any specific reason for doing this?
Thanks a lot."
"請問我看您算accuracy時使用的是看dist來判斷正確性
但這個dist實際上用的是L2norm而不是角度距離

您在test.py則會使用2個距離判斷，
一個是dist, 一個則是sim
請問這是為什麼呢?


"
"Hi@nttstar, is there any code sample on how to extract images in train.rec? thank you "
"In your paper, your said:

> We manually clean the FaceScrub dataset and finally find 605 noisy face images. During testing, we change the noisy face to another right face, which can increase the identification accuracy by about 1%. 

So I'd like to know if your  83.27% identity result on megaface is tested using the cleaned facescrub list  or official list. Because in my test on megaface using official list, I got only 79.65% identity accuracy. Thank you! @nttstar "
"Thanks your for the remarkable work! 
Would you tell me in the procedure of trainning the MS1M dataset, did you use any technical to dual the data imblance problem? or  did you use any data augmentation technical besides rand mirror? Thanks you very much!"
"我调用抽特征函数，用两张相同的照片同时在一次测试中，发现“_embedding = self.model.get_outputs()[0].asnumpy()“第一次比较耗时，大约用1s,第二次只用了0.01s,请问是这是什么原因？？"
"I realize that most of the works just use 5 points  to adjust the roll, not pitch and yaw. Could you tell
me what methods you are going to use to achieve the full pose alignment(adjust roll yaw and pitch the same time)? And is it better than just adjust the roll?
Thank U

@nttstar "
"Hello,
I have some questions about the MS1M clean list (https://pan.baidu.com/s/1eTn6O62):
1. The image number in the list is 3.5M, however, the number stated in ArcFace paper is 3.8M. Why are the numbers different?
2. Could you please explain the naming rules for each item in the list. I'm particularly confused on some long-name items, e.g. line 452-475 as below, 

./m.09q1kt/1U1513P28T3D1331193F328DT200611188655.jpg
./m.09q1kt/1U2519P28T3D2249286F329DT20081113220204.jpg
./m.09q1kt/1U2519P28T3D2249286F329DT200811188766.jpg
./m.09q1kt/1U3088P28T3D3230987F358DT201102188800.jpg
./m.09q1kt/1U765P8T1D210847F62DT20051128144648.jpg
./m.09q1kt/1xin_5520403131049966203664.jpg

Thank you for your help!"
I have a gtx1080ti (11G VRAM) and a titan xp(12G VRAM). But I see the minimun requirements of VRAM is 18G. Is there any solution to train it with some smaller VRAM devices?
"@nttstar  I train the MobilNet,But don't got the best reuslt like your paper.
the params is:
**gpu num: 4**
**num_layers 1**
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=**512**, beta=1000.0, beta_freeze=0, beta_min=5.0, ckpt=1, ctx_num=4, cutoff=0, data_dir='insightface/datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, **loss_type=4**, lr=0.1, lr_steps='', margin=4, margin_a=1.0, margin_b=0.0, **margin_m=0.5,** margin_s=64.0, max_steps=0, mom=0.9, network='m1', num_classes=85164, num_layers=1, per_batch_size=128, power=1.0, prefix='../model-m1-insight/model', pretrained='', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init mobilenet 1
**(1, 'E', 3)**

  header = header._replace(label=np.fromstring(s, np.float32, header.flag))
header0 label [3804847. 3890011.]
id2range 85164
3804846
rand_mirror 1

the best result:
lr-batch-epoch: 0.0001 4156 53
testing verification..
(12000, 512)
infer time 18.578805
[lfw][398000]XNorm: 22.040030
[lfw][398000]Accuracy-Flip: **0.99433+-0.00281**
testing verification..
(14000, 512)
infer time 19.498474
[cfp_fp][398000]XNorm: 18.207268
[cfp_fp][398000]Accuracy-Flip: 0.89029+-0.01814
testing verification..
(12000, 512)
infer time 16.765434
[agedb_30][398000]XNorm: 22.209527
[agedb_30][398000]Accuracy-Flip: 0.96267+-0.00889

can you give me some advise?"
"Here I see that we can call the mx.sym.LSoftmax() directly, I am wondering how to build this API as I see that you implement it  with C++/CU(it seems that you refer other's implementation).

```
fc7 = mx.sym.LSoftmax(data=embedding, label=gt_label, num_hidden=args.num_classes,
                              weight = _weight,
                              beta=args.beta, margin=args.margin, scale=args.scale,
                              beta_min=args.beta_min, verbose=1000, name='fc7')
```"
"I use face2rec2.py to generate the .rec and .idx files, but it seems like something goes wrong, the count does not match the sample number 37884,
time: 0.115676879883  count: 30000
time: 0.114284038544  count: 31000
time: 0.133539915085  count: 32000
time: 0.113406181335  count: 33000
time: 0.107145786285  count: 34000
time: 0.0922582149506  count: 35000
time: 0.0939009189606  count: 36000
time: 0.0914330482483  count: 37000
time: 0.0862410068512  count: 38000
time: 0.0344109535217  count: 39000
time: 0.0345239639282  count: 40000
time: 0.0348250865936  count: 41000
time: 0.0341598987579  count: 42000
time: 0.0342829227448  count: 43000
time: 0.0341610908508  count: 44000
my lst file is:
1       /home1/yx/insightface/datasets/20170530/photos/2017-05-30-0f8c29b0/cf163390.jpg 0
1       /home1/yx/insightface/datasets/20170530/photos/2017-05-30-0f8c29b0/45fd951a.jpg 0
1       /home1/yx/insightface/datasets/20170530/photos/2017-05-30-0f8c29b0/77b35b16.jpg 0
1       /home1/yx/insightface/datasets/20170530/photos/2017-05-30-0f8c29b0/4a58cdd2.jpg 0
1       /home1/yx/insightface/datasets/20170530/photos/2017-05-30-0f8c29b0/96fc4909.jpg 0
------------------------------------------------------------------------------------------------------------
when i use the generated dateset to train, a error occur:
Traceback (most recent call last):
  File ""train_softmax.py"", line 1039, in <module>
    main()
  File ""train_softmax.py"", line 1036, in main
    train_net(args)
  File ""train_softmax.py"", line 1030, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home1/yx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 482, in fit
    next_data_batch = next(data_iter)
  File ""/home1/yx/insightface/src/data.py"", line 980, in next
    ret = self.cur_iter.next()
  File ""/home1/yx/insightface/src/data.py"", line 830, in next
    self.reset()
  File ""/home1/yx/insightface/src/data.py"", line 704, in reset
    self.triplet_reset()
  File ""/home1/yx/insightface/src/data.py"", line 553, in triplet_reset
    self.select_triplets()
  File ""/home1/yx/insightface/src/data.py"", line 506, in select_triplets
    label[i-ba][:] = header.label
  File ""/home1/yx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 437, in __setitem__
    self._set_nd_basic_indexing(key, value)
  File ""/home1/yx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/mxnet/ndarray/ndarray.py"", line 697, in _set_nd_basic_indexing
    value = np.broadcast_to(value, shape)
  File ""/home1/yx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/numpy/lib/stride_tricks.py"", line 173, in broadcast_to
    return _broadcast_to(array, shape, subok=subok, readonly=True)
  File ""/home1/yx/anaconda2/envs/tensorflow/lib/python2.7/site-packages/numpy/lib/stride_tricks.py"", line 128, in _broadcast_to
    op_flags=[op_flag], itershape=shape, order='C').itviews[0]
ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (2,) and requested shape (1,)
"
"hello,I tested in Densenet with InsightFace loss,but i got the acc 50% all the time.Besides, I also tested in DPN with InsightFace loss ,i got the the highest acc 74% in the same epoch as Resnet,but resnet acc is 96%.
Can you help me solve this problem or share some training trick? Thank you !"
"Initially, I meet the issue of out of memory issue on TITAN X 12GB, so I change per GPU batch size from 128 to 64, so the batch_size is 64*4=256. However, the training speed is only **26 examples/sec**. The version of MXNet is 1.2.0

So, I adopt the suggestions (https://github.com/deepinsight/insightface/compare/master...gaohuazuo:tested) from @gaohuazuo (https://github.com/deepinsight/insightface/issues/32) for out of memory issue. In his comments, he tested on 1080Ti x4, mxnet-cu80, r100, per GPU batch size 128. Memory 8.3G, **speed 308 examples/sec**.

But I followed the operations he suggested, the training speed is still very low on my server, it is only **28 examples/sec**. I test on P100x4 with each 16 GB, mxnet-cu80, r100, loss_type=4, per GPU batch size 128, Memory 8.3G (I also try the setting with per GPU batch size 192, Memory 10.3G, also very low only **32 examples/sec**). 

Moreover, If I do not use memonger, P100x4 with each 16 GB, mxnet-cu80, r100, loss_type=4, per GPU batch size 128, the training speed is almost the same as **30 examples/sec**. 

Do you know how to fix the issue of speed ?  
"
"@nttstar 
1. I am new to Face Recognition and I have a limited hardware resources.
Could you approximately tell us the minimum GPU mem required to train `LResNet100-E-IR` on setting `MS1M@ArcFace`? 
For batch_size of 1 and 128 would be perfect.
I currently have 2 GTX 1080s

2. Could you tell us the approximate performance if I were to train `MS1M@Triplet` because in case my resources do not suffice  for Softmax/ArcFace Traing I will go with `MS1M@Triplet`

Thank you very much!"
@nttstar Such as Pamela Anderson_2367 in facescrub_uncropped_features_list.json
"I generate the .rec file use the face2rec2.py, but when i use the generated .rec file to train, it raise an error:

Traceback (most recent call last):
  File ""train_softmax.py"", line 1104, in <module>
    main()
  File ""train_softmax.py"", line 1100, in main
    train_net(args)
  File ""train_softmax.py"", line 1093, in train_net
    epoch_end_callback=checkpoint)
  File ""/home/xiaomin.wu/anaconda2/lib/python2.7/site-packages/mxnet-1.0.0-py2.7.egg/mxnet/module/base_module.py"", line 482, in fit
    next_data_batch = next(data_iter)
  File ""/autofs/data56/xiaomin.wu/code/bitmain/insightface/src/data.py"", line 824, in next
    label, s, bbox, landmark = self.next_sample()
  File ""/autofs/data56/xiaomin.wu/code/bitmain/insightface/src/data.py"", line 750, in next_sample
    header, img = recordio.unpack(s)
  File ""/home/xiaomin.wu/anaconda2/lib/python2.7/site-packages/mxnet-1.0.0-py2.7.egg/mxnet/recordio.py"", line 370, in unpack
    header = header._replace(label=np.frombuffer(s, np.float32, header.flag))
ValueError: buffer is smaller than requested size
"
"Hello. I want to try this module on pytorch. And I want use cropped images for train network. What kind of type train.rec? Maybe you know, how can I use it on pytorch?"
"当我使用test.py文件的的时候，我发现 mtcnn 只使用了detect_face_limited  函数，而没有用detect_face函数，导致lfw有些图片检测不到人脸，之后我把threshold[2]=0.2改为0（按照sphereface论文测试lfw的方法）将lfw原图输入程序进来测试，准确率只有80%。
请问问题是出在mtcnn的方法选择上么？但是readme写的就是只用mtcnn的R+O部分网络就ok了呀？求解"
"Hi, thanks for the amazing work.
I am going through the paper to understand more.
In the above section you removed 578 identities from MS1M dataset because those identities had a close distance with the identities from FaceScrub.

My question is ""Are those removed identities occur in both MS1M and FaceScrub dataset or you remove them only because they have close distance(smaller than 0.45)""

Once more thank you for the work!"
"Great work. But it would be better if the author could provide more notes about the meaning of the paramters.

For example, ""--easy-margin"" ""output-c2c""?"
"Great work, and I try to use the proposed loss to train imagenet 

> loss-type=4: Ours(InsightFace)

However,  it seems that the loss is decreasing but the accuracy is always zero?

Could anyone tell me the possible reasons?

"
"I try to try the loss functions on other datasets and test the basic softmax loss functions, but it tells me that 👍 

> Shape of labels does not match shape of predictions


I will update more details latter.
"
"how should I solve it?
gpu num: 4
num_layers 50
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=4, beta=1000.0, beta_freeze=0, beta_min=5.0, ckpt=1, ctx_num=4, cutoff=0, data_dir='datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, max_steps=0, mom=0.9, network='r50', num_classes=85164, num_layers=50, per_batch_size=1, power=1.0, prefix='models/model-r50', pretrained='', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 50
0 1 E 3 prelu
INFO:root:loading recordio datasets/faces_ms1m_112x112/train.rec...
header0 label [ 3804847.  3890011.]
id2range 85164
3804846
rand_mirror 1
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
lr_steps [12800000, 17920000, 20480000]
[10:02:31] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
call reset()
Traceback (most recent call last):
  File ""/home/liumeng/insightface-master/src/train_softmax.py"", line 464, in <module>
    main()
  File ""/home/liumeng/insightface-master/src/train_softmax.py"", line 461, in main
    train_net(args)
  File ""/home/liumeng/insightface-master/src/train_softmax.py"", line 455, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/liumeng/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/base_module.py"", line 488, in fit
    self.update()
  File ""/home/liumeng/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/module.py"", line 643, in update
    self._kvstore, self._exec_group.param_names)
  File ""/home/liumeng/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/model.py"", line 134, in _update_params_on_kvstore
    kvstore.push(name, grad_list, priority=-index)
  File ""/home/liumeng/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/kvstore.py"", line 224, in push
    self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))
  File ""/home/liumeng/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/base.py"", line 146, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [10:03:34] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory"
"I just want to use the LSoftmax Loss in my own code. And I have saw you write the *.h, *.cc and *.cu file. But I do not know how to use it? I have saw the answers in other issues like this:
![image](https://user-images.githubusercontent.com/27360803/37563276-f58bf6d4-2ab7-11e8-9099-1d3a782b6255.png)
But I still do not know how to use it. Can you explain it a little clear? Thank you so much."
"As the paper arcface said, to use the model, I need to normilize the image, however, when I normlized images and then use the pretrain model LResNet50E-IR, the result is very poor, and when I ignored this step, and use the raw image directly, the result becomes better."
"CUDA_VISIBLE_DEVICES='0,1' python2.7 -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112  --prefix ../model-r100
gpu num: 2
num_layers 100
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=256, beta=1000.0, beta_freeze=0, beta_min=5.0, ckpt=1, ctx_num=2, cutoff=0, data_dir='../datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_a=1.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, max_steps=0, mom=0.9, network='r100', num_classes=85164, num_layers=100, per_batch_size=128, power=1.0, prefix='../model-r100', pretrained='', rand_mirror=1, rescale_threshold=0, scale=0.9993, target='lfw,cfp_fp,agedb_30', use_deformable=0, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 100
0 1 E 3 prelu
INFO:root:loading recordio ../datasets/faces_ms1m_112x112/train.rec...
/home/t/Documents/mxnet/python/mxnet/recordio.py:370: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  header = header._replace(label=np.fromstring(s, np.float32, header.flag))
header0 label [3804847. 3890011.]
id2range 85164
3804846
rand_mirror 1
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [200000, 280000, 320000]
Traceback (most recent call last):
  File ""train_softmax.py"", line 453, in <module>
    main()
  File ""train_softmax.py"", line 450, in main
    train_net(args)
  File ""train_softmax.py"", line 444, in train_net
    epoch_end_callback = epoch_cb )
  File ""/home/t/Documents/mxnet/python/mxnet/module/base_module.py"", line 460, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/home/t/Documents/mxnet/python/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/home/t/Documents/mxnet/python/mxnet/module/executor_group.py"", line 264, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/home/t/Documents/mxnet/python/mxnet/module/executor_group.py"", line 360, in bind_exec
    shared_group))
  File ""/home/t/Documents/mxnet/python/mxnet/module/executor_group.py"", line 638, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/home/t/Documents/mxnet/python/mxnet/symbol/symbol.py"", line 1515, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
[14:46:26] src/storage/storage.cc:118: Compile with USE_CUDA=1 to enable GPU usage

Stack trace returned 10 entries:

Thank you for your help!"
I have one machine which is running mxnetcu75 on old ubuntu. I found model.get_feature generate different feature value for the same face with the other machine running mxnetcu80. How can I get same result for mxnetcu75? Thanks.
"Dear @nttstar ,
   Would like to teach us how to figure gradients respect to w and feature embeding. This particular important to non-autograd framework like caffe. Thanks"
"Thanks for your cleaning list.

In MS1M dataset, each line has the following format:
MID,ImageSearchRank,ImageURL,PageURL,FaceID,FaceRectangle,FaceData
What is the naming conversion of your list?
For example:
m.0933t2\0.jpg
the directory name corresponds to MID, what about the file name? Which item does the file name correspond to?
Thanks a lot.
![qq 20180314162817](https://user-images.githubusercontent.com/10250053/37390971-c2356d30-27a4-11e8-8b7e-f2cc2d5b5896.png)


"
"


Hi,  after reading the code, I didn't understand why the specific value `zy_keep` is chosen, could you help share some inghts here? thanks in advance!
https://github.com/deepinsight/insightface/blob/c9e5188ae289bef0e1a010e8f6846402d66be6c7/src/train_softmax.py#L227

```python
mm = math.sin(math.pi-m)*m
...
zy_keep = zy - s*mm
```"
"ver cfp_fp
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [400000, 560000, 640000]
[15:20:42] src/operator/././cudnn_algoreg-inl.h:112: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[15:20:43] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [15:20:43] src/storage/./pooled_storage_manager.h:102: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7fe6be4d37cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1242238) [0x7fe6bf540238]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1244c0a) [0x7fe6bf542c0a]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4d4db) [0x7fe6bf14b4db]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe549cd) [0x7fe6bf1529cd]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe59f95) [0x7fe6bf157f95]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe5d6ee) [0x7fe6bf15b6ee]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe5dcd4) [0x7fe6bf15bcd4]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBind+0x2261) [0x7fe6bf0da291]
[bt] (9) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7fe6e3522e40]

Traceback (most recent call last):
  File ""train_softmax.py"", line 453, in <module>
    main()
  File ""train_softmax.py"", line 450, in main
    train_net(args)
  File ""train_softmax.py"", line 444, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 460, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 417, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 231, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 327, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 603, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol.py"", line 1479, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
[15:20:43] src/storage/./pooled_storage_manager.h:102: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7fe6be4d37cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1242238) [0x7fe6bf540238]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1244c0a) [0x7fe6bf542c0a]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4d4db) [0x7fe6bf14b4db]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe549cd) [0x7fe6bf1529cd]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe59f95) [0x7fe6bf157f95]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe5d6ee) [0x7fe6bf15b6ee]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe5dcd4) [0x7fe6bf15bcd4]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBind+0x2261) [0x7fe6bf0da291]
[bt] (9) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7fe6e3522e40]
"
"Thanks for sharing!
The link provides MS1M clean list on https://pan.baidu.com/s/1eTn6O62 is dead.
Can someone provide another link? Thanks!
"
" error info:
nvidia@tegra-ubuntu:~/insightface/deploy$ python test.py
loading ../models/model-r34-amf/model 0
[06:18:38] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.12.1. Attempting to upgrade...
[06:18:38] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
[06:18:42] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[06:18:42] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
/usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/model.py:927: DeprecationWarning: mxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.
  **kwargs)
[06:18:42] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[06:18:42] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
[06:18:42] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[06:18:42] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
[06:18:42] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.8.0. Attempting to upgrade...
[06:18:42] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""test.py"", line 22, in <module>
    dist = np.sum(np.square(f1-f2))
TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
"
"error info:

nvidia@tegra-ubuntu:~/insightface/deploy$ python test.py
loading ../models/model-r34-amf/model 0
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    model = face_embedding.FaceModel(args)
  File ""/home/nvidia/insightface/deploy/face_embedding.py"", line 47, in __init__
    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/model.py"", line 420, in load_checkpoint
    symbol = sym.load('%s-symbol.json' % prefix)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/symbol/symbol.py"", line 2526, in load
    check_call(_LIB.MXSymbolCreateFromFile(c_str(fname), ctypes.byref(handle)))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/base.py"", line 149, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [03:59:35] src/io/local_filesys.cc:166: Check failed: allow_null  LocalFileSystem::Open ""../models/model-r34-amf/model-symbol.json"": No such file or directory

Stack trace returned 7 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/libmxnet.so(dmlc::StackTrace[abi:cxx11]()+0x58) [0x7f6bb6e0d0]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/libmxnet.so(dmlc::io::LocalFileSystem::Open(dmlc::io::URI const&, char const*, bool)+0x500) [0x7f6dc56e40]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/libmxnet.so(dmlc::Stream::Create(char const*, char const*, bool)+0x144) [0x7f6dc4ba84]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-1.2.0-py2.7.egg/mxnet/libmxnet.so(MXSymbolCreateFromFile+0x74) [0x7f6db7d3a4]
[bt] (4) /usr/lib/aarch64-linux-gnu/libffi.so.6(ffi_call_SYSV+0x64) [0x7fa039be60]
[bt] (5) /usr/lib/aarch64-linux-gnu/libffi.so.6(ffi_call+0xc0) [0x7fa039c7b8]
[bt] (6) /usr/lib/python2.7/lib-dynload/_ctypes.aarch64-linux-gnu.so(_ctypes_callproc+0x670) [0x7fa03ffb30]
"
"Have someone know how to do with this error?I am new to mxnet,thaks very much!! 

CUDA_VISIBLE_DEVICES='0,1,2' python -u train_softmax.py --network r34 --loss-type 4 --per-batch-size 64 --margin-m 0.35 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../result/model-r34-amsoftmax
gpu num: 3
num_layers 34
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=192, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_mode=-10, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.676161173096705, ctx_num=3, cutoff=0, data_dir='../datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, images_per_identity=0, incay=0.0, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_a=0.0, margin_b=0.0, margin_m=0.35, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.9, network='r34', noise_sgd=0.0, num_classes=85164, num_layers=34, output_c2c=0, patch='0_0_96_112_0', per_batch_size=64, power=1.0, prefix='../result/model-r34-amsoftmax', pretrained='', rand_mirror=1, rescale_threshold=0, retrain=False, scale=0.9993, target='lfw,cfp_fp,agedb_30', train_limit=0, triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 34
0 1 E 3
INFO:root:loading recordio ../datasets/faces_ms1m_112x112/train.rec...
/usr/local/lib/python2.7/dist-packages/mxnet/recordio.py:370: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  header = header._replace(label=np.fromstring(s, np.float32, header.flag))
header0 label [3804847. 3890011.]
id2range 85164
0 0 3804846
c2c_stat [0, 85164]
3804846
rand_mirror 1
(192,)
[11:37:36] src/engine/engine.cc:54: MXNet start using engine: ThreadedEnginePerDevice
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [266666, 373333, 426666]
[11:37:52] src/operator/././cudnn_algoreg-inl.h:112: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:466: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.333333333333 vs. 0.00520833333333). Is this intended?
  optimizer_params=optimizer_params)
call reset()
[11:38:00] src/kvstore/././comm.h:327: only 2 out of 6 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off
[11:38:00] src/kvstore/././comm.h:336: ...
[11:38:00] src/kvstore/././comm.h:336: ..v
[11:38:00] src/kvstore/././comm.h:336: .v.
[11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:58: too large launch parameter: MapReduceKeepDim1[85164,1], [256,1,1]

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20aea71) [0x7ff6de5faa71]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20b3007) [0x7ff6de5ff007]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xfa5304) [0x7ff6dd4f1304]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4aedf) [0x7ff6dd396edf]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe3451d) [0x7ff6dd38051d]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]

[11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [11:38:01] src/engine/./threaded_engine.h:347: [11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:58: too large launch parameter: MapReduceKeepDim1[85164,1], [256,1,1]

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20aea71) [0x7ff6de5faa71]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20b3007) [0x7ff6de5ff007]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xfa5304) [0x7ff6dd4f1304]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4aedf) [0x7ff6dd396edf]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe3451d) [0x7ff6dd38051d]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]

An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 8 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe347c4) [0x7ff6dd3807c4]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (5) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]
[bt] (6) /lib/x86_64-linux-gnu/libpthread.so.0(+0x8184) [0x7ff74c15d184]
[bt] (7) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7ff74be89ffd]

terminate called after throwing an instance of 'dmlc::Error'
  what():  [11:38:01] src/engine/./threaded_engine.h:347: [11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:58: too large launch parameter: MapReduceKeepDim1[85164,1], [256,1,1]

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20aea71) [0x7ff6de5faa71]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20b3007) [0x7ff6de5ff007]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xfa5304) [0x7ff6dd4f1304]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4aedf) [0x7ff6dd396edf]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe3451d) [0x7ff6dd38051d]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]

An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 8 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe347c4) [0x7ff6dd3807c4]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (5) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]
[bt] (6) /lib/x86_64-linux-gnu/libpthread.so.0(+0x8184) [0x7ff74c15d184]
[bt] (7) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7ff74be89ffd]

Aborted (core dumped)
libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]

[11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [11:38:01] src/engine/./threaded_engine.h:347: [11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:58: too large launch parameter: MapReduceKeepDim1[85164,1], [256,1,1]

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20aea71) [0x7ff6de5faa71]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20b3007) [0x7ff6de5ff007]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xfa5304) [0x7ff6dd4f1304]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4aedf) [0x7ff6dd396edf]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe3451d) [0x7ff6dd38051d]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]

An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 8 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe347c4) [0x7ff6dd3807c4]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (5) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]
[bt] (6) /lib/x86_64-linux-gnu/libpthread.so.0(+0x8184) [0x7ff74c15d184]
[bt] (7) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7ff74be89ffd]

terminate called after throwing an instance of 'dmlc::Error'
  what():  [11:38:01] src/engine/./threaded_engine.h:347: [11:38:01] /home/travis/build/dmlc/mxnet-distro/mxnet-build/mshadow/mshadow/././././cuda/tensor_gpu-inl.cuh:58: too large launch parameter: MapReduceKeepDim1[85164,1], [256,1,1]

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20aea71) [0x7ff6de5faa71]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x20b3007) [0x7ff6de5ff007]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xfa5304) [0x7ff6dd4f1304]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe4aedf) [0x7ff6dd396edf]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe3451d) [0x7ff6dd38051d]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (9) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]

An fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 8 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7ff6dc7217cc]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe347c4) [0x7ff6dd3807c4]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37373) [0x7ff6dd383373]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe37576) [0x7ff6dd383576]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0xe353eb) [0x7ff6dd3813eb]
[bt] (5) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60) [0x7ff6f7c01a60]
[bt] (6) /lib/x86_64-linux-gnu/libpthread.so.0(+0x8184) [0x7ff74c15d184]
[bt] (7) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7ff74be89ffd]

Aborted (core dumped)
"
"$ python test.py
loading ../models/model-r50-am-lfw/model 0
[09:47:29] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [09:47:29] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/././json.h:842: JSONReader: Unknown field attrs, candidates are:
""attr""
""backward_source_id""
""control_deps""
""inputs""
""name""
""op""
""param""

Does anyone have similar problems?  please give some solution，thanks

"
"Hi,
In your paper you report results on LFW when trained on VGGface2 in tables 2, 3, 4, 5 and 8, but I think there are quite a lot of overlapping identities between LFW and VGGface2 as both are based on celebrities. Have you checked for overlapping identities? Have you removed them when training? If not, it does not seem quite fair to report those numbers, or maybe it should be mentioned in the paper?

Also, when you say you are training with VGGface2, are you using the train and test set of VGGface2 (i.e. 8,631 +  500  identities) or just the train set of VGGface2 (8,631)?"
Why the filenames in MS1M clean list are inconsistent with the original dataset? How did you rename the images filenames?
"Dear all,

I have error as following.
It seems like load data successfully but occur error after training start...
can anyone help me?

# Here is my command:
nux/insightface/src$ CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112  --prefix ../model-r100


# Here is my log:
gpu num: 1
num_layers 100
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=128, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_mode=-10, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.676161173096705, ctx_num=1, cutoff=0, data_dir='../datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, images_per_identity=0, incay=0.0, logits_verbose=0, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_a=0.0, margin_b=0.0, margin_m=0.5, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.9, network='r100', noise_sgd=0.0, num_classes=85164, num_layers=100, output_c2c=0, patch='0_0_96_112_0', per_batch_size=128, power=1.0, prefix='../model-r100', pretrained='', rand_mirror=1, rescale_threshold=0, retrain=False, scale=0.9993, target='lfw,cfp_fp,agedb_30', train_limit=0, triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_act='prelu', version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 100
0 1 E 3 prelu
INFO:root:loading recordio ../datasets/faces_ms1m_112x112/train.rec...
/home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/recordio.py:370: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead
  header = header._replace(label=np.fromstring(s, np.float32, header.flag))
header0 label [3804847. 3890011.]
id2range 85164
0 0 3804846
c2c_stat [0, 85164]
3804846
rand_mirror 1
(128,)
[18:19:03] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [400000, 560000, 640000]
[18:20:46] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
/home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/module/base_module.py:466: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
  optimizer_params=optimizer_params)
call reset()
terminate called after throwing an instance of 'dmlc::Error'
  what():  [18:20:56] src/engine/./threaded_engine.h:359: [18:20:56] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2ab998) [0x7f6b6eecd998]
[bt] (1) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2abda8) [0x7f6b6eecdda8]
[bt] (2) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x29071ef) [0x7f6b715291ef]
[bt] (3) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x290a4c8) [0x7f6b7152c4c8]
[bt] (4) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x6c9c6f) [0x7f6b6f2ebc6f]
[bt] (5) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2459726) [0x7f6b7107b726]
[bt] (6) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x245f3c0) [0x7f6b710813c0]
[bt] (7) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x243fe0d) [0x7f6b71061e0d]
[bt] (8) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24446fb) [0x7f6b710666fb]
[bt] (9) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24448d6) [0x7f6b710668d6]


A fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 9 entries:
[bt] (0) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2ab998) [0x7f6b6eecd998]
[bt] (1) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2abda8) [0x7f6b6eecdda8]
[bt] (2) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24400b4) [0x7f6b710620b4]
[bt] (3) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24446fb) [0x7f6b710666fb]
[bt] (4) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24448d6) [0x7f6b710668d6]
[bt] (5) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24411ab) [0x7f6b710631ab]
[bt] (6) /home/jiewei/anaconda3/envs/mxnet/lib/python2.7/site-packages/scipy/sparse/../../../../libstdc++.so.6(+0xafc5c) [0x7f6b9ee69c5c]
[bt] (7) /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f6ba60ea6ba]
[bt] (8) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f6ba571041d]
"
"mxnet.base.MXNetError: [17:20:56] g:\deeplearn\mxnet\dmlc-core\include\dmlc\./json.h:842: JSONReader: Unknown field attrs, candidates are: 
""attr""
""backward_source_id""
""control_deps""
""inputs""
""name""
""op""
""param""
"
"您好，首先感谢您的工作！
初学者想请问一下1：N具体是体现在什么地方呢？
看了verification.py的源码，好像做的还是1:1的对比啊。
"
"错误日志：
> [root@flamingo deploy]# python test.py 
loading ../models/model-r50-am-lfw/model 0
[12:44:35] src/nnvm/legacy_json_util.cc:190: Loading symbol saved by previous version v0.12.1. Attempting to upgrade...
[12:44:35] src/nnvm/legacy_json_util.cc:198: Symbol successfully upgraded!
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    model = face_embedding.FaceModel(args)
  File ""/home/insightface/deploy/face_embedding.py"", line 56, in __init__
    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])
  File ""/usr/lib/python2.7/site-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/usr/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 264, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 360, in bind_exec
    shared_group))
  File ""/usr/lib/python2.7/site-packages/mxnet/module/executor_group.py"", line 638, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/lib/python2.7/site-packages/mxnet/symbol/symbol.py"", line 1518, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (1, 3, 112, 112)
[12:44:35] src/storage/storage.cc:118: Compile with USE_CUDA=1 to enable GPU usage

Stack trace returned 10 entries:
[bt] (0) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x192112) [0x7f542a311112]
[bt] (1) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x192738) [0x7f542a311738]
[bt] (2) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x27c165a) [0x7f542c94065a]
[bt] (3) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x27c6254) [0x7f542c945254]
[bt] (4) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x27c6727) [0x7f542c945727]
[bt] (5) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x232da3d) [0x7f542c4aca3d]
[bt] (6) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2337db4) [0x7f542c4b6db4]
[bt] (7) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2338d4c) [0x7f542c4b7d4c]
[bt] (8) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x233d5c8) [0x7f542c4bc5c8]
[bt] (9) /usr/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2347d1a) [0x7f542c4c6d1a]

请问这是什么原因"
""
"Do you pre-train the modified Res50/... backbones weights on imagenet and then fine-tune the entire model + customized loss on face dataset?
"
"[00:17:52] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
/home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/module/base_module.py:466: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.001953125). Is this intended?
  optimizer_params=optimizer_params)
call reset()
terminate called after throwing an instance of 'dmlc::Error'
  what():  [00:18:12] src/engine/./threaded_engine.h:359: [00:18:12] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2ab998) [0x7f4d17152998]
[bt] (1) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2abda8) [0x7f4d17152da8]
[bt] (2) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x29071ef) [0x7f4d197ae1ef]
[bt] (3) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x290a4c8) [0x7f4d197b14c8]
[bt] (4) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x6c9c6f) [0x7f4d17570c6f]
[bt] (5) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2459726) [0x7f4d19300726]
[bt] (6) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x245f3c0) [0x7f4d193063c0]
[bt] (7) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x243fe0d) [0x7f4d192e6e0d]
[bt] (8) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24446fb) [0x7f4d192eb6fb]
[bt] (9) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24448d6) [0x7f4d192eb8d6]


A fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 9 entries:
[bt] (0) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2ab998) [0x7f4d17152998]
[bt] (1) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x2abda8) [0x7f4d17152da8]
[bt] (2) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24400b4) [0x7f4d192e70b4]
[bt] (3) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24446fb) [0x7f4d192eb6fb]
[bt] (4) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24448d6) [0x7f4d192eb8d6]
[bt] (5) /home/ubuntu/miniconda2/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x24411ab) [0x7f4d192e81ab]
[bt] (6) /home/ubuntu/miniconda2/bin/../lib/libstdc++.so.6(+0xafc5c) [0x7f4d42feac5c]
[bt] (7) /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f4d994f76ba]
[bt] (8) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f4d98b1d41d]


Aborted (core dumped)

GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 0000:00:17.0     Off |                    0 |
| N/A   38C    P8    27W / 149W |      2MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K80           On   | 0000:00:18.0     Off |                    0 |
| N/A   35C    P8    29W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla K80           On   | 0000:00:19.0     Off |                    0 |
| N/A   39C    P8    26W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla K80           On   | 0000:00:1A.0     Off |                    0 |
| N/A   36C    P8    29W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla K80           On   | 0000:00:1B.0     Off |                    0 |
| N/A   39C    P8    26W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla K80           On   | 0000:00:1C.0     Off |                    0 |
| N/A   36C    P8    31W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla K80           On   | 0000:00:1D.0     Off |                    0 |
| N/A   42C    P8    25W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla K80           On   | 0000:00:1E.0     Off |                    0 |
| N/A   37C    P8    29W / 149W |      0MiB / 11439MiB |      0%      Default |
"
"为什么我将batch size改小了，然后CUDA_VISIBLE_DEVICES设为０去训练无法达到论文中的效果，
CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --network r50 --loss-type 2 --margin-m 0.35 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-r50-amsoftmax
我训练epoch为12时，lfw 准确率只有９７％　后面基本不会变了
谢谢"
Thanks in advance.
"Hi, thanks for the greate job. It's very powerful, and much more accurate than other repo I had used before such as dlib.
So I heard about that you may going to provide a 3D-alignment library later in #29 , I know there is a project that provide 3D-alignment, it's https://github.com/1adrianb/face-alignment, and it's using pytorch.
I don't know if it can help you, but I'm very looking forward to your 3D-alignment based recognition.
Close it when you read it,thanks."
"I use the Center Loss Function of yours. But when I run my code, some errors show like this:

[21:05:56] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [21:05:56] src/pass/gradient.cc:159: Check failed: (*rit)->inputs.size() == input_grads.size() (5 vs. 2) Gradient function not returning enough gradient

Stack trace returned 10 entries:
[bt] (0) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7f4206bef7cc]
[bt] (1) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d67afa) [0x7f4209781afa]
[bt] (2) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xf9aa1f) [0x7f42079b4a1f]
[bt] (3) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d58f78) [0x7f4209772f78]
[bt] (4) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe0fcae) [0x7f4207829cae]
[bt] (5) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe6bfe5) [0x7f4207885fe5]
[bt] (6) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5a8d8) [0x7f42078748d8]
[bt] (7) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5b28f) [0x7f420787528f]
[bt] (8) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5cfd0) [0x7f4207876fd0]
[bt] (9) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5dcd4) [0x7f4207877cd4]

/home/jiayunpei/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead
  import OpenSSL.SSL
=======train model========
Traceback (most recent call last):
  File ""/home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/symbol.py"", line 1473, in simple_bind
    ctypes.byref(exe_handle)))
  File ""/home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/base.py"", line 129, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [21:05:56] src/pass/gradient.cc:159: Check failed: (*rit)->inputs.size() == input_grads.size() (5 vs. 2) Gradient function not returning enough gradient

Stack trace returned 10 entries:
[bt] (0) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7f4206bef7cc]
[bt] (1) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d67afa) [0x7f4209781afa]
[bt] (2) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xf9aa1f) [0x7f42079b4a1f]
[bt] (3) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d58f78) [0x7f4209772f78]
[bt] (4) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe0fcae) [0x7f4207829cae]
[bt] (5) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe6bfe5) [0x7f4207885fe5]
[bt] (6) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5a8d8) [0x7f42078748d8]
[bt] (7) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5b28f) [0x7f420787528f]
[bt] (8) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5cfd0) [0x7f4207876fd0]
[bt] (9) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5dcd4) [0x7f4207877cd4]


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jiayunpei/mxnet_train/megaface_symbol/resnet_50/train.py"", line 17, in <module>
    softmax_label=(int(cfg.batch_size / len(cfg.gpus)),))
  File ""/home/jiayunpei/mxnet_train/megaface_symbol/resnet_50/memonger.py"", line 132, in search_plan
    cost = get_cost(sym, type_dict, **kwargs)
  File ""/home/jiayunpei/mxnet_train/megaface_symbol/resnet_50/memonger.py"", line 110, in get_cost
    **kwargs)
  File ""/home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/symbol.py"", line 1479, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (65, 3, 248, 248)
softmax_label: (65,)
[21:05:56] src/pass/gradient.cc:159: Check failed: (*rit)->inputs.size() == input_grads.size() (5 vs. 2) Gradient function not returning enough gradient

Stack trace returned 10 entries:
[bt] (0) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x1d57cc) [0x7f4206bef7cc]
[bt] (1) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d67afa) [0x7f4209781afa]
[bt] (2) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xf9aa1f) [0x7f42079b4a1f]
[bt] (3) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x2d58f78) [0x7f4209772f78]
[bt] (4) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe0fcae) [0x7f4207829cae]
[bt] (5) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe6bfe5) [0x7f4207885fe5]
[bt] (6) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5a8d8) [0x7f42078748d8]
[bt] (7) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5b28f) [0x7f420787528f]
[bt] (8) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5cfd0) [0x7f4207876fd0]
[bt] (9) /home/jiayunpei/anaconda3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xe5dcd4) [0x7f4207877cd4]

Do you know why is that?
Thanks for you replying."
"Appreciate for you goog work!  first I train on ms1m dataset until acc is 90% on train dataset.  then I  wan to finetune on my own data. I have make the *.rec and *.idx.  but when I begun to finetune, the program load the pr-trained model, and crash. warning that the number of my dataset classes is not equeal to the ms1m.  it seems we should freezen the FC layer.

is your code offer the function of finetuneing? "
"I have this question, can you help me? thank you very much!

Traceback (most recent call last):
  File ""train_softmax.py"", line 1033, in <module>
    main()
  File ""train_softmax.py"", line 1030, in main
    train_net(args)
  File ""train_softmax.py"", line 1009, in train_net
    model.fit(train_dataiter,
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 460, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 429, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 264, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 360, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 638, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 1518, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
[17:54:55] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: unknown error

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x2ab998) [0x7fd3e2f9d998]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x2abda8) [0x7fd3e2f9dda8]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x29071ef) [0x7fd3e55f91ef]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x290a4c8) [0x7fd3e55fc4c8]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x2466d4d) [0x7fd3e5158d4d]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x24710c4) [0x7fd3e51630c4]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x247205c) [0x7fd3e516405c]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x24768f8) [0x7fd3e51688f8]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x248103a) [0x7fd3e517303a]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x2481734) [0x7fd3e5173734]

"
"Hi, I use ms1m dataset to train r100 model.
During training, the accuracy on lfw is 0.998 as follow:
testing verification..
(12000, 512)
infer time 62.720226
[lfw][308000]XNorm: 22.075580
[lfw][308000]Accuracy-Flip: 0.99800+-0.00245
testing verification..
(14000, 512)
infer time 76.100626
[cfp_ff][308000]XNorm: 20.756458
[cfp_ff][308000]Accuracy-Flip: 0.99800+-0.00214
testing verification..
(14000, 512)
infer time 67.842646
[cfp_fp][308000]XNorm: 20.607653
[cfp_fp][308000]Accuracy-Flip: 0.93457+-0.01542
testing verification..
(12000, 512)
infer time 68.593957
[agedb_30][308000]XNorm: 22.874611
[agedb_30][308000]Accuracy-Flip: 0.97700+-0.00756
saving 77
INFO:root:Saved checkpoint to ""../model-r100-0077.params""


After saving the model-r100-0077.params, I use do_ver.sh to test the model on lfw, but only get 0.99733 on lfw:
python -u verification.py --gpu 1 --data-dir ../../datasets/faces_ms1m_112x112  --model '../../model-r100-softmax1e4/model-r100,77' --target lfw --batch-size 64
image_size [112, 112]
model number 1
loading ../../model-r100-softmax1e4/model-r100 77
[16:38:31] src/nnvm/legacy_json_util.cc:204: Warning: loading symbol saved by MXNet version 10100 with lower version of MXNet v10000. May cause undefined behavior. Please update MXNet if you encounter any issue
[16:38:41] src/operator/././cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
model loading time 13.996371
loading..  lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
testing verification..
(12000, 512)
infer time 207.145642
[lfw]XNorm: 22.055374
[lfw]Accuracy: 0.00000+-0.00000
[lfw]Accuracy-Flip: 0.99733+-0.00291
Max of [lfw] is 0.99733
"
Could you add some description about the parameter in  `train_softmax.py` file. The parameters defined in `parser.add_argument` have no description.
"Hi,
I've been trying to use the deploy/test.py script on two easy examples from LFW, but encountered several problems. 

First, the predefined threshold does not pass any of the images. Now, even when reducing the threshold the cosine similarity is really low (~0.1).

Looking at the images fed into the network 
it seems like there is a problem with the MTCNN alignment, the images are badly cropped, and look different from the images inside the LFW verification bin.


Any ideas? "
"Hi! Congratulations, you have done a great work. I have some questions about the remove_noises.py.
![image](https://user-images.githubusercontent.com/27360803/37131526-ac34f1f8-22c3-11e8-8d5c-46fabe404435.png)
In this code, there are 'a' and 'b', because I can not see the '/raid5data/dplearn/megaface/facescrubr/small_lst' file, so what's the meaning of 'a' and 'b'?
Looking forward to you reply"
I want to know the meaning of margin_verbose
"Hi, InsightFace is an awesome work and I want to use the codes to run my own data. So I use face2rec2.py to generate the .rec and .idx files, but it seems like something goes wrong, In my image lst file,there are around 12500000 samples, But when I use face2rec2.py to generate .rec and .idx files. It seems like the samples has been doubled:

time: 0.0756740570068  count: 25365000
time: 0.0772049427032  count: 25366000
time: 0.0759220123291  count: 25367000
time: 0.0757720470428  count: 25368000
time: 0.0775549411774  count: 25369000
time: 0.0764980316162  count: 25370000
time: 0.0752458572388  count: 25371000
time: 0.0749001502991  count: 25372000
time: 0.0766618251801  count: 25373000
time: 0.0772252082825  count: 25374000
time: 0.0776269435883  count: 25375000
time: 0.0763609409332  count: 25376000
time: 0.0765709877014  count: 25377000
time: 0.0757031440735  count: 25378000
time: 0.0767087936401  count: 25379000
time: 0.0764360427856  count: 25380000
wuxiaomin@taas5:~/code/SeetaBrushRanking/insightface/src/data$ ls -l -h



"
"I find lots of  images in `facescrub` datasets can't open correctly. The image have error, and can't open by software, like `Brianna Brown_10391.jpg`. The `facescrub_images_112x112.tar` dataset doesn't contain these images. Can you share the code that used to clean the error images?"
"Hi i want to ask you about some image preprocessing you've done, in the code you do channel swapping and no normalization, however in the arc face paper, it is mentioned that the RGB value is subtracted first with 127.5 and then divided by 128, will this affect the performance? Which one is the correct ones? thank you very much"
"When i want to merge the Webface dataset to Ms1m. rec file using dataset_merge.py, I need to generate the .lst file.However, The logic of organizing .lst is special, can you share the function to generate the .lst file from the data root? Thanks a lot"
"Hi all, I have tested on some datasets, and found some interesting characteristics/rules:
1.dataset1, 100 different people with 1 image for each person. the accuracy is high
2.dataset2, about 20 pairs of images, each person with 2 images, then compare image pairs of the same person, e.g compare A-1.jpg and A-2.jpg, which belong the same person, then the result is really bad.

all the images captured in the real environment by IP cameras."
"Dear author, I used the default configuration hope to reproduce the resutls, the acc alwyas show 0 after 2000 Batch. Is there anything wrong?
I only have 2 GPUs, so I changed the visible device to 0, 1
CUDA_VISIBLE_DEVICES='0,1' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112  --prefix ../model-r100

Logfile


lyh@lyh-dell:~/workspace/insightface/src$ MXNET_ENABLE_GPU_P2P=0 CUDA_VISIBLE_DEVICES='0,1' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir /data/faces_ms1m_112x112  --prefix ../model-r100
/home/lyh/anaconda2/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead
  import OpenSSL.SSL
gpu num: 2
num_layers 100
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=120, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_mode=-10, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.676161173096705, ctx_num=2, cutoff=0, data_dir='/data/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, images_per_identity=0, incay=0.0, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_a=0.0, margin_m=0.5, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.9, network='r100', noise_sgd=0.0, num_classes=85164, num_layers=100, output_c2c=0, patch='0_0_96_112_0', per_batch_size=60, power=1.0, prefix='../model-r100', pretrained='', rand_mirror=1, rescale_threshold=0, retrain=False, scale=0.9993, target='lfw,cfp_fp,agedb_30', triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 100
0 1 E 3
INFO:root:loading recordio /data/faces_ms1m_112x112/train.rec...
header0 label [ 3804847.  3890011.]
id2range 85164
0 0 3804846
c2c_stat [0, 85164]
3804846
rand_mirror 1
(120,)
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [426666, 597333, 682666]
[16:29:38] src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
/home/lyh/anaconda2/lib/python2.7/site-packages/mxnet/module/base_module.py:466: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.5 vs. 0.00833333333333). Is this intended?
  optimizer_params=optimizer_params)
call reset()
INFO:root:Epoch[0] Batch [20]	Speed: 211.74 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [40]	Speed: 204.20 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [60]	Speed: 200.59 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [80]	Speed: 200.87 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [100]	Speed: 204.17 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [120]	Speed: 203.31 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [140]	Speed: 202.20 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [160]	Speed: 197.89 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [180]	Speed: 196.42 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [200]	Speed: 197.84 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [220]	Speed: 199.84 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [240]	Speed: 199.16 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [260]	Speed: 199.51 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [280]	Speed: 199.33 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [300]	Speed: 199.42 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [320]	Speed: 199.17 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [340]	Speed: 199.13 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [360]	Speed: 199.19 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [380]	Speed: 199.55 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [400]	Speed: 198.83 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [420]	Speed: 196.18 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [440]	Speed: 198.51 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [460]	Speed: 199.76 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [480]	Speed: 197.17 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [500]	Speed: 198.87 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [520]	Speed: 194.67 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [540]	Speed: 192.28 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [560]	Speed: 190.91 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [580]	Speed: 193.99 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [600]	Speed: 193.84 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [620]	Speed: 190.89 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [640]	Speed: 190.69 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [660]	Speed: 191.62 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [680]	Speed: 193.53 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [700]	Speed: 189.48 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [720]	Speed: 187.77 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [740]	Speed: 189.28 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [760]	Speed: 189.19 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [780]	Speed: 190.49 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [800]	Speed: 192.47 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [820]	Speed: 195.38 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [840]	Speed: 197.69 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [860]	Speed: 197.44 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [880]	Speed: 196.46 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [900]	Speed: 194.44 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [920]	Speed: 193.12 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [940]	Speed: 197.04 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [960]	Speed: 199.12 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [980]	Speed: 196.32 samples/sec	acc=0.000000
lr-batch-epoch: 0.1 999 0
INFO:root:Epoch[0] Batch [1000]	Speed: 197.65 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1020]	Speed: 198.27 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1040]	Speed: 199.45 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1060]	Speed: 198.46 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1080]	Speed: 198.32 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1100]	Speed: 197.28 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1120]	Speed: 195.61 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1140]	Speed: 199.11 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1160]	Speed: 199.16 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1180]	Speed: 196.99 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1200]	Speed: 195.02 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1220]	Speed: 197.64 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1240]	Speed: 191.67 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1260]	Speed: 187.53 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1280]	Speed: 190.14 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1300]	Speed: 189.53 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1320]	Speed: 190.74 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1340]	Speed: 189.12 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1360]	Speed: 189.72 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1380]	Speed: 188.79 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1400]	Speed: 188.42 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1420]	Speed: 189.92 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1440]	Speed: 188.11 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1460]	Speed: 190.44 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1480]	Speed: 189.30 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1500]	Speed: 190.49 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1520]	Speed: 190.19 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1540]	Speed: 189.77 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1560]	Speed: 190.97 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1580]	Speed: 188.68 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1600]	Speed: 189.51 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1620]	Speed: 190.87 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1640]	Speed: 191.33 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1660]	Speed: 194.82 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1680]	Speed: 190.51 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1700]	Speed: 191.16 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1720]	Speed: 196.51 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1740]	Speed: 192.80 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1760]	Speed: 197.37 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1780]	Speed: 199.04 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1800]	Speed: 197.57 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1820]	Speed: 199.10 samples/sec	acc=0.000000
INFO:root:Epoch[0] Batch [1840]	Speed: 199.07 samples/sec	acc=0.000000





"
"In file align_celeb.py, line 130 
`            #TODO`           
` #if not img_id in v[1]:`
    `        #  continue`
I wonder does it mean you will use more images than in the cleanlist if you comment these 2 lines? 
Since you don't check if the image_id is the cleanlist or not.  "
"First of all, Thank you for sharing the code!

Every time I train a model with insightface, it says ""`UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.25 vs. 0.001953125)`"", which cause my attention.

In mxnet, SGD(standard updates) are applied by:
```
rescaled_grad = lr * rescale_grad * clip(grad, clip_gradient) + wd * weight
state = momentum * state + rescaled_grad
weight = weight - state
```
[http://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD](url)

I think rescale_grad = 1.0/batch_size/num_workers is more natural,  because the gradient is averaged by batch_size and is closer to the true full-batch gradient. I don't understand why use 1.0 / args.ctx_num instead .

Any help will be appreciated, thanks in advance."
"Thanks for sharing the precious code. The code in train_softmax is not friendly to read, hope the author can organize it to more readable! :)"
"Hello everyone. when I use the train_softmax.py , the program went wrong when execute this statement pickle.load(open(lfw.bin, 'rb'), the error message is : UnicodeDecodeError: 'ascii' codec can't decode byte 0xff in position 0: ordinal not in range(128).   Who knows how to solve this problem? thanks."
"Hi guys, it's a very impressive work you've done. 
I want to reproduce your result and found that the face_preprocess and face_image module is missing. Can you share them? Thank you"
"Is the mtcnn code from https://github.com/pangyupo/mxnet_mtcnn_face_detection? I also posted this issue there but that repo might not receive active attention anymore, so I wonder if anyone could provide any guidance here.

![image](https://user-images.githubusercontent.com/3190888/36845599-652a2592-1d25-11e8-9ad8-5c2bb675f845.png)

I see there are two methods, `detect_face()` and `detect_face_limited()`. The `detect_face()` signature is

```
            img: numpy array, bgr order of shape (1, 3, n, m)
                input image
```

But the actual shape checking treats an image as a `[height, width, channel]` array:

```
        # only works for color image
        if len(img.shape) != 3:
            return None

        # detected boxes
        total_boxes = []

        height, width, _ = img.shape
```

Therefore the detection fails at some point.

I also tried `detect_face_limited()`, and saw the `[1]` and `[2]` of threshold getting used to do the filtering and decreased those values to get more results. But still, e.g. this picture has four faces, and I'm getting as many as only one.

```
detector = MtcnnDetector(model_folder='mtcnn-model', ctx=mx.gpu(0), num_worker=1, accurate_landmark = True, threshold=[0.0,-1,-1], minsize=10)
img = cv2.imread(""180112-news-friends.jpg"")
detector.detect_face_limited(img, 2)

(array([[  5.78239929e+02,  -3.24641838e+01,   1.59770996e+03,
           8.00984070e+02,   2.69610956e-02]], dtype=float32),
 array([[ 932, 1277, 1178,  954, 1229,  302,  275,  414,  628,  599]], dtype=int32))
```

How can I get all those faces?"
""
"您好，
请问下，清洗后的数据集的文件名怎么和原始的图片文件名对应上
清洗后的文件名格式如下：
./m.0933t2/0.jpg
./m.0933t2/10.jpg
./m.0933t2/12.jpg
原始的文件名是下面的格式
m.0109kg/14-FaceId-0.jpg
m.0109kg/58-FaceId-0.jpg
m.010bk0/7-FaceId-0.jpg

谢谢"
"Can you share the MID&&n00xxxx classmap corresponding to labels in ms1m and vggface2 .rec files respectively? I want to merge these two dataset through the labels(the number of same celebrities in them is enormous) try to find the relationship between the depth and width of the dataset.
However, there are extra data in your rec files,  i don't konw which identity in original dataset the label mapping to.As you can see: the first person in the rec file and clean list.
![image](https://user-images.githubusercontent.com/17764091/36707686-50a8502a-1baa-11e8-892f-95db29a40c66.png)
![image](https://user-images.githubusercontent.com/17764091/36707701-69c9cc46-1baa-11e8-9afb-60b1536e28a6.png)
it's definitely not the same person
I will be very appreciate if you can share them."
"Is there any way the pre-trained model for MobileNet could be made available for download?

Looking back through the issues, it seems like that's one of the models causing the most confusion (I'm still not sure why the last pooling layer was removed), and that a majority of the people trying to reproduce this repo want to use."
how could i get the face from camera?   thanks
"In FaceImageIter,data.py,line 62.
what does c2c_threshold  output_c2c  c2c_mode parameter mean?what do they do?in your paper I can find anything related."
"When I am using 4 GPUs to train the network, it seems no speed increase comparing to single GPU. 

When the training is running under 4 GPUs mode, every GPU's ""GPU-Util"" is decreased to 20%~30%. While in single GPU mode, the ""GPU-Util"" is above 60%. Do you get an idea of the problem? 

"
"Hi, I download pretrained model(model-r50-am-lfw.zip) from https://pan.baidu.com/s/1mj6X7MK.
I follow readme to test model-0000.params on megaface. The accuracy is very low.
Is there something wrong with model-0000.params?

result is as follow:
1.000000
Done matching! Score matrix size: 3526 1000000
Saving to ./sphereface_results/otherFiles/facescrub.json_megaface_mxsphereface20c_112x112_1000000_1.bin
Computing test results with 1000000 images for set 1
Loaded 3526 probes spanning 80 classes
Loading from ./sphereface_results/otherFiles/facescrub.json_facescrub.json_mxsphereface20c_112x112.bin
Probe score matrix size: 3526 3526
distractor score matrix size: 3526 1000000
Done loading. Time to compute some stats!
Finding top distractors!
Done sorting distractor scores
Making gallery!
Done Making Gallery!
Allocating ranks (1000080)
Rank 1: 0.020466

"
"Is there any way to get the script to report the number of remaining epochs (or number of remaining batches within an epoch?). I'm training Mobilenet using the InsightFace method using the MSM1 dataset. I'm on the 2nd epoch, but I have no idea how many more epochs remain."
"1.test.py,some of my jpgs can not detect feature,most is side face pics,but other algorithm works well,
2.what is the similarity value for face recognition as one person,0.6? 0.7?0.8?
"
""
"Hi@nttstar, I have tested LResNet50E-IR with CPU mode, it is really slow, it almost cost 2 seconds on my PC(I9 CPU which is powerful)  to extract the feature from 112x112 image. is that correct? or is there anything wrong with my testing? thank you"
"After going through the instructions for adding the dataset, and adding the dependencies, and making sure I'm within the src folder in the repository, I enter the following to train InsightFace on LResNet100E-IR (this has been modified as my machine only has one GPU):
```
CUDA_VISIBLE_DEVICES='0' python -u train_softmax.py --network r100 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-r100
```
However, I get the following output:
```
gpu num: 1
num_layers 100
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=128, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_mode=-10, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.676161173096705, ctx_num=1, data_dir='../datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, images_per_identity=0, incay=0.0, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_m=0.5, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.9, network='r100', num_classes=85164, num_layers=100, output_c2c=0, patch='0_0_96_112_0', per_batch_size=128, power=1.0, prefix='../model-r100', pretrained='', rand_mirror=1, rescale_threshold=0, retrain=False, scale=0.9993, target='lfw,cfp_ff,cfp_fp,agedb_30', triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 100
0 1 E 3
INFO:root:loading recordio ../datasets/faces_ms1m_112x112/train.rec...
header0 label [ 3804847.  3890011.]
id2range 85164
0 0
3804846
rand_mirror 1
(128,)
[20:03:42] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_ff
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [400000, 560000, 640000]
[20:04:18] src/operator/././cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:466: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (1.0 vs. 0.0078125). Is this intended?
  optimizer_params=optimizer_params)
call reset()
[20:04:30] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [20:04:30] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f60919ef5ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bb72f) [0x7f609401c72f]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf958) [0x7f6094020958]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x525d6f) [0x7f6091c86d6f]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23ce416) [0x7f6093b2f416]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23d2600) [0x7f6093b33600]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b42cd) [0x7f6093b152cd]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8a8b) [0x7f6093b19a8b]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8c66) [0x7f6093b19c66]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b553b) [0x7f6093b1653b]

[20:04:30] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [20:04:30] src/engine/./threaded_engine.h:359: [20:04:30] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f60919ef5ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bb72f) [0x7f609401c72f]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf958) [0x7f6094020958]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x525d6f) [0x7f6091c86d6f]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23ce416) [0x7f6093b2f416]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23d2600) [0x7f6093b33600]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b42cd) [0x7f6093b152cd]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8a8b) [0x7f6093b19a8b]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8c66) [0x7f6093b19c66]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b553b) [0x7f6093b1653b]

A fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 8 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f60919ef5ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b4574) [0x7f6093b15574]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8a8b) [0x7f6093b19a8b]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8c66) [0x7f6093b19c66]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b553b) [0x7f6093b1653b]
[bt] (5) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f60d4d20c80]
[bt] (6) /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f60e2b486ba]
[bt] (7) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f60e287e41d]

terminate called after throwing an instance of 'dmlc::Error'
  what():  [20:04:30] src/engine/./threaded_engine.h:359: [20:04:30] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f60919ef5ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bb72f) [0x7f609401c72f]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf958) [0x7f6094020958]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x525d6f) [0x7f6091c86d6f]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23ce416) [0x7f6093b2f416]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23d2600) [0x7f6093b33600]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b42cd) [0x7f6093b152cd]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8a8b) [0x7f6093b19a8b]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8c66) [0x7f6093b19c66]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b553b) [0x7f6093b1653b]

A fatal error occurred in asynchronous engine operation. If you do not know what caused this error, you can try set environment variable MXNET_ENGINE_TYPE to NaiveEngine and run with debugger (i.e. gdb). This will force all operations to be synchronous and backtrace will give you the series of calls that lead to this error. Remember to set MXNET_ENGINE_TYPE back to empty after debugging.

Stack trace returned 8 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f60919ef5ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b4574) [0x7f6093b15574]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8a8b) [0x7f6093b19a8b]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b8c66) [0x7f6093b19c66]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23b553b) [0x7f6093b1653b]
[bt] (5) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f60d4d20c80]
[bt] (6) /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f60e2b486ba]
[bt] (7) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f60e287e41d]

Aborted (core dumped)
```
Can you include more detail in the README about the specific requirements in terms of devices, memory, and CUDA requirements?"
"The training Datasets available from Google Drive have been very useful, but data like the modified VGGFace2 are not accessible.

The dataset files on Baidu are large enough that downloading them requires the Baidu Network Tool, which is not available in English. Could links to these Datasets (VGGFace2 from Part 1) on Google Drive be made available in the README?"
"After going through the instructions for adding the dataset, and adding the dependency, and making sure I'm within the SRC file in the repository, I enter the following to run the model on MobileNet:
```
CUDA_VISIBLE_DEVICES='0,1,2,3' python -u train_softmax.py --network m1 --loss-type 4 --margin-m 0.5 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-r100
```
But when I run that, I get the following output:
```
gpu num: 4
num_layers 1
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=512, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_mode=-10, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.676161173096705, ctx_num=4, data_dir='../datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, images_per_identity=0, incay=0.0, loss_type=4, lr=0.1, lr_steps='', margin=4, margin_m=0.5, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.9, network='m1', num_classes=85164, num_layers=1, output_c2c=0, patch='0_0_96_112_0', per_batch_size=128, power=1.0, prefix='../model-r100', pretrained='', rand_mirror=1, rescale_threshold=0, retrain=False, scale=0.9993, target='lfw,cfp_ff,cfp_fp,agedb_30', triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init mobilenet 1
(1, 'E', 3)
INFO:root:loading recordio ../datasets/faces_ms1m_112x112/train.rec...
header0 label [ 3804847.  3890011.]
id2range 85164
0 0
3804846
rand_mirror 1
(512,)
[20:17:39] src/engine/engine.cc:55: MXNet start using engine: ThreadedEnginePerDevice
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_ff
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [100000, 140000, 160000]
[20:18:13] src/operator/././cudnn_algoreg-inl.h:107: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)
[20:18:16] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [20:18:16] src/storage/storage.cc:63: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: invalid device ordinal

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f08e94825ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28b98a6) [0x7f08ebaad8a6]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf94b) [0x7f08ebab394b]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23db5ad) [0x7f08eb5cf5ad]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e5684) [0x7f08eb5d9684]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e661c) [0x7f08eb5da61c]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23eae98) [0x7f08eb5dee98]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f4e5a) [0x7f08eb5e8e5a]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f5554) [0x7f08eb5e9554]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBind+0x2250) [0x7f08eb557f80]

Traceback (most recent call last):
  File ""train_softmax.py"", line 915, in <module>
    main()
  File ""train_softmax.py"", line 912, in main
    train_net(args)
  File ""train_softmax.py"", line 906, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 460, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 428, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 237, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 333, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 611, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 1494, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
[20:18:16] src/storage/storage.cc:63: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading CUDA: invalid device ordinal

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7f08e94825ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28b98a6) [0x7f08ebaad8a6]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf94b) [0x7f08ebab394b]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23db5ad) [0x7f08eb5cf5ad]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e5684) [0x7f08eb5d9684]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e661c) [0x7f08eb5da61c]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23eae98) [0x7f08eb5dee98]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f4e5a) [0x7f08eb5e8e5a]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f5554) [0x7f08eb5e9554]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBind+0x2250) [0x7f08eb557f80]
```

Can you include more detail in the README about the specific requirements in terms of devices and CUDA requirements?"
"Now I believe in order to achieve fairness, you should remove this repository, or at least remove this list of Megaface, because we are working hard to achieve a high result without taking the effort of other R&D guys, Companies... 

Please respect my opinion, I appreciate your work and efforts, but day by day I think to do this thing because after 1 Month we will see all going to have the same result. This is in contrast to previous periods.

"
can you share us the tool to convert from mxnet into caffe?
"Hi dear author! Happy Chinese new year : )

In paper, you used Prelu as activation function in proposed residual block, but in fresnet.py, you seem to replaced prelu with relu, Am I right?If so, could you explain the reason?

Thanks!"
"I have seen that the current top algorithm in the MegaFace challenge is iBug_DeepInsight, with an accuracy that corresponds with your latest update: *2018.02.13: We achieved state-of-the-art performance on MegaFace-Challenge-1, at 98.06*

After reading your paper and the README in this repo, it seems to me that this accuracy is achieved using the cleaned/refined MegaFace dataset. Is this correct?"
"Hi,
I have 2 questions related to the paper:
- section 3.2.1, can you confirm when you say""instead of conv 7x7 and stride 2"", that are you referring to ResNet first convolution layer? You are changing to 3x3 stride 1 so that the input can be 112x112?
- In table 10, which CNN do you use for Softmax and CosineFace?
Thank you"
Your performance of LResNet34E-IR converted caffemodel  drop ~0.19 on LFW. I'm not familiar with mxnet. Can we convert mxnet model to caffe without lossing precision?
"Hi @nttstar , i notice that you use skimage SimilarityTransform method but not opencv estimateRigidTransform in face_preprocess.py. Is the first one can get a high face recogniton accuracy rate?  Looking forward to your reply."
"Hello, I want to know if has other tricks in your code besides your paper mentioned?"
can you share the landmarks annotations of vggface2 dataset？ 
"Hi, 
In your paper, you used 3.8m images of Ms1m dataset.  But in the clean list you provided, there is only 3.47m images. Could you please release the full clean list? Thank you!"
"I think the codes [here](https://github.com/deepinsight/insightface/blob/master/src/eval/verification.py#L240-L249) shows that you normalize each feature on the whole test dataset's features. 

It is some tricks or I misunderstood the codes?"
"Why do you select MXNET instead of tensorflow for this project?
What's the advantage of mxnet than tensorflow?"
"Have a check at the mobilenet structure, and most of the weights are in the final FC layer:
```
pre_fc1_weight parameter size=25690112, shape=(512L, 50176L)
```
and looking at the code, the last pooling layer is omitted (![mobilenet](https://raw.githubusercontent.com/joshua19881228/my_blogs/master/Computer_Vision/Reading_Note/figures/Reading_Note_20170719_MobileNet_1.png)).
https://github.com/deepinsight/insightface/blob/f1cd542e0f0413ed0e06723e5c49593e3e468825/src/symbols/fmobilenet.py#L75-L80

Is this by design? Ignoring the last pooling layer leads to much larger model size -:("
"- 
Traceback (most recent call last):
  File ""train_softmax.py"", line 873, in <module>
    main()
  File ""train_softmax.py"", line 870, in main
    train_net(args)
  File ""train_softmax.py"", line 593, in train_net
    sym, arg_params, aux_params = get_symbol(args, arg_params, aux_params)
  File ""train_softmax.py"", line 245, in get_symbol
    fc7 = mx.sym.LSoftmax(data=embedding, label=gt_label, num_hidden=args.num_classes,
AttributeError: 'module' object has no attribute 'LSoftmax'

- "
@nttstar  What do you think of applying model compression techniques (e.g Model Distillation) to Arcnet model to reduce run time?
Thanks a lot for sharing your experiment code. I am planning to tinker Mobilenet model on Raspberry Pi. Would it be possible for you to share the model?
"Thank you for the great work! I'm not quite familiar with mxnet framework so sorry for a stupid question. Can you tell me where the backpropagation is implemented in your code, please?"
"Dear: 

      Very impressive code release.   Could you please share a working scrip to train and fine tune triplet loss with softmax?

      Thanks"
"Hi, from the paper and readme, it seems the mobilenet `m1 == fmobilenet ==LMobileNetE `, and curiors  the performance of `fmobilenetv2`."
"@nttstar when I  run the code with three Titanx, but encounter crash, how should I solve it?

CUDA_VISIBLE_DEVICES='0,1,2' python -u train_softmax.py --network r50 --loss-type 2 --margin-m 0.35 --data-dir ../datasets/faces_ms1m_112x112 --prefix ../model-r50-amsoftmax
gpu num: 4
num_layers 50
image_size [112, 112]
num_classes 85164
Called with argument: Namespace(batch_size=512, beta=1000.0, beta_freeze=0, beta_min=5.0, c2c_threshold=0.0, center_alpha=0.5, center_scale=0.003, ckpt=1, coco_scale=8.676161173096705, ctx_num=4, data_dir='../datasets/faces_ms1m_112x112', easy_margin=0, emb_size=512, end_epoch=100000, gamma=0.12, image_channel=3, image_h=112, image_w=112, images_per_identity=0, incay=0.0, loss_type=2, lr=0.1, lr_steps='', margin=4, margin_m=0.35, margin_s=64.0, margin_verbose=0, max_steps=0, mom=0.9, network='r50', num_classes=85164, num_layers=50, output_c2c=0, patch='0_0_96_112_0', per_batch_size=128, power=1.0, prefix='../model-r50-amsoftmax', pretrained='', rand_mirror=1, rescale_threshold=0, retrain=False, scale=0.9993, target='lfw,cfp_ff,cfp_fp,agedb_30', triplet_alpha=0.3, triplet_bag_size=3600, triplet_max_ap=0.0, use_deformable=0, use_val=False, verbose=2000, version_input=1, version_output='E', version_se=0, version_unit=3, wd=0.0005)
init resnet 50
0 1 E 3
INFO:root:loading recordio ../datasets/faces_ms1m_112x112/train.rec...
header0 label [ 3804847.  3890011.]
id2range 85164
0 0
3804846
rand_mirror 1
(512,)
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver lfw
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_ff
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
loading bin 13000
loading bin 14000
(14000L, 3L, 112L, 112L)
ver cfp_fp
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
loading bin 12000
(12000L, 3L, 112L, 112L)
ver agedb_30
lr_steps [100000, 140000, 160000]
[14:06:14] /home/travis/build/dmlc/mxnet-distro/mxnet-build/dmlc-core/include/dmlc/logging.h:308: [14:06:14] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7efbb96995ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bb72f) [0x7efbbbcc672f]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf958) [0x7efbbbcca958]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23db5ad) [0x7efbbb7e65ad]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e5684) [0x7efbbb7f0684]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e661c) [0x7efbbb7f161c]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23eae98) [0x7efbbb7f5e98]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f4e5a) [0x7efbbb7ffe5a]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f5554) [0x7efbbb800554]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBind+0x2250) [0x7efbbb76ef80]

Traceback (most recent call last):
  File ""train_softmax.py"", line 873, in <module>
    main()
  File ""train_softmax.py"", line 870, in main
    train_net(args)
  File ""train_softmax.py"", line 864, in train_net
    epoch_end_callback = epoch_cb )
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py"", line 460, in fit
    for_training=True, force_rebind=force_rebind)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/module.py"", line 428, in bind
    state_names=self._state_names)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 237, in __init__
    self.bind_exec(data_shapes, label_shapes, shared_group)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 333, in bind_exec
    shared_group))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/module/executor_group.py"", line 611, in _bind_ith_exec
    shared_buffer=shared_data_arrays, **input_shapes)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet/symbol/symbol.py"", line 1494, in simple_bind
    raise RuntimeError(error_msg)
RuntimeError: simple_bind error. Arguments:
data: (128, 3, 112, 112)
softmax_label: (128,)
[14:06:14] src/storage/./pooled_storage_manager.h:107: cudaMalloc failed: out of memory

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28e5ac) [0x7efbb96995ac]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bb72f) [0x7efbbbcc672f]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x28bf958) [0x7efbbbcca958]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23db5ad) [0x7efbbb7e65ad]
[bt] (4) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e5684) [0x7efbbb7f0684]
[bt] (5) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23e661c) [0x7efbbb7f161c]
[bt] (6) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23eae98) [0x7efbbb7f5e98]
[bt] (7) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f4e5a) [0x7efbbb7ffe5a]
[bt] (8) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(+0x23f5554) [0x7efbbb800554]
[bt] (9) /usr/local/lib/python2.7/dist-packages/mxnet/libmxnet.so(MXExecutorSimpleBind+0x2250) [0x7efbbb76ef80]
"
"I have ran training to epoch 34, minibatch 1700 using code at https://github.com/deepinsight/insightface/tree/af4a4e8f5c47de08e9f069a6a5e65988b6e91888 with a few modification (adding memonger). Typical performance is

LFW | CFP-FF | CFP-FP | AgeDB-30
--|--|--|--
99.817% | 99.771% | 94.257% | 98.000%

1. Are the reported LFW, CFP-FF, CFP-FP, AgeDB-30 results measured on the same set of network parameters?
2. How many epochs/minibatches need to be run in order to obtain the reported result?
3. Is my current trained network a successful reproduction?"
""
Just want to know the training time cost about the provided `LResNet50E-IR` model.
"Some questions need you guys to help answer, thanks in advance!

- `add fixed margin 44`. 
    - have you guys tried other margin values and see if any improvements?
- `similarity transformation`, 
    - have you guys tried other transformations, I noticed that in `face_preprocess.py` there are some commented code which looks like projective transform.
   - It looks like the similarity transformed image will gain black corner running `test.py`, I use the mtcnn crop the testing images, without resizing and fixed 44 margin. I am not sure if this is by design or I am wrong with the cropping steps -:)  
![0](https://user-images.githubusercontent.com/1740859/35659297-776c0784-0740-11e8-99c4-ab305f8dba13.jpg)
- `mtcnn 5 landmarks`
    - It looks like mtcnn 5 landmarks are not accurate regarding side face, if use more accurate lardmarks, shall it improve the performance?
"
"I have a doubt

Please look at the clean list provided by you, ms1m_clean_list.txt, in this list for the Freebase identity m.05lbbp which has one image 51.jpg 
But in MsCeleb original database the Freebase identity m.05lbbp has 22 images with 51 name such as

51-FaceId-0.jpg   51-FaceId-11.jpg  51-FaceId-13.jpg  51-FaceId-15.jpg  51-FaceId-17.jpg  51-FaceId-19.jpg  51-FaceId-20.jpg  51-FaceId-2.jpg   51-FaceId-4.jpg   51-FaceId-6.jpg   51-FaceId-8.jpg   51-FaceId-10.jpg  51-FaceId-12.jpg  51-FaceId-14.jpg  51-FaceId-16.jpg  51-FaceId-18.jpg  51-FaceId-1.jpg   51-FaceId-21.jpg  51-FaceId-3.jpg   51-FaceId-5.jpg   51-FaceId-7.jpg   51-FaceId-9.jpg

can you please tell us how did you assign names to your images while generating images from MsCelebV1-Faces-Aligned.tsv while following the below format described by MsCeleb

File format: text files, each line is an image record containing 7 columns, delimited by TAB.
    Column1: Freebase MID
    Column2: ImageSearchRank
    Column3: ImageURL
    Column4: PageURL
    Column5: FaceID
    Column6: FaceRectangle_Base64Encoded (four floats, relative coordinates of UpperLeft and BottomRight corner)
    Column7: FaceData_Base64Encoded

you can check the format at 
https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/"
"Hello,

Ca you please give us/upload the 500+ overlap between facescrub identities and ms1m
also when can you upload the megface distractor noise list ?

Thanks,
Gsquared"
"per the code in `deploy/test.py`: 
```python
parser.add_argument('--image-size', default='112,112', help='')
```

it seems that the input should be cropped as square with width as 112, however the mtcnn output may be rectangle, I thought the detailed steps to run `test.py` are as below:

1. use mtcnn to crop the face images with margin 44
2. resize cropped face to 112x112
3. run `test.py` and the script will do face alignment with mtcnn

I am curious if my understanding is wrong, I thought the alignment before resizing should be more reliable?"
"Thanks for your work,but the extra loss I do not know why you do this:
1 The extra loss is makes the l2 norm small,and you do this for hard sample mining?
2 In the code,the extra loss is used only when the loss is softmax or sphereface,why?The other loss can not use it?
"
"Awesome work! 
As I know, there are some overlapped identities between LFW and ms1m, does the clean list has removed the overlapped identities, this may affect the performance on LFW"
""
"Thanks for your amazing work! After reading your paper and some code in this project, i have some questions:
1. What's the meaning of option  ```easy_margin```?
```
    if args.easy_margin:
      cond = mx.symbol.Activation(data=cos_t, act_type='relu')
      #cond_v = cos_t - 0.4
      #cond = mx.symbol.Activation(data=cond_v, act_type='relu')
    else:
      cond_v = cos_t - threshold
      cond = mx.symbol.Activation(data=cond_v, act_type='relu')
```
2. Why do relu activation on cos_t, will it let the minus value in fc7 not use amsoftmax's margin to do finnal softmax loss computing?"
"I see you got very good accuracy on LFW, and it is better than [@davidsandberg's](https://github.com/davidsandberg/facenet).
How can I use your pretraind model with tensorflow as like @davidsandberg's?
Thanks."
"first,it is really appreciate for sharing your codes!Now, i want to know the time consuming!
For single cropped face image(112x112), total inference time is only 17ms on my testing server(Intel E5-2660 @ 2.00GHz, Tesla M40, LResNet34E-IR).
the inference time (17ms) is run CPU or GPU?"
"Thank you for your wonderful work, do you have plan to release the clean list, I want to use it with another alignment method"
After several tries I could not get the detector to work. Can you add a pre-cropped example/image. 
"What is the accuracy you have got on Megaface for both verification, and identification,  because through the published paper 
ArcFace(m=0.5) Rank1@10^6:  83.27 
VR@FAR10^6:    98.48 
LResNet100E-IR@MS1M

Is there any problem with that? "
"Hi,
First congratulations on the great work you did. I am currently struggle on the Megaface Testing because it has so many duplicated images with Facescrub. 
I am looking for the ""remove_noises.py"" you commented on the Readme.md, but it looks it's missing."
"flake8 testing of https://github.com/deepinsight/insightface on Python 2.7.14

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```./src/data/face2rec2.py:194:9: F821 undefined name 'make_list'
        make_list(args)
        ^
1     F821 undefined name 'make_list'
```
There is a __make_list()__ defined at:
* https://github.com/deepinsight/insightface/blob/master/src/data/face2rec.py#L74"
"Hi,
Is it possible to share the dataset somewhere else?"
"$ __python2 -m flake8 --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./src/data.py:1222:32: F821 undefined name 'idx2range'
        print('idx2range', len(idx2range))
                               ^
./src/data.py:1225:26: F821 undefined name 'path_root'
        self.path_root = path_root
                         ^
./src/data.py:1243:31: F821 undefined name 'per_batch_size'
        self.identities = int(per_batch_size/self.images_per_identity)
                              ^
./src/data.py:1250:96: F821 undefined name 'per_batch_size'
          self.provide_data = [(data_name, (batch_size,) + data_shape), ('extra', (batch_size, per_batch_size))]
                                                                                               ^
./src/data.py:1255:21: F821 undefined name 'per_batch_size'
            while a<per_batch_size:
                    ^
./src/data.py:1260:18: F821 undefined name 'per_batch_size'
            c += per_batch_size
                 ^
./src/data.py:1324:19: F821 undefined name 'faiss'
      quantizer = faiss.IndexFlatL2(d)  # the other index
                  ^
./src/data.py:1325:15: F821 undefined name 'faiss'
      index = faiss.IndexIVFFlat(quantizer, d, faiss_params[0], faiss.METRIC_L2)
              ^
./src/data.py:1325:65: F821 undefined name 'faiss'
      index = faiss.IndexIVFFlat(quantizer, d, faiss_params[0], faiss.METRIC_L2)
                                                                ^
./src/data.py:1348:13: F821 undefined name 'offline_reset'
            offline_reset()
            ^
./src/data.py:1679:19: F821 undefined name 'faiss'
      quantizer = faiss.IndexFlatL2(d)  # the other index
                  ^
./src/data.py:1680:15: F821 undefined name 'faiss'
      index = faiss.IndexIVFFlat(quantizer, d, faiss_params[0], faiss.METRIC_L2)
              ^
./src/data.py:1680:65: F821 undefined name 'faiss'
      index = faiss.IndexIVFFlat(quantizer, d, faiss_params[0], faiss.METRIC_L2)
                                                                ^
./src/data/face2rec2.py:194:9: F821 undefined name 'make_list'
        make_list(args)
        ^
```"
"python align_facescrub.py --input-dir ~/face/megaface/facescrub/ --output-dir ./facescrub_align 
Martin Henderson/Martin Henderson_40499
Rebecca Budig/Rebecca Budig_10788
Pamela Anderson/Pamela Anderson_2367
valid keys 3530
Creating networks and loading parameters

AttributeError: 'EasyDict' object has no attribute 'landmark'"
""
"You have made a great work here. However, I believe that adding Angular Triplet Loss (https://github.com/KaleidoZhouYN/Angular-Triplet-Loss) will definitely improve the results on Megaface and LFW. Unfortunately, I do not have enough experience to implement it in MxNet. Hope someone will try it out soon.

Did you try this new loss? or willing to try it?"
What do you mean by mobilenet v2 is it for mobile? 
"What did you mean by sphereface? is it Asoftmax or L-Margin or Isoftmax ? because I think it's not same as Asoftmax ... 
If yes did you test it and get similar behavior to the original one "
""
"Hi there, thanks for the great work, could you please provide the definition of different loss type. It would be many appreciated if the corresponding paper could be indicated. "
thanks for your sharing and which dataset do you used?
"It has been a long time that I want to try dropout and ReLU/PReLU after FC.
Thanks a lot, the result is conductive."
"./examples/train_fp16.sh aognet_s configs/aognet_imagenet_12M.yaml first_try

we use 4 P40 GPUs, so change the ""train_fp16.sh"" like this:
### Change accordingly
GPUS=0,1,2,3
NUM_GPUS=4
NUM_WORKERS=4

the log is below:
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([206])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([824, 206, 1, 1])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1000, 824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.FloatTensor with torch.Size([824])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1000, 824])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1000])
FP16_Optimizer received torch.cuda.HalfTensor with torch.Size([1000])
torch.Size([256, 3, 224, 224])
torch.Size([256, 3, 224, 224])
torch.Size([256, 3, 224, 224])
torch.Size([256, 3, 224, 224])
Epoch: [0][0/1252]	Time 12.503 (12.503)	Speed 81.903 (81.903)	Data 0.984 (0.984)	Loss 6.9218750000 (6.9219)	Prec@1 0.195 (0.195)	Prec@5 0.586 (0.586)	lr 0.000064	
Epoch: [0][100/1252]	Time 2.100 (2.206)	Speed 487.515 (464.191)	Data 0.001 (0.011)	Loss nan (nan)	Prec@1 0.000 (0.003)	Prec@5 0.000 (0.009)	lr 0.006454	
Epoch: [0][200/1252]	Time 2.116 (2.155)	Speed 483.939 (475.125)	Data 0.001 (0.006)	Loss nan (nan)	Prec@1 0.000 (0.001)	Prec@5 0.000 (0.004)	lr 0.012843	
Epoch: [0][300/1252]	Time 2.103 (2.137)	Speed 487.036 (479.160)	Data 0.001 (0.004)	Loss nan (nan)	Prec@1 0.000 (0.001)	Prec@5 0.000 (0.003)	lr 0.019233	
"
"Hello! I'm interested in this work. I use AOGNets to faster R-CNN as your paper intorduced. I use the code from https://github.com/jwyang/faster-rcnn.pytorch/tree/pytorch-1.0. And I make a aog.py in lib/model/faster_rcnn  you do. The issue is that when I run train_val to train the model end to end, The loss is so high normally. I want to know where is the problem. Is it related to FP-optimizer? If yes, what should I do for this issues?
I'm looking forward to your reply!
Best wishes!
![图片](https://user-images.githubusercontent.com/22558591/63741397-86600e80-c8c7-11e9-9dc3-f762343b06f2.png)
![图片](https://user-images.githubusercontent.com/22558591/63741625-6c72fb80-c8c8-11e9-8c2d-24970331c107.png)

I use the pretrained model in your google driver.
"
"About the link to the AOGNets paper in the readme, it currently points to the PDF version. It would be better if it were to point to the HTML version instead, i.e. https://arxiv.org/abs/1711.05847 . Please consider making this change. Readers can always click on the PDF link themselves.

Secondly, the link to ""Mixture Normalization: A Lightweight Integration of Feature Normalization and Attention"" is broken. What should it be pointing to?"
"https://arxiv.org/pdf/1711.05847v3.pdf says:

> The  **code  and  models  are  available**  at https://github.com/iVMCL/AOGNets

So where are they? The last update was over a month ago. It has been over 1.5 years since its first preprint, and there is evidently no code to show for it. Multiple grants have been used too."
"Hello Javier,
  I'm trying to understand your WHD forward function, but fail to understand the dimension of `normalized_y`.
 
```python
            # One by one
            prob_map_b = prob_map[b, :, :]
            gt_b = gt[b]
            orig_size_b = orig_sizes[b, :]
            norm_factor = (orig_size_b/self.resized_size).unsqueeze(0)

            # Pairwise distances between all possible locations and the GTed locations
            n_gt_pts = gt_b.size()[0]
            normalized_x = norm_factor.repeat(self.n_pixels, 1) * self.all_img_locations
            normalized_y = norm_factor.repeat(len(gt_b), 1) * gt_b
            d_matrix = cdist(normalized_x, normalized_y)
```
From my understanding, `gt_b.size() = [H, W]`, `normal_factor.size = (1, 2)`, then `n_gt_pts = H`;  `self.all_img_locations.size() = [HxW, 2]` which leads to `normalized_x.size() = [HxW, 2]` ; 

Then, here comes my puzzle. `norm_factor.repeat(len(gt_b), 1)` gives me `[B, 2]`, but `gt_b.size()  = [H, W]`, how to multiply these 2 tensors?? Did you use some special reshape operations here? 

Thank you!


"
"The maximum possible distance is calculated with resized image size.

https://github.com/javiribera/locating-objects-without-bboxes/blob/e51f75ec925f67bb4054ab4a6e9b20d1d8e689cc/object-locator/losses.py#L143

But this should be calculated with the original image size, as bellow code also calculates distances in the original image.

Bellow two lines need to be changed

https://github.com/javiribera/locating-objects-without-bboxes/blob/e51f75ec925f67bb4054ab4a6e9b20d1d8e689cc/object-locator/losses.py#L224

https://github.com/javiribera/locating-objects-without-bboxes/blob/e51f75ec925f67bb4054ab4a6e9b20d1d8e689cc/object-locator/losses.py#L204

Suggested change


```
            max_dist = (orig_size_b **2).sum().sqrt()
....
            # Corner case: no GT points
            if gt_b.ndimension() == 1 and (gt_b < 0).all().item() == 0:
                terms_1.append(torch.tensor([0],
                                            dtype=torch.get_default_dtype()))
                terms_2.append(torch.tensor([max_dist],
                                            dtype=torch.get_default_dtype()))
                continue
.....
            weighted_d_matrix = (1 - p_replicated)*max_dist  + p_replicated*d_matrix
```

I am sorry, too lazy to make pull request to fix it by myself now."
"Hi, thanks for the great work!

When I was training with my own data, the bottleneck of the whole training was apparently the validation process since it's using batch size = 1. In one epoch of my training, it cost only 20% of time for training and back-prop but cost 80% for validation. Why can't the batch size for validation be greater than 1? Is there any issue or problems to be solved in order to have this feature?"
"In your paper, figure 8 shows the f1-scores on three dataset are over 88.6 on r=5. however,in my opinion, the figure 6 shows the  f1-scores on r=5 is under 60. My question is which database is figure 6 based on ? or I have a wrong understanding on Figure 6. Can you give me some suggestions?
![_T~K{}3U8A7~A (A5VT5 }J](https://user-images.githubusercontent.com/81737024/113252196-19f48f00-92f6-11eb-830e-39d788f9afcf.jpg)
![P 3W}XP{BKJIT)LFX{)6 65](https://user-images.githubusercontent.com/81737024/113252205-1d881600-92f6-11eb-8664-96b6c727753d.png)
@javiribera 
"
"Hi, I was wandering what are is the difference between ""np.sum(detected_pts)"" (true positives count) and ""np.sum(detected_gt)"" which is used to estimate false negatives count?. My understanding is that gt are the real points and pts are the detected ones ( within the radius), but what would be the difference if the r is big enough and why are not the same?.   For instance, in a mall set, you have 5 persons and the program correctly identifies 3 of them. True positives will be 3, false positives 2 and false negatives will also be 2 and therefore that sums(detected) will be equal. I also get that both will be higher with a bigger radius  but the gt one will be bigger that the pts one (Sometimes). Question is because in your paper you wrote as False Negative definition : ""A false negative is counted if a true location does have any estimated location at a distance at most r"" which for me is the same as True Positive definition wich is the same just from the point of the estimated location. And if it is a misspelling why False negative is not the same as False positive?
Thankyou for your attention"
"Can anyone provide the link to download the dataset?

Thank you!"
The requested URL /~ccloy/downloads_mall_dataset.html was not found on this server.
"Have u tried to generate heatmaps around gt, then train the network with focal loss? "
"HI, @chenyuhao @javiribera @dguera 

I met this error when run your elvaluation script.

""cannot import name '_validate_lengths' ""

I installed your conda  install guide ...
 
What's wrong to me ?

Thanks.
Best,

@bemoregt."
"Hi, @chenyuhao @javiribera @dguera 

Your Pupil Dataset's Link page is failed.

I me this internet error message.
""Link page could not be found.""

How can I get the Pupil dataset?

Thanks.
Best,
@bemoregt."
"I use the command: CUDA_VISIBLE_DEVICES=1 python -m object-locator.locate --imgsize 256X256 --dataset mall_dataset/frames/test/ --out mall_dataset/output/ --model checkpoints/mall\,lambdaa\=1\,BS\=32\,Adam\,LR1e-4.ckpt  --evaluate,   and the test set takes pictures from No.seq_001801.jpg to No.seq_002000.jpg. However, the results have large difference from your paper. Can you give me some suggestions? @javiribera 
![bmm_stats](https://user-images.githubusercontent.com/19422722/92190055-b656d000-ee92-11ea-93a9-bd5ba09b7202.png)
![fscore_vs_tau](https://user-images.githubusercontent.com/19422722/92190077-c2db2880-ee92-11ea-86e8-e756baacc6f1.png)
![precision_vs_th](https://user-images.githubusercontent.com/19422722/92190091-cbcbfa00-ee92-11ea-86f1-32c538bc5d3a.png)
![recall_vs_tau](https://user-images.githubusercontent.com/19422722/92190108-d5edf880-ee92-11ea-8963-db8e252648a4.png)
"
"I'm trying to replicate your training process using the “mall,lambdaa=1,BS=32,Adam,LR1e-4.ckpt”. I find that there is still training space after loading weights, and the loss of verification set is very large. I confuse about it, can you give me some suggestions?
![QQ图片20200710151642](https://user-images.githubusercontent.com/19422722/87127415-758e7080-c2c0-11ea-8944-5e24f1768b42.png)
"
"Hi, I have been training with mall dataset,
This is the command I used,
 python -m object-locator.train --train-dir ./object-locator/dataset/mall_dataset/frames/ \ 
--batch-size 32 \
 --lr 1e-3 --optim Adam \
 --resume ./model_08.ckpt --visdom-server localhost --visdom-port 8097 \
--val-dir auto \
 --epochs 1000000 \ 
--nThreads 4 \ 
--val-freq 10 -\
-max-mask-pts 100 \
--paint

Could you please show me your train command, to get your achievement of getting head position?
 especially validation frequency and epoch setting.
"
"dictionary['locations'] = eval(dictionary['locations'])
KeyError: 'locations'

"
"Hi,
while training i am keep getting the following warning:
PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.

is it important?"
"Hi,
i'm trying to train on my own dataset by using this command:
python -m object-locator.train \ --train-dir real_train \ --batch-size 32 \ --visdom-env mytrainingsession \ --visdom-server localhost \ --lr 1e-3 \ --val-dir real_train \ --optim Adam \ --save saved_model.ckpt

**I got this error message:**
Traceback (most recent call last):
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/connection.py"", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/util/connection.py"", line 84, in create_connection
    raise err
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/util/connection.py"", line 74, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 677, in urlopen
    chunked=chunked,
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 392, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/http/client.py"", line 1262, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/http/client.py"", line 1308, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/http/client.py"", line 1257, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/http/client.py"", line 1036, in _send_output
    self.send(msg)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/http/client.py"", line 974, in send
    self.connect()
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/connection.py"", line 187, in connect
    conn = self._new_conn()
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/connection.py"", line 172, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f22cdda79b0>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 725, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/urllib3/util/retry.py"", line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8989): Max retries exceeded with url: /env/mytrainingsession (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22cdda79b0>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/visdom/__init__.py"", line 711, in _send
    data=json.dumps(msg),
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/visdom/__init__.py"", line 677, in _handle_post
    r = self.session.post(url, data=data)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/requests/sessions.py"", line 578, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/requests/sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/requests/sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""/home/odedb/.conda/envs/object-locator/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8989): Max retries exceeded with url: /env/mytrainingsession (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f22cdda79b0>: Failed to establish a new connection: [Errno 111] Connection refused',))
[Errno 111] Connection refused
E: cannot connect to Visdom server localhost:8989

thanks in advance!"
"Would you happen to have any intuition on this?
I'm using a U-net style network (with skipped connections). Output -> 3 channels. The centre of mass, in my case, the pupil centre, is regressed from channel 1.

I use torch.sigmoid on channel 1 before giving it as input to weighted H loss and a sufficiently small learning rate (5e-5) with ADAM.

I observe that the loss reduces 0.03 -> 0.009 and the output starting to look as expected from channel 1, i.e, we start seeing the expected blob. Post convergence to a minima (which happens within 1 epoch), the loss goes its maximum (0.1 in my case) and stays there. I checked the gradient norms and found that there is a lot of fluctuation in the norm values. Furthermore, the loss is jumpy on every iteration.

Would you have an intuition about this?"
"Hello, 
Thanks for this nice paper and the code. 
Of my understanding, in you code (WeightedHausdorffDistance class), the ground truth gt is a list of point corresponding to different objects. Is it possible to have gt a list of point from the same object ?

I am wondering of using your loss for a segmentation task in medical imaging to replace (or in parallel of) Dice Loss. So we have the ground truth segmentation of one object. 

Best Regards, 
Théo"
"Could you comment on the following? I believe it would help me and potentially future readers:

1. Have you used this loss in a multitask sense? For example, combining segmentation and heat-maps from your loss?
2. What happens if you try spatial softmax for the heat-map instead of sigmoid?
3. In a multi-class problem where number of GT points are known and fixed, could you potentially make N heat-maps for N points, softmax across channels and expect your solution to work?"
"Hi,

Great work. Might be including in future papers of mine. Found one issue:
https://github.com/javiribera/locating-objects-without-bboxes/blob/master/object-locator/losses.py

Might help if you explicitly mentioned that in the case for 1 GT point, be sure to provide it as list of a [1, 2] tensors. This shape is consistent with your code and would lead to failure if people provided GT points as [2, ] or [2, 1].

Further, I suggest removed ""device"" dependency during function definition and instead consider something like this `self.all_img_locations = self.all_img_locations.to(prob_map.device)`

Would help future users who train on cluster."
How can we calculate number of points?
"Hi,
The trained network detects objects only in those images which have the same dimensions as of the training images. Can it be modified to detect objects in test images with different dimensions? "
Could you provide a list of required modifications in order for this to work for multiple classes?
"Hello, I am having the same issue as #9 and #11 , which were both closed due to inactivity. Is the only suggested solution to use this on a Unix machine? I am running this on Windows and currently do not have access to a Unix machine.

This the command that I used:  
python -m object-locator.train --train-dir training --val-dir auto --epochs 100 --no-cuda --save lobes.ckpt --visdom-server http://localhost --visdom-port 8097 --visdom-env main

The full error message can be found here: https://pastebin.com/gvcTXYxV

I get the same error when trying to run locate.py with this command: 
python -m  object-locator.locate --dataset mall_dataset --out output --model mall,lambdaa=1,BS=32,Adam,LR1e-4.ckpt --no-cuda"
"I know there's been many issues related with validation time.
But to me, it's not clear.

I am training mall dataset and set --taus as default= -2 since it's faster than other ways.
It still took about 4.5s per image for validation.
I saw one posting that you said 'Only the neural network can use GPU. The thresholding and EM can only use CPU'.
does that mean validation only has to be worked with CPU? Can I know  the reason? 

Thank you"
"Hello,

I tried to run your codes according README.md .
But I can't solve about mall_dataset problem.

Your 'readme' suggests that I should have well-formatted gt.csv file. [at here](https://github.com/javiribera/locating-objects-without-bboxes#datasetformat)
and repos contains some '.txt to .csv' python code.

But mall dataset link's zip file has only .mat ground truth file.

Of course, It'll be fine if i spend more time adjusting .mat file...!,  but I wondered there is another way to solve this problem or something I missed! Thank you :)"
"Hi,
I am using this commit version: commit d8485608c2625d675e61dfd692da675e8dde4225

I tried to train using this train command:  python -m object-locator.train  --train-dir real_train  --batch-size 32  --visdom-env mytrainingsession  --visdom-server localhost  --lr 1e-3  --val-dir real_train  --optim Adam  --save saved_model.ckpt

got this error:
https://pastebin.com/JMbjWZPn

i attached the gt.csv file
[gt.xlsx](https://github.com/javiribera/locating-objects-without-bboxes/files/4707593/gt.xlsx)


thanks in advance!
"
"After I have trained my model, how can I test my model, I only have images without gt(csv or xml).Thank you!"
"Hello!

Several problems appear when I use your codes.

First of all, my data are 4000x6000 drone images, previously cut into 400x400, of animals.
I work with Windows 10 on Anaconda prompt, with a 4GB GPU.

I run a training with this command:
`python -m object-locator.train --train-dir mytraindir --batch-size 1 --visdom-env training --visdom-server localhost --visdom-port 8097 --epochs 2 --lr 1e-3 --val-dir myvaldir --save otherdir\saved_model.ckpt --nThreads 1 --imgsize 400x400`

Training is going well. However, an error occurs during the validation, at each loaded image:
`C:\Users\delplaal\AppData\Local\Continuum\anaconda3\envs\object-locator\lib\site-packages\object-locator\utils.py:86: RuntimeWarning: invalid value encountered in true_divide
  array_scaled = ((array - minn)/(maxx - minn)*255)`

It seems that the denominator is zero, but I do not know where that comes from.

Finally, at the end of this pseudo-validation, this error appears:
`E: Don't overwrite a checkpoint without resuming from it. Are you sure you want to do that? (if you do, remove it manually).`

And I get a broken pipe. So I can't do more than one epoch. 

I also tried without validation and I get this error:
`File ""C:\Users\delplaal\AppData\Local\Continuum\anaconda3\envs\object-locator\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\delplaal\AppData\Local\Continuum\anaconda3\envs\object-locator\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\delplaal\AppData\Local\Continuum\anaconda3\envs\object-locator\lib\site-packages\object-locator\train.py"", line 273, in <module>
    if args.save and (epoch + 1) % args.val_freq == 0:
ZeroDivisionError: integer division or modulo by zero`

I'm stuck. What can I do to fix this?

Here is a sample of my dataset: https://we.tl/t-ybaoqdEqha

Thank you in advance for your answers!

Alexandre

"
"As specified in the paper, you did a train/val/test split on the mall dataset (2000 frames) of 80/10/10 which would give 200 testing frames. How long should it take to locate all the objects for those 200 images? After using a script to turn the .mat file into train/val/test datasets with the proper ground truth files and using the command 
`python -m object-locator.locate --dataset mall_dataset/mall_test/ --out mall_out/ --model mall.ckpt --evaluate`
it takes **~1.5 hours** to run on the entire testing set which is much longer than the training/validation iterations. I'm just running it locally on an i5-6600k and a 1070. Training was taking about 1.5 minutes per epoch and validation was around 5 minutes which makes me wonder what I could be doing wrong for testing. Perhaps this is expected, but I just wanted to check.

Thanks in advance!"
"The code ran for 1800 epochs. The loss didn't get lower anymore. The object locations in training look too wide. I used the following command with the parameters. Could you guide me to improve the accuracy of the object locations?

python -m object-locator.train --train-dir ""data/mall_small_dataset"" --batch-size 10 --lr 1e-6 --val-dir  ""data/mall_small_validate"" --optim adam --val-freq 10  --save ""data/mall_small_dataset-model.ckpt"" --visdom-env mall_500_v2_dataset_training  --visdom-server http://localhost --visdom-port 8097 --imgsize 256x320 --resume ""data/mall_small_dataset-model.ckpt""

![Screenshot from 2020-01-31 15-11-31](https://user-images.githubusercontent.com/14261061/73571496-ada1a580-443c-11ea-8977-20941f3b9e26.png)

Thank you"
"Hi, very excited reading your paper and code.
The loss function containing weighted Hausdorff distance was designed for object detection task in this paper, can weighted Hausdorff distance be directly introduced into semantic segmentation task?

Thank you very much!"
"
[mall_small_dataset.zip](https://github.com/javiribera/locating-objects-without-bboxes/files/4134280/mall_small_dataset.zip)
![seq_000001](https://user-images.githubusercontent.com/14261061/73456186-726d7c80-433f-11ea-89a6-1bf6b678f2d1.jpg)

I have created a smaller dataset to see the training of the network. For faster training, the mall images were cropped to 1/2 in width and height and only 500 images were fed into the training (see the attached dataset, a ground-truth image, and a Visdom screenshot at 1200 epochs). The following command was used for training:

---mall_small_dataset command for training
python -m object-locator.train --train-dir ""data/mall_small_dataset"" --batch-size 16 --lr 1e-4 --val-dir  ""data/mall_small_dataset"" --optim adam --val-freq 10  --save ""data/mall_small_dataset-model.ckpt"" --visdom-env mall_small_dataset_training  --visdom-server http://localhost --visdom-port 8097

The code performed 1200 epochs overnight, but it still hasn't converged to object locations yet. Could you give me guidance to resolve the problem?


![Screenshot from 2020-01-30 08-51-53](https://user-images.githubusercontent.com/14261061/73456714-5b7b5a00-4340-11ea-8e19-a73c4e2c075c.png)
  
Thank you"
"When I run train.py on my own data, it will cost a long time when validates with with very low gpu untils,I wanna know why  and my data is 1000x1000 about thousands of object on each image "
"I implemented a command below 

` python -m object-locator.locate --dataset data --out result --model pupil,lambdaa=1,BS=64,SGD,LR1e-3,p=-1,ultrasmallNet.ckpt --ultrasmallnet
`

and got an error as follows:


`W: The dataset directory data does not contain a CSV file with groundtruth.
   Metrics will not be evaluated. Only estimations will be returned.
Loading checkpoint...
\__ loaded checkpoint 'pupil,lambdaa=1,BS=64,SGD,LR1e-3,p=-1,ultrasmallNet.ckpt' with 6.02M trainable parameters
DONE (took 0.076988 seconds)
  0%|                                                                                                           | 0/1 [00:00<?, ?it/s]<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=612x408 at 0x7FF904D587B8>
Traceback (most recent call last):
  File ""/opt/conda/envs/object-locator/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/conda/envs/object-locator/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/tmp/locating-objects-without-bboxes/object-locator/locate.py"", line 186, in <module>
    total=len(testset_loader)):
  File ""/opt/conda/envs/object-locator/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 940, in __iter__
    for obj in iterable:
  File ""/opt/conda/envs/object-locator/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 637, in __next__
    return self._process_next_batch(batch)
  File ""/opt/conda/envs/object-locator/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 658, in _process_next_batch
    raise batch.exc_type(batch.exc_msg)
KeyError: 'Traceback (most recent call last):\n  File ""/opt/conda/envs/object-locator/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File ""/opt/conda/envs/object-locator/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File ""/tmp/locating-objects-without-bboxes/object-locator/data.py"", line 291, in __getitem__\n    dictionary[\'locations\'] = eval(dictionary[\'locations\'])\nKeyError: \'locations\'\n'
^X^C`


There are any solution? 


My Directory hierarchy is

<img width=""434"" alt=""test"" src=""https://user-images.githubusercontent.com/22805195/71061162-c87ccb00-21aa-11ea-9030-92494d0f05d9.png"">


Ground Truth file gt.csv includes 

<img width=""254"" alt=""gt"" src=""https://user-images.githubusercontent.com/22805195/71061168-cadf2500-21aa-11ea-92b6-b66ad8e9a7d5.png"">


Thanks."
"I use the pre-trained model you gave (ShanghaitechB),and get a pretty good results ,but I use the example you gave about train.py, and try to train the dataset (ShanghaitechB).
But I get totally different results.  Your pre-trained get good results, but my model can not get good results. the esitimated_map are  full of yellow . It can not locate anyone! No matter which epoch I choose(100,500,1000),results are same. Full of yellow blocks"
"the train.py and locate.py are runing .new problem occurs.I found gpu is used when training.but code gets slow when validation.why gpu is not be used when validation.
when locate.py is runing ,it has the same problem with the validation.I am sure my gpu is usable and I have inputed right arguments"
"usage: train.py [-h] --train-dir TRAIN_DIR [--val-dir VAL_DIR] [--imgsize HxW]
                [--batch-size N] [--epochs N] [--nThreads N] [--lr LR] [-p P]
                [--no-cuda] [--no-data-augm] [--drop-last-batch] [--seed S]
                [--resume PATH] [--save PATH] [--log-interval N]
                [--max-trainset-size N] [--max-valset-size N] [--val-freq F]
                [--visdom-env NAME] [--visdom-server SRV] [--visdom-port PRT]
                [--optimizer OPTIM] [--replace-optimizer] [--max-mask-pts M]
                [--paint] [--radius R] [--n-points N] [--ultrasmallnet]
                [--lambdaa L]
train.py: error: unrecognized arguments: --env-name sorghum

code have no argument name ""--env-name"",but you  write it in your example"
"(object_loader) D:\google_download\locating-objects-without-bboxes-master>python -m object-locator.train --train-dir D:/_PNGImages --batch-size 32 --lr 1e-3 --val-dir D:/_PNGImages --optim Adam --save saved_model.ckpt
W: Not connected to any Visdom server. You will not visualize any training/validation plot or intermediate image
replace ballpark
Building network... DONE (took 0.363471 seconds)
Epoch 0 (420 images):   0%|                                                                                                                                                                                                                                     | 0/14 [00:00<?, ?it/s]W
: Not connected to any Visdom server. You will not visualize any training/validation plot or intermediate image
replace ballpark
Building network... DONE (took 0.365666 seconds)
Epoch 0 (420 images):   0%|                                                                                                                                                                                                                                     | 0/14 [00:00<?, ?it/s]T
raceback (most recent call last):
Traceback (most recent call last):
  File ""D:\anaconda\envs\object_loader\lib\runpy.py"", line 193, in _run_module_as_main
  File ""<string>"", line 1, in <module>
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\spawn.py"", line 105, in spawn_main
        ""__main__"", mod_spec)
exitcode = _main(fd)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\spawn.py"", line 114, in _main
  File ""D:\anaconda\envs\object_loader\lib\runpy.py"", line 85, in _run_code
        prepare(preparation_data)exec(code, run_globals)

  File ""D:\google_download\locating-objects-without-bboxes-master\object-locator\train.py"", line 170, in <module>
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\spawn.py"", line 223, in prepare
        for batch_idx, (imgs, dictionaries) in enumerate(iter_train):
_fixup_main_from_name(data['init_main_from_name'])  File ""D:\anaconda\envs\object_loader\lib\site-packages\tqdm\_tqdm.py"", line 940, in __iter__

  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\spawn.py"", line 249, in _fixup_main_from_name
    alter_sys=True)
  File ""D:\anaconda\envs\object_loader\lib\runpy.py"", line 205, in run_module
    return _run_module_code(code, init_globals, run_name, mod_spec)
  File ""D:\anaconda\envs\object_loader\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""D:\anaconda\envs\object_loader\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\google_download\locating-objects-without-bboxes-master\object-locator\train.py"", line 170, in <module>
    for batch_idx, (imgs, dictionaries) in enumerate(iter_train):
  File ""D:\anaconda\envs\object_loader\lib\site-packages\tqdm\_tqdm.py"", line 940, in __iter__
    for obj in iterable:
  File ""D:\anaconda\envs\object_loader\lib\site-packages\torch\utils\data\dataloader.py"", line 278, in __iter__
    for obj in iterable:
  File ""D:\anaconda\envs\object_loader\lib\site-packages\torch\utils\data\dataloader.py"", line 278, in __iter__
        return _MultiProcessingDataLoaderIter(self)
  File ""D:\anaconda\envs\object_loader\lib\site-packages\torch\utils\data\dataloader.py"", line 682, in __init__
return _MultiProcessingDataLoaderIter(self)
  File ""D:\anaconda\envs\object_loader\lib\site-packages\torch\utils\data\dataloader.py"", line 682, in __init__
    w.start()
    w.start()  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\process.py"", line 105, in start

  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\process.py"", line 105, in start
        self._popen = self._Popen(self)self._popen = self._Popen(self)

  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\context.py"", line 223, in _Popen
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\context.py"", line 322, in _Popen
        return Popen(process_obj)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
return _default_context.get_context().Process._Popen(process_obj)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\popen_spawn_win32.py"", line 33, in __init__
    reduction.dump(process_obj, to_child)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\reduction.py"", line 60, in dump
    prep_data = spawn.get_preparation_data(process_obj._name)
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
    _check_not_importing_main()
  File ""D:\anaconda\envs\object_loader\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe
"
"Hi,

I was reading the help information of command `python3.6 -m object-locator.locate -h` offered me when I came across 

> --dataset DATASET  &#160;&#160; Directory with test images. Must contain image files 
&#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; (any format), and CSV or XML file containing a
&#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160;groundtruth file following the API v0.4 (default:
&#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160; &#160;&#160;None)

As far as I know, the groudtruth in mall dataset is a `.mat` file which is  unable to use directly.
I have no clue about what '**API v0.4**' refers to. Is there any documents that I can catch up with? 

Thanks."
"(object-locator) D:\2019\wildandfields\Locating Objects Without Bounding Boxes\locating-objects-without-bboxes-master>python -m object-locator.locate --dataset 20160613_F54_validation --out output --model ""checkpoints\plants_20160613_F54,BS=32,Adam,LR1e-5,p=-1.ckpt"" --evaluate  --no-gpu  --radii 5
Loading checkpoint...
\__ loaded checkpoint 'checkpoints\plants_20160613_F54,BS=32,Adam,LR1e-5,p=-1.ckpt' with 64.8M trainable parameters
DONE (took 3.866121 seconds)
Loading checkpoint...
\__ loaded checkpoint 'checkpoints\plants_20160613_F54,BS=32,Adam,LR1e-5,p=-1.ckpt' with 64.8M trainable parameters
DONE (took 3.257291 seconds)
Traceback (most recent call last):
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""D:\sofewarespace\envs\object-locator\lib\runpy.py"", line 193, in _run_module_as_main
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    ""__main__"", mod_spec)
  File ""D:\sofewarespace\envs\object-locator\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\2019\wildandfields\Locating Objects Without Bounding Boxes\locating-objects-without-bboxes-master\object-locator\locate.py"", line 186, in <module>
    exitcode = _main(fd)
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\spawn.py"", line 114, in _main
    prepare(preparation_data)
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\spawn.py"", line 223, in prepare
    _fixup_main_from_name(data['init_main_from_name'])
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\spawn.py"", line 249, in _fixup_main_from_name
        alter_sys=True)for batch_idx, (imgs, dictionaries) in tqdm(enumerate(testset_loader),

  File ""D:\sofewarespace\envs\object-locator\lib\runpy.py"", line 205, in run_module
  File ""D:\sofewarespace\envs\object-locator\lib\site-packages\torch\utils\data\dataloader.py"", line 819, in __iter__
    return _run_module_code(code, init_globals, run_name, mod_spec)
  File ""D:\sofewarespace\envs\object-locator\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""D:\sofewarespace\envs\object-locator\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""D:\2019\wildandfields\Locating Objects Without Bounding Boxes\locating-objects-without-bboxes-master\object-locator\locate.py"", line 186, in <module>
    return _DataLoaderIter(self)
    for batch_idx, (imgs, dictionaries) in tqdm(enumerate(testset_loader),
  File ""D:\sofewarespace\envs\object-locator\lib\site-packages\torch\utils\data\dataloader.py"", line 560, in __init__
  File ""D:\sofewarespace\envs\object-locator\lib\site-packages\torch\utils\data\dataloader.py"", line 819, in __iter__
        w.start()return _DataLoaderIter(self)

  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\process.py"", line 105, in start
  File ""D:\sofewarespace\envs\object-locator\lib\site-packages\torch\utils\data\dataloader.py"", line 560, in __init__
    w.start()
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\process.py"", line 105, in start
        self._popen = self._Popen(self)self._popen = self._Popen(self)

  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\context.py"", line 223, in _Popen
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\context.py"", line 223, in _Popen
        return _default_context.get_context().Process._Popen(process_obj)return _default_context.get_context().Process._Popen(process_obj)

  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\context.py"", line 322, in _Popen
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
return Popen(process_obj)  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\popen_spawn_win32.py"", line 33, in __init__

  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
        reduction.dump(process_obj, to_child)prep_data = spawn.get_preparation_data(process_obj._name)

  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\reduction.py"", line 60, in dump
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\spawn.py"", line 143, in get_preparation_data
    _check_not_importing_main()
  File ""D:\sofewarespace\envs\object-locator\lib\multiprocessing\spawn.py"", line 136, in _check_not_importing_main
    is not going to be frozen to produce an executable.''')
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.
    ForkingPickler(file, protocol).dump(obj)
BrokenPipeError: [Errno 32] Broken pipe"
"Collecting ballpark==1.4.0 (from -r /test/locating-objects-without-bboxes-master/condaenv.ivbc642u.requirements.txt (line 1))
  Cache entry deserialization failed, entry ignored
  Using cached https://files.pythonhosted.org/packages/8f/5b/e259db671525c63202885c2cad5fc90cf095a65149a2ad3a7586fa73180f/ballpark-1.4.0.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-build-fxtjjwiz/ballpark/setup.py"", line 5, in <module>
        long_description=open('README.rst').read(),
      File ""/root/anaconda3/envs/object-locator/lib/python3.6/encodings/ascii.py"", line 26, in decode
        return codecs.ascii_decode(input, self.errors)[0]
    UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 650: ordinal not in range(128)

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-fxtjjwiz/ballpark/

CondaValueError: pip returned an error
"
"Hi sir,
I want to train the model in mall dataset with your code.
After downloading the dataset, the GT file is a mat format.
Can you provide the XML file for each dataset?

The error log is ：
(object-locator) imre@imre2018-09-01:~/baiyan/locating-objects-without-bboxes$ python -m object-locator.train    --train-dir ./mall_dataset/frames/        --batch-size 32        --visdom-env sorghum        --lr 1e-3        --val-dir ./mall_dataset/frames/        --optim Adam        --save saved_model.ckpt
W: Not connected to any Visdom server. You will not visualize any training/validation plot or intermediate image
W: The dataset directory ./mall_dataset/frames/ does not contain a XML file with groundtruth. Metrics will not be evaluated.Only estimations will be returned.
Traceback (most recent call last):
  File ""/home/imre/anaconda2_v2/envs/object-locator/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/imre/anaconda2_v2/envs/object-locator/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/imre/baiyan/locating-objects-without-bboxes/object-locator/train.py"", line 93, in <module>
    max_valset_size=args.max_valset_size)
  File ""/home/imre/baiyan/locating-objects-without-bboxes/object-locator/data.py"", line 131, in get_train_val_loaders
    seed=seed)
  File ""/home/imre/baiyan/locating-objects-without-bboxes/object-locator/data.py"", line 66, in build_dataset
    seed=seed)
  File ""/home/imre/baiyan/locating-objects-without-bboxes/object-locator/data_plant_stuff.py"", line 93, in __init__
    with open(os.path.join(directory, xml_filename), 'r') as fd:
  File ""/home/imre/anaconda2_v2/envs/object-locator/lib/python3.6/posixpath.py"", line 94, in join
    genericpath._check_arg_types('join', a, *p)
  File ""/home/imre/anaconda2_v2/envs/object-locator/lib/python3.6/genericpath.py"", line 149, in _check_arg_types
    (funcname, s.__class__.__name__)) from None
TypeError: join() argument must be str or bytes, not 'NoneType'
"
"when I don't have xml file its fail in at line  93, data_plant_stuff.py
when trying to read None file :

        # Read all XML as a string
        with open(os.path.join(directory, xml_filename), 'r') as fd:
            xml_str = fd.read()
"
"When using the plant dataset, `object-locator/data.py` should have the following changes to prevent KeyErrors:

`dictionary['locations'] = eval(dictionary['locations'])`
to
`dictionary['locations'] = eval(dictionary['plant_locations'])`

and

`dictionary['count'] = torch.tensor([dictionary['count']], dtype=torch.get_default_dtype())`
to
`dictionary['count'] = torch.tensor([dictionary['plant_count']], dtype=torch.get_default_dtype())`

Didn't make a PR cause I thought you'd probably like to change it in some way that generalizes for all datasets that I am not aware of.

Cheers, thanks for the great repo!"
""
Hi there I did not find a repo with tag used-for-cvpr2019-submission
"Hi, Thanks for providing the code. It is mentioned in the code that a pre-trained model comes with this package. The pre-trained model ""unet_256x256_sorghum.ckpt"" is not available in checkpoints folder. I was trying to test the code initially on the provided test set. It is giving an error on this file while testing. Should I train it again with any data set first then?"
"thank you very much for sharing the codes.

I have some questions about the Averaged Hausdorff Loss. currently i am trying to solve a boundary detection problem based on medical image dataset. I tried to use your codes for AveragedHausdorffLoss, however inputs of your function are two point sets while my inputs are 2-class softmax probability map and ground truth labels.  The critical issue here is that  I have to calculate set1 from the probability map using torch.max() while the torch.max() function is not differentiable and thus not able to be pack-propagated. 

My question is do u know some methods to avoid the 'not being able to be back-propagated' problem or other implementations which directly use the prob map to calculate loss.

Best regards!"
"Hi, I am unable to download checkpoints, the link is not working. Could you please check and update the link for download
"
"How can I train your method on a dataset you didn't consider?

I want to compare your method with mine. Is there a way to replicate exactly your training on my dataset?"
"what is wrong with it？
best wishes"
"Hi, thank you for sharing your code about the LSA model. Having trained the full model, I'm wondering how I can sample from the LSA? Can I just draw a uniform random vector as z and put it into the CPD estimator?"
"I am trying to do training but the loss function increases rather than decrease 

I have attached the following training part
______________________________________________________code start___________________
 for cl_idx, video_id in enumerate(dataset.train_videos):
        
            # Run the train video 
            dataset.train(video_id)
            loader = DataLoader(dataset, collate_fn=dataset.collate_fn)

            # Build score containers
            #sample_llk = np.zeros(shape=(len(loader) + t - 1,))
            #sample_rec = np.zeros(shape=(len(loader) + t - 1,))
            ##uploading the ground truth
            #sample_y = dataset.load_test_sequence_gt(video_id)
            for i, (x, y) in tqdm(enumerate(loader), desc=f'Computing scores for {dataset}'):
                optimizer.zero_grad()
                x = x.to('cuda')
                # Forward pass, get our logits then backward pass, then update weights
                x_r, z, z_dist = model(x)
                 # Calculate the joint loss 
                loss=criterion(x, x_r, z, z_dist)
                #print(loss)
                ## do backward and the update
                loss.backward()
                optimizer.step()
                running_loss += loss.item()  
                print (running_loss)"
"Hello, thank you for your work, can you share your training codes?"
"Hi,  thanks for your work. Could you give more the following details about the training on ShanghaiTech?
"
"Hi，thanks for your work。When I train it ,I get this error。

Traceback (most recent call last):
  File ""train.py"", line 201, in <module>
    main()
  File ""train.py"", line 192, in main
    train_ucsdped2()
  File ""train.py"", line 128, in train_ucsdped2
    x_r, z, z_dist = model(x)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/LSA_ucsd.py"", line 189, in forward
    z = self.encoder(h)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/LSA_ucsd.py"", line 62, in forward
    h = self.conv(h)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/container.py"", line 91, in forward
    input = module(input)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/blocks_3d.py"", line 133, in forward
    activation_fn=self._activation_fn
  File ""/home/dl/VSST/dm/novelty-detection-master/models/blocks_3d.py"", line 33, in residual_op
    ha = f1(ha)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/base.py"", line 33, in __call__
    return super(BaseModule, self).__call__(*args, **kwargs)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/dl/VSST/dm/novelty-detection-master/models/layers/mconv3d.py"", line 29, in forward
    return super(MaskedConv3d, self).forward(x)
  File ""/home/dl/anaconda3/envs/pytorch0.4/lib/python3.7/site-packages/torch/nn/modules/conv.py"", line 421, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected 5-dimensional input for 5-dimensional weight [8, 1, 3, 3, 3], but got input of size [105, 690, 1, 8, 32, 32] instead
"
"`from functools import reduce
from operator import mul
from typing import Tuple
import numpy as np
import torch
import torchvision
import torch.nn as nn
from models.loss_functions.lsaloss import LSALoss
from models.base import BaseModule
from models.blocks_2d import DownsampleBlock
from models.blocks_2d import ResidualBlock
from models.blocks_2d import UpsampleBlock
from models.estimator_1D import Estimator1D
import cv2

class Encoder(BaseModule):
    """"""
    CIFAR10 model encoder.
    """"""
    def __init__(self, input_shape, code_length):
        # type: (Tuple[int, int, int], int) -> None
        """"""
        Class constructor:

        :param input_shape: the shape of CIFAR10 samples.
        :param code_length: the dimensionality of latent vectors.
        """"""
        super(Encoder, self).__init__()

        self.input_shape = input_shape
        self.code_length = code_length

        c, h, w = input_shape

        print (c,h,w)

        activation_fn = nn.LeakyReLU()

        # Convolutional network
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=3, bias=False),
            activation_fn,
            ResidualBlock(channel_in=32, channel_out=32, activation_fn=activation_fn),
            DownsampleBlock(channel_in=32, channel_out=64, activation_fn=activation_fn),
            DownsampleBlock(channel_in=64, channel_out=128, activation_fn=activation_fn),
            DownsampleBlock(channel_in=128, channel_out=256, activation_fn=activation_fn),
        )
        self.deepest_shape = (256, h // 8, w // 8)

        # FC network
        self.fc = nn.Sequential(
            nn.Linear(in_features=reduce(mul, self.deepest_shape), out_features=256),
            nn.BatchNorm1d(num_features=256),
            activation_fn,
            nn.Linear(in_features=256, out_features=code_length),
            nn.Sigmoid()
        )

    def forward(self, x):
        # types: (torch.Tensor) -> torch.Tensor
        """"""
        Forward propagation.

        :param x: the input batch of images.
        :return: the batch of latent vectors.
        """"""
        h = x
        print (type(h))
        h = self.conv(h)
        h = h.view(len(h), -1)
        o = self.fc(h)

        return o


class Decoder(BaseModule):
    """"""
    CIFAR10 model decoder.
    """"""
    def __init__(self, code_length, deepest_shape, output_shape):
        # type: (int, Tuple[int, int, int], Tuple[int, int, int]) -> None
        """"""
        Class constructor.

        :param code_length: the dimensionality of latent vectors.
        :param deepest_shape: the dimensionality of the encoder's deepest convolutional map.
        :param output_shape: the shape of CIFAR10 samples.
        """"""
        super(Decoder, self).__init__()

        self.code_length = code_length
        self.deepest_shape = deepest_shape
        self.output_shape = output_shape

        print (self.output_shape,""--"")

        activation_fn = nn.LeakyReLU()

        # FC network
        self.fc = nn.Sequential(
            nn.Linear(in_features=code_length, out_features=256),
            nn.BatchNorm1d(num_features=256),
            activation_fn,
            nn.Linear(in_features=256, out_features=reduce(mul, deepest_shape)),
            nn.BatchNorm1d(num_features=reduce(mul, deepest_shape)),
            activation_fn
        )

        # Convolutional network
        self.conv = nn.Sequential(
            UpsampleBlock(channel_in=256, channel_out=128, activation_fn=activation_fn),
            UpsampleBlock(channel_in=128, channel_out=64, activation_fn=activation_fn),
            UpsampleBlock(channel_in=64, channel_out=32, activation_fn=activation_fn),
            ResidualBlock(channel_in=32, channel_out=32, activation_fn=activation_fn),
            nn.Conv2d(in_channels=32, out_channels=3, kernel_size=1, bias=False)
        )

    def forward(self, x):
        # types: (torch.Tensor) -> torch.Tensor
        """"""
        Forward propagation.

        :param x: the batch of latent vectors.
        :return: the batch of reconstructions.
        """"""
        h = x
        h = self.fc(h)
        h = h.view(len(h), *self.deepest_shape)
        h = self.conv(h)
        o = h

        return o


class LSACIFAR10(BaseModule):
    """"""
    LSA model for CIFAR10 one-class classification.
    """"""
    def __init__(self,  input_shape, code_length, cpd_channels):
        # type: (Tuple[int, int, int], int, int) -> None
        """"""
        Class constructor.

        :param input_shape: the shape of CIFAR10 samples.
        :param code_length: the dimensionality of latent vectors.
        :param cpd_channels: number of bins in which the multinomial works.
        """"""
        super(LSACIFAR10, self).__init__()

        self.input_shape = input_shape
        self.code_length = code_length

        # Build encoder
        self.encoder = Encoder(
            input_shape=input_shape,
            code_length=code_length
        )

        # Build decoder
        self.decoder = Decoder(
            code_length=code_length,
            deepest_shape=self.encoder.deepest_shape,
            output_shape=input_shape
        )

        # Build estimator
        self.estimator = Estimator1D(
            code_length=code_length,
            fm_list=[32, 32, 32, 32],
            cpd_channels=cpd_channels
        )

    def forward(self, x):
        # type: (torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
        """"""
        Forward propagation.

        :param x: the input batch of images.
        :return: a tuple of torch.Tensors holding reconstructions, latent vectors and CPD estimates.
        """"""
        h = x

        # Produce representations
        z = self.encoder(h)

        # Estimate CPDs with autoregression
        z_dist = self.estimator(z)

        # Reconstruct x
        x_r = self.decoder(z)
        # print (x_r.shape)
        x_r = x_r.view(-1, *self.input_shape)

        return x_r, z, z_dist


def load_dataset(data_path=""/home/jbmai/Downloads/Defect Images-20190705T133320Z-001""):
    # data_path = 'data/train/'
# torchvision.transforms.Grayscale(num_output_channels=1)
    trainTransform  = torchvision.transforms.Compose([
                                    torchvision.transforms.Resize(size=(128,128), interpolation=2),
                                    torchvision.transforms.ToTensor(), 
                                    ])




    train_dataset = torchvision.datasets.ImageFolder(
        root=data_path,
        transform=trainTransform)
    

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=64,
        num_workers=0,
        shuffle=True
    )
    return train_loader


net = LSACIFAR10(input_shape=[3,128,128],code_length = 32,cpd_channels =100)
lossFunction = LSALoss(cpd_channels=100)
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)




try:

    checkpoint = torch.load(""savedWeights/enc.pth"")
    net.encoder.load_state_dict(checkpoint)

    checkpoint = torch.load(""savedWeights/est.pth"")
    net.estimator.load_state_dict(checkpoint)

    checkpoint = torch.load(""savedWeights/dec.pth"")
    net.decoder.load_state_dict(checkpoint)


except Exception as e:
    print (e)


for epoch in range(1000):  # loop over the dataset multiple times

    running_loss = 0.0
    d = load_dataset()
    for i, (data,l) in enumerate(d):
        # get the inputs; data is a list of [inputs, labels]
        
        # print (data.shape)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        # print (data)
        x_r,z,z_dist = net.forward(data)
        # print (x_r.shape)
        # print(data.shape)
        loss = lossFunction(data,x_r,z,z_dist)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 5 == 0:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 5))
            running_loss = 0.0
    if (epoch % 5)== 0 :
        # print (""--------------------{} epoch-----------"".format(epoch))
        # net.encoder.eval()
        # net.estimator.eval()
        # net.decoder.eval()

        # z = net.encoder(data)
        # z_dist = net.estimator(z)
        # x_r = net.decoder(z).permute(0,2,3,1).detach().numpy()
        # out =x_r
        # print (type(out))
        # for i in range(out.shape[0]):
        #     # print (out.shape)
        #     # # out.permute(0,2,3,1)
        #     # print (out.shape)
        #     cv2.imwrite(""constructedImages/outDec{}_{}.jpg"".format(epoch,i),out[i,:,:,:]*255)
        #     # cv2.waitKey(0)
           
        #     # cv2.
        # net.encoder.train()
        # net.estimator.train()
        # net.decoder.eval()
        torch.save(net.encoder.state_dict(),(""savedWeights/enc.pth""))
        torch.save(net.estimator.state_dict(),(""savedWeights/est.pth""))
        torch.save(net.decoder.state_dict(),(""savedWeights/dec.pth""))

print('Finished Training')`

Output : 

<class 'torch.Tensor'>
[1,     1] loss: 727109273.600
<class 'torch.Tensor'>
[2,     1] loss: 2495627954514337382531072.000



Hi can you help me rectify the issue."
"I am trying to download the check point file for a few times and it all gave me the errors for ""curl: (56) Recv failure: Connection reset by peer"". Are the check points files still available for download?
"
"Hey, 

thanks for this great work and the release of the code.
I would like to train on my own data, but I can't find a train.py file. So I would like to ask, how can I train on my own data? "
"hello, thank you for your work, how to training the model with your code?"
"from torch.utils.data.dataloader import _use_shared_memory
from torch.utils.data.dataloader import int_classes
from torch.utils.data.dataloader import numpy_type_map
from torch.utils.data.dataloader import string_classes"
"Hi, I am replicating your experiment's results.  Could you give more the following details about the training on mnist and cifar10 which will replicate the results on your paper?

- lambda (trade-off between llk and reconstruction loss), is it fixed during training
- batch size
- optimizer, the learning rate
- when to stop training? If I use the validate set,  which needs to be monitored? reconstruction loss, or negative llk, or novelty score to monitor

Thanks a lot!
Best,
Gin"
"I find that there is no training code here. Can you provide training code?
thx very much!"
"Thank you for your awesome code!

I am hoping you might open-source the log files you have from training. Maybe the training and validation loss as a function of epoch
(and/or batch) with an estimate of the runtime?"
"Sir , your model is amazing and results are crisp, can we use it for for our commercial purpose"
请问python代码生成的加噪声图片为什么是单通道的呢？
"Hello, thank you for sharing. Can I get all 1000 patches of DND dataset from you?"
"Hi Guo Shi
In your paper, I read
""For a batch of real images, due to the unavailability of ground-truth noise level map, only L_rec and L_TV are considered in training.""
For real image, we have not clean image, so what is the meaning of ""x"" in loss function L_rec?
Thank you for answer!"
"您好：
为了试验您的方法有效性，我将它用来CT图像上进行噪声等级评估。请问您的噪声等级图可以用来评估ct图像的噪声吗，目前我用您的噪声水平评估方法去做，好像无法生成有效的噪声水平图，您能否提供一下您的噪声等级图可视化结果，这对我非常有帮助，谢谢！"
"Hi, I notice that the Poisson-Gaussian noise parameter are randomly sampled. How do you determine the 0.16 and 0.06 range for sigmas? I also read the implementation in the unprocessing paper, they use a quite smaller range (lower noise level).  "
"@GuoShi28  ,hi,I carefully read your literature and code, and download to try to run, but there is an error,could you help me correct the error?Thank you for reading my letter.
@![image](https://user-images.githubusercontent.com/65519597/97382575-a265a280-1906-11eb-924f-73896c00b776.png)

"
如何看模型的搭建以及训练和实现，只能download？
"I've read the Q&A about the JPG image issue. I am not quite understand the reason: after loading the image into memory, they are just array numbers, what are the differences between JPG's array(matrix) and PNG's array? 

I did a test to convert JPG back to PNG then feed to CBDNet, I can see the result is much better. I am wondering if this is a matplotlib.pyplot/pillow  issue? maybe it can not read the JPG data correctly? 

BTW, I am using this code since it provides me a easy to use interface: https://gitlab.com/Yggdrasyll/cbdnet-denoiser 

Here is the test result. 
Origin:
![crop1](https://user-images.githubusercontent.com/3990687/87220117-e6528d00-c393-11ea-8cce-ab31569d31c0.jpg)
Result directly processed from origin JPG image:
![FromJPG](https://user-images.githubusercontent.com/3990687/87220170-45180680-c394-11ea-9fbd-cc2d05800110.png)
Convert JPG to PNG by Mac's preview, then feed to CBDNet:
![fromJPGsPNG](https://user-images.githubusercontent.com/3990687/87220165-36315400-c394-11ea-9cb6-76463309e23c.png)

"
How to select noisy image from two noisy images in each batch ?
"don't have the clean  images of DND/NC12/Nam,how to calculate the PSNR value."
"Hello,thank you for your great work. I use your python code to gengerate noise image, but I can not  find the defination of noise map in your code at ISP_implement. I wonder to know how to get the noise map in this code.
https://github.com/GuoShi28/CBDNet/blob/724a88cd6af4125afc341b28ed4872004eb37e40/SomeISP_operator_python/ISP_implement.py#L401"
"Can I ask about the influence of perceptual loss used in your model? Specifically, the change of PSNR and SSIM?"
"Hi, thanks for the great work! 
I have a question about irradiance dependent noise. In the paper 
sigma^2 = L * sigma_s^2 + sigma_c^2, 
however in the source code (https://github.com/GuoShi28/CBDNet/blob/master/utils/AddNoiseMosai.m#L58)
noise_s_map = bsxfun(@times,permute(sigma_s,[3 1 2]),temp_x)
shouldn't sqrt(temp_x) be used? 

Thank you!"
"Functions CRF_Map and ICRF_Map are very slow in python (each function takes about 8 seconds when processing a 512x512 image)  and are bottlenecks in efficiency. 

I wonder where do you get the two .mat files used in CRF_map & ICRF_Map? Are they generated by formula or collected data? 

In my experience, we only need one lookup table to do gamma mapping, which should be ultra fast. So can we combine the two lookup tables in .mat files to make the algorithms faster?



"
"Hi I tried to reproduce the whole training procedure according to your paper
I set the level map as \sigma_c + \sigma_s*L and converted it to 0~1, and simply concat  the map with rgb image as the input of blind denoise net

The training process is ok and I can get a good result on sidd or etc, when the noise is not very high. 
However the denoised image is a little blurred, did you find this in your origin work?
Also when the noise getting higher the result gets worse, so I tried to give the noise level map a coefficient(x2) as mention in section 4.4 of the paper, but I found it just changed nothing. Even x2 gives barely any changes, which is more strange. I think if the noisy level map matters, giving a zero map should leads to a very bad denoise result right (as denoise ≈ input)?

In fact I've read the paper many times but I did't found how you take the noise map as input, am I right about concating it with the rgb image, or I just missed something? Also did you do any experiments about is? Like training a net without using the noise level map as a supervision (which means let LAMBasy = 0 and LAMBtv = 0), and compare the PSNR changes?"
"I try to use ./SomeISP_operator_python/ISP_implement.py to generate synthetic noisy image & ground truth image(gt), but in many images, i saw artifacts like this

![cbd_synthetic_issue_gt](https://user-images.githubusercontent.com/27390617/61270530-8d7a0400-a7d4-11e9-8c5f-cf8fc187f4ea.png)

and the raw srgb noisy-free image is like this

![cbd_synthetic_issue_clear](https://user-images.githubusercontent.com/27390617/61270570-a5518800-a7d4-11e9-9538-f33a08d9ee6d.png)

I wonder if these artifacts are reasonable in synthetic ISP process, and why these happens?
"
"I have downloaded your 'CBDNet.mat' file. However, I find the wrong message is that ""load -ASCII"" because the model is not the Binary file."
"how do you get your training model ,I use the training code of DnCNN in my own dataset ,there is some trouble in my training.How to solve it?

![image](https://user-images.githubusercontent.com/52694395/60869040-d247ed00-a260-11e9-86c1-87c28cd4e558.png)
"
"Hi, thanks for sharing the details, great work.

Will you release the training data you used for efficiently reproducing?

Thanks a lot!"
"@GuoShi28 
Hi,could you explain how to get the ground-truth noise level sigmas(yi)  ,as stated in your paper.             
 Thanks you very much!"
"In Table 2, the PSNR and SSIM results on the 15 cropped images provided by Nam et al. in CVPR 2016 are not consistent with the paper of Nam et al. (CVPR 2016), MCWNNM, TWSC, the method of NI (neat image software). How do you compute the PSNR and SSIM for Table 2?

Here are my PSNR results:

NI       & CC     & MCWNNM & TWSC & DnCNN+ &FFDNet+&CBDNet
35.68 & 38.37 & 41.13         & 40.76 & 38.02      & 39.35 & 36.68  
34.03 & 35.37 & 37.28         & 36.02 & 35.87      & 36.99 & 35.58  
32.63 & 34.91 & 36.52         & 34.99 & 35.51      & 36.50 & 35.27  
31.78 & 34.98 & 35.53         & 35.32 & 34.75      & 34.96 & 34.01  
35.16 & 35.95 & 37.02          & 37.10  & 35.28      & 36.70 & 35.19  
39.98 & 41.15  & 39.56         & 40.90 & 37.43      & 40.94 & 39.80  
34.84 & 37.99 & 39.26         & 39.23 & 37.63      & 38.62 & 38.03  
38.42 & 40.36 & 41.43         & 41.90 & 38.79      & 41.45 & 40.40
35.79 & 38.30 & 39.55         & 39.06 & 37.07      & 38.76 & 36.86 
38.36 & 39.01 & 38.91          & 40.03 & 35.45     & 40.09 & 38.75 
35.53 & 36.75 & 37.41          & 36.89 & 35.43     & 37.57 & 36.52
40.05 & 39.06 & 39.39         & 41.49 & 34.98     & 41.10 & 38.42 
34.08 & 34.61 & 34.80         & 35.47 & 31.12       & 34.11 & 34.13
32.13 & 33.21 & 33.95          & 34.05 & 31.93      & 33.64 & 33.45
31.52 & 33.22 & 33.94         & 33.88 & 31.79       & 33.68 & 33.45
Average 
35.33 & 36.88 & 37.71          & 37.81 & 35.40       & 37.63 & 36.44"
"Hey, thanks for your python implementation of the isp code.

I tried with the python isp code, and I found the result is really different with the matlab result.

I set the icrf_index and the pattern_index the same.

Does the python implementation is the same as the matlab?"
"can you share your dataset in training your model,there is  some trouble in my reading the matlab code,is there anyone convert it to tensorflow ?"
Are there some key points?
"hi @GuoShi28 
your paper said CBDNet takes about 0.4s to process an 512 × 512 image,
i was consider in what platform it execute?
have you comparison the execute time between CBDnet and BM3D?
ths a lot "
"Hi,
I run the test_patches and test_fullimage demos on the centos7 with the environment matconvnet and cudnn. The error is ""No property `ignoreAverage` for a layer of type `dagnn.Loss`.""
I have modified the path of models. Is there anything that I need to do before run the test file?"
">> Test_Patches
Warning: Name is nonexistent or not a directory: utilities 
> In path (line 109)
  In addpath (line 86)
  In Test_Patches (line 2) 
Error using dagnn.Layer/load (line 200)
No property `ignoreAverage` for a layer of type `dagnn.Loss`.
Error in dagnn.DagNN.loadobj (line 28)
      block.load(struct(s.layers(l).block)) ;
Error in Test_Patches (line 22)
net = dagnn.DagNN.loadobj(net) ; "
""
"您好：
目前我在寻找一种处理真实噪声的网络模型。您预训练的模型十分适用于我的数据集（医疗方向），去噪效果比较优异，但也有low-level上的细节损失，所以我想根据您分享的matlab训练代码用于我的数据集中。
拜读过您的论文以后我有几个问题想了解：
1)论文里您提到噪声估计网络中需要输入noisy observation y来产生noise level map σ_hat(y)，我是否可以理解为：noisy observation y = 带有真实噪声的原图，noisy level map σ_hat(y)是噪声的分布？
2)基于问题1)，CNN_D把CNN_E的输出作为输入，最终输出为去噪图像，那么整个CNN_D+CNN_E是不需要训练集的清晰图像？"
"Hey, I noticed that you mentioned in your paper that 'allowing the user to adjust γ'. 
But when I went through the codes, I didn't find the parameter γ and didn't know how to use it. Is it part of the training network? And is it set up when training the network? "
"Thanks for provide the python code of isp process.
I do not know which part in the ISP_implement.py correspond to the JPEG compression. .Can you tell me the answer?
Very thanks"
"Thanks for provide the python code of isp process, I just find that you add Poisson-Gaussian noise before mosaic_bayer in the matlab code, but you add the noise after mosaicing and inver white balancing in the python code. Is this difference reasonable?"
"Hi Guo Shi, 

I am trying to train the estimator network alone in the tensorflow and have few questions about training settings that I could not get answers from the paper of the GitHub page; would you please answer them?

1. In paper the patch size is mentioned to be 128x128 -- are these patches cropped from images at random? or they are cropped on a grid basis with a stride from each training image? 
2. How many number of batches are there in one epoch?
3. Are the 1600 images from each Waterloo and FiveK data sets are chosen at random befogging the training or they different in each epoch?
4. From paper it takes around 3 days to train full network, do you have any estimate for the estimator network -- since the estimator seems to be a fraction of the full CBDNet. 

Hoping to get a prompt response.
Thank you!
 


"
"Hi Guo Shi,

Can you please explain the UNet being used in CBDNet a little bit more. Few things are unclear from the description in the paper. 

Example:-
1. The original UNet paper uses max-pooling to reduce the size of the image, however it looks like in CBDNet it is done through convolution strides.
2. Also it is unclear at what level the features are concatenated -- shouldn't that be shown by increasing the width of green and yellow rectangles in figure2 of the paper e.g. that is how it was shown in Fig1 of original UNet paper cited in CBDNet.
3. Are the conv layers in UNet being padded -- otherwise the resolution would be different? 

Since the model is in MatConvNet and not everyone has access to Matlab. 

Thank you for your time!
Touqeer"
"Hi,
Great work and very good effort towards actual denoising instead of celebrating AWGN denoising as many others do :) 
I was curious as to why you did not continue using the strategy used in FFDNet where the image is downscaled to four quarter size images and concatenated with the noise map. 

Why not just replace the noise map (based on AWGN sigma) with the noise map generated by Noise Estimator Network? I was wanting to know if you gave FFDNet architecture a try or otherwise what was the reason to deviate from that towards UNet?

Thank you!
"
I found using 1 single gpu is very time consuming. But I have some trouble using multi-gpus with matconvnet. I wonder if you can show me how to modify the code to adapt multi-gpus settings?
"What is the ground truth for noise level map when JPEG compression is considered?
Is it still \sigma_c + L\sigma_s ?
Thank you"
"I suppose `CRF_Map` and `ICRF_Map` is somehow reversed (“写反了”） either revered in comments or in callee function `AddNoiseMosai`.

- Detail:
In `CRF_Map` comment, input image (RGB?) and returns L, while in `ICRF_Map` comment, input L and returns image.
But in `AddNoiseMosai`, I believe it inputs image and returns noised image. But it first calls `ICRF_Map` and then calls `CRF_Map` at last.

So I suppose your comment is wrong."
"Hi, it's an amazing work!
I wonder in your noise model (EQ 3), did you try to train the model with and without CRF? If we train CBDNet without the knowledge of CRF, how much gap will appear in real photo denoising?
I don't find any comparison among EQ 1, 2 and 3."
"In the paper you use various datasets (BSD500, Waterloo etc.) that have various sized images. For training 128x128 sized patches are used. Could you give information about the number of patches extracted (Like in DnCNN paper)?  So basically I want to know the dimensions of the training dataset ?x128x128x3. Also did you use any data augmentation methods?"
Hello. What is the receptive field of the denoising network (CNN_d)? My calculations yield 83. I used the info given in the network illustration and the fact that all filters are 3x3 (strided and transpose conv). Is this correct? I assumed that the receptive field would be as large as the training input (128x128)
"Hello, I noticed you used L2 norm instead of F norm in your paper. 
![image](https://user-images.githubusercontent.com/29296287/47551921-03cc9280-d936-11e8-9fa6-399364dfcde6.png)
![image](https://user-images.githubusercontent.com/29296287/47551955-13e47200-d936-11e8-80c9-f74e290ec835.png)
In the loss function, it seems the variable x and y are matrices because I haven't seen any sum operation.
Notably, with regard to vectors, L2 norm is equal to F norm. But for matrix, L2 norm is totally different from F norm. 
I want to confirm whether you used L2 norm of the matrix because papers often use F norm rather than L2 norm in image denoising. 
Can you help me? Thanks a lot!"
"Hi,

Thanks a lot for sharing your test code! 

I am trying to re-train using the same approach that you mentioned in the paper. I am not able to understand how the ground truth for the noise level map was generated for synthetic data (from the paper/code).

Can you please explain how you create the ground truth for the noise level map? For e.g., each pixel in the noise level map contains the std. dev. information in RGB domain or something else?

 If you can share the code that would be great as well. 

Thanks,
Tejas"
"Hello;
Could you explain to me what is the ground truth for your noise estimation network.
Thank you very much."
"Hi, is it possible to show one or two output image of the noise estimation network? For both synthesis image and real world image? Thanks."
我想从这几个测试模型里加一个vgg19的模型看看他的准确率，我想请问一下具体该怎么做
can you tell me the training epoch number before finetuning for imagenet 143 category and cifar datasets
"Thank you very much for your excellent work. When I ran the code you provided, I encountered some problems, several of which were not solved. I'd like to consult you.
1: What are the specific meanings of ""positive"" and ""negative"" in train.py?
2: In the paper, D_Loss = Ls + Lc1, G_Loss = Lc2 - Ls; but in the program, what I see is
Los_g = λL1 + (1-λ) L2, loss_d =λL1 + (1-λ) L2. Is the definition of loss function in the paper consistent with that in the program?
3: When I validate your program with other data sets, at the step of saving real images, the last four pictures are always black,such as 
![image](https://user-images.githubusercontent.com/46865749/63644819-f3d33a00-c723-11e9-8c05-26c1a6d11857.png). I can't find the reason. I hope you can give me some suggestions.
Thank you again for your help!"
"https://github.com/xuanqing94/RobGAN/blob/8a478cf3435387753baee2d3a82d039236cc4fab/acc_under_attack.py#L65

When evaluating, the test data should be used.

So the correct should be as following:

```python
data = CIFAR10(root=opt.root, train=False, download=False, transform=trans)
```"
"Hi, the PGD attack in your paper is different from original paper.
According to your code, the grad doesn't be dealing with sign function.

You can refer to https://github.com/Harry24k/adversarial-attacks-pytorch/blob/4c6613d19b2081a15b319aed798cdd7f811a8326/torchattacks/attacks/pgd.py#L73

```python

import torch
import torch.nn.functional as F
from torch.autograd import grad, Variable
from .linf_sgd import Linf_SGD

def attack_Linf_PGD(input_v, ones, label_v, dis, Ld, steps, epsilon):
    dis.eval()
    adverse_v = input_v.data.clone()
    adverse_v = Variable(adverse_v, requires_grad=True)
    optimizer = Linf_SGD([adverse_v], lr=0.0078)
    for _ in range(steps):
        optimizer.zero_grad()
        dis.zero_grad()
        d_bin, d_multi = dis(adverse_v)
        loss = -Ld(d_bin, ones, d_multi, label_v, lam=0.5)
        loss.backward()
        #print(loss.data[0])
        optimizer.step()
        diff = adverse_v.data - input_v.data
        diff.clamp_(-epsilon, epsilon)
        adverse_v.data.copy_((diff + input_v.data).clamp_(-1, 1))
    dis.train()
    dis.zero_grad()
    return adverse_v

def attack_Linf_PGD_bin(input_v, ones, dis, Ld, steps, epsilon):
    dis.eval()
    adverse_v = input_v.data.clone()
    adverse_v = Variable(adverse_v, requires_grad=True)
    optimizer = Linf_SGD([adverse_v], lr=0.0078)
    for _ in range(steps):
        optimizer.zero_grad()
        dis.zero_grad()
        d_bin = dis(adverse_v)
        loss = -Ld(d_bin, ones)
        loss.backward()
        #print(loss.data[0])
        optimizer.step()
        diff = adverse_v.data - input_v.data
        diff.clamp_(-epsilon, epsilon)
        adverse_v.data.copy_((diff + input_v.data).clamp_(-1, 1))
    dis.train()
    dis.zero_grad()
    return adverse_v

# performs FGSM attack, and it is differentiable
# @input_v: make sure requires_grad = True
def attack_FGSM(input_v, ones, label_v, dis, Lg):
    dis.eval()
    d_bin, d_multi = dis(input_v)
    loss = -Lg(d_bin, ones, d_multi, label_v, lam=0.5)
    g = grad(loss, [input_v], create_graph=True)[0]
    return input_v - 0.005 * torch.sign(g)


# performs Linf-constraint PGD attack w/o noise
# @epsilon: radius of Linf-norm ball
def attack_label_Linf_PGD(input_v, label_v, dis, steps, epsilon):
    dis.eval()
    adverse_v = input_v.data.clone()
    adverse_v = Variable(adverse_v, requires_grad=True)
    optimizer = Linf_SGD([adverse_v], lr=epsilon / 5)
    for _ in range(steps):
        optimizer.zero_grad()
        dis.zero_grad()
        _, d_multi = dis(adverse_v)
        loss = -F.cross_entropy(d_multi, label_v)
        loss.backward()
        #print(loss.data[0])
        optimizer.step()
        diff = adverse_v.data - input_v.data
        diff.clamp_(-epsilon, epsilon)
        adverse_v.data.copy_((diff + input_v.data).clamp_(-1, 1))
    dis.zero_grad()
    return adverse_v
```"
"What is the perturbation used for training? Are all the columns in the results, which displays testing at different perturbation, are for a single model? i.e, for all columns the model has same adversarial perturbation (what is it?)? 

Results for CIFAR-10 as in Table1 of paper (at: https://web.cs.ucla.edu/~chohsieh/papers/RobustGAN_CVPR_new.pdf) is:
<table>
<tr><th>perturbation</th>    <td>0 (nat acc)</td>   <td>0.02</td>    <td>0.04</td>           <td>0.08</td></tr>
<tr><th>Rob-GAN (w/FT)</th> <td>81.1%</td>      <td>70.41%</td>  <td>57.43%</td>    <td>30.25%</td></tr>
</table>

A bit more detail, about my Q:
- Paper mentions: δ max ∈ np.arange(0, 0.1, 0.01). What does this mean? Does it mean maximum perturbation is 0.1 and step size is 0.01? 
- Code seems to limit perturbation to ε. [[line 24](https://github.com/xuanqing94/RobGAN/blob/8a478cf3435387753baee2d3a82d039236cc4fab/miscs/pgd.py#L10):] <pre>diff.clamp_(-epsilon, epsilon)</pre> The github page says to set ε to 0.03125, but this would effectively make it half as for this code input is in range [-1, 1] not [0, 1]."
"Congrats on great paper. I wanted to run your model on some smaller dataset like CIFAR10 or MNIST but when I run it on CIFAR with the defaults you provided for Imagenet (I also tried it with resnet32) I end up with a model that has acc_r=1, acc_f=0 (with singular cases on first decoder step when it varies by something like ~0.2) throughout the whole 200 epochs and both classification accuracies getting to about 0.6 acc. Could you please recommend some parameters that might work better?"
"First of all, thank you very much for your excellent work. I am a graduate student studying in mainland China. For some reasons, I can't download the Imagenet datasets. Then I want to test the code you provide with my own datasets. So can you give me some details about the data you use, such as the Imagenet-143 datasets you use, how many images are included in the data set, and how many images are used for training and testing?
Thank you very much for your help."
Thanks.
Documentation on `LLC/accelerate_video_LLC.m` does not match with the function's arguments.
"Error on video_features_frames file on function video_features_frames( videoFilename , ofFilename , semanticFilename , yoloDescFilename).

===============================================================
Error using vertcat
Dimensions of matrices being concatenated are not consistent.

Error in video_features_frames (line 53)
     videoFeatures = [movementFeatures; appearanceFeatures; semanticFeatures ];

Error in accelerate_video_LLC (line 140)
        [ videoFeaturesFile.descriptors, videoFeaturesFile.OF ] = video_features_frames(path.in.video, path.in.opticalFlowCSV, path.in.semanticExtracted,
        path.in.YoloDesc);

"
"Hi Steven,

I tried to compile your code under Ubuntu, it seems that the amp library is only available on windows. Could you please suggest how to make your code working under linux environment?

Thanks,
Trung"
"Hi man, Just find your paper this morning,
and I am very interested in your research!
How is the gpu works going?
If your Supervoxel clustering and rgb-d segmentation can be GPGPU implemented, 
the great real time rgb-d segmentation will come true !  "
"Hello, I meet a problem as follows.
Welcome to AOGTracker (v1.0)
=============== Run benchmark evaluation ===============
=============== Processing 1 out of 108 videos ==========
[ParameterLearner]: ptPool size 4000 w.r.t. (maxNumSamples=4000, maxMemoryMB=6000.000000)
[LBFGS Optimization] Function value changing by less than optTol
段错误 (核心已转储)
What should I do?
Thanks!"
When will you update the project for Windows and other OS ? Or will you ?
"Hi, compiling RGM on Ubuntu 14.04LTS I get the compilation error:

`[ 51%] Building CXX object CMakeFiles/RGM.dir/src/AOGrammar.cpp.o
In file included from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/util/UtilLog.hpp:34:0,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:43,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rectangle.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rgm_struct.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/feature_pyr.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3:
/home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:270:25: warning: backslash and newline separated by space
     switch(Dimension) { \        
 ^
In file included from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rectangle.hpp:7:0,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rgm_struct.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/feature_pyr.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3:
/home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:270:25: warning: backslash and newline separated by space
     switch(Dimension) { \        
 ^
In file included from /usr/local/include/boost/archive/detail/interface_iarchive.hpp:22:0,
                 from /usr/local/include/boost/archive/detail/common_iarchive.hpp:23,
                 from /usr/local/include/boost/archive/basic_binary_iarchive.hpp:30,
                 from /usr/local/include/boost/archive/binary_iarchive_impl.hpp:21,
                 from /usr/local/include/boost/archive/binary_iarchive.hpp:20,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:9,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rectangle.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rgm_struct.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/feature_pyr.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3:
/usr/local/include/boost/archive/detail/iserializer.hpp: In instantiation of ‘static void boost::archive::detail::heap_allocation<T>::has_new_operator::invoke_delete(T*) [with T = RGM::Scaleprior]’:
/usr/local/include/boost/archive/detail/iserializer.hpp:264:35:   required from ‘static void boost::archive::detail::heap_allocation<T>::invoke_delete(T*) [with T = RGM::Scaleprior]’
/usr/local/include/boost/archive/detail/iserializer.hpp:272:30:   required from ‘boost::archive::detail::heap_allocation<T>::~heap_allocation() [with T = RGM::Scaleprior]’
/usr/local/include/boost/archive/detail/iserializer.hpp:293:36:   required from ‘void* boost::archive::detail::pointer_iserializer<Archive, T>::heap_allocation() const [with Archive = boost::archive::text_iarchive; T = RGM::Scaleprior]’
/home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3972:1:   required from here
/usr/local/include/boost/archive/detail/iserializer.hpp:236:47: error: invalid conversion from ‘long unsigned int’ to ‘void*’ [-fpermissive]
                 (T::operator delete)(t, sizeof(T));
                                               ^
In file included from /usr/include/eigen3/Eigen/Core:256:0,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:38,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rectangle.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rgm_struct.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/feature_pyr.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3:
/usr/include/eigen3/Eigen/src/Core/util/Memory.h:637:12: note: initializing argument 2 of ‘static void RGM::Scaleprior::operator delete(void*, void*)’
       void operator delete(void * memory, void *ptr) throw() { return ::operator delete(memory,ptr); } \
            ^
/usr/include/eigen3/Eigen/src/Core/util/Memory.h:648:41: note: in expansion of macro ‘EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF’
 #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(true)
                                         ^
/home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:176:5: note: in expansion of macro ‘EIGEN_MAKE_ALIGNED_OPERATOR_NEW’
     EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
     ^
In file included from /usr/local/include/boost/archive/detail/interface_iarchive.hpp:22:0,
                 from /usr/local/include/boost/archive/detail/common_iarchive.hpp:23,
                 from /usr/local/include/boost/archive/basic_binary_iarchive.hpp:30,
                 from /usr/local/include/boost/archive/binary_iarchive_impl.hpp:21,
                 from /usr/local/include/boost/archive/binary_iarchive.hpp:20,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:9,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rectangle.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rgm_struct.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/feature_pyr.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3:
/usr/local/include/boost/archive/detail/iserializer.hpp: In instantiation of ‘static void boost::archive::detail::heap_allocation<T>::has_new_operator::invoke_delete(T*) [with T = RGM::Deformation]’:
/usr/local/include/boost/archive/detail/iserializer.hpp:264:35:   required from ‘static void boost::archive::detail::heap_allocation<T>::invoke_delete(T*) [with T = RGM::Deformation]’
/usr/local/include/boost/archive/detail/iserializer.hpp:272:30:   required from ‘boost::archive::detail::heap_allocation<T>::~heap_allocation() [with T = RGM::Deformation]’
/usr/local/include/boost/archive/detail/iserializer.hpp:293:36:   required from ‘void* boost::archive::detail::pointer_iserializer<Archive, T>::heap_allocation() const [with Archive = boost::archive::text_iarchive; T = RGM::Deformation]’
/home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3972:1:   required from here
/usr/local/include/boost/archive/detail/iserializer.hpp:236:47: error: invalid conversion from ‘long unsigned int’ to ‘void*’ [-fpermissive]
                 (T::operator delete)(t, sizeof(T));
                                               ^
In file included from /usr/include/eigen3/Eigen/Core:256:0,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/common.hpp:38,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rectangle.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/rgm_struct.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/feature_pyr.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:4,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.hpp:7,
                 from /home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/AOGrammar.cpp:3:
/usr/include/eigen3/Eigen/src/Core/util/Memory.h:637:12: note: initializing argument 2 of ‘static void RGM::Deformation::operator delete(void*, void*)’
       void operator delete(void * memory, void *ptr) throw() { return ::operator delete(memory,ptr); } \
            ^
/usr/include/eigen3/Eigen/src/Core/util/Memory.h:648:41: note: in expansion of macro ‘EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF’
 #define EIGEN_MAKE_ALIGNED_OPERATOR_NEW EIGEN_MAKE_ALIGNED_OPERATOR_NEW_IF(true)
                                         ^
/home/sholc/Documents/AirDog/object_tracking/source/RGM-AOGTracker/src/parameters.hpp:232:5: note: in expansion of macro ‘EIGEN_MAKE_ALIGNED_OPERATOR_NEW’
     EIGEN_MAKE_ALIGNED_OPERATOR_NEW
     ^
make[2]: *** [CMakeFiles/RGM.dir/src/AOGrammar.cpp.o] Error 1
make[1]: *** [CMakeFiles/RGM.dir/all] Error 2
make: *** [all] Error 2
`

So all dependences are satisfied.
"
"您好，请问您的论文里测试的M27的数据集使用的是原始分辨率的图像还是3倍下采样后的图像呢？看到您的论文里只有说新出的Middlebury 2014使用的是四分一分辨率的图像（这个可能是您的那篇TCSVT）。
还想请教您一个问题，就是我在利用左右视差真值计算被遮挡区域时，利用一致性计算（数据集给的SDK程序），但是通常得到的被遮挡图的左侧原本应该是被遮挡区域，而我计算得到的有的不是被遮挡，这个问题请问您当时是如何解决的？也就是左图的被遮挡区域应该包括左图像的最左侧几列的区域，而利用一致性计算时有时候那部分并不是遮挡区域。（我思考可能是因为那部分都是和相机平行的平面，视差与未被遮挡的部分相同，导致阈值选择为1时无法筛掉）主要针对Middlebury 2005及2006没有给出遮挡区域的图，故而想问问您！谢谢"
"Hi, 

I was wondering if it was possible to use this algorithm on unrectified images.

Thank you!"
"When I tried to integrate the SSCA algorithm into my project, I encounterd this problem. The Chinese characters of the error information on this picture represents that the compiler could not find the identifier about ""m_mst_value_sum_aggregated_from_child_to_parent"", ""m_mst_value_sum_aggregated_from_parent_to_child"" and ""m_mst_value_to_be_filtered"".  My running environment is as follows:
windows10, 
Visual Studio 2019(community),
opencv 4.3.0.
Is there anyone who can help me with this problem? Thanks!

——
![error](https://user-images.githubusercontent.com/39852388/82414977-6e11ce80-9aaa-11ea-8cd6-1762950343a4.png)

"
"Hi,I want to know the meaning of the cost function.It is hard to understand for me.
Where is this come from?It looks not the same as :
cost=(1-lamda)*costA+lamda*costB
Thanks~"
"It seems that ‘m_mst_value_sum_aggregated_from_parent_to_child’ was not declared in qx_tree_filter.cpp

I searched the whole project and didn't find the declaration of m_mst_value_sum_aggregated_from_parent_to_child"
"when you change opencv version yourself, and change CommFunc.h to your opencv version, but it still procuces this issue. That because you also need to change opencv version in /CAST/StereoDisparity.h .
"
Did you try your method using  parallel computing? I know in your paper you calculate the time complexity although 1/7 seems not to much.(KITTI show your method need 140s ?) 
你好，请问你所达到的计算时间是在debug模式下的吗，我用在debug下，时间都是30多秒，而且看了下都是在s.v阶段花的时间长。
每当我键入参数后就自动结束程序，是键入的格式不对吗？
"why I test on the kitti training set,my error is lower than your result ,futhermore,the difference is  very big,because the dataset have been updated?thank you ,very much"
"hi，while run the program，I found memory leak in the program，but I dont know where the bug？
"
"Is there a readily available Linux installation package for CSSM (with cmake files etc), or do we need to create one on our own?
"
"Hi, appreciate your work, but I failed to 'import filter_fns' in the _train_net.py_ in the _Loss_layer_ folder. 
Any workaround to suggest? Thanks!"
The requested URL /hbilen-data/WSDDN/EdgeBoxesVOC2007test.mat was not found on this server.
"
Hello, we use your source code, but only got the map of 32.99 (vgg16), is this a normal deviation? We have not made any changes to the source configuration."
"Hello, when I run the wsddn_train.m, it shows:
train: epoch 01: 146/5011: 2.6 (2.2) Hz lossTopB: NaN mAP: 12.229 objective: NaN"
"I try to download the pre-computed edge-boxes on the original repository. The trainval file is successfully downloaded while the test file is failed.
![123](https://user-images.githubusercontent.com/46312012/53465111-8b164800-3a87-11e9-9849-69aa720b3833.png)
Is there something wrong with my network? Could you please provide another link for downloading? Thanks a lot. "
"I'm curious why I should use offset on rois? can't I just rescale rois by 1/16? Thanks~

train.m
ss = [16 16] ;
if is_vgg16
  o0 = 8.5 ;
  o1 = 9.5 ;
else
  o0 = 18 ;
  o1 = 9.5 ;
end
rois = [ rois(1,:);
        floor((rois(2,:) - o0 + o1) / ss(1) + 0.5) + 1;
        floor((rois(3,:) - o0 + o1) / ss(2) + 0.5) + 1;
        ceil((rois(4,:) - o0 - o1) / ss(1) - 0.5) + 1;
        ceil((rois(5,:) - o0 - o1) / ss(2) - 0.5) + 1];
"
"matlab version: 2015b
GPU: GTX 1080TI
CUDA: 7.0
cudnn: 4.0

`nvcc fatal   : Unsupported gpu architecture 'compute_61'`

it seems like cuda7.0 doesn't support my GPU


==============================================
however, when I change to matlab 2017a, cuda 8.0
it turns out that there is some function whose name is same as cuda toolkit's funtion.
Error using mex
`/home/tz/projects/WSDDN/matconvnet/matlab/src/bits/impl/pooling_gpu.cu(163): error: function ""atomicAdd(double *, double)""
has already been defined
1 error detected in the compilation of ""/tmp/tmpxft_00007b3f_00000000-7_pooling_gpu.cpp1.ii"".`
and it failed, too.


is there some solutions?"
"In training process, we found that 'vl_nnspp'  function is called but there is no content in WSDDN/layers/matlab/vl_nnspp.m. Maybe we can compile vl_nnspp.cu into a mex file. Is it right and how can we do that? Thanks for your future solution."
"When we train your model WSDDN, we met a problem on reshaping the matrix of confidence on proposals (i.e. imdb.images.boxScores{batch}). The error logs are listed as belows.

`Error using reshape
Size arguments must be integer scalars.

Error in cnn_wsddn_train>getBatch (line 204)
  boxScore = reshape(imdb.images.boxScores{batch},[1 1 1
  numel(imdb.images.boxScores{batch})]);`"
"I want to change the backbone network, e.g. resnet.  In SPP layer, there are two params, `offset1` and `offset2`. I can't figure out what they mean and how to calculate them. Where can I get more detailed explanation, any paper or blog? Thanks a lot."
"o = find((inter ./ (gtArea + area - inter))>obj.minOverlap);
inter gtArea, area and inter  are actually of uint16, so o could only be 0 or 1(when actual value > 0.5), so an setting of minOverlap = 0.6 is useless.

**w = max(0.0, xx2-xx1+1);
 h = max(0.0, yy2-yy1+1);
w and h are still uint16;**

**And (gtArea + area - inter) could overflow**

I use matlab   ##R2014a
"
"Error using vl_nnconv
An input is not a numeric array (or GPU support not compiled).

Error in dagnn.Conv/forward (line 11)
      outputs{1} = vl_nnconv(...

Error in dagnn.Layer/forwardAdvanced (line 85)
      outputs = obj.forward(inputs, {net.params(par).value}) ;

Error in dagnn.DagNN/eval (line 88)
  obj.layers(l).block.forwardAdvanced(obj.layers(l)) ;

Error in cnn_wsddn_demo (line 98)
      net.eval(inputs) ;
 "
"Error using cellfun
Input #2 expected to be a cell array, was struct instead.

Error in prepare_wsddn (line 13)
relu6p = find(cellfun(@(a) strcmp(a.name, 'relu6'), net.layers)==1);

Error in cnn_wsddn_train (line 60)
net = prepare_wsddn(net,nopts);"
"the error messages is as followings:

>尝试将 SCRIPT vl_nnspp 作为函数执行:
/media/liuwq/7A0EA5000EA4B713/WSDDN/layers/matlab/vl_nnspp.m
出错 SPP/forward (line 97)
      outputs{1} = vl_nnspp(inputs{1}, self.levels, rois, ...
 出错 dagnn.Layer/forwardAdvanced (line 85)
      outputs = obj.forward(inputs, {net.params(par).value}) ;
 出错 dagnn.DagNN/eval (line 88)
  obj.layers(l).block.forwardAdvanced(obj.layers(l)) ;
 出错 cnn_wsddn_demo (line 100)
      net.eval(inputs) ;


the scripts are:

>  opts.modelPath= 'models/vggf-edgeboxes-boxsc-spatreg.mat' ;
 opts.imdbPath = 'exp/imdb.mat' ;
opts.train.gpus = 0;
 cnn_wsddn_demo(opts);

So what's the problem?"
i have downloaded vgg16 model for matconvnet and chage the model path to it. I have set `vggdeep = 1` in prepare_wsddn.m. What else should i do?
"Hi, I have a question with compiling the toolbox. I hit this error when I run vl_wsddn_compilenn in MATLAB(I also couldn't find cnn_wsddn_compilenn). 

#warning ""SSSE3 instruction set not enabled. Using slower image conversion routines.""
  ^
    Warning: No source files in argument list. Assuming C source
             code for linking purposes. To override this
             assumption use '-fortran' or '-cxx'.
/usr/bin/ld: cannot find -lstdc++
collect2: error: ld returned 1 exit status
    mex: link of ' ""/home/hanwang/WSDDN/matconvnet/matlab/mex/vl_nnconv.mexa64""' failed.
    Unable to complete successfully.
    Error in vl_wsddn_compilenn>mex_link (line 506)
   mex(mopts{:}) ;
    Error in vl_wsddn_c ompilenn (line 458)
  mex_link(opts, objs, mex_dir, flags.link) ;
 

#

Any idea how do i fix this? Thanks."
"**line 161~162** in 'cnn_wsddn_train.m'

inputs = {'input', im, 'label', labels, 'rois', rois, 'ids', batch, ...
    'boxes', imdb.images.boxes{batch}} ;

When we use **LossSmooth**, that is, set opts.addLossSmooth = 1 in line 23, there is no problem;
But if we **do not** use LossSmooth, line 161~162 should be changed to:

inputs = {'input', im, 'label', labels, 'rois', rois, 'ids', batch} ;

Since we do not need the variable 'boxes' in the training net without LossSmooth.****"
"Hi,

In my understanding, you store boxes in format [y1 x1 y2 x2], yet at https://github.com/hbilen/WSDDN/blob/master/layers/LossTopBoxSmoothProb.m#L43 you are refering to them as [x1 y1 x2 y2]. As far as I get, this minor mis-reference is not a problem because the IOU formula is symmetric wrt to coordinates.

Am I right?
"
"Hi Hakan,

Thanks for putting the code online, it's really a nice and concise code base! I've got two questions concerning the spatial regularizer:

1) As far as I can see in https://github.com/hbilen/WSDDN/blob/master/layers/LossTopBoxSmoothProb.m#L69,
you are also putting the `probs`  values under the square opposite to the formulation in subsection 3.4 of the paper. Also the gradient computation in https://github.com/hbilen/WSDDN/blob/master/layers/LossTopBoxSmoothProb.m#L85 doesn't square the `probs` value that doesn't correspond to the forward pass code. Am I missing something or is it a bug?

2) Do I understand correctly that you differentiate the regularizer only wrt fc7 outputs?

Thanks in advance!
"
"Hi Dr. Oberweger, 

I tried annotating a few frames manually using the pose annotation tool and provided the following annotation in 2d.

![image](https://user-images.githubusercontent.com/8081917/92673138-02b16e00-f2e0-11ea-882b-cbc2c92cfa2c.png)

On clicking solve I got the following 3d view
![image](https://user-images.githubusercontent.com/8081917/92673273-5e7bf700-f2e0-11ea-9cf1-52bd7b1f4c7b.png)

As per the user guide tweaking the joints still didn't help.
For example you can see that the dip joints are over the neighboring fingers rather than being below the pip.

In other images where the base joints are not visible its not possible to place them in the right location.

Is there a way to edit these joints in 3d and get better results?

Best Regards,
Giffy 
"
"Hi Dr. Oberweger,

I am trying to run semi-auto-anno for MSRA15 dataset.
I have incorporated the dataloader from the deep prior pp code to make it uniform with the blender dataloader.
When I run the code with the give default_cube size of 200 for user P0, I get Bound Constraint Violated.

Can you please explain to me what the method getboundcontraint does and how can I resolve this error?
Also when I checked the crop with the cube as 200 I see that many of the joints are cropped off the image.

Cropping worked better when I ran a separate code to find what is the max height or width in all the cropped images that MSRA15 provides.
When I set the cube to that size I get the right crop, however still get the bound constraint error.

Please let me know your thoughts on the same."
"Hi Dr. Oberweger,

I'm trying to use semiautoanno to run an end to end simulation for MSRA and NYU using main_blender.py

In the handconstrain.py there are a few constants such as noise pairs, joint offs and bone lengths, bone ranges and noise pairs.

I'm calculating bone lengths by the formula sqrt((x2-x1)^2 + (y2-y1)^2 + (z2-z1)^2) is this the correct formula?

and how to get the values of the other mentioned constants above?

Best Regards,
Giffy Chris"
"This is a normal message, since the code does not support the full pipeline in a single run, yet. What you have to do now is to copy the list of reference frames `[16, 21, ...]` and assign it to the `subset_idxs` variable at main_blender_semiauto.py:L66 and then rerun the same code.

_Originally posted by @moberweger in https://github.com/moberweger/semi-auto-anno/issues/3#issuecomment-308948432_

After copying and assigning the reference frames in the main_lender_semiauto.py I am getting this error(posted below) as the sift_flow directory does not have this file. How should I generate this output_24580.mat file?

Traceback (most recent call last):
  File ""/home/marnim/Documents/Projects/test_semiauto/src/main_blender_semiauto.py"", line 211, in <module>
    theta = msr.fitTracking(num_iter=None, useLagrange=False)
  File ""/home/marnim/Documents/Projects/test_semiauto/src/semiautoanno.py"", line 532, in fitTracking
    Li, corrected = self.initClosestReference3D_maps(self.li3D_aug)
  File ""/home/marnim/Documents/Projects/test_semiauto/src/semiautoanno.py"", line 2477, in initClosestReference3D_maps
    i, ref_idx, off2D, off3D = self.getClosestSampleForReference(init)
  File ""/home/marnim/Documents/Projects/test_semiauto/src/semiautoanno.py"", line 2163, in getClosestSampleForReference
    off2D, _ = self.alignSIFTFlow(li2D, self.subset_idxs[best_ref_idx], unannotated[best_sample_idx])
  File ""/home/marnim/Documents/Projects/test_semiauto/src/semiautoanno.py"", line 2205, in alignSIFTFlow
    data = scipy.io.loadmat(""./etc/sift_flow/output_{}.mat"".format(os.getpid()))
  File ""/home/marnim/.local/lib/python2.7/site-packages/scipy/io/matlab/mio.py"", line 207, in loadmat
    MR, file_opened = mat_reader_factory(file_name, appendmat, **kwargs)
  File ""/home/marnim/.local/lib/python2.7/site-packages/scipy/io/matlab/mio.py"", line 62, in mat_reader_factory
    byte_stream, file_opened = _open_file(file_name, appendmat)
  File ""/home/marnim/.local/lib/python2.7/site-packages/scipy/io/matlab/mio.py"", line 37, in _open_file
    return open(file_like, 'rb'), True
IOError: [Errno 2] No such file or directory: './etc/sift_flow/output_24580.mat'

"
"`xstart = numpy.where(~numpy.isclose(dm, self.curData[0, 2] + self._seq.config['cube'][2] / 2.))[1].min()`
get error:
`Traceback (most recent call last):
  File ""semi-auto-anno/src/util/interactivedetector.py"", line 398, in nextButton_callback
    self.next()
  File ""semi-auto-anno/src/util/interactivedetector.py"", line 522, in next
    self.showCurrent()
  File ""semi-auto-anno/src/util/interactivedetector.py"", line 207, in showCurrent
    xstart = numpy.where(~numpy.isclose(dm, self.curData[0, 2] + self._seq.config['cube'][2] / 2.))[1].min()
  File ""/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.py"", line 29, in _amin
    return umr_minimum(a, axis, None, out, keepdims)
ValueError: zero-size array to reduction operation minimum which has no identity`
"
"When I run your code using Python2.7 I get the following message:
```
mona@DESKTOP-0JQ770H:/mnt/e/opensource_codes/annotation/semi-auto-anno/src$ python main_blender_semiauto.py
...
0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]
Took 0.0105140209198s for HOG embedding
BEST for i=3039: ref=874, off2D=[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]], off3D=[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]
Got 304 reference frames
[16, 21, 26, 29, 45, 49, 52, 54, 58, 104, 108, 114, 138, 144, 148, 170, 175, 178, 210, 214, 217, 231, 237, 249, 252, 259, 264, 283, 287, 296, 307, 345, 370, 381, 384, 386, 405, 412, 423, 429, 436, 458, 465, 469, 490, 498, 505, 526, 530, 533, 537, 546, 553, 576, 607, 612, 624, 631, 657, 667, 669, 673, 685, 697, 704, 735, 742, 751, 765, 781, 784, 789, 793, 801, 805, 816, 820, 827, 830, 874, 886, 888, 893, 896, 899, 911, 923, 934, 962, 969, 983, 1023, 1027, 1029, 1034, 1046, 1054, 1057, 1070, 1075, 1085, 1093, 1098, 1110, 1114, 1134, 1138, 1146, 1173, 1181, 1184, 1188, 1191, 1194, 1208, 1213, 1221, 1224, 1228, 1241, 1248, 1251, 1255, 1262, 1267, 1274, 1286, 1295, 1308, 1312, 1335, 1341, 1349, 1353, 1383, 1386, 1389, 1410, 1414, 1422, 1432, 1449, 1452, 1455, 1465, 1473, 1477, 1489, 1504, 1523, 1532, 1542, 1550, 1552, 1571, 1580, 1586, 1591, 1609, 1613, 1617, 1628, 1632, 1644, 1653, 1656, 1688, 1694, 1695, 1698, 1709, 1713, 1725, 1745, 1752, 1756, 1762, 1772, 1778, 1795, 1812, 1814, 1817, 1830, 1833, 1848, 1853, 1858, 1864, 1869, 1873, 1887, 1892, 1897, 1904, 1927, 1930, 1934, 1937, 1965, 1973, 1978, 1991, 2017, 2028, 2033, 2048, 2055, 2058, 2067, 2074, 2094, 2131, 2137, 2146, 2150, 2166, 2170, 2177, 2185, 2191, 2196, 2203, 2208, 2213, 2222, 2255, 2269, 2273, 2288, 2291, 2298, 2305, 2325, 2331, 2334, 2339, 2343, 2347, 2351, 2372, 2380, 2390, 2394, 2416, 2428, 2434, 2462, 2468, 2484, 2497, 2504, 2509, 2511, 2515, 2529, 2543, 2566, 2572, 2584, 2590, 2609, 2617, 2627, 2631, 2644, 2651, 2654, 2661, 2685, 2687, 2693, 2702, 2737, 2749, 2754, 2763, 2775, 2778, 2790, 2792, 2808, 2813, 2816, 2820, 2829, 2835, 2852, 2856, 2872, 2891, 2898, 2905, 2911, 2942, 2945, 2949, 2952, 2989, 3011, 3015, 3031, 3034, 3037]
Traceback (most recent call last):
  File ""main_blender_semiauto.py"", line 179, in <module>
    useCache=True, normZeroOne=False, gt3D=train_gt3D, hpe=hpe, debugPrint=False)
  File ""/mnt/e/opensource_codes/annotation/semi-auto-anno/src/semiautoanno.py"", line 238, in __init__
    raise UserWarning(""Rerun with given set of reference frames!"")
UserWarning: Rerun with given set of reference frames!
```
Am I running it wrong or how should I run it correctly?"
"```
How should I exactly run this?
Is this the correct way? And if so, please provide information what's wrong?

E:\opensource_codes\semi-auto-anno\src>python main_blender_semiauto.py
create data
E:\opensource_codes\semi-auto-anno\src\data\importers.py:601: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  gtorig = gtorig.reshape((gtorig.shape[0] / 3, 3))
Traceback (most recent call last):
  File ""main_blender_semiauto.py"", line 38, in <module>
    Seq1_0 = di.loadSequence('hpseq_loop_mv', camera=0, shuffle=False)
  File ""E:\opensource_codes\semi-auto-anno\src\data\importers.py"", line 625, in loadSequence
    dpt, M, com = hd.cropArea3D(gtorig[10], size=config['cube'])
  File ""E:\opensource_codes\semi-auto-anno\src\util\handdetector.py"", line 344, in cropArea3D
    rz = self.resizeCrop(cropped, sz)
  File ""E:\opensource_codes\semi-auto-anno\src\util\handdetector.py"", line 242, in resizeCrop
    rz = cv2.resize(crop, sz, interpolation=cv2.INTER_NEAREST)
TypeError: integer argument expected, got float

```"
"https://github.com/moberweger/semi-auto-anno/blob/master/people.csail.mit.edu/celiu/SIFTflow
![ddd](https://user-images.githubusercontent.com/1892917/27114977-5630cd50-507c-11e7-8028-ea3344a0b877.PNG)
However, seems it could be downloaded from https://github.com/kmatzen/sift-flow
Please correct me if I am wrong.
"
"Dear Pathak,
I'm learning about image inpainting recently .It would be thankful if you can share the Paris Street-View dataset.
My e-mail address is: 2279753641@qq.com
Best Regards."
"Dear Pathak,
I'm learning about image inpainting recently and I would like to use the Paris Street View Dataset for my Masters Dissertation.It would be thankful if you can share the Paris Street-View dataset.
My e-mail address is: 1053899246@qq.com
Best Regards."
"Dear Pathak,
I'm learning about image inpainting recently and I would like to use the Paris Street View Dataset for my Masters Dissertation.It would be thankful if you can share the Paris Street-View dataset.
My e-mail address is: tim0988829376@yahoo.com
Best Regards."
"I am interested in the field of image restoration and have been working on it recently. I have noticed that many researchers use Parisian Street View data sets in their painting tasks. I need to performance test my work on this data set, and I would be grateful if you could share the Paris Street View data. My E-mail address is 736054972@qq.com"
"Hello, Pashak.

I am interested in the field of image restoration and have been working on it recently. I have noticed that many researchers use Parisian Street View data sets in their painting tasks. I need to performance test my work on this data set, and I would be grateful if you could share the **Paris Street View data**. My E-mail address is **951353206@qq.com**

Best regards.

 "
"Hi Pathak,

I'm following the instructions of the Semantic Inpainting Demo. I got the error below when running the script to populate the `./models/` folder with trained models. The file `inpaintCenterModels.tar.gz` doesn't exist.

```$ bash ./models/scripts/download_inpaintCenter_models.sh
Downloading the Inpainting Center Fill models (504MB)...
--2022-06-27 17:01:08--  http://www.cs.berkeley.edu/~pathak/context_encoder/resources/inpaintCenterModels.tar.gz
Resolving www.cs.berkeley.edu (www.cs.berkeley.edu)... 23.185.0.1, 2620:12a:8000::1, 2620:12a:8001::1
Connecting to www.cs.berkeley.edu (www.cs.berkeley.edu)|23.185.0.1|:80... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://www.cs.berkeley.edu/~pathak/context_encoder/resources/inpaintCenterModels.tar.gz [following]
--2022-06-27 17:01:08--  https://www.cs.berkeley.edu/~pathak/context_encoder/resources/inpaintCenterModels.tar.gz
Connecting to www.cs.berkeley.edu (www.cs.berkeley.edu)|23.185.0.1|:443... connected.
WARNING: cannot verify www.cs.berkeley.edu's certificate, issued by ‘/C=US/O=Let's Encrypt/CN=R3’:
  Issued certificate has expired.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://people.eecs.berkeley.edu/~pathak/context_encoder/resources/inpaintCenterModels.tar.gz [following]
--2022-06-27 17:01:08--  https://people.eecs.berkeley.edu/~pathak/context_encoder/resources/inpaintCenterModels.tar.gz
Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190
Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.
HTTP request sent, awaiting response... 404 Not Found
2022-06-27 17:01:09 ERROR 404: Not Found.

Unzipping...

gzip: stdin: unexpected end of file
tar: Child returned status 1
tar: Error is not recoverable: exiting now
Downloading Done.
Checksum is incorrect. DELETE and download again.
```

Indeed, the file doesn't exist at Berkeley website:

![image](https://user-images.githubusercontent.com/54335913/176026930-835fd6c2-3ad6-4147-b871-d33bafa3d894.png)

You might share in Google Drive and provide the link.

Best regards,
Victor."
"Dear Pathak,
I'm learning about image inpainting recently and using the Paris Street View Dataset for my Master-Dissertation. I would be thankful if you are willing to share the Paris Street-View dataset.
My e-mail address: mrinmoy.sadhukhan1996@gmail.com
Best Regards.
"
"Dear Pathak,
I'm learning about image inpainting recently and using the Paris Street View Dataset for my Master-Dissertation. I would be thankful if you are willing to share the Paris Street-View dataset.
My e-mail address: 2352019591@qq.com
Best Regards."
"Dear Pathak,
I'm learning about image inpainting recently and I would like to use the Paris Street View Dataset for my Masters Dissertation.It would be thankful if you can share the Paris Street-View dataset.
My e-mail address is: [1351594605@qq.com]
Best Regards."
"Dear Pathak,
I'm learning about image inpainting recently and I would like to use the Paris Street View Dataset for my Masters Dissertation.It would be thankful if you can share the Paris Street-View dataset.
My e-mail address is: 320707643@qq.com
Best Regards."
"Dear Pathak,
I'm learning about image inpainting recently and I would like to use the Paris Street View Dataset for my Masters Dissertation.It would be thankful if you can share the Paris Street-View dataset.
My e-mail address is:  694341479@qq.com
Best Regards."
"Hi Pathak.

I am interested in the image inpainting field and I would like to use the Paris Street View Dataset for my Masters Dissertation.
Thank you.

My e-mail address is: jwc42@bath.ac.uk

Best Regards."
"Hi Pathak.
I am interested about image inpainting field. I noticed that many researches are employing Paris Street-View dataset in their inpainting tasks. It would be thankful if you can share the Paris Street-View dataset. My e-mail address is  yuanqmok@gmail.com

Best Regards."
"Hi Pathak,
I am currently working on an image inpainting algorithm and trying to train several model architectures. Can you share the dataset through a private link to my email address? That would be really great. Thanks.
email : irawati.ns@gmail.com"
Could you send the link of Paris Street-View dataset to 1554435955@qq.com. Thanks a lot
Could you send the link of Paris Street-View dataset to 675627966@qq.com. Thanks a lot
"Thanks to your share, but I do not see the code about channel-wise FC in your paper. give me back pleaz."
"Hi, Pathak.
I am a student and I want to use Paris Street-View Dataset on my task. Could please share the dataset to me. I I would appreciate it. My e-mail address is 1035597470@qq.com. Thank you!"
"Hi, Pathak.

I am a student, i am doing some research about image inpainting, and I need to use Paris Street-View Dataset. I would appreciate if you can share the dataset. My email address is gyh0316@gmail.com Thank you!

Best Regards."
"Hi Pathak,

I am a student, i am doing some research about image inpainting, and I need to use Paris Street-View Dataset. I would appreciate if you can share the dataset. My email address is 19110240051@fudan.edu.cn Thank you!

Best Regards"
"Hi Pathak,

I am doing some research about image inpainting, and I need to use Paris Street-View Dataset. I would appreciate if you can share the dataset. My email address is cameltr@whu.edu.cn. Thank you very much！

Best Regards

"
"Could you send me this dataset link, I am a student and need this data set to begin research about image inpainting, thank you！
sangsinh789@gmail.com"
"Hi Pathak.
I am interested about image inpainting field. I noticed that many researches are employing Paris Street-View dataset in their inpainting tasks. It would be thankful if you can share the Paris Street-View dataset. My e-mail address is cardongmin@postech.ac.kr

Best Regards."
"Hi Pathak,

I am doing some research about image inpainting, and I need to use Paris Street-View Dataset. I would appreciate if you can share the dataset. My email address is 2635505974@qq.com Thank you.

Best Regards"
""
"Hello, I have tried to develop the code but I get very poor quality images, I don't know if the problem is latent space.

How did you ensure that the reconstruction of the images does not affect the quality? "
"Hi Pathak,

I am a student of Peking University, i am also doing some research about image inpainting, and I need to use Paris Street-View Dataset. I would appreciate if you can share the dataset. My email address is 2101212842@stu.pku.edu.cn     Thank you!

Best Regards"
"Hi Pathak,

I am doing some research about image inpainting, and I need to use Paris Street-View Dataset. I would appreciate if you can share the dataset. My email address is chayh21@mails.tsinghua.edu.cn. Thank you.

Best Regards"
"Hi Pathak,
I am currently working on image inpainting algorithms. Can you share the dataset link by my email address. That would be really great. Thanks.
My e-mail address is 2374623265@qq.com or xulinglong666@gmail.com
best wish you"
"Hi Pathak,
I am currently working on image inpainting algorithms. Can you share the dataset link by my email address. That would be really great. Thanks.My e-mail address is 853971768@qq.com or hy2056947336@gmail.com
best wish you"
"Hi Pathak,

We are currently doing some research about image inpainting, and we need to use Paris Street-View Dataset. We would appreciate if you can share the dataset to us. My email address is faslsun@gmail.com. Thank you.

Best Regards
"
"Hi Pathak,
I am currently working on image inpainting algorithms. Can you share the dataset link by my email address. That would be really great. Thanks.My e-mail address is 315372461@qq.com
best wish you"
"Hi，I'm trying some algorithms of image restoration. Can you provide a data set of Paris StreetView dataset?Thanks,My e-mail address is 1849523917@qq.com

Best，
Chen"
"Hi，I'm trying some algorithms of image restoration. Can you provide a data set of Paris StreetView dataset?Thanks,My e-mail address is 1483079457@qq.com
"
"when I run ""bash ./models/scripts/download_inpaintCenter_models.sh"", it reports:

./models/scripts/download_inpaintCenter_models.sh: line 12: wget: command not found
Unzipping...
tar: Error opening archive: Failed to open 'inpaintCenterModels.tar.gz'
Downloading Done.
cat: inpaintCenterModels.tar.gz: No such file or directory
Checksum is incorrect. DELETE and download again.

What's the problem??"
"TypeError: RangeIndex(...) must be called with integers, map was passed for stop"
"Any idea on how to implement this paper in pytorch would be of a great help.!
And what do these line of the code do?
How do I implement these lines in pytorch

real_ctx[{{},{1},{},{}}][mask_global] = 2 * 117.0/255.0 - 1.0
real_ctx[{{},{2},{},{}}][mask_global] = 2 * 104.0/255.0 - 1.0
real_ctx[{{},{3},{},{}}][mask_global] = 2 * 123.0/255.0 - 1.0
input_ctx:copy(real_ctx)"
"In the file (train.lua and data.lua), why set torch.setnumthreads(1)?

I am a little confused about the setting. Can I set multithreads, such as torch.setnumthreads(torch.getnumthreads())?

Thanks for your time."
"hi
1 I have a question about the loadSize, why should we use 129 not  just 128?
2 why should we use overlapPred?
3 I have a image which fineSize is 32*32. If I want to train this fineSize, then I just change new dataset and fineSize in code.?
4 If I want to train my dataset, how should I adjust the parameter to get best results?
Thank you so much!"
"Hi, 

From page 5:
To customize GANs for this task, one could condition on the given context information; i.e., the mask Mˆ ⊙ x. However, conditional GANs don’t train easily for context prediction task as the adversarial dis- criminator D easily exploits the perceptual discontinuity in generated regions and the original context to easily classify predicted versus real samples.

Does this means that we only rely on the ""L1 reconstruction loss"" to ensure the model to take the (outside) condition part into consideration? 
Since the discriminator only sees the inpainted region, it has no additional information to work on.

Looking forward to your reply! Thanks
"
"Hi, 

First of all, great and interesting work. Congratulations! I have recently started my Ph.D. and your paper was one of the few interesting and helpful baselines for me to grab some knowledge on the inpainting topic.

I tried Context Encoder (CE) on my lab's locally obtained dataset (~0.5 million images) and it outperformed many other inpainting methods that we experimented with.

But recently, I have been trying to use CE for a rather larger dataset (over 4 million images). While training, after the second or third epoch, loss of discriminator starts approaching to 0 which apparently means the generator network is not learning anymore

Can I get your expert opinion on what may be the causes? and, what parameters would be suitable to train the network for such a large dataset?

Below is the screenshot showing learning progress:

![screenshot from 2018-08-09 21-08-01](https://user-images.githubusercontent.com/29110530/43897895-8f4b4622-9c18-11e8-8341-287cc12d233c.png)"
"Hi,
I find that if the opt.conditionAdv is true , then the input of the  Adversarial discriminator net is 128x128,
but if set it false, the input is changed to be 64x64.
so  what is the ""conditionAdv"" for?  and what is the real input size of  Adversarial discriminator net?"
"Hi, can I know how much training time it spent for inpainting using ImageNet ?
And what kind of gpu you use? Thanks!"
"@pathak22 , hi. I tried the train_random.lua.  and the result is as follow:

![image](https://user-images.githubusercontent.com/13776012/39816579-3f2f50ee-53ce-11e8-883a-dbcb9e741ef2.png)

but I think it is only pseudorandom. for a  given training, the random position is also fixed after training. so if i want to inpaint other areas or anywhere. how should i do ?"
"can you provide the train and test files for training the context encoder to inpaint random blocks not regions?
thanks"
"@pathak22 

How to remove the blocking artifact in the patch loction after reconstruction of image?
What value to be set in this part of code?

image_ctx[{{},{1},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*117.0/255.0 - 1.0
image_ctx[{{},{2},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*104.0/255.0 - 1.0
image_ctx[{{},{3},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred},{1 + opt.fineSize/4 + opt.overlapPred, opt.fineSize/2 + opt.fineSize/4 - opt.overlapPred}}] = 2*123.0/255.0 - 1.0

The above values are used to set mean value in R. G. B.
But we want to make this region fully transparent [no blocky artifact].

Can you please guide us on this regard
"
"As I understand it all images in the paper are using the small (not AlexNet) architecture, since the adversarial loss makes the images look much nicer. I was hoping it is possible to share some of the images from the AlexNet implementation so that I can compare with the results of my own implementation.

(If there are other smart ways of performing intermediate validation of the AlexNet implementation before recreating the Table 2 results they are also welcome)"
"@pathak22 : As per our observation, the models rescales the images to 128x128.
Since our input image resolution is HD, we expect the reconstructed output also to be of same resolution.
Please let us know if this is possible?"
""
Hi ! Your work is great but I want to know that if i want to resume training from certain epoch then how to edit this in your training center inpaint code ?
"hello! Thanks for your brilliant paper and code first.

I use the 'train_random.lua' script to train the net to complete an image with random loss area,I train it in a GPU of GTX 1080, I found the train time is so long if I loop 500 times, almost 150 hours, so I only loop 80 times.the effect in visulization web is good,but when I test it in validation data, the result is so bad.

Followings are some results of my training in random areas,the effet is bad
![screenshot from 2017-11-19 17-28-00](https://user-images.githubusercontent.com/19423217/32989328-075c2c4c-cd4f-11e7-8126-a74f20784973.png)

Followings are resuls from  weight in center missing that you offered for me, the result is good.
![screenshot from 2017-11-19 17-33-34](https://user-images.githubusercontent.com/19423217/32989381-ccfcd97e-cd4f-11e7-8568-e00a6b4ababa.png)

is it the reason that my epoch loops is not enough? any advices will be appreciated.
"
"Hi
Is it possible to input already masked images instead of generating masks in the image first and inpaint them ? The problem is, that the masks in my images will not all be in the same relative locations so I should inpaint different image regions but I don't want to generate masks myself on the image. If it is possible, should the masks satisfy some constraints (like pixel values )?
Thx in advance "
"Hi 
In your paper you mentioned that you have implemented the 3 strategies, namely centre  inpainting, random block inpainting and random region inpainting.  However there is currently no way to download a inpaintRandomRegin model in t7 format. Can that be provided or does it need to be trained with train_random.lua?"
"hiii, i m so interested in yr work, i want to be able to make the mask at the top left of the image and be able to in-paint that region successfully, what has to be changed in the training and test code to achieve this
thanks"
"Hi Pathak,
               I need Paris view dataset for my image inpainting B-tech project , please send me as soon as possible.
              ""abhiramchowdary.m99@gmail.com"""
""
"Hi Pathak,
I am doing research related to image restoration, but there is no such Paris StreetView Dataset, please share your data set with me, thank you a lot.
Email:zgkdxuhao@163.com"
"Hi Pathak,
      I am doing research related to image restoration, but there is no such Paris StreetView Dataset, please share your data set with me, thank you a lot.
Email:w406141176@gmail.com"
"Hi Pathak,
Could you please share this inpainting dataset with me as I'm doing my Msc. theses on image inpainting? Thanks a lot!
Email: danydanydood@gmail.com"
"Hi Pathak,
I am working on image inpainting and need these images as training datasets badly. Could you please share a link with me? Thanks a lot!
My E-mail :875376358@qq.com "
"Hi Pathak,
I am working on image inpainting and need these images as training datasets badly. Could you please share a link with me? Thanks a lot!
Email: gina7152316@gmail.com/ gina7152316@yahoo.com.tw"
"Hi Pathak,
I am working on image inpainting and need these images as training datasets badly. Could you please share a link with me? Thanks a lot!
Email:algo_lzw@yahoo.com"
"Hi Pathak,
I am working on image inpainting and need these images as training datasets badly. Could you please share a link with me? Thanks a lot!
Email:1354772109@qq.com"
"Dear Sir,
Could you please email me the paris dataset for the learning image inpainting link.
My email address is 326605441@qq.com
Thanks!

best wishes
"
"Dear Sir,
Could you please email me the dataset for the learning image inpainting link.
My email address is huangsally5686@gmail.com
Thanks!

best wishes
Sally"
"Hi，I'm trying some algorithms of image inpainting. Can you provide a data set of Paris StreetView dataset?Thanks.
Email:734249947@qq.com"
"Dear Sir,
Could you please email me dataset for learning image inpainting link.. my email id is as follow:
aamir.uettaxila@gmail.com
Thanks and best regards:"
"Hi, I am currently studying on inpainting. Could you please share The Paris Street-View Dataset through a private link in my email address. My email is 1614081533@qq.com. Thanks!"
"Hi,I'm studying on inpainting now.Could you share the Paris Street-View Dataset through a private link in my email address. Thanks.My email is ly582751489@gmail.com"
"Hi, could you send me the private link of the Paris street view datasets? My email address is jluisflores357@gmail.com. Thank you very much!"
"Could you please send me the dataset?My e-mail is ssttfit@outlook.com
Thank you so much.That's really helpful."
"Good job.
Could you share me the dataset？
383736929@qq.com"
"Hi, could you send me the private link of the Paris street view datasets? My email address is gogohaoru@gmail.com. Thank you very much!
"
"Hi
Our group is mainly studying the forensics of images and has been working on the forensics of image restoration recently. Could you please share the repaired images and masks you made to me? Thank you very much!
my email is XwangJu@163.com
Xingwang Ju
Nanjing University of information science and technology, China"
"Hi, could you send me the private link of the street view datasets? My email address is renhaha123@foxmail.com. Thank you very much!"
"Hi, could you send me the private link of the street view datasets? My email address is pk2458807@gmail.com. Thank you very much!"
"Hi Pathak,
I'm a student studying on inpainting now. And I'd like to know if I can get dataset by email for private use.

that is my e-mail : a0970739953@gmail.com"
"Hi，I'm trying some  algorithms of image restoration. Can you provide a data set of Paris StreetView dataset?Thanks.
Email:1502930420@qq.com"
"Hi,I'm studying on inpainting now.Could you share the Paris Street-View Dataset through a private link in my email address. Thanks.My email is ucos-iii@qq.com"
"Hi,I'm studying on inpainting now.Could you share the Paris Street-View Dataset through a private link in my email address. Thanks.My email is 18731186906@163.com"
"Hi,I'm studying on inpainting now.Could you share the Paris Street-View Dataset through a private link in my email address. Thanks.My email is 1592884408@qq.com"
"Hi Pathak,
I am currently working on image inpainting algorothim and trying to train several model architectures. Can you share the Paris Street-View Dataset through a private link in my email address. That would be really great. Thanks.
Email: jiafeng5513@outlook.com"
"Hi Pathak,
I am currently working on image inpainting algorothim and trying to train several model architectures. Can you share the Paris Street-View Dataset through a private link in my email address. That would be really great. Thanks.
Email: yyong_ma@outlook.com"
"Hi Pathak,
I'm a student studying on inpainting now. And I'd like to know if I can get dataset by email for private use.
And my email is lauren1997@qq.com. "
"Could you give me the link of Paris Street-View Dataset?
My e-mail is 972346454@qq.com"
"in paper, it mention that overlap is 7, but the parameter is set 0 (default) in code.
is it used?"
"To get similar results in Table 2, how many images are used from Imagenet dataset 1.2M or 100K? "
"@pathak22 
I understood the method to train with the existing dataset for `centre region inpainting model` with the below command as mentioned in README.md

DATA_ROOT=dataset/train display_id=11 name=inpaintCenter overlapPred=4 wtl2=0.999 nBottleneck=4000 niter=500 loadSize=350 fineSize=128 gpu=1 th train.lua  

This will give me the trained model ""inpaintCenter_500_net_G.t7""
However I have two queries:
1) Suppose I want to re-train the already trained model ""inpaintCenter_500_net_G.t7"" with new dataset how do I achieve that?  
By referring the below link,
https://stats.stackexchange.com/questions/325803/retraining-cnn-model  
I understood that I need to load the weights from the old model to the new one. 
But how do we load the weights in this context-encoder code?
Kindly let me know the approach.  

2) Suppose I have trained a model ""inpaintCenter_500_net_G.t7"" with centre patch location, and I want to retrain the same model with different patch location, how do i achieve this?
Since I am a beginner to machine learning, kindly suggest.

"
"hii 
i tried training using the parisview dataset but i always get that error while testing after it finishes the training phase, which i can't figure out why ??
can u help ? @pathak22 
![screenshot from 2018-01-24 17-25-41](https://user-images.githubusercontent.com/26221951/35340385-a6784ba2-012b-11e8-8cd0-b3f3be9663c4.png)
"
"I am trying to run the provided trained net (imagenet_inpaintCenter) with random region inpainting (test_random.lua)

DATA_ROOT=dataset/val net=models/inpaintCenter/imagenet_inpaintCenter.t7 name=test_whatever useOverlapPred=0 manualSeed=222 batchSize=30 loadSize=129 gpu=1 th test_random.lua 

but I get this error: bad argument #2 to '?' (sizes do not match at /<...>/torch/extra/cutorch/lib/THC/generated/../generic/THCTensorMasked.cu:122)

Any idea on how to solve this? 

Thanks :)"
"how many epoches are used for training with paris streetview dataset ?
and how many epoches if i trained with adv. loss on?
thanks sooo much for yr help and support @pathak22 "
"In Table 2 of the paper some segmentation performances are given, but I am not sure of the units. Is it accuracy, since there is a percentage sign? Or is it mean IoU, which is more standard (I don't recall seeing it with a percentage sign before though)

![image](https://user-images.githubusercontent.com/8115763/34908174-12c4f1fa-f88c-11e7-98d0-6a3288e96cec.png)

"
"@pathak22  Could you please share me the dataset that you used to train your models? 
My email id is harshi.384@gmail.com 
 
Also, when I tried to train the model [train.lua] with a different dataset, I obtained the following error:  
/home/ananya/torch/install/bin/luajit: /home/ananya/torch/install/share/lua/5.1/trepl/init.lua:389: module 'display' not found:No LuaRocks module found for display
	no field package.preload['display']
	no file '/home/ananya/.luarocks/share/lua/5.1/display.lua'
	no file '/home/ananya/.luarocks/share/lua/5.1/display/init.lua'
	no file '/home/ananya/torch/install/share/lua/5.1/display.lua'
	no file '/home/ananya/torch/install/share/lua/5.1/display/init.lua'
	no file './display.lua'
	no file '/home/ananya/torch/install/share/luajit-2.1.0-beta1/display.lua'
	no file '/usr/local/share/lua/5.1/display.lua'
	no file '/usr/local/share/lua/5.1/display/init.lua'
	no file '/home/ananya/.luarocks/lib/lua/5.1/display.so'
	no file '/home/ananya/torch/install/lib/lua/5.1/display.so'
	no file '/home/ananya/torch/install/lib/display.so'
	no file './display.so'
	no file '/usr/local/lib/lua/5.1/display.so'
	no file '/usr/local/lib/lua/5.1/loadall.so'
stack traceback:
	[C]: in function 'error'
	/home/ananya/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	train.lua:260: in main chunk
	[C]: in function 'dofile'
	...anya/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50  
Could you please help me resolve this issue?"
"I trained on lfw dataset with the code, but the result is very strange:
![image](https://user-images.githubusercontent.com/13776012/34324471-748acf0a-e8af-11e7-960b-610ccfbc7cd1.png)
![image](https://user-images.githubusercontent.com/13776012/34324472-7eb97ee0-e8af-11e7-8e53-52bab95d71c7.png)

has anyone ever tried on lfw?"
"Hi, I want to train it by myself,but i don't know how to download the datasets.
can you give me a link? any related url are appreciated. "
"I wanted to verify on the number of training epochs for ImageNet.
Found it on the project [website](https://people.eecs.berkeley.edu/~pathak/context_encoder/)"
"At B. Feature Learning part in Supplementary Material, you said,"" Unfortunately, we couldn’t train adversary with Alexnet Encoder."" I wonder whether it is because you use Random region method?"
"hi @pathak22 hope u r fine
can u share the link to download the Paris StreetView Dataset?
thanks for yr help and support"
"Hi,

I have read your paper and hope to run the code, but I cannot find the Paris StreetView Dataset. Since ImageNet is too large and difficult to train, I hope to use Paris StreetView Dataset. So could you please tell me how I can find the dataset? 

Thanks so much for your attention!"
"The prototxt and learned weight for the Alexnet encoder is here, but I wonder if there is caffe /torch implementation for the decoder part, which would include the channel-wise fully connected layer.

I would greatly appreciate if you can share the implementation. "
"Hi, I was wondering if you subtracted the input image by some mean pixel values. If so, would it be possible to let me know what they are? Thanks."
"I want to train it by myself,but i don't know how to download the datasets.can you give me a link? @pathak22 "
"Section 3.2 of the paper (Joint Loss) says that adversarial loss only is used for inpainting experiments, but Figure 6 shows results with L2+Adv. Can you please clarify?"
"hi 
i m trying to train the context encoder but i m getting this error after the epoch no.20, i m using cpu mode as my laptop doesn't support CUDA
Epoch: [20][       3 /        3]	 Time: 29.860  DataTime: 0.002    Err_G_L2: 0.0748   Err_G: 1.6929  Err_D: 0.4178	
/home/nermin/torch/install/bin/lua: ...me/nermin/torch/install/share/lua/5.1/torch/File.lua:210: write error: wrote 24686288 blocks instead of 32768000 at /home/nermin/torch/pkg/torch/lib/TH/THDiskFile.c:353
stack traceback:
	[C]: in function 'write'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:210: in function <...me/nermin/torch/install/share/lua/5.1/torch/File.lua:107>
	[C]: in function 'write'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:210: in function 'writeObject'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
	...ome/nermin/torch/install/share/lua/5.1/nn/Module.lua:188: in function 'write'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:210: in function 'writeObject'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
	...ome/nermin/torch/install/share/lua/5.1/nn/Module.lua:188: in function 'write'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:210: in function 'writeObject'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
	...ome/nermin/torch/install/share/lua/5.1/nn/Module.lua:188: in function 'write'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:210: in function 'writeObject'
	...me/nermin/torch/install/share/lua/5.1/torch/File.lua:388: in function 'save'
	/home/nermin/context-encoder/util.lua:96: in function 'save'
	train.lua:451: in main chunk
	[C]: in function 'dofile'
	.../torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: ?
can u help?
thanks"
"I want to ask you for whether the Channel-wise fully-connected layer is the nBottleneck in the code? My understanding that is the transition layer between the encoder and decoder, is it right?How to expression in the Channel-wise fully-connected layer"
Is there a Channel-wise fully-connected layer in the Torch version codes?I don't find where it is.
"Setting the inpainting region rectangle  result to error. 
Must  the inpainting region be quare?"
"@pathak22 
In the readme file, you wrote how to make the dataset folder
""mkdir -p /path_to_wherever_you_want/mydataset/train/images/
# put all training images inside mydataset/train/images/
mkdir -p /path_to_wherever_you_want/mydataset/val/images/
# put all val images inside mydataset/val/images/""

However, i only set the train folder and not set the val folder, the train process go on wheels . Was the validation data needless? "
"I am trying to run your demo (ubuntu 16.04, cuda 8) but something goes wrong in the forward pass.
The line ""net=models/inpaintCenter/paris_inpaintCenter.t7 name=paris_result imDir=images/paris overlapPred=4 manualSeed=222 batchSize=21 gpu=1 th demo.lua"" gives me the following error.
I haven't changed anything in the code. Could it be GPU memory requirements ? (I have 6GB)
Any help would be hugely appriciated !

net=models/inpaintCenter/paris_inpaintCenter.t7 name=paris_result imDir=images/paris overlapPred=4 manualSeed=222 batchSize=21 gpu=1 th demo.lua
{
  gpu : 1
  net : ""models/inpaintCenter/paris_inpaintCenter.t7""
  overlapPred : 4
  manualSeed : 222
  name : ""paris_result""
  nc : 3
  imDir : ""images/paris""
  batchSize : 21
}
Seed: 222	
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> output]
  (1): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> output]
    (1): nn.SpatialConvolution(3 -> 64, 4x4, 2,2, 1,1)
    (2): nn.LeakyReLU(0.2)
    (3): nn.SpatialConvolution(64 -> 64, 4x4, 2,2, 1,1)
    (4): nn.SpatialBatchNormalization (4D) (64)
    (5): nn.LeakyReLU(0.2)
    (6): nn.SpatialConvolution(64 -> 128, 4x4, 2,2, 1,1)
    (7): nn.SpatialBatchNormalization (4D) (128)
    (8): nn.LeakyReLU(0.2)
    (9): nn.SpatialConvolution(128 -> 256, 4x4, 2,2, 1,1)
    (10): nn.SpatialBatchNormalization (4D) (256)
    (11): nn.LeakyReLU(0.2)
    (12): nn.SpatialConvolution(256 -> 512, 4x4, 2,2, 1,1)
    (13): nn.SpatialBatchNormalization (4D) (512)
    (14): nn.LeakyReLU(0.2)
    (15): nn.SpatialConvolution(512 -> 4000, 4x4)
  }
  (2): nn.SpatialBatchNormalization (4D) (4000)
  (3): nn.LeakyReLU(0.2)
  (4): nn.SpatialFullConvolution(4000 -> 512, 4x4)
  (5): nn.SpatialBatchNormalization (4D) (512)
  (6): nn.ReLU
  (7): nn.SpatialFullConvolution(512 -> 256, 4x4, 2,2, 1,1)
  (8): nn.SpatialBatchNormalization (4D) (256)
  (9): nn.ReLU
  (10): nn.SpatialFullConvolution(256 -> 128, 4x4, 2,2, 1,1)
  (11): nn.SpatialBatchNormalization (4D) (128)
  (12): nn.ReLU
  (13): nn.SpatialFullConvolution(128 -> 64, 4x4, 2,2, 1,1)
  (14): nn.SpatialBatchNormalization (4D) (64)
  (15): nn.ReLU
  (16): nn.SpatialFullConvolution(64 -> 3, 4x4, 2,2, 1,1)
  (17): nn.Tanh
}
Loaded Image Block: 	21 x 3 x 128 x 128	
/home/user/torch/install/bin/luajit: /home/user/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
In 6 module of nn.Sequential:
/home/user/torch/install/share/lua/5.1/nn/THNN.lua:110: bad argument #2 to 'v' (3D or 4D (batch mode) tensor is expected at /home/user/torch/extra/cunn/lib/THCUNN/SpatialConvolutionMM.cu:12)
stack traceback:
	[C]: in function 'v'
	/home/user/torch/install/share/lua/5.1/nn/THNN.lua:110: in function 'SpatialConvolutionMM_updateOutput'
	...er/torch/install/share/lua/5.1/nn/SpatialConvolution.lua:79: in function <...er/torch/install/share/lua/5.1/nn/SpatialConvolution.lua:76>
	[C]: in function 'xpcall'
	/home/user/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	/home/user/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function </home/user/torch/install/share/lua/5.1/nn/Sequential.lua:41>
	[C]: in function 'xpcall'
	/home/user/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	/home/user/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	demo.lua:68: in main chunk
	[C]: in function 'dofile'
	...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00405d50

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
	[C]: in function 'error'
	/home/user/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
	/home/user/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	demo.lua:68: in main chunk
	[C]: in function 'dofile'
	...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00405d50
THCudaCheck FAIL file=/home/user/torch/extra/cutorch/lib/THC/generic/THCStorage.c line=147 error=77 : an illegal memory access was encountered

"
"Hello @pathak22 ✋  
First of all, thanks for sharing your code and research 👍 . I got the demo and training working on macOSX 10.11.5💃. However, I got stuck when trying to modify the algorithm.

I'm trying to change the code so I can specify the location of the region mask myself instead of choosing either random or center (in the testing phase). 
**Can you give me pointers on how to go about doing this?** 
(I was thinking to manually put white patches on the validation images and disable the automatic masking but couldn't figure it out)

Another question is, **would the algorithm work with bigger images say 1024x1024 ?**

P.S: I'm a newbie in NN so apologies if I'm asking stupid questions.
Cheers."
"```
I am reproducing your work in ""Context Encoders: Feature Learning by Inpainting"" and I have trained the networks successfully. However, now I face a problem. I noticed that the netG extracts the features of a picture and then uses the features to fix the missing region of the picture. I am very interested in the features(i.e.,the 4000-unit bottleneck) and I hope to split the netG into two networks, an encode net and a decode net, so that I can use the encode one to transpose a picture into a vector or use the decode one to transpose a vector into a picture. I have no idea how to do it.
I once tried to use "":add"" to put an encode net and a decode net together into a netG but after training I found they are both nil while the netG worked well.
```
"
"When I want to train my own model with my dataset, there is no explanation about the organization method of the data. I just put images into the folder `dataset/train` and `dataset/val`, but received a error:

```
/home/**/torch/install/bin/luajit: /home/**/torch/install/share/lua/5.1/threads/threads.lua:183: [thread 4 callback] /home/**/context-encoder/data/dataset.lua:202: Could not find any image file in the given input paths
stack traceback:
    [C]: in function 'assert'
```

@pathak22 
"
"```
   netD:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)
   netG:apply(function(m) if torch.type(m):find('Convolution') then m.bias:zero() end end)
```

We can see these code in fGx and fDX, I can' understand these code. 

The bias value is determined at the last iterator training phase ?
"
"I am attempting to run your code, but I am encountering an error when running train.lua.

The only changes I made to the code were commenting out some of the trainHook function in donkey_folder.lua since I did not want my images to be randomly cropped and changing the number of channels to be 1 since all of the images in my data are 128 x 128 grayscale images.

I am running the code on Ubuntu 16.04 on CPU only. This is the latest version of torch. I have installed all required dependencies by running luarocks install torch, luarocks install nn, luarocks install optim, luarocks install threads, luarocks install argcheck, luarocks install sys, luarocks install xlua, and luarocks install image.

I've been going nuts for 3 days trying to resolve this error. I'd be grateful for any help I could get.

anne@Anne:~/Desktop/context-encoder$ DATA_ROOT=dataset/train th train.lua
{
  ntrain : inf
  nc : 1
  noiseGen : 0
  beta1 : 0.5
  nThreads : 4
  display_iter : 50
  niter : 25
  batchSize : 64
  ndf : 64
  fineSize : 0
  nz : 100
  wtl2 : 0
  loadSize : 0
  gpu : 0
  ngf : 64
  conditionAdv : 0
  noisetype : ""normal""
  lr : 0.0002
  manualSeed : 0
  name : ""train1""
  overlapPred : 0
  nBottleneck : 100
  nef : 64
  display_id : 10
  display : 0
}
Seed: 5354  
Starting donkey with id: 2 seed: 5356
table: 0x418bb2f8
Starting donkey with id: 1 seed: 5355
table: 0x412fd340
Starting donkey with id: 4 seed: 5358
table: 0x41b19aa8
Starting donkey with id: 3 seed: 5357
table: 0x41e394e0
Loading train metadata from cache
Loading train metadata from cache
Loading train metadata from cache
Loading train metadata from cache
Dataset Size:     200000  
LR of Gen is     1    times Adv  
NetG:    nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> (16) -> (17) -> output](1): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> output](1): nn.SpatialConvolution(1 -> 64, 4x4, 2,2, 1,1)
    (2): nn.LeakyReLU(0.2)
    (3): nn.SpatialConvolution(64 -> 64, 4x4, 2,2, 1,1)
    (4): nn.SpatialBatchNormalization
    (5): nn.LeakyReLU(0.2)
    (6): nn.SpatialConvolution(64 -> 128, 4x4, 2,2, 1,1)
    (7): nn.SpatialBatchNormalization
    (8): nn.LeakyReLU(0.2)
    (9): nn.SpatialConvolution(128 -> 256, 4x4, 2,2, 1,1)
    (10): nn.SpatialBatchNormalization
    (11): nn.LeakyReLU(0.2)
    (12): nn.SpatialConvolution(256 -> 512, 4x4, 2,2, 1,1)
    (13): nn.SpatialBatchNormalization
    (14): nn.LeakyReLU(0.2)
    (15): nn.SpatialConvolution(512 -> 100, 4x4)
  }
  (2): nn.SpatialBatchNormalization
  (3): nn.LeakyReLU(0.2)
  (4): nn.SpatialFullConvolution(100 -> 512, 4x4)
  (5): nn.SpatialBatchNormalization
  (6): nn.ReLU
  (7): nn.SpatialFullConvolution(512 -> 256, 4x4, 2,2, 1,1)
  (8): nn.SpatialBatchNormalization
  (9): nn.ReLU
  (10): nn.SpatialFullConvolution(256 -> 128, 4x4, 2,2, 1,1)
  (11): nn.SpatialBatchNormalization
  (12): nn.ReLU
  (13): nn.SpatialFullConvolution(128 -> 64, 4x4, 2,2, 1,1)
  (14): nn.SpatialBatchNormalization
  (15): nn.ReLU
  (16): nn.SpatialFullConvolution(64 -> 1, 4x4, 2,2, 1,1)
  (17): nn.Tanh
}
NetD:    nn.Sequential {
  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> output](1): nn.SpatialConvolution(1 -> 64, 4x4, 2,2, 1,1)
  (2): nn.LeakyReLU(0.2)
  (3): nn.SpatialConvolution(64 -> 128, 4x4, 2,2, 1,1)
  (4): nn.SpatialBatchNormalization
  (5): nn.LeakyReLU(0.2)
  (6): nn.SpatialConvolution(128 -> 256, 4x4, 2,2, 1,1)
  (7): nn.SpatialBatchNormalization
  (8): nn.LeakyReLU(0.2)
  (9): nn.SpatialConvolution(256 -> 512, 4x4, 2,2, 1,1)
  (10): nn.SpatialBatchNormalization
  (11): nn.LeakyReLU(0.2)
  (12): nn.SpatialConvolution(512 -> 1, 4x4)
  (13): nn.Sigmoid
  (14): nn.View(1)
}
/home/anne/torch/install/bin/luajit: /home/anne/torch/install/share/lua/5.1/threads/threads.lua:183: [thread 1 callback] /home/anne/Desktop/context-encoder/data/dataset.lua:328: inconsistent tensor size at /tmp/luarocks_torch-scm-1-667/torch7/lib/TH/generic/THTensorCopy.c:7
stack traceback:
    [C]: in function 'copy'
    /home/anne/Desktop/context-encoder/data/dataset.lua:328: in function 'tableToOutput'
    /home/anne/Desktop/context-encoder/data/dataset.lua:345: in function </home/anne/Desktop/context-encoder/data/dataset.lua:335>
    [C]: in function 'xpcall'
    /home/anne/torch/install/share/lua/5.1/threads/threads.lua:234: in function 'callback'
    /home/anne/torch/install/share/lua/5.1/threads/queue.lua:65: in function </home/anne/torch/install/share/lua/5.1/threads/queue.lua:41>
    [C]: in function 'pcall'
    /home/anne/torch/install/share/lua/5.1/threads/queue.lua:40: in function 'dojob'
    [string ""  local Queue = require 'threads.queue'...""]:15: in main chunk
stack traceback:
    [C]: in function 'error'
    /home/anne/torch/install/share/lua/5.1/threads/threads.lua:183: in function 'dojob'
    /home/anne/Desktop/context-encoder/data/data.lua:81: in function 'getBatch'
    train.lua:281: in function 'opfunc'
    /home/anne/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'adam'
    train.lua:413: in main chunk
    [C]: in function 'dofile'
    ...anne/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00405d50
"
""
"Hello @abursuc and @feichtenhofer, can you tell how the dictionary is structured in the saved .mat weight file? I tried loading them as usual by simply referencing the keys, but the output does'nt make any sense.

It would be great if you could give some pointers on how to load the dictionary properly so that I can create a model instance and load them. "
"My speed is ""train: epoch 01:   1/994: lr: 1e-03, 1.2 Hz "" with a GTX 1060,it's too low and doesn't make sense. Anyone have the same question?"
"After download the dataset and run 'cat ucf101_jpegs_256.zip* > ucf101_jpegs_256.zip', I try to unzip the file, use the command 'unzip ucf101_jpegs_256.zip', but a problem occured, 'jpegs_256/v_TrampolineJumping_g09_c05/frame000020.jpg  bad CRC 5860d5eb  (should be b9b7fa4a)', is the picture damaged?"
"Hello, I would like to know whether there is a version which is based on Pytorch, and if there is, could that be possible to be sharing to me !
Thanks a lot!"
"there are many "".bin"" files in 'U' folder,can you upload the dataset again, thanks"
"I have downloaded your pre-computed RGB images, but I have some problems in merging the three parts (ucf101_jpegs_256.zip.001, ucf101_jpegs_256.zip.002, ucf101_jpegs_256.zip.003) into one .zip file. I wonder if some files (e.g. ucf101_jpegs_256.zip) are missing?"
"Hi @feichtenhofer,

Thanks for sharing this amazing work. For my work, I also wanted to calculate flow for Olympic Sports dataset also by using the _**gpu_flow**_ library that you have shared. Can you please share with me the recommended values for the following options, so that the flow are coherent with the precomputed flows that you shared. 

1) skip
2) stride
3) clipFlow 
4) MIN_SZ 
5) OUT_SZ 

I would be happy to share the pre-computed flow so that others can also benefit and save some time."
"Hi @feichtenhofer ,


Thank you for your work and for the pretrained models and processed data.

l just downloded your precomputed optical flow images (part1, part2, part3) 
 https://github.com/feichtenhofer/twostreamfusion#data

After unzipping the file l get the following directory :  

`/precomputed_flow/tvl1_flow` which is composed of   **u** and **v** sub-directories.

Do **u** and  **v** stands respectively for horizontal components and vertical components ? 

Thank you for your answer 


"
"Hi,

Could you please provide the SVM scores of the FV-encoded IDT descriptors that you used late fusion in the paper? My interest is purely for academic purposes.

Thank you in advance."
"Hello, I want to test the twostream method by just providing half of each video in UCF101. However, I found that the numbers of frames of each video were written into the pretrained models. Does anyone know which part should I changed to make that work?"
"When I run ""**cnn_ucf101_spatial.m**"",I got this:


`**error dagnn.Layer/load (line 191)
        obj.(f) = s.(f) 
error dagnn.DagNN.loadobj (line 27)
    block.load(struct(s.layers(l).block)) ;
error cnn_ucf101_spatial (line 112)
  net = dagnn.DagNN.loadobj(net);**
`

How can i fix this problem?
"
"The link to pretrained model for ResNet is not available, anyone else encounter this situation?"
"No public property dilate exists for class dagnn.Conv
Error in dagnn.Layer/load(line 191)
obj.(f)=s.(f)
error in dagnn.dagnn.loadobj(lin27)
block.load(struct(s.layers(l).block))
error in cnn_ucf101_spatial(line112)
net=dagnn.DagNN.loadobj(net)
Another question:your dataset has being framed?And if I train on my own dataset,do I only need to use cnn_ucf101_spatial.m,cnn_ucf101_temporal.m and cnn_ucf101_fusion.m??Do i need to change some configuration in other .m files?
Thanks for you attention "
"I followed your installation steps,but I got errors like this:
CMake Error at CMakeLists.txt:12 (find_package):
  By not providing ""FindQt5Widgets.cmake"" in CMAKE_MODULE_PATH this project
  has asked CMake to find a package configuration file provided by
  ""Qt5Widgets"", but CMake did not find one.

  Could not find a package configuration file provided by ""Qt5Widgets"" with
  any of the following names:

    Qt5WidgetsConfig.cmake
    qt5widgets-config.cmake

  Add the installation prefix of ""Qt5Widgets"" to CMAKE_PREFIX_PATH or set
  ""Qt5Widgets_DIR"" to a directory containing one of the above files.  If
  ""Qt5Widgets"" provides a separate development package or SDK, be sure it has
  been installed.


-- Configuring incomplete, errors occurred!
Do I need to install Qt5.7?? I don't know how to add path to cmake.txt,would you give me some advice??
Thank you very much "
"cnn_ucf101_get_im_flow_batch.m(258)
if flip
          sx = fliplr(sx) ;
          imo(:,:,:,i,si) = imt(sy,sx,:) ;
          imo(:,:,1:2:nStack,i,si) = -imt(sy,sx,1:2:nStack) + 255; %invert u if we flip
else
          imo(:,:,:,i,si) = imt(sy,sx,:) ;
I get the error ""Subscripted assignment dimension mismatch"", so is the code wrong?"
"Hello, I am very interested in your work and I am doing some reproduction work based on your work.
Now I have two questions which make me a little confused. May I ask about them?
Is accurary in your paper only use the Validation set ?
would you use test set for your cnn model?"
"When I run cnn_ucf101_fusion.m, I get the error about 
""Subscript indices must either be real positive integers or logicals"" 
at the code
 ""last_frame = min(frameSamples(end), max(nFrames - nStack/4 - opts.nFrameStack,nStack/4 ));""
in file ""cnn_ucf101_get_im_flow_batch.m"".
It seems ""framSamples"" is empty. Does it mean that I didn't load the data successfully?
"
"Hi Feichtenhofer

I have the following version incompatibity problem as follow:

replace classifier layer of fc1000_bias
No public property dilate exists for class dagnn.Conv.

I use the matconv 23 version and the two-stream-matconv+dagnn. thanks a lot!"
""
"No read file ""\ucf101-img-vgg16-split1-dr0.85.mat"".
There is no file or directory. Thanks ."
""
opts.frameList = NaN; I have a questions about this parameter.What does it mean?
"Hi, I haven't used the matconvnet framework, so I don't understand well about the code of the lr setting. 
      opts.train.learningRate =  [1e-3*ones(1, 3) 5e-4*ones(1, 5) 5e-5*ones(1,2) 5e-6*ones(1,2)]  ;
It confuses me, could you tell me the detail about the lr setting? thx"
Is there any reason why the weights for the HMDB51 dataset is not available? I would like to do use the trained weights on the HMDB51 for some experiments. Thanks.
"I have run the fusion.m.And found that this code may need very long tme.I ran about 10 hours and it still in epoch 01 : 600/994. The number of epoch seems to be 300.So this program need about 150 days????Is there something wrong with me?I ran this in one gpu:Tesla k40c. And according to other joiners,muti-gpu can't work?  "
"Hi @feichtenhofer ,
I think there is some issue with matconvnet version used in this code. 
When I am using the matconvnet uploaded to this repo, I get the error about ""No public property dilate exists for class dagnn.Conv"". 
I tried using the latest version of matconvnet, but I get this error: ""The class dagnn.DagNN has no Constant property or Static method named 'setLrWd'. "" 
So, apparretnly, none of these versions work. I was wondering if you happen to see this issue? Could you please help me with this?
Thank you,
Best. "
"Thanks for your sharing!
Could you please tell me how to implement your fusion between two models(rgb vgg and optical flow vgg) with caffe？And I only want to fuse the fc8 layer of both models."
"@feichtenhofer Hi, The dataset cannot be download from the link you give. Could you please check the link. Thanks for your attention."
"hello, I got following error:
Expected input number 1, A, to be nonempty.
Error in imresize>parsePreMethodArgs (line 333)
validateattributes(A, {'numeric', 'logical'}, {'nonsparse', 'nonempty'}, mfilename, 'A', 1);
Error in imresize>parseInputs (line 248)
[params.A, params.map, params.scale, params.output_size] = ...
Error in imresize (line 141)
params = parseInputs(varargin{:});
Error in cnn_ucf101_get_im_flow_batch (line 248)
            imt =   imresize(gather(imt(dy:sz(1)+dy-1,dx:sz(2)+dx-1,:)), [opts.imageSize(1:2)]);
Error in getBatchWrapper_ucf101_rgbflow>getBatch (line 52)
im = cnn_ucf101_get_im_flow_batch(images, opts, ...
Error in getBatchWrapper_ucf101_rgbflow>@(imdb,batch,moreopts)getBatch(imdb,batch,opts,numThreads,trainopts,moreopts) (line 3)
fn = @(imdb,batch, moreopts) getBatch(imdb,batch,opts,numThreads, trainopts, moreopts) ;
Error in cnn_train_dag>process_epoch (line 306)
    [inputs] = state.getBatch(state.imdb, batch, moreopts) ;
Error in cnn_train_dag (line 108)
    s_train = process_epoch(net, state, opts, 'train');
Error in cnn_ucf101_fusion (line 349)
[info] = cnn_train_dag(net, imdb, fn, opts.train) ;
"
Can you please guide me that which code one can use at test time.
"hi, when I use one gpu, the code works well.
But when I change opts.train.gpus = 1; (in cnn_ucf101_temporal.m) to opts.train.gpus = [1,2]; it was fail:

Error using cnn_train_dag (line 120)
Error detected on worker 1.

Error in cnn_ucf101_temporal (line 231)
[info] = cnn_train_dag(net, imdb, fn, opts.train) ;

Caused by:
    Error using cnn_train_dag>map_gradients (line 503)
    Invalid file identifier. Use fopen to generate a valid file identifier.
 

I've tested  opts.train.gpus = [1,2] on the mnisit in Matconvnet example, it works well. anyone could help me?
"
"Hi, thank you a lot for your great work.
I ran your code for several iterations. And I noticed that the error_1(val) for each iteration, is just the same as the error_1(val) of the last val epoch of the iteration.
Shouldn't it be the average of every epoch?

"
"Hi,
I followed the README guide, and run the compile.m, but met the following error: 
incorrect use make_all>check_clpath (line 385)
Unable to find cl.exe

error make_all (line 207)
      check_clpath(); % check whether cl.exe in path

error run (line 96)
evalin('caller', [script ';']);

error compile (line 23)
run(fullfile(fileparts(mfilename('fullpath')), "
"Hello I am very appreciate for your work. And I now I start to try to reproduce your work in HMDB51 case by your code. But I noticed some parts for HMDB51 case is nor included in your webpage, such as 'cnn_hmdb51_setup_data.m', and baseline networks for hmdb51. 

I have just run the training and testing in HMDB51 case by adapting from your UCF case (e.g. adapt your ucf101 baseline networks into networks for 51 categories).

 But my result is a little weird. Could you please kindly upload your HMDB51 part code and network models?

Thank you very much for your precious time."
"Hi, I would like to directly extract feature from the pretrain nets. How can I modify the code since cnn_ucf101_fusion.m is supposed to train the nets. 
"
""
"RGB and optical flow image numbers do not match in some classes. The most matches.

After unzipping these two

- HMDB51 RGB: [part1](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/hmdb51_jpegs_256.zip)
- HMDB51 Flow: [part1](http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/hmdb51_tvl1_flow.zip)

For example, `50_FIRST_DATES_kick_f_cm_np1_ba_med_19`, it has 1 more RGB image than optical flow image.

- `jpegs_256`
    - `50_FIRST_DATES_kick_f_cm_np1_ba_med_19` (48 images)
        - `frame000001.jpg`
        - `frame000002.jpg`
        - ...
    - ...
- `tvl1_flow`
    - `u`
        - `50_FIRST_DATES_kick_f_cm_np1_ba_med_19` (47 images)
            - `frame000001.jpg`
            - `frame000002.jpg`
            - ...
        - ...
    - `v`
        - `50_FIRST_DATES_kick_f_cm_np1_ba_med_19` (47 images)
            - `frame000001.jpg`
            - `frame000002.jpg`
            - ...
        - ..."
"If i don't want to use the dataset from UCF101 or HMDB51.Can i make my own dataset? What are the detailed steps for making your own data set? Hope to get an answer.Thank you very much. 
"
"Hello everyone, I was running the compile.m file and matlab seems have stucked into the following output line. I wonder if anyone has met the same problem and does anyone have some ideas on this problem?

I was using matlab 2016a with cuda 8.0 and g++ 4.7.5 version.

vl_compilenn: MEX LINK: -outdir /home/myhome/twostreamfusion/matconvnet/matlab/mex -lmwblas -ljpeg -L/usr/local/cuda-8.0/lib64 -lcudart -lcublas -lmwgpu -L/usr/local/cuda-8.0/lib64 -lcudnn -largeArrayDims LDFLAGS=$LDFLAGS -Wl,-rpath -Wl,""/usr/local/cuda-8.0/lib64"" LDFLAGS=$LDFLAGS -Wl,-rpath -Wl,""/usr/local/cuda-8.0/lib64"" /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/vl_nnconv.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/data.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/datamex.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnconv.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnfullyconnected.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnsubsample.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnpooling.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnnormalize.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnbnorm.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/nnbias.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/im2row_cpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/subsample_cpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/copy_cpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/pooling_cpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/normalize_cpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/bnorm_cpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/tinythread.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/imread.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/im2row_gpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/subsample_gpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/copy_gpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/pooling_gpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/normalize_gpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/bnorm_gpu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/datacu.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/nnconv_cudnn.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/nnbias_cudnn.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/nnpooling_cudnn.o /home/myhome/twostreamfusion/matconvnet/matlab/mex/.build/bits/impl/imread_libjpeg.o"
"14 hours per epoch on titan X, nvidia-smi shows that GPU is used. And the results is not very good. The loss is like that:
![image](https://cloud.githubusercontent.com/assets/17739876/26310363/18553886-3f33-11e7-97f4-1bc8b24951a1.png)
 Is that normal?
"
"When I am unzipping the dataset file it asks for a password for unzipping it.

Can you please provide the password for unzipping the dataset?"
"I follow the setup procedure and  download the data and model. But when I run cnn_ucf101_fusion(), it occurs one problem ""unable to read file 'models\ucf101-img-vgg16-split1-dr0.85.mat'. No such file or directory"". And I check the pretrained models document below, there is no such file. Could you please tell me where I can get this file? And also later the  'models\ucf101-TVL1flow-vgg16-split1-dr0.9.mat'.
Thanks!"
"I am trying to use multiple GPUs to train the model. So I changed the default value of opts.train.gpus in the cnn_ucf101_fusion file from [1] to [1 2]. But it reports the following error:

line 503 int cnn_train_dag (this is the line that call fwrite)
the file id  is invalid. Please create a valid file id.

Could you please give some instruction on how to fix it? Thanks!"
"Hello. 
I downloaded your code for the paper 'Two-Stream Convolutional Networks for Action Recognition in Videos', and run it directly. I tried many times but I can only get a result of 91.5%. 
Should I change somewhere in your code to get the  92.5% in your paper?"
"when I adjusted your code to the method that use colour images only with 3D filter (get ride of fusion layer), an interesting thing I find is that the result of err1 and err1_spatical are always different, observe from the code, err1_spatial is extracted from dagnn.Loss layers

function stats = extractStats(net)
% -------------------------------------------------------------------------
sel = find(cellfun(@(x) isa(x,'dagnn.Loss'), {net.layers.block})) ;
stats = struct() ;
for i = 1:numel(sel)
  stats.(net.layers(sel(i)).name) = net.layers(sel(i)).block.average ;
end

and err1 is computed by comparing difference between label and prediction 

function [err1, err5] = error_multiclass(opts, labels, predictions)
% -------------------------------------------------------------------------
[~,predictions] = sort(predictions, 3, 'descend') ;
error = ~bsxfun(@eq, predictions, reshape(labels, 1, 1, 1, [])) ;
err1 = sum(sum(sum(error(:,:,1,:)))) ;
err5 = sum(sum(sum(min(error(:,:,1:5,:),[],3)))) ;

so what's the relation between them?
"
"Hi, How many epochs you have used for the training the fusion network? The number of 2000 epochs, which is mentioned in the code, is correct? In my computer system each epoch take 1 day to be completed..

Thanks
"
"  There are 3 models for each kind of pretrained models, for example, ucf101-img-vgg16-split1.mat, ucf101-img-vgg16-split2.mat and ucf101-img-vgg16-split3.mat. I find that you set nSplit=1 in the cnn_ucf101_fusion.m. What do the other two models mean and what are the differences among these three model? Thank you for your help.
"
"The links are invalid for download the datasets and models.
Could you please renew the link you offered so we could download the **datasets** and **models** for study?
Thanks a lot!

The invalid links ↓
![See](https://ooo.0o0.ooo/2016/09/28/57ebd1b46fba7.png)
"
"We are trying to fine tune our own dataset using your method from the vgg-16 model however when we run cnn_ucf101_spatial.m it crashes on line 125 with the following output:

`Undefined function or variable 'replace_last_layer'`

We have looked for the function within your repository and couldn't find it.

Thanks
"
"Hi feichtenhofer,
Thanks for providing RGB data and flow data of UCF101 dataset, It can really reduce my work on these issue and really speed up my experiments:+1:. I wonder if you can provide RGB and flow data of HMDB51 dataset? Sorry for this unreasonable request. 
"
"Hello, I am very interested in your work and I am doing some reproduction work based on your work.
Now I have two questions which make me a little confused. May I ask about them?

1.How do you get your final prediction? For example, if I fuse from 'temporal' to 'spatial', should I only use the prediction  of spatial net or both of the two nets? And when you got your best result in your paper, the 'nFramesPerVid' you used is also only 1?

2.Which of these two performs better in your experiment? Fuse from 'temporal' to 'spatial' or fuse from 'spatial' to 'temporal'?

I am sorry for taking your time and thank you a lot for reading my questions. I'd appreciate it a lot if you could kindly answer my questions.  
"
"i wonder is there a split.txt in the code?  can you give some more detailed about the format? 

here is my error:

error cnn_ucf101_setup_data>get_UCF101_split (line 67)
classes =      textscan(fid, '%s');

 error cnn_ucf101_setup_data (line 16)
[Training_set, Testing_set, cats]=
feval(sprintf('get_%s_split','UCF101'),opts.nSplit,opts.splitDir);

error  cnn_ucf101_fusion (line 94)
      imdb = cnn_ucf101_setup_data('dataPath', opts.dataPath, 'flowDir',opts.flowDir,
      'nSplit', opts.nSplit) ;
"
"When I run cnn_ucf101_fusion(), I got a warning ""Warning: Could not find appropriate function on path loading function handle /home/nvidia/Dropbox/mybox/PhD/ActionRecognition/THUMOS14/action_devkits/cnn_train_dag.m>extractStats"" when loading the pretrained models. I also got an error ""Undefined function or variable 'cnn_train_dag'"" when running the last sentence ""[info] = cnn_train_dag(net, imdb, fn, opts.train) ;"". It seems that there is a missing function ""cnn_train_dag.m"". I used the default function ""cnn_train_dag.m"" of matConvNet but it still didn't work. Could you update the function ""cnn_train_dag.m"" or tell me how to solve this error. Thanks a lot.
"
"do you mind double check you Pre-computed optical flow images and resized rgb frames for the UCF101 and HMDB51 datasets? I tried download and open your data twice, still not able to open it
"
"Hi, is your code support latest official MatConvNet?
"
"Hello! I am doing the reproducing experiment on your code and I met  problems.

---

In cnn_ucf101_fusion.m:

addpath('network_surgery');   %% line 20
[ netB ] = insert_conv_layers( netB, fusionLayerB(end), 'initMethod', opts.initMethod ); %% line 160

---

However, I cannot find the path 'network_surgery' and the function ""insert_conv_layers"" in your folder. May ask if I missed something?

Thank you very much.
"
Where can I find the implement of  global loss  of pairwise similarity siamese network? Is there a pytorch version?
""
"I currently use DeepFool as an estimate for the distance to the decision boundary of a classifier, as suggested in the paper: https://arxiv.org/abs/2002.01810v1

I used the implementation from torchattacks because I found the implementation from the LTS4/DeepFool repo hard to understand and work without throwing errors. In the implementation from torchattacks I found an implementation error, that returned the original images as perturbation when the classifier already misclassified the image. https://github.com/Harry24k/adversarial-attacks-pytorch/issues/51

This might also be the case in this repo, but I havent checked, just wanted to let you know."
"Instead of these lines 
tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=map(lambda x: 1 / x, std)),
                        transforms.Normalize(mean=map(lambda x: -x, mean), std=[1, 1, 1]),
                        transforms.Lambda(clip),
                        transforms.ToPILImage(),
                        transforms.CenterCrop(224)])

it should be like 

tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=**list(map(lambda x: 1 / x, std))**),
                            transforms.Normalize(mean=**list(map(lambda x: -x, mean))**, std=[1, 1, 1]),
                            transforms.Lambda(clip),
                            transforms.ToPILImage(),
                            transforms.CenterCrop(224)])
i
"
"In line 51, that is:
`clip = lambda x: clip_tensor(x, 0, 255)`
should be:
`clip = lambda x: clip_tensor(x, 0, 1)`

In tf,` transforms.Lambda()` is in front, and there is no multiplication by 255 which will be operated by `transforms.ToPILImage() `.

But this should not affect the final result."
"Use test_deepfool.py to test on two test pictures. When the generated disturbed picture enters the network again for forward calculation, the category has not changed.
The images directly generated by deepfool have relatively large changes. After performing some normalization on the disturbed images generated by deepfool in the code, it is obviously closer to the original image, but the category of the generated image has not changed.
for example:
For the picture test_im1.jpg: the original category is macaw, and the deepfool shows that the disturbed picture category is flamingo (the picture directly output by the network has changed dramatically).
But after a series of other operations, it is very similar to the original image, but at the same time, the category is still macaw."
"When running code on binary classification, does it mean that we only need to modify the num_classes = 2 instead of 10 as default?

When I tried it, I found a lot of perturbed samples as NaN value. Have you ever found this before?"
"Hello,I have a question that when I save the adv_img using ""plt.savefig('C:/Users/acer/Desktop/try/deepfool_adv_img.jpg')"",then I put the path of the adv_img I have saved into input_path,but Original label doesn't changed,doesn't it should be the Adversarial label?And what makes this happens?
Thanks!
"
"
  File ""C:\Users\acer\Anaconda3\lib\site-packages\torchvision\transforms\functional.py"", line 207, in normalize
    std = torch.tensor(std, dtype=torch.float32)

TypeError: must be real number, not map

<Figure size 432x288 with 0 Axes>"
"I found an issue when running the python test_deepfool code. Could be my machine, but I this is what I ran into with a fresh pytorch 0.1.12
The test file doesn't work right away for me, I had to change a couple indexing lines that were giving me trouble. 

```python
#Line 81 in deepfool.py should be 
fs = net.forward(x)
#line 53 in test_deepfool.py then is this
plt.imshow(np.clip(tf(pert_image.cpu()[0]), 0, 255))
```
For me this fixed a 5D variable that was being passed into resnet as input, instead of a 4D like it wants. "
"The training code is stopped automatically at epoch 10 due to **key error 10**. I'm not exactly getting why this error occurs.
Could you help me to solve this error.
```Traceback (most recent call last):
  File ""main.py"", line 67, in <module>
    main()
  File ""main.py"", line 64, in main
    main_worker(args.rank, args)
  File ""main.py"", line 54, in main_worker
    trainer.test(epoch)
  File ""/home/DeepDeblur-PyTorch-master/src/train.py"", line 196, in test
    self.evaluate(epoch, 'test')
  File ""/home/DeepDeblur-PyTorch-master/src/train.py"", line 182, in evaluate
    tq.set_description(self.criterion.get_loss_desc())
  File ""/home/DeepDeblur-PyTorch-master/src/loss/__init__.py"", line 311, in geoss_desc
    loss = self.loss_stat[self.mode]['Total'][self.epoch]
KeyError: 10 

"
"Hi @SeungjunNah !
I'm so appreciated you provide us with your work to learn more.I want to test the work on my device .As I know , the current result shows that everything loads from the downloaded files,including the loss ,psnr and ssim.What I want to do is that after loading the model weight that you provide, then test on my device and result new loss, psnr and ssim.I want to know is it can be done? If so ,could you help me?"
"Hi, I've been trying to run this code on my custom dataset but couldn't understand the directory structure. Can someone help"
"I spent many hours trying to get this to work under Windows. I managed to get it to work now, so this is probably useful to others.

## Setup
The first obstacle is the `readline` Python package, which seems to be default on Unix systems, but not on Windows. For this, simply install the `pyreadline`  package, which is a Windows port of readline.

## Understanding the command-line
Example command: `python main.py --save_dir REDS_L1 --demo_input_dir d:/datasets/motion47set/noise_only --demo_output_dir ../results/motion47set`. 
Explanation: specifying `--demo_input_dir` (or `--demo true`) will run an evaluation, using a pretrained model as specified in `--save_dir`. Every image of my motion47set will be evaluated. The results will be saved alongside the folders `src` and `experiments` at the project root, in a folder `results/motion47set`.
Note that even getting this far is not very intuitive, as others have already pointed out. Usually there is a separate python script for just evaluation/testing/inference. Next, the term demo is a bit unusual, at first I was expecting some interactive demonstration of some form. The `save_dir` I had at first used as what `demo_output_dir` does.
Another word of caution, if the output path is given without any `.`, it somehow ends up saving the results at `d:/results/motion47set`, which again took me a while to figure out, i.e. on the root of the same drive that the project is located at. I suggest printing out the absolute output dir with `os.path.abspath` to the user at some point, for clarity.

## Bug
Running the above command will produce the following output:
```
===> Loading demo dataset: Demo
Loading model from ../experiment\REDS_L1\models\model-200.pt
Loading optimizer from ../experiment\REDS_L1\optim\optim-200.pt
Loss function: 1*L1
Metrics: PSNR,SSIM
Loading loss record from ../experiment\REDS_L1\loss.pt
===> Initializing trainer
results are saved in ../results/motion47set
|                                                        | 0/90 [00:00<?, ?it/s]Can't pickle local object 'MultiSaver.begin_background.<locals>.t'
|██▏                                             | 4/90 [00:06<02:14,  1.56s/it]Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""d:\Program Files\Anaconda3\envs\torch gpu\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""d:\Program Files\Anaconda3\envs\torch gpu\lib\multiprocessing\spawn.py"", line 115, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
|█████▋                                         | 11/90 [00:10<01:12,  1.09it/s]forrtl: error (200): program aborting due to control-C event
```
Also note that ctrl+c takes a really long time to terminate for me, and even slows down my entire machine for several seconds.

This is difficult to debug, because there is no fatal exception, and everything seems to run normally, ignoring the errors, which might also just be warnings, for all we know. I did not realize for a while that MultiSaver is a file of this project, which is why there is not much help online in regards to this error/warning. Second, the only that that gives a little stronger hint that this is an error, and not a warning, is the `EOFError`, which I still don't know why or where it even happens. A large part of debugging time was me assuming these were just warnings, and trying to fix the command-line arguments instead, since that is easy to get wrong.

What is actually happening is that the MultiSaver code runs clean on the main thread, but then each spawned thread/process will fail, without the main thread being aware. As a result, the program runs through, attempts to save the output images, which all do nothing since the threads/processes already died. I'm not sure how to to achieve this, but it would be nice if the program stops running when it is unable to save output images (at least in demo mode, where that's about the only purpose).

The keywords to locate the actual issue here are `pickle` and `multiprocessing`. Going into `utils.py` and looking at the class `MultiSaver` shows us a method `begin_background`, with a method-local variable `t` (another method). Defining that method works, however (under Windows) that variable has to be pickled/serialized to hand it over to the `mp.Process`, which will run it in a different thread/process. This fails because pickle does not support local objects.
I tried various ways to change the scope of `t`:
- put `global t` before the definition of `t` (no change)
- move `t` to the outermost scope of the file utils.py, i.e. same level as MutliSaver (can pickle the method, but later fails at a different point)
- the solution that works is putting `t` on the same scope as MultiSaver, and annotating it with `@staticmethod`. The annotation avoids the first method parameter to be used as `self`.

So my modification looks like this
```python
class MultiSaver():
...

    @staticmethod
    def t(queue):
        ...

    def begin_background(self):
        self.queue = mp.Queue()
        
        worker = lambda: mp.Process(target=MultiSaver.t, args=(self.queue,), daemon=False)
    ...
...
```

After this change, everything works as expected. I haven't tested it, but I suspect this will still work under Unix as well.
I'm not sure if this will work if multiple instances of `MultiSaver` are created, and maybe this would give the same result as putting `t` to the outermost scope, i.e. fail again."
"Hi DeepDeblur Team,
Thanks for your great work

I am really sorry to say that, but the steps to run this project on personal device is so complected. I have spent around 4 hours looping around, modifying and testing, still not able to run the demo. inputs are out of numbers and totally unorganized

I want to ask if there is any modification or restructuring is going to be implemented in this regards (as adding some configuration files for the input parameters)

Also, I noted that it doesn't work without GPU, is there is any intention to implement it for both cases where GPU is available and not?

"
"I have tried to run the code on my own dataset. While training the code, I got this error:
from . import pypocketfft as pfft
ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /home/KB/anaconda3/lib/python3.7/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-37m-x86_64-linux-gnu.so)

I have installed latest GCC using conda install -c conda-forge gcc=12.1.0, but the error still persists. 
"
"I spent many hours trying to get this to work under Windows but couldn't understand the error just like that.Can someone help
![捕获](https://user-images.githubusercontent.com/110209555/181665075-5484d5d7-0945-440a-b7b4-6490471933ec.PNG)
"
"Results form demo(test image is from GOPRO):
![0000011](https://user-images.githubusercontent.com/108379675/177005466-81766410-40ee-4219-a806-1f45acc37eab.png)
![0000014](https://user-images.githubusercontent.com/108379675/177005467-4d456154-7221-4d43-b0c2-c5803c1bb43f.png)

Environment:
Windows 10 with Anaconda3,
python 3.7.13, pytorch 1.7.0, cudnn 8.2.1, cudatoolkit 11.0.221,
(Because RTX3060 in my machine do not surpport cuda<=10,
and pytorch 1.6 do not surpport cuda>=11, I cannot use pytorch 1.6.0)
tqdm 4.64.0, imageio 2.9.0, scikit-image 0.18.3, numpy 1.19.2, matplotlib 3.5.1, pyreadline 2.1

Test Command:
`python main.py --save_dir GOPRO_L1 --demo true --demo_input_dir GOPRO_Large/MyTest --demo_output_dir GOPRO_Large/MyResult`

I modified `utils.py` as [this issue](https://github.com/SeungjunNah/DeepDeblur-PyTorch/issues/18), and changed nothing else.
I want to know why I got this result and what should I do to get correct result.


"
"Dear author,
Thank you at first for your great work!

I am trying to use your implementation of `DistributedEvalSampler` for an evaluation purpose, jointly with DDP.
(with `shuffle=Flase` and no calling of `set_epoch()`; after calling `DistributedEvalSampler` for yielding test samples on evaluating a model, my program should be finished)

At the end of the script, my program hangs with charging 100% of GPU utilization in all 2 of 3 GPUs.
(the last device is soley terminated with no errors)
When replaced with `DistributedSampler`, this is not occurred.

I doubted it is because of the logging (e.g., Wandb) is occurred at rank 0 device, 
but it is not the root cause as it is still occurred when I turned off the logging tool.

I wonder if you could point out conditions that I missed, please?
Thank you in advance.


Best,
Adam"
""
"Hello sir,
I have interesting image deblurring and i searched your code.
I had tried your sample data(gopro).
I checked your sample data that is paired datasets.(blurred image vs sharp image)
But i have blurred images only.
Can i try to train your code??

Thanks,
Edward Cho."
"Running: Ubuntu.

Executed command: 
`python main.py --save_dir GOPRO_L1 --demo true --demo_input_dir Input`

Get error:
loss = self.loss_stat[self.mode]['Total'][self.epoch] KeyError: 1001"
"I am running on Anaconda, via terminal.

Any ideas?"
"Thanks for the inspiring work! However, I have a concern on how to generate blurry images. It said ""We took 240 fps videos with GOPRO camera and then averaged varying number (7 - 13) of successive latent frames to produce blurs of different strengths.""

Does it mean blurry image can be obtained by only averaging some images with 240Hz frame rate?
"
"Hi, thanks for your awesome contribution. I want to reimplement your source but the dataset can not be downloaded. 
Can you give a check for that, Thank in advance"
"Hi,
Thanks for sharing your code! I have a question and hope to get your answer.

I found that the image crop into patch_size to training, the initial value of px and py is randomly determined by random.randrange. In this random case, how to ensure that the entire picture can be fully trained?
https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/e92dc7c3337127887da617ffe819b49cb022fd81/src/data/common.py#L32

Looking forward to your reply! "
"Hi,

Amazing contribution I must say, thanks for that.
I was more interested so I digged into the code and implementation, I wanted to produce evaluation results on the test dataset of GOPRO_Large provided by you.

**What i have done**: I have just downloaded, unzipped and placed the trained models and the dataset into their respective folders as suggested in the readme and comments across the code. 

Now **what am I exactly looking** for is the accurate command that just picks up the trained model and uses it to reproduce results on the testing data(right, those 1111 number of images out of 3214 total) , I don't want to train the whole model for that again and neither do I wanna use the demo functionality provided by you, I wanted to run evaluation of model on testing data standalone. However I was little confused at few places while reading your code base that what do I need to change or omit since there was no clear example command for this in the readme.

Can you please guide me through ? I am just looking for the command and/or any change I need to do in the code to run evaluation on testing data. "
"HI, thankyou for your contribution. I ran the pre-trained models REDS_L1 and GOPRO_L1 on my own images and even the example images you provided but i am getting bad results. Any idea what it could be? I am running this on windows.  Thanks in advance.

Orig Image

![blur3](https://user-images.githubusercontent.com/74458556/125900589-7093f1ae-ab7b-4a54-b43d-7d25d6eb2319.jpg)

Deblurred with REDS

![blur3](https://user-images.githubusercontent.com/74458556/125900649-f31f1845-c188-4eba-b3fe-53c7c10bb1ad.png)

DeBlurred with GOPRO

![blur3](https://user-images.githubusercontent.com/74458556/125900685-8558e5ce-a68b-4de9-8e7d-d428a4248da0.png)

Your orig example Flower

![Flower_sharp1](https://user-images.githubusercontent.com/74458556/125900755-64733f19-9026-451a-9e14-56a42844c2b8.png)

Deblurred with REDS

![Flower_blur1](https://user-images.githubusercontent.com/74458556/125901149-2e5b40ba-f858-4983-9290-04f2c21347c6.png)

Deblurred with GOPRO

![Flower_blur1](https://user-images.githubusercontent.com/74458556/125900806-e1e4eba6-b8df-4f98-801c-dd2fcbbb14af.png)

Your orig example istanbul

![Istanbul_sharp1](https://user-images.githubusercontent.com/74458556/125900855-79ea0d28-2f4e-475b-9eb4-708c371b3c08.png)

Deblurred with REDS

![Istanbul_blur1](https://user-images.githubusercontent.com/74458556/125901194-0f3f9737-d982-4b3c-a49e-a446f9aa7663.png)


Deblurred with GOPRO
![Istanbul_blur1](https://user-images.githubusercontent.com/74458556/125900887-d984a38a-f573-47c3-8a94-b7dc716248b4.png)
"
"I follow the ReadMe to download the GOPRO_L1 pretrained model and unzip it to /experiment folder. However, when I test the pretrained model with a GOPRO blurry image, the output image is like:

![000001](https://user-images.githubusercontent.com/55644082/119831087-7effe600-bf2f-11eb-92ae-e3a56c9bec2c.png)

How to deal with it? Many thanks.
"
"Hi, 
How did you prepare non-uniform motion blur data?
Is the motion blur generated by automotive camera sensors/self-driving car camera sensors similar to your case?"
"Hello Mr Author, so i tried to run demo but i have this problem, do you have any solution ? im sorry for my bad english, need ur help
![Capture](https://user-images.githubusercontent.com/50097654/116837987-46eed700-abff-11eb-81bd-812d1ac755e0.PNG)
"
"Hello,When I use demo mode on my own data, the result is strange. 
`!python launch.py --n_GPUs 1 main.py --save_dir results --demo true --demo_input_dir ../test_data --demo_output_dir Refocus`
Ori image is:
![refocus_005](https://user-images.githubusercontent.com/55578436/113985157-70c21180-987e-11eb-9506-4b86f1842d88.png)
The results is:
![image](https://user-images.githubusercontent.com/55578436/113985186-7a4b7980-987e-11eb-9718-65539e69fa8f.png)

"
"Hi

Thanks for sharing your code.
I'd like to run to the training. But, I can't. 
I already did download the code file and the GOPRO_Large file. 
So, please explain how to run the training step by step. 
![test1](https://user-images.githubusercontent.com/41188059/112943807-ef2aff00-916c-11eb-8cdb-6702fd4fe618.png)
"
"MACOS 10.15.5
PYTHON :3.6.4
PYTORCH :1.7.1

HI @SeungjunNah  : 
I try to run demo with this code :
python main.py --device_type cpu --save_dir REDS_L1  --demo true --demo_input_dir ../input --demo_output_dir ../output
then I got an error :



===> Loading demo dataset: Demo
Loading model from ../experiment/REDS_L1/models/model-200.pt
../experiment/REDS_L1/models/model-200.pt
cpu:0
Loading optimizer from ../experiment/REDS_L1/optim/optim-200.pt
Traceback (most recent call last):
  File ""main.py"", line 67, in <module>
    main()
  File ""main.py"", line 64, in main
    main_worker(args.rank, args)
  File ""main.py"", line 18, in main_worker
    optimizer = Optimizer(args, model)
  File ""/Users/youyin/Documents/AI/DeepDeblur-PyTorch/src/optim/__init__.py"", line 148, in __init__
    self.load(args.load_epoch)
  File ""/Users/youyin/Documents/AI/DeepDeblur-PyTorch/src/optim/__init__.py"", line 198, in load
    self.load_state_dict(torch.load(self._save_path(epoch), map_location=self.args.device), epoch=epoch)
  File ""/Users/youyin/.local/lib/python3.6/site-packages/torch/serialization.py"", line 593, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/Users/youyin/.local/lib/python3.6/site-packages/torch/serialization.py"", line 773, in _legacy_load
    result = unpickler.load()
  File ""/Users/youyin/.local/lib/python3.6/site-packages/torch/serialization.py"", line 729, in persistent_load
    deserialized_objects[root_key] = restore_location(obj, location)
  File ""/Users/youyin/.local/lib/python3.6/site-packages/torch/serialization.py"", line 811, in restore_location
    return default_restore_location(storage, str(map_location))
  File ""/Users/youyin/.local/lib/python3.6/site-packages/torch/serialization.py"", line 183, in default_restore_location
    + location + "")"")
RuntimeError: don't know how to restore data location of torch.FloatStorage (tagged with cpu:0)

Do you have any idea about this ?Thank you !"
"Thank you for your answer ~
As a novice, I would like to ask you another question. What are the functions of ""blur"", ""sharp"" and ""blur_gamma"" in training?
I wish you a happy life ~"
"Dear Dr. Nah:

Thank you for answering my last question! I have one final question. PSNR boosts from 29.23dB (29.08 dB for 3 scales, according to the [CVPR2017 paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf)) to 30.40dB for the single precision model.   After studying this repo and your original repo ([DeepDeblur_release](https://github.com/SeungjunNah/DeepDeblur_release)) for several days, I become curious about the key factors that boost performance.  I have the following conjectures:

(1) RGB range [0, 1] --> [0, 255]. But why does this work?

(2) Removal of the GAN loss. According to NTIRE 2020 [UniA's  paper](https://openaccess.thecvf.com/content_CVPRW_2020/html/w31/Brehm_High-Resolution_Dual-Stage_Multi-Level_Feature_Aggregation_for_Single_Image_and_Video_CVPRW_2020_paper.html), GAN loss may not help training. Besides, removal of the discriminator may help training.

(3) Learning rate schedule. Both repos train 1000 epochs using the Adam optimizer. However, the original repo sets the learning rate to its 1/10 on the 150th epoch (code/main.lua), while this code uses 5 warmup epochs and halves the learning rate (default gamma=0.5) on the milestones (the 500th, 750th, 900th epoch). The initial learning rate is 1e-4 (src/option.py, line 92) in this repo and 5e-5 in the original repo (code/opts.lua, line 53).

(4) Augmentation. The augmentation methods in this repo are slightly different from the original ones. First, this repo changes saturation with possibility 1 while the original repo changes saturation with 1/10 possibility (code/donkey.lua, line 59). Second, this repo only rotates 90 degrees while the original repo may rotate 90, 180 or 270 degrees counterclockwise.  Third, this repo uses ""flip_v"" and ""flip_h"" which flip vertically and horizontally respectively, while the original repo only uses ""flip_h"". Both repos use AWGN with sigma=2. 

(5) Change from Torch to PyTorch. This seems weird. Maybe the initialization method has changed (not sure)?

CNN is a ""black box"". Therefore, I believe nobody can answer this question except you.
I am a layman to deblurring. Would you please solve my puzzle? Thanks!"
"My god, the code is a masterpiece!

I have one simple question: what is the initialization method of the convolutional weights, e.g., Xavier initialization or He initialization? Or the code just uses the default initialization method of PyTorch? I cannot find any initialization method in the code, therefore, I think that the code uses the default initialization method of PyTorch. Am I right?

I appreciate your help."
"Dear Author, could you please reupload these files?"
"Hi，

I‘m wondering if I can use the trained result just like YOLO, e.g. :

import python.darknet as dn

dn.set_gpu(0)
net = dn.load_net(str.encode(""cfg/tiny-yolo.cfg""),
                  str.encode(""weights/tiny-yolo.weights""), 0)
meta = dn.load_meta(str.encode(""cfg/coco.data""))
r = dn.detect(net, meta, str.encode(""data/dog.jpg""))
print(r)

I'm doing some research on pose estimation and I feel like deblurring will improve the accuracy. I need to implement it on an autonomous vehicle, so it would be great if I can use the trained result directly in my script."
"hello there, how to test the test dataset, with the train image with 256256, 128128, 6464 ,how to get the result with original image , is that input the image with 1280 760 , 640*360, 320 * 180? I try this, but the result is weird, sorry for my poor English, Looking forward to your reply"
"Hello,

I downloaded your GoPro_Large training results and tested the PSNR.
it is able to achieve 30.4 in PSNR , but why it is about 29.08 in PSNR released in your paper?

"
"Hello, author, , I keep getting errors when I test,I have trained the model. My task is to test a picture of other data. Where is the picture changed, and what needs to be changed in the code part?  Thanks for your answer!

"
"I have some jpg images I'd like to run the demo on, but the output seems to be erroneous in some way. 

Here's an example input image:
![image](https://user-images.githubusercontent.com/14994206/96604477-12639f80-12ed-11eb-9016-c6ee91b118ba.png)

Here's an example output image:
![image](https://user-images.githubusercontent.com/14994206/96604554-26a79c80-12ed-11eb-9cb0-e6cf92f1c827.png)

I followed the instructions for the first demo and have not changed anything in the code. Using Pytorch 1.6, torchvision 0.7.0, and Python 3.6. Using the GOPRO_L1 pretrained experiment weights. Is there an assumption on type of image and range? "
이 부분은 한국어로 작성하도록 하겠습니다. pretrain model을 사용하여 단일 이미지에 대한 deblur 결과값을 얻고싶은데 test할 이미지를 어떤식으로 넣어주어야하는지 알려주실 수 있으실까요?
"When I'm running main.py An error occurred: valueerror: num_ samples should be a positive integer value, but got num_ samples=0

Thank you for your answer!"
""
"hi, i recently began study your paper and run your code, but have stuck into some problems when running for several days.
I copy your project in the PyCharm，
and modify some options in the 'option.py': (change the batchsize=8 and num_workers=0, test_every=1, ),
then run the 'main.py',
and the result of my console is follwed as below:
===> Loading train dataset: GOPRO_Large
===> Loading test dataset: GOPRO_Large
Loss function: 1*L1
Metrics: PSNR,SSIM
===> Initializing trainer
results are saved in ../experiment\2020-08-08_13-24-35\result
[Epoch 1 / lr 1.00e-04]
Train Loss: 28.5: |███████████████████████| 1051/1051 [1:23:37<00:00, 4.77s/it]
Train Loss: 28.5: |███████████████████████| 1051/1051 [1:23:37<00:00, 4.77s/it]
Test Loss: 24.6 PSNR: 23.76 SSIM: 0.7086: | | 1/1111 [00:15<4:51:40, 15.77s/it]Can't pickle local object 'MultiSaver.begin_background..t'
Traceback (most recent call last):
File """", line 1, in 
File ""D:\Anaconda3\install_file\lib\multiprocessing\spawn.py"", line 105, in spawn_main
exitcode = _main(fd)
File ""D:\Anaconda3\install_file\lib\multiprocessing\spawn.py"", line 115, in _main
self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
add: version of torch is '1.7.0.dev20200730', platform is the windows10,
very thanks if you give me some advice!!!"
"Hi Seungjun, 

I am trying to run your PyTorch code in the demo code. I have downloaded your trained models(GOPRO_L1 and GOPRO_L1_amp) and unzip into the corresponding directory and run the main.py in the demo mode successfully. However, I found some wrongs while checking the outputs of your model after applying on the test dataset of the GOPRO_Large. The output picture looked like:
![output](https://s1.ax1x.com/2020/08/02/atTlvR.png)

Could you please give me some tips?

Thanks."
"The readme provides this code to generate predictions from a folder containing blurred images:
```bash
python main.py --save_dir SAVE_DIR --demo true --demo_input_dir INPUT_DIR_NAME --demo_output_dir OUTPUT_DIR_NAME
# demo_output_dir is by default SAVE_DIR/results
# SAVE_DIR is relative to DeepDeblur-PyTorch/experiment
# DEMO_INPUT_DIR and DEMO_OUTPUT_DIR can be both absolute or relative to os.getenv(""HOME"")
# image dataloader looks into DEMO_INPUT_DIR, recursively

# example
# single GPU
python main.py --save_dir REDS_L1 --demo true --demo_input_dir Research/dataset/REDS/test/test_blur
# multi-GPU
python launch.py --n_GPUs 2 main.py --save_dir REDS_L1 --demo true --demo_input_dir Research/dataset/REDS/test/test_blur --demo_output_dir OUTPUT_DIR_NAME
```

But in doing that, how do I specify which folder from the `experiments` directory is to be used to load the pretrained model that will be used to generate predictions?

More specifically, I want to use the `GOPRO_L1` pretrained models but I don't seem to have any way to specify that directly.

Any help will be greatly appreciated!"
"Hi   I am trying the pytorch 1.5 code,
however, after running  10 epoch, when validation starts, it gives an error  as follows,
In `trainer.test(epoch)`  
if`  `img.ndim` < 4:
AttributeError: 'Tensor' object has no attribute 'ndim'

I am not sure about the reason why this happens, could you provide some help?
thanks


"
"![image](https://user-images.githubusercontent.com/28228385/87406864-a53fc000-c5c1-11ea-8945-80f5d634736f.png)
Unfortunately I have this problem. I haven't made any changes in the code. Did I do something wrong? The goal was to use the already trained model."
"Hi Nah！
When I tested the model，the following question occured.
C:\Anaconda\python.exe D:/bishe/DeepDeblur-PyTorch-master/src/main.py
===> Loading test dataset: GOPRO_Large
Loading model from ../experiment\2020-07-09_13-41-50\models\model-2.pt
Loading optimizer from ../experiment\2020-07-09_13-41-50\optim\optim-2.pt
Loss function: 1*L1+1*ADV
Metrics: PSNR,SSIM
Loading loss record from ../experiment\2020-07-09_13-41-50\loss.pt
===> Initializing trainer
results are saved in ../experiment\2020-07-09_13-41-50\result
Loading model from ../experiment\2020-07-09_13-41-50\models\model-2.pt
Loading optimizer from ../experiment\2020-07-09_13-41-50\optim\optim-2.pt
Loading loss record from ../experiment\2020-07-09_13-41-50\loss.pt
Test Loss: 34.6 PSNR: 23.42 SSIM: 0.6894: |   | 1/100 [00:48<1:20:08, 48.57s/it]Can't pickle local object 'MultiSaver.begin_background.<locals>.t'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Anaconda\lib\multiprocessing\spawn.py"", line 105, in spawn_main
    exitcode = _main(fd)
  File ""C:\Anaconda\lib\multiprocessing\spawn.py"", line 115, in _main
    self = reduction.pickle.load(from_parent)
EOFError: Ran out of input
Test Loss: 34.4 PSNR: 23.64 SSIM: 0.7096: |   | 1/100 [01:04<1:46:44, 64.69s/it]
Traceback (most recent call last):
  File ""D:/bishe/DeepDeblur-PyTorch-master/src/main.py"", line 69, in <module>
    main()
  File ""D:/bishe/DeepDeblur-PyTorch-master/src/main.py"", line 66, in main
    main_worker(args.rank, args)
  File ""D:/bishe/DeepDeblur-PyTorch-master/src/main.py"", line 61, in main_worker
    trainer.imsaver.join_background()
  File ""D:\bishe\DeepDeblur-PyTorch-master\src\utils.py"", line 131, in join_background
    p.join()
  File ""C:\Anaconda\lib\multiprocessing\process.py"", line 123, in join
    assert self._popen is not None, 'can only join a started process'
AssertionError: can only join a started process
Could you tell me how to deal with it?"
"ahha, i have a question is about the loss , i  see you set the loss default value is 1*L1, when i use the ADV will error , dataset is the GOPRO_Large,can you help me ?thx"
"hello, thanks for your codes, i want to know how to create the dataset folder likes GOPRO dataset ,thx!"
"Got error :
pyramid = list(pyramid_gaussian(img, n_scales-1, multichannel=True))
  File ""/home/jsgx/.conda/envs/pytorch/lib/python3.6/site-packages/skimage/transform/pyramids.py"", line 197, in pyramid_gaussian
    image = img_as_float(image)
  File ""/home/jsgx/.conda/envs/pytorch/lib/python3.6/site-packages/skimage/util/dtype.py"", line 378, in img_as_float64
    return convert(image, np.float64, force_copy)
  File ""/home/jsgx/.conda/envs/pytorch/lib/python3.6/site-packages/skimage/util/dtype.py"", line 244, in convert
    raise ValueError(""Images of type float must be between -1 and 1."")
ValueError: Images of type float must be between -1 and 1.

`python main.py --n_GPUs 2 --batch_size 16`
It seems image tensors have to be normlized to [-1,1]. Should I add `--rgb_range=1` to solve this problem
"
"Hi, thanks for sharing your code! After reading your code, I found that while training the model on GoPro dataset, you use blur images in _blur_gamma_ folder rather than those in _blur_ folder. Am I correct?"
"Helo @SeungjunNah , Thanks for your great works on this great repository.

But I have some problem here, I have tried to run demo from your repo (I have python 3.8.3), Using the following steps:

1. I clone your repositories, create conda environtment install pytorch 1.5.0 using `conda install -c pytorch pytorch`
2. Install all the requirements, download pretrained model
3. Run using the following command line  `python main.py --demo true --save_dir GOPRO_L1_amp --demo_input_dir INPUT_DIR --demo_output_dir OUTPUT_DIR`

then occurs this errors:
![following Error](https://user-images.githubusercontent.com/39609060/85090549-9a3a7f80-b20f-11ea-9793-98d1a3a43f2a.png)

after I forcing the dimension using some modification line in [MSResNet.py](https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/model/MSResNet.py#L59) like this:
 
```python
input_s = input_pyramid[-1]
for s in scales:    # [2, 1, 0]
      output_pyramid[s] = self.body_models[s](input_s)
      if s > 0:
         up_feat = self.conv_end_models[s](output_pyramid[s])
         bound3 = input_pyramid[s-1].size()[3]
         bound2 = input_pyramid[s-1].size()[2]
         input_s = torch.cat((input_pyramid[s-1], up_feat[:, :, :bound2, :bound3]), 1)
```

I can run the demo scripts, but got the different result with yours as the following: 
![sample22](https://user-images.githubusercontent.com/39609060/85090971-9a874a80-b210-11ea-8899-a20ce20d3460.png)

The first image (top position) is my result, and the second image (bottom position) is yours. Mine is a little bit blurred than yours, can you figure out why? and can you handdle my errors? thank you for your reply 

"
"hi. I'm a huge fan of yours.

which loss function is the best in your example and what's the difference?

eg.)
python main.py --n_GPUs 1 --batch_size 8 --loss 1*L1+1*ADV
python main.py --n_GPUs 1 --batch_size 8 --loss 1*L1+3*ADV
python main.py --n_GPUs 1 --batch_size 8 --loss 1*L1+0.1*ADV

i used REDS datasets for train.

thanks."
"Good day, 

I had tried to run a **test** on the mentioned datasets based on the commands suggested in the repo, but I could not understand the output. 
![image](https://user-images.githubusercontent.com/4167312/84259990-f4f21e00-ab21-11ea-9e5c-da14d6fbd2fa.png)

Is there anything that I am missing or I did something wrong? 

Thanks for the help and time."
"When I doing the # end-to-end training using DSAC
It will get stuck in Sampling 256 hypotheses and generate no error.
-------
here is the cmd 

Loading test set ...
Reading file names... 
Loading script: train_score.lua
TORCH: Loading network from file: score_model_init.net
Loading script: train_obj.lua
TORCH: Loading network from file: obj_model_init.net
Processing test image 0 of 2000.
Predicting object coordinates.
TORCH: Doing a forward pass for 1600 patches.
CNN prediction took 0.671922s.
Sampling 256 hypotheses."
"In CmakeList, I have set LUA and Torch's new locations. But for somewhere, like
`target_link_libraries(train_obj ${PNG_LIBRARY} ${OpenCV_LIBS} ${luajit}) 
target_link_libraries(train_score ${PNG_LIBRARY} ${OpenCV_LIBS} ${luajit})
target_link_libraries(test_ransac ${PNG_LIBRARY} ${OpenCV_LIBS} ${luajit})
target_link_libraries(train_ransac ${PNG_LIBRARY} ${OpenCV_LIBS} ${luajit})
target_link_libraries(test_ransac_softam ${PNG_LIBRARY} ${OpenCV_LIBS} ${luajit})
target_link_libraries(train_ransac_softam ${PNG_LIBRARY} ${OpenCV_LIBS} ${luajit})`

I don't know how to set them,
I try to add senctence like `set(luajit /home/open/torch/install/lib/libluajit.so) `
but when in making, it seems to show errors, like **'undefined reference to `lua_createtable''**
when I don't do anything, it seems to show the same error.
However, when I try to set things like 'set (luajit .././liblua.so)' it seems to make successfully, but when doing experiment, something goes wrong, like **'error loading module 'libpaths' from file'...**
Could u tell me how to set the Cmake, or is there something wrong with my environment settings.
The environment setting is :CUDA8.0, CUDNN5.0, TORCH7, OPENCV2.4,PNG1.2.53,LUA5.3"
"hi eric,
i try to test the dsac results by using your code and your trained model. but when i test the code of dsac, i found out the ""Sampling  hypotheses"" step of processImage function in cnn.h fall into a endless for loop due to it can't safasolve the pnp.
How can i fix it?
![issue_dsac](https://user-images.githubusercontent.com/23427268/79546187-9336aa00-80c4-11ea-9ea7-cffed78ac4ff.png)
"
"Why would you need to divide translation distance by 10:

```
std::min(std::max(rotErr, tErr / 10), MAXLOSS);
```
https://github.com/cvlab-dresden/DSAC/blob/da63cd79a4e4847c45d6f07407bb1ad4027ce26f/core/maxloss.h#L78"
"lzh@lzh-1060:~/DSAC/core/build$ make
[  2%] Linking CXX executable train_ransac
/usr/bin/ld: cannot find -lluajit
collect2: error: ld returned 1 exit status
CMakeFiles/train_ransac.dir/build.make:269: recipe for target 'train_ransac' failed
make[2]: *** [train_ransac] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/train_ransac.dir/all' failed
make[1]: *** [CMakeFiles/train_ransac.dir/all] Error 2
Makefile:83: recipe for target 'all' failed
make: *** [all] Error 2
"
"I am interested how you obtained the data for sensorTrans.dat.

The original [microsoft 7Scenes description](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/) does not contain this information. They only mention principal point and focal length of the kinect. 

Background is, mapping the depth image to the rgb image with your method is a little inaccurate with the method used in this repo. Still much better than the raw data of course :)"
"I can do the component-training fine, but when I start the end-to-end training afterwards the program crashes in this line:

https://github.com/cvlab-dresden/DSAC/blob/master/core/cnn.h#L474

The error message is 

> OpenCV Error: Assertion failed (type == B.type()) in gemm, file /home/mikhail/proglibs/opencv/modules/core/src/matmul.cpp, line 1558

I debugged this line and found out that the rotation matrix has the type CV_32F, while two others have type CV_64F. If i naively convert the matrix type in the line above, the programm crashes somewhere else and I am not ready to dive into this rabbit hole yet. 
If I log the type of the hyps[h] here:

https://github.com/cvlab-dresden/DSAC/blob/master/core/cnn.h#L661

it is always 5, i.e. CV_32F."
" I want to publish a pose transform using the estimate from test_ransac. I have the following but the results I get is not correct.  The pose jumps around , am I setting the fields incorrectly?

```
        tf::Matrix3x3 M(hypR.at<float>(0,0),hypR.at<float>(0,1),hypR.at<float>(0,2),
                        hypR.at<float>(1,0),hypR.at<float>(1,1),hypR.at<float>(1,2),
                        hypR.at<float>(2,0),hypR.at<float>(2,1),hypR.at<float>(2,2));
        tf::Quaternion q;
        M.getRotation(q); 
        tf::Vector3 t(hypV[3], hypV[4],  hypV[5] );    
        tf::Transform filtered;
        filtered.setOrigin(t);
        filtered.setRotation(q);
```
camera rgb calibration: for image stream
The color images 640×480 
```
fx | fy | cx | cy | d0 | d1 | d2 | d3 | d4
520.9 | 521.0 | 325.1 | 249.7 | 0.2312 | -0.7849 | -0.0033 | -0.0001 | 0.9172
```



"
"Right now it takes about over 1 sec per image.  Is there are way to speed it up.  Is the code enabled for GPU?

```
TORCH: Doing a forward pass for 1600 patches.
CNN prediction took 0.58221s.
Sampling 256 hypotheses.
Done in 0.963589s.
Calculating scores.
TORCH: Doing a forward pass for 256 images.
Done in 0.0349079s.
Drawing final Hypothesis.
Done in 8.1989e-05s.
Refining poses:
Done in 0.00228846s.
Final Result:

```

"
interested in trying this with my own image sequences. Want to integrate with ROS  for visualization driving it using a video or ros bag file. Have you considered opening the interface to allow for this.
Is there a viewer available for displaying pose estimates in the ransac_test file?
"Hello Eric, I run your code on each scene (in the 7-scenes dataset) and get similar results as in your paper. But when I train and test on the complete 7-scenes dataset( 260,000 images for training, 170,000 for testing), the performance doesn't match. I subtract the translation (in ""translation.txt"") from the ground truth pose matrices manually for each scene and comment out the corresponding codes in ""read_data.cpp"". The training settings are not changed. Could you tell me how to achieve your performance on the complete 7-scenes dataset? Looking forward to your reply.

zhang xin"
"Hi, 
I tried to run your code on the stair dataset using pre-train score regression CNN and end-to-end training using DSAC.
However, the performance does not match with the reported performance in your paper.

First, I have downloaded stairs scene and TSDF Volume from your website. 
Then, I run ../../core/build/train_obj -oscript train_obj.lua
 In train_obj.cpp , the trainingLimit is set to 300000, which is same as your report. However, it will cost lots of time to  finish all of the training. Should I wait to finish 300000 round?

Then, I try to run  ../../core/build/train_score -oscript train_obj.lua -omodel obj_model_init.net -sscript train_score.lua and  ../../core/build/train_ransac -oscript train_obj.lua -omodel obj_model_init.net -sscript train_score.lua -smodel score_model_init.net
Both of them stagnated like following.

For pre-train score regression CNN:

Successfully loaded sensor transformation:
[0.9998282978278875, 0.008186003849805841, 0.01662420535123559, -0.01393324413905143;
  -0.008090415588790156, 0.9999503986429169, -0.005809081637597863, 0.05228905046770047;
  -0.01667093393273896, 0.005673587475537798, 0.9998449331606215, 0.02712006871814571;
  0, 0, 0, 1]

Parsing config file: default.config
focal length: 525
x shift: 0
y shift: 0
raw data (rescale rgb): 1
secondary focal length: 585
raw x shift: 0
raw y shift: 0
image width: 640
image height: 480
ransac iterations: 256
ransac iterations (refinement): 8
ransac batch size: 100
ransac refinement gradient sub sampling: 0.01
ransac inlier threshold: 10
ransac inlier threshold: 100
random draw: 1
object script: train_obj.lua

Loading training set ...
Reading file names... 
Loading script: train_obj.lua
TORCH: Loading network from file: obj_model_init.net
TORCH: Set model to evaluation mode.
Loading script: train_score.lua
TORCH: Creating network.
TORCH: Set model to evaluation mode.
Choosing validation data.
Starting training round 1 of 80
TORCH: Set model to training mode.
TORCH: Doing a forward pass for 1600 patches.
CNN prediction took 0.692021s.


For end-to-end training using DSAC:

Successfully loaded sensor transformation:
[0.9998282978278875, 0.008186003849805841, 0.01662420535123559, -0.01393324413905143;
  -0.008090415588790156, 0.9999503986429169, -0.005809081637597863, 0.05228905046770047;
  -0.01667093393273896, 0.005673587475537798, 0.9998449331606215, 0.02712006871814571;
  0, 0, 0, 1]

Parsing config file: default.config
focal length: 525
x shift: 0
y shift: 0
raw data (rescale rgb): 1
secondary focal length: 585
raw x shift: 0
raw y shift: 0
image width: 640
image height: 480
ransac iterations: 256
ransac iterations (refinement): 8
ransac batch size: 100
ransac refinement gradient sub sampling: 0.01
ransac inlier threshold: 10
ransac inlier threshold: 100
random draw: 1
object script: train_obj.lua
object model: obj_model_init.net
score script: train_score.lua
score model: score_model_init.net

Loading training set ...
Reading file names... 
Loading script: train_score.lua
TORCH: Loading network from file: score_model_init.net
TORCH: Set model to evaluation mode.
Loading script: train_obj.lua
TORCH: Loading network from file: obj_model_init.net
TORCH: Set model to evaluation mode.
Round 0 of 10000.
TORCH: Set model to training mode.
TORCH: Set model to training mode.
Predicting object coordinates.
TORCH: Doing a forward pass for 1600 patches.
CNN prediction took 0.62037s.
Sampling 256 hypotheses.

I would appreciate if you could give me some advises about possible mistakes or anything that I might missed.

Jeff

"
"When I try to run ../../core/build/train_obj -oscript train_obj.lua
It post the following error:
Successfully loaded sensor transformation:
[0.9998282978278875, 0.008186003849805841, 0.01662420535123559, -0.01393324413905143;
  -0.008090415588790156, 0.9999503986429169, -0.005809081637597863, 0.05228905046770047;
  -0.01667093393273896, 0.005673587475537798, 0.9998449331606215, 0.02712006871814571;
  0, 0, 0, 1]

Parsing config file: default.config
focal length: 525
x shift: 0
y shift: 0
raw data (rescale rgb): 1
secondary focal length: 585
raw x shift: 0
raw y shift: 0
image width: 640
image height: 480
ransac iterations: 256
ransac iterations (refinement): 8
ransac batch size: 100
ransac refinement gradient sub sampling: 0.01
ransac inlier threshold: 10
ransac inlier threshold: 100
random draw: 1
object script: train_obj.lua

Loading training set ...
Reading file names... 
TORCH: Creating network.
TORCH: Set model to evaluation mode.
Training CNN.
Starting training round 1
TORCH: Set model to training mode.
Segmentation fault


I think my previous work are correct, but I am not sure what the problem of this Segmentation fault.
Could you please kindly give me some ideas?
I am appreciate for your help. "
"hi, 
i just clone your code and i want to run it in my debian system.
i already got all the lib needed. while make, i got error message like below:
CMakeFiles/train_ransac_softam.dir/Hypothesis.cpp.o: In function `Hypothesis::Hypothesis()':
Hypothesis.cpp:(.text+0x155): undefined reference to `cv::Mat::eye(int, int, int)'
Hypothesis.cpp:(.text+0x1e5): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x260): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x2c0): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x2dc): undefined reference to `cv::Mat::eye(int, int, int)'
Hypothesis.cpp:(.text+0x379): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x3f4): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x46f): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x489): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x49e): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x4b6): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x4c9): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x4e1): undefined reference to `cv::Mat::deallocate()'
CMakeFiles/train_ransac_softam.dir/Hypothesis.cpp.o:Hypothesis.cpp:(.text+0x4f9): more undefined references to `cv::Mat::deallocate()' follow
CMakeFiles/train_ransac_softam.dir/Hypothesis.cpp.o: In function `Hypothesis::Hypothesis(cv::Mat, cv::Point3_<double>)':
Hypothesis.cpp:(.text+0x744): undefined reference to `cv::Mat::copySize(cv::Mat const&)'
Hypothesis.cpp:(.text+0x781): undefined reference to `cv::Mat::inv(int) const'
Hypothesis.cpp:(.text+0x815): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x890): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x8f0): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0x931): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x949): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x95e): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0x976): undefined reference to `cv::Mat::deallocate()'
CMakeFiles/train_ransac_softam.dir/Hypothesis.cpp.o: In function `Hypothesis::Hypothesis(jp::info_t)':
Hypothesis.cpp:(.text+0xba9): undefined reference to `cv::Mat::create(int, int const*, int)'
Hypothesis.cpp:(.text+0xd14): undefined reference to `cv::Mat::copySize(cv::Mat const&)'
Hypothesis.cpp:(.text+0xd44): undefined reference to `cv::Mat::inv(int) const'
Hypothesis.cpp:(.text+0xde1): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0xe5c): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0xed7): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0xf36): undefined reference to `cv::fastFree(void*)'
Hypothesis.cpp:(.text+0xf84): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0xf99): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0xfb1): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0xfc9): undefined reference to `cv::Mat::deallocate()'
Hypothesis.cpp:(.text+0xfdc): undefined reference to `cv::Mat::deallocate()'

to my surprise, i don't know why it happened here, so i look it up and find this may be caused by something wrong in the file cnn_softam.h. Because when i forbid this file in cmakelist.txt make can be done to the end.

could u find out what is wrong?"
"As we know, the decision boundary in softmax loss is (W1 −W2)x+b1 −b2 =0, where Wi and bi are weights and bias in softmax loss, respectively. If we define x as a feature vector and constrain ∥W1∥=∥W2∥=1 and b1 =b2 =0. But I want to know what is the function of normalizing weights ?
Anyone can give some advises?"
"![image](https://user-images.githubusercontent.com/24217235/95730187-7adccc00-0cb0-11eb-9485-4c2069fbe5d0.png)
![image](https://user-images.githubusercontent.com/24217235/95730211-83350700-0cb0-11eb-9883-bfefa2371f9e.png)
![image](https://user-images.githubusercontent.com/24217235/95730223-87612480-0cb0-11eb-945f-2dc5069beb34.png)

下边这里是怎么推导出来的呢？对应上边3种情况？
![image](https://user-images.githubusercontent.com/24217235/95730256-9647d700-0cb0-11eb-98fd-1d750b56beb7.png)
"
"Hello,
Thanks for the great work. One thing I noticed is that if I change the default mean and scale value, the softmax loss overflows. The problem is that I can not use this 127.5 and 0.0078125 as the scale value somehow. Could anyone please suggest, how to use mean value as 127 and scale as 1.0 and still make the model convergent?
```
  transform_param {
    mean_value: 127.5
    mean_value: 127.5
    mean_value: 127.5
    scale: 0.0078125
    mirror: true
  }
```
Thanks."
"net: ""sphereface/sphereface_model.prototxt""

#文件总数:4686817 , 训练总数:4360447 , 验证总数:326370

test_iter: 5100   #326370 / 256
test_interval: 17033

base_lr: 0.05
lr_policy: ""multistep""
gamma: 0.1


#17033
stepvalue: 34066
stepvalue: 85165
stepvalue: 136264
stepvalue: 170330
max_iter:  200000
iter_size: 2

display: 500
momentum: 0.9
weight_decay: 0.0005
snapshot: 2000
snapshot_prefix: ""sphereface/modal""

solver_mode: GPU









I1218 09:44:23.435408 42442 solver.cpp:289] Solving SpherefaceNet-20
I1218 09:44:23.435416 42442 solver.cpp:290] Learning Rate Policy: multistep
I1218 09:44:23.438875 42442 solver.cpp:347] Iteration 0, Testing net (#0)
I1218 09:44:23.669739 42442 blocking_queue.cpp:49] Waiting for data
I1218 09:48:31.001149 42442 blocking_queue.cpp:49] Waiting for data
I1218 09:52:34.625572 42442 blocking_queue.cpp:49] Waiting for data
I1218 09:55:54.271853 42442 blocking_queue.cpp:49] Waiting for data
I1218 10:00:13.620533 42442 blocking_queue.cpp:49] Waiting for data
I1218 10:04:16.510815 42442 blocking_queue.cpp:49] Waiting for data
I1218 10:04:38.789021 42764 data_layer.cpp:73] Restarting data prefetching from start.
I1218 10:04:38.832135 42442 solver.cpp:414]     Test net output #0: lambda = 11.936
I1218 10:04:38.832242 42442 solver.cpp:414]     Test net output #1: softmax_loss = 11.2369 (* 1 = 11.2369 loss)
I1218 10:04:39.691632 42442 solver.cpp:239] Iteration 0 (-nan iter/s, 1216.21s/500 iters), loss = 9.59401
I1218 10:04:39.691682 42442 solver.cpp:258]     Train net output #0: lambda = 806.452
I1218 10:04:39.691699 42442 solver.cpp:258]     Train net output #1: softmax_loss = 9.58034 (* 1 = 9.58034 loss)
I1218 10:04:39.691731 42442 sgd_solver.cpp:112] Iteration 0, lr = 0.05
I1218 10:15:29.681690 42442 solver.cpp:239] Iteration 500 (0.769271 iter/s, 649.966s/500 iters), loss = 9.75478
I1218 10:15:29.681933 42442 solver.cpp:258]     Train net output #0: lambda = 8.2481
I1218 10:15:29.681959 42442 solver.cpp:258]     Train net output #1: softmax_loss = 9.59993 (* 1 = 9.59993 loss)
I1218 10:15:29.681974 42442 sgd_solver.cpp:112] Iteration 500, lr = 0.05
I1218 10:21:11.482547 42442 blocking_queue.cpp:49] Waiting for data
I1218 10:29:43.602144 42442 solver.cpp:239] Iteration 1000 (0.585557 iter/s, 853.888s/500 iters), loss = 9.61311
I1218 10:29:43.602360 42442 solver.cpp:258]     Train net output #0: lambda = 5
I1218 10:29:43.602385 42442 solver.cpp:258]     Train net output #1: softmax_loss = 9.30922 (* 1 = 9.30922 loss)
I1218 10:29:43.602398 42442 sgd_solver.cpp:112] Iteration 1000, lr = 0.05
I1218 10:36:50.166695 42442 blocking_queue.cpp:49] Waiting for data
I1218 10:44:11.976773 42442 solver.cpp:239] Iteration 1500 (0.57581 iter/s, 868.342s/500 iters), loss = 9.77459
I1218 10:44:11.977003 42442 solver.cpp:258]     Train net output #0: lambda = 5
I1218 10:44:11.977030 42442 solver.cpp:258]     Train net output #1: softmax_loss = 9.92095 (* 1 = 9.92095 loss)
I1218 10:44:11.977046 42442 sgd_solver.cpp:112] Iteration 1500, lr = 0.05
I1218 10:55:30.798585 42442 solver.cpp:464] Snapshotting to binary proto file sphereface/modal/sphereface_solver_iter_2000.caffemodel
I1218 10:55:32.176621 42442 sgd_solver.cpp:284] Snapshotting solver state to binary proto file sphereface/modal/sphereface_solver_iter_2000.solverstate
I1218 10:55:33.252632 42442 solver.cpp:239] Iteration 2000 (0.733945 iter/s, 681.25s/500 iters), loss = 9.62272
I1218 10:55:33.252786 42442 solver.cpp:258]     Train net output #0: lambda = 5
I1218 10:55:33.252812 42442 solver.cpp:258]     Train net output #1: softmax_loss = 9.62173 (* 1 = 9.62173 loss)
I1218 10:55:33.252830 42442 sgd_solver.cpp:112] Iteration 2000, lr = 0.05
I1218 11:02:54.046000 42442 solver.cpp:239] Iteration 2500 (1.13436 iter/s, 440.777s/500 iters), loss = 9.63188
I1218 11:02:54.046308 42442 solver.cpp:258]     Train net output #0: lambda = 5
I1218 11:02:54.046353 42442 solver.cpp:258]     Train net output #1: softmax_loss = 9.62981 (* 1 = 9.62981 loss)
I1218 11:02:54.046381 42442 sgd_solver.cpp:112] Iteration 2500, lr = 0.05
"
之前一直在研究局部描述子特征匹配，发现与人脸识别有很多相似之处，所以想借鉴一下。对于Siamese和Triplet这种度量方式很好理解，但是对于改进softmax这类方法，存在一处疑惑。sphereFace划分的类别数就是数据集中人脸的总数吗？
"为什么我对我得数据集通过face_detect_demo.m脚本处理的时候，明明路径是对的，但是报 Undefined function or variable 'imResample' 这个错误，这个变量明明在代码中没有出现，不知道怎么回事，望有人告知

"
"Hi, I want to crop the images of webface to 112x112, what I need to change in your code ? @wy1iu "
"Does lambda in test have meaning?
usually its bigger than on train, but i guess it have no information in it, am i right?

I0906 19:50:07.181219 41380 solver.cpp:358] Iteration 2700, Testing net (#0)
I0906 19:50:07.997248 41380 solver.cpp:425]     Test net output #0: accuracy/top-1 = 0.93375
I0906 19:50:07.997248 41380 solver.cpp:425]     Test net output #1: lambda = 3.30186
I0906 19:50:07.998245 41380 solver.cpp:425]     Test net output #2: softmax_loss = 0.216583 (* 1 = 0.216583 loss)
I0906 19:50:08.126904 41380 solver.cpp:243] Iteration 2700, loss = 0.465629
I0906 19:50:08.126904 41380 solver.cpp:259]     Train net output #0: lambda = 3.08261
I0906 19:50:08.127900 41380 solver.cpp:259]     Train net output #1: softmax_loss = 0.465629 (* 1 = 0.465629 loss)"
"i am trying to get some results in reidentifiaction task and used your version of caffe
loss converge really good and i added test with accuracy and it shows like 98%
but when i try to use it, i get bad results


Can you recommend me something?

Thank you a lot
[sphereface_deploy.txt](https://github.com/wy1iu/sphereface/files/3585324/sphereface_deploy.txt)
[sphereface_model.txt](https://github.com/wy1iu/sphereface/files/3585325/sphereface_model.txt)
[sphereface_solver.txt](https://github.com/wy1iu/sphereface/files/3585326/sphereface_solver.txt)
[caffe.exe.log.txt](https://github.com/wy1iu/sphereface/files/3585329/caffe.exe.log.txt)



"
"Thanks for opening code!

I found that there were same parameters int the MarginInnerProduct layer, such as base and gamma. In your protext file, the values of base and gamma are 1000 and 0.12 respectively. Now, in my task, the class number of data is 1256, how should I set their values?

In addition, I add the accuracy  in the  training protext file and found that the accuracy of classification is just about 0.6. Is that the A-softmax improves the accuracy of recognition, but reduces the accuracy of classification?

"
"since 
cos(4\theta )=8cos^4\theta-8cos^2\theta+2

why do i care the sign of cos\theta?

thanks

```           
 cos_theta = tf.div(selected_logits, embeddings_norm)
            cos_theta_power = tf.square(cos_theta)
            cos_theta_biq = tf.pow(cos_theta, 4)
            sign0 = tf.sign(cos_theta)
            sign3 = tf.multiply(tf.sign(2*cos_theta_power-1), sign0)
            sign4 = 2*sign0 + sign3 -3
            result=sign3*(8*cos_theta_biq-8*cos_theta_power+1) + sign4
```"
"Hello, I didn't found any tests with m>=5, do you have those results?"
"I reproduce your experience training with CASIA-WebFace and evaluate with LFW using SpherefaceNet-20 twice, but I get the accuracy 98.98% and 99.03% which don't match your achivement 99.30%. The preprocess, train and test all follow you and the config all set default except that I use the official caffe and I use one GPU GTX1070. I know it's reasonable to exist some difference in different environment, but the error rate differ almost 30%. I'm confuse whether it's GPU number that make the difference. And what should I adjust if I want to get your accuracy using one GPU. Thanks. Here are the two train logs：
(1)
1	98.67%
2	98.83%
3	98.33%
4	99.33%
5	99.00%
6	99.33%
7	98.83%
8	98.67%
9	99.83%
10	99.00%
----------------
AVE	98.98%

(2)
1	98.50%
2	98.83%
3	98.50%
4	99.33%
5	98.83%
6	99.50%
7	98.83%
8	98.83%
9	99.83%
10	99.33%
----------------
AVE	99.03%"
"Dear author, we can see that the visualization in your paper is very good, especially the visualization of the 3D surface data distribution. Can you ask how it is implemented?"
"hi, all,
1. Someone can provide the cropped YTF (youtube face, 112x96) dataset? A download link is ok.
2. How to evaluate the model on YTF dataset, since it contains video pairs, not image pairs."
"A quite simple question, why normalize pixels with (img - 127.5)/128?
I didn't find any useful information. Can anyone help me? Thanks."
"thank you for sharing ,I have read your paper, and very interested in MHE . When do you plan to release the code ,looking forward to  the performance。"
"         hello,thanks for sharing your code.
         Can you tell me there is wheather goog strategy for trainning large scale dataset. I have trained sphereface-20 with ms-celeb-1m.Then I finetune spherefacenet20_ms-celeb-1m.caffemodel with VGGFace2 datasets. At the begin of training,the loss is 12 ,iterater 20000 times,loss decrease to 9,after that the loss did chage.I try setting lambda_min = 10 or decreate lr=0.001 but it did not converge. Should I set the hyperparameters and is it really possible to converge? Can you help me?
Using the settings in my prototxt:
type:SINGLE	
base: 5
gamma: 0
power: 1
lambda_min: 5"
"hi all,
in sphereface/test/code/evaluation.m we do feature subtract mean value and then divide the L2_norm value for each feature.

    % get normalized feature
    mu        = mean([featureLs(:, valFold), featureRs(:, valFold)], 2);
    featureLs = bsxfun(@minus, featureLs, mu);
    featureRs = bsxfun(@minus, featureRs, mu);
    featureLs = bsxfun(@rdivide, featureLs, sqrt(sum(featureLs.^2)));
    featureRs = bsxfun(@rdivide, featureRs, sqrt(sum(featureRs.^2)));

I understood divide L2_norm value to embed feature to a standard hypersphere, but why we do feature subtract mean value first?

in my opinion, feature subtract mean value will impact the cosine similarity.
for example, vector [1, 1, 4] subtract mean value equal to [-1, -1, 2], this vector goes to an other quadrant, so it obviously impact the angle between two vectors

@wy1iu , thank you and your team for your great jobs"
Please show me method determine unknown face in realtime face recognition using sphereface.
"
![1234](https://user-images.githubusercontent.com/20920889/45280963-eb7cf180-b508-11e8-8019-71e4a3034c7d.png)

请访问我的github项目https://github.com/zuoqing1988/ZQCNN-v0.0
我在我的model zoo的链接里放了这个网络的mxnet, caffe, ZQCNN三种格式的模型，有兴趣的可以都测一测。
觉得好用的请给我的gitHub点星，谢谢！"
Can anybody please explain why ψ should be monotonically decreasing for every interval?
seeming impossible?
CASIA webface is not available. Can you provide some information about the structure of the dataset?
"我使用sphereface-20在casia_clean上训练的时候，loss一直等于87.3365。改小学习率后loss会慢慢从一个较小的值上升到87.3365。请问这是什么情况？谁可以帮帮我？
下面是我的log：
`
I0717 22:54:56.291251 23375 caffe.cpp:218] Using GPUs 0, 1
I0717 22:54:56.306454 23375 caffe.cpp:223] GPU 0: GeForce GTX TITAN X
I0717 22:54:56.307116 23375 caffe.cpp:223] GPU 1: GeForce GTX TITAN X
I0717 22:54:56.756727 23375 solver.cpp:44] Initializing solver from parameters: 
base_lr: 0.001
display: 100
max_iter: 28000
lr_policy: ""multistep""
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot_prefix: ""result/sphereface_model""
solver_mode: GPU
device_id: 0
net: ""code/sphereface_model.prototxt""
train_state {
  level: 0
  stage: """"
}
stepvalue: 16000
stepvalue: 24000
stepvalue: 28000
I0717 22:54:56.756784 23375 solver.cpp:87] Creating training net from net file: code/sphereface_model.prototxt
I0717 22:54:56.758580 23375 net.cpp:51] Initializing net from parameters: 
name: ""SpherefaceNet-20""
state {
  phase: TRAIN
  level: 0
  stage: """"
}
layer {
  name: ""data""
  type: ""ImageData""
  top: ""data""
  top: ""label""
  transform_param {
    scale: 0.0078125
    mirror: true
    mean_value: 127.5
    mean_value: 127.5
    mean_value: 127.5
  }
  image_data_param {
    source: ""data/CASIA-WebFace-112X96.txt""
    batch_size: 256
    shuffle: true
  }
}
layer {
  name: ""conv1_1""
  type: ""Convolution""
  bottom: ""data""
  top: ""conv1_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu1_1""
  type: ""PReLU""
  bottom: ""conv1_1""
  top: ""conv1_1""
}
layer {
  name: ""conv1_2""
  type: ""Convolution""
  bottom: ""conv1_1""
  top: ""conv1_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu1_2""
  type: ""PReLU""
  bottom: ""conv1_2""
  top: ""conv1_2""
}
layer {
  name: ""conv1_3""
  type: ""Convolution""
  bottom: ""conv1_2""
  top: ""conv1_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu1_3""
  type: ""PReLU""
  bottom: ""conv1_3""
  top: ""conv1_3""
}
layer {
  name: ""res1_3""
  type: ""Eltwise""
  bottom: ""conv1_1""
  bottom: ""conv1_3""
  top: ""res1_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv2_1""
  type: ""Convolution""
  bottom: ""res1_3""
  top: ""conv2_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_1""
  type: ""PReLU""
  bottom: ""conv2_1""
  top: ""conv2_1""
}
layer {
  name: ""conv2_2""
  type: ""Convolution""
  bottom: ""conv2_1""
  top: ""conv2_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_2""
  type: ""PReLU""
  bottom: ""conv2_2""
  top: ""conv2_2""
}
layer {
  name: ""conv2_3""
  type: ""Convolution""
  bottom: ""conv2_2""
  top: ""conv2_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_3""
  type: ""PReLU""
  bottom: ""conv2_3""
  top: ""conv2_3""
}
layer {
  name: ""res2_3""
  type: ""Eltwise""
  bottom: ""conv2_1""
  bottom: ""conv2_3""
  top: ""res2_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv2_4""
  type: ""Convolution""
  bottom: ""res2_3""
  top: ""conv2_4""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_4""
  type: ""PReLU""
  bottom: ""conv2_4""
  top: ""conv2_4""
}
layer {
  name: ""conv2_5""
  type: ""Convolution""
  bottom: ""conv2_4""
  top: ""conv2_5""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_5""
  type: ""PReLU""
  bottom: ""conv2_5""
  top: ""conv2_5""
}
layer {
  name: ""res2_5""
  type: ""Eltwise""
  bottom: ""res2_3""
  bottom: ""conv2_5""
  top: ""res2_5""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_1""
  type: ""Convolution""
  bottom: ""res2_5""
  top: ""conv3_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_1""
  type: ""PReLU""
  bottom: ""conv3_1""
  top: ""conv3_1""
}
layer {
  name: ""conv3_2""
  type: ""Convolution""
  bottom: ""conv3_1""
  top: ""conv3_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_2""
  type: ""PReLU""
  bottom: ""conv3_2""
  top: ""conv3_2""
}
layer {
  name: ""conv3_3""
  type: ""Convolution""
  bottom: ""conv3_2""
  top: ""conv3_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_3""
  type: ""PReLU""
  bottom: ""conv3_3""
  top: ""conv3_3""
}
layer {
  name: ""res3_3""
  type: ""Eltwise""
  bottom: ""conv3_1""
  bottom: ""conv3_3""
  top: ""res3_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_4""
  type: ""Convolution""
  bottom: ""res3_3""
  top: ""conv3_4""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_4""
  type: ""PReLU""
  bottom: ""conv3_4""
  top: ""conv3_4""
}
layer {
  name: ""conv3_5""
  type: ""Convolution""
  bottom: ""conv3_4""
  top: ""conv3_5""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_5""
  type: ""PReLU""
  bottom: ""conv3_5""
  top: ""conv3_5""
}
layer {
  name: ""res3_5""
  type: ""Eltwise""
  bottom: ""res3_3""
  bottom: ""conv3_5""
  top: ""res3_5""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_6""
  type: ""Convolution""
  bottom: ""res3_5""
  top: ""conv3_6""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_6""
  type: ""PReLU""
  bottom: ""conv3_6""
  top: ""conv3_6""
}
layer {
  name: ""conv3_7""
  type: ""Convolution""
  bottom: ""conv3_6""
  top: ""conv3_7""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_7""
  type: ""PReLU""
  bottom: ""conv3_7""
  top: ""conv3_7""
}
layer {
  name: ""res3_7""
  type: ""Eltwise""
  bottom: ""res3_5""
  bottom: ""conv3_7""
  top: ""res3_7""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_8""
  type: ""Convolution""
  bottom: ""res3_7""
  top: ""conv3_8""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_8""
  type: ""PReLU""
  bottom: ""conv3_8""
  top: ""conv3_8""
}
layer {
  name: ""conv3_9""
  type: ""Convolution""
  bottom: ""conv3_8""
  top: ""conv3_9""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_9""
  type: ""PReLU""
  bottom: ""conv3_9""
  top: ""conv3_9""
}
layer {
  name: ""res3_9""
  type: ""Eltwise""
  bottom: ""res3_7""
  bottom: ""conv3_9""
  top: ""res3_9""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv4_1""
  type: ""Convolution""
  bottom: ""res3_9""
  top: ""conv4_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu4_1""
  type: ""PReLU""
  bottom: ""conv4_1""
  top: ""conv4_1""
}
layer {
  name: ""conv4_2""
  type: ""Convolution""
  bottom: ""conv4_1""
  top: ""conv4_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu4_2""
  type: ""PReLU""
  bottom: ""conv4_2""
  top: ""conv4_2""
}
layer {
  name: ""conv4_3""
  type: ""Convolution""
  bottom: ""conv4_2""
  top: ""conv4_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu4_3""
  type: ""PReLU""
  bottom: ""conv4_3""
  top: ""conv4_3""
}
layer {
  name: ""res4_3""
  type: ""Eltwise""
  bottom: ""conv4_1""
  bottom: ""conv4_3""
  top: ""res4_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""fc5""
  type: ""InnerProduct""
  bottom: ""res4_3""
  top: ""fc5""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""fc6""
  type: ""MarginInnerProduct""
  bottom: ""fc5""
  bottom: ""label""
  top: ""fc6""
  top: ""lambda""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  margin_inner_product_param {
    num_output: 10572
    type: QUADRUPLE
    weight_filler {
      type: ""xavier""
    }
    base: 1000
    gamma: 0.12
    power: 1
    iteration: 0
    lambda_min: 5
  }
}
layer {
  name: ""softmax_loss""
  type: ""SoftmaxWithLoss""
  bottom: ""fc6""
  bottom: ""label""
  top: ""softmax_loss""
}
I0717 22:54:56.758893 23375 layer_factory.hpp:77] Creating layer data
I0717 22:54:56.758935 23375 net.cpp:84] Creating Layer data
I0717 22:54:56.758946 23375 net.cpp:380] data -> data
I0717 22:54:56.759002 23375 net.cpp:380] data -> label
I0717 22:54:56.759483 23375 image_data_layer.cpp:38] Opening file data/CASIA-WebFace-112X96.txt
I0717 22:54:56.768184 23375 image_data_layer.cpp:53] Shuffling data
I0717 22:54:56.769104 23375 image_data_layer.cpp:63] A total of 29485 images.
I0717 22:54:56.770115 23375 image_data_layer.cpp:90] output data size: 256,3,112,96
I0717 22:54:56.844602 23375 net.cpp:122] Setting up data
I0717 22:54:56.844641 23375 net.cpp:129] Top shape: 256 3 112 96 (8257536)
I0717 22:54:56.844651 23375 net.cpp:129] Top shape: 256 (256)
I0717 22:54:56.844660 23375 net.cpp:137] Memory required for data: 33031168
I0717 22:54:56.844672 23375 layer_factory.hpp:77] Creating layer label_data_1_split
I0717 22:54:56.844698 23375 net.cpp:84] Creating Layer label_data_1_split
I0717 22:54:56.844709 23375 net.cpp:406] label_data_1_split <- label
I0717 22:54:56.844729 23375 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0717 22:54:56.844749 23375 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0717 22:54:56.844832 23375 net.cpp:122] Setting up label_data_1_split
I0717 22:54:56.844846 23375 net.cpp:129] Top shape: 256 (256)
I0717 22:54:56.844853 23375 net.cpp:129] Top shape: 256 (256)
I0717 22:54:56.844859 23375 net.cpp:137] Memory required for data: 33033216
I0717 22:54:56.844866 23375 layer_factory.hpp:77] Creating layer conv1_1
I0717 22:54:56.844892 23375 net.cpp:84] Creating Layer conv1_1
I0717 22:54:56.844900 23375 net.cpp:406] conv1_1 <- data
I0717 22:54:56.844916 23375 net.cpp:380] conv1_1 -> conv1_1
I0717 22:54:57.131815 23375 net.cpp:122] Setting up conv1_1
I0717 22:54:57.131852 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.131860 23375 net.cpp:137] Memory required for data: 209193984
I0717 22:54:57.131904 23375 layer_factory.hpp:77] Creating layer relu1_1
I0717 22:54:57.131924 23375 net.cpp:84] Creating Layer relu1_1
I0717 22:54:57.131940 23375 net.cpp:406] relu1_1 <- conv1_1
I0717 22:54:57.131953 23375 net.cpp:367] relu1_1 -> conv1_1 (in-place)
I0717 22:54:57.132736 23375 net.cpp:122] Setting up relu1_1
I0717 22:54:57.132753 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.132761 23375 net.cpp:137] Memory required for data: 385354752
I0717 22:54:57.132786 23375 layer_factory.hpp:77] Creating layer conv1_1_relu1_1_0_split
I0717 22:54:57.132798 23375 net.cpp:84] Creating Layer conv1_1_relu1_1_0_split
I0717 22:54:57.132815 23375 net.cpp:406] conv1_1_relu1_1_0_split <- conv1_1
I0717 22:54:57.132833 23375 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_0
I0717 22:54:57.132848 23375 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_1
I0717 22:54:57.132897 23375 net.cpp:122] Setting up conv1_1_relu1_1_0_split
I0717 22:54:57.132910 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.132920 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.132925 23375 net.cpp:137] Memory required for data: 737676288
I0717 22:54:57.132931 23375 layer_factory.hpp:77] Creating layer conv1_2
I0717 22:54:57.132951 23375 net.cpp:84] Creating Layer conv1_2
I0717 22:54:57.132959 23375 net.cpp:406] conv1_2 <- conv1_1_relu1_1_0_split_0
I0717 22:54:57.132972 23375 net.cpp:380] conv1_2 -> conv1_2
I0717 22:54:57.135821 23375 net.cpp:122] Setting up conv1_2
I0717 22:54:57.135839 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.135848 23375 net.cpp:137] Memory required for data: 913837056
I0717 22:54:57.135875 23375 layer_factory.hpp:77] Creating layer relu1_2
I0717 22:54:57.135892 23375 net.cpp:84] Creating Layer relu1_2
I0717 22:54:57.135905 23375 net.cpp:406] relu1_2 <- conv1_2
I0717 22:54:57.135941 23375 net.cpp:367] relu1_2 -> conv1_2 (in-place)
I0717 22:54:57.136690 23375 net.cpp:122] Setting up relu1_2
I0717 22:54:57.136708 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.136715 23375 net.cpp:137] Memory required for data: 1089997824
I0717 22:54:57.136737 23375 layer_factory.hpp:77] Creating layer conv1_3
I0717 22:54:57.136754 23375 net.cpp:84] Creating Layer conv1_3
I0717 22:54:57.136765 23375 net.cpp:406] conv1_3 <- conv1_2
I0717 22:54:57.136778 23375 net.cpp:380] conv1_3 -> conv1_3
I0717 22:54:57.139103 23375 net.cpp:122] Setting up conv1_3
I0717 22:54:57.139122 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.139129 23375 net.cpp:137] Memory required for data: 1266158592
I0717 22:54:57.139154 23375 layer_factory.hpp:77] Creating layer relu1_3
I0717 22:54:57.139165 23375 net.cpp:84] Creating Layer relu1_3
I0717 22:54:57.139176 23375 net.cpp:406] relu1_3 <- conv1_3
I0717 22:54:57.139187 23375 net.cpp:367] relu1_3 -> conv1_3 (in-place)
I0717 22:54:57.139940 23375 net.cpp:122] Setting up relu1_3
I0717 22:54:57.139956 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.139963 23375 net.cpp:137] Memory required for data: 1442319360
I0717 22:54:57.139989 23375 layer_factory.hpp:77] Creating layer res1_3
I0717 22:54:57.140007 23375 net.cpp:84] Creating Layer res1_3
I0717 22:54:57.140017 23375 net.cpp:406] res1_3 <- conv1_1_relu1_1_0_split_1
I0717 22:54:57.140027 23375 net.cpp:406] res1_3 <- conv1_3
I0717 22:54:57.140038 23375 net.cpp:380] res1_3 -> res1_3
I0717 22:54:57.140074 23375 net.cpp:122] Setting up res1_3
I0717 22:54:57.140089 23375 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0717 22:54:57.140096 23375 net.cpp:137] Memory required for data: 1618480128
I0717 22:54:57.140102 23375 layer_factory.hpp:77] Creating layer conv2_1
I0717 22:54:57.140118 23375 net.cpp:84] Creating Layer conv2_1
I0717 22:54:57.140126 23375 net.cpp:406] conv2_1 <- res1_3
I0717 22:54:57.140138 23375 net.cpp:380] conv2_1 -> conv2_1
I0717 22:54:57.142500 23375 net.cpp:122] Setting up conv2_1
I0717 22:54:57.142519 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.142526 23375 net.cpp:137] Memory required for data: 1706560512
I0717 22:54:57.142552 23375 layer_factory.hpp:77] Creating layer relu2_1
I0717 22:54:57.142565 23375 net.cpp:84] Creating Layer relu2_1
I0717 22:54:57.142580 23375 net.cpp:406] relu2_1 <- conv2_1
I0717 22:54:57.142594 23375 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I0717 22:54:57.142751 23375 net.cpp:122] Setting up relu2_1
I0717 22:54:57.142768 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.142776 23375 net.cpp:137] Memory required for data: 1794640896
I0717 22:54:57.142784 23375 layer_factory.hpp:77] Creating layer conv2_1_relu2_1_0_split
I0717 22:54:57.142796 23375 net.cpp:84] Creating Layer conv2_1_relu2_1_0_split
I0717 22:54:57.142808 23375 net.cpp:406] conv2_1_relu2_1_0_split <- conv2_1
I0717 22:54:57.142819 23375 net.cpp:380] conv2_1_relu2_1_0_split -> conv2_1_relu2_1_0_split_0
I0717 22:54:57.142832 23375 net.cpp:380] conv2_1_relu2_1_0_split -> conv2_1_relu2_1_0_split_1
I0717 22:54:57.142879 23375 net.cpp:122] Setting up conv2_1_relu2_1_0_split
I0717 22:54:57.142891 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.142900 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.142906 23375 net.cpp:137] Memory required for data: 1970801664
I0717 22:54:57.142913 23375 layer_factory.hpp:77] Creating layer conv2_2
I0717 22:54:57.142928 23375 net.cpp:84] Creating Layer conv2_2
I0717 22:54:57.142937 23375 net.cpp:406] conv2_2 <- conv2_1_relu2_1_0_split_0
I0717 22:54:57.142949 23375 net.cpp:380] conv2_2 -> conv2_2
I0717 22:54:57.149407 23375 net.cpp:122] Setting up conv2_2
I0717 22:54:57.149425 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.149433 23375 net.cpp:137] Memory required for data: 2058882048
I0717 22:54:57.149458 23375 layer_factory.hpp:77] Creating layer relu2_2
I0717 22:54:57.149469 23375 net.cpp:84] Creating Layer relu2_2
I0717 22:54:57.149497 23375 net.cpp:406] relu2_2 <- conv2_2
I0717 22:54:57.149509 23375 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I0717 22:54:57.149673 23375 net.cpp:122] Setting up relu2_2
I0717 22:54:57.149691 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.149698 23375 net.cpp:137] Memory required for data: 2146962432
I0717 22:54:57.149708 23375 layer_factory.hpp:77] Creating layer conv2_3
I0717 22:54:57.149724 23375 net.cpp:84] Creating Layer conv2_3
I0717 22:54:57.149734 23375 net.cpp:406] conv2_3 <- conv2_2
I0717 22:54:57.149746 23375 net.cpp:380] conv2_3 -> conv2_3
I0717 22:54:57.156194 23375 net.cpp:122] Setting up conv2_3
I0717 22:54:57.156213 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.156221 23375 net.cpp:137] Memory required for data: 2235042816
I0717 22:54:57.156239 23375 layer_factory.hpp:77] Creating layer relu2_3
I0717 22:54:57.156251 23375 net.cpp:84] Creating Layer relu2_3
I0717 22:54:57.156260 23375 net.cpp:406] relu2_3 <- conv2_3
I0717 22:54:57.156270 23375 net.cpp:367] relu2_3 -> conv2_3 (in-place)
I0717 22:54:57.156453 23375 net.cpp:122] Setting up relu2_3
I0717 22:54:57.156468 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.156476 23375 net.cpp:137] Memory required for data: 2323123200
I0717 22:54:57.156484 23375 layer_factory.hpp:77] Creating layer res2_3
I0717 22:54:57.156496 23375 net.cpp:84] Creating Layer res2_3
I0717 22:54:57.156503 23375 net.cpp:406] res2_3 <- conv2_1_relu2_1_0_split_1
I0717 22:54:57.156512 23375 net.cpp:406] res2_3 <- conv2_3
I0717 22:54:57.156522 23375 net.cpp:380] res2_3 -> res2_3
I0717 22:54:57.156563 23375 net.cpp:122] Setting up res2_3
I0717 22:54:57.156575 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.156582 23375 net.cpp:137] Memory required for data: 2411203584
I0717 22:54:57.156589 23375 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0717 22:54:57.156597 23375 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0717 22:54:57.156605 23375 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0717 22:54:57.156615 23375 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0717 22:54:57.156627 23375 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0717 22:54:57.156688 23375 net.cpp:122] Setting up res2_3_res2_3_0_split
I0717 22:54:57.156702 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.156710 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.156716 23375 net.cpp:137] Memory required for data: 2587364352
I0717 22:54:57.156723 23375 layer_factory.hpp:77] Creating layer conv2_4
I0717 22:54:57.156738 23375 net.cpp:84] Creating Layer conv2_4
I0717 22:54:57.156746 23375 net.cpp:406] conv2_4 <- res2_3_res2_3_0_split_0
I0717 22:54:57.156759 23375 net.cpp:380] conv2_4 -> conv2_4
I0717 22:54:57.163316 23375 net.cpp:122] Setting up conv2_4
I0717 22:54:57.163336 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.163343 23375 net.cpp:137] Memory required for data: 2675444736
I0717 22:54:57.163357 23375 layer_factory.hpp:77] Creating layer relu2_4
I0717 22:54:57.163368 23375 net.cpp:84] Creating Layer relu2_4
I0717 22:54:57.163377 23375 net.cpp:406] relu2_4 <- conv2_4
I0717 22:54:57.163386 23375 net.cpp:367] relu2_4 -> conv2_4 (in-place)
I0717 22:54:57.163552 23375 net.cpp:122] Setting up relu2_4
I0717 22:54:57.163566 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.163573 23375 net.cpp:137] Memory required for data: 2763525120
I0717 22:54:57.163583 23375 layer_factory.hpp:77] Creating layer conv2_5
I0717 22:54:57.163599 23375 net.cpp:84] Creating Layer conv2_5
I0717 22:54:57.163609 23375 net.cpp:406] conv2_5 <- conv2_4
I0717 22:54:57.163625 23375 net.cpp:380] conv2_5 -> conv2_5
I0717 22:54:57.174243 23375 net.cpp:122] Setting up conv2_5
I0717 22:54:57.174264 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.174273 23375 net.cpp:137] Memory required for data: 2851605504
I0717 22:54:57.174298 23375 layer_factory.hpp:77] Creating layer relu2_5
I0717 22:54:57.174329 23375 net.cpp:84] Creating Layer relu2_5
I0717 22:54:57.174340 23375 net.cpp:406] relu2_5 <- conv2_5
I0717 22:54:57.174355 23375 net.cpp:367] relu2_5 -> conv2_5 (in-place)
I0717 22:54:57.174533 23375 net.cpp:122] Setting up relu2_5
I0717 22:54:57.174547 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.174554 23375 net.cpp:137] Memory required for data: 2939685888
I0717 22:54:57.174564 23375 layer_factory.hpp:77] Creating layer res2_5
I0717 22:54:57.174582 23375 net.cpp:84] Creating Layer res2_5
I0717 22:54:57.174589 23375 net.cpp:406] res2_5 <- res2_3_res2_3_0_split_1
I0717 22:54:57.174598 23375 net.cpp:406] res2_5 <- conv2_5
I0717 22:54:57.174608 23375 net.cpp:380] res2_5 -> res2_5
I0717 22:54:57.174649 23375 net.cpp:122] Setting up res2_5
I0717 22:54:57.174660 23375 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0717 22:54:57.174669 23375 net.cpp:137] Memory required for data: 3027766272
I0717 22:54:57.174676 23375 layer_factory.hpp:77] Creating layer conv3_1
I0717 22:54:57.174693 23375 net.cpp:84] Creating Layer conv3_1
I0717 22:54:57.174703 23375 net.cpp:406] conv3_1 <- res2_5
I0717 22:54:57.174715 23375 net.cpp:380] conv3_1 -> conv3_1
I0717 22:54:57.179035 23375 net.cpp:122] Setting up conv3_1
I0717 22:54:57.179054 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.179062 23375 net.cpp:137] Memory required for data: 3071806464
I0717 22:54:57.179086 23375 layer_factory.hpp:77] Creating layer relu3_1
I0717 22:54:57.179097 23375 net.cpp:84] Creating Layer relu3_1
I0717 22:54:57.179105 23375 net.cpp:406] relu3_1 <- conv3_1
I0717 22:54:57.179116 23375 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I0717 22:54:57.179275 23375 net.cpp:122] Setting up relu3_1
I0717 22:54:57.179291 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.179297 23375 net.cpp:137] Memory required for data: 3115846656
I0717 22:54:57.179306 23375 layer_factory.hpp:77] Creating layer conv3_1_relu3_1_0_split
I0717 22:54:57.179316 23375 net.cpp:84] Creating Layer conv3_1_relu3_1_0_split
I0717 22:54:57.179324 23375 net.cpp:406] conv3_1_relu3_1_0_split <- conv3_1
I0717 22:54:57.179334 23375 net.cpp:380] conv3_1_relu3_1_0_split -> conv3_1_relu3_1_0_split_0
I0717 22:54:57.179352 23375 net.cpp:380] conv3_1_relu3_1_0_split -> conv3_1_relu3_1_0_split_1
I0717 22:54:57.179412 23375 net.cpp:122] Setting up conv3_1_relu3_1_0_split
I0717 22:54:57.179425 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.179433 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.179440 23375 net.cpp:137] Memory required for data: 3203927040
I0717 22:54:57.179446 23375 layer_factory.hpp:77] Creating layer conv3_2
I0717 22:54:57.179461 23375 net.cpp:84] Creating Layer conv3_2
I0717 22:54:57.179473 23375 net.cpp:406] conv3_2 <- conv3_1_relu3_1_0_split_0
I0717 22:54:57.179486 23375 net.cpp:380] conv3_2 -> conv3_2
I0717 22:54:57.201071 23375 net.cpp:122] Setting up conv3_2
I0717 22:54:57.201093 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.201102 23375 net.cpp:137] Memory required for data: 3247967232
I0717 22:54:57.201125 23375 layer_factory.hpp:77] Creating layer relu3_2
I0717 22:54:57.201138 23375 net.cpp:84] Creating Layer relu3_2
I0717 22:54:57.201146 23375 net.cpp:406] relu3_2 <- conv3_2
I0717 22:54:57.201158 23375 net.cpp:367] relu3_2 -> conv3_2 (in-place)
I0717 22:54:57.201321 23375 net.cpp:122] Setting up relu3_2
I0717 22:54:57.201335 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.201344 23375 net.cpp:137] Memory required for data: 3292007424
I0717 22:54:57.201352 23375 layer_factory.hpp:77] Creating layer conv3_3
I0717 22:54:57.201369 23375 net.cpp:84] Creating Layer conv3_3
I0717 22:54:57.201380 23375 net.cpp:406] conv3_3 <- conv3_2
I0717 22:54:57.201391 23375 net.cpp:380] conv3_3 -> conv3_3
I0717 22:54:57.222266 23375 net.cpp:122] Setting up conv3_3
I0717 22:54:57.222290 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.222296 23375 net.cpp:137] Memory required for data: 3336047616
I0717 22:54:57.222321 23375 layer_factory.hpp:77] Creating layer relu3_3
I0717 22:54:57.222363 23375 net.cpp:84] Creating Layer relu3_3
I0717 22:54:57.222374 23375 net.cpp:406] relu3_3 <- conv3_3
I0717 22:54:57.222385 23375 net.cpp:367] relu3_3 -> conv3_3 (in-place)
I0717 22:54:57.222537 23375 net.cpp:122] Setting up relu3_3
I0717 22:54:57.222550 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.222558 23375 net.cpp:137] Memory required for data: 3380087808
I0717 22:54:57.222573 23375 layer_factory.hpp:77] Creating layer res3_3
I0717 22:54:57.222589 23375 net.cpp:84] Creating Layer res3_3
I0717 22:54:57.222600 23375 net.cpp:406] res3_3 <- conv3_1_relu3_1_0_split_1
I0717 22:54:57.222609 23375 net.cpp:406] res3_3 <- conv3_3
I0717 22:54:57.222620 23375 net.cpp:380] res3_3 -> res3_3
I0717 22:54:57.222666 23375 net.cpp:122] Setting up res3_3
I0717 22:54:57.222678 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.222684 23375 net.cpp:137] Memory required for data: 3424128000
I0717 22:54:57.222692 23375 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0717 22:54:57.222704 23375 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0717 22:54:57.222712 23375 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0717 22:54:57.222721 23375 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0717 22:54:57.222738 23375 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0717 22:54:57.222789 23375 net.cpp:122] Setting up res3_3_res3_3_0_split
I0717 22:54:57.222800 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.222808 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.222815 23375 net.cpp:137] Memory required for data: 3512208384
I0717 22:54:57.222821 23375 layer_factory.hpp:77] Creating layer conv3_4
I0717 22:54:57.222836 23375 net.cpp:84] Creating Layer conv3_4
I0717 22:54:57.222846 23375 net.cpp:406] conv3_4 <- res3_3_res3_3_0_split_0
I0717 22:54:57.222857 23375 net.cpp:380] conv3_4 -> conv3_4
I0717 22:54:57.244524 23375 net.cpp:122] Setting up conv3_4
I0717 22:54:57.244549 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.244556 23375 net.cpp:137] Memory required for data: 3556248576
I0717 22:54:57.244570 23375 layer_factory.hpp:77] Creating layer relu3_4
I0717 22:54:57.244581 23375 net.cpp:84] Creating Layer relu3_4
I0717 22:54:57.244590 23375 net.cpp:406] relu3_4 <- conv3_4
I0717 22:54:57.244601 23375 net.cpp:367] relu3_4 -> conv3_4 (in-place)
I0717 22:54:57.244756 23375 net.cpp:122] Setting up relu3_4
I0717 22:54:57.244771 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.244777 23375 net.cpp:137] Memory required for data: 3600288768
I0717 22:54:57.244787 23375 layer_factory.hpp:77] Creating layer conv3_5
I0717 22:54:57.244804 23375 net.cpp:84] Creating Layer conv3_5
I0717 22:54:57.244814 23375 net.cpp:406] conv3_5 <- conv3_4
I0717 22:54:57.244827 23375 net.cpp:380] conv3_5 -> conv3_5
I0717 22:54:57.265857 23375 net.cpp:122] Setting up conv3_5
I0717 22:54:57.265877 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.265885 23375 net.cpp:137] Memory required for data: 3644328960
I0717 22:54:57.265909 23375 layer_factory.hpp:77] Creating layer relu3_5
I0717 22:54:57.265921 23375 net.cpp:84] Creating Layer relu3_5
I0717 22:54:57.265929 23375 net.cpp:406] relu3_5 <- conv3_5
I0717 22:54:57.265940 23375 net.cpp:367] relu3_5 -> conv3_5 (in-place)
I0717 22:54:57.266609 23375 net.cpp:122] Setting up relu3_5
I0717 22:54:57.266626 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.266633 23375 net.cpp:137] Memory required for data: 3688369152
I0717 22:54:57.266655 23375 layer_factory.hpp:77] Creating layer res3_5
I0717 22:54:57.266667 23375 net.cpp:84] Creating Layer res3_5
I0717 22:54:57.266681 23375 net.cpp:406] res3_5 <- res3_3_res3_3_0_split_1
I0717 22:54:57.266691 23375 net.cpp:406] res3_5 <- conv3_5
I0717 22:54:57.266706 23375 net.cpp:380] res3_5 -> res3_5
I0717 22:54:57.266752 23375 net.cpp:122] Setting up res3_5
I0717 22:54:57.266765 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.266794 23375 net.cpp:137] Memory required for data: 3732409344
I0717 22:54:57.266803 23375 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0717 22:54:57.266814 23375 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0717 22:54:57.266826 23375 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0717 22:54:57.266837 23375 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0717 22:54:57.266854 23375 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0717 22:54:57.266907 23375 net.cpp:122] Setting up res3_5_res3_5_0_split
I0717 22:54:57.266927 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.266937 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.266942 23375 net.cpp:137] Memory required for data: 3820489728
I0717 22:54:57.266948 23375 layer_factory.hpp:77] Creating layer conv3_6
I0717 22:54:57.266965 23375 net.cpp:84] Creating Layer conv3_6
I0717 22:54:57.266974 23375 net.cpp:406] conv3_6 <- res3_5_res3_5_0_split_0
I0717 22:54:57.266988 23375 net.cpp:380] conv3_6 -> conv3_6
I0717 22:54:57.290452 23375 net.cpp:122] Setting up conv3_6
I0717 22:54:57.290483 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.290491 23375 net.cpp:137] Memory required for data: 3864529920
I0717 22:54:57.290510 23375 layer_factory.hpp:77] Creating layer relu3_6
I0717 22:54:57.290529 23375 net.cpp:84] Creating Layer relu3_6
I0717 22:54:57.290551 23375 net.cpp:406] relu3_6 <- conv3_6
I0717 22:54:57.290565 23375 net.cpp:367] relu3_6 -> conv3_6 (in-place)
I0717 22:54:57.290731 23375 net.cpp:122] Setting up relu3_6
I0717 22:54:57.290746 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.290753 23375 net.cpp:137] Memory required for data: 3908570112
I0717 22:54:57.290763 23375 layer_factory.hpp:77] Creating layer conv3_7
I0717 22:54:57.290784 23375 net.cpp:84] Creating Layer conv3_7
I0717 22:54:57.290796 23375 net.cpp:406] conv3_7 <- conv3_6
I0717 22:54:57.290808 23375 net.cpp:380] conv3_7 -> conv3_7
I0717 22:54:57.311866 23375 net.cpp:122] Setting up conv3_7
I0717 22:54:57.311885 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.311893 23375 net.cpp:137] Memory required for data: 3952610304
I0717 22:54:57.311918 23375 layer_factory.hpp:77] Creating layer relu3_7
I0717 22:54:57.311928 23375 net.cpp:84] Creating Layer relu3_7
I0717 22:54:57.311935 23375 net.cpp:406] relu3_7 <- conv3_7
I0717 22:54:57.311945 23375 net.cpp:367] relu3_7 -> conv3_7 (in-place)
I0717 22:54:57.312115 23375 net.cpp:122] Setting up relu3_7
I0717 22:54:57.312129 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.312136 23375 net.cpp:137] Memory required for data: 3996650496
I0717 22:54:57.312145 23375 layer_factory.hpp:77] Creating layer res3_7
I0717 22:54:57.312158 23375 net.cpp:84] Creating Layer res3_7
I0717 22:54:57.312171 23375 net.cpp:406] res3_7 <- res3_5_res3_5_0_split_1
I0717 22:54:57.312180 23375 net.cpp:406] res3_7 <- conv3_7
I0717 22:54:57.312188 23375 net.cpp:380] res3_7 -> res3_7
I0717 22:54:57.312233 23375 net.cpp:122] Setting up res3_7
I0717 22:54:57.312247 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.312252 23375 net.cpp:137] Memory required for data: 4040690688
I0717 22:54:57.312260 23375 layer_factory.hpp:77] Creating layer res3_7_res3_7_0_split
I0717 22:54:57.312269 23375 net.cpp:84] Creating Layer res3_7_res3_7_0_split
I0717 22:54:57.312276 23375 net.cpp:406] res3_7_res3_7_0_split <- res3_7
I0717 22:54:57.312288 23375 net.cpp:380] res3_7_res3_7_0_split -> res3_7_res3_7_0_split_0
I0717 22:54:57.312304 23375 net.cpp:380] res3_7_res3_7_0_split -> res3_7_res3_7_0_split_1
I0717 22:54:57.312376 23375 net.cpp:122] Setting up res3_7_res3_7_0_split
I0717 22:54:57.312389 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.312397 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.312404 23375 net.cpp:137] Memory required for data: 4128771072
I0717 22:54:57.312410 23375 layer_factory.hpp:77] Creating layer conv3_8
I0717 22:54:57.312428 23375 net.cpp:84] Creating Layer conv3_8
I0717 22:54:57.312453 23375 net.cpp:406] conv3_8 <- res3_7_res3_7_0_split_0
I0717 22:54:57.312470 23375 net.cpp:380] conv3_8 -> conv3_8
I0717 22:54:57.333389 23375 net.cpp:122] Setting up conv3_8
I0717 22:54:57.333410 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.333417 23375 net.cpp:137] Memory required for data: 4172811264
I0717 22:54:57.333441 23375 layer_factory.hpp:77] Creating layer relu3_8
I0717 22:54:57.333456 23375 net.cpp:84] Creating Layer relu3_8
I0717 22:54:57.333463 23375 net.cpp:406] relu3_8 <- conv3_8
I0717 22:54:57.333477 23375 net.cpp:367] relu3_8 -> conv3_8 (in-place)
I0717 22:54:57.333642 23375 net.cpp:122] Setting up relu3_8
I0717 22:54:57.333657 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.333662 23375 net.cpp:137] Memory required for data: 4216851456
I0717 22:54:57.333673 23375 layer_factory.hpp:77] Creating layer conv3_9
I0717 22:54:57.333690 23375 net.cpp:84] Creating Layer conv3_9
I0717 22:54:57.333700 23375 net.cpp:406] conv3_9 <- conv3_8
I0717 22:54:57.333714 23375 net.cpp:380] conv3_9 -> conv3_9
I0717 22:54:57.354790 23375 net.cpp:122] Setting up conv3_9
I0717 22:54:57.354810 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.354817 23375 net.cpp:137] Memory required for data: 4260891648
I0717 22:54:57.354841 23375 layer_factory.hpp:77] Creating layer relu3_9
I0717 22:54:57.354854 23375 net.cpp:84] Creating Layer relu3_9
I0717 22:54:57.354863 23375 net.cpp:406] relu3_9 <- conv3_9
I0717 22:54:57.354876 23375 net.cpp:367] relu3_9 -> conv3_9 (in-place)
I0717 22:54:57.355044 23375 net.cpp:122] Setting up relu3_9
I0717 22:54:57.355058 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.355065 23375 net.cpp:137] Memory required for data: 4304931840
I0717 22:54:57.355074 23375 layer_factory.hpp:77] Creating layer res3_9
I0717 22:54:57.355087 23375 net.cpp:84] Creating Layer res3_9
I0717 22:54:57.355099 23375 net.cpp:406] res3_9 <- res3_7_res3_7_0_split_1
I0717 22:54:57.355108 23375 net.cpp:406] res3_9 <- conv3_9
I0717 22:54:57.355123 23375 net.cpp:380] res3_9 -> res3_9
I0717 22:54:57.355175 23375 net.cpp:122] Setting up res3_9
I0717 22:54:57.355188 23375 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0717 22:54:57.355195 23375 net.cpp:137] Memory required for data: 4348972032
I0717 22:54:57.355201 23375 layer_factory.hpp:77] Creating layer conv4_1
I0717 22:54:57.355218 23375 net.cpp:84] Creating Layer conv4_1
I0717 22:54:57.355227 23375 net.cpp:406] conv4_1 <- res3_9
I0717 22:54:57.355242 23375 net.cpp:380] conv4_1 -> conv4_1
I0717 22:54:57.368865 23375 net.cpp:122] Setting up conv4_1
I0717 22:54:57.368886 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.368893 23375 net.cpp:137] Memory required for data: 4370992128
I0717 22:54:57.368917 23375 layer_factory.hpp:77] Creating layer relu4_1
I0717 22:54:57.368932 23375 net.cpp:84] Creating Layer relu4_1
I0717 22:54:57.368939 23375 net.cpp:406] relu4_1 <- conv4_1
I0717 22:54:57.368949 23375 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I0717 22:54:57.369094 23375 net.cpp:122] Setting up relu4_1
I0717 22:54:57.369108 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.369114 23375 net.cpp:137] Memory required for data: 4393012224
I0717 22:54:57.369124 23375 layer_factory.hpp:77] Creating layer conv4_1_relu4_1_0_split
I0717 22:54:57.369137 23375 net.cpp:84] Creating Layer conv4_1_relu4_1_0_split
I0717 22:54:57.369148 23375 net.cpp:406] conv4_1_relu4_1_0_split <- conv4_1
I0717 22:54:57.369158 23375 net.cpp:380] conv4_1_relu4_1_0_split -> conv4_1_relu4_1_0_split_0
I0717 22:54:57.369179 23375 net.cpp:380] conv4_1_relu4_1_0_split -> conv4_1_relu4_1_0_split_1
I0717 22:54:57.369243 23375 net.cpp:122] Setting up conv4_1_relu4_1_0_split
I0717 22:54:57.369257 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.369266 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.369272 23375 net.cpp:137] Memory required for data: 4437052416
I0717 22:54:57.369278 23375 layer_factory.hpp:77] Creating layer conv4_2
I0717 22:54:57.369324 23375 net.cpp:84] Creating Layer conv4_2
I0717 22:54:57.369334 23375 net.cpp:406] conv4_2 <- conv4_1_relu4_1_0_split_0
I0717 22:54:57.369346 23375 net.cpp:380] conv4_2 -> conv4_2
I0717 22:54:57.449687 23375 net.cpp:122] Setting up conv4_2
I0717 22:54:57.449720 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.449728 23375 net.cpp:137] Memory required for data: 4459072512
I0717 22:54:57.449741 23375 layer_factory.hpp:77] Creating layer relu4_2
I0717 22:54:57.449767 23375 net.cpp:84] Creating Layer relu4_2
I0717 22:54:57.449775 23375 net.cpp:406] relu4_2 <- conv4_2
I0717 22:54:57.449790 23375 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I0717 22:54:57.449945 23375 net.cpp:122] Setting up relu4_2
I0717 22:54:57.449959 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.449965 23375 net.cpp:137] Memory required for data: 4481092608
I0717 22:54:57.449975 23375 layer_factory.hpp:77] Creating layer conv4_3
I0717 22:54:57.449996 23375 net.cpp:84] Creating Layer conv4_3
I0717 22:54:57.450006 23375 net.cpp:406] conv4_3 <- conv4_2
I0717 22:54:57.450019 23375 net.cpp:380] conv4_3 -> conv4_3
I0717 22:54:57.530601 23375 net.cpp:122] Setting up conv4_3
I0717 22:54:57.530637 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.530643 23375 net.cpp:137] Memory required for data: 4503112704
I0717 22:54:57.530658 23375 layer_factory.hpp:77] Creating layer relu4_3
I0717 22:54:57.530685 23375 net.cpp:84] Creating Layer relu4_3
I0717 22:54:57.530694 23375 net.cpp:406] relu4_3 <- conv4_3
I0717 22:54:57.530706 23375 net.cpp:367] relu4_3 -> conv4_3 (in-place)
I0717 22:54:57.531402 23375 net.cpp:122] Setting up relu4_3
I0717 22:54:57.531419 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.531426 23375 net.cpp:137] Memory required for data: 4525132800
I0717 22:54:57.531450 23375 layer_factory.hpp:77] Creating layer res4_3
I0717 22:54:57.531464 23375 net.cpp:84] Creating Layer res4_3
I0717 22:54:57.531476 23375 net.cpp:406] res4_3 <- conv4_1_relu4_1_0_split_1
I0717 22:54:57.531484 23375 net.cpp:406] res4_3 <- conv4_3
I0717 22:54:57.531496 23375 net.cpp:380] res4_3 -> res4_3
I0717 22:54:57.531545 23375 net.cpp:122] Setting up res4_3
I0717 22:54:57.531558 23375 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0717 22:54:57.531564 23375 net.cpp:137] Memory required for data: 4547152896
I0717 22:54:57.531570 23375 layer_factory.hpp:77] Creating layer fc5
I0717 22:54:57.531586 23375 net.cpp:84] Creating Layer fc5
I0717 22:54:57.531597 23375 net.cpp:406] fc5 <- res4_3
I0717 22:54:57.531608 23375 net.cpp:380] fc5 -> fc5
I0717 22:54:57.651867 23375 net.cpp:122] Setting up fc5
I0717 22:54:57.651906 23375 net.cpp:129] Top shape: 256 512 (131072)
I0717 22:54:57.651912 23375 net.cpp:137] Memory required for data: 4547677184
I0717 22:54:57.651942 23375 layer_factory.hpp:77] Creating layer fc6
I0717 22:54:57.651970 23375 net.cpp:84] Creating Layer fc6
I0717 22:54:57.651993 23375 net.cpp:406] fc6 <- fc5
I0717 22:54:57.652007 23375 net.cpp:406] fc6 <- label_data_1_split_0
I0717 22:54:57.652024 23375 net.cpp:380] fc6 -> fc6
I0717 22:54:57.652050 23375 net.cpp:380] fc6 -> lambda
I0717 22:54:57.710330 23375 net.cpp:122] Setting up fc6
I0717 22:54:57.710366 23375 net.cpp:129] Top shape: 256 10572 (2706432)
I0717 22:54:57.710374 23375 net.cpp:129] Top shape: 1 (1)
I0717 22:54:57.710379 23375 net.cpp:137] Memory required for data: 4558502916
I0717 22:54:57.710405 23375 layer_factory.hpp:77] Creating layer softmax_loss
I0717 22:54:57.710423 23375 net.cpp:84] Creating Layer softmax_loss
I0717 22:54:57.710433 23375 net.cpp:406] softmax_loss <- fc6
I0717 22:54:57.710443 23375 net.cpp:406] softmax_loss <- label_data_1_split_1
I0717 22:54:57.710459 23375 net.cpp:380] softmax_loss -> softmax_loss
I0717 22:54:57.710489 23375 layer_factory.hpp:77] Creating layer softmax_loss
I0717 22:54:57.717782 23375 net.cpp:122] Setting up softmax_loss
I0717 22:54:57.717809 23375 net.cpp:129] Top shape: (1)
I0717 22:54:57.717816 23375 net.cpp:132]     with loss weight 1
I0717 22:54:57.717862 23375 net.cpp:137] Memory required for data: 4558502920
I0717 22:54:57.717898 23375 net.cpp:198] softmax_loss needs backward computation.
I0717 22:54:57.717913 23375 net.cpp:198] fc6 needs backward computation.
I0717 22:54:57.717928 23375 net.cpp:198] fc5 needs backward computation.
I0717 22:54:57.717936 23375 net.cpp:198] res4_3 needs backward computation.
I0717 22:54:57.717942 23375 net.cpp:198] relu4_3 needs backward computation.
I0717 22:54:57.717948 23375 net.cpp:198] conv4_3 needs backward computation.
I0717 22:54:57.717954 23375 net.cpp:198] relu4_2 needs backward computation.
I0717 22:54:57.717960 23375 net.cpp:198] conv4_2 needs backward computation.
I0717 22:54:57.717968 23375 net.cpp:198] conv4_1_relu4_1_0_split needs backward computation.
I0717 22:54:57.717974 23375 net.cpp:198] relu4_1 needs backward computation.
I0717 22:54:57.717981 23375 net.cpp:198] conv4_1 needs backward computation.
I0717 22:54:57.717988 23375 net.cpp:198] res3_9 needs backward computation.
I0717 22:54:57.717994 23375 net.cpp:198] relu3_9 needs backward computation.
I0717 22:54:57.718001 23375 net.cpp:198] conv3_9 needs backward computation.
I0717 22:54:57.718008 23375 net.cpp:198] relu3_8 needs backward computation.
I0717 22:54:57.718014 23375 net.cpp:198] conv3_8 needs backward computation.
I0717 22:54:57.718022 23375 net.cpp:198] res3_7_res3_7_0_split needs backward computation.
I0717 22:54:57.718029 23375 net.cpp:198] res3_7 needs backward computation.
I0717 22:54:57.718036 23375 net.cpp:198] relu3_7 needs backward computation.
I0717 22:54:57.718042 23375 net.cpp:198] conv3_7 needs backward computation.
I0717 22:54:57.718050 23375 net.cpp:198] relu3_6 needs backward computation.
I0717 22:54:57.718056 23375 net.cpp:198] conv3_6 needs backward computation.
I0717 22:54:57.718062 23375 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0717 22:54:57.718070 23375 net.cpp:198] res3_5 needs backward computation.
I0717 22:54:57.718076 23375 net.cpp:198] relu3_5 needs backward computation.
I0717 22:54:57.718083 23375 net.cpp:198] conv3_5 needs backward computation.
I0717 22:54:57.718089 23375 net.cpp:198] relu3_4 needs backward computation.
I0717 22:54:57.718098 23375 net.cpp:198] conv3_4 needs backward computation.
I0717 22:54:57.718104 23375 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0717 22:54:57.718111 23375 net.cpp:198] res3_3 needs backward computation.
I0717 22:54:57.718118 23375 net.cpp:198] relu3_3 needs backward computation.
I0717 22:54:57.718125 23375 net.cpp:198] conv3_3 needs backward computation.
I0717 22:54:57.718132 23375 net.cpp:198] relu3_2 needs backward computation.
I0717 22:54:57.718139 23375 net.cpp:198] conv3_2 needs backward computation.
I0717 22:54:57.718153 23375 net.cpp:198] conv3_1_relu3_1_0_split needs backward computation.
I0717 22:54:57.718161 23375 net.cpp:198] relu3_1 needs backward computation.
I0717 22:54:57.718168 23375 net.cpp:198] conv3_1 needs backward computation.
I0717 22:54:57.718176 23375 net.cpp:198] res2_5 needs backward computation.
I0717 22:54:57.718184 23375 net.cpp:198] relu2_5 needs backward computation.
I0717 22:54:57.718194 23375 net.cpp:198] conv2_5 needs backward computation.
I0717 22:54:57.718200 23375 net.cpp:198] relu2_4 needs backward computation.
I0717 22:54:57.718209 23375 net.cpp:198] conv2_4 needs backward computation.
I0717 22:54:57.718217 23375 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0717 22:54:57.718226 23375 net.cpp:198] res2_3 needs backward computation.
I0717 22:54:57.718233 23375 net.cpp:198] relu2_3 needs backward computation.
I0717 22:54:57.718241 23375 net.cpp:198] conv2_3 needs backward computation.
I0717 22:54:57.718248 23375 net.cpp:198] relu2_2 needs backward computation.
I0717 22:54:57.718255 23375 net.cpp:198] conv2_2 needs backward computation.
I0717 22:54:57.718263 23375 net.cpp:198] conv2_1_relu2_1_0_split needs backward computation.
I0717 22:54:57.718269 23375 net.cpp:198] relu2_1 needs backward computation.
I0717 22:54:57.718277 23375 net.cpp:198] conv2_1 needs backward computation.
I0717 22:54:57.718286 23375 net.cpp:198] res1_3 needs backward computation.
I0717 22:54:57.718307 23375 net.cpp:198] relu1_3 needs backward computation.
I0717 22:54:57.718317 23375 net.cpp:198] conv1_3 needs backward computation.
I0717 22:54:57.718323 23375 net.cpp:198] relu1_2 needs backward computation.
I0717 22:54:57.718330 23375 net.cpp:198] conv1_2 needs backward computation.
I0717 22:54:57.718338 23375 net.cpp:198] conv1_1_relu1_1_0_split needs backward computation.
I0717 22:54:57.718348 23375 net.cpp:198] relu1_1 needs backward computation.
I0717 22:54:57.718359 23375 net.cpp:198] conv1_1 needs backward computation.
I0717 22:54:57.718367 23375 net.cpp:200] label_data_1_split does not need backward computation.
I0717 22:54:57.718375 23375 net.cpp:200] data does not need backward computation.
I0717 22:54:57.718381 23375 net.cpp:242] This network produces output lambda
I0717 22:54:57.718389 23375 net.cpp:242] This network produces output softmax_loss
I0717 22:54:57.718439 23375 net.cpp:255] Network initialization done.
I0717 22:54:57.718657 23375 solver.cpp:56] Solver scaffolding done.
I0717 22:54:57.721293 23375 caffe.cpp:248] Starting Optimization
I0717 22:54:58.203780 23382 image_data_layer.cpp:38] Opening file data/CASIA-WebFace-112X96.txt
I0717 22:54:58.212021 23382 image_data_layer.cpp:53] Shuffling data
I0717 22:54:58.212898 23382 image_data_layer.cpp:63] A total of 29485 images.
I0717 22:54:58.214020 23382 image_data_layer.cpp:90] output data size: 256,3,112,96
I0717 22:54:59.466454 23375 solver.cpp:272] Solving SpherefaceNet-20
I0717 22:54:59.466539 23375 solver.cpp:273] Learning Rate Policy: multistep
I0717 22:55:00.315739 23375 solver.cpp:218] Iteration 0 (-1.99574e-17 iter/s, 0.828484s/100 iters), loss = 9.28011
I0717 22:55:00.315799 23375 solver.cpp:237]     Train net output #0: lambda = 892.857
I0717 22:55:00.315822 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.28011 (* 1 = 9.28011 loss)
I0717 22:55:00.315865 23375 sgd_solver.cpp:105] Iteration 0, lr = 0.001
I0717 22:56:29.649165 23375 solver.cpp:218] Iteration 100 (1.11938 iter/s, 89.3351s/100 iters), loss = 9.26914
I0717 22:56:29.649252 23375 solver.cpp:237]     Train net output #0: lambda = 76.2195
I0717 22:56:29.649266 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.26914 (* 1 = 9.26914 loss)
I0717 22:56:29.649276 23375 sgd_solver.cpp:105] Iteration 100, lr = 0.001
I0717 22:58:05.758685 23375 solver.cpp:218] Iteration 200 (1.04046 iter/s, 96.1114s/100 iters), loss = 9.34572
I0717 22:58:05.758921 23375 solver.cpp:237]     Train net output #0: lambda = 39.8089
I0717 22:58:05.758956 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.34572 (* 1 = 9.34572 loss)
I0717 22:58:05.758992 23375 sgd_solver.cpp:105] Iteration 200, lr = 0.001
I0717 22:59:41.488473 23375 solver.cpp:218] Iteration 300 (1.04459 iter/s, 95.7316s/100 iters), loss = 9.36941
I0717 22:59:41.488723 23375 solver.cpp:237]     Train net output #0: lambda = 26.9397
I0717 22:59:41.488786 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.36941 (* 1 = 9.36941 loss)
I0717 22:59:41.488835 23375 sgd_solver.cpp:105] Iteration 300, lr = 0.001
I0717 23:01:17.329685 23375 solver.cpp:218] Iteration 400 (1.04337 iter/s, 95.843s/100 iters), loss = 9.38536
I0717 23:01:17.329905 23375 solver.cpp:237]     Train net output #0: lambda = 20.3583
I0717 23:01:17.329939 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.38536 (* 1 = 9.38536 loss)
I0717 23:01:17.329964 23375 sgd_solver.cpp:105] Iteration 400, lr = 0.001
I0717 23:02:53.314219 23375 solver.cpp:218] Iteration 500 (1.04181 iter/s, 95.9864s/100 iters), loss = 9.41078
I0717 23:02:53.314389 23375 solver.cpp:237]     Train net output #0: lambda = 16.3613
I0717 23:02:53.314409 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.41078 (* 1 = 9.41078 loss)
I0717 23:02:53.314421 23375 sgd_solver.cpp:105] Iteration 500, lr = 0.001
I0717 23:04:29.698570 23375 solver.cpp:218] Iteration 600 (1.03749 iter/s, 96.3862s/100 iters), loss = 9.44096
I0717 23:04:29.698819 23375 solver.cpp:237]     Train net output #0: lambda = 13.6761
I0717 23:04:29.698861 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.44096 (* 1 = 9.44096 loss)
I0717 23:04:29.698894 23375 sgd_solver.cpp:105] Iteration 600, lr = 0.001
I0717 23:06:05.978163 23375 solver.cpp:218] Iteration 700 (1.03862 iter/s, 96.2814s/100 iters), loss = 9.43819
I0717 23:06:05.978345 23375 solver.cpp:237]     Train net output #0: lambda = 11.7481
I0717 23:06:05.978374 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.43819 (* 1 = 9.43819 loss)
I0717 23:06:05.978394 23375 sgd_solver.cpp:105] Iteration 700, lr = 0.001
I0717 23:07:42.405726 23375 solver.cpp:218] Iteration 800 (1.03703 iter/s, 96.4294s/100 iters), loss = 9.45485
I0717 23:07:42.405968 23375 solver.cpp:237]     Train net output #0: lambda = 10.2965
I0717 23:07:42.406072 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.45485 (* 1 = 9.45485 loss)
I0717 23:07:42.406129 23375 sgd_solver.cpp:105] Iteration 800, lr = 0.001
I0717 23:09:18.564287 23375 solver.cpp:218] Iteration 900 (1.03993 iter/s, 96.1604s/100 iters), loss = 9.46535
I0717 23:09:18.564471 23375 solver.cpp:237]     Train net output #0: lambda = 9.16422
I0717 23:09:18.564502 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.46535 (* 1 = 9.46535 loss)
I0717 23:09:18.564524 23375 sgd_solver.cpp:105] Iteration 900, lr = 0.001
I0717 23:10:55.007236 23375 solver.cpp:218] Iteration 1000 (1.03686 iter/s, 96.4448s/100 iters), loss = 9.46661
I0717 23:10:55.007517 23375 solver.cpp:237]     Train net output #0: lambda = 8.25628
I0717 23:10:55.007619 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.46661 (* 1 = 9.46661 loss)
I0717 23:10:55.007696 23375 sgd_solver.cpp:105] Iteration 1000, lr = 0.001
I0717 23:12:31.343374 23375 solver.cpp:218] Iteration 1100 (1.03801 iter/s, 96.3379s/100 iters), loss = 9.47498
I0717 23:12:31.343603 23375 solver.cpp:237]     Train net output #0: lambda = 7.51202
I0717 23:12:31.343654 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47498 (* 1 = 9.47498 loss)
I0717 23:12:31.343688 23375 sgd_solver.cpp:105] Iteration 1100, lr = 0.001
I0717 23:14:07.924736 23375 solver.cpp:218] Iteration 1200 (1.03538 iter/s, 96.5832s/100 iters), loss = 9.47666
I0717 23:14:07.924916 23375 solver.cpp:237]     Train net output #0: lambda = 6.89085
I0717 23:14:07.924939 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47666 (* 1 = 9.47666 loss)
I0717 23:14:07.924955 23375 sgd_solver.cpp:105] Iteration 1200, lr = 0.001
I0717 23:15:44.091126 23375 solver.cpp:218] Iteration 1300 (1.03984 iter/s, 96.1683s/100 iters), loss = 9.48519
I0717 23:15:44.092902 23375 solver.cpp:237]     Train net output #0: lambda = 6.36456
I0717 23:15:44.092991 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48519 (* 1 = 9.48519 loss)
I0717 23:15:44.093050 23375 sgd_solver.cpp:105] Iteration 1300, lr = 0.001
I0717 23:17:20.140630 23375 solver.cpp:218] Iteration 1400 (1.04113 iter/s, 96.0498s/100 iters), loss = 9.49182
I0717 23:17:20.140826 23375 solver.cpp:237]     Train net output #0: lambda = 5.91296
I0717 23:17:20.140866 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49182 (* 1 = 9.49182 loss)
I0717 23:17:20.140889 23375 sgd_solver.cpp:105] Iteration 1400, lr = 0.001
I0717 23:18:56.560279 23375 solver.cpp:218] Iteration 1500 (1.03711 iter/s, 96.4215s/100 iters), loss = 9.49072
I0717 23:18:56.560472 23375 solver.cpp:237]     Train net output #0: lambda = 5.5212
I0717 23:18:56.560488 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49072 (* 1 = 9.49072 loss)
I0717 23:18:56.560503 23375 sgd_solver.cpp:105] Iteration 1500, lr = 0.001
I0717 23:20:33.023227 23375 solver.cpp:218] Iteration 1600 (1.03665 iter/s, 96.4648s/100 iters), loss = 9.49102
I0717 23:20:33.023459 23375 solver.cpp:237]     Train net output #0: lambda = 5.17813
I0717 23:20:33.023522 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49102 (* 1 = 9.49102 loss)
I0717 23:20:33.023572 23375 sgd_solver.cpp:105] Iteration 1600, lr = 0.001
I0717 23:22:10.267590 23375 solver.cpp:218] Iteration 1700 (1.02832 iter/s, 97.2462s/100 iters), loss = 9.49192
I0717 23:22:10.267801 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:22:10.267817 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49192 (* 1 = 9.49192 loss)
I0717 23:22:10.267828 23375 sgd_solver.cpp:105] Iteration 1700, lr = 0.001
I0717 23:23:47.751996 23375 solver.cpp:218] Iteration 1800 (1.02579 iter/s, 97.4863s/100 iters), loss = 9.49069
I0717 23:23:47.752184 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:23:47.752218 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49069 (* 1 = 9.49069 loss)
I0717 23:23:47.752243 23375 sgd_solver.cpp:105] Iteration 1800, lr = 0.001
I0717 23:25:25.847000 23375 solver.cpp:218] Iteration 1900 (1.0194 iter/s, 98.0969s/100 iters), loss = 9.48877
I0717 23:25:25.847189 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:25:25.847236 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48877 (* 1 = 9.48877 loss)
I0717 23:25:25.847266 23375 sgd_solver.cpp:105] Iteration 1900, lr = 0.001
I0717 23:27:04.050029 23375 solver.cpp:218] Iteration 2000 (1.01828 iter/s, 98.205s/100 iters), loss = 9.47877
I0717 23:27:04.050200 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:27:04.050226 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47877 (* 1 = 9.47877 loss)
I0717 23:27:04.050246 23375 sgd_solver.cpp:105] Iteration 2000, lr = 0.001
I0717 23:28:42.602609 23375 solver.cpp:218] Iteration 2100 (1.01467 iter/s, 98.5545s/100 iters), loss = 9.47989
I0717 23:28:42.602799 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:28:42.602824 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47989 (* 1 = 9.47989 loss)
I0717 23:28:42.602855 23375 sgd_solver.cpp:105] Iteration 2100, lr = 0.001
I0717 23:30:21.070199 23375 solver.cpp:218] Iteration 2200 (1.01554 iter/s, 98.4695s/100 iters), loss = 9.47797
I0717 23:30:21.070390 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:30:21.070408 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47797 (* 1 = 9.47797 loss)
I0717 23:30:21.070422 23375 sgd_solver.cpp:105] Iteration 2200, lr = 0.001
I0717 23:32:00.421607 23375 solver.cpp:218] Iteration 2300 (1.00651 iter/s, 99.3533s/100 iters), loss = 9.48282
I0717 23:32:00.421826 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:32:00.421886 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48282 (* 1 = 9.48282 loss)
I0717 23:32:00.421928 23375 sgd_solver.cpp:105] Iteration 2300, lr = 0.001
I0717 23:33:39.630357 23375 solver.cpp:218] Iteration 2400 (1.00796 iter/s, 99.2107s/100 iters), loss = 9.4765
I0717 23:33:39.630556 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:33:39.630595 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.4765 (* 1 = 9.4765 loss)
I0717 23:33:39.630620 23375 sgd_solver.cpp:105] Iteration 2400, lr = 0.001
I0717 23:35:19.253130 23375 solver.cpp:218] Iteration 2500 (1.00377 iter/s, 99.6247s/100 iters), loss = 9.48595
I0717 23:35:19.253361 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:35:19.253463 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48595 (* 1 = 9.48595 loss)
I0717 23:35:19.253521 23375 sgd_solver.cpp:105] Iteration 2500, lr = 0.001
I0717 23:36:58.723839 23375 solver.cpp:218] Iteration 2600 (1.0053 iter/s, 99.4726s/100 iters), loss = 9.47591
I0717 23:36:58.724123 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:36:58.724175 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47591 (* 1 = 9.47591 loss)
I0717 23:36:58.724213 23375 sgd_solver.cpp:105] Iteration 2600, lr = 0.001
I0717 23:38:37.742362 23375 solver.cpp:218] Iteration 2700 (1.00989 iter/s, 99.0204s/100 iters), loss = 9.48644
I0717 23:38:37.742578 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:38:37.742678 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48644 (* 1 = 9.48644 loss)
I0717 23:38:37.742763 23375 sgd_solver.cpp:105] Iteration 2700, lr = 0.001
I0717 23:40:17.476645 23375 solver.cpp:218] Iteration 2800 (1.00264 iter/s, 99.7362s/100 iters), loss = 9.47737
I0717 23:40:17.476919 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:40:17.476970 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47737 (* 1 = 9.47737 loss)
I0717 23:40:17.477005 23375 sgd_solver.cpp:105] Iteration 2800, lr = 0.001
I0717 23:41:57.371743 23375 solver.cpp:218] Iteration 2900 (1.00103 iter/s, 99.897s/100 iters), loss = 9.47615
I0717 23:41:57.371912 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:41:57.371934 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47615 (* 1 = 9.47615 loss)
I0717 23:41:57.371953 23375 sgd_solver.cpp:105] Iteration 2900, lr = 0.001
I0717 23:43:37.310703 23375 solver.cpp:218] Iteration 3000 (1.00059 iter/s, 99.9409s/100 iters), loss = 9.48168
I0717 23:43:37.310974 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:43:37.311077 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48168 (* 1 = 9.48168 loss)
I0717 23:43:37.311142 23375 sgd_solver.cpp:105] Iteration 3000, lr = 0.001
I0717 23:45:17.035151 23375 solver.cpp:218] Iteration 3100 (1.00274 iter/s, 99.7263s/100 iters), loss = 9.47358
I0717 23:45:17.035367 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:45:17.035398 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.47358 (* 1 = 9.47358 loss)
I0717 23:45:17.035423 23375 sgd_solver.cpp:105] Iteration 3100, lr = 0.001
I0717 23:46:56.569387 23375 solver.cpp:218] Iteration 3200 (1.00466 iter/s, 99.5362s/100 iters), loss = 9.48637
I0717 23:46:56.569577 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:46:56.569599 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48637 (* 1 = 9.48637 loss)
I0717 23:46:56.569615 23375 sgd_solver.cpp:105] Iteration 3200, lr = 0.001
I0717 23:48:36.361135 23375 solver.cpp:218] Iteration 3300 (1.00207 iter/s, 99.7937s/100 iters), loss = 9.48263
I0717 23:48:36.361359 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:48:36.361464 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48263 (* 1 = 9.48263 loss)
I0717 23:48:36.361524 23375 sgd_solver.cpp:105] Iteration 3300, lr = 0.001
I0717 23:50:16.594089 23375 solver.cpp:218] Iteration 3400 (0.997657 iter/s, 100.235s/100 iters), loss = 9.4873
I0717 23:50:16.594285 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:50:16.594305 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.4873 (* 1 = 9.4873 loss)
I0717 23:50:16.594324 23375 sgd_solver.cpp:105] Iteration 3400, lr = 0.001
I0717 23:51:56.325964 23375 solver.cpp:218] Iteration 3500 (1.00267 iter/s, 99.7338s/100 iters), loss = 9.48688
I0717 23:51:56.326165 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:51:56.326186 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48688 (* 1 = 9.48688 loss)
I0717 23:51:56.326202 23375 sgd_solver.cpp:105] Iteration 3500, lr = 0.001
I0717 23:53:35.833215 23375 solver.cpp:218] Iteration 3600 (1.00493 iter/s, 99.5092s/100 iters), loss = 9.48658
I0717 23:53:35.833441 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:53:35.833540 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48658 (* 1 = 9.48658 loss)
I0717 23:53:35.833611 23375 sgd_solver.cpp:105] Iteration 3600, lr = 0.001
I0717 23:55:15.809948 23375 solver.cpp:218] Iteration 3700 (1.00021 iter/s, 99.9787s/100 iters), loss = 9.49667
I0717 23:55:15.810130 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:55:15.810153 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49667 (* 1 = 9.49667 loss)
I0717 23:55:15.810169 23375 sgd_solver.cpp:105] Iteration 3700, lr = 0.001
I0717 23:56:55.783293 23375 solver.cpp:218] Iteration 3800 (1.00025 iter/s, 99.9753s/100 iters), loss = 9.48145
I0717 23:56:55.783552 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:56:55.783577 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48145 (* 1 = 9.48145 loss)
I0717 23:56:55.783594 23375 sgd_solver.cpp:105] Iteration 3800, lr = 0.001
I0717 23:58:35.957551 23375 solver.cpp:218] Iteration 3900 (0.998242 iter/s, 100.176s/100 iters), loss = 9.49019
I0717 23:58:35.958983 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0717 23:58:35.959013 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49019 (* 1 = 9.49019 loss)
I0717 23:58:35.959036 23375 sgd_solver.cpp:105] Iteration 3900, lr = 0.001
I0718 00:00:16.035022 23375 solver.cpp:218] Iteration 4000 (0.999219 iter/s, 100.078s/100 iters), loss = 9.49466
I0718 00:00:16.035202 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:00:16.035239 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49466 (* 1 = 9.49466 loss)
I0718 00:00:16.035269 23375 sgd_solver.cpp:105] Iteration 4000, lr = 0.001
I0718 00:01:55.931653 23375 solver.cpp:218] Iteration 4100 (1.00102 iter/s, 99.8986s/100 iters), loss = 9.49888
I0718 00:01:55.931844 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:01:55.931907 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49888 (* 1 = 9.49888 loss)
I0718 00:01:55.931946 23375 sgd_solver.cpp:105] Iteration 4100, lr = 0.001
I0718 00:03:36.588934 23375 solver.cpp:218] Iteration 4200 (0.993451 iter/s, 100.659s/100 iters), loss = 9.48687
I0718 00:03:36.589051 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:03:36.589066 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.48687 (* 1 = 9.48687 loss)
I0718 00:03:36.589077 23375 sgd_solver.cpp:105] Iteration 4200, lr = 0.001
I0718 00:05:16.774986 23375 solver.cpp:218] Iteration 4300 (0.998123 iter/s, 100.188s/100 iters), loss = 9.50055
I0718 00:05:16.775229 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:05:16.775264 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50055 (* 1 = 9.50055 loss)
I0718 00:05:16.775287 23375 sgd_solver.cpp:105] Iteration 4300, lr = 0.001
I0718 00:06:57.246660 23375 solver.cpp:218] Iteration 4400 (0.995286 iter/s, 100.474s/100 iters), loss = 9.49493
I0718 00:06:57.246865 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:06:57.246923 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49493 (* 1 = 9.49493 loss)
I0718 00:06:57.246968 23375 sgd_solver.cpp:105] Iteration 4400, lr = 0.001
I0718 00:08:37.206856 23375 solver.cpp:218] Iteration 4500 (1.00038 iter/s, 99.9621s/100 iters), loss = 9.50341
I0718 00:08:37.207090 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:08:37.207141 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50341 (* 1 = 9.50341 loss)
I0718 00:08:37.207175 23375 sgd_solver.cpp:105] Iteration 4500, lr = 0.001
I0718 00:10:17.330324 23375 solver.cpp:218] Iteration 4600 (0.998748 iter/s, 100.125s/100 iters), loss = 9.49508
I0718 00:10:17.330515 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:10:17.330541 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.49508 (* 1 = 9.49508 loss)
I0718 00:10:17.330559 23375 sgd_solver.cpp:105] Iteration 4600, lr = 0.001
I0718 00:11:57.325501 23375 solver.cpp:218] Iteration 4700 (1.00003 iter/s, 99.9971s/100 iters), loss = 9.50086
I0718 00:11:57.332396 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:11:57.332412 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50086 (* 1 = 9.50086 loss)
I0718 00:11:57.332442 23375 sgd_solver.cpp:105] Iteration 4700, lr = 0.001
I0718 00:13:37.620862 23375 solver.cpp:218] Iteration 4800 (0.997102 iter/s, 100.291s/100 iters), loss = 9.50207
I0718 00:13:37.621071 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:13:37.621165 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50207 (* 1 = 9.50207 loss)
I0718 00:13:37.621227 23375 sgd_solver.cpp:105] Iteration 4800, lr = 0.001
I0718 00:15:18.006212 23375 solver.cpp:218] Iteration 4900 (0.996142 iter/s, 100.387s/100 iters), loss = 9.50712
I0718 00:15:18.006400 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:15:18.006418 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50712 (* 1 = 9.50712 loss)
I0718 00:15:18.006429 23375 sgd_solver.cpp:105] Iteration 4900, lr = 0.001
I0718 00:16:58.357555 23375 solver.cpp:218] Iteration 5000 (0.996479 iter/s, 100.353s/100 iters), loss = 9.51641
I0718 00:16:58.357750 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:16:58.357771 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.51641 (* 1 = 9.51641 loss)
I0718 00:16:58.357787 23375 sgd_solver.cpp:105] Iteration 5000, lr = 0.001
I0718 00:18:38.703711 23375 solver.cpp:218] Iteration 5100 (0.996531 iter/s, 100.348s/100 iters), loss = 9.5086
I0718 00:18:38.703933 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:18:38.703989 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5086 (* 1 = 9.5086 loss)
I0718 00:18:38.704028 23375 sgd_solver.cpp:105] Iteration 5100, lr = 0.001
I0718 00:20:19.029466 23375 solver.cpp:218] Iteration 5200 (0.996734 iter/s, 100.328s/100 iters), loss = 9.50929
I0718 00:20:19.029696 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:20:19.029747 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50929 (* 1 = 9.50929 loss)
I0718 00:20:19.029779 23375 sgd_solver.cpp:105] Iteration 5200, lr = 0.001
I0718 00:21:59.355078 23375 solver.cpp:218] Iteration 5300 (0.996735 iter/s, 100.328s/100 iters), loss = 9.5171
I0718 00:21:59.355289 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:21:59.355316 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5171 (* 1 = 9.5171 loss)
I0718 00:21:59.355337 23375 sgd_solver.cpp:105] Iteration 5300, lr = 0.001
I0718 00:23:39.878767 23375 solver.cpp:218] Iteration 5400 (0.994771 iter/s, 100.526s/100 iters), loss = 9.50781
I0718 00:23:39.878928 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:23:39.878952 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.50781 (* 1 = 9.50781 loss)
I0718 00:23:39.878973 23375 sgd_solver.cpp:105] Iteration 5400, lr = 0.001
I0718 00:25:20.258630 23375 solver.cpp:218] Iteration 5500 (0.996196 iter/s, 100.382s/100 iters), loss = 9.51834
I0718 00:25:20.258905 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:25:20.258970 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.51834 (* 1 = 9.51834 loss)
I0718 00:25:20.259016 23375 sgd_solver.cpp:105] Iteration 5500, lr = 0.001
I0718 00:27:00.811462 23375 solver.cpp:218] Iteration 5600 (0.994483 iter/s, 100.555s/100 iters), loss = 9.51766
I0718 00:27:00.811643 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:27:00.811693 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.51766 (* 1 = 9.51766 loss)
I0718 00:27:00.811724 23375 sgd_solver.cpp:105] Iteration 5600, lr = 0.001
I0718 00:28:41.455389 23375 solver.cpp:218] Iteration 5700 (0.993582 iter/s, 100.646s/100 iters), loss = 9.51255
I0718 00:28:41.455567 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:28:41.455591 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.51255 (* 1 = 9.51255 loss)
I0718 00:28:41.455608 23375 sgd_solver.cpp:105] Iteration 5700, lr = 0.001
I0718 00:30:22.007594 23375 solver.cpp:218] Iteration 5800 (0.994489 iter/s, 100.554s/100 iters), loss = 9.52979
I0718 00:30:22.007823 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:30:22.007858 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.52979 (* 1 = 9.52979 loss)
I0718 00:30:22.007879 23375 sgd_solver.cpp:105] Iteration 5800, lr = 0.001
I0718 00:32:02.633090 23375 solver.cpp:218] Iteration 5900 (0.993765 iter/s, 100.627s/100 iters), loss = 9.52491
I0718 00:32:02.636399 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:32:02.636416 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.52491 (* 1 = 9.52491 loss)
I0718 00:32:02.636431 23375 sgd_solver.cpp:105] Iteration 5900, lr = 0.001
I0718 00:33:42.879966 23375 solver.cpp:218] Iteration 6000 (0.997549 iter/s, 100.246s/100 iters), loss = 9.52558
I0718 00:33:42.880300 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:33:42.880367 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.52558 (* 1 = 9.52558 loss)
I0718 00:33:42.880401 23375 sgd_solver.cpp:105] Iteration 6000, lr = 0.001
I0718 00:35:23.213724 23375 solver.cpp:218] Iteration 6100 (0.996655 iter/s, 100.336s/100 iters), loss = 9.5178
I0718 00:35:23.213938 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:35:23.213970 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5178 (* 1 = 9.5178 loss)
I0718 00:35:23.214004 23375 sgd_solver.cpp:105] Iteration 6100, lr = 0.001
I0718 00:37:03.763327 23375 solver.cpp:218] Iteration 6200 (0.994515 iter/s, 100.552s/100 iters), loss = 9.53147
I0718 00:37:03.763550 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:37:03.763587 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.53147 (* 1 = 9.53147 loss)
I0718 00:37:03.763612 23375 sgd_solver.cpp:105] Iteration 6200, lr = 0.001
I0718 00:38:44.126955 23375 solver.cpp:218] Iteration 6300 (0.996358 iter/s, 100.366s/100 iters), loss = 9.52675
I0718 00:38:44.127167 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:38:44.127199 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.52675 (* 1 = 9.52675 loss)
I0718 00:38:44.127230 23375 sgd_solver.cpp:105] Iteration 6300, lr = 0.001
I0718 00:40:24.711383 23375 solver.cpp:218] Iteration 6400 (0.99417 iter/s, 100.586s/100 iters), loss = 9.5218
I0718 00:40:24.711573 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:40:24.711599 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5218 (* 1 = 9.5218 loss)
I0718 00:40:24.711627 23375 sgd_solver.cpp:105] Iteration 6400, lr = 0.001
I0718 00:42:05.129433 23375 solver.cpp:218] Iteration 6500 (0.995817 iter/s, 100.42s/100 iters), loss = 9.5238
I0718 00:42:05.129580 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:42:05.129595 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5238 (* 1 = 9.5238 loss)
I0718 00:42:05.129606 23375 sgd_solver.cpp:105] Iteration 6500, lr = 0.001
I0718 00:43:45.531569 23375 solver.cpp:218] Iteration 6600 (0.995975 iter/s, 100.404s/100 iters), loss = 9.53495
I0718 00:43:45.531738 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:43:45.531752 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.53495 (* 1 = 9.53495 loss)
I0718 00:43:45.531771 23375 sgd_solver.cpp:105] Iteration 6600, lr = 0.001
I0718 00:45:25.769919 23375 solver.cpp:218] Iteration 6700 (0.997602 iter/s, 100.24s/100 iters), loss = 9.55249
I0718 00:45:25.770103 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:45:25.770141 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.55249 (* 1 = 9.55249 loss)
I0718 00:45:25.770170 23375 sgd_solver.cpp:105] Iteration 6700, lr = 0.001
I0718 00:47:06.327023 23375 solver.cpp:218] Iteration 6800 (0.99444 iter/s, 100.559s/100 iters), loss = 9.54259
I0718 00:47:06.327201 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:47:06.327221 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.54259 (* 1 = 9.54259 loss)
I0718 00:47:06.327234 23375 sgd_solver.cpp:105] Iteration 6800, lr = 0.001
I0718 00:48:46.818317 23375 solver.cpp:218] Iteration 6900 (0.995092 iter/s, 100.493s/100 iters), loss = 9.55164
I0718 00:48:46.818508 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:48:46.818542 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.55164 (* 1 = 9.55164 loss)
I0718 00:48:46.818560 23375 sgd_solver.cpp:105] Iteration 6900, lr = 0.001
I0718 00:50:27.752084 23375 solver.cpp:218] Iteration 7000 (0.990729 iter/s, 100.936s/100 iters), loss = 9.52944
I0718 00:50:27.752295 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:50:27.752365 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.52944 (* 1 = 9.52944 loss)
I0718 00:50:27.752398 23375 sgd_solver.cpp:105] Iteration 7000, lr = 0.001
I0718 00:52:08.207271 23375 solver.cpp:218] Iteration 7100 (0.995449 iter/s, 100.457s/100 iters), loss = 9.55609
I0718 00:52:08.207451 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:52:08.207468 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.55609 (* 1 = 9.55609 loss)
I0718 00:52:08.207480 23375 sgd_solver.cpp:105] Iteration 7100, lr = 0.001
I0718 00:53:48.729557 23375 solver.cpp:218] Iteration 7200 (0.994785 iter/s, 100.524s/100 iters), loss = 9.54575
I0718 00:53:48.729765 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:53:48.729805 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.54575 (* 1 = 9.54575 loss)
I0718 00:53:48.729826 23375 sgd_solver.cpp:105] Iteration 7200, lr = 0.001
I0718 00:55:29.914824 23375 solver.cpp:218] Iteration 7300 (0.988267 iter/s, 101.187s/100 iters), loss = 9.54339
I0718 00:55:29.914988 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:55:29.915020 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.54339 (* 1 = 9.54339 loss)
I0718 00:55:29.915036 23375 sgd_solver.cpp:105] Iteration 7300, lr = 0.001
I0718 00:57:10.225174 23375 solver.cpp:218] Iteration 7400 (0.996886 iter/s, 100.312s/100 iters), loss = 9.54715
I0718 00:57:10.225352 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:57:10.225370 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.54715 (* 1 = 9.54715 loss)
I0718 00:57:10.225383 23375 sgd_solver.cpp:105] Iteration 7400, lr = 0.001
I0718 00:58:50.597375 23375 solver.cpp:218] Iteration 7500 (0.996272 iter/s, 100.374s/100 iters), loss = 9.5382
I0718 00:58:50.597594 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 00:58:50.597657 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5382 (* 1 = 9.5382 loss)
I0718 00:58:50.597707 23375 sgd_solver.cpp:105] Iteration 7500, lr = 0.001
I0718 01:00:31.547341 23375 solver.cpp:218] Iteration 7600 (0.99057 iter/s, 100.952s/100 iters), loss = 9.56639
I0718 01:00:31.547561 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:00:31.547662 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.56639 (* 1 = 9.56639 loss)
I0718 01:00:31.547736 23375 sgd_solver.cpp:105] Iteration 7600, lr = 0.001
I0718 01:02:12.075420 23375 solver.cpp:218] Iteration 7700 (0.994728 iter/s, 100.53s/100 iters), loss = 9.56359
I0718 01:02:12.075592 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:02:12.075630 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.56359 (* 1 = 9.56359 loss)
I0718 01:02:12.075673 23375 sgd_solver.cpp:105] Iteration 7700, lr = 0.001
I0718 01:03:52.690448 23375 solver.cpp:218] Iteration 7800 (0.993868 iter/s, 100.617s/100 iters), loss = 9.55839
I0718 01:03:52.690675 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:03:52.690707 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.55839 (* 1 = 9.55839 loss)
I0718 01:03:52.690729 23375 sgd_solver.cpp:105] Iteration 7800, lr = 0.001
I0718 01:05:33.400193 23375 solver.cpp:218] Iteration 7900 (0.992933 iter/s, 100.712s/100 iters), loss = 9.56858
I0718 01:05:33.400396 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:05:33.400434 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.56858 (* 1 = 9.56858 loss)
I0718 01:05:33.400460 23375 sgd_solver.cpp:105] Iteration 7900, lr = 0.001
I0718 01:07:13.816089 23375 solver.cpp:218] Iteration 8000 (0.995839 iter/s, 100.418s/100 iters), loss = 9.54795
I0718 01:07:13.816309 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:07:13.816380 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.54795 (* 1 = 9.54795 loss)
I0718 01:07:13.816418 23375 sgd_solver.cpp:105] Iteration 8000, lr = 0.001
I0718 01:08:54.593657 23375 solver.cpp:218] Iteration 8100 (0.992265 iter/s, 100.78s/100 iters), loss = 9.55898
I0718 01:08:54.593909 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:08:54.593933 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.55898 (* 1 = 9.55898 loss)
I0718 01:08:54.593955 23375 sgd_solver.cpp:105] Iteration 8100, lr = 0.001
I0718 01:10:35.152470 23375 solver.cpp:218] Iteration 8200 (0.994424 iter/s, 100.561s/100 iters), loss = 9.5747
I0718 01:10:35.152688 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:10:35.152711 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5747 (* 1 = 9.5747 loss)
I0718 01:10:35.152729 23375 sgd_solver.cpp:105] Iteration 8200, lr = 0.001
I0718 01:12:16.430047 23375 solver.cpp:218] Iteration 8300 (0.987366 iter/s, 101.28s/100 iters), loss = 9.56932
I0718 01:12:16.430253 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:12:16.430279 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.56932 (* 1 = 9.56932 loss)
I0718 01:12:16.430296 23375 sgd_solver.cpp:105] Iteration 8300, lr = 0.001
I0718 01:13:57.841338 23375 solver.cpp:218] Iteration 8400 (0.986064 iter/s, 101.413s/100 iters), loss = 9.55695
I0718 01:13:57.841531 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:13:57.841563 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.55695 (* 1 = 9.55695 loss)
I0718 01:13:57.841599 23375 sgd_solver.cpp:105] Iteration 8400, lr = 0.001
I0718 01:15:38.419020 23375 solver.cpp:218] Iteration 8500 (0.994237 iter/s, 100.58s/100 iters), loss = 9.57992
I0718 01:15:38.419250 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:15:38.419345 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.57992 (* 1 = 9.57992 loss)
I0718 01:15:38.419409 23375 sgd_solver.cpp:105] Iteration 8500, lr = 0.001
I0718 01:17:19.383666 23375 solver.cpp:218] Iteration 8600 (0.990427 iter/s, 100.967s/100 iters), loss = 9.56554
I0718 01:17:19.383877 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:17:19.383909 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.56554 (* 1 = 9.56554 loss)
I0718 01:17:19.383934 23375 sgd_solver.cpp:105] Iteration 8600, lr = 0.001
I0718 01:19:00.598119 23375 solver.cpp:218] Iteration 8700 (0.987982 iter/s, 101.216s/100 iters), loss = 9.5877
I0718 01:19:00.598275 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:19:00.598296 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.5877 (* 1 = 9.5877 loss)
I0718 01:19:00.598316 23375 sgd_solver.cpp:105] Iteration 8700, lr = 0.001
I0718 01:20:41.311868 23375 solver.cpp:218] Iteration 8800 (0.992893 iter/s, 100.716s/100 iters), loss = 9.58056
I0718 01:20:41.312000 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:20:41.312019 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.58056 (* 1 = 9.58056 loss)
I0718 01:20:41.312041 23375 sgd_solver.cpp:105] Iteration 8800, lr = 0.001
I0718 01:22:22.032943 23375 solver.cpp:218] Iteration 8900 (0.992821 iter/s, 100.723s/100 iters), loss = 9.59353
I0718 01:22:22.033154 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:22:22.033219 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.59353 (* 1 = 9.59353 loss)
I0718 01:22:22.033267 23375 sgd_solver.cpp:105] Iteration 8900, lr = 0.001
I0718 01:24:03.039510 23375 solver.cpp:218] Iteration 9000 (0.990015 iter/s, 101.009s/100 iters), loss = 9.59018
I0718 01:24:03.039672 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:24:03.039695 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.59018 (* 1 = 9.59018 loss)
I0718 01:24:03.039722 23375 sgd_solver.cpp:105] Iteration 9000, lr = 0.001
I0718 01:25:43.787031 23375 solver.cpp:218] Iteration 9100 (0.992561 iter/s, 100.75s/100 iters), loss = 9.59735
I0718 01:25:43.787191 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:25:43.787206 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.59735 (* 1 = 9.59735 loss)
I0718 01:25:43.787217 23375 sgd_solver.cpp:105] Iteration 9100, lr = 0.001
I0718 01:27:24.981426 23375 solver.cpp:218] Iteration 9200 (0.988177 iter/s, 101.196s/100 iters), loss = 9.6094
I0718 01:27:24.981668 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:27:24.981731 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.6094 (* 1 = 9.6094 loss)
I0718 01:27:24.981793 23375 sgd_solver.cpp:105] Iteration 9200, lr = 0.001
I0718 01:29:06.123481 23375 solver.cpp:218] Iteration 9300 (0.988689 iter/s, 101.144s/100 iters), loss = 9.59759
I0718 01:29:06.123719 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:29:06.123787 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.59759 (* 1 = 9.59759 loss)
I0718 01:29:06.123833 23375 sgd_solver.cpp:105] Iteration 9300, lr = 0.001
I0718 01:30:47.224764 23375 solver.cpp:218] Iteration 9400 (0.989088 iter/s, 101.103s/100 iters), loss = 9.624
I0718 01:30:47.224983 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:30:47.225083 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.624 (* 1 = 9.624 loss)
I0718 01:30:47.225152 23375 sgd_solver.cpp:105] Iteration 9400, lr = 0.001
I0718 01:32:28.338583 23375 solver.cpp:218] Iteration 9500 (0.988965 iter/s, 101.116s/100 iters), loss = 9.60335
I0718 01:32:28.339771 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:32:28.339797 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.60335 (* 1 = 9.60335 loss)
I0718 01:32:28.339813 23375 sgd_solver.cpp:105] Iteration 9500, lr = 0.001
I0718 01:34:09.695852 23375 solver.cpp:218] Iteration 9600 (0.986599 iter/s, 101.358s/100 iters), loss = 9.59501
I0718 01:34:09.696063 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:34:09.696162 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.59501 (* 1 = 9.59501 loss)
I0718 01:34:09.696235 23375 sgd_solver.cpp:105] Iteration 9600, lr = 0.001
I0718 01:35:50.598244 23375 solver.cpp:218] Iteration 9700 (0.991037 iter/s, 100.904s/100 iters), loss = 9.61592
I0718 01:35:50.598482 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:35:50.598534 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.61592 (* 1 = 9.61592 loss)
I0718 01:35:50.598567 23375 sgd_solver.cpp:105] Iteration 9700, lr = 0.001
I0718 01:37:31.993999 23375 solver.cpp:218] Iteration 9800 (0.986216 iter/s, 101.398s/100 iters), loss = 9.61773
I0718 01:37:31.994256 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:37:31.994319 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.61773 (* 1 = 9.61773 loss)
I0718 01:37:31.994367 23375 sgd_solver.cpp:105] Iteration 9800, lr = 0.001
I0718 01:39:13.163355 23375 solver.cpp:218] Iteration 9900 (0.988423 iter/s, 101.171s/100 iters), loss = 9.63601
I0718 01:39:13.163547 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:39:13.163581 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.63601 (* 1 = 9.63601 loss)
I0718 01:39:13.163610 23375 sgd_solver.cpp:105] Iteration 9900, lr = 0.001
I0718 01:40:53.986366 23375 solver.cpp:218] Iteration 10000 (0.991818 iter/s, 100.825s/100 iters), loss = 9.6269
I0718 01:40:53.986589 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:40:53.986654 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.6269 (* 1 = 9.6269 loss)
I0718 01:40:53.986703 23375 sgd_solver.cpp:105] Iteration 10000, lr = 0.001
I0718 01:42:34.983361 23375 solver.cpp:218] Iteration 10100 (0.990109 iter/s, 100.999s/100 iters), loss = 9.62735
I0718 01:42:34.983563 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:42:34.983585 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.62735 (* 1 = 9.62735 loss)
I0718 01:42:34.983603 23375 sgd_solver.cpp:105] Iteration 10100, lr = 0.001
I0718 01:44:16.297634 23375 solver.cpp:218] Iteration 10200 (0.987009 iter/s, 101.316s/100 iters), loss = 9.6151
I0718 01:44:16.297828 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:44:16.297873 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.6151 (* 1 = 9.6151 loss)
I0718 01:44:16.297904 23375 sgd_solver.cpp:105] Iteration 10200, lr = 0.001
I0718 01:45:57.275441 23375 solver.cpp:218] Iteration 10300 (0.990298 iter/s, 100.98s/100 iters), loss = 9.6494
I0718 01:45:57.275616 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:45:57.275632 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.6494 (* 1 = 9.6494 loss)
I0718 01:45:57.275655 23375 sgd_solver.cpp:105] Iteration 10300, lr = 0.001
I0718 01:47:38.138640 23375 solver.cpp:218] Iteration 10400 (0.991422 iter/s, 100.865s/100 iters), loss = 9.62177
I0718 01:47:38.138869 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:47:38.138921 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.62177 (* 1 = 9.62177 loss)
I0718 01:47:38.138954 23375 sgd_solver.cpp:105] Iteration 10400, lr = 0.001
I0718 01:49:19.323482 23375 solver.cpp:218] Iteration 10500 (0.988271 iter/s, 101.187s/100 iters), loss = 9.61187
I0718 01:49:19.323706 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:49:19.323757 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.61187 (* 1 = 9.61187 loss)
I0718 01:49:19.323789 23375 sgd_solver.cpp:105] Iteration 10500, lr = 0.001
I0718 01:51:00.506584 23375 solver.cpp:218] Iteration 10600 (0.988288 iter/s, 101.185s/100 iters), loss = 9.64353
I0718 01:51:00.506723 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:51:00.506739 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.64353 (* 1 = 9.64353 loss)
I0718 01:51:00.506752 23375 sgd_solver.cpp:105] Iteration 10600, lr = 0.001
I0718 01:52:41.715276 23375 solver.cpp:218] Iteration 10700 (0.988038 iter/s, 101.211s/100 iters), loss = 9.65135
I0718 01:52:41.715502 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:52:41.715602 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.65135 (* 1 = 9.65135 loss)
I0718 01:52:41.715677 23375 sgd_solver.cpp:105] Iteration 10700, lr = 0.001
I0718 01:54:22.478708 23375 solver.cpp:218] Iteration 10800 (0.992404 iter/s, 100.765s/100 iters), loss = 9.61249
I0718 01:54:22.478956 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:54:22.479012 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.61249 (* 1 = 9.61249 loss)
I0718 01:54:22.479051 23375 sgd_solver.cpp:105] Iteration 10800, lr = 0.001
I0718 01:56:03.490126 23375 solver.cpp:218] Iteration 10900 (0.989968 iter/s, 101.013s/100 iters), loss = 9.65992
I0718 01:56:03.490305 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:56:03.490334 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.65992 (* 1 = 9.65992 loss)
I0718 01:56:03.490353 23375 sgd_solver.cpp:105] Iteration 10900, lr = 0.001
I0718 01:57:44.796871 23375 solver.cpp:218] Iteration 11000 (0.987082 iter/s, 101.309s/100 iters), loss = 9.65611
I0718 01:57:44.797101 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:57:44.797204 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.65611 (* 1 = 9.65611 loss)
I0718 01:57:44.797277 23375 sgd_solver.cpp:105] Iteration 11000, lr = 0.001
I0718 01:59:26.008668 23375 solver.cpp:218] Iteration 11100 (0.988008 iter/s, 101.214s/100 iters), loss = 9.64895
I0718 01:59:26.008852 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 01:59:26.008874 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.64895 (* 1 = 9.64895 loss)
I0718 01:59:26.008890 23375 sgd_solver.cpp:105] Iteration 11100, lr = 0.001
I0718 02:01:07.147578 23375 solver.cpp:218] Iteration 11200 (0.98872 iter/s, 101.141s/100 iters), loss = 9.67362
I0718 02:01:07.147811 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:01:07.147862 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.67362 (* 1 = 9.67362 loss)
I0718 02:01:07.147894 23375 sgd_solver.cpp:105] Iteration 11200, lr = 0.001
I0718 02:02:48.036325 23375 solver.cpp:218] Iteration 11300 (0.991172 iter/s, 100.891s/100 iters), loss = 9.65906
I0718 02:02:48.039923 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:02:48.039995 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.65906 (* 1 = 9.65906 loss)
I0718 02:02:48.040053 23375 sgd_solver.cpp:105] Iteration 11300, lr = 0.001
I0718 02:04:29.055425 23375 solver.cpp:218] Iteration 11400 (0.989926 iter/s, 101.018s/100 iters), loss = 9.6886
I0718 02:04:29.060425 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:04:29.060458 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.6886 (* 1 = 9.6886 loss)
I0718 02:04:29.060478 23375 sgd_solver.cpp:105] Iteration 11400, lr = 0.001
I0718 02:06:09.476619 23375 solver.cpp:218] Iteration 11500 (0.995834 iter/s, 100.418s/100 iters), loss = 9.70347
I0718 02:06:09.476863 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:06:09.476920 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.70347 (* 1 = 9.70347 loss)
I0718 02:06:09.476963 23375 sgd_solver.cpp:105] Iteration 11500, lr = 0.001
I0718 02:07:50.586043 23375 solver.cpp:218] Iteration 11600 (0.989009 iter/s, 101.111s/100 iters), loss = 9.6619
I0718 02:07:50.586215 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:07:50.586249 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.6619 (* 1 = 9.6619 loss)
I0718 02:07:50.586267 23375 sgd_solver.cpp:105] Iteration 11600, lr = 0.001
I0718 02:09:31.547524 23375 solver.cpp:218] Iteration 11700 (0.990457 iter/s, 100.963s/100 iters), loss = 9.66946
I0718 02:09:31.547735 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:09:31.547802 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.66946 (* 1 = 9.66946 loss)
I0718 02:09:31.547848 23375 sgd_solver.cpp:105] Iteration 11700, lr = 0.001
I0718 02:11:12.518673 23375 solver.cpp:218] Iteration 11800 (0.990363 iter/s, 100.973s/100 iters), loss = 9.75233
I0718 02:11:12.518879 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:11:12.518913 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.75233 (* 1 = 9.75233 loss)
I0718 02:11:12.518936 23375 sgd_solver.cpp:105] Iteration 11800, lr = 0.001
I0718 02:12:53.521211 23375 solver.cpp:218] Iteration 11900 (0.990055 iter/s, 101.005s/100 iters), loss = 9.70765
I0718 02:12:53.524392 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:12:53.524417 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.70765 (* 1 = 9.70765 loss)
I0718 02:12:53.524435 23375 sgd_solver.cpp:105] Iteration 11900, lr = 0.001
I0718 02:14:34.693346 23375 solver.cpp:218] Iteration 12000 (0.988424 iter/s, 101.171s/100 iters), loss = 9.7229
I0718 02:14:34.693536 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:14:34.693573 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.7229 (* 1 = 9.7229 loss)
I0718 02:14:34.693606 23375 sgd_solver.cpp:105] Iteration 12000, lr = 0.001
I0718 02:16:15.833027 23375 solver.cpp:218] Iteration 12100 (0.988712 iter/s, 101.142s/100 iters), loss = 9.70516
I0718 02:16:15.833204 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:16:15.833230 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.70516 (* 1 = 9.70516 loss)
I0718 02:16:15.833247 23375 sgd_solver.cpp:105] Iteration 12100, lr = 0.001
I0718 02:17:57.059334 23375 solver.cpp:218] Iteration 12200 (0.987866 iter/s, 101.228s/100 iters), loss = 9.77852
I0718 02:17:57.059520 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:17:57.059554 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.77852 (* 1 = 9.77852 loss)
I0718 02:17:57.059576 23375 sgd_solver.cpp:105] Iteration 12200, lr = 0.001
I0718 02:19:38.427109 23375 solver.cpp:218] Iteration 12300 (0.986487 iter/s, 101.37s/100 iters), loss = 9.68515
I0718 02:19:38.427276 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:19:38.427292 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.68515 (* 1 = 9.68515 loss)
I0718 02:19:38.427302 23375 sgd_solver.cpp:105] Iteration 12300, lr = 0.001
I0718 02:21:19.691793 23375 solver.cpp:218] Iteration 12400 (0.987492 iter/s, 101.267s/100 iters), loss = 9.71064
I0718 02:21:19.696393 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:21:19.696421 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.71064 (* 1 = 9.71064 loss)
I0718 02:21:19.696475 23375 sgd_solver.cpp:105] Iteration 12400, lr = 0.001
I0718 02:23:00.995018 23375 solver.cpp:218] Iteration 12500 (0.987159 iter/s, 101.301s/100 iters), loss = 9.75937
I0718 02:23:00.995246 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:23:00.995348 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.75937 (* 1 = 9.75937 loss)
I0718 02:23:00.995421 23375 sgd_solver.cpp:105] Iteration 12500, lr = 0.001
I0718 02:24:42.110776 23375 solver.cpp:218] Iteration 12600 (0.988946 iter/s, 101.118s/100 iters), loss = 9.79198
I0718 02:24:42.110944 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:24:42.110966 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.79198 (* 1 = 9.79198 loss)
I0718 02:24:42.110983 23375 sgd_solver.cpp:105] Iteration 12600, lr = 0.001
I0718 02:26:23.103086 23375 solver.cpp:218] Iteration 12700 (0.990155 iter/s, 100.994s/100 iters), loss = 9.78138
I0718 02:26:23.103265 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:26:23.103286 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.78138 (* 1 = 9.78138 loss)
I0718 02:26:23.103303 23375 sgd_solver.cpp:105] Iteration 12700, lr = 0.001
I0718 02:28:04.318502 23375 solver.cpp:218] Iteration 12800 (0.987972 iter/s, 101.217s/100 iters), loss = 9.85041
I0718 02:28:04.324422 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:28:04.324460 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.85041 (* 1 = 9.85041 loss)
I0718 02:28:04.324486 23375 sgd_solver.cpp:105] Iteration 12800, lr = 0.001
I0718 02:29:45.189729 23375 solver.cpp:218] Iteration 12900 (0.9914 iter/s, 100.867s/100 iters), loss = 9.79824
I0718 02:29:45.189877 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:29:45.189909 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.79824 (* 1 = 9.79824 loss)
I0718 02:29:45.189927 23375 sgd_solver.cpp:105] Iteration 12900, lr = 0.001
I0718 02:31:26.785132 23375 solver.cpp:218] Iteration 13000 (0.984277 iter/s, 101.597s/100 iters), loss = 9.86833
I0718 02:31:26.785362 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:31:26.785403 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.86833 (* 1 = 9.86833 loss)
I0718 02:31:26.785431 23375 sgd_solver.cpp:105] Iteration 13000, lr = 0.001
I0718 02:33:07.923915 23375 solver.cpp:218] Iteration 13100 (0.988721 iter/s, 101.141s/100 iters), loss = 9.86156
I0718 02:33:07.924057 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:33:07.924078 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.86156 (* 1 = 9.86156 loss)
I0718 02:33:07.924121 23375 sgd_solver.cpp:105] Iteration 13100, lr = 0.001
I0718 02:34:49.499821 23375 solver.cpp:218] Iteration 13200 (0.984466 iter/s, 101.578s/100 iters), loss = 9.88825
I0718 02:34:49.500028 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:34:49.500084 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.88825 (* 1 = 9.88825 loss)
I0718 02:34:49.500123 23375 sgd_solver.cpp:105] Iteration 13200, lr = 0.001
I0718 02:36:30.655930 23375 solver.cpp:218] Iteration 13300 (0.988552 iter/s, 101.158s/100 iters), loss = 9.86975
I0718 02:36:30.656152 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:36:30.656201 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.86975 (* 1 = 9.86975 loss)
I0718 02:36:30.656232 23375 sgd_solver.cpp:105] Iteration 13300, lr = 0.001
I0718 02:38:11.857650 23375 solver.cpp:218] Iteration 13400 (0.988106 iter/s, 101.204s/100 iters), loss = 9.86568
I0718 02:38:11.857839 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:38:11.857867 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.86568 (* 1 = 9.86568 loss)
I0718 02:38:11.857887 23375 sgd_solver.cpp:105] Iteration 13400, lr = 0.001
I0718 02:39:53.006444 23375 solver.cpp:218] Iteration 13500 (0.988623 iter/s, 101.151s/100 iters), loss = 9.92247
I0718 02:39:53.006673 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:39:53.006705 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.92247 (* 1 = 9.92247 loss)
I0718 02:39:53.006731 23375 sgd_solver.cpp:105] Iteration 13500, lr = 0.001
I0718 02:41:34.374572 23375 solver.cpp:218] Iteration 13600 (0.986484 iter/s, 101.37s/100 iters), loss = 9.91687
I0718 02:41:34.374780 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:41:34.374842 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.91687 (* 1 = 9.91687 loss)
I0718 02:41:34.374889 23375 sgd_solver.cpp:105] Iteration 13600, lr = 0.001
I0718 02:43:15.572646 23375 solver.cpp:218] Iteration 13700 (0.988142 iter/s, 101.2s/100 iters), loss = 10.036
I0718 02:43:15.572847 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:43:15.572913 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.036 (* 1 = 10.036 loss)
I0718 02:43:15.572952 23375 sgd_solver.cpp:105] Iteration 13700, lr = 0.001
I0718 02:44:56.751641 23375 solver.cpp:218] Iteration 13800 (0.988328 iter/s, 101.181s/100 iters), loss = 10.0123
I0718 02:44:56.751866 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:44:56.751924 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.0123 (* 1 = 10.0123 loss)
I0718 02:44:56.751978 23375 sgd_solver.cpp:105] Iteration 13800, lr = 0.001
I0718 02:46:37.961447 23375 solver.cpp:218] Iteration 13900 (0.988027 iter/s, 101.212s/100 iters), loss = 9.98887
I0718 02:46:37.961634 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:46:37.961658 23375 solver.cpp:237]     Train net output #1: softmax_loss = 9.98887 (* 1 = 9.98887 loss)
I0718 02:46:37.961674 23375 sgd_solver.cpp:105] Iteration 13900, lr = 0.001
I0718 02:48:19.150601 23375 solver.cpp:218] Iteration 14000 (0.988229 iter/s, 101.191s/100 iters), loss = 10.0788
I0718 02:48:19.150749 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:48:19.150773 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.0788 (* 1 = 10.0788 loss)
I0718 02:48:19.150789 23375 sgd_solver.cpp:105] Iteration 14000, lr = 0.001
I0718 02:50:00.538832 23375 solver.cpp:218] Iteration 14100 (0.986288 iter/s, 101.39s/100 iters), loss = 10.2177
I0718 02:50:00.539016 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:50:00.539049 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.2177 (* 1 = 10.2177 loss)
I0718 02:50:00.539077 23375 sgd_solver.cpp:105] Iteration 14100, lr = 0.001
I0718 02:51:41.650775 23375 solver.cpp:218] Iteration 14200 (0.988984 iter/s, 101.114s/100 iters), loss = 10.1459
I0718 02:51:41.650957 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:51:41.651010 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.1459 (* 1 = 10.1459 loss)
I0718 02:51:41.651036 23375 sgd_solver.cpp:105] Iteration 14200, lr = 0.001
I0718 02:53:22.820586 23375 solver.cpp:218] Iteration 14300 (0.988418 iter/s, 101.172s/100 iters), loss = 10.223
I0718 02:53:22.820796 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:53:22.820837 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.223 (* 1 = 10.223 loss)
I0718 02:53:22.820865 23375 sgd_solver.cpp:105] Iteration 14300, lr = 0.001
I0718 02:55:03.754406 23375 solver.cpp:218] Iteration 14400 (0.990729 iter/s, 100.936s/100 iters), loss = 10.2284
I0718 02:55:03.754614 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:55:03.754647 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.2284 (* 1 = 10.2284 loss)
I0718 02:55:03.754668 23375 sgd_solver.cpp:105] Iteration 14400, lr = 0.001
I0718 02:56:44.712685 23375 solver.cpp:218] Iteration 14500 (0.990489 iter/s, 100.96s/100 iters), loss = 10.4358
I0718 02:56:44.712895 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:56:44.712944 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.4358 (* 1 = 10.4358 loss)
I0718 02:56:44.712994 23375 sgd_solver.cpp:105] Iteration 14500, lr = 0.001
I0718 02:58:26.059729 23375 solver.cpp:218] Iteration 14600 (0.986689 iter/s, 101.349s/100 iters), loss = 10.595
I0718 02:58:26.060015 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 02:58:26.060066 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.595 (* 1 = 10.595 loss)
I0718 02:58:26.060097 23375 sgd_solver.cpp:105] Iteration 14600, lr = 0.001
I0718 03:00:07.232529 23375 solver.cpp:218] Iteration 14700 (0.988389 iter/s, 101.175s/100 iters), loss = 10.593
I0718 03:00:07.232692 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:00:07.232707 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.593 (* 1 = 10.593 loss)
I0718 03:00:07.232718 23375 sgd_solver.cpp:105] Iteration 14700, lr = 0.001
I0718 03:01:48.242653 23375 solver.cpp:218] Iteration 14800 (0.98998 iter/s, 101.012s/100 iters), loss = 10.7012
I0718 03:01:48.242818 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:01:48.242835 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.7012 (* 1 = 10.7012 loss)
I0718 03:01:48.242846 23375 sgd_solver.cpp:105] Iteration 14800, lr = 0.001
I0718 03:03:29.195673 23375 solver.cpp:218] Iteration 14900 (0.99054 iter/s, 100.955s/100 iters), loss = 10.5982
I0718 03:03:29.195848 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:03:29.195873 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.5982 (* 1 = 10.5982 loss)
I0718 03:03:29.195902 23375 sgd_solver.cpp:105] Iteration 14900, lr = 0.001
I0718 03:05:10.552527 23375 solver.cpp:218] Iteration 15000 (0.986594 iter/s, 101.359s/100 iters), loss = 10.6676
I0718 03:05:10.552716 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:05:10.552742 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.6676 (* 1 = 10.6676 loss)
I0718 03:05:10.552762 23375 sgd_solver.cpp:105] Iteration 15000, lr = 0.001
I0718 03:06:51.931542 23375 solver.cpp:218] Iteration 15100 (0.986378 iter/s, 101.381s/100 iters), loss = 10.7951
I0718 03:06:51.931728 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:06:51.931763 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.7951 (* 1 = 10.7951 loss)
I0718 03:06:51.931804 23375 sgd_solver.cpp:105] Iteration 15100, lr = 0.001
I0718 03:08:33.012925 23375 solver.cpp:218] Iteration 15200 (0.989282 iter/s, 101.083s/100 iters), loss = 10.8098
I0718 03:08:33.013139 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:08:33.013178 23375 solver.cpp:237]     Train net output #1: softmax_loss = 10.8098 (* 1 = 10.8098 loss)
I0718 03:08:33.013204 23375 sgd_solver.cpp:105] Iteration 15200, lr = 0.001
I0718 03:10:13.910589 23375 solver.cpp:218] Iteration 15300 (0.991084 iter/s, 100.9s/100 iters), loss = 11.204
I0718 03:10:13.910778 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:10:13.910814 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.204 (* 1 = 11.204 loss)
I0718 03:10:13.910836 23375 sgd_solver.cpp:105] Iteration 15300, lr = 0.001
I0718 03:11:55.425107 23375 solver.cpp:218] Iteration 15400 (0.985061 iter/s, 101.517s/100 iters), loss = 11.0729
I0718 03:11:55.425320 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:11:55.425374 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.0729 (* 1 = 11.0729 loss)
I0718 03:11:55.425410 23375 sgd_solver.cpp:105] Iteration 15400, lr = 0.001
I0718 03:13:36.269032 23375 solver.cpp:218] Iteration 15500 (0.991612 iter/s, 100.846s/100 iters), loss = 11.6686
I0718 03:13:36.269181 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:13:36.269207 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.6686 (* 1 = 11.6686 loss)
I0718 03:13:36.269220 23375 sgd_solver.cpp:105] Iteration 15500, lr = 0.001
I0718 03:15:17.771124 23375 solver.cpp:218] Iteration 15600 (0.985182 iter/s, 101.504s/100 iters), loss = 11.3176
I0718 03:15:17.771304 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:15:17.771325 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.3176 (* 1 = 11.3176 loss)
I0718 03:15:17.771340 23375 sgd_solver.cpp:105] Iteration 15600, lr = 0.001
I0718 03:16:58.923072 23375 solver.cpp:218] Iteration 15700 (0.988592 iter/s, 101.154s/100 iters), loss = 11.325
I0718 03:16:58.923270 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:16:58.923287 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.325 (* 1 = 11.325 loss)
I0718 03:16:58.923298 23375 sgd_solver.cpp:105] Iteration 15700, lr = 0.001
I0718 03:18:40.182162 23375 solver.cpp:218] Iteration 15800 (0.987546 iter/s, 101.261s/100 iters), loss = 11.6198
I0718 03:18:40.182380 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:18:40.182448 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.6198 (* 1 = 11.6198 loss)
I0718 03:18:40.182495 23375 sgd_solver.cpp:105] Iteration 15800, lr = 0.001
I0718 03:20:20.931315 23375 solver.cpp:218] Iteration 15900 (0.992545 iter/s, 100.751s/100 iters), loss = 11.4196
I0718 03:20:20.931530 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:20:20.931555 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.4196 (* 1 = 11.4196 loss)
I0718 03:20:20.931577 23375 sgd_solver.cpp:105] Iteration 15900, lr = 0.001
I0718 03:22:02.503970 23382 sgd_solver.cpp:46] MultiStep Status: Iteration 16000, step = 1
I0718 03:22:02.503943 23375 solver.cpp:218] Iteration 16000 (0.984498 iter/s, 101.575s/100 iters), loss = 11.9011
I0718 03:22:02.504101 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:22:02.504115 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.9011 (* 1 = 11.9011 loss)
I0718 03:22:02.504127 23375 sgd_solver.cpp:46] MultiStep Status: Iteration 16000, step = 1
I0718 03:22:02.504132 23375 sgd_solver.cpp:105] Iteration 16000, lr = 0.0001
I0718 03:23:43.836830 23375 solver.cpp:218] Iteration 16100 (0.986827 iter/s, 101.335s/100 iters), loss = 11.5191
I0718 03:23:43.837067 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:23:43.837167 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.5191 (* 1 = 11.5191 loss)
I0718 03:23:43.837235 23375 sgd_solver.cpp:105] Iteration 16100, lr = 0.0001
I0718 03:25:25.219125 23375 solver.cpp:218] Iteration 16200 (0.986347 iter/s, 101.384s/100 iters), loss = 11.7713
I0718 03:25:25.219349 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:25:25.219401 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.7713 (* 1 = 11.7713 loss)
I0718 03:25:25.219434 23375 sgd_solver.cpp:105] Iteration 16200, lr = 0.0001
I0718 03:27:07.024163 23375 solver.cpp:218] Iteration 16300 (0.982251 iter/s, 101.807s/100 iters), loss = 11.9396
I0718 03:27:07.024397 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:27:07.024464 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.9396 (* 1 = 11.9396 loss)
I0718 03:27:07.024497 23375 sgd_solver.cpp:105] Iteration 16300, lr = 0.0001
I0718 03:28:48.423980 23375 solver.cpp:218] Iteration 16400 (0.986176 iter/s, 101.402s/100 iters), loss = 12.5972
I0718 03:28:48.424211 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:28:48.424264 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.5972 (* 1 = 12.5972 loss)
I0718 03:28:48.424305 23375 sgd_solver.cpp:105] Iteration 16400, lr = 0.0001
I0718 03:30:30.105623 23375 solver.cpp:218] Iteration 16500 (0.983443 iter/s, 101.684s/100 iters), loss = 12.3007
I0718 03:30:30.105859 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:30:30.105926 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.3007 (* 1 = 12.3007 loss)
I0718 03:30:30.105973 23375 sgd_solver.cpp:105] Iteration 16500, lr = 0.0001
I0718 03:32:11.550696 23375 solver.cpp:218] Iteration 16600 (0.985736 iter/s, 101.447s/100 iters), loss = 12.1697
I0718 03:32:11.550879 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:32:11.550930 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.1697 (* 1 = 12.1697 loss)
I0718 03:32:11.550992 23375 sgd_solver.cpp:105] Iteration 16600, lr = 0.0001
I0718 03:33:52.923959 23375 solver.cpp:218] Iteration 16700 (0.986434 iter/s, 101.375s/100 iters), loss = 11.5297
I0718 03:33:52.924245 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:33:52.924280 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.5297 (* 1 = 11.5297 loss)
I0718 03:33:52.924307 23375 sgd_solver.cpp:105] Iteration 16700, lr = 0.0001
I0718 03:35:34.712141 23375 solver.cpp:218] Iteration 16800 (0.982414 iter/s, 101.79s/100 iters), loss = 12.0361
I0718 03:35:34.712353 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:35:34.712373 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.0361 (* 1 = 12.0361 loss)
I0718 03:35:34.712385 23375 sgd_solver.cpp:105] Iteration 16800, lr = 0.0001
I0718 03:37:16.118378 23375 solver.cpp:218] Iteration 16900 (0.986114 iter/s, 101.408s/100 iters), loss = 12.9416
I0718 03:37:16.118590 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:37:16.118615 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.9416 (* 1 = 12.9416 loss)
I0718 03:37:16.118638 23375 sgd_solver.cpp:105] Iteration 16900, lr = 0.0001
I0718 03:38:57.707641 23375 solver.cpp:218] Iteration 17000 (0.984337 iter/s, 101.591s/100 iters), loss = 12.3086
I0718 03:38:57.707798 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:38:57.707813 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.3086 (* 1 = 12.3086 loss)
I0718 03:38:57.707828 23375 sgd_solver.cpp:105] Iteration 17000, lr = 0.0001
I0718 03:40:39.504076 23375 solver.cpp:218] Iteration 17100 (0.982333 iter/s, 101.798s/100 iters), loss = 12.5921
I0718 03:40:39.504298 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:40:39.504427 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.5921 (* 1 = 12.5921 loss)
I0718 03:40:39.504500 23375 sgd_solver.cpp:105] Iteration 17100, lr = 0.0001
I0718 03:42:21.242467 23375 solver.cpp:218] Iteration 17200 (0.982894 iter/s, 101.74s/100 iters), loss = 12.9024
I0718 03:42:21.242656 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:42:21.242682 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.9024 (* 1 = 12.9024 loss)
I0718 03:42:21.242708 23375 sgd_solver.cpp:105] Iteration 17200, lr = 0.0001
I0718 03:44:02.881615 23375 solver.cpp:218] Iteration 17300 (0.983854 iter/s, 101.641s/100 iters), loss = 12.1955
I0718 03:44:02.881870 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:44:02.881908 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.1955 (* 1 = 12.1955 loss)
I0718 03:44:02.881935 23375 sgd_solver.cpp:105] Iteration 17300, lr = 0.0001
I0718 03:45:44.614516 23375 solver.cpp:218] Iteration 17400 (0.982948 iter/s, 101.735s/100 iters), loss = 11.9262
I0718 03:45:44.614753 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:45:44.614854 23375 solver.cpp:237]     Train net output #1: softmax_loss = 11.9262 (* 1 = 11.9262 loss)
I0718 03:45:44.614928 23375 sgd_solver.cpp:105] Iteration 17400, lr = 0.0001
I0718 03:47:26.181357 23375 solver.cpp:218] Iteration 17500 (0.984554 iter/s, 101.569s/100 iters), loss = 12.3545
I0718 03:47:26.181530 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:47:26.181558 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.3545 (* 1 = 12.3545 loss)
I0718 03:47:26.181572 23375 sgd_solver.cpp:105] Iteration 17500, lr = 0.0001
I0718 03:49:07.985127 23375 solver.cpp:218] Iteration 17600 (0.982262 iter/s, 101.806s/100 iters), loss = 12.3374
I0718 03:49:07.985288 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:49:07.985311 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.3374 (* 1 = 12.3374 loss)
I0718 03:49:07.985337 23375 sgd_solver.cpp:105] Iteration 17600, lr = 0.0001
I0718 03:50:49.738795 23375 solver.cpp:218] Iteration 17700 (0.982746 iter/s, 101.756s/100 iters), loss = 13.0622
I0718 03:50:49.739045 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:50:49.739085 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.0622 (* 1 = 13.0622 loss)
I0718 03:50:49.739109 23375 sgd_solver.cpp:105] Iteration 17700, lr = 0.0001
I0718 03:52:31.356045 23375 solver.cpp:218] Iteration 17800 (0.984066 iter/s, 101.619s/100 iters), loss = 13.0866
I0718 03:52:31.356253 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:52:31.356292 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.0866 (* 1 = 13.0866 loss)
I0718 03:52:31.356307 23375 sgd_solver.cpp:105] Iteration 17800, lr = 0.0001
I0718 03:54:13.171886 23375 solver.cpp:218] Iteration 17900 (0.982146 iter/s, 101.818s/100 iters), loss = 12.2311
I0718 03:54:13.172099 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:54:13.172199 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.2311 (* 1 = 12.2311 loss)
I0718 03:54:13.172266 23375 sgd_solver.cpp:105] Iteration 17900, lr = 0.0001
I0718 03:55:55.138980 23375 solver.cpp:218] Iteration 18000 (0.980689 iter/s, 101.969s/100 iters), loss = 12.9129
I0718 03:55:55.139223 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:55:55.139283 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.9129 (* 1 = 12.9129 loss)
I0718 03:55:55.139353 23375 sgd_solver.cpp:105] Iteration 18000, lr = 0.0001
I0718 03:57:36.926277 23375 solver.cpp:218] Iteration 18100 (0.982422 iter/s, 101.789s/100 iters), loss = 13.2387
I0718 03:57:36.926478 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:57:36.926528 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.2387 (* 1 = 13.2387 loss)
I0718 03:57:36.926559 23375 sgd_solver.cpp:105] Iteration 18100, lr = 0.0001
I0718 03:59:18.695649 23375 solver.cpp:218] Iteration 18200 (0.982595 iter/s, 101.771s/100 iters), loss = 13.4404
I0718 03:59:18.695840 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 03:59:18.695952 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.4404 (* 1 = 13.4404 loss)
I0718 03:59:18.696030 23375 sgd_solver.cpp:105] Iteration 18200, lr = 0.0001
I0718 04:01:00.688779 23375 solver.cpp:218] Iteration 18300 (0.980439 iter/s, 101.995s/100 iters), loss = 13.0604
I0718 04:01:00.688961 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:01:00.688977 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.0604 (* 1 = 13.0604 loss)
I0718 04:01:00.688987 23375 sgd_solver.cpp:105] Iteration 18300, lr = 0.0001
I0718 04:02:41.645645 23375 solver.cpp:218] Iteration 18400 (0.990503 iter/s, 100.959s/100 iters), loss = 12.8835
I0718 04:02:41.645860 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:02:41.645913 23375 solver.cpp:237]     Train net output #1: softmax_loss = 12.8835 (* 1 = 12.8835 loss)
I0718 04:02:41.645946 23375 sgd_solver.cpp:105] Iteration 18400, lr = 0.0001
I0718 04:04:22.581609 23375 solver.cpp:218] Iteration 18500 (0.990708 iter/s, 100.938s/100 iters), loss = 13.5228
I0718 04:04:22.581841 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:04:22.581879 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.5228 (* 1 = 13.5228 loss)
I0718 04:04:22.581904 23375 sgd_solver.cpp:105] Iteration 18500, lr = 0.0001
I0718 04:06:03.344220 23375 solver.cpp:218] Iteration 18600 (0.992413 iter/s, 100.765s/100 iters), loss = 13.7903
I0718 04:06:03.344441 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:06:03.344509 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.7903 (* 1 = 13.7903 loss)
I0718 04:06:03.344558 23375 sgd_solver.cpp:105] Iteration 18600, lr = 0.0001
I0718 04:07:43.337277 23375 solver.cpp:218] Iteration 18700 (1.00005 iter/s, 99.995s/100 iters), loss = 13.8854
I0718 04:07:43.337550 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:07:43.337589 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.8854 (* 1 = 13.8854 loss)
I0718 04:07:43.337616 23375 sgd_solver.cpp:105] Iteration 18700, lr = 0.0001
I0718 04:09:23.478971 23375 solver.cpp:218] Iteration 18800 (0.998566 iter/s, 100.144s/100 iters), loss = 14.1654
I0718 04:09:23.479183 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:09:23.479198 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.1654 (* 1 = 14.1654 loss)
I0718 04:09:23.479209 23375 sgd_solver.cpp:105] Iteration 18800, lr = 0.0001
I0718 04:11:03.865905 23375 solver.cpp:218] Iteration 18900 (0.996126 iter/s, 100.389s/100 iters), loss = 13.4725
I0718 04:11:03.866143 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:11:03.866194 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.4725 (* 1 = 13.4725 loss)
I0718 04:11:03.866226 23375 sgd_solver.cpp:105] Iteration 18900, lr = 0.0001
I0718 04:12:43.354475 23375 solver.cpp:218] Iteration 19000 (1.00512 iter/s, 99.4905s/100 iters), loss = 13.3963
I0718 04:12:43.354707 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:12:43.354760 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.3963 (* 1 = 13.3963 loss)
I0718 04:12:43.354802 23375 sgd_solver.cpp:105] Iteration 19000, lr = 0.0001
I0718 04:14:23.426224 23375 solver.cpp:218] Iteration 19100 (0.999264 iter/s, 100.074s/100 iters), loss = 13.9681
I0718 04:14:23.426466 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:14:23.426518 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.9681 (* 1 = 13.9681 loss)
I0718 04:14:23.426558 23375 sgd_solver.cpp:105] Iteration 19100, lr = 0.0001
I0718 04:16:03.649289 23375 solver.cpp:218] Iteration 19200 (0.997755 iter/s, 100.225s/100 iters), loss = 13.8304
I0718 04:16:03.649545 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:16:03.649585 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.8304 (* 1 = 13.8304 loss)
I0718 04:16:03.649610 23375 sgd_solver.cpp:105] Iteration 19200, lr = 0.0001
I0718 04:17:43.198233 23375 solver.cpp:218] Iteration 19300 (1.00451 iter/s, 99.5508s/100 iters), loss = 13.8486
I0718 04:17:43.198426 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:17:43.198463 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.8486 (* 1 = 13.8486 loss)
I0718 04:17:43.198493 23375 sgd_solver.cpp:105] Iteration 19300, lr = 0.0001
I0718 04:19:23.259099 23375 solver.cpp:218] Iteration 19400 (0.999372 iter/s, 100.063s/100 iters), loss = 14.1039
I0718 04:19:23.259367 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:19:23.259440 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.1039 (* 1 = 14.1039 loss)
I0718 04:19:23.259487 23375 sgd_solver.cpp:105] Iteration 19400, lr = 0.0001
I0718 04:21:02.853718 23375 solver.cpp:218] Iteration 19500 (1.00405 iter/s, 99.5965s/100 iters), loss = 13.9205
I0718 04:21:02.853945 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:21:02.854017 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.9205 (* 1 = 13.9205 loss)
I0718 04:21:02.854065 23375 sgd_solver.cpp:105] Iteration 19500, lr = 0.0001
I0718 04:22:42.746265 23375 solver.cpp:218] Iteration 19600 (1.00106 iter/s, 99.8945s/100 iters), loss = 14.9087
I0718 04:22:42.746497 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:22:42.746551 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.9087 (* 1 = 14.9087 loss)
I0718 04:22:42.746592 23375 sgd_solver.cpp:105] Iteration 19600, lr = 0.0001
I0718 04:24:22.662598 23375 solver.cpp:218] Iteration 19700 (1.00082 iter/s, 99.9183s/100 iters), loss = 13.4047
I0718 04:24:22.662863 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:24:22.662915 23375 solver.cpp:237]     Train net output #1: softmax_loss = 13.4047 (* 1 = 13.4047 loss)
I0718 04:24:22.662950 23375 sgd_solver.cpp:105] Iteration 19700, lr = 0.0001
I0718 04:26:02.364008 23375 solver.cpp:218] Iteration 19800 (1.00298 iter/s, 99.7033s/100 iters), loss = 14.8873
I0718 04:26:02.364228 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:26:02.364251 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.8873 (* 1 = 14.8873 loss)
I0718 04:26:02.364269 23375 sgd_solver.cpp:105] Iteration 19800, lr = 0.0001
I0718 04:27:42.089037 23375 solver.cpp:218] Iteration 19900 (1.00274 iter/s, 99.7269s/100 iters), loss = 14.4596
I0718 04:27:42.089349 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:27:42.089398 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.4596 (* 1 = 14.4596 loss)
I0718 04:27:42.089431 23375 sgd_solver.cpp:105] Iteration 19900, lr = 0.0001
I0718 04:29:21.890444 23375 solver.cpp:218] Iteration 20000 (1.00197 iter/s, 99.8032s/100 iters), loss = 15.204
I0718 04:29:21.890668 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:29:21.890709 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.204 (* 1 = 15.204 loss)
I0718 04:29:21.890725 23375 sgd_solver.cpp:105] Iteration 20000, lr = 0.0001
I0718 04:31:01.822417 23375 solver.cpp:218] Iteration 20100 (1.00066 iter/s, 99.9339s/100 iters), loss = 14.7785
I0718 04:31:01.822662 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:31:01.822715 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.7785 (* 1 = 14.7785 loss)
I0718 04:31:01.822755 23375 sgd_solver.cpp:105] Iteration 20100, lr = 0.0001
I0718 04:32:41.817499 23375 solver.cpp:218] Iteration 20200 (1.00003 iter/s, 99.997s/100 iters), loss = 15.4901
I0718 04:32:41.817747 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:32:41.817800 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.4901 (* 1 = 15.4901 loss)
I0718 04:32:41.817834 23375 sgd_solver.cpp:105] Iteration 20200, lr = 0.0001
I0718 04:34:21.743630 23375 solver.cpp:218] Iteration 20300 (1.00072 iter/s, 99.928s/100 iters), loss = 14.8394
I0718 04:34:21.743849 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:34:21.743870 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.8394 (* 1 = 14.8394 loss)
I0718 04:34:21.743885 23375 sgd_solver.cpp:105] Iteration 20300, lr = 0.0001
I0718 04:36:01.669637 23375 solver.cpp:218] Iteration 20400 (1.00072 iter/s, 99.9279s/100 iters), loss = 14.0352
I0718 04:36:01.669893 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:36:01.669945 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.0352 (* 1 = 14.0352 loss)
I0718 04:36:01.669981 23375 sgd_solver.cpp:105] Iteration 20400, lr = 0.0001
I0718 04:37:41.330935 23375 solver.cpp:218] Iteration 20500 (1.00338 iter/s, 99.6632s/100 iters), loss = 14.8281
I0718 04:37:41.331174 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:37:41.331209 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.8281 (* 1 = 14.8281 loss)
I0718 04:37:41.331238 23375 sgd_solver.cpp:105] Iteration 20500, lr = 0.0001
I0718 04:39:20.986649 23375 solver.cpp:218] Iteration 20600 (1.00344 iter/s, 99.6576s/100 iters), loss = 15.9396
I0718 04:39:20.986860 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:39:20.986879 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.9396 (* 1 = 15.9396 loss)
I0718 04:39:20.986894 23375 sgd_solver.cpp:105] Iteration 20600, lr = 0.0001
I0718 04:41:00.709322 23375 solver.cpp:218] Iteration 20700 (1.00276 iter/s, 99.7246s/100 iters), loss = 14.935
I0718 04:41:00.709509 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:41:00.709532 23375 solver.cpp:237]     Train net output #1: softmax_loss = 14.935 (* 1 = 14.935 loss)
I0718 04:41:00.709553 23375 sgd_solver.cpp:105] Iteration 20700, lr = 0.0001
I0718 04:42:40.648712 23375 solver.cpp:218] Iteration 20800 (1.00059 iter/s, 99.9413s/100 iters), loss = 15.1079
I0718 04:42:40.648938 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:42:40.648972 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.1079 (* 1 = 15.1079 loss)
I0718 04:42:40.648999 23375 sgd_solver.cpp:105] Iteration 20800, lr = 0.0001
I0718 04:44:20.543596 23375 solver.cpp:218] Iteration 20900 (1.00103 iter/s, 99.8968s/100 iters), loss = 15.949
I0718 04:44:20.543844 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:44:20.543882 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.949 (* 1 = 15.949 loss)
I0718 04:44:20.543905 23375 sgd_solver.cpp:105] Iteration 20900, lr = 0.0001
I0718 04:46:00.340456 23375 solver.cpp:218] Iteration 21000 (1.00202 iter/s, 99.7988s/100 iters), loss = 15.5318
I0718 04:46:00.340636 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:46:00.340653 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.5318 (* 1 = 15.5318 loss)
I0718 04:46:00.340665 23375 sgd_solver.cpp:105] Iteration 21000, lr = 0.0001
I0718 04:47:40.057806 23375 solver.cpp:218] Iteration 21100 (1.00281 iter/s, 99.7193s/100 iters), loss = 16.1853
I0718 04:47:40.058023 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:47:40.058053 23375 solver.cpp:237]     Train net output #1: softmax_loss = 16.1853 (* 1 = 16.1853 loss)
I0718 04:47:40.058075 23375 sgd_solver.cpp:105] Iteration 21100, lr = 0.0001
I0718 04:49:20.149658 23375 solver.cpp:218] Iteration 21200 (0.999063 iter/s, 100.094s/100 iters), loss = 15.4122
I0718 04:49:20.156378 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:49:20.156399 23375 solver.cpp:237]     Train net output #1: softmax_loss = 15.4122 (* 1 = 15.4122 loss)
I0718 04:49:20.156412 23375 sgd_solver.cpp:105] Iteration 21200, lr = 0.0001
I0718 04:51:00.201026 23375 solver.cpp:218] Iteration 21300 (0.999532 iter/s, 100.047s/100 iters), loss = 16.8806
I0718 04:51:00.201215 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:51:00.201277 23375 solver.cpp:237]     Train net output #1: softmax_loss = 16.8806 (* 1 = 16.8806 loss)
I0718 04:51:00.201326 23375 sgd_solver.cpp:105] Iteration 21300, lr = 0.0001
I0718 04:52:39.898344 23375 solver.cpp:218] Iteration 21400 (1.00302 iter/s, 99.6993s/100 iters), loss = 16.6844
I0718 04:52:39.898587 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:52:39.898623 23375 solver.cpp:237]     Train net output #1: softmax_loss = 16.6844 (* 1 = 16.6844 loss)
I0718 04:52:39.898655 23375 sgd_solver.cpp:105] Iteration 21400, lr = 0.0001
I0718 04:54:19.660748 23375 solver.cpp:218] Iteration 21500 (1.00236 iter/s, 99.7643s/100 iters), loss = 16.895
I0718 04:54:19.660974 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:54:19.661027 23375 solver.cpp:237]     Train net output #1: softmax_loss = 16.895 (* 1 = 16.895 loss)
I0718 04:54:19.661068 23375 sgd_solver.cpp:105] Iteration 21500, lr = 0.0001
I0718 04:55:59.497836 23375 solver.cpp:218] Iteration 21600 (1.00161 iter/s, 99.839s/100 iters), loss = 17.2402
I0718 04:55:59.498066 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:55:59.498118 23375 solver.cpp:237]     Train net output #1: softmax_loss = 17.2402 (* 1 = 17.2402 loss)
I0718 04:55:59.498152 23375 sgd_solver.cpp:105] Iteration 21600, lr = 0.0001
I0718 04:57:39.620931 23375 solver.cpp:218] Iteration 21700 (0.998751 iter/s, 100.125s/100 iters), loss = 17.5965
I0718 04:57:39.621168 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:57:39.621240 23375 solver.cpp:237]     Train net output #1: softmax_loss = 17.5965 (* 1 = 17.5965 loss)
I0718 04:57:39.621289 23375 sgd_solver.cpp:105] Iteration 21700, lr = 0.0001
I0718 04:59:19.231623 23375 solver.cpp:218] Iteration 21800 (1.00389 iter/s, 99.6126s/100 iters), loss = 17.853
I0718 04:59:19.231856 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 04:59:19.231904 23375 solver.cpp:237]     Train net output #1: softmax_loss = 17.853 (* 1 = 17.853 loss)
I0718 04:59:19.231927 23375 sgd_solver.cpp:105] Iteration 21800, lr = 0.0001
I0718 05:00:59.481902 23375 solver.cpp:218] Iteration 21900 (0.997484 iter/s, 100.252s/100 iters), loss = 18.0341
I0718 05:00:59.482137 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:00:59.482177 23375 solver.cpp:237]     Train net output #1: softmax_loss = 18.0341 (* 1 = 18.0341 loss)
I0718 05:00:59.482203 23375 sgd_solver.cpp:105] Iteration 21900, lr = 0.0001
I0718 05:02:39.687793 23375 solver.cpp:218] Iteration 22000 (0.997926 iter/s, 100.208s/100 iters), loss = 17.6992
I0718 05:02:39.688107 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:02:39.688144 23375 solver.cpp:237]     Train net output #1: softmax_loss = 17.6992 (* 1 = 17.6992 loss)
I0718 05:02:39.688170 23375 sgd_solver.cpp:105] Iteration 22000, lr = 0.0001
I0718 05:04:19.452347 23375 solver.cpp:218] Iteration 22100 (1.00234 iter/s, 99.7664s/100 iters), loss = 18.5267
I0718 05:04:19.452574 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:04:19.452603 23375 solver.cpp:237]     Train net output #1: softmax_loss = 18.5267 (* 1 = 18.5267 loss)
I0718 05:04:19.452625 23375 sgd_solver.cpp:105] Iteration 22100, lr = 0.0001
I0718 05:05:59.736675 23375 solver.cpp:218] Iteration 22200 (0.997146 iter/s, 100.286s/100 iters), loss = 18.9672
I0718 05:05:59.736934 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:05:59.736973 23375 solver.cpp:237]     Train net output #1: softmax_loss = 18.9672 (* 1 = 18.9672 loss)
I0718 05:05:59.737002 23375 sgd_solver.cpp:105] Iteration 22200, lr = 0.0001
I0718 05:07:39.994922 23375 solver.cpp:218] Iteration 22300 (0.997405 iter/s, 100.26s/100 iters), loss = 19.805
I0718 05:07:39.995091 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:07:39.995112 23375 solver.cpp:237]     Train net output #1: softmax_loss = 19.805 (* 1 = 19.805 loss)
I0718 05:07:39.995126 23375 sgd_solver.cpp:105] Iteration 22300, lr = 0.0001
I0718 05:09:20.517047 23375 solver.cpp:218] Iteration 22400 (0.994786 iter/s, 100.524s/100 iters), loss = 20.6046
I0718 05:09:20.517282 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:09:20.517307 23375 solver.cpp:237]     Train net output #1: softmax_loss = 20.6046 (* 1 = 20.6046 loss)
I0718 05:09:20.517323 23375 sgd_solver.cpp:105] Iteration 22400, lr = 0.0001
I0718 05:11:00.380619 23375 solver.cpp:218] Iteration 22500 (1.00135 iter/s, 99.8655s/100 iters), loss = 19.7292
I0718 05:11:00.380854 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:11:00.380898 23375 solver.cpp:237]     Train net output #1: softmax_loss = 19.7292 (* 1 = 19.7292 loss)
I0718 05:11:00.380934 23375 sgd_solver.cpp:105] Iteration 22500, lr = 0.0001
I0718 05:12:40.391530 23375 solver.cpp:218] Iteration 22600 (0.999872 iter/s, 100.013s/100 iters), loss = 20.9109
I0718 05:12:40.391777 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:12:40.391844 23375 solver.cpp:237]     Train net output #1: softmax_loss = 20.9109 (* 1 = 20.9109 loss)
I0718 05:12:40.391891 23375 sgd_solver.cpp:105] Iteration 22600, lr = 0.0001
I0718 05:14:20.802080 23375 solver.cpp:218] Iteration 22700 (0.995892 iter/s, 100.412s/100 iters), loss = 21.101
I0718 05:14:20.802263 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:14:20.802278 23375 solver.cpp:237]     Train net output #1: softmax_loss = 21.101 (* 1 = 21.101 loss)
I0718 05:14:20.802289 23375 sgd_solver.cpp:105] Iteration 22700, lr = 0.0001
I0718 05:16:01.125777 23375 solver.cpp:218] Iteration 22800 (0.996754 iter/s, 100.326s/100 iters), loss = 22.0934
I0718 05:16:01.125977 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:16:01.126011 23375 solver.cpp:237]     Train net output #1: softmax_loss = 22.0934 (* 1 = 22.0934 loss)
I0718 05:16:01.126039 23375 sgd_solver.cpp:105] Iteration 22800, lr = 0.0001
I0718 05:17:41.214746 23375 solver.cpp:218] Iteration 22900 (0.999092 iter/s, 100.091s/100 iters), loss = 22.1792
I0718 05:17:41.214992 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:17:41.215029 23375 solver.cpp:237]     Train net output #1: softmax_loss = 22.1792 (* 1 = 22.1792 loss)
I0718 05:17:41.215055 23375 sgd_solver.cpp:105] Iteration 22900, lr = 0.0001
I0718 05:19:21.709095 23375 solver.cpp:218] Iteration 23000 (0.995062 iter/s, 100.496s/100 iters), loss = 25.9184
I0718 05:19:21.709283 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:19:21.709300 23375 solver.cpp:237]     Train net output #1: softmax_loss = 25.9184 (* 1 = 25.9184 loss)
I0718 05:19:21.709312 23375 sgd_solver.cpp:105] Iteration 23000, lr = 0.0001
I0718 05:21:01.723312 23375 solver.cpp:218] Iteration 23100 (0.999838 iter/s, 100.016s/100 iters), loss = 27.9238
I0718 05:21:01.723593 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:21:01.723645 23375 solver.cpp:237]     Train net output #1: softmax_loss = 27.9238 (* 1 = 27.9238 loss)
I0718 05:21:01.723678 23375 sgd_solver.cpp:105] Iteration 23100, lr = 0.0001
I0718 05:22:41.868067 23375 solver.cpp:218] Iteration 23200 (0.998536 iter/s, 100.147s/100 iters), loss = 28.737
I0718 05:22:41.868259 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:22:41.868332 23375 solver.cpp:237]     Train net output #1: softmax_loss = 28.737 (* 1 = 28.737 loss)
I0718 05:22:41.868366 23375 sgd_solver.cpp:105] Iteration 23200, lr = 0.0001
I0718 05:24:22.075830 23375 solver.cpp:218] Iteration 23300 (0.997907 iter/s, 100.21s/100 iters), loss = 31.4656
I0718 05:24:22.076073 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:24:22.076125 23375 solver.cpp:237]     Train net output #1: softmax_loss = 31.4656 (* 1 = 31.4656 loss)
I0718 05:24:22.076159 23375 sgd_solver.cpp:105] Iteration 23300, lr = 0.0001
I0718 05:26:01.970469 23375 solver.cpp:218] Iteration 23400 (1.00104 iter/s, 99.8965s/100 iters), loss = 35.8096
I0718 05:26:01.970715 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:26:01.970777 23375 solver.cpp:237]     Train net output #1: softmax_loss = 35.8096 (* 1 = 35.8096 loss)
I0718 05:26:01.970809 23375 sgd_solver.cpp:105] Iteration 23400, lr = 0.0001
I0718 05:27:41.934793 23375 solver.cpp:218] Iteration 23500 (1.00034 iter/s, 99.9662s/100 iters), loss = 38.7695
I0718 05:27:41.935073 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:27:41.935127 23375 solver.cpp:237]     Train net output #1: softmax_loss = 38.7695 (* 1 = 38.7695 loss)
I0718 05:27:41.935158 23375 sgd_solver.cpp:105] Iteration 23500, lr = 0.0001
I0718 05:29:22.286622 23375 solver.cpp:218] Iteration 23600 (0.996475 iter/s, 100.354s/100 iters), loss = 43.6644
I0718 05:29:22.286840 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:29:22.286864 23375 solver.cpp:237]     Train net output #1: softmax_loss = 43.6644 (* 1 = 43.6644 loss)
I0718 05:29:22.286880 23375 sgd_solver.cpp:105] Iteration 23600, lr = 0.0001
I0718 05:31:02.214758 23375 solver.cpp:218] Iteration 23700 (1.0007 iter/s, 99.9301s/100 iters), loss = 47.6553
I0718 05:31:02.214937 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:31:02.214952 23375 solver.cpp:237]     Train net output #1: softmax_loss = 47.6553 (* 1 = 47.6553 loss)
I0718 05:31:02.214964 23375 sgd_solver.cpp:105] Iteration 23700, lr = 0.0001
I0718 05:32:42.143663 23375 solver.cpp:218] Iteration 23800 (1.00069 iter/s, 99.9309s/100 iters), loss = 56.6737
I0718 05:32:42.143847 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:32:42.143882 23375 solver.cpp:237]     Train net output #1: softmax_loss = 56.6737 (* 1 = 56.6737 loss)
I0718 05:32:42.143908 23375 sgd_solver.cpp:105] Iteration 23800, lr = 0.0001
I0718 05:34:22.459383 23375 solver.cpp:218] Iteration 23900 (0.996833 iter/s, 100.318s/100 iters), loss = 61.9464
I0718 05:34:22.459596 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:34:22.459635 23375 solver.cpp:237]     Train net output #1: softmax_loss = 61.9464 (* 1 = 61.9464 loss)
I0718 05:34:22.459663 23375 sgd_solver.cpp:105] Iteration 23900, lr = 0.0001
I0718 05:36:02.544474 23382 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 2
I0718 05:36:02.544461 23375 solver.cpp:218] Iteration 24000 (0.999131 iter/s, 100.087s/100 iters), loss = 68.1618
I0718 05:36:02.544657 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:36:02.544685 23375 solver.cpp:237]     Train net output #1: softmax_loss = 68.1618 (* 1 = 68.1618 loss)
I0718 05:36:02.544705 23375 sgd_solver.cpp:46] MultiStep Status: Iteration 24000, step = 2
I0718 05:36:02.544715 23375 sgd_solver.cpp:105] Iteration 24000, lr = 1e-05
I0718 05:37:42.786944 23375 solver.cpp:218] Iteration 24100 (0.997562 iter/s, 100.244s/100 iters), loss = 59.9943
I0718 05:37:42.787235 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:37:42.787271 23375 solver.cpp:237]     Train net output #1: softmax_loss = 59.9943 (* 1 = 59.9943 loss)
I0718 05:37:42.787299 23375 sgd_solver.cpp:105] Iteration 24100, lr = 1e-05
I0718 05:39:22.856284 23375 solver.cpp:218] Iteration 24200 (0.999288 iter/s, 100.071s/100 iters), loss = 64.8688
I0718 05:39:22.856539 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:39:22.856591 23375 solver.cpp:237]     Train net output #1: softmax_loss = 64.8688 (* 1 = 64.8688 loss)
I0718 05:39:22.856626 23375 sgd_solver.cpp:105] Iteration 24200, lr = 1e-05
I0718 05:41:02.379397 23375 solver.cpp:218] Iteration 24300 (1.00477 iter/s, 99.525s/100 iters), loss = 68.2187
I0718 05:41:02.379653 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:41:02.379688 23375 solver.cpp:237]     Train net output #1: softmax_loss = 68.2187 (* 1 = 68.2187 loss)
I0718 05:41:02.379712 23375 sgd_solver.cpp:105] Iteration 24300, lr = 1e-05
I0718 05:42:41.885717 23375 solver.cpp:218] Iteration 24400 (1.00494 iter/s, 99.5082s/100 iters), loss = 69.9827
I0718 05:42:41.885949 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:42:41.886003 23375 solver.cpp:237]     Train net output #1: softmax_loss = 69.9827 (* 1 = 69.9827 loss)
I0718 05:42:41.886035 23375 sgd_solver.cpp:105] Iteration 24400, lr = 1e-05
I0718 05:44:21.087523 23375 solver.cpp:218] Iteration 24500 (1.00803 iter/s, 99.2037s/100 iters), loss = 77.629
I0718 05:44:21.087703 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:44:21.087731 23375 solver.cpp:237]     Train net output #1: softmax_loss = 77.629 (* 1 = 77.629 loss)
I0718 05:44:21.087754 23375 sgd_solver.cpp:105] Iteration 24500, lr = 1e-05
I0718 05:46:00.381355 23375 solver.cpp:218] Iteration 24600 (1.00709 iter/s, 99.2958s/100 iters), loss = 82.4393
I0718 05:46:00.381531 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:46:00.381548 23375 solver.cpp:237]     Train net output #1: softmax_loss = 82.4393 (* 1 = 82.4393 loss)
I0718 05:46:00.381563 23375 sgd_solver.cpp:105] Iteration 24600, lr = 1e-05
I0718 05:47:40.185420 23375 solver.cpp:218] Iteration 24700 (1.00194 iter/s, 99.806s/100 iters), loss = 82.2192
I0718 05:47:40.185647 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:47:40.185699 23375 solver.cpp:237]     Train net output #1: softmax_loss = 82.2192 (* 1 = 82.2192 loss)
I0718 05:47:40.185734 23375 sgd_solver.cpp:105] Iteration 24700, lr = 1e-05
I0718 05:49:19.170284 23375 solver.cpp:218] Iteration 24800 (1.01024 iter/s, 98.9868s/100 iters), loss = 85.4827
I0718 05:49:19.170536 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:49:19.170604 23375 solver.cpp:237]     Train net output #1: softmax_loss = 85.4827 (* 1 = 85.4827 loss)
I0718 05:49:19.170655 23375 sgd_solver.cpp:105] Iteration 24800, lr = 1e-05
I0718 05:50:58.400024 23375 solver.cpp:218] Iteration 24900 (1.00774 iter/s, 99.2316s/100 iters), loss = 82.8186
I0718 05:50:58.400240 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:50:58.400292 23375 solver.cpp:237]     Train net output #1: softmax_loss = 82.8186 (* 1 = 82.8186 loss)
I0718 05:50:58.400352 23375 sgd_solver.cpp:105] Iteration 24900, lr = 1e-05
I0718 05:52:37.344985 23375 solver.cpp:218] Iteration 25000 (1.01064 iter/s, 98.9469s/100 iters), loss = 79.2832
I0718 05:52:37.345221 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:52:37.345263 23375 solver.cpp:237]     Train net output #1: softmax_loss = 79.2832 (* 1 = 79.2832 loss)
I0718 05:52:37.345290 23375 sgd_solver.cpp:105] Iteration 25000, lr = 1e-05
I0718 05:54:16.702543 23375 solver.cpp:218] Iteration 25100 (1.00645 iter/s, 99.3595s/100 iters), loss = 87.3365
I0718 05:54:16.702791 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:54:16.702827 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 05:54:16.702858 23375 sgd_solver.cpp:105] Iteration 25100, lr = 1e-05
I0718 05:55:39.561513 23375 solver.cpp:218] Iteration 25200 (1.20685 iter/s, 82.8605s/100 iters), loss = 87.3365
I0718 05:55:39.561780 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:55:39.561841 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 05:55:39.561874 23375 sgd_solver.cpp:105] Iteration 25200, lr = 1e-05
I0718 05:57:01.939848 23375 solver.cpp:218] Iteration 25300 (1.21389 iter/s, 82.3798s/100 iters), loss = 87.3365
I0718 05:57:01.940047 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:57:01.940065 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 05:57:01.940079 23375 sgd_solver.cpp:105] Iteration 25300, lr = 1e-05
I0718 05:58:24.279306 23375 solver.cpp:218] Iteration 25400 (1.21446 iter/s, 82.341s/100 iters), loss = 87.3365
I0718 05:58:24.279542 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:58:24.279594 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 05:58:24.279628 23375 sgd_solver.cpp:105] Iteration 25400, lr = 1e-05
I0718 05:59:46.595676 23375 solver.cpp:218] Iteration 25500 (1.2148 iter/s, 82.3179s/100 iters), loss = 87.3365
I0718 05:59:46.595825 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 05:59:46.595840 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 05:59:46.595852 23375 sgd_solver.cpp:105] Iteration 25500, lr = 1e-05
I0718 06:01:08.970805 23375 solver.cpp:218] Iteration 25600 (1.21394 iter/s, 82.3767s/100 iters), loss = 87.3365
I0718 06:01:08.971032 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:01:08.971081 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:01:08.971104 23375 sgd_solver.cpp:105] Iteration 25600, lr = 1e-05
I0718 06:02:31.289988 23375 solver.cpp:218] Iteration 25700 (1.21476 iter/s, 82.3207s/100 iters), loss = 87.3365
I0718 06:02:31.290182 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:02:31.290231 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:02:31.290269 23375 sgd_solver.cpp:105] Iteration 25700, lr = 1e-05
I0718 06:03:53.523237 23375 solver.cpp:218] Iteration 25800 (1.21603 iter/s, 82.2348s/100 iters), loss = 87.3365
I0718 06:03:53.523470 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:03:53.523505 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:03:53.523528 23375 sgd_solver.cpp:105] Iteration 25800, lr = 1e-05
I0718 06:05:15.849563 23375 solver.cpp:218] Iteration 25900 (1.21466 iter/s, 82.3278s/100 iters), loss = 87.3365
I0718 06:05:15.849723 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:05:15.849736 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:05:15.849758 23375 sgd_solver.cpp:105] Iteration 25900, lr = 1e-05
I0718 06:06:38.125905 23375 solver.cpp:218] Iteration 26000 (1.21539 iter/s, 82.2779s/100 iters), loss = 87.3365
I0718 06:06:38.126121 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:06:38.126163 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:06:38.126199 23375 sgd_solver.cpp:105] Iteration 26000, lr = 1e-05
I0718 06:08:00.496372 23375 solver.cpp:218] Iteration 26100 (1.214 iter/s, 82.372s/100 iters), loss = 87.3365
I0718 06:08:00.496546 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:08:00.496568 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:08:00.496579 23375 sgd_solver.cpp:105] Iteration 26100, lr = 1e-05
I0718 06:09:22.831267 23375 solver.cpp:218] Iteration 26200 (1.21453 iter/s, 82.3364s/100 iters), loss = 87.3365
I0718 06:09:22.831477 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:09:22.831518 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:09:22.831547 23375 sgd_solver.cpp:105] Iteration 26200, lr = 1e-05
I0718 06:10:45.098703 23375 solver.cpp:218] Iteration 26300 (1.21553 iter/s, 82.269s/100 iters), loss = 87.3365
I0718 06:10:45.098937 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:10:45.098960 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:10:45.098975 23375 sgd_solver.cpp:105] Iteration 26300, lr = 1e-05
I0718 06:12:07.472759 23375 solver.cpp:218] Iteration 26400 (1.21395 iter/s, 82.3756s/100 iters), loss = 87.3365
I0718 06:12:07.472980 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:12:07.473023 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:12:07.473049 23375 sgd_solver.cpp:105] Iteration 26400, lr = 1e-05
I0718 06:13:29.755089 23375 solver.cpp:218] Iteration 26500 (1.21531 iter/s, 82.2838s/100 iters), loss = 87.3365
I0718 06:13:29.755290 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:13:29.755340 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:13:29.755373 23375 sgd_solver.cpp:105] Iteration 26500, lr = 1e-05
I0718 06:14:52.034380 23375 solver.cpp:218] Iteration 26600 (1.21535 iter/s, 82.2808s/100 iters), loss = 87.3365
I0718 06:14:52.034575 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:14:52.034623 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:14:52.034652 23375 sgd_solver.cpp:105] Iteration 26600, lr = 1e-05
I0718 06:16:14.429927 23375 solver.cpp:218] Iteration 26700 (1.21363 iter/s, 82.3971s/100 iters), loss = 87.3365
I0718 06:16:14.430156 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:16:14.430191 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:16:14.430219 23375 sgd_solver.cpp:105] Iteration 26700, lr = 1e-05
I0718 06:17:36.700991 23375 solver.cpp:218] Iteration 26800 (1.21547 iter/s, 82.2726s/100 iters), loss = 87.3365
I0718 06:17:36.701217 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:17:36.701267 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:17:36.701297 23375 sgd_solver.cpp:105] Iteration 26800, lr = 1e-05
I0718 06:18:58.985504 23375 solver.cpp:218] Iteration 26900 (1.21527 iter/s, 82.286s/100 iters), loss = 87.3365
I0718 06:18:58.985733 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:18:58.985786 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:18:58.985828 23375 sgd_solver.cpp:105] Iteration 26900, lr = 1e-05
I0718 06:20:21.300926 23375 solver.cpp:218] Iteration 27000 (1.21482 iter/s, 82.3169s/100 iters), loss = 87.3365
I0718 06:20:21.301146 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:20:21.301185 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:20:21.301211 23375 sgd_solver.cpp:105] Iteration 27000, lr = 1e-05
I0718 06:21:43.529839 23375 solver.cpp:218] Iteration 27100 (1.21609 iter/s, 82.2304s/100 iters), loss = 87.3365
I0718 06:21:43.530045 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:21:43.530097 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:21:43.530133 23375 sgd_solver.cpp:105] Iteration 27100, lr = 1e-05
I0718 06:23:05.944376 23375 solver.cpp:218] Iteration 27200 (1.21336 iter/s, 82.4161s/100 iters), loss = 87.3365
I0718 06:23:05.944566 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:23:05.944604 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:23:05.944628 23375 sgd_solver.cpp:105] Iteration 27200, lr = 1e-05
I0718 06:24:28.162952 23375 solver.cpp:218] Iteration 27300 (1.21625 iter/s, 82.2201s/100 iters), loss = 87.3365
I0718 06:24:28.163229 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:24:28.163264 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:24:28.163288 23375 sgd_solver.cpp:105] Iteration 27300, lr = 1e-05
I0718 06:25:50.370781 23375 solver.cpp:218] Iteration 27400 (1.21641 iter/s, 82.2093s/100 iters), loss = 87.3365
I0718 06:25:50.370991 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:25:50.371028 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:25:50.371053 23375 sgd_solver.cpp:105] Iteration 27400, lr = 1e-05
I0718 06:27:12.708086 23375 solver.cpp:218] Iteration 27500 (1.21449 iter/s, 82.3388s/100 iters), loss = 87.3365
I0718 06:27:12.708300 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:27:12.708359 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:27:12.708389 23375 sgd_solver.cpp:105] Iteration 27500, lr = 1e-05
I0718 06:28:34.971388 23375 solver.cpp:218] Iteration 27600 (1.21559 iter/s, 82.2648s/100 iters), loss = 87.3365
I0718 06:28:34.971621 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:28:34.971657 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:28:34.971693 23375 sgd_solver.cpp:105] Iteration 27600, lr = 1e-05
I0718 06:29:57.238597 23375 solver.cpp:218] Iteration 27700 (1.21553 iter/s, 82.2687s/100 iters), loss = 87.3365
I0718 06:29:57.238765 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:29:57.238781 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:29:57.238793 23375 sgd_solver.cpp:105] Iteration 27700, lr = 1e-05
I0718 06:31:19.551584 23375 solver.cpp:218] Iteration 27800 (1.21485 iter/s, 82.3145s/100 iters), loss = 87.3365
I0718 06:31:19.551820 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:31:19.551858 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:31:19.551884 23375 sgd_solver.cpp:105] Iteration 27800, lr = 1e-05
I0718 06:32:41.778841 23375 solver.cpp:218] Iteration 27900 (1.21612 iter/s, 82.2288s/100 iters), loss = 87.3365
I0718 06:32:41.779072 23375 solver.cpp:237]     Train net output #0: lambda = 5
I0718 06:32:41.779124 23375 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0718 06:32:41.779165 23375 sgd_solver.cpp:105] Iteration 27900, lr = 1e-05
I0718 06:34:03.251044 23375 solver.cpp:447] Snapshotting to binary proto file result/sphereface_model_iter_28000.caffemodel
I0718 06:34:03.783634 23375 sgd_solver.cpp:273] Snapshotting solver state to binary proto file result/sphereface_model_iter_28000.solverstate
I0718 06:34:04.175076 23375 solver.cpp:310] Iteration 28000, loss = 87.3365
I0718 06:34:04.175137 23375 solver.cpp:315] Optimization Done.
I0718 06:34:04.322540 23375 caffe.cpp:259] Optimization Done.

`"
I found that SphereFace network removed batch normalization layer? Why?
""
""
"This issue is not meant to degrade your work. I may have done something wrong in my test and I need some help in verifying if my test procedure is right. I have verified my code many times and I couldn't find any error.

The dataset is: http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/. I used your face_detect_demo.m and face_align_demo.m to detect and align faces in vggface2_train (8631 people) and vggface2_test (500 people). 

Following megaface procedure with slight modification, I make vggface2_train the test set and vggface2_test the distractor. 

For each identity in test set, I compute the max intraclass distance of one photo (reference) with the rest of the photos. Then, I compute the min interclass distance of this reference with the rest of the photos in both the test set and the distractor, excluding photos of the identity being tested.

A good face recognition algorithm should have min interclass distance > max intraclass distance. Each image that has this property is considered correct.

Let the number of correct photo be C. Then, if there are N total photos in the dataset, the accuracy will be C/N.

Although nobody has done such test for face recognition, I believe this is the most rigorous, stringent and robust testing method.

The code for my test is here: https://gist.github.com/yxchng/dec1ec6fe306082684af70d85f2590e2.
I save each features into a text file and read them in the code. You may do it your way if you want to test it. Hopefully, you can check that there is no logic error in my code.

**The result I get for sphereface is a mind blowing 0%, which means there are always cases where the model misrecognize for each identity. Do you think this result is expected given my test setting? What am I doing wrong here. caffeface also give same result. If there is nothing wrong with my test, it is amazing that face recognition fails even with just 9131 people.**

"
"I retrained the SpherefaceNet-20 from scratch but the performance on LFW cannt reach 99.30%.
Evaluation log is as below:
```
fold	ACC
----------------
1	98.83%
2	98.83%
3	98.50%
4	99.17%
5	99.00%
6	99.50%
7	99.17%
8	99.50%
9	99.83%
10	99.33%
----------------
AVE	99.17%
```


My training enrironments are:
* network: **SpherefaceNet-20**;
* Training data: CASIA-WebFace, landmarks are obtained via MTCNN (actually I downloaded cropped images from the web, this may be an uncertain factor.)
* GPUs: I use **2 GPUs** and each one processes 256 samples at a single iteration.
* LFW: I download the LFW dataset from its official website and use your provided preprocessing code to process them.

All other seetings are kept the same with your default configurations prototx files, the only thing that
may be not clear is the number of GPUs

There are two suspicious factors that may cause the failure:
1. the actual batch-size. Since I use 2 GPUs x 256 batch-size
2. the training data. I downloaded the cropped images from the web.

Here is my training log <http://data.kaiz.xyz/log/retrain_sphereface_June2-2018.log>.
___
Additionally, with the released pretrain model `sphereface_model.caffemodel` I only obtain the average accuracy of 99.27. This may be a minor problem and it has been mentioned in <https://github.com/wy1iu/sphereface/issues/93>."
"Hey wy1iu,
We are trying evaluate your model, but the performance is not as good as frontal non occluded faces with faces with glasses or partial side faces. Any suggestions to improve the same would be helpful. "
"
![image](https://user-images.githubusercontent.com/26598440/40870477-efd0d390-6662-11e8-94bb-817bf55ff677.png)

I think that the lambda is used as a tradeoff between the original softmax and A-softmax.
But how could I adjust the base, power, lambda_min according to different task?
Could you give me some empirical suggestion?"
"I use the same prototxt  as yours, and adjuse the batch size to 128.
Thanks."
"

```
fold	ACC
----------------
1	99.33%
2	99.33%
3	99.00%
4	99.50%
5	98.67%
6	99.33%
7	99.17%
8	99.00%
9	100.00%
10	99.33%
----------------
AVE	99.27%
```

This is the result I get by running your code and model. You put the released model as model 3 which you mentioned in the repo to have the result of 99.3%. Is it a mistake?

![screenshot from 2018-05-29 14-46-59](https://user-images.githubusercontent.com/10518587/40642263-24a1aaa2-634f-11e8-8977-ad696c4b4f61.png)

Also what you mean by ""going through the pipeline 5 times""? Is it that you train 5 times and get 5 models?
"
"Im wondering if ur using washed CASIA as training data.
if  yes, can u provide the washed list of CASIA?  I can't find any valid link to download it.
Thanks! "
"I followed the procedure given by the readme file. When running run ~/sphereface/test/code/evaluation.m, an error occurred:
Error using CHECK (line 4)
invalid Net handle

Error in caffe.Net (line 36)
      CHECK(is_valid_handle(hNet_net), 'invalid Net handle');

Error in caffe.get_net (line 28)
net = caffe.Net(hNet);

Error in caffe.Net (line 31)
        self = caffe.get_net(varargin{:});

Error in evaluation (line 35)
net     = caffe.Net(model, weights, 'test');

Error in run (line 96)
evalin('caller', [script ';']);

Can anyone help to solve this problem?"
"What‘s your accurate in your casia-webface dataset, when your lfw accuracy is 99.42? What do you think is the relationship between the two?thanks."
"Hey wy1iu,
I am trying to find if two images(custom) are of the same person or not. I tried using the code. I am not quite sure on how it works. Can you please help me in this. I ran evaluation for LFW dataset, that works fine. I tried providing my own pairs.txt and the images data. "
"Hi, @wy1iu
     Thanks for great job~ Will weights of sphereNet-64 be released?"
"hi wy1iu,
       I have gone through the repository and found that the license type used in this repository is ""MIT"".
1.Is the model file also covered under MIT license? 
2.With your knowledge, any idea about the licensing of datasets you used? "
""
"I have tested SpherefaceNet-04 and SpherefaceNet-06 with and without BatchNorm.

Training on WebFace,
SpherefaceNet-04 WITHOUT BN can easily achieve ~98.00% (the paper reports 98.2%),
SpherefaceNet-04 WITH BN        can easily achieve ~98.20% (fine-tuning 98.35%),
SpherefaceNet-06 WITHOUT BN can easily achieve ~98.50% (the paper doesnot give),
SpherefaceNet-06 WITH BN        can easily achieve ~98.70% (fine-tuning 98.80%).

It should be noted that SpherefaceNet-04's forward only costs ~29ms (with BN ~33ms) and SpherefaceNet-06 costs ~49ms (with BN ~55ms), tested on E5-1650V4 3.6GHz, mini-caffe, openblas single thread. 

The attachments are the nets with BN.


[spherefacenet-04-06.zip](https://github.com/wy1iu/sphereface/files/1926231/spherefacenet-04-06.zip)
"
"At the beginning, the loss is about 9.x, After training 28,000 times, how much is the loss value. My loss value is approximately 4.x, the accuracy is 91%, and I try to reduce lr, but the loss also didn't ruduce "
"Why in MarginInnerProductParameter_MarginType_DOUBLE,  _TRIPLE,  _QUADRUPLE, there are following codes:
Dtype coeff_norm = sqrt(coeff_w * coeff_w + coeff_x * coeff_x);

            coeff_w = coeff_w / coeff_norm;

            coeff_x = coeff_x / coeff_norm;
Why do add the three lines codes, what is mean?
"
"at the beginning，i use m  as  “type：SINGLE”，and  the loss converge，acc can is about  0.98.
but  when i  change the  m as “type：QUADRUPLE”，the  loss become  larger and larger，and acc come down also。
  so  how to change  the type  correctly？
   hope  the  help"
"@wy1iu 
hi，
  1 have two problems：
   1）when i train the model  with “type: SINGLE”，the loss can not converge，it is always 9.xx
    2）then  i use the model trained  by 1），to  train  a  new  model，but  the loss become  bigger and bigger
   i  don't know why，hope your help。
  my sovlver：
base_lr: 0.1
momentum: 0.9
lr_policy: ""multistep""
stepvalue: 16000
stepvalue: 24000
stepvalue: 28000
gamma: 0.1
weight_decay:0.0005
display: 100
max_iter: 28000
snapshot: 5000
snapshot_prefix: ""snapshot/""
solver_mode: GPU

my dataset  is 10572 person，total 24308 pictures。its my own data，not the CASIA-WebFace data。


  thx！ "
i want to train it
"I tried to train MNIST dataset only with 0-7, and the resulting feature space looks good for 0-7.
however, when I test the network with 8 and 9, they are not discriminative any more. 
So I am wondering if this method can generate to new classes?




![figure_1](https://user-images.githubusercontent.com/5336719/36944278-9e9834d0-1fd3-11e8-83d8-8dfadbd6b7cb.png)


Basically, the problem I am facing is that today I want to verify 100 classes (but not necessary face), and tomorrow I will have to verify 105 classes, and next day more. I would like not to train the network everyday. So can I use method like in this paper to avoid retraining?

"
"First of all, thanks for sharing the code.
I have trained a model with accuracy 99.27% following the instructions. However, when I use it in C++ programs, I find there is slight difference between C++ and matlab. Loading the same 96*112 cropped image (even .bmp format), the extracted features are not exactly the same between C++ and matlab.
I have written a C++ code following the matlab code `test\code\evaluation.m', and the ACC is 99.08%.
Though this accuracy is also acceptable, I wonder why this happens.

code [code.zip](https://github.com/wy1iu/sphereface/files/1762010/code.zip)
"
"i have a question about this program https://github.com/HaoLiuHust/sphereface_mxnet_gluon/blob/master/net_sphere.py 
in your project , the network used above is different from your Caffe2 achievement and the Visualizations of network architecture you afford here
http://ethereon.github.io/netscope/#/gist/20f6ddf70a35dec5019a539a502bccc5
the skip location in BasicBlock2 is after a convolution ,is different. Is this a adjustment in MXNet  ,or something wrong
"
"Thank you for he sphereface code!
     but I don't know how to get to get the coord5point in face_align_demo.m ."
"Hi 

it seems the matlab script to produce the video demo is missing. Can anyone guide me to it?

I would appreciate it."
"Hi, 
   Can you tell us the way to do 3-path ensemble?  Thanks"
"I am using the sphereface code implemented by pytorch in the third-party re-implementation lists.
After 20 epochs, I only get `93.3490%` accuracy on training dataset. Does this correct?
This model get  `98.85%` accuracy on lfw."
"my solver ：
base_lr: 0.01
lr_policy: ""multistep""
gamma: 0.1

stepvalue: 16000
stepvalue: 24000
stepvalue: 28000
max_iter: 28000

display: 100
momentum: 0.9
weight_decay: 0.0005
#snapshot: 1000
snapshot_prefix: ""result/sphereface_model_old""
"
"Thanks for your contribution.
But I have a doubt that before feeding the testset into the model we trained already, should we first detect faces in the images of lfw?In order to make the embeddings contain only the part of face which is more pure?"
"Hi,
Thank you for sharing your implementation.

I am new to Caffe so I may have misunderstood, but it would be great if you can kindly clarify this:

In the Large margin paper you mentioned:
`f_yi =[lambda * Wn_yi * xn_i * cost_yi + Wn_yi * xn_i *psi(t_yi)]/(1+lambda) `
Where Wn, xn are normalized forms of W and x and Wn = 1 (for sphereface)

But in the code below,
I observed (for m= 2, lets say)
On line 192 
`top_data = XW^t_forall_i`
On line 207 update for yi
`top_data += (XW^t)_iff_i==yi`
But on line 211 again you do:
`top_data += lambda *XW^t`
and then `top_data /=(1+lambda)`

According to this implementation
`f_yi =[Wn_i * xn_i * cost_i + lambda * Wn_yi * xn_i * cost_yi + Wn_yi * xn_i *psi(t_yi)]/(1+lambda) `
and 
`f_i =[Wn_i * xn_i * cost_i + lambda * Wn_i * xn_i * cost_i]/(1+lambda)
=> f_i = [Wn_i * xn_i * cost_i] `

Even if gradually `lamdba` becomes zero, f_yi still receives `Wn_i * xn_i * cost_i`. 
Although I understand if predictions are correction then this contribution will be negligible, but wouldnt this impact the misclassified ones?
Can you please explain why it is so? or am I missing something.

https://github.com/wy1iu/sphereface/blob/master/tools/caffe-sphereface/src/caffe/layers/margin_inner_product_layer.cpp#L192
`  caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasTrans, M_, N_, K_, (Dtype)1.,
      bottom_data, weight, (Dtype)0., top_data);`

And again:
https://github.com/wy1iu/sphereface/blob/master/tools/caffe-sphereface/src/caffe/layers/margin_inner_product_layer.cpp#L211
`caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasTrans, M_, N_, K_, lambda_,
      bottom_data, weight, (Dtype)1., top_data);`"
"Thanks for your good jobs!
And what's the difference between official caffe and your caffe-sphereface?"
I was wondering whether it is possible for you to provide us with the script which you used to produce the video demo in the repository?
""
How did you choose this number? why not squares? Thank you very much.
"I test the 20 layers sphereface network on my own database,then test the features on tSNE  that generate by sphereface.But the result on tSNE doesn't perform better than the lcnn model.However it perform better than lcnn models(I trained) on lfw database.The sphereface net I had used is trained on MSceleb database and test on lfw database gets 98.65% acc.why the result of sphereface on the tSNE  doesn't  perform better than lcnn？ Do you have test it on tSNE?"
"Hi, actually, follow your instruction, after 28000 iterations, the loss decrease to 4.x. So, my question is one model can be used correct, what loss should be decreased to? Or, I should reset the max iterations, 10000 or 200000?
Thank you"
"hello,my tutor asks me to use caffemodel  to make a video demo,can you tell me how to do ,
thanks"
"https://github.com/wy1iu/sphereface/blob/50047da6b021a96f73298d2b67fcd1984e5cd77f/tools/caffe-sphereface/src/caffe/layers/margin_inner_product_layer.cpp#L377

comment describes coeff_x as
1 / (-|x|) * (sign_3 * (24 * cos_theta_quartic - 8 * cos_theta_quadratic - 1) + sign_4) * x
where the operator before sign_4_data is add
            
while in your code 
Dtype coeff_x = (Dtype)1. / (-x_norm_data[i]) * (sign_3_data[i * N_ + j] * 
                            ((Dtype)24. * cos_theta_quartic_data[i * N_ + j] - 
                            (Dtype)8. * cos_theta_quadratic_data[i * N_ + j] - (Dtype)1.) - 
                             sign_4_data[i * N_ + j]);

the operator before sign_4_data is minus

So which one is right? By the way, could you describe more details about how to deduce the derivative? I   only get the right coeff_w while coeff_x is too difficult to deduce."
"Hi authors, I have tried your network structure with sphere loss and it can give similar result which mentioned in your paper. However, when I switched to use common resnet-50 or resnet-101 I can just got LFW score at about 97.xx%.   Did you ever tried any other network structures? 

I thought that the problem was caused by small norm(fx) where fx is the 512-dim embedding. As the W is normalized to one in sphereface, fx*W will become very small if norm(fx) is small to affect the total loss value.
According to this I tried two approaches:  
   1) Normalize fx to a larger value like 32,  
   2) Use Prelu everywhere and set wd of the bias var in last FC layer to 0 and also remove the last BN layer. 

Neither of them can work. 
Do you have any idea or give some hints?"
"Hi, @wy1iu, sphereface is quite a great work and i have some questions about the formula derivation in margin_inner_product_layer. Such as in ""margin_inner_product_layer.hpp"" file, why set ""sign_0 = sign(cos_theta)"", ""sign_1 = sign(abs(cos_theta) - 0.5)"", ""sign_2 = sign_0 * (1 + sign_1) - 2"", ""sign_3 = sign_0 * sign(2 * cos_theta_quadratic_ - 1)"", ""sign_4 = 2 * sign_0 + sign_3 - 3"".
If i want get a pent type, what variables  ""sign_5"" should i set.
"
"hello,
there are some paper normalize the features before input Softmax layer, and I think it's make sense because
(1)it will make the feature space more dense which suit for hyperplane to do classification.
(2)all feature be normalized will give larger gradients to hard samples.

I also think these ideas don't conflict with sphereface which depend on angular margin.
Do i think it right? and have you did such experiment?
"
"Hello,

I tried to run the latest caffe version with largemargin layer however I get an error in the largemargin_inner_product_layer.hpp  where it says the the Layer<Dtype> does take 2 arguments and not one (Ftype and Btype). Could you please update the code in the largemargin layer.

Thank you very much,
Best"
"In sphereface, margin_inner_product_layer.cpp, line 277-278
you get gradient with respect to w using the same formula regardless of m: caffe_cpu_gemm(CblasTrans, CblasNoTrans, N_, K_, M_, (Dtype)1., top_diff, bottom_data, (Dtype)1., this->blobs_[0]->mutable_cpu_diff()); but

whereas in LargeMargin_softmax_loss, largemargin_inner_product_layer.cpp, line 263-356 you get gradient with respect to w according to m.

Can you tell me why?
Thanks."
"Hi, when I trained the sphereface-27 with my dataset, I always run into a problem：loss=87.33, and my prototxt as shown below:

layer {
  name: ""fc6-""
  type: ""MarginInnerProduct""
  bottom: ""fc5""
  bottom: ""label""
  top: ""fc6""
  top: ""lambda""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  margin_inner_product_param {
    num_output: 11954
    type: QUADRUPLE
    weight_filler {
      type: ""xavier""
    }
    base: 10
    gamma: 2e-05
    power: 1
    iteration: 0
    lambda_min: 5
  }
}

I0922 23:43:59.428972 192876 solver.cpp:219] Iteration 7000 (0.325212 iter/s, 307.492s/100 iters), loss = 10.0837
I0922 23:43:59.429009 192876 solver.cpp:238]     Train net output #0: accuracy = 0.574219
I0922 23:43:59.429015 192876 solver.cpp:238]     Train net output #1: lambda = 8.77178
I0922 23:43:59.429021 192876 solver.cpp:238]     Train net output #2: softmax_loss = 10.0837 (* 1 = 10.0837 loss)
I0922 23:43:59.429025 192876 sgd_solver.cpp:105] Iteration 7000, lr = 1e-05
I0922 23:45:04.017473 192883 data_layer.cpp:73] Restarting data prefetching from start.
I0922 23:49:06.527763 192876 solver.cpp:219] Iteration 7100 (0.325632 iter/s, 307.096s/100 iters), loss = 7.03009
I0922 23:49:06.527886 192876 solver.cpp:238]     Train net output #0: accuracy = 0.6875
I0922 23:49:06.527894 192876 solver.cpp:238]     Train net output #1: lambda = 8.75641
I0922 23:49:06.527901 192876 solver.cpp:238]     Train net output #2: softmax_loss = 7.03009 (* 1 = 7.03009 loss)
I0922 23:49:06.527905 192876 sgd_solver.cpp:105] Iteration 7100, lr = 1e-05
I0922 23:54:13.578683 192876 solver.cpp:219] Iteration 7200 (0.325682 iter/s, 307.048s/100 iters), loss = 8.27169
I0922 23:54:13.578783 192876 solver.cpp:238]     Train net output #0: accuracy = 0.65625
I0922 23:54:13.578791 192876 solver.cpp:238]     Train net output #1: lambda = 8.74111
I0922 23:54:13.578797 192876 solver.cpp:238]     Train net output #2: softmax_loss = 8.27169 (* 1 = 8.27169 loss)
I0922 23:54:13.578801 192876 sgd_solver.cpp:105] Iteration 7200, lr = 1e-05
I0922 23:59:20.621175 192876 solver.cpp:219] Iteration 7300 (0.325691 iter/s, 307.039s/100 iters), loss = 12.1695
I0922 23:59:20.621300 192876 solver.cpp:238]     Train net output #0: accuracy = 0.617188
I0922 23:59:20.621309 192876 solver.cpp:238]     Train net output #1: lambda = 8.72585
I0922 23:59:20.621316 192876 solver.cpp:238]     Train net output #2: softmax_loss = 12.1695 (* 1 = 12.1695 loss)
I0922 23:59:20.621321 192876 sgd_solver.cpp:105] Iteration 7300, lr = 1e-05
I0923 00:04:27.686278 192876 solver.cpp:219] Iteration 7400 (0.325667 iter/s, 307.062s/100 iters), loss = 10.945
I0923 00:04:27.686388 192876 solver.cpp:238]     Train net output #0: accuracy = 0.664062
I0923 00:04:27.686394 192876 solver.cpp:238]     Train net output #1: lambda = 8.71065
I0923 00:04:27.686416 192876 solver.cpp:238]     Train net output #2: softmax_loss = 10.945 (* 1 = 10.945 loss)
I0923 00:04:27.686421 192876 sgd_solver.cpp:105] Iteration 7400, lr = 1e-05
I0923 00:09:29.417439 192876 solver.cpp:448] Snapshotting to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_7500.caffemodel
I0923 00:09:31.971767 192876 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_7500.solverstate
I0923 00:09:35.155453 192876 solver.cpp:219] Iteration 7500 (0.325239 iter/s, 307.466s/100 iters), loss = 10.7305
I0923 00:09:35.155522 192876 solver.cpp:238]     Train net output #0: accuracy = 0.644531
I0923 00:09:35.155529 192876 solver.cpp:238]     Train net output #1: lambda = 8.6955
I0923 00:09:35.155535 192876 solver.cpp:238]     Train net output #2: softmax_loss = 10.7305 (* 1 = 10.7305 loss)
I0923 00:09:35.155540 192876 sgd_solver.cpp:105] Iteration 7500, lr = 1e-05
I0923 00:14:42.234946 192876 solver.cpp:219] Iteration 7600 (0.325652 iter/s, 307.076s/100 iters), loss = 12.6641
I0923 00:14:42.235061 192876 solver.cpp:238]     Train net output #0: accuracy = 0.652344
I0923 00:14:42.235069 192876 solver.cpp:238]     Train net output #1: lambda = 8.6804
I0923 00:14:42.235076 192876 solver.cpp:238]     Train net output #2: softmax_loss = 12.6641 (* 1 = 12.6641 loss)
I0923 00:14:42.235080 192876 sgd_solver.cpp:105] Iteration 7600, lr = 1e-05
I0923 00:19:49.294009 192876 solver.cpp:219] Iteration 7700 (0.325674 iter/s, 307.056s/100 iters), loss = 13.261
I0923 00:19:49.294123 192876 solver.cpp:238]     Train net output #0: accuracy = 0.617188
I0923 00:19:49.294131 192876 solver.cpp:238]     Train net output #1: lambda = 8.66536
I0923 00:19:49.294138 192876 solver.cpp:238]     Train net output #2: softmax_loss = 13.261 (* 1 = 13.261 loss)
I0923 00:19:49.294142 192876 sgd_solver.cpp:105] Iteration 7700, lr = 1e-05
I0923 00:24:56.366257 192876 solver.cpp:219] Iteration 7800 (0.32566 iter/s, 307.069s/100 iters), loss = 13.8046
I0923 00:24:56.366369 192876 solver.cpp:238]     Train net output #0: accuracy = 0.628906
I0923 00:24:56.366377 192876 solver.cpp:238]     Train net output #1: lambda = 8.65037
I0923 00:24:56.366386 192876 solver.cpp:238]     Train net output #2: softmax_loss = 13.8046 (* 1 = 13.8046 loss)
I0923 00:24:56.366391 192876 sgd_solver.cpp:105] Iteration 7800, lr = 1e-05
I0923 00:30:03.426719 192876 solver.cpp:219] Iteration 7900 (0.325672 iter/s, 307.057s/100 iters), loss = 14.3722
I0923 00:30:03.426854 192876 solver.cpp:238]     Train net output #0: accuracy = 0.640625
I0923 00:30:03.426863 192876 solver.cpp:238]     Train net output #1: lambda = 8.63543
I0923 00:30:03.426870 192876 solver.cpp:238]     Train net output #2: softmax_loss = 14.3722 (* 1 = 14.3722 loss)
I0923 00:30:03.426877 192876 sgd_solver.cpp:105] Iteration 7900, lr = 1e-05
I0923 00:35:05.096544 192876 solver.cpp:448] Snapshotting to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_8000.caffemodel
I0923 00:35:07.648367 192876 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_8000.solverstate
I0923 00:35:10.833468 192876 solver.cpp:219] Iteration 8000 (0.325305 iter/s, 307.403s/100 iters), loss = 17.3922
I0923 00:35:10.833523 192876 solver.cpp:238]     Train net output #0: accuracy = 0.648438
I0923 00:35:10.833528 192876 solver.cpp:238]     Train net output #1: lambda = 8.62054
I0923 00:35:10.833534 192876 solver.cpp:238]     Train net output #2: softmax_loss = 17.3922 (* 1 = 17.3922 loss)
I0923 00:35:10.833539 192876 sgd_solver.cpp:105] Iteration 8000, lr = 1e-05
I0923 00:40:17.756762 192876 solver.cpp:219] Iteration 8100 (0.325818 iter/s, 306.92s/100 iters), loss = 17.3765
I0923 00:40:17.756888 192876 solver.cpp:238]     Train net output #0: accuracy = 0.644531
I0923 00:40:17.756896 192876 solver.cpp:238]     Train net output #1: lambda = 8.6057
I0923 00:40:17.756903 192876 solver.cpp:238]     Train net output #2: softmax_loss = 17.3765 (* 1 = 17.3765 loss)
I0923 00:40:17.756908 192876 sgd_solver.cpp:105] Iteration 8100, lr = 1e-05
I0923 00:45:24.701261 192876 solver.cpp:219] Iteration 8200 (0.325795 iter/s, 306.941s/100 iters), loss = 17.4908
I0923 00:45:24.701335 192876 solver.cpp:238]     Train net output #0: accuracy = 0.625
I0923 00:45:24.701341 192876 solver.cpp:238]     Train net output #1: lambda = 8.59092
I0923 00:45:24.701349 192876 solver.cpp:238]     Train net output #2: softmax_loss = 17.4908 (* 1 = 17.4908 loss)
I0923 00:45:24.701354 192876 sgd_solver.cpp:105] Iteration 8200, lr = 1e-05
I0923 00:50:31.609172 192876 solver.cpp:219] Iteration 8300 (0.325834 iter/s, 306.905s/100 iters), loss = 21.0124
I0923 00:50:31.609232 192876 solver.cpp:238]     Train net output #0: accuracy = 0.625
I0923 00:50:31.609238 192876 solver.cpp:238]     Train net output #1: lambda = 8.57618
I0923 00:50:31.609246 192876 solver.cpp:238]     Train net output #2: softmax_loss = 21.0124 (* 1 = 21.0124 loss)
I0923 00:50:31.609249 192876 sgd_solver.cpp:105] Iteration 8300, lr = 1e-05
I0923 00:55:38.455067 192876 solver.cpp:219] Iteration 8400 (0.3259 iter/s, 306.843s/100 iters), loss = 26.4586
I0923 00:55:38.455188 192876 solver.cpp:238]     Train net output #0: accuracy = 0.546875
I0923 00:55:38.455196 192876 solver.cpp:238]     Train net output #1: lambda = 8.5615
I0923 00:55:38.455205 192876 solver.cpp:238]     Train net output #2: softmax_loss = 26.4586 (* 1 = 26.4586 loss)
I0923 00:55:38.455210 192876 sgd_solver.cpp:105] Iteration 8400, lr = 1e-05
I0923 01:00:39.942590 192876 solver.cpp:448] Snapshotting to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_8500.caffemodel
I0923 01:00:42.497731 192876 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_8500.solverstate
I0923 01:00:45.680984 192876 solver.cpp:219] Iteration 8500 (0.325497 iter/s, 307.223s/100 iters), loss = 27.8068
I0923 01:00:45.681020 192876 solver.cpp:238]     Train net output #0: accuracy = 0.578125
I0923 01:00:45.681025 192876 solver.cpp:238]     Train net output #1: lambda = 8.54686
I0923 01:00:45.681032 192876 solver.cpp:238]     Train net output #2: softmax_loss = 27.8068 (* 1 = 27.8068 loss)
I0923 01:00:45.681036 192876 sgd_solver.cpp:105] Iteration 8500, lr = 1e-05
I0923 01:05:52.483103 192876 solver.cpp:219] Iteration 8600 (0.325946 iter/s, 306.799s/100 iters), loss = 28.9902
I0923 01:05:52.483191 192876 solver.cpp:238]     Train net output #0: accuracy = 0.574219
I0923 01:05:52.483199 192876 solver.cpp:238]     Train net output #1: lambda = 8.53228
I0923 01:05:52.483207 192876 solver.cpp:238]     Train net output #2: softmax_loss = 28.9902 (* 1 = 28.9902 loss)
I0923 01:05:52.483211 192876 sgd_solver.cpp:105] Iteration 8600, lr = 1e-05
I0923 01:10:59.224231 192876 solver.cpp:219] Iteration 8700 (0.326011 iter/s, 306.738s/100 iters), loss = 45.9285
I0923 01:10:59.224365 192876 solver.cpp:238]     Train net output #0: accuracy = 0.429688
I0923 01:10:59.224373 192876 solver.cpp:238]     Train net output #1: lambda = 8.51774
I0923 01:10:59.224380 192876 solver.cpp:238]     Train net output #2: softmax_loss = 45.9285 (* 1 = 45.9285 loss)
I0923 01:10:59.224385 192876 sgd_solver.cpp:105] Iteration 8700, lr = 1e-05
I0923 01:16:05.965823 192876 solver.cpp:219] Iteration 8800 (0.326011 iter/s, 306.738s/100 iters), loss = 62.0409
I0923 01:16:05.965976 192876 solver.cpp:238]     Train net output #0: accuracy = 0.289062
I0923 01:16:05.965996 192876 solver.cpp:238]     Train net output #1: lambda = 8.50326
I0923 01:16:05.966014 192876 solver.cpp:238]     Train net output #2: softmax_loss = 62.0409 (* 1 = 62.0409 loss)
I0923 01:16:05.966025 192876 sgd_solver.cpp:105] Iteration 8800, lr = 1e-05
I0923 01:21:12.843422 192876 solver.cpp:219] Iteration 8900 (0.325866 iter/s, 306.874s/100 iters), loss = 67.5738
I0923 01:21:12.843535 192876 solver.cpp:238]     Train net output #0: accuracy = 0.222656
I0923 01:21:12.843542 192876 solver.cpp:238]     Train net output #1: lambda = 8.48882
I0923 01:21:12.843550 192876 solver.cpp:238]     Train net output #2: softmax_loss = 67.5738 (* 1 = 67.5738 loss)
I0923 01:21:12.843554 192876 sgd_solver.cpp:105] Iteration 8900, lr = 1e-05
I0923 01:26:14.555048 192876 solver.cpp:448] Snapshotting to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_9000.caffemodel
I0923 01:26:17.105406 192876 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/unique/data-face/face_model/WT_sphereface27-40_iter_9000.solverstate
I0923 01:26:20.287829 192876 solver.cpp:219] Iteration 9000 (0.325265 iter/s, 307.441s/100 iters), loss = 65.0543
I0923 01:26:20.287881 192876 solver.cpp:238]     Train net output #0: accuracy = 0.238281
I0923 01:26:20.287886 192876 solver.cpp:238]     Train net output #1: lambda = 8.47443
I0923 01:26:20.287894 192876 solver.cpp:238]     Train net output #2: softmax_loss = 65.0543 (* 1 = 65.0543 loss)
I0923 01:26:20.287899 192876 sgd_solver.cpp:105] Iteration 9000, lr = 1e-05
I0923 01:31:27.157495 192876 solver.cpp:219] Iteration 9100 (0.325875 iter/s, 306.866s/100 iters), loss = 87.3365
I0923 01:31:27.157562 192876 solver.cpp:238]     Train net output #0: accuracy = 0
I0923 01:31:27.157569 192876 solver.cpp:238]     Train net output #1: lambda = 8.46009
I0923 01:31:27.157577 192876 solver.cpp:238]     Train net output #2: softmax_loss = 87.3365 (* 1 = 87.3365 loss)

What should I do to solve the problem? Thanks!

"
"why the weight_diff is treated seperately according to m (SINGLE, DOUBLE, etc) in LargeMargin_softmax_loss wheras in sphereface, it is treated the same?"
Is there any pretrained model using this sphereface concept? If so how do i get the face embeddings of a custom image using that pre trained model?
"first , i train my model using original softmax
then, i want to finetune it using a-softmax
i intend to initialize the a-softmax's fc6 layer param using the original softmax, but the log says that they don't have the same number of param blobs, I found that margin inner product layer has no bias param blob.
i tried xavier initialization , but when i freeze the layers before fc6 and only learn fc6, the loss cannot converge .Do i have to relax the lr of the layers before fc6? "
"In CenterFace and your SphereFace, your net models are both based on ResNets. But I noticed that neither of you use any BatchNorm layers (which differ from original resnets). I tried to add BatchNorm layers back. but got bad results (loss only converged to about 3.5).
So I'm really curious why BatchNorm layers are not good here. Any idea to explain this?"
"Hi,I followed your instruction to train CASIA-WebFace dataset, but the training loss stays around 9.3 at the very beginning. Could you please give some possible advises to solve this problem? Here is part of my train log.
####train log
I0907 20:56:08.035786 10605 caffe.cpp:218] Using GPUs 0
I0907 20:56:08.148684 10605 caffe.cpp:223] GPU 0: Tesla P40
I0907 20:56:08.807466 10605 solver.cpp:44] Initializing solver from parameters: 

base_lr: 0.01
display: 100
max_iter: 28000
lr_policy: ""multistep""
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: ""result/sphereface_model_maxpy_clean/ckpt0907_2""
solver_mode: GPU
device_id: 0
net: ""code/sphereface_model_maxpy_clean.prototxt""
train_state {
  level: 0
  stage: """"
}
stepvalue: 16000
stepvalue: 24000
stepvalue: 28000
I0907 20:56:08.812855 10605 solver.cpp:87] Creating training net from net file: code/sphereface_model_maxpy_clean.prototxt
I0907 20:56:08.815649 10605 net.cpp:51] Initializing net from parameters: 
name: ""SpherefaceNet-20""
###skip the model part
I0907 20:56:11.030130 10605 solver.cpp:218] Iteration 0 (-1.9811e-31 iter/s, 0.203529s/100 iters), loss = 9.20592
I0907 20:56:11.030190 10605 solver.cpp:237]     Train net output #0: lambda = 892.857
I0907 20:56:11.030210 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.20592 (* 1 = 9.20592 loss)
I0907 20:56:11.030252 10605 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0907 20:56:11.079816 10605 blocking_queue.cpp:49] Waiting for data
I0907 20:57:30.478466 10605 solver.cpp:218] Iteration 100 (1.25873 iter/s, 79.4451s/100 iters), loss = 9.33074
I0907 20:57:30.478658 10605 solver.cpp:237]     Train net output #0: lambda = 76.2195
I0907 20:57:30.478673 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.33074 (* 1 = 9.33074 loss)
I0907 20:57:30.478684 10605 sgd_solver.cpp:105] Iteration 100, lr = 0.01
I0907 20:58:46.554661 10605 solver.cpp:218] Iteration 200 (1.31453 iter/s, 76.073s/100 iters), loss = 9.30099
I0907 20:58:46.554803 10605 solver.cpp:237]     Train net output #0: lambda = 39.8089
I0907 20:58:46.554821 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.30099 (* 1 = 9.30099 loss)
I0907 20:58:46.554834 10605 sgd_solver.cpp:105] Iteration 200, lr = 0.01
I0907 21:00:32.839994 10605 solver.cpp:218] Iteration 300 (0.940902 iter/s, 106.281s/100 iters), loss = 9.41046
I0907 21:00:32.840147 10605 solver.cpp:237]     Train net output #0: lambda = 26.9397
I0907 21:00:32.840165 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.41046 (* 1 = 9.41046 loss)
I0907 21:00:32.840178 10605 sgd_solver.cpp:105] Iteration 300, lr = 0.01
I0907 21:02:01.073664 10605 solver.cpp:218] Iteration 400 (1.1334 iter/s, 88.23s/100 iters), loss = 9.35314
I0907 21:02:01.073825 10605 solver.cpp:237]     Train net output #0: lambda = 20.3583
I0907 21:02:01.073848 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.35314 (* 1 = 9.35314 loss)
I0907 21:02:01.073860 10605 sgd_solver.cpp:105] Iteration 400, lr = 0.01
I0907 21:03:35.687438 10605 solver.cpp:218] Iteration 500 (1.05697 iter/s, 94.6099s/100 iters), loss = 9.42017
I0907 21:03:35.687583 10605 solver.cpp:237]     Train net output #0: lambda = 16.3613
I0907 21:03:35.687602 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.42017 (* 1 = 9.42017 loss)
I0907 21:03:35.687613 10605 sgd_solver.cpp:105] Iteration 500, lr = 0.01
I0907 21:05:06.772781 10605 solver.cpp:218] Iteration 600 (1.09792 iter/s, 91.0816s/100 iters), loss = 9.3796
I0907 21:05:06.772953 10605 solver.cpp:237]     Train net output #0: lambda = 13.6761
I0907 21:05:06.772974 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.3796 (* 1 = 9.3796 loss)
I0907 21:05:06.772987 10605 sgd_solver.cpp:105] Iteration 600, lr = 0.01
I0907 21:06:23.862064 10605 solver.cpp:218] Iteration 700 (1.29725 iter/s, 77.086s/100 iters), loss = 9.37048
I0907 21:06:23.862254 10605 solver.cpp:237]     Train net output #0: lambda = 11.7481
I0907 21:06:23.862272 10605 solver.cpp:237]     Train net output #1: softmax_loss = 9.37048 (* 1 = 9.37048 loss)
P.S I've read the [issue #14 ](https://github.com/wy1iu/sphereface/issues/14) and [issue #7 ](https://github.com/wy1iu/sphereface/issues/7) , but neither provides feasible solutions. Could you please provide your solver settings and the lambda related parameters(lambda min, base and
    gamma) for trainning? Thx a lot."
"Hi,
  I have just evaluated your 20-layer CNN architecture caffemodel in Megaface.
The results are below:
Rank-1 Identification Accuracy with 1 Million Distractors, Set1 (FaceScrub )
77.6892%  (75.766% is their 68 layers published in the official homepage of Megaface )
Rank-1 Identification Accuracy with 1 Million Distractors, Set1, (testing age-invariant recognition at scale, FGNet)
23.5023% (47.555% is their 68 layers published in the official homepage of Megaface )

  Is it right?  did any other guys evaluate it as well?"
"Hi guys, 
I tested A-Softmax  loss on CASIA dataset and it really does well, reaching a ~99% performance on lfw without careful tuning. But when I switch to use the MS-Celeb-1M datasets for training(roughly cleaned, 98.8% lfw with Lighten CNN), either the 28-layer net nor the 64-layer net seems not to converge. The 64-layer net is the same as mentioned in your paper. 
Have you tried A-Softmax in such large dataset? I am still trying to tune my net but have no idea why it doesn't converge."
"Hello, 
Your previous submit use MTCNNv2 as the preprocessing algorithm.
However, the latest submit change to MTCNNv1.
Is there any reason for this change?
Which version is used in your paper?

Thanks very much."
"Hi,
The verification results reported on megaface website - http://megaface.cs.washington.edu/results/facescrubresults.html#verification - are:
Set 1 | Set 2 | Set 3
90.045% | 89.355% | 90.045%

However in your paper (table 5) you report the verification @ 10e-6 to be 85.561% (single model) and 89.142% (3patch ensemble), both being different than the number on MegaFace website, so which one are the correct results?

Thank you!


"
""
"
"
"when I make mattest，I meet the follow problem.
Invalid MEX-file
'/home/caffe/matlab/+caffe/private/caffe_.mexa64':
/usr/local/lib/libopencv_imgcodecs.so.3.4: undefined symbol:
_ZN2cv6detail17check_failed_autoEmmRKNS0_12CheckContextE.
 
Error in caffe.set_mode_cpu (line 5)
caffe_('set_mode_cpu');
Error in caffe.run_tests (line 6)
caffe.set_mode_cpu();"
"Thanks for sharing, when I run evaluation.m, I met the follow problem, how can I solve it? @wy1iu 

![捕获](https://user-images.githubusercontent.com/56579909/68819783-155d1580-06c4-11ea-8678-b774d07151e0.JPG)



"
"In your release, feature about LFW is 1024-dim, but your model output is 512-dim, is there some problems? https://drive.google.com/open?id=0B_geeR2lTMegenU0cGJYZmlRUlU"
"Hi,do you have the code using c++? @wy1iu "
"Hi,

Following your code. I encountered an error which is related to build matcaffe.
I am tried to build matcaffe by changing my gcc and g++ version to 4.9, but I am failed.
The reason why I am tried to solve this issue is to derive 'CASIA-WebFace-112X96.txt' file.

To push a bunch of triplet pairs, I guess you used txt file as an input source. I try to construct txt file similar with your 'CASIA-WebFace-112X96.txt' format.
Could you describe the detail of the txt file? 

Thanks!

"
"> Specifically, half-space MHE constructs a colinear virtual neuron with opposite direction for every
    existing neuron.

Which means if there exists a ``W = {w1, w2, .....}``, the opposite direction is ``W' = {-w1, -w2, ......}``?"
"In file included from src/caffe/layers/margin_inner_product_layer.cpp:8:
./include/caffe/layers/margin_inner_product_layer.hpp:46:3: error: unknown type name 'MarginInnerProductParameter_MarginType'
  MarginInnerProductParameter_MarginType type_;
  ^
src/caffe/layers/margin_inner_product_layer.cpp:23:30: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  iter_ = this->layer_param_.margin_inner_product_param().iteration();
          ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<float>::LayerSetUp' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:26:45: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  const int num_output = this->layer_param_.margin_inner_product_param().num_output();
                         ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:29:26: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
      this->layer_param_.margin_inner_product_param().axis());
      ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:46:28: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
        this->layer_param_.margin_inner_product_param().weight_filler()));
        ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:57:26: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
      this->layer_param_.margin_inner_product_param().axis());
      ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<float>::Reshape' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:112:36: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype base_ = this->layer_param_.margin_inner_product_param().base();
                ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<float>::Forward_cpu' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:113:37: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype gamma_ = this->layer_param_.margin_inner_product_param().gamma();
                 ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:114:37: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype power_ = this->layer_param_.margin_inner_product_param().power();
                 ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:115:42: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype lambda_min_ = this->layer_param_.margin_inner_product_param().lambda_min();
                      ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:194:16: warning: unused variable 'label' [-Wunused-variable]
  const Dtype* label = bottom[1]->cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:195:16: warning: unused variable 'x_norm_data' [-Wunused-variable]
  const Dtype* x_norm_data = x_norm_.cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:272:16: warning: unused variable 'label' [-Wunused-variable]
  const Dtype* label = bottom[1]->cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<float>::Backward_cpu' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:273:16: warning: unused variable 'weight' [-Wunused-variable]
  const Dtype* weight = this->blobs_[0]->cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:284:18: warning: unused variable 'x_norm_data' [-Wunused-variable]
    const Dtype* x_norm_data = x_norm_.cpu_data();
                 ^
src/caffe/layers/margin_inner_product_layer.cpp:23:30: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  iter_ = this->layer_param_.margin_inner_product_param().iteration();
          ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<double>::LayerSetUp' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:26:45: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  const int num_output = this->layer_param_.margin_inner_product_param().num_output();
                         ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:29:26: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
      this->layer_param_.margin_inner_product_param().axis());
      ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:46:28: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
        this->layer_param_.margin_inner_product_param().weight_filler()));
        ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:57:26: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
      this->layer_param_.margin_inner_product_param().axis());
      ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<double>::Reshape' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:112:36: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype base_ = this->layer_param_.margin_inner_product_param().base();
                ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<double>::Forward_cpu' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:113:37: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype gamma_ = this->layer_param_.margin_inner_product_param().gamma();
                 ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:114:37: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype power_ = this->layer_param_.margin_inner_product_param().power();
                 ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:115:42: error: no member named 'margin_inner_product_param' in 'caffe::LayerParameter'
  Dtype lambda_min_ = this->layer_param_.margin_inner_product_param().lambda_min();
                      ~~~~~~~~~~~~~~~~~~ ^
src/caffe/layers/margin_inner_product_layer.cpp:194:16: warning: unused variable 'label' [-Wunused-variable]
  const Dtype* label = bottom[1]->cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:195:16: warning: unused variable 'x_norm_data' [-Wunused-variable]
  const Dtype* x_norm_data = x_norm_.cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:272:16: warning: unused variable 'label' [-Wunused-variable]
  const Dtype* label = bottom[1]->cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:410:19: note: in instantiation of member function 'caffe::MarginInnerProductLayer<double>::Backward_cpu' requested here
INSTANTIATE_CLASS(MarginInnerProductLayer);
                  ^
src/caffe/layers/margin_inner_product_layer.cpp:273:16: warning: unused variable 'weight' [-Wunused-variable]
  const Dtype* weight = this->blobs_[0]->cpu_data();
               ^
src/caffe/layers/margin_inner_product_layer.cpp:284:18: warning: unused variable 'x_norm_data' [-Wunused-variable]
    const Dtype* x_norm_data = x_norm_.cpu_data();
                 ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
10 warnings and 20 errors generated.
make: *** [.build_release/src/caffe/layers/margin_inner_product_layer.o] Error 1
make: *** Waiting for unfinished jobs....
src/caffe/layers/crop_layer.cpp:45:11: error: no member named 'Reshape' in 'std::__1::vector<int, std::__1::allocator<int> >'
  offsets.Reshape(offsets_shape);
  ~~~~~~~ ^
src/caffe/layers/crop_layer.cpp:46:30: error: no member named 'mutable_cpu_data' in 'std::__1::vector<int, std::__1::allocator<int> >'
  int* offset_data = offsets.mutable_cpu_data();
                     ~~~~~~~ ^
src/caffe/layers/crop_layer.cpp:72:3: error: use of undeclared identifier 'src_strides_'
  src_strides_.Reshape(offsets_shape);
  ^
src/caffe/layers/crop_layer.cpp:73:3: error: use of undeclared identifier 'dest_strides_'
  dest_strides_.Reshape(offsets_shape);
  ^
src/caffe/layers/crop_layer.cpp:75:5: error: use of undeclared identifier 'src_strides_'
    src_strides_.mutable_cpu_data()[i] = bottom[0]->count(i + 1, input_dim);
    ^
src/caffe/layers/crop_layer.cpp:76:5: error: use of undeclared identifier 'dest_strides_'
    dest_strides_.mutable_cpu_data()[i] = top[0]->count(i + 1, input_dim);
    ^
src/caffe/layers/crop_layer.cpp:81:24: error: out-of-line definition of 'crop_copy' does not match any declaration in 'CropLayer<Dtype>'
void CropLayer<Dtype>::crop_copy(const vector<Blob<Dtype>*>& bottom,
                       ^~~~~~~~~
src/caffe/layers/crop_layer.cpp:127:34: error: no member named 'cpu_data' in 'std::__1::vector<int, std::__1::allocator<int> >'
  crop_copy(bottom, top, offsets.cpu_data(), indices, 0, bottom_data, top_data,
                         ~~~~~~~ ^
src/caffe/layers/crop_layer.cpp:140:36: error: no member named 'cpu_data' in 'std::__1::vector<int, std::__1::allocator<int> >'
    crop_copy(bottom, top, offsets.cpu_data(), indices, 0, top_diff,
                           ~~~~~~~ ^
9 errors generated.
make: *** [.build_release/src/caffe/layers/crop_layer.o] Error 1"
"Hi all, I check the evaluation.m and noticed that there is no preprocess before extract face features. Does it mean I don't need to do the face alignment before feed my face picture into the net during test?

Thank you！"
""
"<img width=""492"" alt=""screen shot 2018-05-14 at 1 05 23 am"" src=""https://user-images.githubusercontent.com/10518587/39969772-d0c5d990-5713-11e8-94cf-a9f9144485e0.png"">

可以说一下为什么 max intra-class angle 是

<img width=""104"" alt=""screen shot 2018-05-14 at 1 14 22 am"" src=""https://user-images.githubusercontent.com/10518587/39969796-2968dc28-5714-11e8-9d7a-d7f0230435d9.png"">

吗？"
@wy1iu Can someone use directly the output of fc5 to verify if a person is the same across multiple photos (with L2 distance for example)?
Do you have windows build version?
"I try to preprocess as described in index page and this error masage.

Number of field names must match number of fields in new structure.

Error in face_detect_demo>collectData (line 103)
    list       = cell2struct([files dataset], {'file', 'dataset'}, 2);

Error in face_detect_demo (line 27)
trainList = collectData(fullfile(pwd, 'data/CASIA-WebFace'), 'CASIA-WebFace');

What can be wrong?"
i want to learn how to relize A-softmax loss
"Hello,thanks for your good job.
I have trained sphereface-20 and sphereface-64 with only one GTX1060 or GTX1070,and can get 99.22% 99.32%at LFW.
Now,I want to do this on the machine with 3 GTX1080,but I get some error about it.At the begin of training,the loss is 87.3365 already with nothing changed.It can do training with change type: QUADRUPLE to be SINGLE.Those are my training debug info.
Can anyone tell me why this happen?
Thanks very much.

I0315 11:08:21.413416 11950 caffe.cpp:218] Using GPUs 0, 1
I0315 11:08:21.590229 11950 caffe.cpp:223] GPU 0: GeForce GTX 1080
I0315 11:08:21.591059 11950 caffe.cpp:223] GPU 1: GeForce GTX 1080
I0315 11:08:22.190205 11950 solver.cpp:44] Initializing solver from parameters: 
base_lr: 0.1
display: 100
max_iter: 28000
lr_policy: ""multistep""
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot_prefix: ""result/sphereface_model""
solver_mode: GPU
device_id: 0
debug_info: true
net: ""code/sphereface_model.prototxt""
train_state {
  level: 0
  stage: """"
}
stepvalue: 16000
stepvalue: 24000
stepvalue: 28000
I0315 11:08:22.190274 11950 solver.cpp:87] Creating training net from net file: code/sphereface_model.prototxt
I0315 11:08:22.192152 11950 net.cpp:51] Initializing net from parameters: 
name: ""SpherefaceNet-20""
state {
  phase: TRAIN
  level: 0
  stage: """"
}
layer {
  name: ""data""
  type: ""ImageData""
  top: ""data""
  top: ""label""
  transform_param {
    scale: 0.0078125
    mirror: true
    mean_value: 127.5
    mean_value: 127.5
    mean_value: 127.5
  }
  image_data_param {
    source: ""data/CASIA-WebFace-112X96.txt""
    batch_size: 256
    shuffle: true
  }
}
layer {
  name: ""conv1_1""
  type: ""Convolution""
  bottom: ""data""
  top: ""conv1_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu1_1""
  type: ""PReLU""
  bottom: ""conv1_1""
  top: ""conv1_1""
}
layer {
  name: ""conv1_2""
  type: ""Convolution""
  bottom: ""conv1_1""
  top: ""conv1_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu1_2""
  type: ""PReLU""
  bottom: ""conv1_2""
  top: ""conv1_2""
}
layer {
  name: ""conv1_3""
  type: ""Convolution""
  bottom: ""conv1_2""
  top: ""conv1_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu1_3""
  type: ""PReLU""
  bottom: ""conv1_3""
  top: ""conv1_3""
}
layer {
  name: ""res1_3""
  type: ""Eltwise""
  bottom: ""conv1_1""
  bottom: ""conv1_3""
  top: ""res1_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv2_1""
  type: ""Convolution""
  bottom: ""res1_3""
  top: ""conv2_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_1""
  type: ""PReLU""
  bottom: ""conv2_1""
  top: ""conv2_1""
}
layer {
  name: ""conv2_2""
  type: ""Convolution""
  bottom: ""conv2_1""
  top: ""conv2_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_2""
  type: ""PReLU""
  bottom: ""conv2_2""
  top: ""conv2_2""
}
layer {
  name: ""conv2_3""
  type: ""Convolution""
  bottom: ""conv2_2""
  top: ""conv2_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_3""
  type: ""PReLU""
  bottom: ""conv2_3""
  top: ""conv2_3""
}
layer {
  name: ""res2_3""
  type: ""Eltwise""
  bottom: ""conv2_1""
  bottom: ""conv2_3""
  top: ""res2_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv2_4""
  type: ""Convolution""
  bottom: ""res2_3""
  top: ""conv2_4""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_4""
  type: ""PReLU""
  bottom: ""conv2_4""
  top: ""conv2_4""
}
layer {
  name: ""conv2_5""
  type: ""Convolution""
  bottom: ""conv2_4""
  top: ""conv2_5""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu2_5""
  type: ""PReLU""
  bottom: ""conv2_5""
  top: ""conv2_5""
}
layer {
  name: ""res2_5""
  type: ""Eltwise""
  bottom: ""res2_3""
  bottom: ""conv2_5""
  top: ""res2_5""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_1""
  type: ""Convolution""
  bottom: ""res2_5""
  top: ""conv3_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_1""
  type: ""PReLU""
  bottom: ""conv3_1""
  top: ""conv3_1""
}
layer {
  name: ""conv3_2""
  type: ""Convolution""
  bottom: ""conv3_1""
  top: ""conv3_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_2""
  type: ""PReLU""
  bottom: ""conv3_2""
  top: ""conv3_2""
}
layer {
  name: ""conv3_3""
  type: ""Convolution""
  bottom: ""conv3_2""
  top: ""conv3_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_3""
  type: ""PReLU""
  bottom: ""conv3_3""
  top: ""conv3_3""
}
layer {
  name: ""res3_3""
  type: ""Eltwise""
  bottom: ""conv3_1""
  bottom: ""conv3_3""
  top: ""res3_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_4""
  type: ""Convolution""
  bottom: ""res3_3""
  top: ""conv3_4""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_4""
  type: ""PReLU""
  bottom: ""conv3_4""
  top: ""conv3_4""
}
layer {
  name: ""conv3_5""
  type: ""Convolution""
  bottom: ""conv3_4""
  top: ""conv3_5""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_5""
  type: ""PReLU""
  bottom: ""conv3_5""
  top: ""conv3_5""
}
layer {
  name: ""res3_5""
  type: ""Eltwise""
  bottom: ""res3_3""
  bottom: ""conv3_5""
  top: ""res3_5""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_6""
  type: ""Convolution""
  bottom: ""res3_5""
  top: ""conv3_6""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_6""
  type: ""PReLU""
  bottom: ""conv3_6""
  top: ""conv3_6""
}
layer {
  name: ""conv3_7""
  type: ""Convolution""
  bottom: ""conv3_6""
  top: ""conv3_7""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_7""
  type: ""PReLU""
  bottom: ""conv3_7""
  top: ""conv3_7""
}
layer {
  name: ""res3_7""
  type: ""Eltwise""
  bottom: ""res3_5""
  bottom: ""conv3_7""
  top: ""res3_7""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv3_8""
  type: ""Convolution""
  bottom: ""res3_7""
  top: ""conv3_8""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_8""
  type: ""PReLU""
  bottom: ""conv3_8""
  top: ""conv3_8""
}
layer {
  name: ""conv3_9""
  type: ""Convolution""
  bottom: ""conv3_8""
  top: ""conv3_9""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu3_9""
  type: ""PReLU""
  bottom: ""conv3_9""
  top: ""conv3_9""
}
layer {
  name: ""res3_9""
  type: ""Eltwise""
  bottom: ""res3_7""
  bottom: ""conv3_9""
  top: ""res3_9""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""conv4_1""
  type: ""Convolution""
  bottom: ""res3_9""
  top: ""conv4_1""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu4_1""
  type: ""PReLU""
  bottom: ""conv4_1""
  top: ""conv4_1""
}
layer {
  name: ""conv4_2""
  type: ""Convolution""
  bottom: ""conv4_1""
  top: ""conv4_2""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu4_2""
  type: ""PReLU""
  bottom: ""conv4_2""
  top: ""conv4_2""
}
layer {
  name: ""conv4_3""
  type: ""Convolution""
  bottom: ""conv4_2""
  top: ""conv4_3""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: ""gaussian""
      std: 0.01
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""relu4_3""
  type: ""PReLU""
  bottom: ""conv4_3""
  top: ""conv4_3""
}
layer {
  name: ""res4_3""
  type: ""Eltwise""
  bottom: ""conv4_1""
  bottom: ""conv4_3""
  top: ""res4_3""
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: ""fc5""
  type: ""InnerProduct""
  bottom: ""res4_3""
  top: ""fc5""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: ""xavier""
    }
    bias_filler {
      type: ""constant""
      value: 0
    }
  }
}
layer {
  name: ""fc6""
  type: ""MarginInnerProduct""
  bottom: ""fc5""
  bottom: ""label""
  top: ""fc6""
  top: ""lambda""
  param {
    lr_mult: 1
    decay_mult: 1
  }
  margin_inner_product_param {
    num_output: 10572
    type: QUADRUPLE
    weight_filler {
      type: ""xavier""
    }
    base: 1000
    gamma: 0.12
    power: 1
    iteration: 0
    lambda_min: 5
  }
}
layer {
  name: ""softmax_loss""
  type: ""SoftmaxWithLoss""
  bottom: ""fc6""
  bottom: ""label""
  top: ""softmax_loss""
}
I0315 11:08:22.192536 11950 layer_factory.hpp:77] Creating layer data
I0315 11:08:22.192586 11950 net.cpp:84] Creating Layer data
I0315 11:08:22.192596 11950 net.cpp:380] data -> data
I0315 11:08:22.192627 11950 net.cpp:380] data -> label
I0315 11:08:22.193110 11950 image_data_layer.cpp:38] Opening file data/CASIA-WebFace-112X96.txt
I0315 11:08:22.400406 11950 image_data_layer.cpp:53] Shuffling data
I0315 11:08:22.482203 11950 image_data_layer.cpp:63] A total of 452722 images.
I0315 11:08:22.484470 11950 image_data_layer.cpp:90] output data size: 256,3,112,96
I0315 11:08:22.583492 11950 net.cpp:122] Setting up data
I0315 11:08:22.583549 11950 net.cpp:129] Top shape: 256 3 112 96 (8257536)
I0315 11:08:22.583557 11950 net.cpp:129] Top shape: 256 (256)
I0315 11:08:22.583564 11950 net.cpp:137] Memory required for data: 33031168
I0315 11:08:22.583576 11950 layer_factory.hpp:77] Creating layer label_data_1_split
I0315 11:08:22.583611 11950 net.cpp:84] Creating Layer label_data_1_split
I0315 11:08:22.583621 11950 net.cpp:406] label_data_1_split <- label
I0315 11:08:22.583642 11950 net.cpp:380] label_data_1_split -> label_data_1_split_0
I0315 11:08:22.583657 11950 net.cpp:380] label_data_1_split -> label_data_1_split_1
I0315 11:08:22.583807 11950 net.cpp:122] Setting up label_data_1_split
I0315 11:08:22.583820 11950 net.cpp:129] Top shape: 256 (256)
I0315 11:08:22.583827 11950 net.cpp:129] Top shape: 256 (256)
I0315 11:08:22.583832 11950 net.cpp:137] Memory required for data: 33033216
I0315 11:08:22.583837 11950 layer_factory.hpp:77] Creating layer conv1_1
I0315 11:08:22.583865 11950 net.cpp:84] Creating Layer conv1_1
I0315 11:08:22.583879 11950 net.cpp:406] conv1_1 <- data
I0315 11:08:22.583889 11950 net.cpp:380] conv1_1 -> conv1_1
I0315 11:08:23.034631 11950 net.cpp:122] Setting up conv1_1
I0315 11:08:23.034693 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.034700 11950 net.cpp:137] Memory required for data: 209193984
I0315 11:08:23.034761 11950 layer_factory.hpp:77] Creating layer relu1_1
I0315 11:08:23.034824 11950 net.cpp:84] Creating Layer relu1_1
I0315 11:08:23.034832 11950 net.cpp:406] relu1_1 <- conv1_1
I0315 11:08:23.034840 11950 net.cpp:367] relu1_1 -> conv1_1 (in-place)
I0315 11:08:23.036139 11950 net.cpp:122] Setting up relu1_1
I0315 11:08:23.036154 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.036159 11950 net.cpp:137] Memory required for data: 385354752
I0315 11:08:23.036166 11950 layer_factory.hpp:77] Creating layer conv1_1_relu1_1_0_split
I0315 11:08:23.036193 11950 net.cpp:84] Creating Layer conv1_1_relu1_1_0_split
I0315 11:08:23.036198 11950 net.cpp:406] conv1_1_relu1_1_0_split <- conv1_1
I0315 11:08:23.036219 11950 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_0
I0315 11:08:23.036227 11950 net.cpp:380] conv1_1_relu1_1_0_split -> conv1_1_relu1_1_0_split_1
I0315 11:08:23.036262 11950 net.cpp:122] Setting up conv1_1_relu1_1_0_split
I0315 11:08:23.036269 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.036274 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.036278 11950 net.cpp:137] Memory required for data: 737676288
I0315 11:08:23.036281 11950 layer_factory.hpp:77] Creating layer conv1_2
I0315 11:08:23.036308 11950 net.cpp:84] Creating Layer conv1_2
I0315 11:08:23.036311 11950 net.cpp:406] conv1_2 <- conv1_1_relu1_1_0_split_0
I0315 11:08:23.036317 11950 net.cpp:380] conv1_2 -> conv1_2
I0315 11:08:23.039396 11950 net.cpp:122] Setting up conv1_2
I0315 11:08:23.039412 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.039419 11950 net.cpp:137] Memory required for data: 913837056
I0315 11:08:23.039429 11950 layer_factory.hpp:77] Creating layer relu1_2
I0315 11:08:23.039436 11950 net.cpp:84] Creating Layer relu1_2
I0315 11:08:23.039441 11950 net.cpp:406] relu1_2 <- conv1_2
I0315 11:08:23.039491 11950 net.cpp:367] relu1_2 -> conv1_2 (in-place)
I0315 11:08:23.039677 11950 net.cpp:122] Setting up relu1_2
I0315 11:08:23.039688 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.039691 11950 net.cpp:137] Memory required for data: 1089997824
I0315 11:08:23.039696 11950 layer_factory.hpp:77] Creating layer conv1_3
I0315 11:08:23.039713 11950 net.cpp:84] Creating Layer conv1_3
I0315 11:08:23.039717 11950 net.cpp:406] conv1_3 <- conv1_2
I0315 11:08:23.039723 11950 net.cpp:380] conv1_3 -> conv1_3
I0315 11:08:23.041738 11950 net.cpp:122] Setting up conv1_3
I0315 11:08:23.041752 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.041756 11950 net.cpp:137] Memory required for data: 1266158592
I0315 11:08:23.041764 11950 layer_factory.hpp:77] Creating layer relu1_3
I0315 11:08:23.041779 11950 net.cpp:84] Creating Layer relu1_3
I0315 11:08:23.041784 11950 net.cpp:406] relu1_3 <- conv1_3
I0315 11:08:23.041788 11950 net.cpp:367] relu1_3 -> conv1_3 (in-place)
I0315 11:08:23.041986 11950 net.cpp:122] Setting up relu1_3
I0315 11:08:23.041996 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.041999 11950 net.cpp:137] Memory required for data: 1442319360
I0315 11:08:23.042007 11950 layer_factory.hpp:77] Creating layer res1_3
I0315 11:08:23.042033 11950 net.cpp:84] Creating Layer res1_3
I0315 11:08:23.042037 11950 net.cpp:406] res1_3 <- conv1_1_relu1_1_0_split_1
I0315 11:08:23.042043 11950 net.cpp:406] res1_3 <- conv1_3
I0315 11:08:23.042048 11950 net.cpp:380] res1_3 -> res1_3
I0315 11:08:23.042080 11950 net.cpp:122] Setting up res1_3
I0315 11:08:23.042088 11950 net.cpp:129] Top shape: 256 64 56 48 (44040192)
I0315 11:08:23.042093 11950 net.cpp:137] Memory required for data: 1618480128
I0315 11:08:23.042095 11950 layer_factory.hpp:77] Creating layer conv2_1
I0315 11:08:23.042110 11950 net.cpp:84] Creating Layer conv2_1
I0315 11:08:23.042115 11950 net.cpp:406] conv2_1 <- res1_3
I0315 11:08:23.042120 11950 net.cpp:380] conv2_1 -> conv2_1
I0315 11:08:23.045675 11950 net.cpp:122] Setting up conv2_1
I0315 11:08:23.045691 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.045694 11950 net.cpp:137] Memory required for data: 1706560512
I0315 11:08:23.045701 11950 layer_factory.hpp:77] Creating layer relu2_1
I0315 11:08:23.045706 11950 net.cpp:84] Creating Layer relu2_1
I0315 11:08:23.045711 11950 net.cpp:406] relu2_1 <- conv2_1
I0315 11:08:23.045716 11950 net.cpp:367] relu2_1 -> conv2_1 (in-place)
I0315 11:08:23.045871 11950 net.cpp:122] Setting up relu2_1
I0315 11:08:23.045881 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.045884 11950 net.cpp:137] Memory required for data: 1794640896
I0315 11:08:23.045899 11950 layer_factory.hpp:77] Creating layer conv2_1_relu2_1_0_split
I0315 11:08:23.045905 11950 net.cpp:84] Creating Layer conv2_1_relu2_1_0_split
I0315 11:08:23.045908 11950 net.cpp:406] conv2_1_relu2_1_0_split <- conv2_1
I0315 11:08:23.045920 11950 net.cpp:380] conv2_1_relu2_1_0_split -> conv2_1_relu2_1_0_split_0
I0315 11:08:23.045928 11950 net.cpp:380] conv2_1_relu2_1_0_split -> conv2_1_relu2_1_0_split_1
I0315 11:08:23.045959 11950 net.cpp:122] Setting up conv2_1_relu2_1_0_split
I0315 11:08:23.045966 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.045970 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.045974 11950 net.cpp:137] Memory required for data: 1970801664
I0315 11:08:23.045976 11950 layer_factory.hpp:77] Creating layer conv2_2
I0315 11:08:23.045984 11950 net.cpp:84] Creating Layer conv2_2
I0315 11:08:23.045989 11950 net.cpp:406] conv2_2 <- conv2_1_relu2_1_0_split_0
I0315 11:08:23.045994 11950 net.cpp:380] conv2_2 -> conv2_2
I0315 11:08:23.050727 11950 net.cpp:122] Setting up conv2_2
I0315 11:08:23.050742 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.050746 11950 net.cpp:137] Memory required for data: 2058882048
I0315 11:08:23.050752 11950 layer_factory.hpp:77] Creating layer relu2_2
I0315 11:08:23.050767 11950 net.cpp:84] Creating Layer relu2_2
I0315 11:08:23.050783 11950 net.cpp:406] relu2_2 <- conv2_2
I0315 11:08:23.050789 11950 net.cpp:367] relu2_2 -> conv2_2 (in-place)
I0315 11:08:23.050937 11950 net.cpp:122] Setting up relu2_2
I0315 11:08:23.050947 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.050951 11950 net.cpp:137] Memory required for data: 2146962432
I0315 11:08:23.050956 11950 layer_factory.hpp:77] Creating layer conv2_3
I0315 11:08:23.050972 11950 net.cpp:84] Creating Layer conv2_3
I0315 11:08:23.050976 11950 net.cpp:406] conv2_3 <- conv2_2
I0315 11:08:23.050982 11950 net.cpp:380] conv2_3 -> conv2_3
I0315 11:08:23.056737 11950 net.cpp:122] Setting up conv2_3
I0315 11:08:23.056753 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.056757 11950 net.cpp:137] Memory required for data: 2235042816
I0315 11:08:23.056767 11950 layer_factory.hpp:77] Creating layer relu2_3
I0315 11:08:23.056774 11950 net.cpp:84] Creating Layer relu2_3
I0315 11:08:23.056778 11950 net.cpp:406] relu2_3 <- conv2_3
I0315 11:08:23.056783 11950 net.cpp:367] relu2_3 -> conv2_3 (in-place)
I0315 11:08:23.056927 11950 net.cpp:122] Setting up relu2_3
I0315 11:08:23.056936 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.056939 11950 net.cpp:137] Memory required for data: 2323123200
I0315 11:08:23.056944 11950 layer_factory.hpp:77] Creating layer res2_3
I0315 11:08:23.056959 11950 net.cpp:84] Creating Layer res2_3
I0315 11:08:23.056964 11950 net.cpp:406] res2_3 <- conv2_1_relu2_1_0_split_1
I0315 11:08:23.056969 11950 net.cpp:406] res2_3 <- conv2_3
I0315 11:08:23.056974 11950 net.cpp:380] res2_3 -> res2_3
I0315 11:08:23.056996 11950 net.cpp:122] Setting up res2_3
I0315 11:08:23.057003 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.057005 11950 net.cpp:137] Memory required for data: 2411203584
I0315 11:08:23.057008 11950 layer_factory.hpp:77] Creating layer res2_3_res2_3_0_split
I0315 11:08:23.057013 11950 net.cpp:84] Creating Layer res2_3_res2_3_0_split
I0315 11:08:23.057018 11950 net.cpp:406] res2_3_res2_3_0_split <- res2_3
I0315 11:08:23.057023 11950 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_0
I0315 11:08:23.057029 11950 net.cpp:380] res2_3_res2_3_0_split -> res2_3_res2_3_0_split_1
I0315 11:08:23.057056 11950 net.cpp:122] Setting up res2_3_res2_3_0_split
I0315 11:08:23.057062 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.057066 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.057070 11950 net.cpp:137] Memory required for data: 2587364352
I0315 11:08:23.057080 11950 layer_factory.hpp:77] Creating layer conv2_4
I0315 11:08:23.057090 11950 net.cpp:84] Creating Layer conv2_4
I0315 11:08:23.057096 11950 net.cpp:406] conv2_4 <- res2_3_res2_3_0_split_0
I0315 11:08:23.057102 11950 net.cpp:380] conv2_4 -> conv2_4
I0315 11:08:23.061331 11950 net.cpp:122] Setting up conv2_4
I0315 11:08:23.061342 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.061347 11950 net.cpp:137] Memory required for data: 2675444736
I0315 11:08:23.061352 11950 layer_factory.hpp:77] Creating layer relu2_4
I0315 11:08:23.061357 11950 net.cpp:84] Creating Layer relu2_4
I0315 11:08:23.061360 11950 net.cpp:406] relu2_4 <- conv2_4
I0315 11:08:23.061365 11950 net.cpp:367] relu2_4 -> conv2_4 (in-place)
I0315 11:08:23.061511 11950 net.cpp:122] Setting up relu2_4
I0315 11:08:23.061520 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.061523 11950 net.cpp:137] Memory required for data: 2763525120
I0315 11:08:23.061528 11950 layer_factory.hpp:77] Creating layer conv2_5
I0315 11:08:23.061545 11950 net.cpp:84] Creating Layer conv2_5
I0315 11:08:23.061550 11950 net.cpp:406] conv2_5 <- conv2_4
I0315 11:08:23.061556 11950 net.cpp:380] conv2_5 -> conv2_5
I0315 11:08:23.067370 11950 net.cpp:122] Setting up conv2_5
I0315 11:08:23.067385 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.067389 11950 net.cpp:137] Memory required for data: 2851605504
I0315 11:08:23.067395 11950 layer_factory.hpp:77] Creating layer relu2_5
I0315 11:08:23.067411 11950 net.cpp:84] Creating Layer relu2_5
I0315 11:08:23.067416 11950 net.cpp:406] relu2_5 <- conv2_5
I0315 11:08:23.067421 11950 net.cpp:367] relu2_5 -> conv2_5 (in-place)
I0315 11:08:23.067574 11950 net.cpp:122] Setting up relu2_5
I0315 11:08:23.067584 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.067586 11950 net.cpp:137] Memory required for data: 2939685888
I0315 11:08:23.067600 11950 layer_factory.hpp:77] Creating layer res2_5
I0315 11:08:23.067607 11950 net.cpp:84] Creating Layer res2_5
I0315 11:08:23.067611 11950 net.cpp:406] res2_5 <- res2_3_res2_3_0_split_1
I0315 11:08:23.067616 11950 net.cpp:406] res2_5 <- conv2_5
I0315 11:08:23.067621 11950 net.cpp:380] res2_5 -> res2_5
I0315 11:08:23.067646 11950 net.cpp:122] Setting up res2_5
I0315 11:08:23.067651 11950 net.cpp:129] Top shape: 256 128 28 24 (22020096)
I0315 11:08:23.067653 11950 net.cpp:137] Memory required for data: 3027766272
I0315 11:08:23.067657 11950 layer_factory.hpp:77] Creating layer conv3_1
I0315 11:08:23.067672 11950 net.cpp:84] Creating Layer conv3_1
I0315 11:08:23.067677 11950 net.cpp:406] conv3_1 <- res2_5
I0315 11:08:23.067682 11950 net.cpp:380] conv3_1 -> conv3_1
I0315 11:08:23.074049 11950 net.cpp:122] Setting up conv3_1
I0315 11:08:23.074064 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.074069 11950 net.cpp:137] Memory required for data: 3071806464
I0315 11:08:23.074074 11950 layer_factory.hpp:77] Creating layer relu3_1
I0315 11:08:23.074080 11950 net.cpp:84] Creating Layer relu3_1
I0315 11:08:23.074086 11950 net.cpp:406] relu3_1 <- conv3_1
I0315 11:08:23.074092 11950 net.cpp:367] relu3_1 -> conv3_1 (in-place)
I0315 11:08:23.074223 11950 net.cpp:122] Setting up relu3_1
I0315 11:08:23.074232 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.074236 11950 net.cpp:137] Memory required for data: 3115846656
I0315 11:08:23.074250 11950 layer_factory.hpp:77] Creating layer conv3_1_relu3_1_0_split
I0315 11:08:23.074256 11950 net.cpp:84] Creating Layer conv3_1_relu3_1_0_split
I0315 11:08:23.074260 11950 net.cpp:406] conv3_1_relu3_1_0_split <- conv3_1
I0315 11:08:23.074266 11950 net.cpp:380] conv3_1_relu3_1_0_split -> conv3_1_relu3_1_0_split_0
I0315 11:08:23.074273 11950 net.cpp:380] conv3_1_relu3_1_0_split -> conv3_1_relu3_1_0_split_1
I0315 11:08:23.074306 11950 net.cpp:122] Setting up conv3_1_relu3_1_0_split
I0315 11:08:23.074313 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.074318 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.074321 11950 net.cpp:137] Memory required for data: 3203927040
I0315 11:08:23.074324 11950 layer_factory.hpp:77] Creating layer conv3_2
I0315 11:08:23.074333 11950 net.cpp:84] Creating Layer conv3_2
I0315 11:08:23.074337 11950 net.cpp:406] conv3_2 <- conv3_1_relu3_1_0_split_0
I0315 11:08:23.074344 11950 net.cpp:380] conv3_2 -> conv3_2
I0315 11:08:23.091429 11950 net.cpp:122] Setting up conv3_2
I0315 11:08:23.091444 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.091449 11950 net.cpp:137] Memory required for data: 3247967232
I0315 11:08:23.091455 11950 layer_factory.hpp:77] Creating layer relu3_2
I0315 11:08:23.091470 11950 net.cpp:84] Creating Layer relu3_2
I0315 11:08:23.091475 11950 net.cpp:406] relu3_2 <- conv3_2
I0315 11:08:23.091487 11950 net.cpp:367] relu3_2 -> conv3_2 (in-place)
I0315 11:08:23.091627 11950 net.cpp:122] Setting up relu3_2
I0315 11:08:23.091637 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.091640 11950 net.cpp:137] Memory required for data: 3292007424
I0315 11:08:23.091645 11950 layer_factory.hpp:77] Creating layer conv3_3
I0315 11:08:23.091661 11950 net.cpp:84] Creating Layer conv3_3
I0315 11:08:23.091665 11950 net.cpp:406] conv3_3 <- conv3_2
I0315 11:08:23.091671 11950 net.cpp:380] conv3_3 -> conv3_3
I0315 11:08:23.108340 11950 net.cpp:122] Setting up conv3_3
I0315 11:08:23.108355 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.108358 11950 net.cpp:137] Memory required for data: 3336047616
I0315 11:08:23.108364 11950 layer_factory.hpp:77] Creating layer relu3_3
I0315 11:08:23.108381 11950 net.cpp:84] Creating Layer relu3_3
I0315 11:08:23.108386 11950 net.cpp:406] relu3_3 <- conv3_3
I0315 11:08:23.108392 11950 net.cpp:367] relu3_3 -> conv3_3 (in-place)
I0315 11:08:23.108530 11950 net.cpp:122] Setting up relu3_3
I0315 11:08:23.108539 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.108543 11950 net.cpp:137] Memory required for data: 3380087808
I0315 11:08:23.108553 11950 layer_factory.hpp:77] Creating layer res3_3
I0315 11:08:23.108568 11950 net.cpp:84] Creating Layer res3_3
I0315 11:08:23.108572 11950 net.cpp:406] res3_3 <- conv3_1_relu3_1_0_split_1
I0315 11:08:23.108577 11950 net.cpp:406] res3_3 <- conv3_3
I0315 11:08:23.108582 11950 net.cpp:380] res3_3 -> res3_3
I0315 11:08:23.108606 11950 net.cpp:122] Setting up res3_3
I0315 11:08:23.108611 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.108614 11950 net.cpp:137] Memory required for data: 3424128000
I0315 11:08:23.108618 11950 layer_factory.hpp:77] Creating layer res3_3_res3_3_0_split
I0315 11:08:23.108626 11950 net.cpp:84] Creating Layer res3_3_res3_3_0_split
I0315 11:08:23.108633 11950 net.cpp:406] res3_3_res3_3_0_split <- res3_3
I0315 11:08:23.108640 11950 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_0
I0315 11:08:23.108647 11950 net.cpp:380] res3_3_res3_3_0_split -> res3_3_res3_3_0_split_1
I0315 11:08:23.108680 11950 net.cpp:122] Setting up res3_3_res3_3_0_split
I0315 11:08:23.108686 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.108690 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.108693 11950 net.cpp:137] Memory required for data: 3512208384
I0315 11:08:23.108703 11950 layer_factory.hpp:77] Creating layer conv3_4
I0315 11:08:23.108713 11950 net.cpp:84] Creating Layer conv3_4
I0315 11:08:23.108716 11950 net.cpp:406] conv3_4 <- res3_3_res3_3_0_split_0
I0315 11:08:23.108723 11950 net.cpp:380] conv3_4 -> conv3_4
I0315 11:08:23.125412 11950 net.cpp:122] Setting up conv3_4
I0315 11:08:23.125427 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.125432 11950 net.cpp:137] Memory required for data: 3556248576
I0315 11:08:23.125438 11950 layer_factory.hpp:77] Creating layer relu3_4
I0315 11:08:23.125444 11950 net.cpp:84] Creating Layer relu3_4
I0315 11:08:23.125448 11950 net.cpp:406] relu3_4 <- conv3_4
I0315 11:08:23.125453 11950 net.cpp:367] relu3_4 -> conv3_4 (in-place)
I0315 11:08:23.125588 11950 net.cpp:122] Setting up relu3_4
I0315 11:08:23.125597 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.125600 11950 net.cpp:137] Memory required for data: 3600288768
I0315 11:08:23.125612 11950 layer_factory.hpp:77] Creating layer conv3_5
I0315 11:08:23.125632 11950 net.cpp:84] Creating Layer conv3_5
I0315 11:08:23.125635 11950 net.cpp:406] conv3_5 <- conv3_4
I0315 11:08:23.125643 11950 net.cpp:380] conv3_5 -> conv3_5
I0315 11:08:23.142313 11950 net.cpp:122] Setting up conv3_5
I0315 11:08:23.142328 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.142333 11950 net.cpp:137] Memory required for data: 3644328960
I0315 11:08:23.142338 11950 layer_factory.hpp:77] Creating layer relu3_5
I0315 11:08:23.142344 11950 net.cpp:84] Creating Layer relu3_5
I0315 11:08:23.142349 11950 net.cpp:406] relu3_5 <- conv3_5
I0315 11:08:23.142354 11950 net.cpp:367] relu3_5 -> conv3_5 (in-place)
I0315 11:08:23.142498 11950 net.cpp:122] Setting up relu3_5
I0315 11:08:23.142508 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.142511 11950 net.cpp:137] Memory required for data: 3688369152
I0315 11:08:23.142525 11950 layer_factory.hpp:77] Creating layer res3_5
I0315 11:08:23.142532 11950 net.cpp:84] Creating Layer res3_5
I0315 11:08:23.142535 11950 net.cpp:406] res3_5 <- res3_3_res3_3_0_split_1
I0315 11:08:23.142540 11950 net.cpp:406] res3_5 <- conv3_5
I0315 11:08:23.142545 11950 net.cpp:380] res3_5 -> res3_5
I0315 11:08:23.142568 11950 net.cpp:122] Setting up res3_5
I0315 11:08:23.142575 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.142588 11950 net.cpp:137] Memory required for data: 3732409344
I0315 11:08:23.142592 11950 layer_factory.hpp:77] Creating layer res3_5_res3_5_0_split
I0315 11:08:23.142597 11950 net.cpp:84] Creating Layer res3_5_res3_5_0_split
I0315 11:08:23.142601 11950 net.cpp:406] res3_5_res3_5_0_split <- res3_5
I0315 11:08:23.142606 11950 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_0
I0315 11:08:23.142616 11950 net.cpp:380] res3_5_res3_5_0_split -> res3_5_res3_5_0_split_1
I0315 11:08:23.142647 11950 net.cpp:122] Setting up res3_5_res3_5_0_split
I0315 11:08:23.142655 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.142659 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.142663 11950 net.cpp:137] Memory required for data: 3820489728
I0315 11:08:23.142665 11950 layer_factory.hpp:77] Creating layer conv3_6
I0315 11:08:23.142681 11950 net.cpp:84] Creating Layer conv3_6
I0315 11:08:23.142685 11950 net.cpp:406] conv3_6 <- res3_5_res3_5_0_split_0
I0315 11:08:23.142693 11950 net.cpp:380] conv3_6 -> conv3_6
I0315 11:08:23.159404 11950 net.cpp:122] Setting up conv3_6
I0315 11:08:23.159420 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.159425 11950 net.cpp:137] Memory required for data: 3864529920
I0315 11:08:23.159431 11950 layer_factory.hpp:77] Creating layer relu3_6
I0315 11:08:23.159446 11950 net.cpp:84] Creating Layer relu3_6
I0315 11:08:23.159451 11950 net.cpp:406] relu3_6 <- conv3_6
I0315 11:08:23.159457 11950 net.cpp:367] relu3_6 -> conv3_6 (in-place)
I0315 11:08:23.159601 11950 net.cpp:122] Setting up relu3_6
I0315 11:08:23.159610 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.159613 11950 net.cpp:137] Memory required for data: 3908570112
I0315 11:08:23.159618 11950 layer_factory.hpp:77] Creating layer conv3_7
I0315 11:08:23.159638 11950 net.cpp:84] Creating Layer conv3_7
I0315 11:08:23.159643 11950 net.cpp:406] conv3_7 <- conv3_6
I0315 11:08:23.159649 11950 net.cpp:380] conv3_7 -> conv3_7
I0315 11:08:23.176368 11950 net.cpp:122] Setting up conv3_7
I0315 11:08:23.176383 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.176388 11950 net.cpp:137] Memory required for data: 3952610304
I0315 11:08:23.176393 11950 layer_factory.hpp:77] Creating layer relu3_7
I0315 11:08:23.176399 11950 net.cpp:84] Creating Layer relu3_7
I0315 11:08:23.176403 11950 net.cpp:406] relu3_7 <- conv3_7
I0315 11:08:23.176409 11950 net.cpp:367] relu3_7 -> conv3_7 (in-place)
I0315 11:08:23.176553 11950 net.cpp:122] Setting up relu3_7
I0315 11:08:23.176563 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.176565 11950 net.cpp:137] Memory required for data: 3996650496
I0315 11:08:23.176570 11950 layer_factory.hpp:77] Creating layer res3_7
I0315 11:08:23.176576 11950 net.cpp:84] Creating Layer res3_7
I0315 11:08:23.176580 11950 net.cpp:406] res3_7 <- res3_5_res3_5_0_split_1
I0315 11:08:23.176585 11950 net.cpp:406] res3_7 <- conv3_7
I0315 11:08:23.176591 11950 net.cpp:380] res3_7 -> res3_7
I0315 11:08:23.176614 11950 net.cpp:122] Setting up res3_7
I0315 11:08:23.176618 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.176621 11950 net.cpp:137] Memory required for data: 4040690688
I0315 11:08:23.176625 11950 layer_factory.hpp:77] Creating layer res3_7_res3_7_0_split
I0315 11:08:23.176638 11950 net.cpp:84] Creating Layer res3_7_res3_7_0_split
I0315 11:08:23.176642 11950 net.cpp:406] res3_7_res3_7_0_split <- res3_7
I0315 11:08:23.176647 11950 net.cpp:380] res3_7_res3_7_0_split -> res3_7_res3_7_0_split_0
I0315 11:08:23.176653 11950 net.cpp:380] res3_7_res3_7_0_split -> res3_7_res3_7_0_split_1
I0315 11:08:23.176687 11950 net.cpp:122] Setting up res3_7_res3_7_0_split
I0315 11:08:23.176692 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.176697 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.176700 11950 net.cpp:137] Memory required for data: 4128771072
I0315 11:08:23.176704 11950 layer_factory.hpp:77] Creating layer conv3_8
I0315 11:08:23.176722 11950 net.cpp:84] Creating Layer conv3_8
I0315 11:08:23.176736 11950 net.cpp:406] conv3_8 <- res3_7_res3_7_0_split_0
I0315 11:08:23.176744 11950 net.cpp:380] conv3_8 -> conv3_8
I0315 11:08:23.193452 11950 net.cpp:122] Setting up conv3_8
I0315 11:08:23.193467 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.193470 11950 net.cpp:137] Memory required for data: 4172811264
I0315 11:08:23.193477 11950 layer_factory.hpp:77] Creating layer relu3_8
I0315 11:08:23.193483 11950 net.cpp:84] Creating Layer relu3_8
I0315 11:08:23.193487 11950 net.cpp:406] relu3_8 <- conv3_8
I0315 11:08:23.193492 11950 net.cpp:367] relu3_8 -> conv3_8 (in-place)
I0315 11:08:23.193635 11950 net.cpp:122] Setting up relu3_8
I0315 11:08:23.193645 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.193647 11950 net.cpp:137] Memory required for data: 4216851456
I0315 11:08:23.193652 11950 layer_factory.hpp:77] Creating layer conv3_9
I0315 11:08:23.193661 11950 net.cpp:84] Creating Layer conv3_9
I0315 11:08:23.193665 11950 net.cpp:406] conv3_9 <- conv3_8
I0315 11:08:23.193672 11950 net.cpp:380] conv3_9 -> conv3_9
I0315 11:08:23.209722 11950 net.cpp:122] Setting up conv3_9
I0315 11:08:23.209736 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.209739 11950 net.cpp:137] Memory required for data: 4260891648
I0315 11:08:23.209753 11950 layer_factory.hpp:77] Creating layer relu3_9
I0315 11:08:23.209760 11950 net.cpp:84] Creating Layer relu3_9
I0315 11:08:23.209764 11950 net.cpp:406] relu3_9 <- conv3_9
I0315 11:08:23.209769 11950 net.cpp:367] relu3_9 -> conv3_9 (in-place)
I0315 11:08:23.209913 11950 net.cpp:122] Setting up relu3_9
I0315 11:08:23.209921 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.209925 11950 net.cpp:137] Memory required for data: 4304931840
I0315 11:08:23.209929 11950 layer_factory.hpp:77] Creating layer res3_9
I0315 11:08:23.209944 11950 net.cpp:84] Creating Layer res3_9
I0315 11:08:23.209949 11950 net.cpp:406] res3_9 <- res3_7_res3_7_0_split_1
I0315 11:08:23.209952 11950 net.cpp:406] res3_9 <- conv3_9
I0315 11:08:23.209959 11950 net.cpp:380] res3_9 -> res3_9
I0315 11:08:23.209982 11950 net.cpp:122] Setting up res3_9
I0315 11:08:23.209987 11950 net.cpp:129] Top shape: 256 256 14 12 (11010048)
I0315 11:08:23.209990 11950 net.cpp:137] Memory required for data: 4348972032
I0315 11:08:23.209995 11950 layer_factory.hpp:77] Creating layer conv4_1
I0315 11:08:23.210002 11950 net.cpp:84] Creating Layer conv4_1
I0315 11:08:23.210006 11950 net.cpp:406] conv4_1 <- res3_9
I0315 11:08:23.210012 11950 net.cpp:380] conv4_1 -> conv4_1
I0315 11:08:23.219504 11950 net.cpp:122] Setting up conv4_1
I0315 11:08:23.219521 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.219525 11950 net.cpp:137] Memory required for data: 4370992128
I0315 11:08:23.219532 11950 layer_factory.hpp:77] Creating layer relu4_1
I0315 11:08:23.219537 11950 net.cpp:84] Creating Layer relu4_1
I0315 11:08:23.219542 11950 net.cpp:406] relu4_1 <- conv4_1
I0315 11:08:23.219547 11950 net.cpp:367] relu4_1 -> conv4_1 (in-place)
I0315 11:08:23.219668 11950 net.cpp:122] Setting up relu4_1
I0315 11:08:23.219677 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.219681 11950 net.cpp:137] Memory required for data: 4393012224
I0315 11:08:23.219684 11950 layer_factory.hpp:77] Creating layer conv4_1_relu4_1_0_split
I0315 11:08:23.219691 11950 net.cpp:84] Creating Layer conv4_1_relu4_1_0_split
I0315 11:08:23.219696 11950 net.cpp:406] conv4_1_relu4_1_0_split <- conv4_1
I0315 11:08:23.219700 11950 net.cpp:380] conv4_1_relu4_1_0_split -> conv4_1_relu4_1_0_split_0
I0315 11:08:23.219707 11950 net.cpp:380] conv4_1_relu4_1_0_split -> conv4_1_relu4_1_0_split_1
I0315 11:08:23.219739 11950 net.cpp:122] Setting up conv4_1_relu4_1_0_split
I0315 11:08:23.219745 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.219749 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.219753 11950 net.cpp:137] Memory required for data: 4437052416
I0315 11:08:23.219755 11950 layer_factory.hpp:77] Creating layer conv4_2
I0315 11:08:23.219785 11950 net.cpp:84] Creating Layer conv4_2
I0315 11:08:23.219790 11950 net.cpp:406] conv4_2 <- conv4_1_relu4_1_0_split_0
I0315 11:08:23.219796 11950 net.cpp:380] conv4_2 -> conv4_2
I0315 11:08:23.281510 11950 net.cpp:122] Setting up conv4_2
I0315 11:08:23.281529 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.281533 11950 net.cpp:137] Memory required for data: 4459072512
I0315 11:08:23.281539 11950 layer_factory.hpp:77] Creating layer relu4_2
I0315 11:08:23.281548 11950 net.cpp:84] Creating Layer relu4_2
I0315 11:08:23.281551 11950 net.cpp:406] relu4_2 <- conv4_2
I0315 11:08:23.281556 11950 net.cpp:367] relu4_2 -> conv4_2 (in-place)
I0315 11:08:23.281684 11950 net.cpp:122] Setting up relu4_2
I0315 11:08:23.281693 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.281697 11950 net.cpp:137] Memory required for data: 4481092608
I0315 11:08:23.281702 11950 layer_factory.hpp:77] Creating layer conv4_3
I0315 11:08:23.281718 11950 net.cpp:84] Creating Layer conv4_3
I0315 11:08:23.281723 11950 net.cpp:406] conv4_3 <- conv4_2
I0315 11:08:23.281730 11950 net.cpp:380] conv4_3 -> conv4_3
I0315 11:08:23.343734 11950 net.cpp:122] Setting up conv4_3
I0315 11:08:23.343753 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.343757 11950 net.cpp:137] Memory required for data: 4503112704
I0315 11:08:23.343763 11950 layer_factory.hpp:77] Creating layer relu4_3
I0315 11:08:23.343772 11950 net.cpp:84] Creating Layer relu4_3
I0315 11:08:23.343776 11950 net.cpp:406] relu4_3 <- conv4_3
I0315 11:08:23.343783 11950 net.cpp:367] relu4_3 -> conv4_3 (in-place)
I0315 11:08:23.345024 11950 net.cpp:122] Setting up relu4_3
I0315 11:08:23.345038 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.345042 11950 net.cpp:137] Memory required for data: 4525132800
I0315 11:08:23.345047 11950 layer_factory.hpp:77] Creating layer res4_3
I0315 11:08:23.345058 11950 net.cpp:84] Creating Layer res4_3
I0315 11:08:23.345062 11950 net.cpp:406] res4_3 <- conv4_1_relu4_1_0_split_1
I0315 11:08:23.345067 11950 net.cpp:406] res4_3 <- conv4_3
I0315 11:08:23.345072 11950 net.cpp:380] res4_3 -> res4_3
I0315 11:08:23.345118 11950 net.cpp:122] Setting up res4_3
I0315 11:08:23.345124 11950 net.cpp:129] Top shape: 256 512 7 6 (5505024)
I0315 11:08:23.345126 11950 net.cpp:137] Memory required for data: 4547152896
I0315 11:08:23.345130 11950 layer_factory.hpp:77] Creating layer fc5
I0315 11:08:23.345157 11950 net.cpp:84] Creating Layer fc5
I0315 11:08:23.345162 11950 net.cpp:406] fc5 <- res4_3
I0315 11:08:23.345167 11950 net.cpp:380] fc5 -> fc5
I0315 11:08:23.422861 11950 net.cpp:122] Setting up fc5
I0315 11:08:23.422935 11950 net.cpp:129] Top shape: 256 512 (131072)
I0315 11:08:23.422940 11950 net.cpp:137] Memory required for data: 4547677184
I0315 11:08:23.422958 11950 layer_factory.hpp:77] Creating layer fc6
I0315 11:08:23.422984 11950 net.cpp:84] Creating Layer fc6
I0315 11:08:23.422991 11950 net.cpp:406] fc6 <- fc5
I0315 11:08:23.423000 11950 net.cpp:406] fc6 <- label_data_1_split_0
I0315 11:08:23.423022 11950 net.cpp:380] fc6 -> fc6
I0315 11:08:23.423058 11950 net.cpp:380] fc6 -> lambda
I0315 11:08:23.460299 11950 net.cpp:122] Setting up fc6
I0315 11:08:23.460355 11950 net.cpp:129] Top shape: 256 10572 (2706432)
I0315 11:08:23.460361 11950 net.cpp:129] Top shape: 1 (1)
I0315 11:08:23.460364 11950 net.cpp:137] Memory required for data: 4558502916
I0315 11:08:23.460374 11950 layer_factory.hpp:77] Creating layer softmax_loss
I0315 11:08:23.460405 11950 net.cpp:84] Creating Layer softmax_loss
I0315 11:08:23.460412 11950 net.cpp:406] softmax_loss <- fc6
I0315 11:08:23.460419 11950 net.cpp:406] softmax_loss <- label_data_1_split_1
I0315 11:08:23.460443 11950 net.cpp:380] softmax_loss -> softmax_loss
I0315 11:08:23.460459 11950 layer_factory.hpp:77] Creating layer softmax_loss
I0315 11:08:23.468171 11950 net.cpp:122] Setting up softmax_loss
I0315 11:08:23.468191 11950 net.cpp:129] Top shape: (1)
I0315 11:08:23.468195 11950 net.cpp:132]     with loss weight 1
I0315 11:08:23.468291 11950 net.cpp:137] Memory required for data: 4558502920
I0315 11:08:23.468297 11950 net.cpp:198] softmax_loss needs backward computation.
I0315 11:08:23.468300 11950 net.cpp:198] fc6 needs backward computation.
I0315 11:08:23.468304 11950 net.cpp:198] fc5 needs backward computation.
I0315 11:08:23.468308 11950 net.cpp:198] res4_3 needs backward computation.
I0315 11:08:23.468313 11950 net.cpp:198] relu4_3 needs backward computation.
I0315 11:08:23.468317 11950 net.cpp:198] conv4_3 needs backward computation.
I0315 11:08:23.468320 11950 net.cpp:198] relu4_2 needs backward computation.
I0315 11:08:23.468325 11950 net.cpp:198] conv4_2 needs backward computation.
I0315 11:08:23.468329 11950 net.cpp:198] conv4_1_relu4_1_0_split needs backward computation.
I0315 11:08:23.468333 11950 net.cpp:198] relu4_1 needs backward computation.
I0315 11:08:23.468338 11950 net.cpp:198] conv4_1 needs backward computation.
I0315 11:08:23.468340 11950 net.cpp:198] res3_9 needs backward computation.
I0315 11:08:23.468345 11950 net.cpp:198] relu3_9 needs backward computation.
I0315 11:08:23.468349 11950 net.cpp:198] conv3_9 needs backward computation.
I0315 11:08:23.468354 11950 net.cpp:198] relu3_8 needs backward computation.
I0315 11:08:23.468358 11950 net.cpp:198] conv3_8 needs backward computation.
I0315 11:08:23.468363 11950 net.cpp:198] res3_7_res3_7_0_split needs backward computation.
I0315 11:08:23.468367 11950 net.cpp:198] res3_7 needs backward computation.
I0315 11:08:23.468371 11950 net.cpp:198] relu3_7 needs backward computation.
I0315 11:08:23.468375 11950 net.cpp:198] conv3_7 needs backward computation.
I0315 11:08:23.468379 11950 net.cpp:198] relu3_6 needs backward computation.
I0315 11:08:23.468384 11950 net.cpp:198] conv3_6 needs backward computation.
I0315 11:08:23.468386 11950 net.cpp:198] res3_5_res3_5_0_split needs backward computation.
I0315 11:08:23.468390 11950 net.cpp:198] res3_5 needs backward computation.
I0315 11:08:23.468395 11950 net.cpp:198] relu3_5 needs backward computation.
I0315 11:08:23.468400 11950 net.cpp:198] conv3_5 needs backward computation.
I0315 11:08:23.468403 11950 net.cpp:198] relu3_4 needs backward computation.
I0315 11:08:23.468407 11950 net.cpp:198] conv3_4 needs backward computation.
I0315 11:08:23.468411 11950 net.cpp:198] res3_3_res3_3_0_split needs backward computation.
I0315 11:08:23.468415 11950 net.cpp:198] res3_3 needs backward computation.
I0315 11:08:23.468420 11950 net.cpp:198] relu3_3 needs backward computation.
I0315 11:08:23.468425 11950 net.cpp:198] conv3_3 needs backward computation.
I0315 11:08:23.468430 11950 net.cpp:198] relu3_2 needs backward computation.
I0315 11:08:23.468433 11950 net.cpp:198] conv3_2 needs backward computation.
I0315 11:08:23.468437 11950 net.cpp:198] conv3_1_relu3_1_0_split needs backward computation.
I0315 11:08:23.468441 11950 net.cpp:198] relu3_1 needs backward computation.
I0315 11:08:23.468446 11950 net.cpp:198] conv3_1 needs backward computation.
I0315 11:08:23.468449 11950 net.cpp:198] res2_5 needs backward computation.
I0315 11:08:23.468453 11950 net.cpp:198] relu2_5 needs backward computation.
I0315 11:08:23.468457 11950 net.cpp:198] conv2_5 needs backward computation.
I0315 11:08:23.468462 11950 net.cpp:198] relu2_4 needs backward computation.
I0315 11:08:23.468466 11950 net.cpp:198] conv2_4 needs backward computation.
I0315 11:08:23.468469 11950 net.cpp:198] res2_3_res2_3_0_split needs backward computation.
I0315 11:08:23.468472 11950 net.cpp:198] res2_3 needs backward computation.
I0315 11:08:23.468477 11950 net.cpp:198] relu2_3 needs backward computation.
I0315 11:08:23.468480 11950 net.cpp:198] conv2_3 needs backward computation.
I0315 11:08:23.468483 11950 net.cpp:198] relu2_2 needs backward computation.
I0315 11:08:23.468487 11950 net.cpp:198] conv2_2 needs backward computation.
I0315 11:08:23.468490 11950 net.cpp:198] conv2_1_relu2_1_0_split needs backward computation.
I0315 11:08:23.468493 11950 net.cpp:198] relu2_1 needs backward computation.
I0315 11:08:23.468497 11950 net.cpp:198] conv2_1 needs backward computation.
I0315 11:08:23.468508 11950 net.cpp:198] res1_3 needs backward computation.
I0315 11:08:23.468513 11950 net.cpp:198] relu1_3 needs backward computation.
I0315 11:08:23.468515 11950 net.cpp:198] conv1_3 needs backward computation.
I0315 11:08:23.468519 11950 net.cpp:198] relu1_2 needs backward computation.
I0315 11:08:23.468523 11950 net.cpp:198] conv1_2 needs backward computation.
I0315 11:08:23.468525 11950 net.cpp:198] conv1_1_relu1_1_0_split needs backward computation.
I0315 11:08:23.468529 11950 net.cpp:198] relu1_1 needs backward computation.
I0315 11:08:23.468533 11950 net.cpp:198] conv1_1 needs backward computation.
I0315 11:08:23.468538 11950 net.cpp:200] label_data_1_split does not need backward computation.
I0315 11:08:23.468541 11950 net.cpp:200] data does not need backward computation.
I0315 11:08:23.468545 11950 net.cpp:242] This network produces output lambda
I0315 11:08:23.468549 11950 net.cpp:242] This network produces output softmax_loss
I0315 11:08:23.468591 11950 net.cpp:255] Network initialization done.
I0315 11:08:23.468860 11950 solver.cpp:56] Solver scaffolding done.
I0315 11:08:23.470926 11950 caffe.cpp:248] Starting Optimization
I0315 11:08:24.131170 11961 image_data_layer.cpp:38] Opening file data/CASIA-WebFace-112X96.txt
I0315 11:08:24.317317 11961 image_data_layer.cpp:53] Shuffling data
I0315 11:08:24.397723 11961 image_data_layer.cpp:63] A total of 452722 images.
I0315 11:08:24.400276 11961 image_data_layer.cpp:90] output data size: 256,3,112,96
I0315 11:08:25.441251 11950 solver.cpp:272] Solving SpherefaceNet-20
I0315 11:08:25.441318 11950 solver.cpp:273] Learning Rate Policy: multistep
I0315 11:08:25.455034 11950 net.cpp:591]     [Forward] Layer data, top blob data data: 0.44488
I0315 11:08:25.455124 11950 net.cpp:591]     [Forward] Layer data, top blob label data: 4239.83
I0315 11:08:25.455212 11950 net.cpp:591]     [Forward] Layer label_data_1_split, top blob label_data_1_split_0 data: 4239.83
I0315 11:08:25.455281 11950 net.cpp:591]     [Forward] Layer label_data_1_split, top blob label_data_1_split_1 data: 4239.83
I0315 11:08:25.460875 11950 net.cpp:591]     [Forward] Layer conv1_1, top blob conv1_1 data: 0
I0315 11:08:25.460968 11950 net.cpp:603]     [Forward] Layer conv1_1, param blob 0 data: 0.168128
I0315 11:08:25.461045 11950 net.cpp:603]     [Forward] Layer conv1_1, param blob 1 data: 0
I0315 11:08:25.468133 11950 net.cpp:591]     [Forward] Layer relu1_1, top blob conv1_1 data: 0
I0315 11:08:25.468230 11950 net.cpp:603]     [Forward] Layer relu1_1, param blob 0 data: 0.25
I0315 11:08:25.469084 11950 net.cpp:591]     [Forward] Layer conv1_1_relu1_1_0_split, top blob conv1_1_relu1_1_0_split_0 data: 0
I0315 11:08:25.469925 11950 net.cpp:591]     [Forward] Layer conv1_1_relu1_1_0_split, top blob conv1_1_relu1_1_0_split_1 data: 0
I0315 11:08:25.475119 11950 net.cpp:591]     [Forward] Layer conv1_2, top blob conv1_2 data: 0
I0315 11:08:25.475211 11950 net.cpp:603]     [Forward] Layer conv1_2, param blob 0 data: 0.00796481
I0315 11:08:25.475288 11950 net.cpp:603]     [Forward] Layer conv1_2, param blob 1 data: 0
I0315 11:08:25.482187 11950 net.cpp:591]     [Forward] Layer relu1_2, top blob conv1_2 data: 0
I0315 11:08:25.482278 11950 net.cpp:603]     [Forward] Layer relu1_2, param blob 0 data: 0.25
I0315 11:08:25.487435 11950 net.cpp:591]     [Forward] Layer conv1_3, top blob conv1_3 data: 0
I0315 11:08:25.487522 11950 net.cpp:603]     [Forward] Layer conv1_3, param blob 0 data: 0.00796308
I0315 11:08:25.487602 11950 net.cpp:603]     [Forward] Layer conv1_3, param blob 1 data: 0
I0315 11:08:25.494400 11950 net.cpp:591]     [Forward] Layer relu1_3, top blob conv1_3 data: 0
I0315 11:08:25.494487 11950 net.cpp:603]     [Forward] Layer relu1_3, param blob 0 data: 0.25
I0315 11:08:25.503013 11950 net.cpp:591]     [Forward] Layer res1_3, top blob res1_3 data: 0
I0315 11:08:25.506372 11950 net.cpp:591]     [Forward] Layer conv2_1, top blob conv2_1 data: 0
I0315 11:08:25.506456 11950 net.cpp:603]     [Forward] Layer conv2_1, param blob 0 data: 0.0360487
I0315 11:08:25.506517 11950 net.cpp:603]     [Forward] Layer conv2_1, param blob 1 data: 0
I0315 11:08:25.510627 11950 net.cpp:591]     [Forward] Layer relu2_1, top blob conv2_1 data: 0
I0315 11:08:25.510707 11950 net.cpp:603]     [Forward] Layer relu2_1, param blob 0 data: 0.25
I0315 11:08:25.511135 11950 net.cpp:591]     [Forward] Layer conv2_1_relu2_1_0_split, top blob conv2_1_relu2_1_0_split_0 data: 0
I0315 11:08:25.511554 11950 net.cpp:591]     [Forward] Layer conv2_1_relu2_1_0_split, top blob conv2_1_relu2_1_0_split_1 data: 0
I0315 11:08:25.514693 11950 net.cpp:591]     [Forward] Layer conv2_2, top blob conv2_2 data: 0
I0315 11:08:25.514771 11950 net.cpp:603]     [Forward] Layer conv2_2, param blob 0 data: 0.00795091
I0315 11:08:25.514830 11950 net.cpp:603]     [Forward] Layer conv2_2, param blob 1 data: 0
I0315 11:08:25.518846 11950 net.cpp:591]     [Forward] Layer relu2_2, top blob conv2_2 data: 0
I0315 11:08:25.518942 11950 net.cpp:603]     [Forward] Layer relu2_2, param blob 0 data: 0.25
I0315 11:08:25.522125 11950 net.cpp:591]     [Forward] Layer conv2_3, top blob conv2_3 data: 0
I0315 11:08:25.522203 11950 net.cpp:603]     [Forward] Layer conv2_3, param blob 0 data: 0.00797117
I0315 11:08:25.522259 11950 net.cpp:603]     [Forward] Layer conv2_3, param blob 1 data: 0
I0315 11:08:25.526121 11950 net.cpp:591]     [Forward] Layer relu2_3, top blob conv2_3 data: 0
I0315 11:08:25.526195 11950 net.cpp:603]     [Forward] Layer relu2_3, param blob 0 data: 0.25
I0315 11:08:25.530987 11950 net.cpp:591]     [Forward] Layer res2_3, top blob res2_3 data: 0
I0315 11:08:25.531431 11950 net.cpp:591]     [Forward] Layer res2_3_res2_3_0_split, top blob res2_3_res2_3_0_split_0 data: 0
I0315 11:08:25.531852 11950 net.cpp:591]     [Forward] Layer res2_3_res2_3_0_split, top blob res2_3_res2_3_0_split_1 data: 0
I0315 11:08:25.535022 11950 net.cpp:591]     [Forward] Layer conv2_4, top blob conv2_4 data: 0
I0315 11:08:25.535109 11950 net.cpp:603]     [Forward] Layer conv2_4, param blob 0 data: 0.00797199
I0315 11:08:25.535176 11950 net.cpp:603]     [Forward] Layer conv2_4, param blob 1 data: 0
I0315 11:08:25.539007 11950 net.cpp:591]     [Forward] Layer relu2_4, top blob conv2_4 data: 0
I0315 11:08:25.539080 11950 net.cpp:603]     [Forward] Layer relu2_4, param blob 0 data: 0.25
I0315 11:08:25.542259 11950 net.cpp:591]     [Forward] Layer conv2_5, top blob conv2_5 data: 0
I0315 11:08:25.542330 11950 net.cpp:603]     [Forward] Layer conv2_5, param blob 0 data: 0.00798824
I0315 11:08:25.542385 11950 net.cpp:603]     [Forward] Layer conv2_5, param blob 1 data: 0
I0315 11:08:25.546180 11950 net.cpp:591]     [Forward] Layer relu2_5, top blob conv2_5 data: 0
I0315 11:08:25.546252 11950 net.cpp:603]     [Forward] Layer relu2_5, param blob 0 data: 0.25
I0315 11:08:25.550938 11950 net.cpp:591]     [Forward] Layer res2_5, top blob res2_5 data: 0
I0315 11:08:25.553053 11950 net.cpp:591]     [Forward] Layer conv3_1, top blob conv3_1 data: 0
I0315 11:08:25.553124 11950 net.cpp:603]     [Forward] Layer conv3_1, param blob 0 data: 0.0255338
I0315 11:08:25.553179 11950 net.cpp:603]     [Forward] Layer conv3_1, param blob 1 data: 0
I0315 11:08:25.555686 11950 net.cpp:591]     [Forward] Layer relu3_1, top blob conv3_1 data: 0
I0315 11:08:25.555759 11950 net.cpp:603]     [Forward] Layer relu3_1, param blob 0 data: 0.25
I0315 11:08:25.555996 11950 net.cpp:591]     [Forward] Layer conv3_1_relu3_1_0_split, top blob conv3_1_relu3_1_0_split_0 data: 0
I0315 11:08:25.556227 11950 net.cpp:591]     [Forward] Layer conv3_1_relu3_1_0_split, top blob conv3_1_relu3_1_0_split_1 data: 0
I0315 11:08:25.558290 11950 net.cpp:591]     [Forward] Layer conv3_2, top blob conv3_2 data: 0
I0315 11:08:25.558369 11950 net.cpp:603]     [Forward] Layer conv3_2, param blob 0 data: 0.0079777
I0315 11:08:25.558424 11950 net.cpp:603]     [Forward] Layer conv3_2, param blob 1 data: 0
I0315 11:08:25.560906 11950 net.cpp:591]     [Forward] Layer relu3_2, top blob conv3_2 data: 0
I0315 11:08:25.560976 11950 net.cpp:603]     [Forward] Layer relu3_2, param blob 0 data: 0.25
I0315 11:08:25.563031 11950 net.cpp:591]     [Forward] Layer conv3_3, top blob conv3_3 data: 0
I0315 11:08:25.563118 11950 net.cpp:603]     [Forward] Layer conv3_3, param blob 0 data: 0.00796582
I0315 11:08:25.563175 11950 net.cpp:603]     [Forward] Layer conv3_3, param blob 1 data: 0
I0315 11:08:25.565608 11950 net.cpp:591]     [Forward] Layer relu3_3, top blob conv3_3 data: 0
I0315 11:08:25.565677 11950 net.cpp:603]     [Forward] Layer relu3_3, param blob 0 data: 0.25
I0315 11:08:25.568531 11950 net.cpp:591]     [Forward] Layer res3_3, top blob res3_3 data: 0
I0315 11:08:25.568783 11950 net.cpp:591]     [Forward] Layer res3_3_res3_3_0_split, top blob res3_3_res3_3_0_split_0 data: 0
I0315 11:08:25.569017 11950 net.cpp:591]     [Forward] Layer res3_3_res3_3_0_split, top blob res3_3_res3_3_0_split_1 data: 0
I0315 11:08:25.571022 11950 net.cpp:591]     [Forward] Layer conv3_4, top blob conv3_4 data: 0
I0315 11:08:25.571092 11950 net.cpp:603]     [Forward] Layer conv3_4, param blob 0 data: 0.00797946
I0315 11:08:25.571146 11950 net.cpp:603]     [Forward] Layer conv3_4, param blob 1 data: 0
I0315 11:08:25.573557 11950 net.cpp:591]     [Forward] Layer relu3_4, top blob conv3_4 data: 0
I0315 11:08:25.573626 11950 net.cpp:603]     [Forward] Layer relu3_4, param blob 0 data: 0.25
I0315 11:08:25.575599 11950 net.cpp:591]     [Forward] Layer conv3_5, top blob conv3_5 data: 0
I0315 11:08:25.575675 11950 net.cpp:603]     [Forward] Layer conv3_5, param blob 0 data: 0.00798108
I0315 11:08:25.575726 11950 net.cpp:603]     [Forward] Layer conv3_5, param blob 1 data: 0
I0315 11:08:25.578054 11950 net.cpp:591]     [Forward] Layer relu3_5, top blob conv3_5 data: 0
I0315 11:08:25.578121 11950 net.cpp:603]     [Forward] Layer relu3_5, param blob 0 data: 0.25
I0315 11:08:25.580909 11950 net.cpp:591]     [Forward] Layer res3_5, top blob res3_5 data: 0
I0315 11:08:25.581153 11950 net.cpp:591]     [Forward] Layer res3_5_res3_5_0_split, top blob res3_5_res3_5_0_split_0 data: 0
I0315 11:08:25.581385 11950 net.cpp:591]     [Forward] Layer res3_5_res3_5_0_split, top blob res3_5_res3_5_0_split_1 data: 0
I0315 11:08:25.583519 11950 net.cpp:591]     [Forward] Layer conv3_6, top blob conv3_6 data: 0
I0315 11:08:25.583595 11950 net.cpp:603]     [Forward] Layer conv3_6, param blob 0 data: 0.00797484
I0315 11:08:25.583648 11950 net.cpp:603]     [Forward] Layer conv3_6, param blob 1 data: 0
I0315 11:08:25.585988 11950 net.cpp:591]     [Forward] Layer relu3_6, top blob conv3_6 data: 0
I0315 11:08:25.586056 11950 net.cpp:603]     [Forward] Layer relu3_6, param blob 0 data: 0.25
I0315 11:08:25.588062 11950 net.cpp:591]     [Forward] Layer conv3_7, top blob conv3_7 data: 0
I0315 11:08:25.588136 11950 net.cpp:603]     [Forward] Layer conv3_7, param blob 0 data: 0.00796863
I0315 11:08:25.588188 11950 net.cpp:603]     [Forward] Layer conv3_7, param blob 1 data: 0
I0315 11:08:25.590622 11950 net.cpp:591]     [Forward] Layer relu3_7, top blob conv3_7 data: 0
I0315 11:08:25.590689 11950 net.cpp:603]     [Forward] Layer relu3_7, param blob 0 data: 0.25
I0315 11:08:25.593466 11950 net.cpp:591]     [Forward] Layer res3_7, top blob res3_7 data: 0
I0315 11:08:25.593719 11950 net.cpp:591]     [Forward] Layer res3_7_res3_7_0_split, top blob res3_7_res3_7_0_split_0 data: 0
I0315 11:08:25.593950 11950 net.cpp:591]     [Forward] Layer res3_7_res3_7_0_split, top blob res3_7_res3_7_0_split_1 data: 0
I0315 11:08:25.595870 11950 net.cpp:591]     [Forward] Layer conv3_8, top blob conv3_8 data: 0
I0315 11:08:25.595950 11950 net.cpp:603]     [Forward] Layer conv3_8, param blob 0 data: 0.00798183
I0315 11:08:25.596017 11950 net.cpp:603]     [Forward] Layer conv3_8, param blob 1 data: 0
I0315 11:08:25.602036 11950 net.cpp:591]     [Forward] Layer relu3_8, top blob conv3_8 data: 0
I0315 11:08:25.602159 11950 net.cpp:603]     [Forward] Layer relu3_8, param blob 0 data: 0.25
I0315 11:08:25.604939 11950 net.cpp:591]     [Forward] Layer conv3_9, top blob conv3_9 data: 0
I0315 11:08:25.605036 11950 net.cpp:603]     [Forward] Layer conv3_9, param blob 0 data: 0.00798614
I0315 11:08:25.605113 11950 net.cpp:603]     [Forward] Layer conv3_9, param blob 1 data: 0
I0315 11:08:25.608140 11950 net.cpp:591]     [Forward] Layer relu3_9, top blob conv3_9 data: 0
I0315 11:08:25.608232 11950 net.cpp:603]     [Forward] Layer relu3_9, param blob 0 data: 0.25
I0315 11:08:25.611748 11950 net.cpp:591]     [Forward] Layer res3_9, top blob res3_9 data: 0
I0315 11:08:25.614158 11950 net.cpp:591]     [Forward] Layer conv4_1, top blob conv4_1 data: 0
I0315 11:08:25.614246 11950 net.cpp:603]     [Forward] Layer conv4_1, param blob 0 data: 0.0180296
I0315 11:08:25.614300 11950 net.cpp:603]     [Forward] Layer conv4_1, param blob 1 data: 0
I0315 11:08:25.616585 11950 net.cpp:591]     [Forward] Layer relu4_1, top blob conv4_1 data: 0
I0315 11:08:25.616662 11950 net.cpp:603]     [Forward] Layer relu4_1, param blob 0 data: 0.25
I0315 11:08:25.616806 11950 net.cpp:591]     [Forward] Layer conv4_1_relu4_1_0_split, top blob conv4_1_relu4_1_0_split_0 data: 0
I0315 11:08:25.616945 11950 net.cpp:591]     [Forward] Layer conv4_1_relu4_1_0_split, top blob conv4_1_relu4_1_0_split_1 data: 0
I0315 11:08:25.619177 11950 net.cpp:591]     [Forward] Layer conv4_2, top blob conv4_2 data: 0
I0315 11:08:25.619309 11950 net.cpp:603]     [Forward] Layer conv4_2, param blob 0 data: 0.00797674
I0315 11:08:25.619391 11950 net.cpp:603]     [Forward] Layer conv4_2, param blob 1 data: 0
I0315 11:08:25.621994 11950 net.cpp:591]     [Forward] Layer relu4_2, top blob conv4_2 data: 0
I0315 11:08:25.622099 11950 net.cpp:603]     [Forward] Layer relu4_2, param blob 0 data: 0.25
I0315 11:08:25.624644 11950 net.cpp:591]     [Forward] Layer conv4_3, top blob conv4_3 data: 0
I0315 11:08:25.624774 11950 net.cpp:603]     [Forward] Layer conv4_3, param blob 0 data: 0.00797456
I0315 11:08:25.624860 11950 net.cpp:603]     [Forward] Layer conv4_3, param blob 1 data: 0
I0315 11:08:25.627444 11950 net.cpp:591]     [Forward] Layer relu4_3, top blob conv4_3 data: 0
I0315 11:08:25.627548 11950 net.cpp:603]     [Forward] Layer relu4_3, param blob 0 data: 0.25
I0315 11:08:25.630362 11950 net.cpp:591]     [Forward] Layer res4_3, top blob res4_3 data: 0
I0315 11:08:25.632479 11950 net.cpp:591]     [Forward] Layer fc5, top blob fc5 data: 0
I0315 11:08:25.632745 11950 net.cpp:603]     [Forward] Layer fc5, param blob 0 data: 0.00590581
I0315 11:08:25.632899 11950 net.cpp:603]     [Forward] Layer fc5, param blob 1 data: 0
I0315 11:08:25.649562 11950 net.cpp:591]     [Forward] Layer fc6, top blob fc6 data: nan
I0315 11:08:25.649607 11950 net.cpp:591]     [Forward] Layer fc6, top blob lambda data: 892.857
I0315 11:08:25.649758 11950 net.cpp:603]     [Forward] Layer fc6, param blob 0 data: 0.0382762
I0315 11:08:25.655622 11950 net.cpp:591]     [Forward] Layer softmax_loss, top blob softmax_loss data: 87.3365
I0315 11:08:25.655975 11950 net.cpp:619]     [Backward] Layer softmax_loss, bottom blob fc6 diff: nan
I0315 11:08:25.671777 11950 net.cpp:619]     [Backward] Layer fc6, bottom blob fc5 diff: nan
I0315 11:08:25.671934 11950 net.cpp:630]     [Backward] Layer fc6, param blob 0 diff: nan
I0315 11:08:25.676364 11950 net.cpp:619]     [Backward] Layer fc5, bottom blob res4_3 diff: nan
I0315 11:08:25.677337 11950 net.cpp:630]     [Backward] Layer fc5, param blob 0 diff: nan
I0315 11:08:25.677492 11950 net.cpp:630]     [Backward] Layer fc5, param blob 1 diff: nan
I0315 11:08:25.684801 11950 net.cpp:619]     [Backward] Layer res4_3, bottom blob conv4_1_relu4_1_0_split_1 diff: nan
I0315 11:08:25.685173 11950 net.cpp:619]     [Backward] Layer res4_3, bottom blob conv4_3 diff: nan
I0315 11:08:25.687925 11950 net.cpp:619]     [Backward] Layer relu4_3, bottom blob conv4_3 diff: nan
I0315 11:08:25.688014 11950 net.cpp:630]     [Backward] Layer relu4_3, param blob 0 diff: nan
I0315 11:08:25.690428 11950 net.cpp:619]     [Backward] Layer conv4_3, bottom blob conv4_2 diff: 0
I0315 11:08:25.690546 11950 net.cpp:630]     [Backward] Layer conv4_3, param blob 0 diff: 0
I0315 11:08:25.691568 11950 net.cpp:619]     [Backward] Layer relu4_2, bottom blob conv4_2 diff: 0
I0315 11:08:25.691666 11950 net.cpp:630]     [Backward] Layer relu4_2, param blob 0 diff: 0
I0315 11:08:25.694039 11950 net.cpp:619]     [Backward] Layer conv4_2, bottom blob conv4_1_relu4_1_0_split_0 diff: 0
I0315 11:08:25.694176 11950 net.cpp:630]     [Backward] Layer conv4_2, param blob 0 diff: 0
I0315 11:08:25.696651 11950 net.cpp:619]     [Backward] Layer conv4_1_relu4_1_0_split, bottom blob conv4_1 diff: nan
I0315 11:08:25.697362 11950 net.cpp:619]     [Backward] Layer relu4_1, bottom blob conv4_1 diff: nan
I0315 11:08:25.697428 11950 net.cpp:630]     [Backward] Layer relu4_1, param blob 0 diff: nan
I0315 11:08:25.725559 11950 net.cpp:619]     [Backward] Layer conv4_1, bottom blob res3_9 diff: nan
I0315 11:08:25.725649 11950 net.cpp:630]     [Backward] Layer conv4_1, param blob 0 diff: 0
I0315 11:08:25.725721 11950 net.cpp:630]     [Backward] Layer conv4_1, param blob 1 diff: nan
I0315 11:08:25.730612 11950 net.cpp:619]     [Backward] Layer res3_9, bottom blob res3_7_res3_7_0_split_1 diff: nan
I0315 11:08:25.730877 11950 net.cpp:619]     [Backward] Layer res3_9, bottom blob conv3_9 diff: nan
I0315 11:08:25.732214 11950 net.cpp:619]     [Backward] Layer relu3_9, bottom blob conv3_9 diff: nan
I0315 11:08:25.732287 11950 net.cpp:630]     [Backward] Layer relu3_9, param blob 0 diff: nan
I0315 11:08:25.734536 11950 net.cpp:619]     [Backward] Layer conv3_9, bottom blob conv3_8 diff: 0
I0315 11:08:25.734621 11950 net.cpp:630]     [Backward] Layer conv3_9, param blob 0 diff: 0
I0315 11:08:25.736027 11950 net.cpp:619]     [Backward] Layer relu3_8, bottom blob conv3_8 diff: 0
I0315 11:08:25.736099 11950 net.cpp:630]     [Backward] Layer relu3_8, param blob 0 diff: 0
I0315 11:08:25.738459 11950 net.cpp:619]     [Backward] Layer conv3_8, bottom blob res3_7_res3_7_0_split_0 diff: 0
I0315 11:08:25.738555 11950 net.cpp:630]     [Backward] Layer conv3_8, param blob 0 diff: 0
I0315 11:08:25.741384 11950 net.cpp:619]     [Backward] Layer res3_7_res3_7_0_split, bottom blob res3_7 diff: nan
I0315 11:08:25.745432 11950 net.cpp:619]     [Backward] Layer res3_7, bottom blob res3_5_res3_5_0_split_1 diff: nan
I0315 11:08:25.745700 11950 net.cpp:619]     [Backward] Layer res3_7, bottom blob conv3_7 diff: nan
I0315 11:08:25.747017 11950 net.cpp:619]     [Backward] Layer relu3_7, bottom blob conv3_7 diff: nan
I0315 11:08:25.747095 11950 net.cpp:630]     [Backward] Layer relu3_7, param blob 0 diff: nan
I0315 11:08:25.749466 11950 net.cpp:619]     [Backward] Layer conv3_7, bottom blob conv3_6 diff: 0
I0315 11:08:25.749583 11950 net.cpp:630]     [Backward] Layer conv3_7, param blob 0 diff: 0
I0315 11:08:25.751052 11950 net.cpp:619]     [Backward] Layer relu3_6, bottom blob conv3_6 diff: 0
I0315 11:08:25.751132 11950 net.cpp:630]     [Backward] Layer relu3_6, param blob 0 diff: 0
I0315 11:08:25.753417 11950 net.cpp:619]     [Backward] Layer conv3_6, bottom blob res3_5_res3_5_0_split_0 diff: 0
I0315 11:08:25.753509 11950 net.cpp:630]     [Backward] Layer conv3_6, param blob 0 diff: 0
I0315 11:08:25.756314 11950 net.cpp:619]     [Backward] Layer res3_5_res3_5_0_split, bottom blob res3_5 diff: nan
I0315 11:08:25.759999 11950 net.cpp:619]     [Backward] Layer res3_5, bottom blob res3_3_res3_3_0_split_1 diff: nan
I0315 11:08:25.760260 11950 net.cpp:619]     [Backward] Layer res3_5, bottom blob conv3_5 diff: nan
I0315 11:08:25.762676 11950 net.cpp:619]     [Backward] Layer relu3_5, bottom blob conv3_5 diff: nan
I0315 11:08:25.762763 11950 net.cpp:630]     [Backward] Layer relu3_5, param blob 0 diff: nan
I0315 11:08:25.770011 11950 net.cpp:619]     [Backward] Layer conv3_5, bottom blob conv3_4 diff: 0
I0315 11:08:25.770107 11950 net.cpp:630]     [Backward] Layer conv3_5, param blob 0 diff: 0
I0315 11:08:25.771533 11950 net.cpp:619]     [Backward] Layer relu3_4, bottom blob conv3_4 diff: 0
I0315 11:08:25.771611 11950 net.cpp:630]     [Backward] Layer relu3_4, param blob 0 diff: 0
I0315 11:08:25.773836 11950 net.cpp:619]     [Backward] Layer conv3_4, bottom blob res3_3_res3_3_0_split_0 diff: 0
I0315 11:08:25.773924 11950 net.cpp:630]     [Backward] Layer conv3_4, param blob 0 diff: 0
I0315 11:08:25.776639 11950 net.cpp:619]     [Backward] Layer res3_3_res3_3_0_split, bottom blob res3_3 diff: nan
I0315 11:08:25.780270 11950 net.cpp:619]     [Backward] Layer res3_3, bottom blob conv3_1_relu3_1_0_split_1 diff: nan
I0315 11:08:25.780531 11950 net.cpp:619]     [Backward] Layer res3_3, bottom blob conv3_3 diff: nan
I0315 11:08:25.781829 11950 net.cpp:619]     [Backward] Layer relu3_3, bottom blob conv3_3 diff: nan
I0315 11:08:25.781900 11950 net.cpp:630]     [Backward] Layer relu3_3, param blob 0 diff: nan
I0315 11:08:25.784045 11950 net.cpp:619]     [Backward] Layer conv3_3, bottom blob conv3_2 diff: 0
I0315 11:08:25.784135 11950 net.cpp:630]     [Backward] Layer conv3_3, param blob 0 diff: 0
I0315 11:08:25.785527 11950 net.cpp:619]     [Backward] Layer relu3_2, bottom blob conv3_2 diff: 0
I0315 11:08:25.785605 11950 net.cpp:630]     [Backward] Layer relu3_2, param blob 0 diff: 0
I0315 11:08:25.788274 11950 net.cpp:619]     [Backward] Layer conv3_2, bottom blob conv3_1_relu3_1_0_split_0 diff: 0
I0315 11:08:25.788378 11950 net.cpp:630]     [Backward] Layer conv3_2, param blob 0 diff: 0
I0315 11:08:25.791115 11950 net.cpp:619]     [Backward] Layer conv3_1_relu3_1_0_split, bottom blob conv3_1 diff: nan
I0315 11:08:25.792434 11950 net.cpp:619]     [Backward] Layer relu3_1, bottom blob conv3_1 diff: nan
I0315 11:08:25.792508 11950 net.cpp:630]     [Backward] Layer relu3_1, param blob 0 diff: nan
I0315 11:08:25.825995 11950 net.cpp:619]     [Backward] Layer conv3_1, bottom blob res2_5 diff: nan
I0315 11:08:25.826079 11950 net.cpp:630]     [Backward] Layer conv3_1, param blob 0 diff: 0
I0315 11:08:25.826148 11950 net.cpp:630]     [Backward] Layer conv3_1, param blob 1 diff: nan
I0315 11:08:25.834389 11950 net.cpp:619]     [Backward] Layer res2_5, bottom blob res2_3_res2_3_0_split_1 diff: nan
I0315 11:08:25.834833 11950 net.cpp:619]     [Backward] Layer res2_5, bottom blob conv2_5 diff: nan
I0315 11:08:25.837211 11950 net.cpp:619]     [Backward] Layer relu2_5, bottom blob conv2_5 diff: nan
I0315 11:08:25.837297 11950 net.cpp:630]     [Backward] Layer relu2_5, param blob 0 diff: nan
I0315 11:08:25.839736 11950 net.cpp:619]     [Backward] Layer conv2_5, bottom blob conv2_4 diff: 0
I0315 11:08:25.839819 11950 net.cpp:630]     [Backward] Layer conv2_5, param blob 0 diff: 0
I0315 11:08:25.842298 11950 net.cpp:619]     [Backward] Layer relu2_4, bottom blob conv2_4 diff: 0
I0315 11:08:25.842370 11950 net.cpp:630]     [Backward] Layer relu2_4, param blob 0 diff: 0
I0315 11:08:25.850472 11950 net.cpp:619]     [Backward] Layer conv2_4, bottom blob res2_3_res2_3_0_split_0 diff: 0
I0315 11:08:25.850555 11950 net.cpp:630]     [Backward] Layer conv2_4, param blob 0 diff: 0
I0315 11:08:25.854145 11950 net.cpp:619]     [Backward] Layer res2_3_res2_3_0_split, bottom blob res2_3 diff: nan
I0315 11:08:25.858965 11950 net.cpp:619]     [Backward] Layer res2_3, bottom blob conv2_1_relu2_1_0_split_1 diff: nan
I0315 11:08:25.859412 11950 net.cpp:619]     [Backward] Layer res2_3, bottom blob conv2_3 diff: nan
I0315 11:08:25.861799 11950 net.cpp:619]     [Backward] Layer relu2_3, bottom blob conv2_3 diff: nan
I0315 11:08:25.861868 11950 net.cpp:630]     [Backward] Layer relu2_3, param blob 0 diff: nan
I0315 11:08:25.864426 11950 net.cpp:619]     [Backward] Layer conv2_3, bottom blob conv2_2 diff: 0
I0315 11:08:25.864511 11950 net.cpp:630]     [Backward] Layer conv2_3, param blob 0 diff: 0
I0315 11:08:25.866966 11950 net.cpp:619]     [Backward] Layer relu2_2, bottom blob conv2_2 diff: 0
I0315 11:08:25.867040 11950 net.cpp:630]     [Backward] Layer relu2_2, param blob 0 diff: 0
I0315 11:08:25.869474 11950 net.cpp:619]     [Backward] Layer conv2_2, bottom blob conv2_1_relu2_1_0_split_0 diff: 0
I0315 11:08:25.869556 11950 net.cpp:630]     [Backward] Layer conv2_2, param blob 0 diff: 0
I0315 11:08:25.872987 11950 net.cpp:619]     [Backward] Layer conv2_1_relu2_1_0_split, bottom blob conv2_1 diff: nan
I0315 11:08:25.875388 11950 net.cpp:619]     [Backward] Layer relu2_1, bottom blob conv2_1 diff: nan
I0315 11:08:25.875459 11950 net.cpp:630]     [Backward] Layer relu2_1, param blob 0 diff: nan
I0315 11:08:25.959208 11950 net.cpp:619]     [Backward] Layer conv2_1, bottom blob res1_3 diff: nan
I0315 11:08:25.959291 11950 net.cpp:630]     [Backward] Layer conv2_1, param blob 0 diff: 0
I0315 11:08:25.959345 11950 net.cpp:630]     [Backward] Layer conv2_1, param blob 1 diff: nan
I0315 11:08:25.966053 11950 net.cpp:619]     [Backward] Layer res1_3, bottom blob conv1_1_relu1_1_0_split_1 diff: nan
I0315 11:08:25.966897 11950 net.cpp:619]     [Backward] Layer res1_3, bottom blob conv1_3 diff: nan
I0315 11:08:25.971491 11950 net.cpp:619]     [Backward] Layer relu1_3, bottom blob conv1_3 diff: nan
I0315 11:08:25.971539 11950 net.cpp:630]     [Backward] Layer relu1_3, param blob 0 diff: nan
I0315 11:08:25.974290 11950 net.cpp:619]     [Backward] Layer conv1_3, bottom blob conv1_2 diff: 0
I0315 11:08:25.974350 11950 net.cpp:630]     [Backward] Layer conv1_3, param blob 0 diff: 0
I0315 11:08:25.979085 11950 net.cpp:619]     [Backward] Layer relu1_2, bottom blob conv1_2 diff: 0
I0315 11:08:25.979137 11950 net.cpp:630]     [Backward] Layer relu1_2, param blob 0 diff: 0
I0315 11:08:25.981963 11950 net.cpp:619]     [Backward] Layer conv1_2, bottom blob conv1_1_relu1_1_0_split_0 diff: 0
I0315 11:08:25.982022 11950 net.cpp:630]     [Backward] Layer conv1_2, param blob 0 diff: 0
I0315 11:08:25.986893 11950 net.cpp:619]     [Backward] Layer conv1_1_relu1_1_0_split, bottom blob conv1_1 diff: nan
I0315 11:08:25.991500 11950 net.cpp:619]     [Backward] Layer relu1_1, bottom blob conv1_1 diff: nan
I0315 11:08:25.991562 11950 net.cpp:630]     [Backward] Layer relu1_1, param blob 0 diff: nan
I0315 11:08:25.997321 11950 net.cpp:630]     [Backward] Layer conv1_1, param blob 0 diff: nan
I0315 11:08:25.997373 11950 net.cpp:630]     [Backward] Layer conv1_1, param blob 1 diff: nan
E0315 11:08:26.005578 11961 net.cpp:719]     [Backward] All net params (data, diff): L1 norm = (385685, nan); L2 norm = (115.523, nan)
E0315 11:08:26.005789 11950 net.cpp:719]     [Backward] All net params (data, diff): L1 norm = (385685, nan); L2 norm = (115.523, nan)
I0315 11:08:26.005841 11950 solver.cpp:218] Iteration 0 (0 iter/s, 0.551509s/100 iters), loss = 87.3365
I0315 11:08:26.005861 11950 solver.cpp:237]     Train net output #0: lambda = 892.857
I0315 11:08:26.005875 11950 solver.cpp:237]     Train net output #1: softmax_loss = 87.3365 (* 1 = 87.3365 loss)
I0315 11:08:26.005889 11950 sgd_solver.cpp:105] Iteration 0, lr = 0.1"
"I am working for face recognition. I want to follow your procedure to do the preprocessing, however, I don't install the Caffe on my desktop.  Can you make the 'dataList.mat' which saved the landmarks for CASIA and LFW datasets public? I will cite your paper if I can use it. :)"
"For 3-patch ensemble, do you train 3 different models for 3 patches or do you train single trained model on all 3 patches?"
"i found if the img is too big, eg. 3000*2000, the detection function is corrupt. so i want to know why"
"The parameter 'a' of prelu may be shared by all channels or not be.
I find the only re-implement version above 99.0% (99.2%) uses the way NOT sharing the 'a' for all channels.
In my tensorflow version, not sharing 'a' is much better.
But I checked the caffe docs, it says 'a' is shared by all channels all the time.
Quite confusing, how the way is in the original caffe version. 
  "
"I tried it by adding 'type: ""Adam""' in solver.prototxt, but it still uses SGD. Anything else I missed for that? Or how can I reach it? Many thanks. 
  
  "
"Hi, @wy1iu 
In backward step, coeff_x and coeff_w are both normalized (coeff_x ^2 + coeff_w ^ 2 = 1). This implement is different from paper ""large-margin softmax loss for convolutional neural networks"". What is the use of the normalization? Is it a trick?"
"hi @wy1iu ：
   “These face images are horizontally filpped for data augmentation”, Do you horizontally filpped the image manual before training or flipped the image automatic with the paremeter ""mirror"" in the datalayer of caffe？ "
"In face_align_demo.m,there are 5 coordinate points used for alignment,so the question is how you get the values of them?
Are they located in the original image which is 250 * 250,or the 112 * 96 cropped image?"
"hi,proffesor, my data is much less than webface, how can i adjust superparameter to make the loss converge?"
"How to evaluate the sphereface on Megaface?

Can i directly save the normalized embeddings and run the code ""run_experiment.py"" in the megaface devkit?

thanks
 "
"I check the code, I think it can not ensure the theta in [0,pi/m]. if the theta is not in the [0,pi/m], I think it will be some wrong."
""
"Since the WebFace contains 10,575 subjects, why using 10572?
Is that because mtcnn could only detect and align 10572 persons?

I'm using a face detection model and alignment method trained by myself."
"Since I only have 2\*1080Ti, using batch_size=256 will out of mermory.
I set batch_size=64, so how to set the stepvalue,
stepvalue: 16000\*4
stepvalue: 24000\*4
stepvalue: 28000\*4
max_iter: 28000\*4
other parameters are same as the solver providied by this repository .
is it ok?
And if I use Relu instead of PRelu, is it just affecting the coverage time. How about the precision?"
"have you train your a-softmax on a much more dataset for example msceleb1m.
I trained a-softmax on a 10m dataset, abount 200k identities, the loss did not converge,
but softmax or center loss done well."
"As I seen in your prototxt file, you didn't use neither batchnorm layer nor dropout. Why? 
Does such layers affect the network converging?
Thanks in advance."
"I have two question:
1. In weight initialization of released model you use sometimes `xavier`, sometimes `gaussion`. Any reason why? I have implemented this network in PyTorch and `xavier` for all layer works well for all network configuration expect `64` model (model diverge). And I'm searching reason why it does not work, maybe because of not mixed initialization? I checked other init method and non of them is working. 

2. I would like to learn baseline model using pure SoftMax. Did you add any layer between embedding (size 512) and classification layer (size num_class), like PReLu or Dropout? I have learned model (20) without adding anything, but it overfitt ( I split data to train/val set)."
"lfw acc:99.42% is incredible good;
About training set CASIA-WebFace, did you use clean_list or all images from CASIA?
I use this cleaned version casia,
https://groups.google.com/forum/#!topic/cmu-openface/Xue_D4_mxDQ
best result is 
Accuracy: 0.990+-0.004
Validation rate: 0.92067+-0.02112 @ FAR=0.00100"
完全按照步骤要求进行训练，唯一的区别是限于只有单个6G显存的980T，只能单GPU以128的batchsize进行训练。始终不能收敛，请问这是batchsize过小引起的收敛问题吗？ 谢谢！
"@wy1iu @ydwen 
In your forward code, you set w to be w' (w' = w/||w||). But in the backward code, you just use ∂E/∂w' as the gradient of w. Actually it should be ∂E/∂w' * ∂w'/∂W, is it a bug?"
""
"Hi,@wy1iu，i used the 20 layers network trained on casita dataset and extract feature on lfw dataset by concate original face image and flip face image features and finally get 98.97% accuracy, is is normal?  I write a 64 layers network according to your paper and finally get 98.88% accuracy, why the deeper network get a worse result? "
在video demo 里 ， 人脸框并不是正的，这是怎么实现的？MTCNN冰没有这个功能啊？
"Hi,

in the figures of your papers you can see that the 3-patch net outperforms the one patch net and you say that the 3-patch feature vector is formed by concatenating the feature vectors of each patch. However, you do not say in your paper how these patches are computed. Do you use a different alignment strategy and train a new net on each type of aligned images or do you introduce a small offset and extract the 3-patch features by just transforming the original image a bit and computing the features with the original net?
Furthermore I get better results when not concatenating the features of the mirrored image. Has anyone experimented on that (patch selection/(not) mirroring) ? "
"It is said only one image is registered for each person (could be the face detected  for the first time), so in this case can I ask whether it is an open set test demo? E.g. the similarity between the registered face and the test titled one is always larger than the pre-set threshold (recognized as the same person). It seems once the face is detected, the recognition result is always right. Looking forward to your reply."
"Hey, I trained the model as you mentioned in the README using the same ide and  face datebase. But the loss increased from 7 to 87 after 10k iterations. Could you tell me how to analyse and solve this problem."
"Hi --

This is a question better directed at the https://github.com/ydwen/caffe-face issues, but issues are disabled and that project was written by some of the same authors.

The LFW features are available for download on the `caffe-face` projects page -- is there evaluation code available to reproduce the 99% accuracy reported in the paper?  The paper mentions using PCA on the features, but I haven't been able to get that kind of accuracy using the pretrained vectors.  Any ideas?

Thanks"
""
"First of all thank you for your open code. You have developed a 20-layer Sphereface framework, and Is it necessary to study the 64-layer? Which of the two is better?"
"From the form of the loss function, I think adding the lambda can be seen as using smaller m. I have plotted the curve of `lambda=5, m=4` and find that it is approximately m=1.5.
![image](https://user-images.githubusercontent.com/3897999/29196184-d5384540-7e00-11e7-8360-d1f564d64660.png)

Is my thought right?"
"Hi,
   I set the random_seed in solver.prototxt, use the same CNN architecture and the same training set for evaluating the repeatability.
In softmax loss or softmax with centre loss, I can get the same softmax loss and same center loss in every iteration during training, but it is not the case in a-softmax (sphereface).   In a-softmax, the first iteration is same, but it becomes difference when I repeat to train the model from scratch.  
  Would you like to explain it?
"
"First of all, thank you for providing the Sphereface training code for us. 

I have retrained your model following your instruction. However, I only got 67.35% on Rank-1 Identification Accuracy on Megaface testing, while you got 72.73%. We used your code and trained from scratch. 

Training with Softmax + CenterLoss rendered the correct result as reported in your paper, i.e. ~65%. 

I believe this is the same protocol you used for Megaface Testing, right? If not, could you kindly point out the difference? Thank you very much! "
"Loss always stays around 9.3, not down
I set the learning rate to 0.01 and 0.06, and loss didn't converge？？？
How does the training network need to be modified?？？？？
How is the training parameter set?？？？？
Hope to get your help！！thanks!!
@wdwen "
"Hi wy1iu,

Asoftmax is a great work.I have some questions about how to set gamma and power for lambda.
Using the settings in your prototxt
(
    base: 1000
    gamma: 0.12
    power: 1
    lambda_min: 5
)
lambda will drop very quickly to lambda_min, is it really possible to converge? 
what is the best way to setting these for different networks and different size of training data?

Thanks."
"Hi, @wy1iu @ydwen 

Thank you for sharing the code. I have trained a model using 28layer (as the center face) with A-softmax, which works well. However, I have struggled training with the 64layer structure you mentioned in the original paper for several days. Could you please kindly release one of you network architectures in the paper (either 20layer or 36layer or 64layer) so I can compare the difference between the prototxt I wrote from yours. 

BTW, it seems there is no pooling layer in your architecture, right? Just using convolution layer of stride 2 instead of pooling?

Thanks in advance!"
"Are you able to explain a couple of bits about the implementation of the `margin_inner_product_layer`?

1) What is `lambda_`?  It looks like it's a constant that decreases w/ the iterations, but doesn't seem to be mentioned in the paper.  ** Edit: Looks like you mix `x'w` and `margin(x'w)` via `(margin(x'w) + lambda_ * x'w) / (1 + lambda_)` where `lambda_` decreases exponentially w/ iterations.  Is that right?  **

2) What is the `type` parameter? (eg. `SINGLE, DOUBLE, TRIPLE, QUADRUPLE`)  I'm guessing this is how you set the value of `m` from the paper? ** Edit: I gather these are ways of implementing the margin for `m={1,2,3,4}`.  Any particular reason why you implemented this way?  Numerical stability? **

Thanks"
"Hi --

What are the results (eg LFW accuracy) that we should expect when running this code as described in the README?   I don't have MATLAB so I'll need to run w/ a different face detection/alignment pipeline, so I want to see how much error is introduced by my variant.

Thanks
"
"In Margin Inner Product, the gradient with respect to weight is very simple:

```
  // Gradient with respect to weight
  if (this->param_propagate_down_[0]) {
    caffe_cpu_gemm<Dtype>(CblasTrans, CblasNoTrans, N_, K_, M_, (Dtype)1.,
        top_diff, bottom_data, (Dtype)1., this->blobs_[0]->mutable_cpu_diff());
  }
```

But in large margin softmax, the gradient calcuation is much more complex...

Can you please tell me how to simplify the gradient calcuation?
I failed to derive it ...
"
"I really grateful that @wy1iu release the code. You and @ydwen are really pushing the Face-Verification forward.

 I have some question regarding the paper and the code:
1. What is the major change between L-SoftMax and A-SoftMax. For the equation it look like that in L-SoftMax weight are transformed to norm of weights and in A-SofrMax weight are transformed to normalized weight, right? If this is true, the main motivation was section 3.3 in Large-Margin Softmax Loss for Convolutional Neural Networks?
2. Could you explain how did you choose function ψ (which replace cos(θ))? 
3. In both paper you use Taylor Series of cos(mθ) (Eq. 7 in Large Margin), right? What was the idea behind using different degree of series based on margin value? Why not using same for all margin?
4. Here is my intuition behind this both paper: in fact  we just scale the output from Linear layer by matrix of ones with different numbers (<1)  on target classes. Both paper propose different method of scaling (with theoretical explanation). I think that maybe there is possible to make implementation which would just use scale matrix. I must think about it as there are many non-linear operation here. 
5. I was thinking about using CenterLoss but using cosine similarity. But then I realized than it is equivalent to SoftMax layer without bias (and SoftMax also compare features to other class center as well, not only target, so it make features even better).  Do you agree with my interpretation?"
"/home/ubuntu/csr-dcf-master/mex_src/hog/gradientMex.cpp: In function ‘mxArray* mxCreateMatrix3(int, int, int,
mxClassID, bool, void**)’:
/home/ubuntu/csr-dcf-master/mex_src/hog/gradientMex.cpp:329:44: error: cannot convert ‘const int*’ to ‘const
mwSize* {aka const long unsigned int*}’ for argument ‘2’ to ‘int mxSetDimensions_730(mxArray*, const mwSize*, mwSize)’
   mxSetData(M,*I); mxSetDimensions(M,dims,3); return M;
                                            ^
/home/ubuntu/csr-dcf-master/mex_src/hog/gradientMex.cpp: In function ‘void checkArgs(int, mxArray**, int, const
mxArray**, int, int, int, int, int*, int*, int*, mxClassID, void**)’:
/home/ubuntu/csr-dcf-master/mex_src/hog/gradientMex.cpp:339:71: error: cannot convert ‘const mwSize* {aka const
long unsigned int*}’ to ‘const int*’ in assignment
   nDims = mxGetNumberOfDimensions(pr[0]); dims = mxGetDimensions(pr[0]);
                                                                       ^


error compile (line 9)
mex gradientMex.cpp

How can I solve this problem？
"
"CSR-DCF has been incorporated in opencv and i have used in one of my object tracking project. However, at some point in tracking falling coins, the tracker's bounding box remains in a place for some time. This is undesirable. Is there any particular reason for this? 

![image](https://user-images.githubusercontent.com/54139813/68929013-59d0da00-07c6-11ea-9ad7-b3437dffdffc.png)

![image](https://user-images.githubusercontent.com/54139813/68928996-4de51800-07c6-11ea-903d-6c6b9bcbc17a.png)

I have tried changing the psr_threshold as suggested here: https://stackoverflow.com/questions/54785230/csrt-algorithm-not-updating-target

However, is was able to make this box move, but again made another box stick to a spot for sometime.

How can I solve this issue. It'll be convenient if you can suggest some configurations of this algorithm via opencv.

Environment: Linux OS, python2.7
OpenCv: 4.1.1"
"Hi,
I'm curious about what happen if we take another step for this awesome algorithm and add GPU support to it? does the performance may be added dramatically ?"
"The results provided for otb100 has wrong with the sequence of clifbar, it has 472 frames, but the result only has 329 frames"
"Hello, is there a way to set the value of psr_threshold at the initialization of the CSRT Tracker ? I checked the public function TrackerCSRT::create(const TrackerCSRT::Params & parameters) and TrackerCSRT::Params::read(const FileNode &) but I wasn't able to set this parameter using a FileStorage. Any help/example would be appreciated.
Moreover, is there a public method to get the max_val from CSRTImpl::estimate_new_position in order to determine if the target is visible, partially lost or completely lost ?
PS : I don't want to modify the source code because I'm only using the module from openCV as it is."
"gradientMex.cpp
D:\visual object tracking\Discriminative Correlation Filter with Channel and Spatial
Reliability\csr-dcf-master\mex_src\hog\gradientMex.cpp(329): error C2664: “int mxSetDimensions_730(mxArray *,const size_t
*,std::size_t)”: 无法将参数 2 从“const int [3]”转换为“const size_t *”
D:\visual object tracking\Discriminative Correlation Filter with Channel and Spatial
Reliability\csr-dcf-master\mex_src\hog\gradientMex.cpp(329): note: 与指向的类型无关；转换要求 reinterpret_cast、C 样式转换或函数样式转换
D:\visual object tracking\Discriminative Correlation Filter with Channel and Spatial
Reliability\csr-dcf-master\mex_src\hog\gradientMex.cpp(339): warning C4267: “=”: 从“size_t”转换到“int”，可能丢失数据
D:\visual object tracking\Discriminative Correlation Filter with Channel and Spatial
Reliability\csr-dcf-master\mex_src\hog\gradientMex.cpp(339): error C2440: “=”: 无法从“const size_t *”转换为“const int *”
D:\visual object tracking\Discriminative Correlation Filter with Channel and Spatial
Reliability\csr-dcf-master\mex_src\hog\gradientMex.cpp(339): note: 与指向的类型无关；转换要求 reinterpret_cast、C 样式转换或函数样式转换



出错 compile (line 9)
mex gradientMex.cpp

I don't know how to repair!!!"
"Hello!
Thank you for your paper and code!
I'm planning to read the paper in the next days,  but now I've tested OpenCV implementation of the algorithm and I can say it works a way better than other algos for my videos.
Can you please describe what params can I tune to increase fps rate but maybe sacrifice the precision. Also I have bg subtraction mask, maybe there is a way to utilize it inside the algorithm (with the code tweaks).
OpenCV params are listed here:
https://github.com/opencv/opencv_contrib/blob/6ef1983f0876fdf65083666d8e73abfecaf9d4f4/modules/tracking/src/trackerCSRT.cpp#L715"
"Hello, I want to visualize spatial reliability maps.
I try to do it with extract masks from the function tracker_csr_tracker and visualize it in visualize step.
What is exactly a spatial reliability map variable on your code?
I am confuse which I have to use among mask, valid_pixels_mask and fg.

I appreciate your answer."
"https://github.com/alanlukezic/csr-dcf/blob/469959d14638d1b57014451dc254bda39df78d42/mex_src/hog/gradientMex.cpp#L336

argument of type ""const int *"" is incompatible with parameter of type ""const mwSize={size_t={unsigned
__int64}} *""
    mxSetData(M,*I); mxSetDimensions(M,dims,3); return M;

Should  ""dim"" not rather be a ""const unsigned __int64"" (which is what mxSetDimensions and mxGetDimensions) expects?"
"Dear @alanlukezic,
Thank you for your nice work. I have question about your code. 
How one can inform that your tracker fails to track the target?
I mean that, in some tracking algorithms we can inform when the tracker fails to track the target. For example, by using the L1-disnatce between the visual features of the target and the output of the tracker. More specifically, something like this:
```python
if (L1-distance_between_target_bounding_box_and_tracker_output > threshold):
    print('The tracker fails to track the target')
```
I will really appreciate you if your answer cover your C++ implementation (i.e. both implementations (MATLAB & C++ one)). 
"
How to use the function of csr_wrapper to generate a output.txt file?
Do you have a plan to make the source code of CSR-DCF++ public available ?
"![reliability-cf](https://user-images.githubusercontent.com/12802864/26883749-54b16eae-4b9e-11e7-8506-94c211331218.png)
"
"Hello Alan,
    I'm Jesse ,  I read your paper D3S recently．　It is amazing.
Here are some confusions about your experiments in detail.
 1:How  to arrange 64 image pairs every batch to compute the foreground and background similarity channels?
2:  In Videomatch , how  the matching is perfomed in pytorch?
 Looking forward for your letter.

Best wishes,
Jesse"
"Hello there,

I have a question regarding the meaning of ""channel"". I thought it meant the RGB channel, but after looking at the video of channel reliability weights visualization, there are so many of them. I thought there should be only 3 weights, one for each color channel.

If there are so many of them, then, channel can't be the RGB channel. 

![Selection_002](https://user-images.githubusercontent.com/326807/57112583-807d6680-6cf5-11e9-9efd-424667a53e85.png)

But by reading the code, I still think channel here is really the RGB channel, right?

Then how come the visualization shows so many weights? 

Thanks,"
"Hello Mr.Alan Lukežič,
When i compiled the Compile.m function ,something wrong like this happened
Building with 'Microsoft Visual C++ 2015'.
Error using mex
gradientMex.cpp
C:\Users\Administrator\Desktop\csr-dcf-master\mex_src\hog\gradientMex.cpp(329):
error C2664: “int mxSetDimensions_730(mxArray *,const size_t *,std::size_t)”:
cannot convert from“const int [3]”to“const size_t *”
C:\Users\Administrator\Desktop\csr-dcf-master\mex_src\hog\gradientMex.cpp(329):
note: Irrelevant to the type of direction；Conversion requires reinterpret_cast, C style conversion, or function style conversion
C:\Users\Administrator\Desktop\csr-dcf-master\mex_src\hog\gradientMex.cpp(339):
warning C4267: “=”: From ""size_t"" to ""int"", may lose data
C:\Users\Administrator\Desktop\csr-dcf-master\mex_src\hog\gradientMex.cpp(339):
error C2440: “=”: cannot convert from“const size_t *”to“const int *”
C:\Users\Administrator\Desktop\csr-dcf-master\mex_src\hog\gradientMex.cpp(339):
note: Irrelevant to the type of direction；Conversion requires reinterpret_cast, C style conversion, or function style conversion


Error in compile (line 9)
mex gradientMex.cpp
the platform is MATLAB 2017b + VS2015+Opencv 2.4.13
so what should i do to fix it?
thank you in advance"
"I follow the instructions to compile the files and success to run the demo_csr.m on Inter Core i7-7700K 4.2GHz. However, the fps is so low, about 6. I have tried some other objects, the results range from 3 to 12 fps. The demo video shows that the tracking speed is high, can reach to 200 fps. Is it because of using gpu devices?"
"Dear @alanlukezic,
Thank you for your fantastic project. Does any Python version of this project exists? (or any plan for release python version)

 "
"I run csr-dcf in vot2016 and its EAO is 0.3381, the same as claimed in paper. But the EAO in vot2017 is 0.2223, which is different from the EAO in the table of vot2017 results paper
[http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Kristan_The_Visual_Object_ICCV_2017_paper.pdf](url)"
"When I tried to compile on my Ubuntu machine, I get this error:

Error using mex
/home/ugurkart/Codebase/csr-dcf/mex_src/hog/gradientMex.cpp:
In function ‘mxArray* mxCreateMatrix3(int, int,
int, mxClassID, bool, void**)’:
/home/ugurkart/Codebase/csr-dcf/mex_src/hog/gradientMex.cpp:329:44:
error: cannot convert ‘const int*’ to ‘const
size_t* {aka const long unsigned int*}’ for
argument ‘2’ to ‘int mxSetDimensions(mxArray*,
const size_t*, size_t)’
   mxSetData(M,*I); mxSetDimensions(M,dims,3);
   return M;
                                            ^
/home/ugurkart/Codebase/csr-dcf/mex_src/hog/gradientMex.cpp:
In function ‘void checkArgs(int, mxArray**,
int, const mxArray**, int, int, int, int, int*,
int*, int*, mxClassID, void**)’:
/home/ugurkart/Codebase/csr-dcf/mex_src/hog/gradientMex.cpp:339:48:
error: cannot convert ‘const size_t* {aka const
long unsigned int*}’ to ‘const int*’ in
assignment
   nDims = mxGetNumberOfDimensions(pr[0]); dims
   = mxGetDimensions(pr[0]);
                                                ^


Error in compile (line 9)
mex gradientMex.cpp

Any idea about how to solve it ?"
"Why the shape of running_mean in Instancenorm is [C] not [N,C] ?
the code "" self.running_var:copy(self.bn.running_var:view(input:size(1),self.nOutput):mean(1))""
I don't the reason running_mean/var view to (N,C) and mean(1)? So it similar to Batchnorm?
I find the cuda-kernel of Instancenorm used batchnorm's kernel ,the shape of running_men in Bn is [C], Instance's also [ C]too?"
"Dear all,
Does anybody know the real difference of the model(net) written in the code between this repository and Johnson's fast-neural-style? I mean, in this repo, the default model parameter is 'johnson', If I use the 'johnson' mode, and since the Johnson's newest version has implement 'InstanceNormalization' yet, What is the real difference? and the output is different too. Anybody knows? I'll appreciate any reply.
this repo's output:
![image](https://user-images.githubusercontent.com/21032349/54592438-59314980-4a67-11e9-85a7-c6c467fbbf77.png)
Johnson's papers output:
![image](https://user-images.githubusercontent.com/21032349/54592487-7534eb00-4a67-11e9-8a26-3a4d23a23862.png)
"
"Link to a demo in README is broken
https://riseml.com/DmitryUlyanov/texture_nets"
"I'm new to torch and have a fresh install just to test out fast style transfer.

Installing cudnn from deb packages installs it in `/usr/lib/x86_64-linux-gnu/` as referenced [here](https://github.com/NVIDIA/nvidia-docker/issues/172) and in this comment [here](https://github.com/NVIDIA/nvidia-docker/issues/168#issuecomment-238924455).

I think support for that in this code is missing. I would add it myself but am quite new to torch and so adding an issue.
```
$ th train.lua -data ./datasets/ -style_image ./data/textures/862.png -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2
/home/user/torch/install/bin/luajit: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: /home/user/torch/install/share/lua/5.1/cudnn/ffi.lua:1603: 'libcudnn (R5) not found in library path.
Please install CuDNN from https://developer.nvidia.com/cuDNN
Then make sure files named as libcudnn.so.5 or libcudnn.5.dylib are placed in
your library load path (for example /usr/local/lib , or manually add a path to LD_LIBRARY_PATH)

Alternatively, set the path to libcudnn.so.5 or libcudnn.5.dylib
to the environment variable CUDNN_PATH and rerun torch.
For example: export CUDNN_PATH=""/usr/local/cuda/lib64/libcudnn.so.5""

stack traceback:
	[C]: in function 'error'
	...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	train.lua:5: in main chunk
	[C]: in function 'dofile'
	...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50

```
Solution being : 
`export CUDNN_PATH=""/usr/lib/x86_64-linux-gnu/libcudnn.so.5""`"
"I found default transfer parameter is 
cmd:option('-content_weight', 1)
cmd:option('-style_weight', 1)
cmd:option('-tv_weight', 0, 'Total variation weight.')

seems not same with common option(Gatys set 1000:1), is it your pretrained option? and do you try transfer result with and without tv loss?"
"I have tried this now on many different configurations and different batch sizes and models. I am not managing to get any stylized images but plain color outputs (mostly black, sometimes green).

The loss seems to be going up most of the time as well in training time.  Any clue what could be wrong?

Is there a ""frozen"" configuration somewhere that I can access with torch + rocks instances that worked at the time?

Thanks!"
"Hi,
 i am trying to train models using Johnson model , bu what ever i try low images are appearing 
- i have tried to make the image width multiple of 32
- i have tried to decrease learning rate 


what is the best solution to get good results in my case

`th train.lua -data dataset -style_image ./AdobeStock_90768625.jpeg -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2 -gpu 0
`
 
![ballon](https://user-images.githubusercontent.com/25582838/32208084-d0dce88c-be08-11e7-9681-287e26fc42d9.jpg)

"
"I am using the following command to train the model 

**`th train.lua -data dataset -style_image ./AdobeStock_90768625.jpeg -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2 -cpu`**


but this error is raised :-
```
/home/src/torch/install/bin/luajit: ...erspace/src/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
...aperspace/src/torch/install/share/lua/5.1/cudnn/init.lua:171: assertion failed!
stack traceback:
	[C]: in function 'assert'
	...aperspace/src/torch/install/share/lua/5.1/cudnn/init.lua:171: in function 'toDescriptor'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:123: in function 'createIODescriptors'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:188: in function <...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:186>
	[C]: in function 'xpcall'
	...erspace/src/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	...rspace/src/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:212: in function 'opfunc'
	...aperspace/src/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:241: in main chunk
	[C]: in function 'dofile'
	.../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
	[C]: in function 'error'
	...erspace/src/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
	...rspace/src/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:212: in function 'opfunc'
	...aperspace/src/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:241: in main chunk
	[C]: in function 'dofile'
	.../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

```"
"I am trying to print the torch model options of training, I have used the provided model **model.t7** 

I am using this code to print the option 
```
require 'torch'
require 'nn'

require 'InstanceNormalization'


--[[
Prints the options that were used to train a a feedforward model.
--]]


local cmd = torch.CmdLine()
cmd:option('-model', 'models/instance_norm/candy.t7')
local opt = cmd:parse(arg)

print('Loading model from ' .. opt.model)
local checkpoint = torch.load(opt.model)

for k, v in pairs(checkpoint.opt) do
  if type(v) == 'table' then
    v = table.concat(v, ',')
  end
  print(string.format('%s: %s', k, v))
end




```



but I am getting this error:
```
/src/torch/install/bin/luajit: print_options.lua:22: bad argument #1 to 'pairs' (table expected, got nil)
stack traceback:
	[C]: in function 'pairs'
	print_options.lua:22: in main chunk
	[C]: in function 'dofile'
	.../src/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670

```"
"Hi, 
It would be great if you can update the readme. The current readme follows a stale branch."
"@DmitryUlyanov  first, thanks for your great work which is really helpful to my research. I have a question after reading the paper. To apply your method to style transfer, do we need to input content image during training phase? If not, the content loss is generated for what? Based on my understanding, the content loss is used for preserving the spatial structure of the content image. But there is no content image option in train.lua."
"I followed the basic command and try to  train the  dataset, an error occurred.
Following is the command:
`th train.lua -data  dataset  -style_image wave.jpg  -cpu`
Following is the output on the stdout:
`=> Generating list of images	
 | finding all validation images	
 | finding all training images	
 | saving list of images to /home/jiehang/texture_nets/gen/style.t7	
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 574671192
Successfully loaded data/pretrained/VGG_ILSVRC_19_layers.caffemodel
conv1_1: 64 3 3 3
conv1_2: 64 64 3 3
conv2_1: 128 64 3 3
conv2_2: 128 128 3 3
conv3_1: 256 128 3 3
conv3_2: 256 256 3 3
conv3_3: 256 256 3 3
conv3_4: 256 256 3 3
conv4_1: 512 256 3 3
conv4_2: 512 512 3 3
conv4_3: 512 512 3 3
conv4_4: 512 512 3 3
conv5_1: 512 512 3 3
conv5_2: 512 512 3 3
conv5_3: 512 512 3 3
conv5_4: 512 512 3 3
fc6: 1 1 25088 4096
fc7: 1 1 4096 4096
fc8: 1 1 4096 1000
Using TV loss with weight 	0	
Setting up texture layer  	2	:	relu1_1	
Setting up texture layer  	7	:	relu2_1	
Setting up texture layer  	12	:	relu3_1	
Setting up texture layer  	21	:	relu4_1	
Setting up content layer	23	:	relu4_2	
        Optimize        	
/home/jiehang/torch/install/bin/luajit: /home/jiehang/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
/home/jiehang/torch/install/share/lua/5.1/cudnn/init.lua:171: assertion failed!
stack traceback:
	[C]: in function 'assert'
	/home/jiehang/torch/install/share/lua/5.1/cudnn/init.lua:171: in function 'toDescriptor'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:123: in function 'createIODescriptors'
	...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:188: in function <...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:186>
	[C]: in function 'xpcall'
	/home/jiehang/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
	/home/jiehang/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:211: in function 'opfunc'
	/home/jiehang/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:240: in main chunk
	[C]: in function 'dofile'
	...hang/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
	[C]: in function 'error'
	/home/jiehang/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
	/home/jiehang/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
	./src/descriptor_net.lua:28: in function 'forward'
	train.lua:211: in function 'opfunc'
	/home/jiehang/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:240: in main chunk
	[C]: in function 'dofile'
	...hang/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50
`
"
"Is there any way I can transfer the .t7 model to apple machine learning model using the latest iOS 11 core ml? In that case, I can call the model in iPhone without renting a server"
"I have an original image shown as follows. Due to the moon, the lower half of the sky is brighter than the upper half. The part of the sky around the moon is particularly bright.
 
![shanghai_moon](https://user-images.githubusercontent.com/23154573/27513794-ad1b4da0-5939-11e7-852d-65b0a5e99be8.jpg)

When I use the ""Times"" filter in the Vinci app, the output shows an uneven sky as follows, which reflects the unsmooth sky in the original image.
![shanghai_moon_times](https://user-images.githubusercontent.com/23154573/27513817-dcf448dc-593a-11e7-9e70-c70896ad1bab.JPG)

However, when I use a different filter, i.e., ""Harvest"" filter, the output shows an amazing smooth sky as follows.
![shanghai_moon_harvest](https://user-images.githubusercontent.com/23154573/27513821-0764858c-593b-11e7-8d70-3a0c719a70e9.JPG)

I also tried to use texture_nets to stylize the original image using different style images, the output always had the uneven sky like the one with the ""Times"" filter. I have tried to use different combinations of layers for styles and contents, changed the style sizes and image sizes, no luck so far.

Any one has idea how to generate a smooth sky like the one produced by the ""Harvest"" filter? Thanks. 



"
"**I have installed all dependencies that this repository need,but something gose wrong when running the command bellow:**

th test.lua -input_image /data/artwork/content/huaban.jpeg -model_t7 data/checkpoints/model.t7 -gpu 0

THCudaCheck FAIL file=/tmp/luarocks_cutorch-scm-1-3022/cutorch/init.c line=734 error=10 : 
**_invalid device ordinal_**
/root/AI/torch/install/bin/luajit: test.lua:26: cuda runtime error (10) : invalid device ordinal at /tmp/luarocks_cutorch-scm-1-3022/cutorch/init.c:734
stack traceback:
	[C]: in function 'setDevice'
	test.lua:26: in main chunk
	[C]: in function 'dofile'
	...t/AI/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00406670
`
**bellow is my GPU info**

`+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro P5000        Off  | 0000:03:00.0      On |                  Off |
| 26%   39C    P8     8W / 180W |    110MiB / 16264MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1387    G   /usr/lib/xorg/Xorg                             108MiB |
+-----------------------------------------------------------------------------+
`
I really have no idea where the problem is."
"I tried to find the code to reflect the term ""lamda"" in Equation (10), which is on Page 4 of the paper ""Improved Texture Networks: Maximizing Quality and Diversity in Feed-forward Stylization and Texture Synthesis"". But I couldn't find it.

Anyone can tell me in which .lua file Equation (10) is implemented? Thanks."
"I train on my laptop with Geforce 940m 8gb RAM 
My Settings
echo $PATH
/home/alex/torch-cl/install/bin:/usr/local/cuda-8.0/bin:/home/alex/torch/install/bin:/home/alex/torch-cl/install/bin:/home/alex/torch/install/bin:/home/alex/bin:/home/alex/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
echo $LD_LIBRARY_PATH
/home/alex/torch-cl/install/lib:/usr/local/cuda-8.0/lib64:/home/alex/torch/install/lib:/home/alex/torch-cl/install/lib:/home/alex/torch/install/lib:

My calling
test.lua -input_image karya.jpg -model_t7 model.t7
/home/alex/torch-cl/install/bin/luajit: /home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:334: unknown Torch class <nn.SpatialReplicationPadding>
stack traceback:
	[C]: in function 'error'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:334: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:360: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:360: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/nn/Module.lua:154: in function 'read'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:342: in function 'readObject'
	/home/alex/torch-cl/install/share/lua/5.1/torch/File.lua:391: in function 'load'
	test.lua:17: in main chunk
	[C]: in function 'dofile'
	...x/torch-cl/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
	[C]: at 0x00405e90
"
"Please, help me. Describing my problem - i try to trim the train and value datasets, because my laptop cannot work with this huge sets (write,that i have little RAM (cpu calcukating) or have little memory on harddisk (GPU) ). I have ubuntu and try copy a part of datasets like this (sort by name and copy first 1000 items)
 find ./datasets/train2014  -maxdepth 1 -type f | sort |head -1000 |xargs cp -t ./datasets/train/dummy
find ./datasets/val2014  -maxdepth 1 -type f | sort | head -1000|xargs cp -t ./datasets/val/dummy
this work good, but when i try to teach network, i get this error 

Optimize        	
/home/alex/torch/install/bin/lua: ...alex/torch/install/share/lua/5.1/threads/threads.lua:183: [thread 1 callback] ./datasets/style.lua:53: Error reading: /home/alex/Desktop/texture_nets/datasets/train/dummy/COCO_train2014_000000286564.jpg
stack traceback:
	[C]: in function 'assert'
	./datasets/style.lua:53: in function '_loadImage'
	./datasets/style.lua:32: in function 'get'
	./dataloader.lua:92: in function <./dataloader.lua:84>
	(tail call): ?
	[C]: in function 'xpcall'
	...alex/torch/install/share/lua/5.1/threads/threads.lua:234: in function 'callback'
	...e/alex/torch/install/share/lua/5.1/threads/queue.lua:65: in function <...e/alex/torch/install/share/lua/5.1/threads/queue.lua:41>
	[C]: in function 'pcall'
	...e/alex/torch/install/share/lua/5.1/threads/queue.lua:40: in function 'dojob'
	[string ""  local Queue = require 'threads.queue'...""]:15: in main chunk
stack traceback:
	[C]: in function 'error'
	...alex/torch/install/share/lua/5.1/threads/threads.lua:183: in function 'dojob'
	./dataloader.lua:143: in function 'loop'
	./dataloader.lua:62: in function 'get'
	train.lua:137: in function 'opfunc'
	...home/alex/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
	train.lua:174: in main chunk
	[C]: in function 'dofile'
	.../torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: ?


why style.lua tryes to read COCO_train2014_000000286564.jpg (i dont have this file in train directory, last image in train - COCO_train2014_000000007510.jpg in train , and in value - COCO_val2014_000000014226.jpg)

How to trim train and value directory CORRECT ?"
"I tried to replace the VGG-19 with ReSNet-50 for the loss network used in the training.
I downloaded the prototxt and caffemodel files from https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777, which is given on https://github.com/KaimingHe/deep-residual-networks

Then I tried to launch the training as following:
 th train.lua -style_image style/face_bottle_o.png -style_size 512 -image_size 256 -style_layers res2b_relu,res3b_relu,res4b_relu,res5b_relu -content_layers res4b_relu -style_weight 20 -content_weight 1 -proto_file data/pretrained/ResNet-50-deploy.prototxt -model_file data/pretrained/ResNet-50-model.caffemodel -checkpoints_path checkpoint/ -checkpoints_name face_bottle_o.s512i256.sL2b3b4b5b.cL4b.sw20.cw1.ResNet50.4x -num_iterations 10000 -batch_size 4 -save_every 2000

I encountered the following errors.
torch.display not found. unable to plot
Using TV loss with weight       1e-06
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 26:26: Message type ""caffe.LayerParameter"" has no field named ""batch_norm_param"".
Successfully loaded data/pretrained/ResNet-50-model.caffemodel
warning: module 'bn_conv1 [type BatchNorm]' not found
warning: module 'scale_conv1 [type Scale]' not found
warning: module 'pool1_pool1_0_split [type Split]' not found
warning: module 'bn2a_branch1 [type BatchNorm]' not found
warning: module 'scale2a_branch1 [type Scale]' not found
warning: module 'bn2a_branch2a [type BatchNorm]' not found
warning: module 'scale2a_branch2a [type Scale]' not found
..........(similar warning messages)
warning: module 'bn5c_branch2b [type BatchNorm]' not found
warning: module 'scale5c_branch2b [type Scale]' not found
warning: module 'bn5c_branch2c [type BatchNorm]' not found
warning: module 'scale5c_branch2c [type Scale]' not found
warning: module 'res5c [type Eltwise]' not found
conv1: 64 3 7 7
res2a_branch1: 256 64 1 1
Segmentation fault (core dumped)

Any idea how to resolve this issue? Thanks.
"
"Hi, I know this has been asked but I've been getting odd results with my style transfer.

With the default parameters, I used this drawing:
http://i.imgur.com/TpbblRf.jpg

And got this result:
http://imgur.com/a/B8PcY

I'd really like something more styled according to the original drawing, but I'm not sure what parameters to use? I can't find an explanation of what the parameters do. I'm going to try increasing style-size from 600 to 960 – will this help? Both the style image and the images I'm styling are very large.

Thank you."
"@DmitryUlyanov Currently a single weight is assigned to all style layers during the training process. Is it possible to assign different weights to different style layers?

For example, if ""style_layers"" = ""relu1_2,relu2_2,relu3_2,relu4_2"", is it possible to use ""style_weight"" = ""2,4,5,3"" so that the weights of the four layers (relu1_2,relu2_2,relu3_2,relu4_2) are 2, 4, 5, 3, respectively, in training? Thanks."
"When using MSCOCO dataset for training, I tried to set the image_size > 600, say 768. Then I found that the GPU utilization stayed at 0%. Following was the output of the command nvidia-smi:

Sun Dec 25 13:45:10 2016       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.55                 Driver Version: 367.55                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40c          On   | 0000:01:00.0     Off |                    0 |
| 29%   60C    P0    62W / 235W |    968MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40c          On   | 0000:02:00.0     Off |                    0 |
| 23%   39C    P8    23W / 235W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

The command was as follows.

nohup th train.lua -style_image style/transverse.jpg -style_size 768 -image_size 768 -checkpoints_path checkpoint/ -checkpoints_name transverse.s768i768.sw10.1x -style_weight 10 -num_iterations 40000 -batch_size 1 > train_0.out &

I was using the johnson model. When the image_size <=600, there was no issue."
"The model set to"" pyramid"" by default,when change it to johnson,get different results.
what different between them?
"
"This is not a issue, just want to say that the new parameter in README generate really good result. Compare to previous default parameter, I used to get image with a lot of broken glasses image, or a lot of fragments. Now it's smooth, even with 1 batch. Any idea why you change style_layers from ""relu1_1,relu2_1,relu3_1,relu4_1"" to ""relu1_2,relu2_2,relu3_2,relu4_2"" ?  Thanks for the improvement.
"
"Hi. Comparing TVLoss:updateOutput(input) with TextureLoss:updateOutput(input),  I found that the code didn't calculate TVLoss during forward pass but only calculate the gradient of TVLoss during backward pass. Any idea why?
"
"I've tested these on a computer with AMD FX(tm)-8350 Eight-Core Processor, with 32G RAM installed, with test.lua, I can get a stylized.jpg in 6 seconds. After reading this post https://github.com/DmitryUlyanov/texture_nets/issues/41
I think it's possible to generate high resolution images if I have larger RAM.
I got a VM running on XenServer, with 32 CPU core(Intel(R) Xeon(R) CPU E7- 4820  @ 2.00GHz) and 64G RAM, but when I ran test.lua with regular params, it was extreamly slow than the former computer.
It costs over 15 minutes to generate one single image. What's wrong with that?
I noticed the script runs on only single one CPU core, is that normal?

This is what I used:
th test.lua -input_image images/forbidden_city.jpg -model_t7 data/checkpoints/model.t7 -cpu

Any body have experience on this?

Thanks.
"
"I see that the pretrained model you provided runs at higher resolutions compared to the models I trained for a few style images. What might be the reason for this?Did you use VGG19 or normalised vgg? 
"
"Ive been trying to train the network for the Van Gogh starry night style by modifying the style_weight and tv_weight (0.001). I am not able to get good results. 
![stylized_v2](https://cloud.githubusercontent.com/assets/14928283/18140421/517407b2-6f6a-11e6-83be-c895ac3f7aeb.jpg)

Any suggestions on how to improve this? Has anyone tried using Van Gogh starry night? How can i reduce the blocky appearance?
"
"I am attempting to train a model using the default pyramid model with this command (also tried using all defaults):

```
th train.lua -style_weight 5   -tv_weight 0.000085  -style_size 512 -model pyramid  -image_size 512 -num_iterations 50000  -batch_size 1 -data ../coco2014 -style_image ../candy_512.jpg  
```

When I try to generate an image with it, I am getting black output, even though the input image's dimension is a multiple of 32:

```
th test.lua  -image_size 1024  -input_image ../Input_square1568.jpg  -model_t7 pyramid/model_20000.t7  -save_path stylized.jpg
```

The issue occurs if I don't use the image_size flag as well. The weird thing is, it seemed to work a couple times, but now all I get is black output. Using johnson's model does not create the issue, but skip_unpool model also creates blank output.

I have tried creating smaller images to 256, 512 etc. but they are all blank.

I am just trying to figure out how to train the model to achieve results similar to the default model. As others have posted, it doesn't seem possible with the current code. It seems like others have experienced the black output as well. Please advise! 
"
"I use 

th train.lua -data <path to any image dataset>  -style_image path/to/img.jpg

it gen 
model_1000.t7
model_2000.t7
model_3000.t7
......
model_49000.t7
model_50000.t7

but train model_50000.t7 to gen new picture seems not ok, is any wrong from my operate?

th test.lua -input_image tubingen.jpg -model_t7 data/checkpoints/rio.jpg/model_50000.t7

[rio.zip](https://github.com/DmitryUlyanov/texture_nets/files/442499/rio.zip)

thanks
"
"is it possible to train models without GPU?
always get error:  module 'cutorch' not found
"
"Hi, i find i can't tune the style scale in result using test.lua, also i can't find any option about it or is it possible in theory to scale style after the training procedure?

recently i trained a style, and created the following results on a photo with size 668 x 448 and with its original size 4272 × 2848:

![700px](https://cloud.githubusercontent.com/assets/465136/17917134/1d0608c8-69ec-11e6-8a2d-cbf4fa6c5d2f.jpg)
668 x 448

![12m](https://cloud.githubusercontent.com/assets/465136/17917141/26c5cee8-69ec-11e6-9863-faedab7a85f3.jpg)
4272 x 2848

i can understand both of the results are right, and the style is applied correctly. but if i can tune the style scale on large size image as artist using large brush, i think it will be better to user to see consistency between thumbnail(he get this quickly) and the final size result(he get this after a long time). 

sorry i don't know if this is mentioned on paper or there is any progress in recently research?
"
"![trainlua](https://cloud.githubusercontent.com/assets/20099577/17881827/f0f1724c-693a-11e6-974e-13367e8876ec.png)
![testlua](https://cloud.githubusercontent.com/assets/20099577/17881830/f882134a-693a-11e6-948b-67e154ca1369.png)
"
"Hi,
 I have used the model by your download link, the result look nice. But can you provide more input how you train this model, for example the content and style weight and also is it possible to output original ratio size photo rather than complete square output? Also if you can provide more models for test would be appreciated! Anyway, awesome job on the feed forward method.

Thanks
"
"Hi guys, i meet this certain situation, i confirm an amazing effect on some checkpoint but the training progress is stopped before. so i think if i can continue the training instead of discarding those intermediate result(for example have been trained 8K+ iters) and starting an new one(this will consume more unnecessary time), this will be very helpful to save our time.

or if i missed something in train.lua? 

thanks in advance. 
"
"Figure 3 in ""Texture Networks: Feed-forward Synthesis of Texture and Stylized Images"" is fantastic. For style of Picasso's painting (first row of figure 3), I trained the model with default parameter. But the result with style of Picasso's painting is bad. 
Can you share the parameter setting for style of Picasso's painting ,or the trained model? 
Thank you very much!
"
"Hi! It looks like there is a problem where all the ""brush strokes"" in a style are all the same size. You can really see it here: 
![karya-oil_07-stylweight3-stylesize512-learning0-001-model 40000](https://cloud.githubusercontent.com/assets/10135842/17484147/b0449174-5d56-11e6-9ed1-1b3039c60714.jpg)
This was the style image I used:
![oil_07](https://cloud.githubusercontent.com/assets/10135842/17484178/c5f2c4a0-5d56-11e6-843b-598b65897a97.jpg)

40,000 iterations of the microsoft coco dataset. styleweight of 3, stylesize of 512, image size of 512, learning set to .001, batch of 1. I got the same results with different style weights and learning sizes. Unfortunately this looks like a big limitation :(
"
"The result in paper's fig.4 was excellent. But I trained the model by the code with the default parameter, the result was not so good. 
"
"Hi I have this error at line 33 of test.lua : 

```
torch/install/share/lua/5.1/nn/Concat.lua:25: bad argument #1 to 'copy' (sizes do not match at /tmp/luarocks_cutorch-scm-1-9130/cutorch/lib/THC/THCTensorCopy.cu:30)
stack traceback:
    [C]: in function 'copy'

```

at the line 33 of test.lua when doing `model:forward(input:cuda())`
It's doing that with my own model I have pretrained with train.lua, it's not doing that with the pretrained model downloaded from the readme...

How do I use my own pretrained styles ?

Thanks in advance :) .
"
"Hi,
I tried to train my model based on the imagenet validation set which contains 50k images. At the beginning, i.e. the iteration step is smaller than some small numbers like 8000, I can get reasonable test result using the trained model. But as the training going on, I get a black image, where is output of the pixel value is NaN!  All the parameters used for training are unchanged, and the training image is the "" cezanne.jpg"" which is included in the branch of texture_nets_v1. 
Shoud I change the learning late? Could you please give me some advice about this problem? 
Thanks!
"
"I'm trying to train a style image using the MS COCO dataset but am having trouble setting up the directories in the proper way. I tried to follow your instructions in the README and have the following structure:

```
image-training/
image-training/train
image-training/train/dummy (images are in this folder)
image-training/val
image-training/val/dummy (images are in this folder)
```

When I run this command `th train.lua -data image-training/  -style_image data/textures/cezanne.jpg`, I get the following error:

`/root/torch/install/bin/luajit: /root/texture_nets/datasets/style-gen.lua:67: class not found: val`

Do you know if the structure of my `image-training` folder is incorrect?
"
"Exemple:
th stylization_train.lua -style_image data/textures/cezanne.jpg -train_hdf5 <path/to/generated/hdf5> -noise_depth 3 -model_name pyramid -normalize_gradients true -train_images_path <path/to/ILSVRC2012> -content_weight 0.8

Can you please explain every param:
-style_image - path to style image (.jpg, .png(??))
-train_hdf5 - path to generated hdf5 file, generated by this command (th scripts/extract4_2.lua -images_path <path/ILSVRC2012>) ???
-train_images_path - ???
"
""
"HI, i faced a problem as follows

th train.lua -data dataset  -style_image data/style/style1.jpg
/home/zhao/torch/install/bin/luajit: /home/zhao/torch/install/share/lua/5.1/threads/threads.lua:183: [thread 2 callback] bad argument #2 to '?' (dimension 1 out of range of 0D tensor at /home/zhao/torch/pkg/torch/generic/Tensor.c:19)
stack traceback:
	[C]: at 0x7f29911898b0
	[C]: in function 'xpcall'
	/home/zhao/torch/install/share/lua/5.1/threads/threads.lua:234: in function 'callback'
	/home/zhao/torch/install/share/lua/5.1/threads/queue.lua:65: in function </home/zhao/torch/install/share/lua/5.1/threads/queue.lua:41>
	[C]: in function 'pcall'
	/home/zhao/torch/install/share/lua/5.1/threads/queue.lua:40: in function 'dojob'
	[string ""  local Queue = require 'threads.queue'...""]:13: in main chunk
stack traceback:
	[C]: in function 'error'
	/home/zhao/torch/install/share/lua/5.1/threads/threads.lua:183: in function 'dojob'
	/home/zhao/torch/install/share/lua/5.1/threads/threads.lua:264: in function 'synchronize'
	/home/zhao/torch/install/share/lua/5.1/threads/threads.lua:142: in function 'specific'
	/home/zhao/torch/install/share/lua/5.1/threads/threads.lua:125: in function 'Threads'
	./dataloader.lua:48: in function '__init'
	/home/zhao/torch/install/share/lua/5.1/torch/init.lua:91: in function </home/zhao/torch/install/share/lua/5.1/torch/init.lua:87>
	[C]: in function 'DataLoader'
	./dataloader.lua:25: in function 'create'
	train.lua:143: in main chunk
	[C]: in function 'dofile'
	...zhao/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50

Could someone can help me to solve this ?

Many thanks."
"So, after #91 and #92 and setting --gpu flag as in [here](https://github.com/torch/cutorch/issues/503#issuecomment-386280004)

Is this usage normal?!

![screenshot from 2018-05-04 02-05-24](https://user-images.githubusercontent.com/10103473/39594828-5dba2922-4f40-11e8-92e2-56798379f02a.png)

The command I ran is : 
`CUDA_VISIBLE_DEVICES=4,9 th train.lua -data ./datasets/ -style_image ./data/textures/862.png -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2 -gpu 1
`

`nvidia-smi` shows about 800gigs + 350gigs allotted on the 2 GPU cards I made visible, but I'm unsure if they are being used. The CPU usage as can be seen is 400 percent.

What is most shocking is the RAM usage. 60+gigs?! Is this normal? My `train/dummy` folder has 15 images and `val/dummy` has 5 images; although I think the RAM usage is independent of this number... Any guidance is appreciated."
"I'm new to torch and have a fresh install just to test out fast style transfer.

After using fix in #91 changed as : `export CUDNN_PATH=""/usr/lib/x86_64-linux-gnu/libcudnn.so.7""
` the error is : 

```
$ th train.lua -data ./datasets/ -style_image ./data/textures/862.png -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2
Found Environment variable CUDNN_PATH = /usr/lib/x86_64-linux-gnu/libcudnn.so.7/home/user/torch/install/bin/luajit: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: ...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: /home/user/torch/install/share/lua/5.1/cudnn/ffi.lua:1618: 
These bindings are for CUDNN 5.x (5005 <= cudnn.version > 6000) , while the loaded CuDNN is version: 7101  
Are you using an older or newer version of CuDNN?
stack traceback:
	[C]: in function 'error'
	...e/user/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	train.lua:5: in main chunk
	[C]: in function 'dofile'
	...user/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50
```

Is there a quick fix way to add support for CUDNN 7? If so, how can I get it done. I would really like to test this code.

I realize, being new to this, I may be missing something fundamental because every one of tensorflow, cuda, cudnn, torch, etc. have a combination of version numbers that is tested and only that combination works without hacks. Is the process to hack in CUDNN 7 complicated?"
"Excuse me, can the source code generate arbitrary size texture from a single texture example? And how to realize it?"
"Hi and thanks for the great work. I am not familiar with lua torch so I better ask: is the BN layer in ""train"" or ""eval"" mode during testing? I know that in GANs they keep the BN layer in train mode always for stability https://discuss.pytorch.org/t/why-dont-we-put-models-in-train-or-eval-modes-in-dcgan-example/7422/2
How is it implemented in your code?"
"It is quite strange! I have multiple gpus. I use your code of Instance Norm in my model and train it on GPU-4. When I load it on GPU-5 and continue training, it errors like this:
```bash
InstanceNormalization.lua:50: arguments are located on different GPUs at
 /home/clp001/torch/extra/cutorch/lib/THC/generated/../generic/THCTensorMathReduce.cu:38
stack traceback:
        [C]: in function 'mean'
```
Then I try to continue training on the same GPU-4, it also breaks with the same error."
"Improved Texture Networks mentioned in page 7 that more details can be found in the supplementary material. However I can not find the material, could you please  offer it to me? "
""
"Instead of initializing with random, is there a way to initialize with the content image? It seems from jcjohnson/neural-style many people have gotten better results with this."
"hi, 
I am confused by the preprocess.
In the training phase, the net is fed with image substracted mean values, however, in the test phase, the net is fed with raw image and do extra deprocess. 

any help will be appreciated."
"Hi, 

For some weird reason training with johnson model terminates for me after 1300 iterations with message ""Killed"". I am using the ""default"" command line as provided in the readme file for johnson model with different style images (I tried candy style and some custom doodle styles and it is always killed after 1300 iterations). Any ideas on why this happens or how can I deal with this?

Thanks"
"No error when using model ""johnson"" in training. However, when try to use model ""pyramid"" in training, an error occurred. 

Following is the command:
th train.lua -style_image style/witch.jpg -style_size 600 -checkpoints_path checkpoint/ -checkpoints_name witch.sw3.p. -style_weight 3 -model pyramid -num_iterations 10000 -batch_size 2

Following is the output on the stdout:
torch.display not found. unable to plot
Using TV loss with weight       1e-06
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:505] Reading dangerously large protocol message.  If the message turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 574671192
Successfully loaded data/pretrained/VGG_ILSVRC_19_layers.caffemodel
conv1_1: 64 3 3 3
conv1_2: 64 64 3 3
conv2_1: 128 64 3 3
conv2_2: 128 128 3 3
conv3_1: 256 128 3 3
conv3_2: 256 256 3 3
conv3_3: 256 256 3 3
conv3_4: 256 256 3 3
conv4_1: 512 256 3 3
conv4_2: 512 512 3 3
conv4_3: 512 512 3 3
conv4_4: 512 512 3 3
conv5_1: 512 512 3 3
conv5_2: 512 512 3 3
conv5_3: 512 512 3 3
conv5_4: 512 512 3 3
fc6: 1 1 25088 4096
fc7: 1 1 4096 4096
fc8: 1 1 4096 1000
Setting up texture layer        4       :       relu1_2
Setting up texture layer        9       :       relu2_2
Setting up texture layer        14      :       relu3_2
Setting up content layer        23      :       relu4_2
Setting up texture layer        23      :       relu4_2
        Optimize        
/home/mqhuang/torch/install/bin/luajit: /home/mqhuang/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
In 1 module of nn.Concat:
In 12 module of nn.Sequential:
/home/mqhuang/torch/install/share/lua/5.1/torch/Tensor.lua:457: expecting a contiguous tensor
stack traceback:
        [C]: in function 'assert'
        /home/mqhuang/torch/install/share/lua/5.1/torch/Tensor.lua:457: in function 'view'
        ./InstanceNormalization.lua:71: in function 'updateGradInput'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Module.lua:31: in function </home/mqhuang/torch/install/share/lua/5.1/nn/Module.lua:29>
        [C]: in function 'xpcall'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Sequential.lua:84: in function </home/mqhuang/torch/install/share/lua/5.1/nn/Sequential.lua:78>
        [C]: in function 'xpcall'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Concat.lua:91: in function </home/mqhuang/torch/install/share/lua/5.1/nn/Concat.lua:47>
        [C]: in function 'xpcall'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'
        train.lua:150: in function 'opfunc'
        /home/mqhuang/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
        train.lua:175: in main chunk
        [C]: in function 'dofile'
        ...uang/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x004065d0

WARNING: If you see a stack trace below, it doesn't point to the place where this error occurred. Please use only the one above.
stack traceback:
        [C]: in function 'error'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
        /home/mqhuang/torch/install/share/lua/5.1/nn/Sequential.lua:88: in function 'backward'
        train.lua:150: in function 'opfunc'
        /home/mqhuang/torch/install/share/lua/5.1/optim/adam.lua:37: in function 'optim_method'
        train.lua:175: in main chunk
        [C]: in function 'dofile'
        ...uang/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x004065d0

The OS is ubuntu 14.04. I have one Tesla K40 and one Titan X-Pascal in the system. The error happens to both GPUs. 

Any idea is appreciated."
"```bash
ubuntu@ip-xx-x-xxx-xxx:~/texture_nets$ th train.lua -data dataset -style_image style.jpg -style_size 600 -image_size 512 -model johnson -batch_size 4 -learning_rate 1e-2 -style_weight 10 -style_layers relu1_2,relu2_2,relu3_2,relu4_2 -content_layers relu4_2
torch.display not found. unable to plot
Using TV loss with weight 	0
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 574671192
Successfully loaded data/pretrained/VGG_ILSVRC_19_layers.caffemodel
conv1_1: 64 3 3 3
conv1_2: 64 64 3 3
conv2_1: 128 64 3 3
conv2_2: 128 128 3 3
conv3_1: 256 128 3 3
conv3_2: 256 256 3 3
conv3_3: 256 256 3 3
conv3_4: 256 256 3 3
conv4_1: 512 256 3 3
conv4_2: 512 512 3 3
conv4_3: 512 512 3 3
conv4_4: 512 512 3 3
conv5_1: 512 512 3 3
conv5_2: 512 512 3 3
conv5_3: 512 512 3 3
conv5_4: 512 512 3 3
fc6: 1 1 25088 4096
fc7: 1 1 4096 4096
fc8: 1 1 4096 1000
Setting up texture layer  	4	:	relu1_2
Setting up texture layer  	9	:	relu2_2
Setting up texture layer  	14	:	relu3_2
Setting up content layer	23	:	relu4_2
Setting up texture layer  	23	:	relu4_2
        Optimize
THCudaCheck FAIL file=/tmp/luarocks-cutorch/lib/THC/generic/THCStorage.c line=182 error=77 : an illegal memory access was encountered
/home/ubuntu/torch/install/bin/luajit: cuda runtime error (77) : an illegal memory access was encountered at /tmp/luarocks-cutorch/lib/THC/generic/THCStorage.c:182
```

I get the following error with any given image. This was working about a week ago, I think a change in one of the libraries/packages used in torch has recently broken something.

The installation script can be found [here](https://github.com/algorithmiaio/deepfilter-training/blob/master/install_environment.sh)

I've tried pegging the version of cutorch by checking out to a version 2 weeks old, but that hasn't solved the problem yet.

OS version is Ubuntu 16.04.

Any constructive criticism is appreciated.

-Besir"
"I tried many times, but every thing is not like what supposed to be. Until I noticed the output message ""Setting up texture layer ""
With neural style, the output is ""Setting up style layer""
Check the codes, it's texture in it, but the param -mode is style
It seems that whatever you set -mode, it works with texture!
That's a huge mistake
"
"I'm trying to train some model, it takes many many hours to generate one. Actually I haven't got any of it after running scripts a whole day. Is it better to train it on a GPU?

I have  a AMD R2 920, but no NVIDIA, how to make this script runs on my AMD GPU?

Thank you.
"
"/home/ml/torch/install/bin/luajit: /home/ml/torch/install/share/lua/5.1/torch/File.lua:141: Unwritable object <cdata> at <?>.<?>.modules.2.<?>.biasDesc
stack traceback:
        [C]: in function 'error'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:141: in function 'writeObject'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
        /home/ml/torch/install/share/lua/5.1/nn/Module.lua:154: in function 'write'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:210: in function 'writeObject'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:235: in function 'writeObject'
        /home/ml/torch/install/share/lua/5.1/nn/Module.lua:154: in function 'write'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:210: in function 'writeObject'
        /home/ml/torch/install/share/lua/5.1/torch/File.lua:388: in function 'save'
        train.lua:203: in main chunk
        [C]: in function 'dofile'
        ...e/ml/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x00406670
"
"Hi, 

what are the correct params for train? I'm trying to train a model but the results are wrong :(
"
"The training process usually need one hour. Is it possible to use multi-gpus for training ? 
"
"Hi I use this setting: 
th train.lua -data datasets/ -style_image ../new_style/cosmo512.jpg -tv_weight 0.001

I can see the output image between model_1000.t7 to model_10000.t7. But after that, all the test image show black photo, is there something wrong with the training data?
"
"I tried to modified the models/johnson.lua's network structure, aimed to reduce running memory and shorten running time, just as follows:

model:add(pad(4, 4, 4, 4))
model:add(backend.SpatialConvolution(3, 32, 9, 9, 2, 2, 0, 0))         --src_stride=1
model:add(normalization(32))
model:add(nn.ReLU(true))  
 ..........
 ..........
model:add(nn.SpatialFullConvolution(32, 3, 3, 3, 2, 2, 1, 1, 1, 1))     --source is SpatialConvolution
model:add(normalization(3))
model:add(nn.ReLU(true))   

return model:add(nn.TVLoss(params.tv_weight))

and training dataSet is COCO, image_size=512, style_size=512, content_weight=10, style_weight=10, tv_weight=0.001,  I also tried others  content_weight and style_weight, it seemed it had no significant difference, here is my result image by tubingen_kandinsky.jpg
![tubingen512_kandinsky_512_512_w5010_9000_norgrad](https://cloud.githubusercontent.com/assets/12368977/18129320/d07e4be2-6fbc-11e6-9ad1-5d7be2de00a9.jpg)
"
"root@ubuntu:/home/fengbin/texture2# th test.lua -input_image 11.jpg -model_t7 model.t7 -cpu 
/usr/local/bin/luajit: /usr/local/share/lua/5.1/torch/File.lua:361: table index is nil
stack traceback:
    /usr/local/share/lua/5.1/torch/File.lua:361: in function 'readObject'
    /usr/local/share/lua/5.1/nn/Module.lua:154: in function 'read'
    /usr/local/share/lua/5.1/torch/File.lua:342: in function 'readObject'
    /usr/local/share/lua/5.1/torch/File.lua:391: in function 'load'
    test.lua:17: in main chunk
    [C]: in function 'dofile'
    /usr/local/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x0804d6d0
"
"Hi! I didn't manage to find in your code an option to choose the number of graphics card. If there is such an option, how to use it? 
Thank you in advance. 
Sincerely
"
"Hi Dmitry! has the instance normalization layer been implemented already? I haven't found reference in the code. Maybe you can refer me to the file/function that contains this addition. Thanks!
"
"It seems to me that the network is not being run in evaluation mode (and therefore use the population mean/variance for the batch/instance normalization layers) when running feed forward/generating styled images. Is this intentional? I read your paper and wasn't sure if 

> Differently from batch normalization, furthermore, the instance normalization layer is applied at test time as well

meant to calculate the means/variances during test time as well/not use population statistics.
"
"I'm getting the below error when I try to run `test.lua` on any image that wasn't included in the repository. Even if I download an image from the repository (i.e. `https://raw.githubusercontent.com/DmitryUlyanov/texture_nets/master/data/readme_pics/karya.jpg`) I get the same error. Has anyone else run into this?

```
/root/torch/install/bin/luajit: /root/torch/install/share/lua/5.1/image/init.lua:220: Not a JPEG file: starts with 0x2d 0x2d
stack traceback:
    [C]: in function 'load'
    /root/torch/install/share/lua/5.1/image/init.lua:220: in function 'loader'
    /root/torch/install/share/lua/5.1/image/init.lua:368: in function 'load'
    test.lua:23: in main chunk
    [C]: in function 'dofile'
    /root/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00406670
```

Similarly, for PNGs I get

```
/root/torch/install/bin/luajit: /root/torch/install/share/lua/5.1/image/init.lua:156: [read_png_file] File data/readme_pics/house.png is not recognized as a PNG file
stack traceback:
    [C]: in function 'load'
    /root/torch/install/share/lua/5.1/image/init.lua:156: in function 'loader'
    /root/torch/install/share/lua/5.1/image/init.lua:368: in function 'load'
    test.lua:23: in main chunk
    [C]: in function 'dofile'
    /root/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00406670
```
"
"Hi, i'm training a model with style ""van gogh's starry night"" and got some result as follows. there are some weird border like effects in result image no matter how i tune the weight or tvloss parameters. i also tried different image size or style size and even with cropped style image, the results' quality is slightly different but none of them the border effect is disappear. 
![wechatimg1](https://cloud.githubusercontent.com/assets/465136/17722693/b6c612fe-6466-11e6-8fe3-77ee323a3e47.jpeg)
different content image with the same effect:
![test1024_style02_48000](https://cloud.githubusercontent.com/assets/465136/17722591/f5d1a0ae-6465-11e6-913b-05ddcc58f397.jpg)

i use these parameters: -model johnson -image_size 512 -style_size 512 -content_weight 10 -style_weight 500 -learning_rate 0.001 -normalize_gradients true -tv_weight 0.000085

i checked the result after training 1000, 5000,10000 and 40000 iters, things didn't change.

has any body meet the same problem? any suggestion is appreciated, thanks!
"
"while training the data this error is coming.

`threads/threads.lua:183: [thread 1 callback] bad argument #2 to '?' (dimension 1 out of range of 0D tensor at /home/xyz/torch/pkg/torch/generic/Tensor.c:19)
stack traceback:
        [C]: at 0x7ff3c8ad2830
        [C]: in function 'xpcall'
        .../xyz/torch/install/share/lua/5.1/threads/threads.lua:234: in function 'callback'
        ...t6/xyz/torch/install/share/lua/5.1/threads/queue.lua:65: in function <...t6/xyz/torch/install/share/lua/5.1/threads/queue.lua:41>
        [C]: in function 'pcall'
        ...t6/xyz/torch/install/share/lua/5.1/threads/queue.lua:40: in function 'dojob'
        [string ""  local Queue = require 'threads.queue'...""]:13: in main chunk
stack traceback:
        [C]: in function 'error'
        .../xyz/torch/install/share/lua/5.1/threads/threads.lua:183: in function 'dojob'
        .../xyz/torch/install/share/lua/5.1/threads/threads.lua:264: in function 'synchronize'
        .../xyz/torch/install/share/lua/5.1/threads/threads.lua:142: in function 'specific'
        .../xyz/torch/install/share/lua/5.1/threads/threads.lua:125: in function 'Threads'
        ./dataloader.lua:52: in function '__init'
        ...xyza/torch/install/share/lua/5.1/torch/init.lua:91: in function <...xyz/torch/install/share/lua/5.1/torch/init.lua:87>
        [C]: in function 'DataLoader'
        ./dataloader.lua:28: in function 'create'
        train.lua:79: in main chunk
        [C]: in function 'dofile'
        ...xyz/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x00406670
`
"
"I am training on MS_COCO dataset. i am running this command

th train.lua -data /train/dummy -style_image data/textures/cezanne.jpg

but i am getting error
`/usr/local/bin/luajit: /usr/local/share/lua/5.1/trepl/init.lua:383: module 'display' not found:No LuaRocks module found for display
        no field package.preload['display']
        no file '/home/bfomitchev/.luarocks/share/lua/5.1/display.lua'
        no file '/home/bfomitchev/.luarocks/share/lua/5.1/display/init.lua'
        no file '/usr/local/share/lua/5.1/display.lua'
        no file '/usr/local/share/lua/5.1/display/init.lua'
        no file './display.lua'
        no file '/usr/local/share/luajit-2.1.0-beta1/display.lua'
        no file '/home/guest6/.luarocks/share/lua/5.1/display.lua'
        no file '/home/guest6/.luarocks/share/lua/5.1/display/init.lua'
        no file '/home/bfomitchev/.luarocks/lib/lua/5.1/display.so'
        no file '/usr/local/lib/lua/5.1/display.so'
        no file './display.so'
        no file '/usr/local/lib/lua/5.1/loadall.so'
        no file '/home/guest6/.luarocks/lib/lua/5.1/display.so'
stack traceback:
        [C]: in function 'error'
        /usr/local/share/lua/5.1/trepl/init.lua:383: in function 'require'
        train.lua:10: in main chunk
        [C]: in function 'dofile'
        /usr/local/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x00406670
`

Thanks
"
"Hi,
Does the current version texture_nets support _instance normalisation_? I could not find function like that in the project, instead, a BatchNormalisation found in the utils.lua.

Best
"
"Hi, since my GPU is not much good for full image size in test method, and i want to check some .t7 models generated from other machine. i think if test can support CPU(the speed is not too slow i think anyway), it will be more helpful.
"
"Hi, for what is model.state? thanks
"
"Hi, I noticed a place( in part `Network architecure` in `Section 3.2 Generator network for texture synthesis` ) in your paper on texture nets where you mentioned that you use different scale of noise tensors which are `1/2M, 1/4M, ....`of the image size repectively. However, in your code on texture nets, you do use `noise tensor` of size `M x M`, which is the same as the input image size. Does this means that in your paper, variable `i` should be choosen to be `i = 0， 1, ..., K-1`, other than i starting from index 1 up to K
"
"Can you share the error plots for training? How many times does the error get reduced when compared to initial error? 
"
"I have this directory structure: 
DmitryCode
 ---   train
 ---   val
 ---   texture_nets

And when I launch:   ""th train.lua -data ../ -style_image style.jpg""
I get this error :(
"" 
=> Generating list of images
 | finding all validation images
find: ‘standard output’: Tubería rota
find: write error
/home/pixie/pixify/torch/install/bin/luajit: /home/pixie/pixify/trainlua/datasets/style-gen.lua:67: class not found: val
stack traceback:
        [C]: in function 'assert'
        /home/pixie/pixify/trainlua/datasets/style-gen.lua:67: in function 'findImages'
        /home/pixie/pixify/trainlua/datasets/style-gen.lua:102: in function 'exec'
        ./datasets/init.lua:28: in function 'create'
        ./dataloader.lua:24: in function 'create'
        train.lua:79: in main chunk
        [C]: in function 'dofile'
        ...xify/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
        [C]: at 0x004064d0
""

Somebody knows how could I fix this? Thanks 
"
"I am trying to figure out how to make use of display. Is there a default IP/Port used to connect via the browser to view images at every 50th iteration?

I am not seeing ip/port initialisation in the code so I assume there must be some default values like 127.0.0.1:80... but I don't get anything there.

Based on the actual code display need to be installed for train to work... so it appear to be a hard requirement.
"
"Hi, It's great to see the code updating. But I cannot find a pretrained model in this version. It will be very nice if you can upload a pretrained model by which I can have a test quickly!

Thank you~
"
"When I ran the code:th stylization_process.lua -input_image data/readme_pics/kitty.jpg -model data/out/model.t7  -noise_depth 16  -save_path stylized.png.It just told me:...torch/install/share/lua/5.1/cudnn/SpatialConvolution.lua:108: input has to contain: 16 feature maps, but received input of size: 1 x 19 x 29 x 29.
The model.t7 is the one that produced by:th texture_train.lua -texture data/textures/red-peppers256.o.jpg -model_name pyramid -backend cudnn -num_iterations 1500 -vgg_no_pad true -normalize_gradients true -batch_size 15.
I don't know where is wrong.Could you give a solution?Thanks a lot.
Looking forward to your reply.
![snapshot1](https://cloud.githubusercontent.com/assets/13002561/17058660/23cc1a78-5054-11e6-9392-46c3d2e73fb8.png)
"
"hi @DmitryUlyanov 

Recently, I have used your code to train my own style. I finish the training process, however, it it very difficult to get a plausible result based my own image style. There are many hyper parameters may affect the result. So any advice to get a stable model?
"
"Just decided to have a bit of fun with stylized images, but I got the symbol lookup error. I have installed all the needed dependencies through the script several times, but in vain.

Here is what I got:

```
Successfully loaded data/pretrained/VGG_ILSVRC_19_layers.caffemodel
conv1_1: 64 3 3 3
conv1_2: 64 64 3 3
conv2_1: 128 64 3 3
conv2_2: 128 128 3 3
conv3_1: 256 128 3 3
conv3_2: 256 256 3 3
conv3_3: 256 256 3 3
conv3_4: 256 256 3 3
conv4_1: 512 256 3 3
conv4_2: 512 512 3 3
conv4_3: 512 512 3 3
conv4_4: 512 512 3 3
conv5_1: 512 512 3 3
conv5_2: 512 512 3 3
conv5_3: 512 512 3 3
conv5_4: 512 512 3 3
fc6: 1 1 25088 4096
fc7: 1 1 4096 4096
fc8: 1 1 4096 1000
conv1_1 : padding set to 0  
Setting up texture layer    2   :   relu1_1 
/usr/bin/luajit: symbol lookup error: /usr/lib/lua/5.1/libTHCUNN.so: undefined symbol: THC_overlappingIndices
```

Thank you for your help. 
"
"Command:
th stylization_train.lua -style_image data/textures/style.jpg -train_hdf5 data/256.hdf5 -noise_depth 3 -model_name pyramid2 -normalize_gradients true -train_images_path data/256.hdf5 -content_weight 0.8 -backend cudnn

Error:
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:537] Reading dangerously large protocol message.  If the message turns out to be larger than 1073741824 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
[libprotobuf WARNING google/protobuf/io/coded_stream.cc:78] The total number of bytes read was 574671192
Successfully loaded data/pretrained/VGG_ILSVRC_19_layers.caffemodel
conv1_1: 64 3 3 3
conv1_2: 64 64 3 3
conv2_1: 128 64 3 3
conv2_2: 128 128 3 3
conv3_1: 256 128 3 3
conv3_2: 256 256 3 3
conv3_3: 256 256 3 3
conv3_4: 256 256 3 3
conv4_1: 512 256 3 3
conv4_2: 512 512 3 3
conv4_3: 512 512 3 3
conv4_4: 512 512 3 3
conv5_1: 512 512 3 3
conv5_2: 512 512 3 3
conv5_3: 512 512 3 3
conv5_4: 512 512 3 3
fc6: 1 1 25088 4096
fc7: 1 1 4096 4096
fc8: 1 1 4096 1000
Setting up texture layer    2   :   relu1_1 
Setting up texture layer    7   :   relu2_1 
Setting up texture layer    12  :   relu3_1 
Setting up texture layer    21  :   relu4_1 
Setting up content layer    23  :   relu4_2 
Setting up texture layer    30  :   relu5_1 
WARNING: Skipping content loss  
        Optimize  
/home/ai2/torch/install/bin/luajit: stylization_train.lua:110: attempt to concatenate a nil value
stack traceback:
    stylization_train.lua:110: in function 'get_input_train'
    stylization_train.lua:141: in function 'opfunc'
    /home/ai2/torch/install/share/lua/5.1/optim/adam.lua:33: in function 'optim_method'
    stylization_train.lua:181: in main chunk
    [C]: in function 'dofile'
    .../ai2/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00405d50

Any idea ???
"
"Command:
th stylization_process.lua -model supplementary/stylization_models/mosaic_style.t7 -input_image data/textures/tiger.jpg -noise_depth 3

/home/ai2/torch/install/bin/luajit: /home/ai2/torch/install/share/lua/5.1/nn/Container.lua:67: 
In 1 module of nn.Sequential:
In 1 module of nn.Concat:
In 1 module of nn.Sequential:
In 1 module of nn.Concat:
In 1 module of nn.Sequential:
In 1 module of nn.Concat:
In 1 module of nn.Sequential:
In 1 module of nn.Concat:
In 2 module of nn.Sequential:
bad argument #3 to '?' (number expected, got nil)
stack traceback:
    [C]: at 0x7fd61fcb31a0
    [C]: in function '__newindex'
    ./src/utils.lua:148: in function <./src/utils.lua:144>
    [C]: in function 'xpcall'
    /home/ai2/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
    /home/ai2/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function </home/ai2/torch/install/share/lua/5.1/nn/Sequential.lua:41>
    [C]: in function 'xpcall'
    /home/ai2/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
    /home/ai2/torch/install/share/lua/5.1/nn/Concat.lua:12: in function </home/ai2/torch/install/share/lua/5.1/nn/Concat.lua:9>
    [C]: in function 'xpcall'
    ...
    [C]: in function 'xpcall'
    /home/ai2/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
    /home/ai2/torch/install/share/lua/5.1/nn/Concat.lua:12: in function </home/ai2/torch/install/share/lua/5.1/nn/Concat.lua:9>
    [C]: in function 'xpcall'
    /home/ai2/torch/install/share/lua/5.1/nn/Container.lua:63: in function 'rethrowErrors'
    /home/ai2/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
    stylization_process.lua:38: in main chunk
    [C]: in function 'dofile'
    .../ai2/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00405d50

WARNING: If you see a stack trace below, it doesn't point to the place where this error occured. Please use only the one above.
stack traceback:
    [C]: in function 'error'
    /home/ai2/torch/install/share/lua/5.1/nn/Container.lua:67: in function 'rethrowErrors'
    /home/ai2/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
    stylization_process.lua:38: in main chunk
    [C]: in function 'dofile'
    .../ai2/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00405d50
"
"Hi, I found that your stylization_process couldn't adaptive to different resolution,just using the resolution 800*800,etc. I fixed it. 
"
"well, i run the pretrain model code , and i encounter the problem:attempt to index field 'gradInput' (a nil value).And i think i use the gpu, and what's wrong?
"
"Hi. out of memory at /home/whr/torch-cl/extra/cutorch/lib/THC/generic/THCStorage.cu:40  when i run the  ""th texture_train.lua -texture data/textures/red-peppers256.o.jpg -model_name pyramid -backend cudnn -num_iterations 1500 -vgg_no_pad true -normalize_gradients true"". My GPU is Nvidia Black Titan X. What's wrong? Thx.
"
"hello! any chance you can help me trying to make it work on CPU only mode? I read about the Batchnormalization error due to your mistake, can you give details?
thanks!
"
"Original code:

``` lua
        -- Batch norm before merging   -- line 40
        seq:add(bn(conv_num))          -- line 41
        cur_temp:add(bn(conv_num))     -- line 42
```

Maybe the code below could be better:

``` lua
        -- Batch norm before merging       -- line 40
        seq:add(bn(conv_num))              -- line 41
        cur_temp:add(bn(conv_num*(i-1)))   -- line 42
```

![texture_nets_bug](https://cloud.githubusercontent.com/assets/7439398/14075265/1705b9d4-f50a-11e5-85cf-61a0e8005e9d.png)
"
"Hi

I've run the command:

`th stylization_process.lua -model supplementary/stylization_models/starry_style.t7 -input_image data/readme_pics/kitty.jpg -noise_depth 3`

I get an error which corresponds with the GenNoise class `updateOuput`:

`/home/siavash/torch/install/bin/luajit: bad argument #3 to '?' (number expected, got nil)
stack traceback:
    [C]: at 0x7f10f8684f30
    [C]: in function '__newindex'
    ./src/utils.lua:138: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Concat.lua:12: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Concat.lua:12: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Concat.lua:12: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Concat.lua:12: in function 'updateOutput'
    /home/siavash/torch/install/share/lua/5.1/nn/Sequential.lua:44: in function 'forward'
    stylization_process.lua:37: in main chunk
    [C]: in function 'dofile'
    ...vash/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:145: in main chunk
    [C]: at 0x00405d70
`
"
"I tried 
`th texture_train.lua -texture data/textures/red-peppers256.o.jpg -model_name pyramid -backend nn -num_iterations 1500 -vgg_no_pad -normalize_gradients`
but there's an error message: `module 'src/utils.lua' not found:No LuaRocks module found for src/utils.lua`

I also used th in the root directory of texture_net, and typed `require 'src/utils.lua'`, the error message was the same.

I'm very new to Torch, and I almost installed all the packages. It seems like a path setting issue, but I have no idea how to fix it. Could anyone give me some hint? 
Thanks!
"
"Excuse me, recently our group is working on weakly supervised segmentation task, and we want to use the HTK methods to be one of the baselines. But the data in /master/examples)/cvpr2017/ can not be downloaded through the urls. Could you update your links in your free time? That's very much appreciated."
"Page: https://github.com/alexanderrichard/squirrel/tree/master/examples/cvpr2017
The URL to download the data and trained model are both not accessible anymore.
1) download the data:
    **wget -O data.zip https://uni-bonn.sciebo.de/index.php/s/yt0MHmQJNzQalJR/download**
    unzip data.zip
    rm data.zip
2) train the model:
    ./train.sh
   OR download our trained model for Breakfast here:
   **https://uni-bonn.sciebo.de/index.php/s/lFTlv5obv8JZkc5/download**"
"Hi, is it also possible to release the code for Fisher vector computation?
""The Fisher vector representation for each frame is computed over a sliding window of 20 frames"" - Does it mean that each frame corresponds to one sliding window of 20 frames (with this frame in the middle of the window?), and all the 15-frame-dense-trajectories which are completely included in this sliding window are used to compute one 8192-d (2x64x64) Fisher vector for this frame? Thanks in advance.
Regards"
"\# pwd
$squirrel_root_dir/examples/cvpr2017
\# ./train.sh 
Maximum number of threads for CPU matrix operations: 1
Maximum number of threads for CPU matrix operations: 1


Seems it just run in cpu mode, could anyone help to tell me how to set it running in gpu mode? Thanks."
"what parameter need to be passed to evaluate early prediction and  taxonomy prediction for fbnet,resnet,vgg."
"In the research paper, feedback Net performance is computed for depth 12,8 and 4 but this code is not running for that given depth."
Do I still need to use the 'rnn' module from Element-Research or using the 'rnn' module provided by torch works fine?
"@amir32002 Hi, amir, really appreciate for your awesome work. I want to ask that whether you will release the Pytorch implementation version. Thanks very much !!!"
"Hi, @amir32002 ,

I got the following ""module 'ConvLSTM_bn"" error:

```
root@milton-OptiPlex-9010:/data/code/feedback-networks# th main.lua --help
/root/torch/install/bin/luajit: /root/torch/install/share/lua/5.1/trepl/init.lua:389: /root/torch/install/share/lua/5.1/trepl/init.lua:389: module 'ConvLSTM_bn' not found:No LuaRocks module found for ConvLSTM_bn
	no field package.preload['ConvLSTM_bn']
	no file '/root/.luarocks/share/lua/5.1/ConvLSTM_bn.lua'
	no file '/root/.luarocks/share/lua/5.1/ConvLSTM_bn/init.lua'
	no file '/root/torch/install/share/lua/5.1/ConvLSTM_bn.lua'
	no file '/root/torch/install/share/lua/5.1/ConvLSTM_bn/init.lua'
	no file './ConvLSTM_bn.lua'
	no file '/root/torch/install/share/luajit-2.1.0-beta1/ConvLSTM_bn.lua'
	no file '/usr/local/share/lua/5.1/ConvLSTM_bn.lua'
	no file '/usr/local/share/lua/5.1/ConvLSTM_bn/init.lua'
	no file '/root/.luarocks/lib/lua/5.1/ConvLSTM_bn.so'
	no file '/root/torch/install/lib/lua/5.1/ConvLSTM_bn.so'
	no file '/root/torch/install/lib/ConvLSTM_bn.so'
	no file './ConvLSTM_bn.so'
	no file '/usr/local/lib/lua/5.1/ConvLSTM_bn.so'
	no file '/usr/local/lib/lua/5.1/loadall.so'
stack traceback:
	[C]: in function 'error'
	/root/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	main.lua:14: in main chunk
	[C]: in function 'dofile'
	/root/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405e40
root@milton-OptiPlex-9010:/data/code/feedback-networks# 
root@milton-OptiPlex-9010:/data/code/feedback-networks# 
root@milton-OptiPlex-9010:/data/code/feedback-networks# th main.lua --help
/root/torch/install/bin/luajit: /root/torch/install/share/lua/5.1/trepl/init.lua:389: /root/torch/install/share/lua/5.1/trepl/init.lua:389: module 'ConvLSTM_bn' not found:No LuaRocks module found for ConvLSTM_bn
	no field package.preload['ConvLSTM_bn']
	no file '/root/.luarocks/share/lua/5.1/ConvLSTM_bn.lua'
	no file '/root/.luarocks/share/lua/5.1/ConvLSTM_bn/init.lua'
	no file '/root/torch/install/share/lua/5.1/ConvLSTM_bn.lua'
	no file '/root/torch/install/share/lua/5.1/ConvLSTM_bn/init.lua'
	no file './ConvLSTM_bn.lua'
	no file '/root/torch/install/share/luajit-2.1.0-beta1/ConvLSTM_bn.lua'
	no file '/usr/local/share/lua/5.1/ConvLSTM_bn.lua'
	no file '/usr/local/share/lua/5.1/ConvLSTM_bn/init.lua'
	no file '/root/.luarocks/lib/lua/5.1/ConvLSTM_bn.so'
	no file '/root/torch/install/lib/lua/5.1/ConvLSTM_bn.so'
	no file '/root/torch/install/lib/ConvLSTM_bn.so'
	no file './ConvLSTM_bn.so'
	no file '/usr/local/lib/lua/5.1/ConvLSTM_bn.so'
	no file '/usr/local/lib/lua/5.1/loadall.so'
stack traceback:
	[C]: in function 'error'
	/root/torch/install/share/lua/5.1/trepl/init.lua:389: in function 'require'
	main.lua:14: in main chunk
	[C]: in function 'dofile'
	/root/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405e40

```


Any suggestion to fix this issue?

THX!"
"So colorization as a proxy task in self-supervised learning just means that we pretrained the vgg16 or ResNet network for image colorization, and then transfer the weights in the pretrained model to the downstream task?
What about features that we get?"
The `test()` function is missing. Also if you have code to finetune vgg-16 and alexnet models for semantic segmentation that will be very useful to make a fair comparison against your published results.
"Hi Gustav,

I downloaded the pretrained VGG-16 model from http://people.cs.uchicago.edu/~larsson/color-proxy/models/vgg16.caffemodel.h5 and tried to fine tune it for pascal classification using the script in `selfsup/evaluate/__main__.py` . Unfortunately, the pretrained model's `conv1_1` filters are meant for single channel grayscale inputs but the model defined by `voc_classification.py` is for colour images. This results in the following asserting failure 

```
(tensorflow3) aravindh@gnodeb1:~/projects/self-supervision$ CUDA_VISIBLE_DEVICES=0 python3 selfsup/evaluate/ voc2007-classification /users/aravindh/scratch/autocolorize/vgg16.caffemodel.h5 -n vgg16 --output /users/aravindh/scratch/self_supervision/gustavia/voc_vgg16/classification/ --limit 100 
Traceback (most recent call last):
  File ""/usr/lib64/python3.4/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.4/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""selfsup/evaluate/__main__.py"", line 26, in <module>
    main()
  File ""selfsup/evaluate/__main__.py"", line 23, in main
    time_limit=args.limit, iterations=args.iterations, network_type=args.network)
  File ""/users/aravindh/projects/self-supervision/selfsup/evaluate/voc_classification.py"", line 476, in train_and_test
    train(*args, **kwargs)
  File ""/users/aravindh/projects/self-supervision/selfsup/evaluate/voc_classification.py"", line 191, in train
    network_type=network_type)
  File ""/users/aravindh/projects/self-supervision/selfsup/evaluate/voc_classification.py"", line 91, in build_network
    use_dropout=True)
  File ""/users/aravindh/projects/self-supervision/selfsup/model/vgg16.py"", line 362, in build_network
    z = conv(z, 64, name='conv1_1')
  File ""/users/aravindh/projects/self-supervision/selfsup/model/vgg16.py"", line 303, in conv
    return vgg_conv(z, num(ch), **kwargs)
  File ""/users/aravindh/projects/self-supervision/selfsup/model/vgg16.py"", line 164, in vgg_conv
    assert W_shape is None or tuple(W_shape) == tuple(shape), ""Incorrect weights shape for {} (file: {}, spec: {})"".format(name, W_shape, shape)
AssertionError: Incorrect weights shape for conv1_1 (file: (3, 3, 1, 64), spec: [3, 3, 3, 64])
```

In order to replicate the results in column 1 of table 1 in your paper (http://arxiv.org/pdf/1703.04044.pdf), please let me know what should be changed.

Best wishes,
Aravindh Mahendran"
"Hi Gustav,

I got the output from  the neural net but its a (1,32) vector of h and c values whereas the input image had dimensions (514,514). I compared it with the output of the default neural net in autocolorize  and it generates (1,32,512,512) output. Could you please see and provide any changes to the model needed to get a colorized output image?

Regards,
Abhay"
"Hi @gustavla,

I tried running the Tensorflow code that you have uploaded using the following instructions that you mentioned:

```
import deepdish as dd
import selfsup.model.vgg16

data = dd.io.load('vgg16.caffemodel.h5')
x = tf.placeholder(tf.float32, shape=[1, 224, 224, 3], name='x')
phase_test = tf.placeholder(tf.bool, name='phase_test')
z = selfsup.model.vgg16.build_network(x, parameters=data, phase_test=phase_test)
```

The code gets stuck somewhere. I did a preliminary debug and it seems that  it occurs on calls to `vgg_conv()` in line 302 of vgg16.py. I ran it on a titanx gpu for 4 hours and it was not able to return from `build_network()`. Can you please see if you can fix it?

Regards,
Abhay"
The paper inspires me a lot. Great work! I am wondering when the code will be released. Looking forward to it.  Thanks a lot.
"Hi,

I am working with Prof. Erik Learned Miller from UMass Amherst. We are trying to use the pretrained models but can not find the testing code in the repository. Can you please upload it or point to the right direction?

Regards,
Abhay Mittal"
"Torch version 0.3 is too old. If I use version 1.0 or above, what changes should I make？
Thanks for your code😄"
"Config:

python 3.6.4
torch 1.2.0

This error is frequently triggered stopping the training.

Traceback (most recent call last):
  File ""learning/main.py"", line 607, in <module>
    main()
  File ""learning/main.py"", line 455, in main
    train_metrics, _ = train()
  File ""learning/main.py"", line 296, in train
    outputs = model.ecc(embeddings[0], clouds_data[4:6])
  File ""/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/graphnet.py"", line 145, in forward
    input = module(input)
  File ""/home/thomas/.pyenv/versions/spg3.6.4/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/modules.py"", line 88, in forward
    input = ecc.GraphConvFunction(nc, nc, idxn, idxe, degs, degs_gpu, self._edge_mem_limit)(hx, weights)
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/ecc/GraphConvModule.py"", line 67, in forward
    cuda_kernels.conv_aggregate_fw(output.narrow(0,startd,numd), products.view(-1,self._out_channels), self._degs_gpu.narrow(0,startd,numd))
  File ""/home/thomas/HELIX/superpoint-graph-job/superpointgraph2/learning/../learning/ecc/cuda_kernels.py"", line 123, in conv_aggregate_fw
    csdegs = torch.cumsum(degs,0)
RuntimeError: scan failed to synchronize: an illegal memory access was encountered

Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 193, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 82, in cupy.cuda.driver.check_status
TypeError: 'NoneType' object is not callable
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 193, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 82, in cupy.cuda.driver.check_status
TypeError: 'NoneT"
"Hey,

I am working deeply on your code.
I would like to ask you a favor, and if you could please help me to understand the cuda kernels.
My email adress is thomasc@helix.re

I have benchmark your code vs pytorch geometric own implementation and I would say yours is approximately 3 times faster to train.

Best,"
"I am trying to replicate the MNIST classification experiment through ECC by following the model configuration reported in your paper. There are some issues that I was not able to solve:
1 - the described network configuration (  C(16)-MP(2,3.4)-C(32)-MP(4,6.8)-C(64)-MP(8,30)-C(128)-D(0.5)-FC(10) ) does not return the proper output dimensions. It seems there is some pooling layer(s) missing. Could you confirm that?
Maybe I am missing something in the following description (network configuration in Section 4.4), when a 4x4 to 1 points map is quoted, how should I get points down-sampling with convolutional layer in ECC?
2 -  Could you provide further details about data augmentation and other parameters settings for the same MNIST experiment? "
https://github.com/rusty1s/pytorch_scatter
"Hi,
I would like to ask if it is possible to you to explain the backpropagation step of the graph convolution. I would like to know if you can give the equations and their correspondence to the code. I am trying to understand how is the derivative calculated, but I am not able to do it. 

Sorry if it is a silly question.."
"Hi,

I have some issues executing your code.  First, I tried to execute your example with modelnet 10 using the command provided. It seemed to work but an advanced epoch the code crash with this error:


```
Traceback (most recent call last):
  File ""main.py"", line 315, in <module>
    main()
  File ""main.py"", line 217, in main
    acc_train, loss, t_loader, t_trainer = train(epoch)
  File ""main.py"", line 155, in train
    loss_meter.add(loss.data[0])
RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /pytorch/torch/lib/THC/generic/THCStorage.c:32

Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
=
```
I executed the code several times and the error appears randomly, it is not always in the same epoch, also it is not appearing in the same part of the code, here you can see an other example of the error:

```
File ""main.py"", line 315, in <module>
    main()
  File ""main.py"", line 217, in main
    acc_train, loss, t_loader, t_trainer = train(epoch)
  File ""main.py"", line 152, in train
    loss.backward()
  File ""/projects/env/ecc/lib/python3.6/site-packages/torch/autograd/variable.py"", line 167, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)
  File ""/projects/env/ecc/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 99, in backward
    variables, grad_variables, retain_graph)
RuntimeError: cublas runtime error : an internal operation failed at /pytorch/torch/lib/THC/THCBlas.cu:247

Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Exception ignored in: 'cupy.cuda.function.Module.__dealloc__'
Traceback (most recent call last):
  File ""cupy/cuda/driver.pyx"", line 159, in cupy.cuda.driver.moduleUnload
  File ""cupy/cuda/driver.pyx"", line 75, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
```


I tried different versions of pytorch: 0.2 0.3 0.4. The three versions was installed using pip, and also I tried to execute the code with a compiled from source version (0.2) the same error appears. I am using a machine with: 60gb of ram, Intel Xeon and a titan X with 12gb of ram. Moreover I tried to use different versions of open3d: 0.2.0 and 0.3.0. Finally I modified your sample command and I add edge_mem_limit in order to limit the memory used on the gpu without success.

Also I tested the code using the Sydney Urban Objects example, but in this case, this error is appearing at the begging of the execution:

```
File ""main.py"", line 315, in <module>
    main()
  File ""main.py"", line 217, in main
    acc_train, loss, t_loader, t_trainer = train(epoch)
  File ""main.py"", line 148, in train
    outputs = model(inputs)
  File ""/project/env/ecc/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/project/code/ecc/models.py"", line 103, in forward
    input = module(input)
  File ""/project/env/ecc/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/project/code/ecc/ecc/GraphConvModule.py"", line 171, in forward
    return GraphConvFunction(self._in_channels, self._out_channels, idxn, idxe, degs, degs_gpu, self._edge_mem_limit)(input, weights)
  File ""/project/code/ecc/ecc/GraphConvModule.py"", line 63, in forward
    self._multiply(sel_input, sel_weights, products, lambda a: a.unsqueeze(1))
  File ""/project/code/ecc/ecc/GraphConvModule.py"", line 36, in _multiply
    torch.bmm(f_a(a) if f_a else a, f_b(b) if f_b else b, out=out)
TypeError: torch.bmm received an invalid combination of arguments - got (torch.DoubleTensor, torch.FloatTensor, out=torch.DoubleTensor), but expected (torch.DoubleTensor source, torch.DoubleTensor mat2, *, torch.DoubleTensor out)
```

Please can you give me some hint in order to solve the issues?

Thanks,"
"Hi, it seems that you did not sample points in Urban Sydney Dataset to ensure they have the same point number. "
"Hi. Great paper and great work. I installed pytorch -0.3 in a conda environment and installed dependencies. The code runs successfully. 

I was wondering when the code for generalized graph classification shall be released?"
"pretrained model link to 404, do you have another link to the captioning model?"
"Hi, 

we would like to use densecap to predict caption for already computed bounding boxes. 
I tried using the `im_detect()` function in `/lib/fast_rcnn/test.py` which has a `boxes` argument. 

I would expect the function to output the same number of boxes as i put in, which does not happen. Instead,  it looks like the network predicts new boxes in the RPN. 

I tried setting the `cfg.TEST.HAS_RPN` parameter to `False` in order to load the rois blobs in the `_get_blobs()` function -> The boxes are loaded into `blobs`, but this has no effect on the outcome. Are they used at all in this case?

Do i need to adjust the feature_net (vgg_region_global_feature.prototxt) in some way or set some other parameters in order for the network to work as expected? Or did I miss something else? 

Thanks 
"
"Hey,

I've been trying to get the system to work for a couple of days now, but keep on running into trouble with Caffe.

When running the `demo.py` in lib/tools, I get the  following error message:

```
[libprotobuf ERROR google/protobuf/text_format.cc:274] Error parsing text-format caffe.NetParameter: 426:18: Message type ""caffe.LayerParameter"" has no field named ""reshape_param"".

WARNING: Logging before InitGoogleLogging() is written to STDERR
F0314 06:48:26.013850 25088 upgrade_proto.cpp:928] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: models/dense_cap/vgg_region_global_feature.prototxt
```

Could you provide a link to the specific Caffe version / fork that you currently use for the project? I understand that you need Ross Girschick's fork for Fast R-CNN to enable ROI pooling. My system has CUDA 8.0 and CuDNN 7.1."
"I want to reproduce your code and want to run it on Visual Genome1.4 dataset. However, I cannot find the corresponding TXT file for train, val, and test when loading the dataset. Can you put these three files on GitHub?"
Nothing
Nothing
"I installed caffe on ubuntu 18.04 using sudo apt install caffe-cuda.
Importing on python3 works, but when I run 
python3 ./lib/tools/demo.py --image images/ --gpu 0 --net VGG_ILSVRC_16_layers.caffemodel 
an error occurred:
ModuleNotFoundError: No module named 'caffe._caffe'
How I can fix this?

"
"Hi
Dear Linjie,

I want to get class names before captioning (e.g. women or man and etc) is this possible to show me an example to do that in this project?
is there any variable in ./lib/tools/demo.py (fast_rcnn/test.py) in densecap project or I need to implant py-faster-rcnn into this project ?

**I will appreciate you if you help me to get object classes in densecap project.**

Thank you so much. "
"Hi! This could be a minor question, in the paper it was mentioned the bounding boxes with IoU higher than 0.7 are merged into one. In such cases, how do you merge the caption for each bounding box accordingly during training? Because if I understand correctly each box should originally have one phrase? 

Or I got it wrong, there was no merging of the bounding boxes during training phase at all?"
"When following the instructions in the README to train a model the `dense_cap_train.sh` throws an error.

```
[...]
models/dense_cap/solver_joint_inference.prototxt
WARNING: Logging before InitGoogleLogging() is written to STDERR
F1026 16:31:58.601896  5156 io.cpp:36] Check failed: fd != -1 (-1 vs. -1) File not found: models/dense_cap/solver_joint_inference.prototxt
```

The file models/dense_cap/solver_joint_inference.prototxt appears to be missing. I simply renamed the `solver_joint_inference_finetune.prototxt` file in order to train the model but I am not sure whether this is the correct approach. 

Is it possible to use the `solver_joint_inference_finetune.prototxt` file? If not, could you publish the correct file? "
"Hello again.

So, i've launched ""as_rigid_as_possible"" experiment, trained a model, it saved in the pts folder. But what's next? How to use it to draw something?"
"Hello and thanks for your code.

Don't know if i'll get any answers, but anyway.

I'm trying to launch main.py (with default settings) in mesh_mnist folder (i've downloaded mesh mnist of course from the google drive links). Training crashes on the line:
`outputs = model(inputs, laplacian, mask)`

If we'll look deeper, it crashes here:
`xs = [x, torch.mm(L,x.view(-1, feat)).view(batch, node, feat)]` (167 line of src/utils/utils_pt.py file).

It says `The expanded size of the tensor (64) must match the existing size (503) at non-singleton dimension 0.  Target sizes: [64, 64].  Tensor sizes: [503, 64]`

It is torch.mm failing because L is (64,503,503) and x.view(-1, feat) is (32192, 64). By the way, as i know, torch.mm uses 2D tensors as input, no? Well, does anyone launched that and could advice something?"
""
"I would like to try running ModelNet40 classification task and hard to find preprocessing the data.
Could you share the code for the preprocessing?
Thank you. "
"In line 104 of main.py in mesh_mnist:
Di.append(samples[ind]['Di'])
It has errors as KeyError. But I couldn't understand where is 'Di' in samples which are training dataset. 

In line 255~259 of main.py in dense_correspondence. It also has errors as log doesn't exist. I wonder how can I run the code without existing log files?"
"Thank you for the great code!
I have a problem. When i run the program on gpu,  the output is as follows:

`Load data
Preprocess Dataset
100% (60000 of 60000) |####################| Elapsed Time: 0:00:20 Time: 0:00:20
100% (10000 of 10000) |####################| Elapsed Time: 0:00:03 Time: 0:00:03
Num parameters 90314
N/A% (0 of 937) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--Traceback (most recent call last):
  File ""main.py"", line 213, in <module>
    main()
  File ""main.py"", line 155, in main
    outputs = model(inputs, laplacian, mask)
  File ""/home/jiang/work/ping/local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/jiang/work/ping/SurfaceNetworks/src/mesh_mnist/models.py"", line 43, in forward
    x = self._modules['rn{}'.format(i)](L, mask, x)
  File ""/home/jiang/work/ping/local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/jiang/work/ping/SurfaceNetworks/src/utils/utils_pt.py"", line 125, in forward
    xs = [x, SparseBMMFunc()(L, x)]
  File ""/home/jiang/work/ping/SurfaceNetworks/src/utils/cuda/sparse_bmm_func.py"", line 39, in forward
    col_ind, col_ptr = batch_csr(matrix1._indices(), matrix1.size())
  File ""/home/jiang/work/ping/SurfaceNetworks/src/utils/cuda/batch_csr.py"", line 39, in __call__
    m.load(bytes(ptx.encode()))
  File ""cupy/cuda/function.pyx"", line 175, in cupy.cuda.function.Module.load
  File ""cupy/cuda/function.pyx"", line 176, in cupy.cuda.function.Module.load
  File ""cupy/cuda/driver.pyx"", line 141, in cupy.cuda.driver.moduleLoadData
  File ""cupy/cuda/driver.pyx"", line 72, in cupy.cuda.driver.check_status
cupy.cuda.driver.CUDADriverError: CUDA_ERROR_NOT_INITIALIZED: initialization error
100% (937 of 937) |########################| Elapsed Time: 0:00:00 Time: 0:00:00`

Is there any problem with my operation?

**System information**

- Python version: 2.7.15
- CUDA/cuDNN version: 10.0.130 / 7.5.0
- GPU model and memory: Nvidia GeForce GTX 980
- Nvidia driver version: 410.48
- Linux Ubuntu 18.04"
"Hello,

First, thank your for publishing your code here :)

I would like to run the neural network to test it, and try to adapt it to what I want to do.

I succeded to preprocess the data, by running add_laplacian.py on my meshes.

But now, I don't understand what to do next. I didn't find any documentation about it, except the first readme.

I tried to run train_4_normals.py, but I have issues for missing modules : 

```
ModuleNotFoundError: No module named 'pyigl'
``` 

It seems it is related to what you wrote in the Readme :
(optional)Python bindings for libigl is used for geometry processing. It's not required for reproducing the experiment, but if you would like to recompute Laplacian, Dirac etc, it would be convenient to install through pip install git+https://github.com/jiangzhongshi/libigl@cluster-pyigl#egg=pyigl

However, this command doesn't work anymore, it can't find the correct branch.

Can you help me a bit, by telling what I'm doing wrong, or what I have to do ?

Best Regards,
Vlad"
"'Di.append(samples[ind]['Di'])
DiA.append(samples[ind]['DiA'])':
KeyError:'Di'

i looked into the samples and saw no elements named 'Di' nor 'DiA'...
should them be 'V' and 'F' instead?

thank u ;)
"
"I can not find the code to calculate the Inter-human score. I use ""kendalltau"" provided by ""scipy.stats"" to get the mean value(0.739) of AB and AC which is not matched with yours. (ABC is assumed as three scorers in Flickr8k)
However, if I keep 2 decimal places, the result is 0.73 which matched with the metric ""SPICE"".
Wish for your reply~"
"The ./download.sh doesn't download the required data from the given aws link.
Is there any way to get those relevant data files."
"Hello,
Thank you for sharing your code. It is very helpful. However, in preparation part, I encounter a problem during downloading cvpr18-caption-eval.zip. It returns ""HTTP request sent, awaiting response 403 forbidden"" error. It is possible that you could give a hint to solve this problem or is there anywhere else to download this zip file ? "
"Since it is a learnable model, so how to train it or to run the code where should I download the trained model?"
Did someone run the code? Where is the relationship module code？
"
compilation terminated.
Makefile:393: recipe for target 'build/src/operator/contrib/multibox_detection.o' failed
make: *** [build/src/operator/contrib/multibox_detection.o] Error 1
In file included from /home/fsr/Relation-Networks-for-Object-Detection-master/incubator-mxnet/mshadow/mshadow/tensor.h:16:0,
                 from include/mxnet/./base.h:32,
                 from include/mxnet/operator.h:38,
                 from src/operator/contrib/./deformable_psroi_pooling-inl.h:32,
                 from src/operator/contrib/deformable_psroi_pooling.cc:27:
/home/fsr/Relation-Networks-for-Object-Detection-master/incubator-mxnet/mshadow/mshadow/./base.h:147:23: fatal error: cblas.h: 没有那个文件或目录
compilation terminated.
Makefile:393: recipe for target 'build/src/operator/contrib/deformable_psroi_pooling.o' failed
make: *** [build/src/operator/contrib/deformable_psroi_pooling.o] Error 1
In file included from /home/fsr/Relation-Networks-for-Object-Detection-master/incubator-mxnet/mshadow/mshadow/tensor.h:16:0,
                 from include/mxnet/./base.h:32,
                 from include/mxnet/operator.h:38,
                 from src/operator/contrib/./psroi_pooling-inl.h:14,
                 from src/operator/contrib/psroi_pooling.cc:28:
/home/fsr/Relation-Networks-for-Object-Detection-master/incubator-mxnet/mshadow/mshadow/./base.h:147:23: fatal error: cblas.h: 没有那个文件或目录
compilation terminated.
Makefile:393: recipe for target 'build/src/operator/contrib/psroi_pooling.o' failed
make: *** [build/src/operator/contrib/psroi_pooling.o] Error 1
"
"Hello , 
Does anyone know how to add a validation at the end of each epoch?
Thanks!!"
Can this project run on Windows？
I recurrent the code based on vgg16 and I found that the map is the same as the faster-rcnn . Does the relation module works based on vgg16? thank you!
"hi! do you know how 'nongt_dim' works?
https://github.com/msracver/Relation-Networks-for-Object-Detection/blob/e83e911d828e3c86624ce0aeb8d742d5ee67d5ba/relation_rcnn/symbols/resnet_v1_101_rcnn_attention_1024_pairwise_position_multi_head_16_learn_nms.py#L85
for example, nongt_dim = 2000(rois per image), when train, there is 2 images per batch, so 2000*2 rois. it will get the relation between every rois and first 2000 rois. but the rois of 2th image also get the relation about rois of 1th image. it is unreasonable.
thanks!"
"Because there are overlap bboxes in the same object, when compute the mAP, whether the overlap bboxes influence the mAP or not?"
"/home/dlc/anaconda3/envs/torch3_py27/bin/python2.7 /home/dlc/sll/Relation-Networks-for-Object-Detection-master/experiments/relation_rcnn/rcnn_end2end_train_test.py --cfg experiments/relation_rcnn/cfgs/resnet_v1_101_coco_trainvalminus_rcnn_fpn_relation_learn_nms_8epoch.yaml
('Called with argument:', Namespace(cfg='experiments/relation_rcnn/cfgs/resnet_v1_101_coco_trainvalminus_rcnn_fpn_relation_learn_nms_8epoch.yaml', frequent=100))
Traceback (most recent call last):
  File ""/home/dlc/sll/Relation-Networks-for-Object-Detection-master/experiments/relation_rcnn/rcnn_end2end_train_test.py"", line 21, in <module>
    train_end2end.main()
  File ""/home/dlc/sll/Relation-Networks-for-Object-Detection-master/experiments/relation_rcnn/../../relation_rcnn/train_end2end.py"", line 184, in main
    config.TRAIN.model_prefix,config.TRAIN.begin_epoch, config.TRAIN.end_epoch, config.TRAIN.lr, config.TRAIN.lr_step)
  File ""/home/dlc/sll/Relation-Networks-for-Object-Detection-master/experiments/relation_rcnn/../../relation_rcnn/train_end2end.py"", line 66, in train_net
    sym = sym_instance.get_symbol(config, is_train=True)
  File ""/home/dlc/sll/Relation-Networks-for-Object-Detection-master/experiments/relation_rcnn/../../relation_rcnn/../lib/utils/symbol.py"", line 25, in get_symbol
    raise NotImplementedError()
NotImplementedError
跪求解决方法呜呜~~"
"Hi, I want to visualize the bounding box of a new image instead of datasets with annotations.
How can I modify test.py file?"
""
"I don't understand codes below, in 
`resnet_v1_101_rcnn_attention_1024_pairwise_position_multi_head_16_learn_nms.py`

You added one of `Relation Module` inputs and `Relation Module` output together as the input of later layer. But you did not explain why you do this in your paper. 

```
nms_attention_1, nms_softmax_1 = self.attention_module_nms_multi_head(
                nms_embedding_feat, nms_position_matrix,
                num_rois=first_n, index=1, group=16,
                dim=(1024, 1024, 128), fc_dim=(64, 16), feat_dim=128
)
nms_all_feat_1 = nms_embedding_feat + nms_attention_1
```

I think that the network figure should add the green path 
![image](https://user-images.githubusercontent.com/12170795/45477555-57af5d80-b774-11e8-8aa6-0362bce9318f.png)



btw, `nms_softmax_1` is redundant, I personally recommend deleting it in your release code."
""
"I set the YAML as NUM_CLASSES: 189.

And I use tool `https://raw.githubusercontent.com/withyou1771/Detectron_FocalLoss/master/tools/xml_to_json.py` to get COCO-like data from VOC-like data.

then run 
```
python experiments/relation_rcnn/rcnn_end2end_train_test.py --cfg experiments/relation_rcnn/cfgs/epoch8.yaml
```
HERE the error info.
```
('lr', 0.0005, 'lr_epoch_diff', [5.33], 'lr_iters', [79950])
Epoch[0] Batch [100]    Speed: 3.82 samples/sec Train-RPNAcc=0.964979,  RPNLogLoss=0.105434,    RPNL1Loss=0.111539,     RCNNAcc=0.837523,       RCNNLogLoss=1.683923,   RCNNL1Loss=0.280678,    NMSLoss_pos=0.052915,      NMSLoss_neg=0.014347,   NMSAcc_pos=0.000000,    NMSAcc_neg=1.000000,    
Epoch[0] Batch [200]    Speed: 3.83 samples/sec Train-RPNAcc=0.971723,  RPNLogLoss=0.088618,    RPNL1Loss=0.084358,     RCNNAcc=0.865341,       RCNNLogLoss=1.241336,   RCNNL1Loss=0.223327,    NMSLoss_pos=0.056996,      NMSLoss_neg=0.010787,   NMSAcc_pos=0.000000,    NMSAcc_neg=1.000000,    
Error in CustomOp.forward: Traceback (most recent call last):
  File ""/home/hri/anaconda3/envs/relations/lib/python2.7/site-packages/mxnet/operator.py"", line 987, in forward_entry
    aux=tensors[4])
  File ""experiments/relation_rcnn/../../relation_rcnn/operator_py/box_annotator_ohem.py"", line 36, in forward
    per_roi_loss_cls = per_roi_loss_cls[np.arange(per_roi_loss_cls.shape[0], dtype='int'), labels.astype('int')]
IndexError: index 189 is out of bounds for axis 1 with size 189

terminate called after throwing an instance of 'dmlc::Error'
  what():  [21:01:01] src/operator/custom/custom.cc:347: Check failed: reinterpret_cast<CustomOpFBFunc>( params.info->callbacks[kCustomOpForward])( ptrs.size(), const_cast<void**>(ptrs.data()), const_cast<int*>(tags.data()), reinterpret_cast<const int*>(req.data()), static_cast<int>(ctx.is_train), params.info->contexts[kCustomOpForward]) 

Stack trace returned 7 entries:
[bt] (0) /home/hri/anaconda3/envs/relations/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x30756a) [0x7f50471e156a]
[bt] (1) /home/hri/anaconda3/envs/relations/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x307b91) [0x7f50471e1b91]
[bt] (2) /home/hri/anaconda3/envs/relations/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x4853f7) [0x7f504735f3f7]
[bt] (3) /home/hri/anaconda3/envs/relations/lib/python2.7/site-packages/mxnet/libmxnet.so(+0x46b128) [0x7f5047345128]
[bt] (4) /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb8c80) [0x7f50b2816c80]
[bt] (5) /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba) [0x7f50b903e6ba]
[bt] (6) /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d) [0x7f50b866441d]

```
Any help? Thanks"
How do you draw the picture of [ Object Pairs with High Relation Weights] and [ Class Co-Occurrence Information is Learnt ] in your slides of CVPR2018
"When I'm training on the coco as the README declared,I meet this problem just like the blod log,and then the NMSLoss_pos and the NMSLoss_neg become nan,does anyone meet the same problem and give me some help?


![_20180702195240](https://user-images.githubusercontent.com/22448530/42162559-ad9a8534-7e31-11e8-8fe3-822155307341.png)
 
('lr', 0.0005, 'lr_epoch_diff', [5.33], 'lr_iters', [625027])
Epoch[0] Batch [100]    Speed: 5.08 samples/sec Train-RPNAcc=0.847250,  RPNLogLoss=0.376764,    RPNL1Loss=0.187504,     RCNNAcc=0.801361,       RCNNLogLoss=1.674762,   RCNNL1Loss=0.311297,    NMSLoss_pos=0.035744,   NMSLoss_neg=0.016391,   NMSAcc_pos=0.000000,    NMSAcc_neg=1.000000,
Epoch[0] Batch [200]    Speed: 5.10 samples/sec Train-RPNAcc=0.865089,  RPNLogLoss=0.328289,    RPNL1Loss=0.176516,     RCNNAcc=0.811237,       RCNNLogLoss=1.380794,   RCNNL1Loss=0.316205,    NMSLoss_pos=0.048681,   NMSLoss_neg=0.013534,   NMSAcc_pos=0.000000,    NMSAcc_neg=1.000000,
Epoch[0] Batch [300]    Speed: 5.11 samples/sec Train-RPNAcc=0.874916,  RPNLogLoss=0.302038,    RPNL1Loss=0.159570,     RCNNAcc=0.802546,       RCNNLogLoss=1.319950,   RCNNL1Loss=0.352934,    NMSLoss_pos=0.057433,   NMSLoss_neg=0.013499,   NMSAcc_pos=0.000000,    NMSAcc_neg=1.000000,
**experiments/relation_rcnn/../../relation_rcnn/../lib/bbox/bbox_transform.py:128: RuntimeWarning: overflow encountered in exp
  pred_w = np.exp(dw) * widths[:, np.newaxis]
experiments/relation_rcnn/../../relation_rcnn/../lib/bbox/bbox_transform.py:129: RuntimeWarning: overflow encountered in exp
  pred_h = np.exp(dh) * heights[:, np.newaxis]
experiments/relation_rcnn/../../relation_rcnn/../lib/bbox/bbox_transform.py:133: RuntimeWarning: invalid value encountered in subtract
  pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * (pred_w - 1.0)
experiments/relation_rcnn/../../relation_rcnn/../lib/bbox/bbox_transform.py:135: RuntimeWarning: invalid value encountered in subtract
  pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * (pred_h - 1.0)
experiments/relation_rcnn/../../relation_rcnn/../lib/bbox/bbox_transform.py:137: RuntimeWarning: invalid value encountered in add
  pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * (pred_w - 1.0)
experiments/relation_rcnn/../../relation_rcnn/../lib/bbox/bbox_transform.py:139: RuntimeWarning: invalid value encountered in add
  pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * (pred_h - 1.0)
experiments/relation_rcnn/../../relation_rcnn/operator_py/proposal.py:180: RuntimeWarning: invalid value encountered in greater_equal
  keep = np.where((ws >= min_size) & (hs >= min_size))[0]**
Epoch[0] Batch [400]    Speed: 5.02 samples/sec Train-RPNAcc=0.871289,  RPNLogLoss=nan, RPNL1Loss=nan,  RCNNAcc=0.810123,       RCNNLogLoss=1.576645,   RCNNL1Loss=0.334166,    NMSLoss_pos=0.054120,   NMSLoss_neg=nan,        NMSAcc_pos=0.000000,    NMSAcc_neg=0.999650,
Epoch[0] Batch [500]    Speed: 4.91 samples/sec Train-RPNAcc=0.859804,  RPNLogLoss=nan, RPNL1Loss=nan,  RCNNAcc=0.836702,       RCNNLogLoss=1.888214,   RCNNL1Loss=0.267614,    NMSLoss_pos=nan,        NMSLoss_neg=nan,        NMSAcc_pos=0.000000,    NMSAcc_neg=0.999720,
Epoch[0] Batch [600]    Speed: 4.99 samples/sec Train-RPNAcc=0.850682,  RPNLogLoss=nan, RPNL1Loss=nan,  RCNNAcc=0.853031,       RCNNLogLoss=1.725999,   RCNNL1Loss=0.223882,    NMSLoss_pos=nan,        NMSLoss_neg=nan,        NMSAcc_pos=0.000000,    NMSAcc_neg=0.999767,
Epoch[0] Batch [700]    Speed: 4.98 samples/sec Train-RPNAcc=0.844466,  RPNLogLoss=nan, RPNL1Loss=nan,  RCNNAcc=0.865544,       RCNNLogLoss=1.547918,   RCNNL1Loss=0.192278,    NMSLoss_pos=nan,        NMSLoss_neg=nan,        NMSAcc_pos=0.000000,    NMSAcc_neg=0.999800,


"
""
"Hello, I converted the scripts to python3. My mxnet is mxnet-cu101 version 1.6.0 for CUDA10.1. But when I ran the train_endtoend.py in ./relation_rcnn I am running into the following error after the config file is read: 

'''
 'symbol': 'resnet_v1_101_rcnn_dcn_attention_1024_pairwise_position_multi_head_16_learn_nms'}
loading annotations into memory...
Done (t=0.81s)
creating index...
index created!
num_images 7017
wrote gt roidb to ./cache/COCO_train_800x800_gt_roidb.pkl
filtered 0 roidb entries: 7017 -> 7017
[('data', (1, 3, 800, 800))]
[('data', (1, 3, 800, 800))]
[('label', (1, 30000)), ('bbox_target', (1, 48, 50, 50)), ('bbox_weight', (1, 48, 50, 50))]
providing maximum shape [('data', (1, 3, 800, 800)), ('gt_boxes', (1, 100, 5))] [('label', (1, 30000)), ('bbox_target', (1, 48, 50, 50)), ('bbox_weight', (1, 48, 50, 50))]
*********************Input Dictionary *********************
{'data': (1, 3, 800, 800), 'im_info': (1, 3), 'gt_boxes': (1, 13, 5), 'label': (1, 30000), 'bbox_target': (1, 48, 50, 50), 'bbox_weight': (1, 48, 50, 50)}
infer_shape error. Arguments:
  data: (1, 3, 800, 800)
  im_info: (1, 3)
  gt_boxes: (1, 13, 5)
  label: (1, 30000)
  bbox_target: (1, 48, 50, 50)
  bbox_weight: (1, 48, 50, 50)
Traceback (most recent call last):
  File ""rcnn_end2end_train_test.py"", line 23, in <module>
    train_end2end.main()
  File ""../../relation_rcnn/train_end2end.py"", line 188, in main
    config.TRAIN.begin_epoch, config.TRAIN.end_epoch, config.TRAIN.lr, config.TRAIN.lr_step)
  File ""../../relation_rcnn/train_end2end.py"", line 101, in train_net
    sym_instance.infer_shape(data_shape_dict)
  File ""../../relation_rcnn/../lib/utils/symbol.py"", line 39, in infer_shape
    arg_shape, out_shape, aux_shape = self.sym.infer_shape(**data_shape_dict)
  File ""/mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/symbol/symbol.py"", line 1103, in infer_shape
    res = self._infer_shape_impl(False, *args, **kwargs)
  File ""/mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/symbol/symbol.py"", line 1267, in _infer_shape_impl
    ctypes.byref(complete)))
  File ""/mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/base.py"", line 255, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: Error in operator _plus2: [06:13:23] src/operator/contrib/./../elemwise_op_common.h:135: Check failed: assign(&dattr, vec.at(i)): Incompatible attr in node _plus2 at 1-th input: expected [313,16,300], got [19,16,18]
Stack trace:
  [bt] (0) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x6b8b5b) [0x7f1934352b5b]
  [bt] (1) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x878f39) [0x7f1934512f39]
  [bt] (2) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x8797db) [0x7f19345137db]
  [bt] (3) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0xb48036) [0x7f19347e2036]
  [bt] (4) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x382fe3c) [0x7f19374c9e3c]
  [bt] (5) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x38336a8) [0x7f19374cd6a8]
  [bt] (6) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(+0x377bc31) [0x7f1937415c31]
  [bt] (7) /mnt/keshav/relnet_python3/lib/python3.6/site-packages/mxnet/libmxnet.so(MXSymbolInferShapeEx+0xc1) [0x7f19374162c1]
  [bt] (8) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f1973c75dae]
'''

Can someone please explain what might the issue exactly be?
"
"hi!  rois -> sliced_rois
`sliced_rois = mx.sym.slice_axis(rois, axis=1, begin=1, end=None)`
why ""sliced_rois"" start from 1?
https://github.com/msracver/Relation-Networks-for-Object-Detection/blob/e83e911d828e3c86624ce0aeb8d742d5ee67d5ba/relation_rcnn/symbols/resnet_v1_101_rcnn_attention_1024_pairwise_position_multi_head_16_learn_nms.py#L337"
"Hello,  I am chaojie from Renmin University of China, Beijing.
Thanks for you excellent work in object detection. 
But i have a problem when i try it with my own data,
It is all good for rcnn_attention and rcnn_dcn training and test, but when it comes rcnn_fpn symbol,
**I have to provide the previously generated proposals in the directory like this ""./proposal/resnet_v1_101_fpn/rpn_data/{}_{}_rpn.pkl""**,  so could you please give tips to me about how to generate this file for another data with annotations like COCO. I am dying to know it, thank you very much!
"
"Thank you for wonderful code! `std` and `mean` of boxes are stored in params of last regression layer. When `cfg.TRAIN.RESUME ` is `True`, they should be restored?"
"Title. Many thanks for the help!

Here is the location of this function call: 
line 33 of 

https://github.com/msracver/Relation-Networks-for-Object-Detection/blob/master/relation_rcnn/symbols/resnet_v1_101_rcnn_attention_1024_pairwise_position_multi_head_16.py#L33

looks like: mx.sym.full((1,), wave_length)

Best,
Chu."
"I don't know what the meaning of ""nongt_dim"" is in _extract_position_embedding_ and _attention_module_embedding_. If it means ""the number of rois that are not ground truth"", how to determine which roi is not ground truth during training. Maybe I misunderstand it. Could anyone help me?"
"Hi,

there is an error when calling output_shapes (https://github.com/msracver/Relation-Networks-for-Object-Detection/blob/master/relation_rcnn/core/module.py#L216)


```
  File ""experiments/relation_rcnn/rcnn_end2end_train_test.py"", line 21, in <module>
    train_end2end.main()
  File ""experiments/relation_rcnn/../../relation_rcnn/train_end2end.py"", line 184, in main
    config.TRAIN.begin_epoch, config.TRAIN.end_epoch, config.TRAIN.lr, config.TRAIN.lr_step)
  File ""experiments/relation_rcnn/../../relation_rcnn/train_end2end.py"", line 177, in train_net
    arg_params=arg_params, aux_params=aux_params, begin_epoch=begin_epoch, num_epoch=end_epoch)
  File ""experiments/relation_rcnn/../../relation_rcnn/core/module.py"", line 1001, in fit
    print ('output shape {}'.format(self.output_shapes))
  File ""experiments/relation_rcnn/../../relation_rcnn/core/module.py"", line 801, in output_shapes
    return self._curr_module.output_shapes
  File ""experiments/relation_rcnn/../../relation_rcnn/core/module.py"", line 223, in output_shapes
    return self._exec_group.get_output_shapes()
AttributeError: 'DataParallelExecutorGroup' object has no attribute 'get_output_shapes'

```

Any advise?"
"Hi! This is a great work!
But I wonder why the FPN baseline (2FC + softnms(0.6) ResNet-101) 36.8 mAP is lower than the [Detectron FPN baseline (R-101-FPN) 38.5 mAP](https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md) ? You both use ResNet101 and pre-computed proposals. Is there anything different in implementation?"
"<img width=""716"" alt=""ddd"" src=""https://user-images.githubusercontent.com/17775886/43998962-5b236be0-9e34-11e8-8c4b-584a5c3d3a37.png"">

"
"My **batchsize**(first_n) is 64. This brings a severe data imbalance problem.
Is it normal? Or I made some mistake?
"
Will you provide support for voc format dataset?
"Thanks for sharing the code. Nice work.
I can not open any link of onedirve. They all show:
![image](https://user-images.githubusercontent.com/17979833/42615689-ea62133e-85dd-11e8-8428-765cafb349ef.png)
"
I'd like to train a Faster R-CNN + Relation module (as in the readme with coco) on my own dataset. How do I go about it?
"Hi, thanks for your work! I have a question about greedy-NMS usage: do you completely replace NMS with your duplicate removal network or add it complementary to greedy-NMS? From Table 1 it seems that greedy-NMS is still a part of pipeline..."
"When try to run this network on the VOC datasets, I use the VOC datasets IO function from the Deformable ConvNets (https://github.com/msracver/Deformable-ConvNets) 
but always find the problem with the  function sys_instance.infer_shape() which in the ./relation_rcnn/train_end2end.py line 99 ,I track this problem to the file ""my_mxnet_root/symbol/symbol.py"", line 1119, in _infer_shape_impl ctypes.byref(complete), where check_call find error in operator slice_axis1: check failed: (*end <= axis_size) && (*end >=0) invalid begin, end, get begin=0, end =300

If someone knows how to solve this problem, or has done the implementation of this algorithm on the VOC dataset, please help me. 
 "
"Hi,
I apply the resnet_v1_101_coco_trainvalminus_rcnn_dcn_end2end_relation_learn_nms_8epoch.yaml to train a model. Then using the trained model to inference the test image, I find there are multiple boxes appear on the same object. Do I not train the nms layer thoroughly? So I need to add nms to remove the duplicate boxes."
"Does it mean to propose first N proposals or first N detections for one image?
Configs in `resnet_v1_101_coco_trainvalminus_rcnn_end2end_relation_learn_nms_8epoch.yaml`."
"2018-06-26 10:15:50,538 {'bbox_pred_bias': (8L,),
 'bbox_pred_weight': (8L, 1024L),
 'bbox_target': (1L, 48L, 38L, 38L),
 'bbox_weight': (1L, 48L, 38L, 38L),
 'bn2a_branch1_beta': (256L,),
 'bn2a_branch1_gamma': (256L,),
 'bn2a_branch2a_beta': (64L,),
 'bn2a_branch2a_gamma': (64L,),
 'bn2a_branch2b_beta': (64L,),
 'bn2a_branch2b_gamma': (64L,),
 'bn2a_branch2c_beta': (256L,),
 'bn2a_branch2c_gamma': (256L,),
 'bn2b_branch2a_beta': (64L,),
 'bn2b_branch2a_gamma': (64L,),
 'bn2b_branch2b_beta': (64L,),
 'bn2b_branch2b_gamma': (64L,),
 'bn2b_branch2c_beta': (256L,),
 'bn2b_branch2c_gamma': (256L,),
 'bn2c_branch2a_beta': (64L,),
 'bn2c_branch2a_gamma': (64L,),
 'bn2c_branch2b_beta': (64L,),
 'bn2c_branch2b_gamma': (64L,),
 'bn2c_branch2c_beta': (256L,),
 'bn2c_branch2c_gamma': (256L,),
 'bn3a_branch1_beta': (512L,),
 'bn3a_branch1_gamma': (512L,),
 'bn3a_branch2a_beta': (128L,),
 'bn3a_branch2a_gamma': (128L,),
 'bn3a_branch2b_beta': (128L,),
 'bn3a_branch2b_gamma': (128L,),
 'bn3a_branch2c_beta': (512L,),
 'bn3a_branch2c_gamma': (512L,),
 'bn3b1_branch2a_beta': (128L,),
 'bn3b1_branch2a_gamma': (128L,),
 'bn3b1_branch2b_beta': (128L,),
 'bn3b1_branch2b_gamma': (128L,),
 'bn3b1_branch2c_beta': (512L,),
 'bn3b1_branch2c_gamma': (512L,),
 'bn3b2_branch2a_beta': (128L,),
 'bn3b2_branch2a_gamma': (128L,),
 'bn3b2_branch2b_beta': (128L,),
 'bn3b2_branch2b_gamma': (128L,),
 'bn3b2_branch2c_beta': (512L,),
 'bn3b2_branch2c_gamma': (512L,),
 'bn3b3_branch2a_beta': (128L,),
 'bn3b3_branch2a_gamma': (128L,),
 'bn3b3_branch2b_beta': (128L,),
 'bn3b3_branch2b_gamma': (128L,),
 'bn3b3_branch2c_beta': (512L,),
 'bn3b3_branch2c_gamma': (512L,),
 'bn4a_branch1_beta': (1024L,),
 'bn4a_branch1_gamma': (1024L,),
 'bn4a_branch2a_beta': (256L,),
 'bn4a_branch2a_gamma': (256L,),
 'bn4a_branch2b_beta': (256L,),
 'bn4a_branch2b_gamma': (256L,),
 'bn4a_branch2c_beta': (1024L,),
 'bn4a_branch2c_gamma': (1024L,),
 'bn4b10_branch2a_beta': (256L,),
 'bn4b10_branch2a_gamma': (256L,),
 'bn4b10_branch2b_beta': (256L,),
 'bn4b10_branch2b_gamma': (256L,),
 'bn4b10_branch2c_beta': (1024L,),
 'bn4b10_branch2c_gamma': (1024L,),
 'bn4b11_branch2a_beta': (256L,),
 'bn4b11_branch2a_gamma': (256L,),
 'bn4b11_branch2b_beta': (256L,),
 'bn4b11_branch2b_gamma': (256L,),
 'bn4b11_branch2c_beta': (1024L,),
 'bn4b11_branch2c_gamma': (1024L,),
 'bn4b12_branch2a_beta': (256L,),
 'bn4b12_branch2a_gamma': (256L,),
 'bn4b12_branch2b_beta': (256L,),
 'bn4b12_branch2b_gamma': (256L,),
 'bn4b12_branch2c_beta': (1024L,),
 'bn4b12_branch2c_gamma': (1024L,),
 'bn4b13_branch2a_beta': (256L,),
 'bn4b13_branch2a_gamma': (256L,),
 'bn4b13_branch2b_beta': (256L,),
 'bn4b13_branch2b_gamma': (256L,),
 'bn4b13_branch2c_beta': (1024L,),
 'bn4b13_branch2c_gamma': (1024L,),
 'bn4b14_branch2a_beta': (256L,),
 'bn4b14_branch2a_gamma': (256L,),
 'bn4b14_branch2b_beta': (256L,),
 'bn4b14_branch2b_gamma': (256L,),
 'bn4b14_branch2c_beta': (1024L,),
 'bn4b14_branch2c_gamma': (1024L,),
 'bn4b15_branch2a_beta': (256L,),
 'bn4b15_branch2a_gamma': (256L,),
 'bn4b15_branch2b_beta': (256L,),
 'bn4b15_branch2b_gamma': (256L,),
 'bn4b15_branch2c_beta': (1024L,),
 'bn4b15_branch2c_gamma': (1024L,),
 'bn4b16_branch2a_beta': (256L,),
 'bn4b16_branch2a_gamma': (256L,),
 'bn4b16_branch2b_beta': (256L,),
 'bn4b16_branch2b_gamma': (256L,),
 'bn4b16_branch2c_beta': (1024L,),
 'bn4b16_branch2c_gamma': (1024L,),
 'bn4b17_branch2a_beta': (256L,),
 'bn4b17_branch2a_gamma': (256L,),
 'bn4b17_branch2b_beta': (256L,),
 'bn4b17_branch2b_gamma': (256L,),
 'bn4b17_branch2c_beta': (1024L,),
 'bn4b17_branch2c_gamma': (1024L,),
 'bn4b18_branch2a_beta': (256L,),
 'bn4b18_branch2a_gamma': (256L,),
 'bn4b18_branch2b_beta': (256L,),
 'bn4b18_branch2b_gamma': (256L,),
 'bn4b18_branch2c_beta': (1024L,),
 'bn4b18_branch2c_gamma': (1024L,),
 'bn4b19_branch2a_beta': (256L,),
 'bn4b19_branch2a_gamma': (256L,),
 'bn4b19_branch2b_beta': (256L,),
 'bn4b19_branch2b_gamma': (256L,),
 'bn4b19_branch2c_beta': (1024L,),
 'bn4b19_branch2c_gamma': (1024L,),
 'bn4b1_branch2a_beta': (256L,),
 'bn4b1_branch2a_gamma': (256L,),
 'bn4b1_branch2b_beta': (256L,),
 'bn4b1_branch2b_gamma': (256L,),
 'bn4b1_branch2c_beta': (1024L,),
 'bn4b1_branch2c_gamma': (1024L,),
 'bn4b20_branch2a_beta': (256L,),
 'bn4b20_branch2a_gamma': (256L,),
 'bn4b20_branch2b_beta': (256L,),
 'bn4b20_branch2b_gamma': (256L,),
 'bn4b20_branch2c_beta': (1024L,),
 'bn4b20_branch2c_gamma': (1024L,),
 'bn4b21_branch2a_beta': (256L,),
 'bn4b21_branch2a_gamma': (256L,),
 'bn4b21_branch2b_beta': (256L,),
 'bn4b21_branch2b_gamma': (256L,),
 'bn4b21_branch2c_beta': (1024L,),
 'bn4b21_branch2c_gamma': (1024L,),
 'bn4b22_branch2a_beta': (256L,),
 'bn4b22_branch2a_gamma': (256L,),
 'bn4b22_branch2b_beta': (256L,),
 'bn4b22_branch2b_gamma': (256L,),
 'bn4b22_branch2c_beta': (1024L,),
 'bn4b22_branch2c_gamma': (1024L,),
 'bn4b2_branch2a_beta': (256L,),
 'bn4b2_branch2a_gamma': (256L,),
 'bn4b2_branch2b_beta': (256L,),
 'bn4b2_branch2b_gamma': (256L,),
 'bn4b2_branch2c_beta': (1024L,),
 'bn4b2_branch2c_gamma': (1024L,),
 'bn4b3_branch2a_beta': (256L,),
 'bn4b3_branch2a_gamma': (256L,),
 'bn4b3_branch2b_beta': (256L,),
 'bn4b3_branch2b_gamma': (256L,),
 'bn4b3_branch2c_beta': (1024L,),
 'bn4b3_branch2c_gamma': (1024L,),
 'bn4b4_branch2a_beta': (256L,),
 'bn4b4_branch2a_gamma': (256L,),
 'bn4b4_branch2b_beta': (256L,),
 'bn4b4_branch2b_gamma': (256L,),
 'bn4b4_branch2c_beta': (1024L,),
 'bn4b4_branch2c_gamma': (1024L,),
 'bn4b5_branch2a_beta': (256L,),
 'bn4b5_branch2a_gamma': (256L,),
 'bn4b5_branch2b_beta': (256L,),
 'bn4b5_branch2b_gamma': (256L,),
 'bn4b5_branch2c_beta': (1024L,),
 'bn4b5_branch2c_gamma': (1024L,),
 'bn4b6_branch2a_beta': (256L,),
 'bn4b6_branch2a_gamma': (256L,),
 'bn4b6_branch2b_beta': (256L,),
 'bn4b6_branch2b_gamma': (256L,),
 'bn4b6_branch2c_beta': (1024L,),
 'bn4b6_branch2c_gamma': (1024L,),
 'bn4b7_branch2a_beta': (256L,),
 'bn4b7_branch2a_gamma': (256L,),
 'bn4b7_branch2b_beta': (256L,),
 'bn4b7_branch2b_gamma': (256L,),
 'bn4b7_branch2c_beta': (1024L,),
 'bn4b7_branch2c_gamma': (1024L,),
 'bn4b8_branch2a_beta': (256L,),
 'bn4b8_branch2a_gamma': (256L,),
 'bn4b8_branch2b_beta': (256L,),
 'bn4b8_branch2b_gamma': (256L,),
 'bn4b8_branch2c_beta': (1024L,),
 'bn4b8_branch2c_gamma': (1024L,),
 'bn4b9_branch2a_beta': (256L,),
 'bn4b9_branch2a_gamma': (256L,),
 'bn4b9_branch2b_beta': (256L,),
 'bn4b9_branch2b_gamma': (256L,),
 'bn4b9_branch2c_beta': (1024L,),
 'bn4b9_branch2c_gamma': (1024L,),
 'bn5a_branch1_beta': (2048L,),
 'bn5a_branch1_gamma': (2048L,),
 'bn5a_branch2a_beta': (512L,),
 'bn5a_branch2a_gamma': (512L,),
 'bn5a_branch2b_beta': (512L,),
 'bn5a_branch2b_gamma': (512L,),
 'bn5a_branch2c_beta': (2048L,),
 'bn5a_branch2c_gamma': (2048L,),
 'bn5b_branch2a_beta': (512L,),
 'bn5b_branch2a_gamma': (512L,),
 'bn5b_branch2b_beta': (512L,),
 'bn5b_branch2b_gamma': (512L,),
 'bn5b_branch2c_beta': (2048L,),
 'bn5b_branch2c_gamma': (2048L,),
 'bn5c_branch2a_beta': (512L,),
 'bn5c_branch2a_gamma': (512L,),
 'bn5c_branch2b_beta': (512L,),
 'bn5c_branch2b_gamma': (512L,),
 'bn5c_branch2c_beta': (2048L,),
 'bn5c_branch2c_gamma': (2048L,),
 'bn_conv1_beta': (64L,),
 'bn_conv1_gamma': (64L,),
 'cls_score_bias': (11L,),
 'cls_score_weight': (11L, 1024L),
 'conv1_weight': (64L, 3L, 7L, 7L),
 'conv_new_1_bias': (256L,),
 'conv_new_1_weight': (256L, 2048L, 1L, 1L),
 'data': (1L, 3L, 600L, 600L),
 'fc_new_1_bias': (1024L,),
 'fc_new_1_weight': (1024L, 12544L),
 'fc_new_2_bias': (1024L,),
 'fc_new_2_weight': (1024L, 1024L),
 'gt_boxes': (1L, 19L, 5L),
 'im_info': (1L, 3L),
 'key_1_bias': (1024L,),
 'key_1_weight': (1024L, 1024L),
 'key_2_bias': (1024L,),
 'key_2_weight': (1024L, 1024L),
 'label': (1L, 17328L),
 'linear_out_1_bias': (1024L,),
 'linear_out_1_weight': (1024L, 1024L, 1L, 1L),
 'linear_out_2_bias': (1024L,),
 'linear_out_2_weight': (1024L, 1024L, 1L, 1L),
 'nms_key_1_bias': (1024L,),
 'nms_key_1_weight': (1024L, 128L),
 'nms_linear_out_1_bias': (128L,),
 'nms_linear_out_1_weight': (128L, 128L, 1L, 1L),
 'nms_logit_bias': (5L,),
 'nms_logit_weight': (5L, 128L),
 'nms_pair_pos_fc1_1_bias': (16L,),
 'nms_pair_pos_fc1_1_weight': (16L, 64L),
 'nms_query_1_bias': (1024L,),
 'nms_query_1_weight': (1024L, 128L),
 'nms_rank_bias': (128L,),
 'nms_rank_weight': (128L, 1024L),
 'pair_pos_fc1_1_bias': (16L,),
 'pair_pos_fc1_1_weight': (16L, 64L),
 'pair_pos_fc1_2_bias': (16L,),
 'pair_pos_fc1_2_weight': (16L, 64L),
 'query_1_bias': (1024L,),
 'query_1_weight': (1024L, 1024L),
 'query_2_bias': (1024L,),
 'query_2_weight': (1024L, 1024L),
 'res2a_branch1_weight': (256L, 64L, 1L, 1L),
 'res2a_branch2a_weight': (64L, 64L, 1L, 1L),
 'res2a_branch2b_weight': (64L, 64L, 3L, 3L),
 'res2a_branch2c_weight': (256L, 64L, 1L, 1L),
 'res2b_branch2a_weight': (64L, 256L, 1L, 1L),
 'res2b_branch2b_weight': (64L, 64L, 3L, 3L),
 'res2b_branch2c_weight': (256L, 64L, 1L, 1L),
 'res2c_branch2a_weight': (64L, 256L, 1L, 1L),
 'res2c_branch2b_weight': (64L, 64L, 3L, 3L),
 'res2c_branch2c_weight': (256L, 64L, 1L, 1L),
 'res3a_branch1_weight': (512L, 256L, 1L, 1L),
 'res3a_branch2a_weight': (128L, 256L, 1L, 1L),
 'res3a_branch2b_weight': (128L, 128L, 3L, 3L),
 'res3a_branch2c_weight': (512L, 128L, 1L, 1L),
 'res3b1_branch2a_weight': (128L, 512L, 1L, 1L),
 'res3b1_branch2b_weight': (128L, 128L, 3L, 3L),
 'res3b1_branch2c_weight': (512L, 128L, 1L, 1L),
 'res3b2_branch2a_weight': (128L, 512L, 1L, 1L),
 'res3b2_branch2b_weight': (128L, 128L, 3L, 3L),
 'res3b2_branch2c_weight': (512L, 128L, 1L, 1L),
 'res3b3_branch2a_weight': (128L, 512L, 1L, 1L),
 'res3b3_branch2b_weight': (128L, 128L, 3L, 3L),
 'res3b3_branch2c_weight': (512L, 128L, 1L, 1L),
 'res4a_branch1_weight': (1024L, 512L, 1L, 1L),
 'res4a_branch2a_weight': (256L, 512L, 1L, 1L),
 'res4a_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4a_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b10_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b10_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b10_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b11_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b11_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b11_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b12_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b12_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b12_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b13_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b13_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b13_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b14_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b14_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b14_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b15_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b15_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b15_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b16_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b16_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b16_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b17_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b17_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b17_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b18_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b18_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b18_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b19_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b19_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b19_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b1_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b1_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b1_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b20_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b20_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b20_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b21_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b21_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b21_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b22_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b22_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b22_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b2_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b2_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b2_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b3_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b3_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b3_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b4_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b4_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b4_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b5_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b5_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b5_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b6_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b6_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b6_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b7_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b7_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b7_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b8_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b8_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b8_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res4b9_branch2a_weight': (256L, 1024L, 1L, 1L),
 'res4b9_branch2b_weight': (256L, 256L, 3L, 3L),
 'res4b9_branch2c_weight': (1024L, 256L, 1L, 1L),
 'res5a_branch1_weight': (2048L, 1024L, 1L, 1L),
 'res5a_branch2a_weight': (512L, 1024L, 1L, 1L),
 'res5a_branch2b_weight': (512L, 512L, 3L, 3L),
 'res5a_branch2c_weight': (2048L, 512L, 1L, 1L),
 'res5b_branch2a_weight': (512L, 2048L, 1L, 1L),
 'res5b_branch2b_weight': (512L, 512L, 3L, 3L),
 'res5b_branch2c_weight': (2048L, 512L, 1L, 1L),
 'res5c_branch2a_weight': (512L, 2048L, 1L, 1L),
 'res5c_branch2b_weight': (512L, 512L, 3L, 3L),
 'res5c_branch2c_weight': (2048L, 512L, 1L, 1L),
 'roi_feat_embedding_bias': (128L,),
 'roi_feat_embedding_weight': (128L, 1024L),
 'rpn_bbox_pred_bias': (48L,),
 'rpn_bbox_pred_weight': (48L, 512L, 1L, 1L),
 'rpn_cls_score_bias': (24L,),
 'rpn_cls_score_weight': (24L, 512L, 1L, 1L),
 'rpn_conv_3x3_bias': (512L,),
 'rpn_conv_3x3_weight': (512L, 1024L, 3L, 3L)}
2018-06-26 10:15:51,964 conv1_weight is fixed.
2018-06-26 10:15:51,965 bn_conv1_gamma is fixed.
2018-06-26 10:15:51,966 bn_conv1_beta is fixed.
2018-06-26 10:15:51,966 res2a_branch1_weight is fixed.
2018-06-26 10:15:51,966 bn2a_branch1_gamma is fixed.
2018-06-26 10:15:51,966 bn2a_branch1_beta is fixed.
2018-06-26 10:15:51,967 res2a_branch2a_weight is fixed.
2018-06-26 10:15:51,967 bn2a_branch2a_gamma is fixed.
2018-06-26 10:15:51,967 bn2a_branch2a_beta is fixed.
2018-06-26 10:15:51,967 res2a_branch2b_weight is fixed.
2018-06-26 10:15:51,967 bn2a_branch2b_gamma is fixed.
2018-06-26 10:15:51,967 bn2a_branch2b_beta is fixed.
2018-06-26 10:15:51,967 res2a_branch2c_weight is fixed.
2018-06-26 10:15:51,967 bn2a_branch2c_gamma is fixed.
2018-06-26 10:15:51,967 bn2a_branch2c_beta is fixed.
2018-06-26 10:15:51,968 res2b_branch2a_weight is fixed.
2018-06-26 10:15:51,968 bn2b_branch2a_gamma is fixed.
2018-06-26 10:15:51,968 bn2b_branch2a_beta is fixed.
2018-06-26 10:15:51,968 res2b_branch2b_weight is fixed.
2018-06-26 10:15:51,968 bn2b_branch2b_gamma is fixed.
2018-06-26 10:15:51,968 bn2b_branch2b_beta is fixed.
2018-06-26 10:15:51,968 res2b_branch2c_weight is fixed.
2018-06-26 10:15:51,968 bn2b_branch2c_gamma is fixed.
2018-06-26 10:15:51,969 bn2b_branch2c_beta is fixed.
2018-06-26 10:15:51,969 res2c_branch2a_weight is fixed.
2018-06-26 10:15:51,969 bn2c_branch2a_gamma is fixed.
2018-06-26 10:15:51,969 bn2c_branch2a_beta is fixed.
2018-06-26 10:15:51,969 res2c_branch2b_weight is fixed.
2018-06-26 10:15:51,969 bn2c_branch2b_gamma is fixed.
2018-06-26 10:15:51,969 bn2c_branch2b_beta is fixed.
2018-06-26 10:15:51,969 res2c_branch2c_weight is fixed.
2018-06-26 10:15:51,969 bn2c_branch2c_gamma is fixed.
2018-06-26 10:15:51,970 bn2c_branch2c_beta is fixed.
2018-06-26 10:15:51,970 bn3a_branch1_gamma is fixed.
2018-06-26 10:15:51,970 bn3a_branch1_beta is fixed.
2018-06-26 10:15:51,970 bn3a_branch2a_gamma is fixed.
2018-06-26 10:15:51,970 bn3a_branch2a_beta is fixed.
2018-06-26 10:15:51,970 bn3a_branch2b_gamma is fixed.
2018-06-26 10:15:51,970 bn3a_branch2b_beta is fixed.
2018-06-26 10:15:51,970 bn3a_branch2c_gamma is fixed.
2018-06-26 10:15:51,970 bn3a_branch2c_beta is fixed.
2018-06-26 10:15:51,970 bn3b1_branch2a_gamma is fixed.
2018-06-26 10:15:51,970 bn3b1_branch2a_beta is fixed.
2018-06-26 10:15:51,970 bn3b1_branch2b_gamma is fixed.
2018-06-26 10:15:51,970 bn3b1_branch2b_beta is fixed.
2018-06-26 10:15:51,970 bn3b1_branch2c_gamma is fixed.
2018-06-26 10:15:51,970 bn3b1_branch2c_beta is fixed.
2018-06-26 10:15:51,970 bn3b2_branch2a_gamma is fixed.
2018-06-26 10:15:51,970 bn3b2_branch2a_beta is fixed.
2018-06-26 10:15:51,970 bn3b2_branch2b_gamma is fixed.
2018-06-26 10:15:51,971 bn3b2_branch2b_beta is fixed.
2018-06-26 10:15:51,971 bn3b2_branch2c_gamma is fixed.
2018-06-26 10:15:51,971 bn3b2_branch2c_beta is fixed.
2018-06-26 10:15:51,971 bn3b3_branch2a_gamma is fixed.
2018-06-26 10:15:51,971 bn3b3_branch2a_beta is fixed.
2018-06-26 10:15:51,971 bn3b3_branch2b_gamma is fixed.
2018-06-26 10:15:51,971 bn3b3_branch2b_beta is fixed.
2018-06-26 10:15:51,971 bn3b3_branch2c_gamma is fixed.
2018-06-26 10:15:51,971 bn3b3_branch2c_beta is fixed.
2018-06-26 10:15:51,971 bn4a_branch1_gamma is fixed.
2018-06-26 10:15:51,971 bn4a_branch1_beta is fixed.
2018-06-26 10:15:51,971 bn4a_branch2a_gamma is fixed.
2018-06-26 10:15:51,971 bn4a_branch2a_beta is fixed.
2018-06-26 10:15:51,971 bn4a_branch2b_gamma is fixed.
2018-06-26 10:15:51,972 bn4a_branch2b_beta is fixed.
2018-06-26 10:15:51,972 bn4a_branch2c_gamma is fixed.
2018-06-26 10:15:51,972 bn4a_branch2c_beta is fixed.
2018-06-26 10:15:51,972 bn4b1_branch2a_gamma is fixed.
2018-06-26 10:15:51,972 bn4b1_branch2a_beta is fixed.
2018-06-26 10:15:51,972 bn4b1_branch2b_gamma is fixed.
2018-06-26 10:15:51,972 bn4b1_branch2b_beta is fixed.
2018-06-26 10:15:51,972 bn4b1_branch2c_gamma is fixed.
2018-06-26 10:15:51,972 bn4b1_branch2c_beta is fixed.
2018-06-26 10:15:51,972 bn4b2_branch2a_gamma is fixed.
2018-06-26 10:15:51,972 bn4b2_branch2a_beta is fixed.
2018-06-26 10:15:51,972 bn4b2_branch2b_gamma is fixed.
2018-06-26 10:15:51,972 bn4b2_branch2b_beta is fixed.
2018-06-26 10:15:51,972 bn4b2_branch2c_gamma is fixed.
2018-06-26 10:15:51,973 bn4b2_branch2c_beta is fixed.
2018-06-26 10:15:51,973 bn4b3_branch2a_gamma is fixed.
2018-06-26 10:15:51,973 bn4b3_branch2a_beta is fixed.
2018-06-26 10:15:51,973 bn4b3_branch2b_gamma is fixed.
2018-06-26 10:15:51,973 bn4b3_branch2b_beta is fixed.
2018-06-26 10:15:51,973 bn4b3_branch2c_gamma is fixed.
2018-06-26 10:15:51,973 bn4b3_branch2c_beta is fixed.
2018-06-26 10:15:51,973 bn4b4_branch2a_gamma is fixed.
2018-06-26 10:15:51,973 bn4b4_branch2a_beta is fixed.
2018-06-26 10:15:51,973 bn4b4_branch2b_gamma is fixed.
2018-06-26 10:15:51,973 bn4b4_branch2b_beta is fixed.
2018-06-26 10:15:51,973 bn4b4_branch2c_gamma is fixed.
2018-06-26 10:15:51,975 bn4b4_branch2c_beta is fixed.
2018-06-26 10:15:51,976 bn4b5_branch2a_gamma is fixed.
2018-06-26 10:15:51,976 bn4b5_branch2a_beta is fixed.
2018-06-26 10:15:51,976 bn4b5_branch2b_gamma is fixed.
2018-06-26 10:15:51,976 bn4b5_branch2b_beta is fixed.
2018-06-26 10:15:51,976 bn4b5_branch2c_gamma is fixed.
2018-06-26 10:15:51,978 bn4b5_branch2c_beta is fixed.
2018-06-26 10:15:51,979 bn4b6_branch2a_gamma is fixed.
2018-06-26 10:15:51,979 bn4b6_branch2a_beta is fixed.
2018-06-26 10:15:51,979 bn4b6_branch2b_gamma is fixed.
2018-06-26 10:15:51,979 bn4b6_branch2b_beta is fixed.
2018-06-26 10:15:51,979 bn4b6_branch2c_gamma is fixed.
2018-06-26 10:15:51,979 bn4b6_branch2c_beta is fixed.
2018-06-26 10:15:51,979 bn4b7_branch2a_gamma is fixed.
2018-06-26 10:15:51,980 bn4b7_branch2a_beta is fixed.
2018-06-26 10:15:51,980 bn4b7_branch2b_gamma is fixed.
2018-06-26 10:15:51,980 bn4b7_branch2b_beta is fixed.
2018-06-26 10:15:51,980 bn4b7_branch2c_gamma is fixed.
2018-06-26 10:15:51,980 bn4b7_branch2c_beta is fixed.
2018-06-26 10:15:51,980 bn4b8_branch2a_gamma is fixed.
2018-06-26 10:15:51,981 bn4b8_branch2a_beta is fixed.
2018-06-26 10:15:51,981 bn4b8_branch2b_gamma is fixed.
2018-06-26 10:15:51,981 bn4b8_branch2b_beta is fixed.
2018-06-26 10:15:51,981 bn4b8_branch2c_gamma is fixed.
2018-06-26 10:15:51,981 bn4b8_branch2c_beta is fixed.
2018-06-26 10:15:51,981 bn4b9_branch2a_gamma is fixed.
2018-06-26 10:15:51,981 bn4b9_branch2a_beta is fixed.
2018-06-26 10:15:51,982 bn4b9_branch2b_gamma is fixed.
2018-06-26 10:15:51,982 bn4b9_branch2b_beta is fixed.
2018-06-26 10:15:51,982 bn4b9_branch2c_gamma is fixed.
2018-06-26 10:15:51,982 bn4b9_branch2c_beta is fixed.
2018-06-26 10:15:51,982 bn4b10_branch2a_gamma is fixed.
2018-06-26 10:15:51,982 bn4b10_branch2a_beta is fixed.
2018-06-26 10:15:51,982 bn4b10_branch2b_gamma is fixed.
2018-06-26 10:15:51,982 bn4b10_branch2b_beta is fixed.
2018-06-26 10:15:51,983 bn4b10_branch2c_gamma is fixed.
2018-06-26 10:15:51,983 bn4b10_branch2c_beta is fixed.
2018-06-26 10:15:51,983 bn4b11_branch2a_gamma is fixed.
2018-06-26 10:15:51,983 bn4b11_branch2a_beta is fixed.
2018-06-26 10:15:51,983 bn4b11_branch2b_gamma is fixed.
2018-06-26 10:15:51,983 bn4b11_branch2b_beta is fixed.
2018-06-26 10:15:51,983 bn4b11_branch2c_gamma is fixed.
2018-06-26 10:15:51,983 bn4b11_branch2c_beta is fixed.
2018-06-26 10:15:51,983 bn4b12_branch2a_gamma is fixed.
2018-06-26 10:15:51,984 bn4b12_branch2a_beta is fixed.
2018-06-26 10:15:51,984 bn4b12_branch2b_gamma is fixed.
2018-06-26 10:15:51,984 bn4b12_branch2b_beta is fixed.
2018-06-26 10:15:51,984 bn4b12_branch2c_gamma is fixed.
2018-06-26 10:15:51,984 bn4b12_branch2c_beta is fixed.
2018-06-26 10:15:51,986 bn4b13_branch2a_gamma is fixed.
2018-06-26 10:15:51,987 bn4b13_branch2a_beta is fixed.
2018-06-26 10:15:51,987 bn4b13_branch2b_gamma is fixed.
2018-06-26 10:15:51,987 bn4b13_branch2b_beta is fixed.
2018-06-26 10:15:51,987 bn4b13_branch2c_gamma is fixed.
2018-06-26 10:15:51,987 bn4b13_branch2c_beta is fixed.
2018-06-26 10:15:51,987 bn4b14_branch2a_gamma is fixed.
2018-06-26 10:15:51,988 bn4b14_branch2a_beta is fixed.
2018-06-26 10:15:51,988 bn4b14_branch2b_gamma is fixed.
2018-06-26 10:15:51,988 bn4b14_branch2b_beta is fixed.
2018-06-26 10:15:51,988 bn4b14_branch2c_gamma is fixed.
2018-06-26 10:15:51,988 bn4b14_branch2c_beta is fixed.
2018-06-26 10:15:51,988 bn4b15_branch2a_gamma is fixed.
2018-06-26 10:15:51,988 bn4b15_branch2a_beta is fixed.
2018-06-26 10:15:51,988 bn4b15_branch2b_gamma is fixed.
2018-06-26 10:15:51,989 bn4b15_branch2b_beta is fixed.
2018-06-26 10:15:51,989 bn4b15_branch2c_gamma is fixed.
2018-06-26 10:15:51,989 bn4b15_branch2c_beta is fixed.
2018-06-26 10:15:51,989 bn4b16_branch2a_gamma is fixed.
2018-06-26 10:15:51,989 bn4b16_branch2a_beta is fixed.
2018-06-26 10:15:51,997 bn4b16_branch2b_gamma is fixed.
2018-06-26 10:15:51,998 bn4b16_branch2b_beta is fixed.
2018-06-26 10:15:51,998 bn4b16_branch2c_gamma is fixed.
2018-06-26 10:15:51,998 bn4b16_branch2c_beta is fixed.
2018-06-26 10:15:51,998 bn4b17_branch2a_gamma is fixed.
2018-06-26 10:15:51,998 bn4b17_branch2a_beta is fixed.
2018-06-26 10:15:51,998 bn4b17_branch2b_gamma is fixed.
2018-06-26 10:15:51,998 bn4b17_branch2b_beta is fixed.
2018-06-26 10:15:51,999 bn4b17_branch2c_gamma is fixed.
2018-06-26 10:15:51,999 bn4b17_branch2c_beta is fixed.
2018-06-26 10:15:51,999 bn4b18_branch2a_gamma is fixed.
2018-06-26 10:15:51,999 bn4b18_branch2a_beta is fixed.
2018-06-26 10:15:51,999 bn4b18_branch2b_gamma is fixed.
2018-06-26 10:15:51,999 bn4b18_branch2b_beta is fixed.
2018-06-26 10:15:51,999 bn4b18_branch2c_gamma is fixed.
2018-06-26 10:15:51,999 bn4b18_branch2c_beta is fixed.
2018-06-26 10:15:51,999 bn4b19_branch2a_gamma is fixed.
2018-06-26 10:15:51,999 bn4b19_branch2a_beta is fixed.
2018-06-26 10:15:51,999 bn4b19_branch2b_gamma is fixed.
2018-06-26 10:15:51,999 bn4b19_branch2b_beta is fixed.
2018-06-26 10:15:51,999 bn4b19_branch2c_gamma is fixed.
2018-06-26 10:15:51,999 bn4b19_branch2c_beta is fixed.
2018-06-26 10:15:51,999 bn4b20_branch2a_gamma is fixed.
2018-06-26 10:15:51,999 bn4b20_branch2a_beta is fixed.
2018-06-26 10:15:51,999 bn4b20_branch2b_gamma is fixed.
2018-06-26 10:15:51,999 bn4b20_branch2b_beta is fixed.
2018-06-26 10:15:51,999 bn4b20_branch2c_gamma is fixed.
2018-06-26 10:15:51,999 bn4b20_branch2c_beta is fixed.
2018-06-26 10:15:52,000 bn4b21_branch2a_gamma is fixed.
2018-06-26 10:15:52,000 bn4b21_branch2a_beta is fixed.
2018-06-26 10:15:52,000 bn4b21_branch2b_gamma is fixed.
2018-06-26 10:15:52,000 bn4b21_branch2b_beta is fixed.
2018-06-26 10:15:52,000 bn4b21_branch2c_gamma is fixed.
2018-06-26 10:15:52,000 bn4b21_branch2c_beta is fixed.
2018-06-26 10:15:52,000 bn4b22_branch2a_gamma is fixed.
2018-06-26 10:15:52,000 bn4b22_branch2a_beta is fixed.
2018-06-26 10:15:52,000 bn4b22_branch2b_gamma is fixed.
2018-06-26 10:15:52,000 bn4b22_branch2b_beta is fixed.
2018-06-26 10:15:52,000 bn4b22_branch2c_gamma is fixed.
2018-06-26 10:15:52,000 bn4b22_branch2c_beta is fixed.
2018-06-26 10:15:52,000 bn5a_branch1_gamma is fixed.
2018-06-26 10:15:52,000 bn5a_branch1_beta is fixed.
2018-06-26 10:15:52,000 bn5a_branch2a_gamma is fixed.
2018-06-26 10:15:52,000 bn5a_branch2a_beta is fixed.
2018-06-26 10:15:52,000 bn5a_branch2b_gamma is fixed.
2018-06-26 10:15:52,001 bn5a_branch2b_beta is fixed.
2018-06-26 10:15:52,001 bn5a_branch2c_gamma is fixed.
2018-06-26 10:15:52,001 bn5a_branch2c_beta is fixed.
2018-06-26 10:15:52,001 bn5b_branch2a_gamma is fixed.
2018-06-26 10:15:52,001 bn5b_branch2a_beta is fixed.
2018-06-26 10:15:52,001 bn5b_branch2b_gamma is fixed.
2018-06-26 10:15:52,001 bn5b_branch2b_beta is fixed.
2018-06-26 10:15:52,001 bn5b_branch2c_gamma is fixed.
2018-06-26 10:15:52,001 bn5b_branch2c_beta is fixed.
2018-06-26 10:15:52,002 bn5c_branch2a_gamma is fixed.
2018-06-26 10:15:52,002 bn5c_branch2a_beta is fixed.
2018-06-26 10:15:52,002 bn5c_branch2b_gamma is fixed.
2018-06-26 10:15:52,002 bn5c_branch2b_beta is fixed.
2018-06-26 10:15:52,002 bn5c_branch2c_gamma is fixed.
2018-06-26 10:15:52,002 bn5c_branch2c_beta is fixed.
2018-06-26 10:15:52,002 data is not fixed.
2018-06-26 10:15:52,003 res3a_branch1_weight is not fixed.
2018-06-26 10:15:52,003 res3a_branch2a_weight is not fixed.
2018-06-26 10:15:52,003 res3a_branch2b_weight is not fixed.
2018-06-26 10:15:52,003 res3a_branch2c_weight is not fixed.
2018-06-26 10:15:52,003 res3b1_branch2a_weight is not fixed.
2018-06-26 10:15:52,003 res3b1_branch2b_weight is not fixed.
2018-06-26 10:15:52,003 res3b1_branch2c_weight is not fixed.
2018-06-26 10:15:52,003 res3b2_branch2a_weight is not fixed.
2018-06-26 10:15:52,004 res3b2_branch2b_weight is not fixed.
2018-06-26 10:15:52,004 res3b2_branch2c_weight is not fixed.
2018-06-26 10:15:52,004 res3b3_branch2a_weight is not fixed.
2018-06-26 10:15:52,004 res3b3_branch2b_weight is not fixed.
2018-06-26 10:15:52,004 res3b3_branch2c_weight is not fixed.
2018-06-26 10:15:52,004 res4a_branch1_weight is not fixed.
2018-06-26 10:15:52,004 res4a_branch2a_weight is not fixed.
2018-06-26 10:15:52,004 res4a_branch2b_weight is not fixed.
2018-06-26 10:15:52,005 res4a_branch2c_weight is not fixed.
2018-06-26 10:15:52,005 res4b1_branch2a_weight is not fixed.
2018-06-26 10:15:52,005 res4b1_branch2b_weight is not fixed.
2018-06-26 10:15:52,005 res4b1_branch2c_weight is not fixed.
2018-06-26 10:15:52,005 res4b2_branch2a_weight is not fixed.
2018-06-26 10:15:52,005 res4b2_branch2b_weight is not fixed.
2018-06-26 10:15:52,005 res4b2_branch2c_weight is not fixed.
2018-06-26 10:15:52,005 res4b3_branch2a_weight is not fixed.
2018-06-26 10:15:52,005 res4b3_branch2b_weight is not fixed.
2018-06-26 10:15:52,006 res4b3_branch2c_weight is not fixed.
2018-06-26 10:15:52,006 res4b4_branch2a_weight is not fixed.
2018-06-26 10:15:52,006 res4b4_branch2b_weight is not fixed.
2018-06-26 10:15:52,006 res4b4_branch2c_weight is not fixed.
2018-06-26 10:15:52,006 res4b5_branch2a_weight is not fixed.
2018-06-26 10:15:52,006 res4b5_branch2b_weight is not fixed.
2018-06-26 10:15:52,006 res4b5_branch2c_weight is not fixed.
2018-06-26 10:15:52,006 res4b6_branch2a_weight is not fixed.
2018-06-26 10:15:52,006 res4b6_branch2b_weight is not fixed.
2018-06-26 10:15:52,007 res4b6_branch2c_weight is not fixed.
2018-06-26 10:15:52,007 res4b7_branch2a_weight is not fixed.
2018-06-26 10:15:52,007 res4b7_branch2b_weight is not fixed.
2018-06-26 10:15:52,007 res4b7_branch2c_weight is not fixed.
2018-06-26 10:15:52,007 res4b8_branch2a_weight is not fixed.
2018-06-26 10:15:52,007 res4b8_branch2b_weight is not fixed.
2018-06-26 10:15:52,007 res4b8_branch2c_weight is not fixed.
2018-06-26 10:15:52,007 res4b9_branch2a_weight is not fixed.
2018-06-26 10:15:52,007 res4b9_branch2b_weight is not fixed.
2018-06-26 10:15:52,007 res4b9_branch2c_weight is not fixed.
2018-06-26 10:15:52,007 res4b10_branch2a_weight is not fixed.
2018-06-26 10:15:52,007 res4b10_branch2b_weight is not fixed.
2018-06-26 10:15:52,007 res4b10_branch2c_weight is not fixed.
2018-06-26 10:15:52,007 res4b11_branch2a_weight is not fixed.
2018-06-26 10:15:52,007 res4b11_branch2b_weight is not fixed.
2018-06-26 10:15:52,007 res4b11_branch2c_weight is not fixed.
2018-06-26 10:15:52,008 res4b12_branch2a_weight is not fixed.
2018-06-26 10:15:52,008 res4b12_branch2b_weight is not fixed.
2018-06-26 10:15:52,008 res4b12_branch2c_weight is not fixed.
2018-06-26 10:15:52,008 res4b13_branch2a_weight is not fixed.
2018-06-26 10:15:52,008 res4b13_branch2b_weight is not fixed.
2018-06-26 10:15:52,008 res4b13_branch2c_weight is not fixed.
2018-06-26 10:15:52,008 res4b14_branch2a_weight is not fixed.
2018-06-26 10:15:52,008 res4b14_branch2b_weight is not fixed.
2018-06-26 10:15:52,012 res4b14_branch2c_weight is not fixed.
2018-06-26 10:15:52,012 res4b15_branch2a_weight is not fixed.
2018-06-26 10:15:52,012 res4b15_branch2b_weight is not fixed.
2018-06-26 10:15:52,012 res4b15_branch2c_weight is not fixed.
2018-06-26 10:15:52,012 res4b16_branch2a_weight is not fixed.
2018-06-26 10:15:52,012 res4b16_branch2b_weight is not fixed.
2018-06-26 10:15:52,012 res4b16_branch2c_weight is not fixed.
2018-06-26 10:15:52,013 res4b17_branch2a_weight is not fixed.
2018-06-26 10:15:52,013 res4b17_branch2b_weight is not fixed.
2018-06-26 10:15:52,013 res4b17_branch2c_weight is not fixed.
2018-06-26 10:15:52,013 res4b18_branch2a_weight is not fixed.
2018-06-26 10:15:52,013 res4b18_branch2b_weight is not fixed.
2018-06-26 10:15:52,013 res4b18_branch2c_weight is not fixed.
2018-06-26 10:15:52,013 res4b19_branch2a_weight is not fixed.
2018-06-26 10:15:52,013 res4b19_branch2b_weight is not fixed.
2018-06-26 10:15:52,013 res4b19_branch2c_weight is not fixed.
2018-06-26 10:15:52,013 res4b20_branch2a_weight is not fixed.
2018-06-26 10:15:52,013 res4b20_branch2b_weight is not fixed.
2018-06-26 10:15:52,013 res4b20_branch2c_weight is not fixed.
2018-06-26 10:15:52,013 res4b21_branch2a_weight is not fixed.
2018-06-26 10:15:52,013 res4b21_branch2b_weight is not fixed.
2018-06-26 10:15:52,013 res4b21_branch2c_weight is not fixed.
2018-06-26 10:15:52,013 res4b22_branch2a_weight is not fixed.
2018-06-26 10:15:52,014 res4b22_branch2b_weight is not fixed.
2018-06-26 10:15:52,014 res4b22_branch2c_weight is not fixed.
2018-06-26 10:15:52,014 rpn_conv_3x3_weight is not fixed.
2018-06-26 10:15:52,014 rpn_conv_3x3_bias is not fixed.
2018-06-26 10:15:52,014 rpn_cls_score_weight is not fixed.
2018-06-26 10:15:52,014 rpn_cls_score_bias is not fixed.
2018-06-26 10:15:52,014 label is not fixed.
2018-06-26 10:15:52,014 bbox_weight is not fixed.
2018-06-26 10:15:52,014 rpn_bbox_pred_weight is not fixed.
2018-06-26 10:15:52,014 rpn_bbox_pred_bias is not fixed.
2018-06-26 10:15:52,014 bbox_target is not fixed.
2018-06-26 10:15:52,014 res5a_branch1_weight is not fixed.
2018-06-26 10:15:52,014 res5a_branch2a_weight is not fixed.
2018-06-26 10:15:52,014 res5a_branch2b_weight is not fixed.
2018-06-26 10:15:52,014 res5a_branch2c_weight is not fixed.
2018-06-26 10:15:52,014 res5b_branch2a_weight is not fixed.
2018-06-26 10:15:52,014 res5b_branch2b_weight is not fixed.
2018-06-26 10:15:52,014 res5b_branch2c_weight is not fixed.
2018-06-26 10:15:52,014 res5c_branch2a_weight is not fixed.
2018-06-26 10:15:52,015 res5c_branch2b_weight is not fixed.
2018-06-26 10:15:52,015 res5c_branch2c_weight is not fixed.
2018-06-26 10:15:52,015 conv_new_1_weight is not fixed.
2018-06-26 10:15:52,015 conv_new_1_bias is not fixed.
2018-06-26 10:15:52,015 im_info is not fixed.
2018-06-26 10:15:52,015 gt_boxes is not fixed.
2018-06-26 10:15:52,015 fc_new_1_weight is not fixed.
2018-06-26 10:15:52,015 fc_new_1_bias is not fixed.
2018-06-26 10:15:52,015 pair_pos_fc1_1_weight is not fixed.
2018-06-26 10:15:52,015 pair_pos_fc1_1_bias is not fixed.
2018-06-26 10:15:52,015 query_1_weight is not fixed.
2018-06-26 10:15:52,015 query_1_bias is not fixed.
2018-06-26 10:15:52,015 key_1_weight is not fixed.
2018-06-26 10:15:52,015 key_1_bias is not fixed.
2018-06-26 10:15:52,015 linear_out_1_weight is not fixed.
2018-06-26 10:15:52,015 linear_out_1_bias is not fixed.
2018-06-26 10:15:52,015 fc_new_2_weight is not fixed.
2018-06-26 10:15:52,015 fc_new_2_bias is not fixed.
2018-06-26 10:15:52,016 pair_pos_fc1_2_weight is not fixed.
2018-06-26 10:15:52,016 pair_pos_fc1_2_bias is not fixed.
2018-06-26 10:15:52,016 query_2_weight is not fixed.
2018-06-26 10:15:52,016 query_2_bias is not fixed.
2018-06-26 10:15:52,016 key_2_weight is not fixed.
2018-06-26 10:15:52,016 key_2_bias is not fixed.
2018-06-26 10:15:52,016 linear_out_2_weight is not fixed.
2018-06-26 10:15:52,016 linear_out_2_bias is not fixed.
2018-06-26 10:15:52,016 cls_score_weight is not fixed.
2018-06-26 10:15:52,016 cls_score_bias is not fixed.
2018-06-26 10:15:52,016 bbox_pred_weight is not fixed.
2018-06-26 10:15:52,016 bbox_pred_bias is not fixed.
2018-06-26 10:15:52,016 roi_feat_embedding_weight is not fixed.
2018-06-26 10:15:52,016 roi_feat_embedding_bias is not fixed.
2018-06-26 10:15:52,020 nms_rank_weight is not fixed.
2018-06-26 10:15:52,020 nms_rank_bias is not fixed.
2018-06-26 10:15:52,020 nms_pair_pos_fc1_1_weight is not fixed.
2018-06-26 10:15:52,020 nms_pair_pos_fc1_1_bias is not fixed.
2018-06-26 10:15:52,020 nms_query_1_weight is not fixed.
2018-06-26 10:15:52,020 nms_query_1_bias is not fixed.
2018-06-26 10:15:52,020 nms_key_1_weight is not fixed.
2018-06-26 10:15:52,021 nms_key_1_bias is not fixed.
2018-06-26 10:15:52,021 nms_linear_out_1_weight is not fixed.
2018-06-26 10:15:52,021 nms_linear_out_1_bias is not fixed.
2018-06-26 10:15:52,021 nms_logit_weight is not fixed.
2018-06-26 10:15:52,021 nms_logit_bias is not fixed.

when I use my own dataset to train the model, in the terminal I input _python experiments/relation_rcnn/rcnn_end2end_train_test.py --cfg experiments/relation_rcnn/cfgs/resnet_v1_101_coco_trainvalminus_rcnn_end2end_relation_learn_nms_8epoch.yaml --ignore_cache_ , there are some problems. Can you tell me the reason? Thanks in advance."
"After training for several epoches, it raised the following error:

```
Epoch[7] Batch [2280]	Speed: 3.99 samples/sec	Train-RPNAcc=0.995009,	RPNLogLoss=0.016979,	RPNL1Loss=0.034412,	RCNNAcc=0.797916,	RCNNLogLoss=0.433694,	RCNNL1Loss=0.423676,	
Epoch[7] Batch [2300]	Speed: 3.96 samples/sec	Train-RPNAcc=0.995029,	RPNLogLoss=0.016926,	RPNL1Loss=0.034388,	RCNNAcc=0.797866,	RCNNLogLoss=0.433422,	RCNNL1Loss=0.423580,	
Epoch[7] Batch [2320]	Speed: 3.69 samples/sec	Train-RPNAcc=0.995011,	RPNLogLoss=0.017027,	RPNL1Loss=0.034292,	RCNNAcc=0.798346,	RCNNLogLoss=0.432565,	RCNNL1Loss=0.422573,
[17:51:53] /home/fallingdust/workspace/mxnet/dmlc-core/include/dmlc/./logging.h:308: [17:51:53] src/engine/naive_engine.cc:168: Check failed: this->req_completed_ NaiveEngine only support synchronize Push so far

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine11NaiveEngine9PushAsyncESt8functionIFvNS_10RunContextENS0_18CallbackOnCompleteEEENS_7ContextERKSt6vectorIPNS0_3VarESaISA_EESE_NS_10FnPropertyEiPKc+0x3b3) [0x7f921cb635a3]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine11NaiveEngine4PushEPNS0_3OprENS_7ContextEib+0x8f) [0x7f921cb644af]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet4exec13GraphExecutor6RunOpsEbmm+0x724) [0x7f921cc08e84]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(MXExecutorForward+0x11) [0x7f921cb9ab81]
[bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f9230c02e40]
[bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7f9230c028ab]
[bt] (6) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x48f) [0x7f9230e123df]
[bt] (7) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(+0x11d82) [0x7f9230e16d82]
[bt] (8) python(PyObject_Call+0x43) [0x4b0c93]
[bt] (9) python(PyEval_EvalFrameEx+0x602f) [0x4c9f9f]

Traceback (most recent call last):
  File ""experiments/relation_rcnn/rcnn_end2end_train_test.py"", line 21, in <module>
    train_end2end.main()
  File ""experiments/relation_rcnn/../../relation_rcnn/train_end2end.py"", line 193, in main
    config.TRAIN.begin_epoch, config.TRAIN.end_epoch, config.TRAIN.lr, config.TRAIN.lr_step)
  File ""experiments/relation_rcnn/../../relation_rcnn/train_end2end.py"", line 186, in train_net
    arg_params=arg_params, aux_params=aux_params, begin_epoch=begin_epoch, num_epoch=end_epoch)
  File ""experiments/relation_rcnn/../../relation_rcnn/core/module.py"", line 999, in fit
    self.forward_backward(data_batch)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/module/base_module.py"", line 191, in forward_backward
    self.forward(data_batch, is_train=True)
  File ""experiments/relation_rcnn/../../relation_rcnn/core/module.py"", line 1074, in forward
    self._curr_module.forward(data_batch, is_train=is_train)
  File ""experiments/relation_rcnn/../../relation_rcnn/core/module.py"", line 554, in forward
    self._exec_group.forward(data_batch, is_train)
  File ""experiments/relation_rcnn/../../relation_rcnn/core/DataParallelExecutorGroup.py"", line 360, in forward
    exec_.forward(is_train=is_train)
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/executor.py"", line 150, in forward
    ctypes.c_int(int(is_train))))
  File ""/usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/base.py"", line 146, in check_call
    raise MXNetError(py_str(_LIB.MXGetLastError()))
mxnet.base.MXNetError: [17:51:53] src/engine/naive_engine.cc:168: Check failed: this->req_completed_ NaiveEngine only support synchronize Push so far

Stack trace returned 10 entries:
[bt] (0) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine11NaiveEngine9PushAsyncESt8functionIFvNS_10RunContextENS0_18CallbackOnCompleteEEENS_7ContextERKSt6vectorIPNS0_3VarESaISA_EESE_NS_10FnPropertyEiPKc+0x3b3) [0x7f921cb635a3]
[bt] (1) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet6engine11NaiveEngine4PushEPNS0_3OprENS_7ContextEib+0x8f) [0x7f921cb644af]
[bt] (2) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(_ZN5mxnet4exec13GraphExecutor6RunOpsEbmm+0x724) [0x7f921cc08e84]
[bt] (3) /usr/local/lib/python2.7/dist-packages/mxnet-1.0.0-py2.7.egg/mxnet/libmxnet.so(MXExecutorForward+0x11) [0x7f921cb9ab81]
[bt] (4) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call_unix64+0x4c) [0x7f9230c02e40]
[bt] (5) /usr/lib/x86_64-linux-gnu/libffi.so.6(ffi_call+0x2eb) [0x7f9230c028ab]
[bt] (6) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(_ctypes_callproc+0x48f) [0x7f9230e123df]
[bt] (7) /usr/lib/python2.7/lib-dynload/_ctypes.x86_64-linux-gnu.so(+0x11d82) [0x7f9230e16d82]
[bt] (8) python(PyObject_Call+0x43) [0x4b0c93]
[bt] (9) python(PyEval_EvalFrameEx+0x602f) [0x4c9f9f]

[17:51:53] src/engine/naive_engine.cc:55: Engine shutdown
```

I trained the network on just one GPU using `CUDA_VISIBLE_DEVICES=0` although I have two GPUs.
Please give me some advice or help to fix it. Thank you."
"Hi,
just found the one drive link for the pre-trained models: res101, faster rcnn and FPN is not working with **ERR_ADDRESS_UNREACHABLE**

Thanks."
"I have train on my own dataset after changing the configure file. And everything is looking good. Here is the snapshot of training log.
![image](https://user-images.githubusercontent.com/40010065/41732395-c4aa31b6-7535-11e8-9b07-2d2d64187df7.png)
![image](https://user-images.githubusercontent.com/40010065/41732405-cd84e10a-7535-11e8-8e2a-ea58a5d01613.png)
![image](https://user-images.githubusercontent.com/40010065/41732414-d4dfb952-7535-11e8-9f58-297428b20880.png)
![image](https://user-images.githubusercontent.com/40010065/41732418-d8195fec-7535-11e8-993f-94592c518969.png)
![image](https://user-images.githubusercontent.com/40010065/41732421-dab2838c-7535-11e8-9c29-e7e2ac89e3ea.png)
![image](https://user-images.githubusercontent.com/40010065/41732429-dea9e6d8-7535-11e8-9961-91c5e89d078a.png)
![image](https://user-images.githubusercontent.com/40010065/41732519-13b57c34-7536-11e8-88af-18440ae6e1a6.png)
![image](https://user-images.githubusercontent.com/40010065/41732524-1648b5c4-7536-11e8-9977-69bb85ce08ff.png)
![image](https://user-images.githubusercontent.com/40010065/41732528-19d79412-7536-11e8-8de5-375929c23b12.png)

Here is the snapshot of the test log.
![image](https://user-images.githubusercontent.com/40010065/41732713-9e110ae2-7536-11e8-88ca-8fe446cd9dea.png)

I have two questions to ask. 
1. From the train log, the Train-RPNACC and the Train-RPNLogLoss are getting worse. Does that mean I need to change the hyper parameters? And how do I change it? 
2. There is not any output in the result json file after finish testing. What happened? 

Thanks in advance. 

"
"Could you please show us how to arrange COCO dataset's directory layout(including train/val/test)?

I found it different between your specification in **cfg** file and official COCO dataset, e.g. **valminusminival2014** and **minival2014**.

The annotation files I pulled from COCO's official site include `annotations_trainval2014.zip` and `image_info_test2015.zip`. The content inside zip file looks like this for `trainval2014` and `test2015` respectively:
![image](https://user-images.githubusercontent.com/12222396/41694770-7eeeae28-753e-11e8-8175-d8b77e8e5f9a.png)
![image](https://user-images.githubusercontent.com/12222396/41694787-900e32b4-753e-11e8-86fa-3c355f6edef8.png)


Therefore, I got error message like this:
![image](https://user-images.githubusercontent.com/12222396/41694694-1a353588-753e-11e8-8ffa-9b6564cf2cb1.png)

@chengdazhi "
"Hi, I have prepared all the data/pretrained model/environment setting, then I run the command below:
`
python experiments\relation_rcnn\rcnn_end2end_train_test.py --cfg experiments/relation_rcnn/cfgs/resnet_v1_101_coco_trainvalminus_rcnn_end2end_relation_learn_nms_8epoch.yamllminus_rcnn_end2end_relation_learn_nms_8epoch.yaml
`
However, I got the error message below:
![image](https://user-images.githubusercontent.com/12222396/41670832-e7438568-74e7-11e8-9e44-13459bd44035.png)

After I uninstalled and reinstalled `numpy`, it turned to be wrong with `skimage`.
"
"There are several mAP in the paper, for example mAP@0.5, mAP@0.75, mAP@S, does any one know that what does it mean? Is it mean average precision, but if so, does it so low for the detection result? Because the mAP for the faster RCNN is more than 70%, but the best result for this paper is just around 50%. So I think it has different meaning. Many thanks.


"
"I can't download pretrained model ResNet-v1-101 from `https://1drv.ms/u/s!Am-5JzdW2XHzhqpCxvNTMZDlcDTpSA`, can you check this link or provide another link?"
"When I run     sh ./init.sh, an error comes
                  x86_64-linux-gnu-gcc: error: maskApi.c: No such file or directory
And I have installed  cython.  
It seems when run in lib/ dataset/pycocotools/
                 python setup_linux.py build_ext --inplace
"
Is it essential to exclude groundtruth box when training with relation module?And why?
""
"Getting Keyerror: segment.


  segment_raw = sio.loadmat(os.path.join(
      tf.app.flags.FLAGS.segment_dir, image_id))['segment'] #getting the error on this line
  segment_raw = process_segment_map(segment_raw, image.shape[0], image.shape[1])
  pose_raw = sio.loadmat(os.path.join(FLAGS.pose_dir, image_id))

What would have been the issue and how to resolve it:?
"
"I am trying to make a custom dataset identical to the VITON data for training. I have tried running the densepose which outputs two images: one IUV and one INDS image. However, As test rn i compared both the outputs by converting them to numpy files with the numpy files in the train_densepose folder. The outputs don't match exactly. Also, the shape is the same but the original numpy file is three rows shorter. Can you kindly guide me at what settings did you run the densepose to get the required .npy files included in train_densepose folder. "
"I am trying to inference on pictures beside the pictures that is on dataset right now. the problem is that the human parsing segmentation that is used in dataset have 14 classes but the lip dataset and github repository that you introduced for human parsing have 20 classes.
anyone knows, in dataset, which network is used for human parsing segmentation?"
"I am trying to train models using my dataset .
After compiling extract_tps( https://github.com/xthan/VITON/blob/master/prepare_data/extract_tps.m ) i got this error at line number 32 .
 

warning: textread is obsolete; use textscan instead
h = 128
w = 96
ans = 1
**error: V2(_,481): out of bound 256 (dimensions are 192x256)
error: called from
    extract_tps at line 32 column 10**


I got this error even if my data set images size is  192 * 256(width * height)
could you please help me to resolve it."
"Given the mask, how to convert between two mask images 
![0245_0](https://user-images.githubusercontent.com/49933243/70679027-e5a72a80-1cce-11ea-8826-e494952d8a73.png)
![0008_0](https://user-images.githubusercontent.com/49933243/70679030-e8098480-1cce-11ea-9c04-11335cede8de.png)
"
"For different clothing materials, such as linen clothes and a clothing figure wearing cotton, can this project be realized, if it is different fat and thin people?"
"The stage one output is totally weird. I did run teststage1. I have attached a samples output for test stage 1.
I also did modify line 52 from tf.flags.DEFINE_integer(""end"", ""2032"", """") to tf.flags.DEFINE_integer(""end"", ""1"", """") or else the loop was trying to run for 2032 times and i got this error.
  File ""model_zalando_mask_content_test.py"", line 161, in main
    info = test_info[j].split()
IndexError: list index out of range

![example_clothing](https://user-images.githubusercontent.com/47244652/67630536-57f2b580-f8af-11e9-9388-cbcb011951bf.jpg)
![example_person](https://user-images.githubusercontent.com/47244652/67630539-5d500000-f8af-11e9-8139-3fb4f479297e.jpg)
![00015000_example_person jpg_example_clothing jpg](https://user-images.githubusercontent.com/47244652/67630542-62ad4a80-f8af-11e9-9cda-20250ca30819.png)
![00015000_example_person jpg_example_clothing jpg (2)](https://user-images.githubusercontent.com/47244652/67630680-86719000-f8b1-11e9-99ec-24df2d6c1f0c.png)

Can someone please help me with this?  "
"Hi, thank you for your work. Could you please give a detailed description about the runtime environment? For example, what is the version of Python, Tensorflow and Matlab.
"
"Hi, I am trying to test the Model. I have both of the Person Segmentation and Pose Estimation setup, but I am unsure of what I need to save and use as input to Stage 1 of VITON. In prepare_data/build_viton.py, it is looking for a pickle file. But for testing, it does not specify what file type it is looking for.."
"I ran  shape_context_warp.m first, since got the control points, then used this control points as input to ran  tps_transformr.py.(I don't know it is right or wrong). At last, I got the  image by tps-transforms. but it has wrong position and size. I hope someone can help me.
![A2](https://user-images.githubusercontent.com/48465583/55926311-62b36900-5c43-11e9-9061-e3585e4e208c.png)
![A1](https://user-images.githubusercontent.com/48465583/55926317-65ae5980-5c43-11e9-8c76-04f010c69388.png)
![transformed](https://user-images.githubusercontent.com/48465583/55926320-67781d00-5c43-11e9-9c7b-b483bed5719c.png)
"
""
is it possible to provide an example test pair of images and their mat files+pose? I.e. everything needed to test the test part of the code? to make sure all works. even synthetic or toy will work. 
"Can you provide some tips on what sites you chose to scrape the training data?

Thank you very much"
"I know that that ""the dataset is no longer publicly available"", however I wanted to get access to the data to try code. How can I get access to the dataset?"
"I try to run your code but I am getting this error:

Traceback (most recent call last):
  File ""model_zalando_mask_content_test.py"", line 236, in <module>
    tf.app.run()
  File ""/BS/wild-search-gaze2/work/3D_body_shape/SMPLify/SMP_env/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""model_zalando_mask_content_test.py"", line 136, in main
    prod_segment_holder, image_holder)
  File ""/BS/wild-search-gaze2/archive00/VITON-master/model_zalando_mask_content.py"", line 200, in create_model
    vgg_real = build_vgg19(image, FLAGS.vgg_model_path)
  File ""/BS/wild-search-gaze2/archive00/VITON-master/utils.py"", line 482, in build_vgg19
    vgg_rawnet = scipy.io.loadmat(model_path)
  File ""/BS/wild-search-gaze2/work/3D_body_shape/SMPLify/SMP_env/local/lib/python2.7/site-packages/scipy/io/matlab/mio.py"", line 141, in loadmat
    MR, file_opened = mat_reader_factory(file_name, appendmat, **kwargs)
  File ""/BS/wild-search-gaze2/work/3D_body_shape/SMPLify/SMP_env/local/lib/python2.7/site-packages/scipy/io/matlab/mio.py"", line 64, in mat_reader_factory
    byte_stream, file_opened = _open_file(file_name, appendmat)
TypeError: 'NoneType' object is not iterable
"
"Getting error in test stage1.

`loading model from checkpoint
model/stage1/model-15000
INFO:tensorflow:Restoring parameters from model/stage1/model-15000
Traceback (most recent call last):
  File ""model_zalando_mask_content_test.py"", line 236, in <module>
    tf.app.run()
  File ""/home/gamut/.conda/envs/tf_3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_zalando_mask_content_test.py"", line 156, in main
    test_info = open(FLAGS.test_label).read().splitlines()
FileNotFoundError: [Errno 2] No such file or directory: 'data/viton_test_pairs.txt'
`
I have dowloaded the models and moved them in model folder as mentioned.
How to move forward now?
"
"![2018-07-21 10 19 17](https://user-images.githubusercontent.com/37466856/43044160-4ed5c404-8ddc-11e8-817b-d8694d6c66fd.png)


Hello, I want to generate VITON from your pretrained model, and when I use, It generated weird Image like upper image.

Is it necessary to train model with Dataset? Does not pretrain Model?"
I'm trying to generate tps files in data/tps for my data by running extract_tps in Matlab. But after generating for few images it is hanging.
"Hello,

I am running this code.

Stage 1 works fine.

Stage 2 runs but outputs black images. I am using Octave and not Matlab to run shape_context_warp.m. Is the output of this available online?

Regards,

Daniel"
"I understand that it is not available for commercial use, but we would like to conduct research strictly for academic purposes and thesis work. How can we do this?"
"I tested the model on some images. I rotated the segment map as well. But, I can't get good results as shown in the paper. I have used the following repositories for pose model and human parser.

Pose Model : https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation
Human Parsing : https://github.com/Engineering-Course/LIP_JPPNet

![image](https://user-images.githubusercontent.com/23500985/53867792-94b92600-401a-11e9-8165-e4c8900c750d.png)

I haven't gone through the code yet. Do I have to do anything more to get a good result? Has anyone got results the same as mentioned in the paper? Are there any restrictions on outputs of pose and human parsing models like fixed resolution, etc.,?
"
"leaky relu work great compare to relu , but in vition stage1 network for encoder activation function leaky rely and for decoder expect last layer activation relu.
can I know reason why relu for decoder network not leaky relu.
I have googled a lot , but didnot find any good explanation
 "
"in vgg19 loss your used some value ,I was wonder how did these values are taken , I gone to your paper , but there is no information regarding this
      p1 = compute_error(vgg_real['conv1_2'],
                           vgg_fake['conv1_2']) / 5.3 * **2.5**  # 128*128*64
        p2 = compute_error(vgg_real['conv2_2'],
                           vgg_fake['conv2_2']) / 2.7  / **1.2** # 64*64*128
        p3 = compute_error(vgg_real['conv3_2'],
                           vgg_fake['conv3_2']) / 1.35 / **2.3** # 32*32*256
        p4 = compute_error(vgg_real['conv4_2'],
                           vgg_fake['conv4_2']) / 0.67 / **8.2** # 16*16*512
        p5 = compute_error(vgg_real['conv5_2'],
                           vgg_fake['conv5_2']) / 0.16  # 8*8*512
can your help about how did got this value 2.5 ,1.2 , 2.3 , 8.2 , p1-p4 ?"
"I'm having problem getting into
https://drive.google.com/drive/folders/1qFU4KmvnEr4CwEFXQZS_6Ebw5dPJAE21?usp=sharing
Is that because of mine internet setting?Can you check it please?"
"1.According to your paper,your coarse network takes no rgb information other than the rgb segmentation of the women's hair and head.
I'm wondering how does the network manage to 'guess' the color of the pants and the shape and color the the woman's arm ,when no information tells the network that the color of the pants is gray?
Is this a sign of overfitting?
![screenshot from 2019-01-17 14-31-27](https://user-images.githubusercontent.com/40993476/51299819-a80d3380-1a64-11e9-9a45-b397f2253d2b.png)
![screenshot from 2019-01-17 14-33-03](https://user-images.githubusercontent.com/40993476/51299874-d3901e00-1a64-11e9-823d-c39043f801fa.png)
2.If you can already get the segmentation of a person using the LIP, why bother to train the network and get the mask yourself?
3.When I implemented LIP,I found that its segmentation results are not very satisfying, far from being able to create an accurate mask of the clothes or hair of head? Do you have similar problems?
Thanks  a lot. 
"
No such file or directory: 'data/viton_test_pairs.txt'
"Hi, I want to train this work on my own data, and when I run extract_tps.m, I always get ans='not enough'. Also there is no folder ’data/tps/‘. I don't know why. Looking forward to hearing from you. Thank you so much."
What is the format of the mat file needed for input images? can someone please specify?
"I have trained the model on my data set. While testing(stage1) it's assigning same color product to all the result images. When I change the mode to train in test_stage1.sh, it is giving good results. Was there anything wrong with my training?"
"I make data/vision_test_pairs.txt like this,
001_0.jpg 001_1.jpg
When I run test_stage1.sh, I get this error. I'm sorry I don't know how to move forward. Are there some examples of pictures could share with me. I don't know how to fix the data of pose, segment and women_top. Thank you so much."
"I found the segmentation result of person image sometimes is annoying. For example, for the upper clothes, there may output several different labels(such as upper_clothes, dress, skirt etc.) As a result, it's hard to decide to take which part as the real upper clothes region. 

Could you please show the code for processing this kind of cases? Thanks a lot.
"
"Hi, I'm trying to make your model works with other types of clothing like pants, skirt.
But none of my ideas seem to work. Can you give me some ideas of how to make your model more generalize? Thanks"
"Hi, I just want to ask question about how can keep background with wild image. I try with wild image but background disapear"
"Hello!

I 'm now trying to run preprocess_viton.sh to generate TF records. But it is hanging now with a TypeError saying ""a bytes-like"" object us required, not 'str' (shown in following screenshot). Is there anything wrong with my Python? I am new to Python. I will be appreciate if someone helps solve this problem~

Regards,
Zhidong
![tim 20180630015622](https://user-images.githubusercontent.com/24770779/42125405-bb7626e8-7ca8-11e8-89fc-a475bb1daacb.png)
"
""
"I realise the dataset isn't available for free use now, but would it be possible to shed light on what kind of data was used?"
"hi, how can I create dataset located in data folder like pose, segment, ... I want to train with my images. thank you"
""
""
"Excuse me , I want to test my dataset to this but I don't know how to generate the pose file.
I went to see the document on README but i still don't know what is subset. 
The candidate is the x,y of the keypoint, right? 
What is the subset ?"
""
"Hi,
Can you please share the annotated training data for FCN, and the FCN source code as well if possible.

Thanks"
"hi, 
I also met the problem : the first train stage hangs and just shows the infomation:
I use the parameters:
```
--output_dir ""model/stage1/"" 
--input_file_pattern ""./prepare_data/tfrecord/zalando-train-?????-of-00032""
```
I have read : [https://github.com/xthan/VITON/issues/2](url)
I test tensorflow 1 & 4 & 5 but it still hangs and shows:
```
INFO:tensorflow:Starting standard services.
INFO:tensorflow:parameter_count = 29269632
INFO:tensorflow:Starting queue runners.
```

and then nothing happen.
I want to know about the required GPU capacity and the required environment.

Any ideas ?

Thanks."
"Hi, I found that in your 'model_zalando_tps_warp.py'
in the create_model() function, you will do ""create_generator"" again, which has done in stage1.
So, this code ('model_zalando_tps_warp.py') will train stage1 and then stage2, rather than train only stage2?

Thanks a lot again!"
"Hi, I test your stage2 model with test_stage2.sh, some results are good, but some are all black.
Do you know possible problem? Thank you!"
"The input images are resized to 256*192， is it to fit the VGGnet that the height is set to 256 ？
Have you tried the larger input size to train the network? Will the larger size conduct worse results？
Looking forward to your reply，thanks.
@xthan "
"Hi!
Thanks for your project!

I wanted to train a network with my own data, that's why I had to use network from Realtime Multi-Person Pose Estimation repository to prepare data.
But in pose.pkl there are dictionaries with ""candidate"" and ""subset"" keys and I don't see them in [OpenPose output](https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md). Maybe it's possible to match them?"
"What configurations, aspect ratios & paddings did you use to generate segmentation maps?

LIP Human Parser crops images to 640X640. So if I use my 762X1100 image it crops it to 640. Now, if I use a resized image(640X640) for segmentation & use 762X1100 as the test image. I think the segmentation map generated on 640X640 doesn't maps well onto 762X1100 test image.

Please explain what padding & aspect ratios did you use for training & testing and segmentation map generation


Thank you"
"Hi,

I tried training the first stage using ""model_zalando_mask_content.py"", and set the following parameters:

             --output_dir ""model/stage1/"" 
             --input_file_pattern ""./prepare_data/tfrecord/zalando-train-?????-of-00032""

However, the training hangs when executing the following line:
              results = sess.run(fetches, options=options, run_metadata=run_metadata)

Any ideas?

Thanks a lot!"
"hey!

this is awesome! how's the implementation for the other stages coming along? if it's not on the roadmap i'm happy to have a go."
"Thanks for providing the code. :) 

Could you also provide the links of the dataset that you used in the paper. "
Is there a code for this?
"hello,where can I download the f30k-caption  in your annFile = 'f30k-caption/annotations/dataset_flickr30k.json'?when I use the flickr30,the eval code takes wrong.Is there the eval code for flickr30?"
""
"When I train the model with RL using run_att_d.sh, the CIDEr score got a significant drop,
I saw that you have provided the curve of training VSE model#11,
I wonder would you like to provide the training curve of RL stage?
Thank you very much :D"
您好！我在跑您的代码时候，发现opt文件中默认每张图片使用一个句子 ，但是在最后联合训练的脚本中，又专门指定了每张图片使用一个句子，但是其他两个脚本并没有指定。我对此感到很困惑，请问您在实现的时候，训练自检索模型、预训练caption模型和最后的联合训练每张图片分别用了几个句子呢？
"I think your work is very similar to ""Deep Reinforcement Learning-based Image Captioning with Embedding Reward"".  I wonder what is the difference with them?"
"excuse me, would you mind explaining   the function about the att_mask?  "
"Hello, luo
when I pretrain the VSEFCmodel,  the vse_loss doesn't converge well , just around 51.2. is there some mistakes in my experiments, how about your vse_loss when you pretrain VSEFCmodel?"
"when I try to run on TopDown model,
I got the following error:

  File ""/home/code/DiscCaptioning/models/AttModel.py"", line 476, in forward
    att_lstm_input = torch.cat([prev_h, fc_feats, xt], 1) 
RuntimeError: zero-dimensional tensor (at position 0) cannot be concatenated

Would you please tell me which parts of code I need to modify so I can train on the TopDown model?"
"![图片](https://user-images.githubusercontent.com/13494034/69141284-64a9a880-0aff-11ea-82f9-944c447ca5c9.png)
In Rama's paper, they said that they use the split from Karpathy's paper,
but in your README.md you mention that it's different from standard Karpathy's split.

I wonder what's the differences between these 2 splits, thanks!"
"The negatives used for the retrieval  model are all the rest images of the entire  batch?  In your paper you mentioned  B images, how many  are those images? "
"pytorch 1.0.0 
```
bash eval.sh att_d1 test
```

Traceback (most recent call last):
  File ""eval.py"", line 146, in <module>
    vars(opt))
  File ""/content/DiscCaptioning/eval_utils.py"", line 92, in eval_split
    data = loader.get_batch(split)
  File ""/content/DiscCaptioning/dataloader.py"", line 137, in get_batch
    ix, tmp_wrapped = self._prefetch_process[split].get()
  File ""/content/DiscCaptioning/dataloader.py"", line 256, in get
    self.reset()
  File ""/content/DiscCaptioning/dataloader.py"", line 235, in reset
    collate_fn=lambda x: x[0]))
  File ""/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py"", line 805, in __init__
    batch_sampler = BatchSampler(sampler, batch_size, drop_last)
  File ""/usr/local/lib/python2.7/dist-packages/torch/utils/data/sampler.py"", line 146, in __init__
    .format(sampler))
ValueError: sampler should be an instance of torch.utils.data.Sampler, but got sampler=[......]
Terminating BlobFetcher
"
"Thanks for the implementation.

I downloaded the pre-trained models but the file is not a folder which eval.sh code is asking for and the downloaded file is in a file format which is not accessible.

I think the file gets corrupted after downloading from the drive.

IOError: [Errno 20] Not a directory: 'log_att_d1/infos_att_d1.pkl'"
"When I try to train the retrieval model using `bash run_fc_con.sh`, there is a problem before saving model. Actually, line 147 of FCModel.py `xt = self.new_img_embed(fc_feats[k:k+1], fc_feats_d.chunk(batch_size)[k]).expand(beam_size, self.input_encoding_size)` results in the issue, because it reminds that there is no attribute named `new_img_embed`. Also, `fc_feats_d` is an error, because the pycharm says that ""Unresolved reference"".

But after switching `--caption_model fc` to `--caption_model att2in2` in `run_fc_con.sh` file, the issue will be solved, so I think the inference of FC model could be wrong.

BTW, (1) when training retrieval model, why do you need to use the caption model to generate captions?
(2) the image features is extracted before training the model, so I think you do not fine-tune the CNN, right?"
"Thanks for your works! Could you provide "" infos_att_d1.pkl"" for us?"
"okenization...
PTBTokenizer tokenized 99344 tokens at 721579.12 tokens per second.
PTBTokenizer tokenized 16786 tokens at 239467.86 tokens per second.
setting up scorers...
computing Bleu score...
{'reflen': 15132, 'guess': [15165, 13543, 11921, 10299], 'testlen': 15165, 'correct': [30, 0, 0, 0]}
ratio: 1.00218080888
Bleu_1: 0.002
Bleu_2: 0.000
Bleu_3: 0.000
Bleu_4: 0.000
computing METEOR score...
METEOR: 0.009
computing Rouge score...
ROUGE_L: 0.002
computing CIDEr score...
CIDEr: 0.001
computing SPICE score...
Parsing reference captions
Parsing test captions
SPICE evaluation took: 2.144 s
SPICE: 0.002
loss:  {'loss': tensor(31.6388, device='cuda:0'), 'cap_xe': tensor(31.6419, device='cuda:0'), 'retrieval_loss_greedy': tensor(7.4241, device='cuda:0'), 'retrieval_sc_loss': tensor(1.00000e-03 *
       -3.1324, device='cuda:0'), 'loss_vse': tensor(0., device='cuda:0'), 'loss_cap': tensor(31.6419, device='cuda:0'), 'retrieval_loss': tensor(7.6047, device='cuda:0')}
{u'SPICE_Object': '0.006404463463649654', u'SPICE_Cardinality': '0.0', u'SPICE_Attribute': '0.0', 'CIDEr': '0.001079661462843171', u'SPICE_Size': '0.0', 'Bleu_4': 1.04439324421061e-15, 'Bleu_3': 2.3054219540753186e-14, 'Bleu_2': 1.208598304910465e-11, 'Bleu_1': 0.001978239366963272, u'SPICE_Color': '0.0', 'ROUGE_L': '0.001795472073475935', 'METEOR': 0.009059195566343728, u'SPICE_Relation': '0.0', 'SPICE': '0.0024048127567198488'}
Terminating BlobFetcher

**Is this evaluating the image caption model? It looks like the retrieval model.**
image 474190: woods conditioner china memorial scraper sash bringing woods interstate sunroof distant
image 277907: woods pairs china listed want listed bringing woods crowd
image 43033: woods hanging service woods peep dinosaurs cooking wonder
image 542103: woods conditioner china memorial gooey bringing cooking gain woody adorable
image 356116: woods majestically rice bringing cooking gain woody woods peep
image 538581: woods hanging service woods windsurfer dinosaurs cooking weeds woody woods windsurfer
image 359354: woods hanging effects woods silver dinosaurs woods silver
image 457146: woods captive honk bringing retrieve china woods holds
image 75305: woods majestically honk lots woods goofing woody woods silver
image 249968: woods troll honk bringing cooking fir china woods bubble foreheads
image 480451: woods hanging catchers woods tightly hollow bringing woods tightly hitting
image 379596: woods hangings china pouches want pouches bringing woods goofing
image 322362: woods patch benched honk bringing woods holds woody woods overgrowth gains
image 495233: woods conditioner china memorial honk bringing woods caddy musical woods overgrowth gains
image 366948: woods conditioner china lipstick rice dinosaurs woods mirrors
image 332833: woods burrito levels honk bringing cooking lock woody woods keypad
image 512346: woods hanging service woods draining buddhist dinosaurs woods peek
evaluating validation preformance... 2049/5000 (31.236956)

"
"Hi, ruotian. When I ran ‘bash run_fc_con.sh’, the training is terminated by the above error. 
I have checked the '100000.npy ', which is 1.8G  and cannot be loaded by numpy.  So I wonder if this file is not right."
"After training the retrieval model with ""bash run_fc_con.sh"", I pretrain the captioning model with ""bash run_att.sh"". However, it is not successful, with the following error:

 DataLoader loading json file:  data/cocotalk.json
vocab size is  9487
DataLoader loading h5 file:  data/cocotalk_fc data/cocobu_att data/cocotalk_label.h5
max sequence length in data is 16
read 123287 image features
assigned 113287 images to split train
assigned 5000 images to split val
assigned 5000 images to split test
Make sure the vse opt are the same !!!!!
Make sure the vse opt are the same !!!!!
Make sure the vse opt are the same !!!!!
Make sure the vse opt are the same !!!!!
...

key caption_generator.core.a2c.weight in model.state_dict() not in loaded state_dict
key caption_generator.core.h2h.bias in model.state_dict() not in loaded state_dict
key caption_generator.att_embed.0.weight in model.state_dict() not in loaded state_dict
...
key caption_generator.core.h2h.weight in model.state_dict() not in loaded state_dict
Read data: 0.360612869263
/home/jzheng/PycharmProjects/DiscCaptioning/misc/utils.py:123: UserWarning: volatile was removed (Variable.volatile is always False)
  if isinstance(x, Variable) and volatile!=x.volatile:
/home/jzheng/PycharmProjects/DiscCaptioning/models/AttModel.py:514: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  weight = F.softmax(dot)                             # batch * att_size
/home/jzheng/PycharmProjects/DiscCaptioning/models/AttModel.py:125: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.logit(output))
/home/jzheng/PycharmProjects/DiscCaptioning/models/AttModel.py:131: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  self._loss['xe'] = loss.data[0]
/home/jzheng/PycharmProjects/DiscCaptioning/models/JointModel.py:122: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  self._loss['loss_cap'] = loss_cap.data[0]
iter 0 (epoch 0), train_loss = 9.190, time/batch = 0.157
loss_cap = 9.190 loss = 9.190 cap_xe = 9.190 loss_vse = 0.000 
Read data: 0.176112890244
iter 1 (epoch 0), train_loss = 8.774, time/batch = 0.129
loss_cap = 8.774 loss = 8.774 cap_xe = 8.774 loss_vse = 0.000 
Read data: 0.179361104965
iter 2 (epoch 0), train_loss = 8.381, time/batch = 0.116
loss_cap = 8.381 loss = 8.381 cap_xe = 8.381 loss_vse = 0.000 
...

Because of this error, the model is not learning.
"
"While I was training with the command ""bash run_fc_con.sh"", the training was terminated because the following error:

Traceback (most recent call last):
  File ""train.py"", line 250, in <module>
    train(opt)
  File ""train.py"", line 163, in train
    if opt.evaluation_retrieval:
AttributeError: 'Namespace' object has no attribute 'evaluation_retrieval'
Terminating BlobFetcher

However, in the opt.py file, I don't see the argument of ""evaluation_retrieval"". 
"
"To train on our own, do we have to pre-process for self-critical model? If so, I think a python file is missing here in this repo."
"  File ""/home/jzheng/PycharmProjects/DiscCaptioning/eval_utils.py"", line 114, in eval_split
    data['att_masks'][np.arange(loader.batch_size) * loader.seq_per_img]]
KeyError: 'att_masks'
There's no att_masks key in the data dict. Neither labels and masks. Am  I missing sth? 

I'm testing on val2014 dataset.
"
"Whenever I run the following command:

python main.py --model condensenet_converted -b 64 -j 4 C:\CondenseNet-master --stages 4-6-8-10-8 --growth 8-16-32-64-128 --group-1x1 4 --group-3x3 4 --condense-factor 4 --evaluate-from C:\CondenseNet-master/converted_condensenet_4.pth.tar --gpu 0

I get the following error:

RuntimeError: INDICES element is out of DATA bounds, id=53888868763566084 axis_dim=2064

any idea how to solve this issue?

Thank you in advance."
"Hi,

I have some questions on the consistency of implementation of dropping and paper.

1) When you take the sum, you did not use absolute values as written in the paper.
https://github.com/ShichenLiu/CondenseNet/blob/833a91d5f859df25579f70a2439dfd62f7fefb29/layers.py#L86

2) You drop during the stage, not when the stage finishes, as written in the paper.
https://github.com/ShichenLiu/CondenseNet/blob/833a91d5f859df25579f70a2439dfd62f7fefb29/layers.py#L62

Am I wrong or would you explain about it ? Thank you."
"Getting following error message when running the trained ImageNet model for image classification on my machine, which I downloaded from author's Dropbox link posted in this repo's readme link:

`model.load_state_dict(torch.load(PATH, map_location=torch.device(""cpu""))[\'state_dict\'])\n', '  File ""C:\\Program Files\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py"", line 1052, in load_state_dict\n    self.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n', 'RuntimeError: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: ""module.features.denseblock_1.denselayer_1.conv_1._count"", ""module.features.denseblock_1.denselayer_1.conv_1._stage"", ""module.features.denseblock_1.denselayer_1.conv_1._mask"", ""module.features.denseblock_1.denselayer_2.conv_1._count"", ""module.features.denseblock_1.denselayer_2.conv_1._stage"", ""module.features.denseblock_1.denselayer_2.conv_1._mask"", ""module.features.denseblock_1.denselayer_3.conv_1._count"", ""module.features.denseblock_1.denselayer_3.conv_1._stage"", ""module.features.denseblock_1.denselayer_3.conv_1._mask"", ""module.features.denseblock_1.denselayer_4.conv_1._count"", ""module.features.denseblock_1.denselayer_4.conv_1._stage"", ""module.features.denseblock_1.denselayer_4.conv_1._mask"", ""module.features.denseblock_2.denselayer_1.conv_1._count"", ""module.features.denseblock_2.denselayer_1.conv_1._stage"", ""module.features.denseblock_2.denselayer_1.conv_1._mask"", ""module.features.denseblock_2.denselayer_2.conv_1._count"", ""module.features.denseblock_2.denselayer_2.conv_1._stage"", ""module.features.denseblock_2.denselayer_2.conv_1._mask"", ""module.features.denseblock_2.denselayer_3.conv_1._count"", ""module.features.denseblock_2.denselayer_3.conv_1._stage"", ""module.features.denseblock_2.denselayer_3.conv_1._mask"", ""module.features.denseblock_2.denselayer_4.conv_1._count"", ""module.features.denseblock_2.denselayer_4.conv_1._stage"", ""module.features.denseblock_2.denselayer_4.conv_1._mask"", ""module.features.denseblock_2.denselayer_5.conv_1._count"", ""module.features.denseblock_2.denselayer_5.conv_1._stage"", ""module.features.denseblock_2.denselayer_5.conv_1._mask"", ""module.features.denseblock_2.denselayer_6.conv_1._count"", ""module.features.denseblock_2.denselayer_6.conv_1._stage"", ""module.features.denseblock_2.denselayer_6.conv_1._mask"", ""module.features.denseblock_3.denselayer_1.conv_1._count"", ""module.features.denseblock_3.denselayer_1.conv_1._stage"", ""module.features.denseblock_3.denselayer_1.conv_1._mask"", ""module.features.denseblock_3.denselayer_2.conv_1._count"", ""module.features.denseblock_3.denselayer_2.conv_1._stage"", ""module.features.denseblock_3.denselayer_2.conv_1._mask"", ""module.features.denseblock_3.denselayer_3.conv_1._count"", ""module.features.denseblock_3.denselayer_3.conv_1._stage"", ""module.features.denseblock_3.denselayer_3.conv_1._mask"", ""module.features.denseblock_3.denselayer_4.conv_1._count"", ""module.features.denseblock_3.denselayer_4.conv_1._stage"", ""module.features.denseblock_3.denselayer_4.conv_1._mask"", ""module.features.denseblock_3.denselayer_5.conv_1._count"", ""module.features.denseblock_3.denselayer_5.conv_1._stage"", ""module.features.denseblock_3.denselayer_5.conv_1._mask"", ""module.features.denseblock_3.denselayer_6.conv_1._count"", ""module.features.denseblock_3.denselayer_6.conv_1._stage"", ""module.features.denseblock_3.denselayer_6.conv_1._mask"", ""module.features.denseblock_3.denselayer_7.conv_1._count"", ""module.features.denseblock_3.denselayer_7.conv_1._stage"", ""module.features.denseblock_3.denselayer_7.conv_1._mask"", ""module.features.denseblock_3.denselayer_8.conv_1._count"", ""module.features.denseblock_3.denselayer_8.conv_1._stage"", ""module.features.denseblock_3.denselayer_8.conv_1._mask"", ""module.features.denseblock_4.denselayer_1.conv_1._count"", ""module.features.denseblock_4.denselayer_1.conv_1._stage"", ""module.features.denseblock_4.denselayer_1.conv_1._mask"", ""module.features.denseblock_4.denselayer_2.conv_1._count"", ""module.features.denseblock_4.denselayer_2.conv_1._stage"", ""module.features.denseblock_4.denselayer_2.conv_1._mask"", ""module.features.denseblock_4.denselayer_3.conv_1._count"", ""module.features.denseblock_4.denselayer_3.conv_1._stage"", ""module.features.denseblock_4.denselayer_3.conv_1._mask"", ""module.features.denseblock_4.denselayer_4.conv_1._count"", ""module.features.denseblock_4.denselayer_4.conv_1._stage"", ""module.features.denseblock_4.denselayer_4.conv_1._mask"", ""module.features.denseblock_4.denselayer_5.conv_1._count"", ""module.features.denseblock_4.denselayer_5.conv_1._stage"", ""module.features.denseblock_4.denselayer_5.conv_1._mask"", ""module.features.denseblock_4.denselayer_6.conv_1._count"", ""module.features.denseblock_4.denselayer_6.conv_1._stage"", ""module.features.denseblock_4.denselayer_6.conv_1._mask"", ""module.features.denseblock_4.denselayer_7.conv_1._count"", ""module.features.denseblock_4.denselayer_7.conv_1._stage"", ""module.features.denseblock_4.denselayer_7.conv_1._mask"", ""module.features.denseblock_4.denselayer_8.conv_1._count"", ""module.features.denseblock_4.denselayer_8.conv_1._stage"", ""module.features.denseblock_4.denselayer_8.conv_1._mask"", ""module.features.denseblock_4.denselayer_9.conv_1._count"", ""module.features.denseblock_4.denselayer_9.conv_1._stage"", ""module.features.denseblock_4.denselayer_9.conv_1._mask"", ""module.features.denseblock_4.denselayer_10.conv_1._count"", ""module.features.denseblock_4.denselayer_10.conv_1._stage"", ""module.features.denseblock_4.denselayer_10.conv_1._mask"", ""module.features.denseblock_5.denselayer_1.conv_1._count"", ""module.features.denseblock_5.denselayer_1.conv_1._stage"", ""module.features.denseblock_5.denselayer_1.conv_1._mask"", ""module.features.denseblock_5.denselayer_2.conv_1._count"", ""module.features.denseblock_5.denselayer_2.conv_1._stage"", ""module.features.denseblock_5.denselayer_2.conv_1._mask"", ""module.features.denseblock_5.denselayer_3.conv_1._count"", ""module.features.denseblock_5.denselayer_3.conv_1._stage"", ""module.features.denseblock_5.denselayer_3.conv_1._mask"", ""module.features.denseblock_5.denselayer_4.conv_1._count"", ""module.features.denseblock_5.denselayer_4.conv_1._stage"", ""module.features.denseblock_5.denselayer_4.conv_1._mask"", ""module.features.denseblock_5.denselayer_5.conv_1._count"", ""module.features.denseblock_5.denselayer_5.conv_1._stage"", ""module.features.denseblock_5.denselayer_5.conv_1._mask"", ""module.features.denseblock_5.denselayer_6.conv_1._count"", ""module.features.denseblock_5.denselayer_6.conv_1._stage"", ""module.features.denseblock_5.denselayer_6.conv_1._mask"", ""module.features.denseblock_5.denselayer_7.conv_1._count"", ""module.features.denseblock_5.denselayer_7.conv_1._stage"", ""module.features.denseblock_5.denselayer_7.conv_1._mask"", ""module.features.denseblock_5.denselayer_8.conv_1._count"", ""module.features.denseblock_5.denselayer_8.conv_1._stage"", ""module.features.denseblock_5.denselayer_8.conv_1._mask"", ""module.classifier.weight"", ""module.classifier.bias"". \n\tUnexpected key(s) in state_dict: ""module.features.denseblock_1.denselayer_1.conv_1.index"", ""module.features.denseblock_1.denselayer_2.conv_1.index"", ""module.features.denseblock_1.denselayer_3.conv_1.index"", ""module.features.denseblock_1.denselayer_4.conv_1.index"", ""module.features.denseblock_2.denselayer_1.conv_1.index"", ""module.features.denseblock_2.denselayer_2.conv_1.index"", ""module.features.denseblock_2.denselayer_3.conv_1.index"", ""module.features.denseblock_2.denselayer_4.conv_1.index"", ""module.features.denseblock_2.denselayer_5.conv_1.index"", ""module.features.denseblock_2.denselayer_6.conv_1.index"", ""module.features.denseblock_3.denselayer_1.conv_1.index"", ""module.features.denseblock_3.denselayer_2.conv_1.index"", ""module.features.denseblock_3.denselayer_3.conv_1.index"", ""module.features.denseblock_3.denselayer_4.conv_1.index"", ""module.features.denseblock_3.denselayer_5.conv_1.index"", ""module.features.denseblock_3.denselayer_6.conv_1.index"", ""module.features.denseblock_3.denselayer_7.conv_1.index"", ""module.features.denseblock_3.denselayer_8.conv_1.index"", ""module.features.denseblock_4.denselayer_1.conv_1.index"", ""module.features.denseblock_4.denselayer_2.conv_1.index"", ""module.features.denseblock_4.denselayer_3.conv_1.index"", ""module.features.denseblock_4.denselayer_4.conv_1.index"", ""module.features.denseblock_4.denselayer_5.conv_1.index"", ""module.features.denseblock_4.denselayer_6.conv_1.index"", ""module.features.denseblock_4.denselayer_7.conv_1.index"", ""module.features.denseblock_4.denselayer_8.conv_1.index"", ""module.features.denseblock_4.denselayer_9.conv_1.index"", ""module.features.denseblock_4.denselayer_10.conv_1.index"", ""module.features.denseblock_5.denselayer_1.conv_1.index"", ""module.features.denseblock_5.denselayer_2.conv_1.index"", ""module.features.denseblock_5.denselayer_3.conv_1.index"", ""module.features.denseblock_5.denselayer_4.conv_1.index"", ""module.features.denseblock_5.denselayer_5.conv_1.index"", ""module.features.denseblock_5.denselayer_6.conv_1.index"", ""module.features.denseblock_5.denselayer_7.conv_1.index"", ""module.features.denseblock_5.denselayer_8.conv_1.index"", ""module.classifier.index"", ""module.classifier.linear.weight"", ""module.classifier.linear.bias"". \n\tsize mismatch for module.features.denseblock_1.denselayer_1.conv_1.conv.weight: copying a param with shape torch.Size([32, 2, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 16, 1, 1]).\n\tsize mismatch for module.features.denseblock_1.denselayer_1.conv_2.conv.weight: copying a param with shape torch.Size([8, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([8, 8, 3, 3]).\n\tsize mismatch for module.features.denseblock_1.denselayer_2.conv_1.conv.weight: copying a param with shape torch.Size([32, 3, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 24, 1, 1]).\n\tsize mismatch for module.features.denseblock_1.denselayer_2.conv_2.conv.weight: copying a param with shape torch.Size([8, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([8, 8, 3, 3]).\n\tsize mismatch for module.features.denseblock_1.denselayer_3.conv_1.conv.weight: copying a param with shape torch.Size([32, 4, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 32, 1, 1]).\n\tsize mismatch for module.features.denseblock_1.denselayer_3.conv_2.conv.weight: copying a param with shape torch.Size([8, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([8, 8, 3, 3]).\n\tsize mismatch for module.features.denseblock_1.denselayer_4.conv_1.conv.weight: copying a param with shape torch.Size([32, 5, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 40, 1, 1]).\n\tsize mismatch for module.features.denseblock_1.denselayer_4.conv_2.conv.weight: copying a param with shape torch.Size([8, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([8, 8, 3, 3]).\n\tsize mismatch for module.features.denseblock_2.denselayer_1.conv_1.conv.weight: copying a param with shape torch.Size([64, 6, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 48, 1, 1]).\n\tsize mismatch for module.features.denseblock_2.denselayer_1.conv_2.conv.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for module.features.denseblock_2.denselayer_2.conv_1.conv.weight: copying a param with shape torch.Size([64, 8, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for module.features.denseblock_2.denselayer_2.conv_2.conv.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for module.features.denseblock_2.denselayer_3.conv_1.conv.weight: copying a param with shape torch.Size([64, 10, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 80, 1, 1]).\n\tsize mismatch for module.features.denseblock_2.denselayer_3.conv_2.conv.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for module.features.denseblock_2.denselayer_4.conv_1.conv.weight: copying a param with shape torch.Size([64, 12, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 96, 1, 1]).\n\tsize mismatch for module.features.denseblock_2.denselayer_4.conv_2.conv.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for module.features.denseblock_2.denselayer_5.conv_1.conv.weight: copying a param with shape torch.Size([64, 14, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 112, 1, 1]).\n\tsize mismatch for module.features.denseblock_2.denselayer_5.conv_2.conv.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for module.features.denseblock_2.denselayer_6.conv_1.conv.weight: copying a param with shape torch.Size([64, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n\tsize mismatch for module.features.denseblock_2.denselayer_6.conv_2.conv.weight: copying a param with shape torch.Size([16, 8, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_1.conv_1.conv.weight: copying a param with shape torch.Size([128, 18, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 144, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_1.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_2.conv_1.conv.weight: copying a param with shape torch.Size([128, 22, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 176, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_2.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_3.conv_1.conv.weight: copying a param with shape torch.Size([128, 26, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 208, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_3.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_4.conv_1.conv.weight: copying a param with shape torch.Size([128, 30, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 240, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_4.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_5.conv_1.conv.weight: copying a param with shape torch.Size([128, 34, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 272, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_5.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_6.conv_1.conv.weight: copying a param with shape torch.Size([128, 38, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 304, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_6.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_7.conv_1.conv.weight: copying a param with shape torch.Size([128, 42, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 336, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_7.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_3.denselayer_8.conv_1.conv.weight: copying a param with shape torch.Size([128, 46, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 368, 1, 1]).\n\tsize mismatch for module.features.denseblock_3.denselayer_8.conv_2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_1.conv_1.conv.weight: copying a param with shape torch.Size([256, 50, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 400, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_1.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_2.conv_1.conv.weight: copying a param with shape torch.Size([256, 58, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 464, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_2.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_3.conv_1.conv.weight: copying a param with shape torch.Size([256, 66, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 528, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_3.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_4.conv_1.conv.weight: copying a param with shape torch.Size([256, 74, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 592, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_4.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_5.conv_1.conv.weight: copying a param with shape torch.Size([256, 82, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 656, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_5.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_6.conv_1.conv.weight: copying a param with shape torch.Size([256, 90, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 720, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_6.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_7.conv_1.conv.weight: copying a param with shape torch.Size([256, 98, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 784, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_7.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_8.conv_1.conv.weight: copying a param with shape torch.Size([256, 106, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 848, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_8.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_9.conv_1.conv.weight: copying a param with shape torch.Size([256, 114, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 912, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_9.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_4.denselayer_10.conv_1.conv.weight: copying a param with shape torch.Size([256, 122, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 976, 1, 1]).\n\tsize mismatch for module.features.denseblock_4.denselayer_10.conv_2.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_1.conv_1.conv.weight: copying a param with shape torch.Size([512, 130, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1040, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_1.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_2.conv_1.conv.weight: copying a param with shape torch.Size([512, 146, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1168, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_2.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_3.conv_1.conv.weight: copying a param with shape torch.Size([512, 162, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1296, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_3.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_4.conv_1.conv.weight: copying a param with shape torch.Size([512, 178, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1424, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_4.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_5.conv_1.conv.weight: copying a param with shape torch.Size([512, 194, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1552, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_5.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_6.conv_1.conv.weight: copying a param with shape torch.Size([512, 210, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1680, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_6.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_7.conv_1.conv.weight: copying a param with shape torch.Size([512, 226, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1808, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_7.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for module.features.denseblock_5.denselayer_8.conv_1.conv.weight: copying a param with shape torch.Size([512, 242, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 1936, 1, 1]).\n\tsize mismatch for module.features.denseblock_5.denselayer_8.conv_2.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n']`

This is the training argument I have used in my image classification prediction script:
`args = parser.parse_args([""--model"", ""condensenet_converted"", ""-b"", ""64"", ""-j"", ""20"", ""imagenet"", ""--stages"", ""4-6-8-10-8"", ""--growth"", ""8-16-32-64-128"", ""--gpu"", ""0""])`. I have tried both, (C=G=4) and (C=G=8) pre-trained models from this repo. Thank you."
"Hello, 
would it be possible to upload +FDC model? 
Since I am new to pytorch I am afraid of making mistakes when applying FDC by myself. 

"
" 
 "
"There are several problems of train-loader from torch.vision using my torch version (1.0.1), so could you show the requirements.txt or a latest code version using torch 1.0.1 ?"
"Hello dear brother,
i am running condensenet and densent_LGC in pycahem IDE with Python 3.6.5 :: Anaconda, Inc. with following configurations:
dataset - cifar10
epochs - 200
bottleneck - 3
growth - 12-12-12
C=G=4
batch size = 64

i am getting this error exactly at 34th epoch every time. i have tried to remove 'inplace addition operations'  and ' inplace=True' from Relu, but nothing has worked . Can you please help, how can it be fixed. Regards

Epoch -  31
          * Accuracy@1 89.150 Accuracy@5 99.520
Epoch -  32
          * Accuracy@1 88.120 Accuracy@5 99.610
Epoch -  33
          * Accuracy@1 88.640 Accuracy@5 99.630
Epoch -  34
Traceback (most recent call last):
  File ""/home/supernet/PycharmProjects/untitled/nets/CondenseNet-Nauman/main.py"", line 497, in <module>
    main()
  File ""/home/supernet/PycharmProjects/untitled/nets/CondenseNet-Nauman/main.py"", line 257, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File ""/home/supernet/PycharmProjects/untitled/nets/CondenseNet-Nauman/main.py"", line 340, in train
    loss.backward()
  File ""/home/supernet/anaconda3/lib/python3.6/site-packages/torch/tensor.py"", line 93, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/home/supernet/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py"", line 89, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation"
"Hello,
I am getting good accuracy with CondenseNet on my dataset when training from scratch, but I feel like I could boost the results if I could train from a checkpoint pretrained on ImageNet.
You offer the converted checkpoints on ImageNet, but my understanding of the situation is that I can't use that for transfer learning because its missing some dicts like the optimizer.
Since I am new to pytorch I feel like I am missing something. Is it possible to train from the converted checkpoint? If no, would it be possible to upload the unconverted model?"
"Hi, i see the benchmark from `README`. I have some questions. 
What's the platform used for inference time testing?
It's there any neon acceleration for depthwise conv in mobilenet?
There is a great difference between theoretical acceleration and actual acceleration. Although the amount of computation of mobilenet is twice as much as that of condensenet, I still want to know the speed difference after specific optimization.

### Inference time on ARM platform

| Model | FLOPs | Top-1 | Time(s) |
|---|---|---|---|
| VGG-16 | 15,300M | 28.5 | 354 |
| ResNet-18 | 1,818M | 30.2 | 8.14 |
| 1.0 MobileNet-224 | 569M | 29.4 | 1.96 |
| CondenseNet-74 (C=G=4) | 529M | 26.2 | 1.89 |
| CondenseNet-74 (C=G=8) | 274M | 29.0 | 0.99 |


"
"@ShichenLiu 

Do you have a plan to implement object detection, like SSD or faster-rcnn using condensenet?

Thanks,
"
"I compared the forward pass speed of the larger ImageNet model with DenseNet-121 and the latter actually works faster. After benchmarking my guess is that CondenseConv layer is the cause of the slowdown due to memory transfers in ShuffleLayer and torch.index_select.
@ShichenLiu can you comment on this, did you get better performance compared to DenseNet-121 in your experiments?"
"Getting the following error message:

`Traceback (most recent call last):`
`  File ""main.py"", line 479, in <module>`
`main()`
`  File ""main.py"", line 239, in main`
`    train(train_loader, model, criterion, optimizer, epoch)`
`  File ""main.py"", line 314, in train`
`    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))`
`  File ""main.py"", line 473, in accuracy`
`    correct_k = correct[:k].view(-1).float().sum(0)`
`RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.`

@lvdmaaten @ShichenLiu @gaohuang @ironfist2 Can CondenseNet be updated to be compatible with the latest PyTorch version 1.9.0? Or can you please tell us what changes need to be made?

Thank you!

EDIT: I just replaced the `view` function with `reshape` as suggested in the error and it works. Though I am still not sure of the difference between the two functions in this context."
"I am aware that you have tested CondenseNet model with PyTorch on CPU (which is an ARM processor) of Jetson TX2 which has Nvidia CUDA support.

However, can this model be tested with PyTorch on a ARM CPU/system without CUDA support i.e. using only CPU resources? We have NXP BlueBox 2.0 and it does not support CUDA.

At the moment, I am getting this error on my non-CUDA system: 
`RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.`

After I add `map_location=torch.device('cpu')` to `torch.load` I get this error:
`RuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu`

When I run the base script on a GPU machine with Nvidia CUDA, model testing runs without any issue."
Can you please update CondenseNet to support PyTorch's latest version 1.6.0? Thanks
"@ShichenLiu and @gaohuang : I want to run this model on my machine without GPU.  How do I exactly do this? What lines of code should I modify, if any?

Command: `python main.py --model condensenet -b 64 -j 12 cifar10 --stages 14-14-14 --growth 8-16-32 --gpu 0 --resume` requires my machine to have a GPU which my machine doesn’t have. 

My goal is to run this model on CPU (machine without GPU). Thank you!"
"Hi I notice the aim of the function(CondensingLinear) is mentioned in this answer https://github.com/ShichenLiu/CondenseNet/issues/6#issuecomment-351598403,
Does CondensingLinear in layers.py only be used at convert_model phase, not in the training phase?
Therefore the evaluation error on converted model will be higher than the model in the 
training phase, right? It is like directly pruning out 50% of the last fully connected layers
without finetuning.
"
"Hi, I noticed that condensenet-86 on cifar10 is 0.52M on cifar10
However using torchsummary package, the total calculated params are as follow:
![cifar10](https://user-images.githubusercontent.com/41462408/65400377-63584a00-ddf4-11e9-967c-3e49f0f8e660.png)
the parameters are calculated as 
```python
    from torchsummary import summary
    summary(model, (3, 32, 32), device=""cpu"")
    exit(0)
```
Do you know why is there the difference?
Thanks in advance"
"Hi, I noticed that the dropout is placed before convolution layer,
In the original densenet-torch implementation, the order in each block is
BN-->relu-->conv-->dropout
Is there a particular reason for doing so?
```python
    def forward(self, x):
        self._check_drop()
        x = self.norm(x)
        x = self.relu(x)
        if self.dropout_rate > 0:
            x = self.drop(x)
        ### Masked output
        weight = self.conv.weight * self.mask
        return F.conv2d(x, weight, None, self.conv.stride,
                        self.conv.padding, self.conv.dilation, 1)
```"
"Hi, the paper states that the group lasso term is added to the total cost function, with the coefficient of 1e-5 on ImageNet Dataset. Have you compared the model without group lasso term on the same Dataset? In other words, the group lasso term improves the final validation accuracy by how much percent? 

Thanks in advance
"
"Hi, I run CondenseNet-182* using command provided by issue 11 on Cfiar100
```shell
python main.py --model condensenet -b 64 -j 2 cifar100 --epochs 600 --stages 30-30-30 --growth 12-24-48
```
The result of the first run is  19.73%, the result in the second run is 19. 86%
The result in the paper is 18.47% (Table)
I just used all the default arguments in the code provide, do we need to make other changes?

Thanks in advance 
"
"Hi, I notice there is a minor problem in the code
https://github.com/ShichenLiu/CondenseNet/blob/master/main.py#L246
```python
is_best = val_prec1 < best_prec1
best_prec1 = max(val_prec1, best_prec1)
```
should be changed to 
```python
is_best = val_prec1 < best_prec1
best_prec1 = min(val_prec1, best_prec1)
```
Since the val_prec1 returned from the train function is classification error rate
Otherwise, the trainer will save results in every epoch

should also change best_prec1 = 100 in the begin"
"Hi, I have a question on clap weights
https://github.com/ShichenLiu/CondenseNet/blob/master/layers.py#L125
```python
weight = weight.sum(0).clamp(min=1e-6).sqrt()
```
I don't understand the clamp function here. I tried to train condensenet-86 on cifar10 . with and without clamp functions
with clamp:  error rate = 95.06 
without clamp: error rate = 94.96

Thanks in advance"
"Hi, I have one question on function dropping in layers.py.
I don't understand why learned group convolution still needs the shuffling operation? 
```python
        weight = weight.view(d_out, self.groups, self.in_channels)
        weight = weight.transpose(0, 1).contiguous()
        weight = weight.view(self.out_channels, self.in_channels)
```
https://github.com/ShichenLiu/CondenseNet/blob/master/layers.py#L78

I notice there is a shuffle operation mentioned in 4.1's first graph:
""we permute the output channels of the first 1x1_conv learned group convolution layer, 
such that the features generated by each of its groups are evenly used by all the groups of 
the subsequent 3x3 group convolutional layer""
However, this operation aims to shuffle feature maps, not convolutional kernels.

Can you explain a little bit?
Thanks in advance"
"Hi, thanks for your work, I have one question on the function CondensingConv from layers.py
https://github.com/ShichenLiu/CondenseNet/blob/master/layers.py#165
```python
self.in_channels = model.conv.in_channels * model.groups // model.condense_factor
```
The input channel in a given convolutional layer in the paper is floor(R/C)
why is it different here?

Thanks in advance"
"Hi, 
I am reproducing your work in tensorflow, but I found that dropping during training has taken a lot of time. I would like to ask if you have encountered such a problem. What do you think might be the reason?"
"Thanks for sharing your work. I am working with Densenet in the non-mobile environment (such as supper computer, self-driving...) for classification task. I am wondering about Condensenet and DenseNet performance in accuracy. Do you think which network should I used in term of accuracy, densenet or condensenet? Thanks for your advice"
"Hi, this question might be trivial but what is the exact value for `group_lasso_lambda` that you are using? Is it `1e-5` according to the paper? Thanks!"
Would you please share the pretrained model in Baidu disk or Google Drive? I could not access Drop Box.
"Hi this is not an issue. I am trying to reproduce condenseNet in tensorflow. But in which part of the code is the implementation of **we connect input layers to all subsequent layers in the network, even if these layers are located in different dense blocks** as mentioned in the paper available. I can see only one place inside the dense layers where **torch.cat** function is used where it connects the inputs inside one dense block. Thanks in advance. "
"See: https://github.com/ShichenLiu/CondenseNet/blob/3b4398ed1987f6f7c891d81a470578dcc5c5562c/layers.py#L88

Weird stuff in the Pytorch API:
```
self._mask[i::self.groups, d, :, :].fill_(0)
```
... does not fill in place. So you must do:
```
self._mask[i::self.groups, d, :, :] = self._mask[i::self.groups, d, :, :].fill_(0)
```
https://github.com/pytorch/pytorch/issues/2599#issuecomment-326775742
"
"A question about the _dropping function in the LearnedGroupConv class.
In 88 line: self._mask[i::self.groups, d, :, :].fill_(0)
Why the first dimension of the mask is ""**i::self.groups**""?
I thought it should be ""i * d_out:(i + 1) * d_out"".
Thank you!
"
"Hi,
The paper mentions CondenseNet{light-160*, 182*, light-94, 84} for CIFAR, though is not clear about the details of the architecture. Could you share the architectures and how those results can be reproduced?"
"Hi, 

I am facing the following problem when I attempt to train the network with: 

```
python main.py --model condensenet -b 256 -j 26 person-reid/market1501 --stages 4-6-8-10-8 --growth 8-16-32-64-128 --gpu 0,1,2,3 --savedir person-reid/results_market1501-24kgen --resume
```

```
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THCUNN/ClassNLLCriterion.cu:57: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.
THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.c line=32 error=59 : device-side assert triggered
Traceback (most recent call last):
  File ""main.py"", line 480, in <module>
    main()
  File ""main.py"", line 239, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File ""main.py"", line 314, in train
    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
  File ""main.py"", line 474, in accuracy
    correct_k = correct[:k].view(-1).float().sum(0)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/tensor.py"", line 43, in float
    return self.type(type(self).__module__ + '.FloatTensor')
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/cuda/__init__.py"", line 278, in type
    return super(_CudaBase, self).type(*args, **kwargs)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/_utils.py"", line 35, in _type
    return new_type(self.size()).copy_(self, async)
RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/THCTensorCopy.cu:204
terminate called without an active exception
THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/THCTensorCopy.cu line=204 error=59 : device-side assert triggered

[...]

```

The assertion error also occurs in the line 362:

```
Traceback (most recent call last):
  File ""main.py"", line 480, in <module>
    main()
  File ""main.py"", line 242, in main
    val_prec1, val_prec5 = validate(val_loader, model, criterion)
  File ""main.py"", line 362, in validate
    losses.update(loss.data[0], input.size(0))
RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.c:32
THCudaCheckWarn FAIL file=/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/THCStream.cpp line=50 error=29 : driver shutting down
THCudaCheckWarn FAIL file=/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/THCStream.cpp line=50 error=29 : driver shutting down
```

Training and testing images are 64x128, and I also tried by resizing only the training images to 256x256. 

It seems to be caused by an inconsistency with the number of classes that I am trying to figure out. In the evaluation, it might occur that there are no samples for some of the test classes, which I noticed it can be problematic for your network if the directory classes do not match properly. A successful solution I have found for this in another dataset I am working is to create the same set of class-directories with exactly the same name both in training `train` and testing `val` partitions, leaving empty those class-directories where there are no samples for that class in testing. In this dataset, however, I get that error which is a bit confusing to me.

Another question I have is whether your network can be used to classify images of classes that exist in the test partition but not in the train partition. For instance, in a dataset where half of the classes are used for training and the other half for testing.

I would appreciate any comment if you have any clue about what might be causing that error and the last question about the classes.

Thanks a lot"
"Hi,
Cool ideas in regard to condense net!

I seem to have an issue running the pretrained network 'converted_condensenet_8.pth.tar'. Running the command:

`python main.py --model condensenet_converted -b 32 -j 20 ~/imagenet/ --stages 4-6-8-10-8 --growth 8-16-32-64-128 --gpu 1 --resume --evaluate-from ../converted_condensenet_8.pth.tar`

Results in an error:

Traceback (most recent call last):

```
  File ""main.py"", line 479, in <module>
    main()
  File ""main.py"", line 168, in main
    model.load_state_dict(state_dict)
  File ""/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py"", line 487, in load_state_dict
    .format(name, own_state[name].size(), param.size()))
RuntimeError: While copying the parameter named module.features.denseblock_1.denselayer_1.conv_1.conv.weight, whose dimensions in the model are torch.Size([32, 4, 1, 1]) and whose dimensions in the checkpoint are torch.Size([32, 2, 1, 1]).
```
It appears that perhaps the parameters for 'stages' or 'growth' may not be set correctly? Please let me know if you have any insights!"
"Dear @ShichenLiu ,
  I did not found any shuffle layer related stuff in models.condensenet, which use layers.LearnedGroupConv as LGC. However, the paper says we should use it clearly. Is it a mismatch? 
  Thanks"
"Hi @gaohuang  and @ShichenLiu , 

Thank you for great work. I have the following concerns when I try running your code and read your paper:
- Condensation criterion: In the paper, you use L1-norm value of weights within the same group to find column indices for pruning weights with small values. I saw these indices were also applied for the other groups in the code self._mask[i::self.groups, d, :, :].fill_(0). 
- Have you tested learned group convolution for larger kernel filters (3x3)?  If yes, how 's about the efficiency?
- In the code, why do you shuffle weights for group lasso loss?
- Why do you drop 50% input channels by CondensingLinear(child, 0.5) for converting models?

Thanks,
Hai
"
"Hi, 

I am attempting to reproduce your code for the CondenseNet for training a dataset of 7 classes, and approximately between 100K - 150K training images splitted (non-equally) for those clases. My images consist of bounding boxes of different sizes. For that, first I'm using a similar setting you use to train the ImageNet, pointing to my dataset and preparing the class folders to find the paths properly. I resized all images to 256x256 as you did in your paper. Therefore, this is the command line I use for training the new dataset:

``
python main.py --model condensenet -b 256 -j 28 lima_train --stages 4-6-8-10-8 --growth 8-16-32-64-128 --gpu 0 --resume
``

where `lima_train` is a link file pointing to the folder containing all training data splitted in class subfolders as required.

I'm using a datacenter whose GPU nodes use NVIDIA Tesla P100 of 16 GB each, and CUDA 8 with cuDNN. In this sense, I presume the training should not be a problem. I understand that a GPU of 16GB or even 8GB should be enough to train this network, shouldn't be? However, I'm getting the out of memory problem shown below. I modified the parameters to reduce the batch size to 64 and the number of workers according to the machine. Probably I am missing some step or I should modify the command line according to the settings of my data.

I would appreciate any feedback. 

Thanks in advance and congratulations for this work.

``
THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory
Traceback (most recent call last):
  File ""main.py"", line 479, in <module>
    main()
  File ""main.py"", line 239, in main
    train(train_loader, model, criterion, optimizer, epoch)
  File ""main.py"", line 303, in train
    output = model(input_var, progress)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py"", line 58, in forward
    return self.module(*inputs[0], **kwargs[0])
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/CondenseNet/models/condensenet.py"", line 127, in forward
    features = self.features(x)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 67, in forward
    input = module(input)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 67, in forward
    input = module(input)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/CondenseNet/models/condensenet.py"", line 33, in forward
    x = self.conv_1(x)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/CondenseNet/layers.py"", line 42, in forward
    x = self.norm(x)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 224, in __call__
    result = self.forward(*input, **kwargs)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py"", line 37, in forward
    self.training, self.momentum, self.eps)
  File ""/mnt/storage/home/vp17941/.conda/envs/condensenet/lib/python3.6/site-packages/torch/nn/functional.py"", line 639, in batch_norm
    return f(input, weight, bias)
RuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCStorage.cu:66
srun: error: gpu09: task 0: Exited with exit code 1
``"
"How did you test speed on ARM? Using Caffe2, pytorch compiled for ARM?"
""
"Hi. Thanks for the codes and the detailed instruction.

I implemented sparse convolution into my encoder:
```python
with tf.variable_scope('featureEncoder'):
	auxiShape = (self.inputShape[0], self.inputShape[1], self.inputShape[2], 7)
	featureShape = (self.inputShape[0], self.inputShape[1], self.inputShape[2], 32)
	blockSize = 8
	blockStride = (8,8)
	blockOffset = (0,0)
	blockCount = (self.divup(self.inputShape[1], blockStride[0]), self.divup(self.inputShape[2], blockStride[1]))
	inBlockParams = { ""dynamic_bsize"": (blockSize, blockSize), ""dynamic_boffset"": blockOffset, ""dynamic_bstride"": blockStride }
	outBlockParams = { ""dynamic_bsize"": (blockSize, blockSize), ""dynamic_boffset"": blockOffset, ""dynamic_bstride"": blockStride }
	
	if not self.training:
		indices = sbnet_module.reduce_mask(self.mask, blockCount, tol=0.1, **inBlockParams)
	
		# stack active overlapping tiles to batch dimension
		stack = sbnet_module.sparse_gather(
			auxi, indices.bin_counts, indices.active_block_indices, transpose=False, **inBlockParams)
	else:
		stack = auxi
	# perform dense convolution on a sparse stack of tiles
	stack = self.conv_layer2(stack, 7, 32, name='1')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='2')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='3')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='4')
	stack = tf.nn.leaky_relu(stack)
	stack = self.conv_layer2(stack, 32,32, name='5')
	stack = tf.nn.leaky_relu(stack)

	# write/scatter the tiles back on top of original tensor
	# note that the output tensor is reduced by 1 on each side due to 'VALID' convolution
	if not self.training:
		feature=sbnet_module.sparse_scatter(
			stack, indices.bin_counts, indices.active_block_indices,
			self.lastFeature, transpose=False, add=False, atomic=False, **outBlockParams)
		feature.set_shape(featureShape)
	else:
		feature=stack
```

`self.training` is set `False` when training and `True` when testing. Variable `mask` is generated outside the network and fed in via `tf.placeholder`. So does `self.lastFeature`.

I tried to measure the inference time with timeline:
```python
feed_dict = {model.source: src, model.target: tgt, model.batch_size:src_hdr.shape[0], model.mask:Mask, model.feature:Feature}
denoised_1_bd, Feature = sess.run([model.fake_image, model.feature], feed_dict, options=run_options, run_metadata=run_metadata)
tl = timeline.Timeline(run_metadata.step_stats)
ctf = tl.generate_chrome_trace_format(show_memory=True)
with open(os.path.join(errorlog_dir, 'timeline.json'),'w') as wd:
	wd.write(ctf)
```

![timeline](https://user-images.githubusercontent.com/28486541/127764964-e58c3a77-afdc-49ba-831d-d2d4f2ee0edc.png)

However, I can't find time records of layers under 'featureEncoder'. And there are two bars captioned unknown, the second of which is strangely long. Some Pooling and LeakyRelu‘s time is also strange, costing nearly 2ms.

![unknown](https://user-images.githubusercontent.com/28486541/127765056-52b29203-e8ef-40d5-8809-5bd9684499c5.png)

I wonder how I can get the proper time measurement. Thanks.

**My Environment**
TensorFlow Version: 1.15.0
Operating System: Ubuntu 16.04
Python Version: 3.6.13
CUDA Version: 10.0
CUDNN Version: 7.6.4
GPU Type: RTX 2080ti
Nvidia Driver Version: 460.67"
"When I run sample.py, I get the following error:


Traceback (most recent call last):
  File ""sample.py"", line 61, in <module>
    indices = sbnet_module.reduce_mask(mask, blockCount, tol=0.5, **inBlockParams)
TypeError: reduce_mask() got an unexpected keyword argument 'bsize'


This gets fixed when I rename bsize, boffset and bstride to dynamic_bsize, dynamic_boffset and dynamic_bstride in the inBlockParams and outBlockParams."
"You have an error with TF 1.11.0. It's impossible to build the library.

![image](https://user-images.githubusercontent.com/10923599/46665628-18d2c300-cbcd-11e8-8261-d9651fb61aea.png)

I manage to build it with TF 1.8.0 / 1.9.0 / 1.10.0 but with TF 1.11.0 it's broken"
"Hi, I just want to know is there any pytorch version operation of sbnet?"
"I was able to see the claimed speedup in terms of inference times, but there is a huge loss in terms of accuracy. Please see a simple MNIST example where I artificially created the images and masks. I am seeing a precision of about 11% which is as good as random guessing with a 10 class problem. I also tried to train the algorithm longer, but the precision saturates at about 11%. Without using SbNet you can get to a precision of ~80% after a few epochs. 


[MNIST_sbnet_repr.py.tar.gz](https://github.com/uber/sbnet/files/2337932/MNIST_sbnet_repr.py.tar.gz)
"
"`import sys
import numpy as np
import tensorflow as tf
sys.path.insert(0, 'sbnet/sbnet_tensorflow/benchmark')
from sparse_conv_lib import convert_mask_to_indices_custom, sparse_conv2d_custom, \
    calc_block_params, sparse_conv2d, convert_mask_to_indices

size = [1, 704, 800, 6]
grid = (np.random.rand(*size) > 0.95).astype(np.float32)

block_params = calc_block_params(in_size=size,
                                 bsize=[1, 3, 3, 1],
                                 ksize=[3, 3, 1, 1],
                                 strides=[1, 1, 1, 1],
                                 padding='SAME')
with tf.Session() as sess:

    x = tf.placeholder(tf.float32, size)
    mask = x

    w = tf.constant(np.ones(shape=[3, 3, 6, 64]), dtype=tf.float32)

    indices = convert_mask_to_indices_custom(mask, block_params, tol=0.1)

    y_dense = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')
    y_sparse_custom = sparse_conv2d_custom(x, w,
                                           indices,
                                           block_params,
                                           [1, 1, 1, 1], transpose=True)
    # import pdb;pdb.set_trace();
    print(""Dense:"")
    a = sess.run([y_dense], feed_dict={x: grid})
    print(np.array(a).shape)

    print(""Sparse (custom):"")
    b = sess.run([y_sparse_custom], feed_dict={x: grid})
    print(np.array(b).shape)
`

Outputs:
Dense:
(1, 1, 704, 800, 64)
Sparse (custom):
(1, 1, 704, 800, 6)
"
"TensorFlow installed from (source or binary): pip (conda env)
TensorFlow version (use command below): 1.8.0
Python version: 2.7 
GCC/Compiler version (if compiling from source):5.4.0
CUDA/cuDNN version:9/7.1.2
GPU model and memory: GTX1060 6G
Exact command to reproduce: make test

-------------------------------------------------------------------------
cd ../benchmark && bash run_all_unittests.bash # unit tests
sparse_res_block_tests (unittest.loader.ModuleImportFailure) ... ERROR
reduce_mask_tests (unittest.loader.ModuleImportFailure) ... ERROR
sparse_conv_tests (unittest.loader.ModuleImportFailure) ... ERROR
sparse_scatter_tests (unittest.loader.ModuleImportFailure) ... ERROR
sparse_gather_tests (unittest.loader.ModuleImportFailure) ... ERROR
test_calc_out_size (tf_conv_dims_tests.CalcOutSizeDeconvTests) ... ok
test_session (tf_conv_dims_tests.CalcOutSizeDeconvTests)
Returns a TensorFlow Session for use in executing tests. ... ok
test_calc_out_size (tf_conv_dims_tests.CalcOutSizeTests) ... ok
test_session (tf_conv_dims_tests.CalcOutSizeTests)
Returns a TensorFlow Session for use in executing tests. ... ok
test_calc_padding (tf_conv_dims_tests.CalcPaddingTests) ... ok
test_calc_padding_err_ksize_list (tf_conv_dims_tests.CalcPaddingTests) ... ok
test_calc_padding_err_strides_list (tf_conv_dims_tests.CalcPaddingTests) ... ok
test_calc_padding_err_strides_tensor (tf_conv_dims_tests.CalcPaddingTests) ... ok
test_calc_padding_stride (tf_conv_dims_tests.CalcPaddingTests) ... ok
test_calc_padding_valid (tf_conv_dims_tests.CalcPaddingTests) ... ok
test_session (tf_conv_dims_tests.CalcPaddingTests)
Returns a TensorFlow Session for use in executing tests. ... ok

======================================================================
ERROR: sparse_res_block_tests (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: sparse_res_block_tests
Traceback (most recent call last):
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 32, in <module>
    from sparse_conv_lib import _get_offset_array
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 74, in <module>
    sbnet_module = tf.load_op_library('../sbnet_ops/libsbnet.so')
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
NotFoundError: ../sbnet_ops/libsbnet.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv


======================================================================
ERROR: reduce_mask_tests (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: reduce_mask_tests
Traceback (most recent call last):
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/reduce_mask_tests.py"", line 25, in <module>
    from sparse_conv_lib import convert_mask_to_indices, convert_mask_to_indices_custom
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 74, in <module>
    sbnet_module = tf.load_op_library('../sbnet_ops/libsbnet.so')
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
NotFoundError: ../sbnet_ops/libsbnet.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv


======================================================================
ERROR: sparse_conv_tests (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: sparse_conv_tests
Traceback (most recent call last):
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_conv_tests.py"", line 29, in <module>
    from sparse_conv_lib import _get_offset_array
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 74, in <module>
    sbnet_module = tf.load_op_library('../sbnet_ops/libsbnet.so')
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
NotFoundError: ../sbnet_ops/libsbnet.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv


======================================================================
ERROR: sparse_scatter_tests (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: sparse_scatter_tests
Traceback (most recent call last):
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_scatter_tests.py"", line 26, in <module>
    from sparse_conv_lib import sbnet_module
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 74, in <module>
    sbnet_module = tf.load_op_library('../sbnet_ops/libsbnet.so')
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
NotFoundError: ../sbnet_ops/libsbnet.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv


======================================================================
ERROR: sparse_gather_tests (unittest.loader.ModuleImportFailure)
----------------------------------------------------------------------
ImportError: Failed to import test module: sparse_gather_tests
Traceback (most recent call last):
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 254, in _find_tests
    module = self._get_module_from_name(name)
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/unittest/loader.py"", line 232, in _get_module_from_name
    __import__(name)
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_gather_tests.py"", line 26, in <module>
    from sparse_conv_lib import convert_mask_to_block_indices, convert_mask_to_indices_custom
  File ""/home/jnghhk/PycharmProjects/py2/sbnet/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 74, in <module>
    sbnet_module = tf.load_op_library('../sbnet_ops/libsbnet.so')
  File ""/home/jnghhk/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py"", line 56, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
NotFoundError: ../sbnet_ops/libsbnet.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv


----------------------------------------------------------------------
Ran 16 tests in 4.775s

FAILED (errors=5)
Makefile:14: recipe for target 'test' failed
make: *** [test] Error 1
"
"I am getting the following error using a very small block size(1-7), for block size everything works fine.
`GPUassert: invalid configuration argument reduce_mask.cu 87`


"
"Trying to reproduce the paper's result with the recommended sparse_scatter_var method, but I am getting an error which says the gradients are not registered
`LookupError: No gradient defined for operation 'network/SparseScatterVar' (op type: SparseScatterVar)
`"
"Why was ABI flag set to 0 in Makefile?
`ABI=-D_GLIBCXX_USE_CXX11_ABI=0`"
"I am trying to use sparse_scatter_var with multiple CONV layers chained together. After the first sparse scatter operation, I am unable to cast a tf.Tensor to a Variable, which throws me a ValueError
`ValueError: Input 'ybase' passed float expected ref type while building NodeDef 'Variable_4/SparseScatterVar_Variable_4_0' using Op<name=SparseScatterVar; signature=x:T, bin_counts:int32, active_block_indices:int16, ybase:Ref(T), dynamic_bsize:int32, dynamic_bstride:int32, dynamic_boffset:int32 -> y:Ref(T); attr=T:type,allowed=[DT_FLOAT]; attr=add:bool; attr=atomic:bool,default=false; attr=transpose:bool,default=false>
`"
"The description says using mutable tensors(tf.Variable) would be more efficient because of in-place copy, but the perf degrades using sparse_scatter_var as opposed to sparse_scatter in some of the scenarios with the difference in input size to the Sbnet block. What is the tradeoff for using it ?"
"How do I call the functions from C++ pipeline, do I still need to load the .so using ```TF_LoadLibrary```? Any use cases would be useful."
"Hello,

Nice paper! Unfortunately, I am unable to reproduce your speed-ups.

This is what I do:

```
from sparse_conv_lib import convert_mask_to_indices_custom, sparse_conv2d_custom, \
    calc_block_params, sparse_conv2d, convert_mask_to_indices

size = [1, 1024, 1024, 1]
grid = (np.random.rand(*size) > 0.95).astype(np.float32)

block_params = calc_block_params(in_size=size,
                                 bsize=[1, 3, 3, 1],
                                 ksize=[3, 3, 1, 1],
                                 strides=[1, 1, 1, 1],
                                 padding='SAME')
with tf.Session() as sess:

    x = tf.placeholder(tf.float32, size)
    mask = x
    
    w = tf.constant(np.ones(shape=[3, 3, 1, 64]), dtype=tf.float32)

    indices = convert_mask_to_indices_custom(mask, block_params, tol=0.1)
    
    y_dense = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')
    y_sparse_custom = sparse_conv2d_custom(x, w,
                                           indices, 
                                           block_params,
                                           [1, 1, 1, 1], transpose=True)
    
    print(""Dense:"")
    %timeit -n10 sess.run([y_dense], feed_dict={x: grid})
    
    print(""Sparse (custom):"")
    %timeit -n10 sess.run([y_sparse_custom], feed_dict={x: grid})
```
and it seems the sparse version, despite ~95% sparsity, is 10x slower!?
```
Dense:
46.5 ms ± 19.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
Sparse (custom):
400 ms ± 29.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

I am pretty sure there is something I am misunderstanding from your code. Could you clarify?
I tried looking into your benchmark scripts, I'd try to isolate a single layer first...

Thanks!"
"File ""/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 123, in _sparse_scatter_grad
    dout_dx = sbnet_module.sparse_gather(
AttributeError: module 'f21c708d1ddc75dcce283dd13fe531f7' has no attribute 'sparse_gather'
"
"======================================================================
ERROR: test_sparse_resblock_gradients (sparse_res_block_tests.SparseResBlockGradientTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/dhingratul/OneDrive/Projects/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 273, in test_sparse_resblock_gradients
    xval, mask, bsize, strides, padding, data_format='NHWC', dynamic_size=dynamic_size)
  File ""/home/dhingratul/OneDrive/Projects/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 211, in _test_sparse_resblock_gradients
    py_inds = sess.run([tf_ind])
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 900, in run
    run_metadata_ptr)
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 427, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 245, in for_fetch
    return _ListFetchMapper(fetch)
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 352, in __init__
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 245, in for_fetch
    return _ListFetchMapper(fetch)
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 352, in __init__
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 253, in for_fetch
    return _ElementFetchMapper(fetches, contraction_fn)
  File ""/home/dhingratul/.virtualenvs/sbnet/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 289, in __init__
    'Tensor. (%s)' % (fetch, str(e)))
ValueError: Fetch argument <tf.Tensor 'ReduceMask_1:0' shape=<unknown> dtype=int16> cannot be interpreted as a Tensor. (Tensor Tensor(""ReduceMask_1:0"", dtype=int16) is not an element of this graph.)

----------------------------------------------------------------------
Ran 40 tests in 389.501s

FAILED (errors=1)
Makefile:14: recipe for target 'test' failed
make: *** [test] Error 1"
"I want to use the model on KITTI LiDAR dataset.
But I only find the test and implementation codes for operators.  
I cannot find the model definition. So I'd like to know  whether the model is open-soureced."
"Hi,
Can I use the code with variable input size?
As we can see, the functions below use in_size as the first input parameter:
`def calc_block_params(in_size, bsize, ksize, strides, padding, static=True):`
`def calc_block_params_res_block(in_size, bsize, ksize_list, strides, padding):`

I'm using FCN so my network can handle with variable input size, but the function above require explicit input size.

Thanks for your help,
Rafael.  "
"I got the following error when I run `make test`, I am using GP100 and tensorflow1.2


======================================================================
ERROR: test_sparse_resblock_gradients (sparse_res_block_tests.SparseResBlockGradientTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 267, in test_sparse_resblock_gradients
    xval, mask, bsize, strides, padding, data_format='NHWC')
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 229, in _test_sparse_resblock_gradients
    yval = y.eval()
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 606, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3928, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 789, in run
    run_metadata_ptr)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
ResourceExhaustedError: OOM when allocating tensor with shape[1008726972,5,5,4]
         [[Node: SparseGather = SparseGather[T=DT_FLOAT, boffset=[-1, -1], bsize=[5, 5], bstride=[3, 3], transpose=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](Const, Variable_1/read/_69, Variable/read/_71)]]
         [[Node: SparseScatter/_73 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_64_SparseScatter"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op u'SparseGather', defined at:
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/runpy.py"", line 174, in _run_module_as_main
    ""__main__"", fname, loader, pkg_name)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/runpy.py"", line 72, in _run_code
    exec code in run_globals
  File ""/h/14/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/__main__.py"", line 12, in <module>
    main(module=None)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/main.py"", line 95, in __init__
    self.runTests()
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/main.py"", line 232, in runTests
    self.result = testRunner.run(self.test)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/runner.py"", line 151, in run
    test(result)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/case.py"", line 393, in __call__
    return self.run(*args, **kwds)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 267, in test_sparse_resblock_gradients
    xval, mask, bsize, strides, padding, data_format='NHWC')
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_res_block_tests.py"", line 221, in _test_sparse_resblock_gradients
    use_var=False)
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 770, in sparse_res_block_bottleneck
    transpose=transpose)
  File ""<string>"", line 39, in sparse_gather
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1008726972,5,5,4]
         [[Node: SparseGather = SparseGather[T=DT_FLOAT, boffset=[-1, -1], bsize=[5, 5], bstride=[3, 3], transpose=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](Const, Variable_1/read/_69, Variable/read/_71)]]
         [[Node: SparseScatter/_73 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_64_SparseScatter"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


======================================================================
FAIL: test_sparse_conv2d_with_mask_same (sparse_conv_tests.SparseConv2DCustomTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_conv_tests.py"", line 460, in test_sparse_conv2d_with_mask_same
    self._test_sparse_conv2d_custom_with_mask(mask, bsize, ksize, strides, padding, y_exp)
  File ""/ais/gobi5/linghuan/sbnet/sbnet_tensorflow/benchmark/sparse_conv_tests.py"", line 435, in _test_sparse_conv2d_custom_with_mask
    np.testing.assert_array_equal(y_act.reshape(y_exp.shape), y_exp)
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 855, in assert_array_equal
    verbose=verbose, header='Arrays are not equal')
  File ""/u/linghuan/anaconda/envs/tf1.2/lib/python2.7/site-packages/numpy/testing/nose_tools/utils.py"", line 779, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Arrays are not equal

(mismatch 52.0%)
 x: array([[[[1.],
         [1.],
         [1.],...
 y: array([[[[1.],
         [6.],
         [6.],...

----------------------------------------------------------------------
Ran 40 tests in 186.952s

FAILED (failures=1, errors=1)
Makefile:14: recipe for target 'test' failed
make: *** [test] Error 1"
"I have built the yolov2 object detector using sbnet, but it takes too long to compute the result of sbnet_module.reduce_mask.
I need to compute sbnet_module.reduce_mask every frame, because the mask change every frame.

```
# yolov2 with dense convnet
Forwarding 1 inputs ...
Forwarding time = 0.0349409580231 sec

# yolov2 with sbnet (sparsity = 0.92)
Forwarding 1 inputs ...
Fowarding time = 0.0198512077332 sec
 + time(sbnet_module.reduce_mask) = 0.0325801372528 sec
```

When I applied sbnet model on yolov2(darknet) model, 
forwarding time was about 1.7 times faster.  However, it took longer than expected to compute the **reduce_mask results** which are needed to perform sparse_gather and sparse_scatter.


Below is my code to compute reduce_mask for conv1s ~ conv5.
Currently, it takes 0.03 sec to execute this code, but it's too slow considering the forwarding time of the detector (detector forwarding time is almost 0.03 sec).
Is there any fastest way to compute sbnet_module.reduce_mask?

```
# compute block_params for all different size in conv1s ~ conv5
block_params_p0_k3 = calc_block_params([1, 416, 864, None],
                                       [1, 34, 34, 1],
                                       [3, 3, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p1_k3 = calc_block_params([1, 208, 432, None],
                                       [1, 18, 18, 1],
                                       [3, 3, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p2_k3 = calc_block_params([1, 104, 216, None],
                                       [1, 10, 10, 1],
                                       [3, 3, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p2_k1 = calc_block_params([1, 104, 216, None],
                                       [1, 8, 8, 1],
                                       [1, 1, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p3_k3 = calc_block_params([1, 52, 108, None],
                                       [1, 6, 6, 1],
                                       [3, 3, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p3_k1 = calc_block_params([1, 52, 108, None],
                                       [1, 4, 4, 1],
                                       [1, 1, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p4_k3 = calc_block_params([1, 26, 54, None],
                                       [1, 4, 4, 1],
                                       [3, 3, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p4_k1 = calc_block_params([1, 26, 54, None],
                                       [1, 2, 2, 1],
                                       [1, 1, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p5_k3 = calc_block_params([1, 13, 27, None],
                                       [1, 3, 3, 1],
                                       [3, 3, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
block_params_p5_k1 = calc_block_params([1, 13, 27, None],
                                       [1, 1, 1, 1],
                                       [1, 1, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')

# compute random binaray mask depending on bndbox for conv1s ~ conv5
mask_p0 = np.zeros([1, 416, 864], dtype=np.float32)
if bndbox:
    for bbox in bndbox:
        xmin = max(0, int(round(bbox[0] * 416)))
        ymin = max(0, int(round(bbox[1] * 864)))
        xmax = min(int(round(bbox[2] * 416)), 415)
        ymax = min(int(round(bbox[3] * 864)), 863)
        mask_p0[:, xmin:xmax, ymin:ymax] = 1.0

mask_p5 = block_reduce(mask_p0, (1, 32, 32), np.max)
mask_p4 = mask_p5.repeat(2, axis=1).repeat(2, axis=2)
mask_p3 = mask_p4.repeat(2, axis=1).repeat(2, axis=2)
mask_p2 = mask_p3.repeat(2, axis=1).repeat(2, axis=2)
mask_p1 = mask_p2.repeat(2, axis=1).repeat(2, axis=2)
mask_p0 = mask_p1.repeat(2, axis=1).repeat(2, axis=2)

with tf.Graph().as_default():
    mask_p0_tf = tf.constant(mask_p0, dtype=tf.float32)
    mask_p1_tf = tf.constant(mask_p1, dtype=tf.float32)
    mask_p2_tf = tf.constant(mask_p2, dtype=tf.float32)
    mask_p3_tf = tf.constant(mask_p3, dtype=tf.float32)
    mask_p4_tf = tf.constant(mask_p4, dtype=tf.float32)
    mask_p5_tf = tf.constant(mask_p5, dtype=tf.float32)

    # compute sbnet for conv1s ~ conv5
    sbnet_p0_k3 = sbnet_module.reduce_mask(mask_p0_tf,
        tf.constant(block_params_p0_k3.bcount, dtype=tf.int32),
         bsize=block_params_p0_k3.bsize,
         boffset=block_params_p0_k3.boffset,
         bstride=block_params_p0_k3.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p1_k3 = sbnet_module.reduce_mask(mask_p1_tf,
         tf.constant(block_params_p1_k3.bcount, dtype=tf.int32),
         bsize=block_params_p1_k3.bsize,
         boffset=block_params_p1_k3.boffset,
         bstride=block_params_p1_k3.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p2_k3 = sbnet_module.reduce_mask(mask_p2_tf,
         tf.constant(block_params_p2_k3.bcount, dtype=tf.int32),
         bsize=block_params_p2_k3.bsize,
         boffset=block_params_p2_k3.boffset,
         bstride=block_params_p2_k3.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p2_k1 = sbnet_module.reduce_mask(mask_p2_tf,
         tf.constant(block_params_p2_k1.bcount, dtype=tf.int32),
         bsize=block_params_p2_k1.bsize,
         boffset=block_params_p2_k1.boffset,
         bstride=block_params_p2_k1.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p3_k3 = sbnet_module.reduce_mask(mask_p3_tf,
         tf.constant(block_params_p3_k3.bcount, dtype=tf.int32),
         bsize=block_params_p3_k3.bsize,
         boffset=block_params_p3_k3.boffset,
         bstride=block_params_p3_k3.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p3_k1 = sbnet_module.reduce_mask(mask_p3_tf,
         tf.constant(block_params_p3_k1.bcount, dtype=tf.int32),
         bsize=block_params_p3_k1.bsize,
         boffset=block_params_p3_k1.boffset,
         bstride=block_params_p3_k1.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p4_k3 = sbnet_module.reduce_mask(mask_p4_tf,
         tf.constant(block_params_p4_k3.bcount, dtype=tf.int32),
         bsize=block_params_p4_k3.bsize,
         boffset=block_params_p4_k3.boffset,
         bstride=block_params_p4_k3.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p4_k1 = sbnet_module.reduce_mask(mask_p4_tf,
         tf.constant(block_params_p4_k1.bcount, dtype=tf.int32),
         bsize=block_params_p4_k1.bsize,
         boffset=block_params_p4_k1.boffset,
         bstride=block_params_p4_k1.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p5_k3 = sbnet_module.reduce_mask(mask_p5_tf,
         tf.constant(block_params_p5_k3.bcount, dtype=tf.int32),
         bsize=block_params_p5_k3.bsize,
         boffset=block_params_p5_k3.boffset,
         bstride=block_params_p5_k3.bstrides,
         tol=0.0,
         avgpool=True)
    sbnet_p5_k1 = sbnet_module.reduce_mask(mask_p5_tf,
         tf.constant(block_params_p5_k1.bcount, dtype=tf.int32),
         bsize=block_params_p5_k1.bsize,
         boffset=block_params_p5_k1.boffset,
         bstride=block_params_p5_k1.bstrides,
         tol=0.0,
         avgpool=True)

    with tf.Session() as sess:
        ind_val_p0_k3, ind_val_p1_k3, \
        ind_val_p2_k3, ind_val_p2_k1, \
        ind_val_p3_k3, ind_val_p3_k1, \
        ind_val_p4_k3, ind_val_p4_k1, \
        ind_val_p5_k3, ind_val_p5_k1, \
        bin_val_p0_k3, bin_val_p1_k3, \
        bin_val_p2_k3, bin_val_p2_k1, \
        bin_val_p3_k3, bin_val_p3_k1, \
        bin_val_p4_k3, bin_val_p4_k1, \
        bin_val_p5_k3, bin_val_p5_k1 = \
            sess.run([sbnet_p0_k3.active_block_indices,
                      sbnet_p1_k3.active_block_indices,
                      sbnet_p2_k3.active_block_indices,
                      sbnet_p2_k1.active_block_indices,
                      sbnet_p3_k3.active_block_indices,
                      sbnet_p3_k1.active_block_indices,
                      sbnet_p4_k3.active_block_indices,
                      sbnet_p4_k1.active_block_indices,
                      sbnet_p5_k3.active_block_indices,
                      sbnet_p5_k1.active_block_indices,
                      sbnet_p0_k3.bin_counts,
                      sbnet_p1_k3.bin_counts,
                      sbnet_p2_k3.bin_counts,
                      sbnet_p2_k1.bin_counts,
                      sbnet_p3_k3.bin_counts,
                      sbnet_p3_k1.bin_counts,
                      sbnet_p4_k3.bin_counts,
                      sbnet_p4_k1.bin_counts,
                      sbnet_p5_k3.bin_counts,
                      sbnet_p5_k1.bin_counts])

# After that, these values go into feed_dict.
```
"
"#### The main code is creating randomly changing mask indices for every loop using sbnet_module.
```
batch_size = 50
block_params_conv1 = calc_block_params([batch_size, 28, 28, 1],
                                       [1, 5, 5, 1],
                                       [5, 5, 1, 1],
                                       [1, 1, 1, 1],
                                       padding='VALID')
t_check = Timer()

print (""Starting 1st session..."")
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(2000):
        t_check.tic()

        mask_conv1 = generate_random_mask([batch_size, 28, 28, 1], 0.90)
        a_conv1 = tf.constant(mask_conv1, dtype=tf.float32)
        b_conv1 = sbnet_module.reduce_mask(a_conv1,
                      tf.constant(block_params_conv1.bcount, dtype=tf.int32),
                      bsize=block_params_conv1.bsize,
                      boffset=block_params_conv1.boffset,
                      bstride=block_params_conv1.bstrides,
                      tol=0.0,
                      avgpool=True)
        ind_val_conv1, bin_val_conv1 = sess.run([b_conv1.active_block_indices,
                                                 b_conv1.bin_counts])

        if i % 100 == 0:
            time_check = t_check.toc()
            print('step %d  \t= %f (sec)' % (i, time_check))
```
#### And the result was getting slower as the session run multiple times.
```
Starting 1st session...
step    0  = 0.006941 (sec)
step  100  = 0.008982 (sec)
step  200  = 0.012545 (sec)
step  300  = 0.016614 (sec)
step  400  = 0.018152 (sec)
step  500  = 0.023373 (sec)
step  600  = 0.026576 (sec)
step  700  = 0.028291 (sec)
step  800  = 0.031587 (sec)
step  900  = 0.037221 (sec)
step 1000  = 0.043062 (sec)
step 1100  = 0.048337 (sec)
step 1200  = 0.055366 (sec)
step 1300  = 0.060677 (sec)
step 1400  = 0.058936 (sec)
step 1500  = 0.072439 (sec)
step 1600  = 0.068025 (sec)
step 1700  = 0.073672 (sec)
step 1800  = 0.077006 (sec)
step 1900  = 0.083827 (sec)
```

#### So, my question is
I want to **run sbnet_module with a randomly changing mask for every detection time**
just like SBNet + Predicted Mask experiment, it seems to me that run sbnet_module.reduce_mask every time while detecting. 
However, my result means that whenever using sbnet_module.reduce_mask, tensorflow graph grows so that speed has slowed down when detecting with sbnet_module.reduce_mask.
How can I use sbnet_module.reduce_mask without loss of time in this situation.
"
"If you can provide me with 3D LiDAR Dataset, I would be greatly appreciated."
"**I have `make` and `make test` passed.**

**When I run this command:**
`cd sbnet_tensorflow/benchmarks && ./run_all_behchmarks.bash`

**Error:**
TypeError: Input 'active_block_indices' of 'SparseGather' Op has type int64 that does not match expected type of int32.
TypeError: Input 'active_block_indices' of 'SparseGather' Op has type int64 that does not match expected type of int32.
  File ""/home/znjs/sbnet-master/sbnet_tensorflow/benchmark/sparse_conv_lib.py"", line 542, in sparse_conv2d_custom
    transpose=transpose)
  File ""<string>"", line 39, in sparse_gather
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 513, in apply_op
    (prefix, dtypes.as_dtype(input_arg.type).name))
TypeError: Input 'active_block_indices' of 'SparseGather' Op has type int64 that does not match expected type of int32.
TypeError: Input 'active_block_indices' of 'SparseGather' Op has type int64 that does not match expected type of int32.

**Ubuntu 14.04** 
**GTX 1050Ti**
**tensorflow 1.2.1**
**Python 2.7.6**
**CUDA 8.0**
**cuDNN 5.1**"
"I try to train model with MNIST dataset using sbnet_module, but 

#### LookupError: No gradient defined for operation 'conv2/SparseScatterVar' (op type: SparseScatterVar)
How can I update gradient using sbnet_module?
I don't know how to use @ops.RegsiterGradient(""SparseGather"") and @ops.RegsiterGradient(""SparseScatter"")
Below is a sbnet_module conv2d function for training.


```
from sparse_conv_lib import calc_block_params, convert_mask_to_indices_custom

def sparse_conv2d(x, W, hw):
    xsize_ = [batch, hw, hw, 1]

    mask = generate_top_left_mask(xsize_, 0.90)
    block_params = calc_block_params(xsize_,
                                     bsize_,
                                     ksize_,
                                     strides,
                                     padding='VALID')
    ind = convert_mask_to_indices_custom(mask, block_params, 0.0, True)
    x_ = tf.Variable(x)
    p = sbnet_module.sparse_gather(
        x_, 
        ind.bin_counts,
        ind.active_block_indices,
        bsize=block_params.bsize,
        boffset=block_params.boffset,
        bstride=block_params.bstrides,
        transpose=True)
    q = tf.nn.conv2d(p, W, strides, 'VALID', data_format='NCHW', use_cudnn_on_gpu=True)
    y = sbnet_module.sparse_scatter_var(
        q,
        ind.bin_counts,
        ind.active_block_indices,
        x_,
        bsize=block_params.bsize_out,
        boffset=[0, 0],
        bstride=block_params.bstrides,
        add=False,
        transpose=True,
        atomic=False)
    return y
```

"
"Hi, I have a compilation error. It complains about cuda_jetbrains.h, I couldn't find anything about such a file online. How to resolve this?"
"I want to transfer the sparse Kitti depth image into a dense one. Could someone give me some guide?

"
"Does the data you used in KITTI contains both left and right camera, or just left camera?"
"Hi, 

I know this question may sound stupid. But I found for nyu dataset, the labeled image have white boundary around, and in the ground truth, those pixels have valid depth value (as image attached).   

I wonder what is the conventional way to handle it in the test time. I read a few codes, but it seems no one tries to handle it. So is the network supposed to predict the value for these pixels as well? 

I am very new in this field and feel it is a little bit weird to ask network to predict value in these meaningless region. Could any one tell me the right way to do it? Crop or mask out the region? 

![image](https://user-images.githubusercontent.com/34109262/87692706-07cbc400-c75a-11ea-87da-789cead248c0.png)
"
"Hi,

I wonder how the depth maps are interpolated for training? there doesn't seem to be any details of this in the paper

Thanks"
"Hi, dear all,
     when I tried to train the net, i met a problem about the gradient  calculation with Equation(4). I tried to write the LossLayer, but I don't know how to write the backward. which layer's output is the x(w,h)  in the equation? 

Thanks,"
"Thanks for make the code open-sourced.
When I run 
```
python3 -m pudb demo_nyuv2.py --filename=./data/NYUV2/demo_01.png --outputroot=./result/NYUV2
```
I meet this error:
```
WARNING: Logging before InitGoogleLogging() is written to STDERR
W1117 10:42:23.021567 29694 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W1117 10:42:23.021591 29694 _caffe.cpp:140] Use this instead (with the named ""weights"" parameter):
W1117 10:42:23.021595 29694 _caffe.cpp:142] Net('models/NYUV2/deploy.prototxt', 1, weights='models/NYUV2/cvpr_nyuv2.caffemodel')
[libprotobuf ERROR /var/tmp/portage/dev-libs/protobuf-3.8.0/work/protobuf-3.8.0/src/google/protobuf/text_format.cc:317] Error parsing text-format caffe.NetParameter: 52:12: Message type ""caffe.LayerParameter"" has no field named ""bn_param"".
F1117 10:42:23.022581 29694 upgrade_proto.cpp:90] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: models/NYUV2/deploy.prototxt
*** Check failure stack trace: ***
Aborted (core dumped)

```
I need some hints on the error
```
Message type ""caffe.LayerParameter"" has no field named ""bn_param""
```
Could you please give some hints?"
"Thank you for your contribution first of all. 

After setting up the pycaffe of the repo and downloading the model I ran ""demo_kitti.py"" and ""demo_nyuv2"", which saves the inference results as png.

For NYUV2, I can recreate ""result/NYUV2/demo_01_pred.png"" exactly.
For KITTI however, the file ""result/KITTI/demo_01_pred.png"" is slightly different (see image below).

Can it be that the KITTI checkpoint model is not up to date?

P.s.: On a related note: When performing inference on the 697 images from the Eigen split using Garg crop and evaluating, I get an abs-rel-error of 0.098 instead of 0.072 as reported in the paper. I believe, both symptoms may have the same root cause?


![_comparison](https://user-images.githubusercontent.com/4158176/65526270-24f48500-def1-11e9-93da-6c9c6262be13.png)
"
I want to know the code only generate the depth image from RGB or also generate the point cloud from RGB depth?
"Hi,
I'm interested in converting the depth image in point cloud with RGP Infos.
I found this script on the internet:

https://svncvpr.in.tum.de/cvpr-ros-pkg/trunk/rgbd_benchmark/rgbd_benchmark_tools/src/rgbd_benchmark_tools/generate_pointcloud.py

but I think I have some informations missing like focal length and scaling factor.

Thanks for your help
"
"I notice that in `kitti_demo.py`, you use 
```
ord_score = ord_score/counts - 1.0
ord_score = (ord_score + 40.0)/25.0
ord_score = np.exp(ord_score)
```
to decode the ordinal regression result (index) to depth in meters. What is the relationship between this equation and the equations in the original paper, which are:
![image](https://user-images.githubusercontent.com/23130908/58078542-963dc780-7be1-11e9-8848-d24783a823ac.png)
![image](https://user-images.githubusercontent.com/23130908/58078574-a786d400-7be1-11e9-8c74-847734d1a43b.png)

How do you merge these two equations into one, and what alpha and beta do you use?"
"In equation 4,

![Screenshot from 2019-05-15 11-07-02](https://user-images.githubusercontent.com/36392866/57743820-97fb1d00-7701-11e9-9c81-40589327a648.png)

what is the meaning of P(w,h)-1 part? why are you subtracting 1?

Thank you in advance"
Please provide some instructions to train.
"Hi,
I just wanted to ask about the KITTI data: when training the model do you use sparse depth data from the LIDAR pointcloud, or do you use interpolated depthmaps as shown in figure 5 as GT?
Thanks!
Daniyar"
"Hi Dear,
Could you tell me what is the license of this software, please?
According to _GitHub's Policy_, all repositories without an explicit license are considered Copyrighted materials. Do the authors intend to make this software free?
Thank you!"
""
"Hi,
Thank you for sharing the inference code of your work.
Can you please implement it in Tensorflow framework?

Thanks"
"Hi, I've read that supervised learning methods tend to overfit on the data they are trained on. For example, if you train on CityScapes but evaluate on KITTI, you get bad results. I wonder how your models perform in this kind of situation(different data at testing)? In the paper, there are only results for models trained and tested on the same dataset.

Thank you very much."
"Since many issues are presented about SID, here, I offer my realization.
The alpha and beta are determined from the ground truth, where you should compute the max and min depth value for one certain dataset. When you fine-tune on a new dataset, don't forget to modify the value.
The K setting is shown in the Exp section 4.2.3, 80 is the best.
Though I haven't reproduced the perfect results on NYU v2 because of the different network architecture, it seems this strategy offers the promotion for accuracy.
I also try the learnable alpha and beta value but it seems it varies a lot for different scenes. When the alpha changes stably, I believe it suffers from overfitting.
Using the statistic value of alpha and beta can directly reduce the error since you will not get a too small or too large value. I think it can be deemed as an interesting trick to reduce the accuracy, but I haven't tried whether changing the alpha or beta to a more free value like 0 and 10 is helpful."
"Hi again,

When I was reading the paper a long time ago, a part of loss function didn't make sense to me. I think there is a potentially important typo there. Please confirm if I am correct or wrong.

In equation (2) the second term should be:
`SIGMA (1- log(P) )` instead of  `SIGMA(log(1-P))`

Let's assume if a true depth is k (in uniform distribution),  you want:
```
P0 = 1
P1 = 1 
.
.
.
P(K-1) = 1
```

and

```
PK = 0
P(K+1) = 0
.
.
PN = 0

```
This means there is a typo because your equation will give you` log(0) for Pi when i>k-1` which will be infinity in LOSS. "
"Hello, in your article you inform that the encoder used has 51M trainable parameters. What about the total number of trainable parameters? Does this value include the ""Dense feature extractor"" + ""Scene Understanding modular"" parameters?"
"Hi, I find that you add a post process to direct network output `ord_score = np.exp((ord_score + 40.0)/25.0)`. I want to know how this come about. 
What is more, do you process the training labels in the inverse way?"
"Hi, I find that you split a kitti image into 4 slices by width in your `demo_kitti.py`, I wonder if you also split the input images when training ?
BTW: could you please offer detail image split files of eigen split? I have confusion in spliting it myself according to section 4.2 in [eigen's paper](http://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network)"
"Hi, 

I was wondering if you could share your evaluation code or tell me which code did you use for evaluation?official kitti evaluation?
Did you use lidar raw data or post processed groundtruth provided by kitti? "
"Hello.

Thanks for your awesome paper and I'd prepared to reproduce your training code.

But I have a question that how to use the SID? It seems like we need to calculate K thresholds by SID in the data loading stage and transmit the thresholds to loss layers. Is right?

Thanks."
"Hi, 
Thank you for sharing the inference code of your work.
Can you release the same code in either tensorflow or pytorch.

Thanks."
What makes your python layer difference from https://github.com/luoyetx/OrdinalRegression ?
""
"Hi, thanks to your awesome work. When I try test demo.py, it shows:

ImportError: No module named ordinal_decode_layer


 I make successfully pycaffe."
"Hi, I just read your paper and codes. I have been wondering what steps you have taken to augment your images. And would it be possible to share how you deal with images while training? "
"When I tried to run the demo_kitti.py, I failed with the error:

```
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0705 09:34:31.505373 33699 _caffe.cpp:135] DEPRECATION WARNING - deprecated use of Python interface
W0705 09:34:31.505487 33699 _caffe.cpp:136] Use this instead (with the named ""weights"" parameter):
W0705 09:34:31.505496 33699 _caffe.cpp:138] Net('./models/KITTI/deploy.prototxt', 1, weights='./models/KITTI/cvpr_kitti.caffemodel')
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 51:12: Message type ""caffe.LayerParameter"" has no field named ""bn_param"".
F0705 09:34:31.509136 33699 upgrade_proto.cpp:88] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: ./models/KITTI/deploy.prototxt
*** Check failure stack trace: ***
Aborted (core dumped)
```

Then I replaced the ""bn_param"" with ""batch_norm_param"" and faced a new problem:

```
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0705 12:38:15.151402 33968 _caffe.cpp:135] DEPRECATION WARNING - deprecated use of Python interface
W0705 12:38:15.151561 33968 _caffe.cpp:136] Use this instead (with the named ""weights"" parameter):
W0705 12:38:15.151607 33968 _caffe.cpp:138] Net('./models/KITTI/deploy.prototxt', 1, weights='./models/KITTI/cvpr_kitti.caffemodel')
[libprotobuf ERROR google/protobuf/text_format.cc:245] Error parsing text-format caffe.NetParameter: 52:18: Message type ""caffe.BatchNormParameter"" has no field named ""slope_filler"".
F0705 12:38:15.156330 33968 upgrade_proto.cpp:88] Check failed: ReadProtoFromTextFile(param_file, param) Failed to parse NetParameter file: ./models/KITTI/deploy.prototxt
*** Check failure stack trace: ***
Aborted (core dumped)

```

I have tried many times and I can't solve it. Could you please provide a solution or an idea? Thanks!"
"I0627 14:09:56.936329  3141 layer_factory.hpp:77] Creating layer decode_ord
ImportError: No module named 'ordinal_decode_layer'
Traceback (most recent call last):
  File ""demo_nyuv2.py"", line 15, in <module>
    net = caffe.Net('models/NYUV2/deploy.prototxt', 'models/NYUV2/cvpr_nyuv2.caffemodel', caffe.TEST)
SystemError: <Boost.Python.function object at 0x1d53a50> returned NULL without setting an error
"
"Hello,

First, congratulations on your results. I'm also working with Monocular Depth Estimation and I have some questions about the metrics used in the Kitti Depth Prediction Benchmark.

1) Does your Network predict depth in meters (m)?

2) If yes, did you change anything for applying the following metrics?

SILog: Scale invariant logarithmic error [log(m)*100]
iRMSE: Root mean squared error of the inverse depth [1/km]

I'm asking because they use these different units: [log(m)*100)] and [1/km]."
"Hey, I noticed that your resnet model is a dilated version. So I wonder what architecture do you use? Do you use this [DRN](https://github.com/fyu/drn)? Or some other architecture?

Thanks"
"Hello! Thanks for your outstanding work! But i have some questions about your paper.
When i read your paper, i wonder why the number of Y is 2K. And what's the double layers in Ordinal regression stand for?
 "
""
"Hi,

thanks for sharing. Does kitti model uses kitti depth dataset for training ? or it uses the Eigen split? or it is the model you used for Robust vision challenge? 
If not is it possible to share Eigen model you evaluated in the paper and also Robust vision challenge pretrained model aswell?

Thanks"
"@JingchunCheng There's no module named Surgery in solve.py, so I'm unable to run training script."
"I can't open the www.dropbox.com . Is there way  to get the model ? thank you
![image](https://user-images.githubusercontent.com/55472563/66796111-43223380-ef39-11e9-9052-2f0309671452.png)
"
"@JingchunCheng following is the error I'm receiving on Running the bash:

<_io.TextIOWrapper name='<stderr>' mode='w' encoding='UTF-8'> loading ../data/DAVIS2016//JPEGImages/480p/blackswan/00000.jpg
<_io.TextIOWrapper name='<stderr>' mode='w' encoding='UTF-8'> box num = 246
F1119 18:42:55.289429  1572 im2col.cu:61] Check failed: error == cudaSuccess (48 vs. 0)  no kernel image is available for execution on the device
*** Check failure stack trace: ***
Aborted (core dumped)"
"Sorry to interrupt u.I just have got my tracking results,the images with bounding boxes ,but how can i test ur ROI SegNet with my tracking results?I hope u could give me some advice.Thank u so much!!!"
"I found that if I wanna use my own data, I have to use the .mat.mat file in the part-tracking folder. So how could I generate this file? Do I need to use the tracker first?"
"hi Jing chun,

So great work and clear code!

I'm running your code but can find the main func.
Can you please tell me how to run the code clearly.

Thanks so much !

best,
Lichao"
"Hi, I have tested the trained model(download_PieAPPv0.1_PT_weights.sh) that you released, but the performance is lower than the result in your paper by about 2% in both PLCC, SRCC, and KRCC. Also, the reimplement is also lower than the result in your paper by about 2%. Can I ask for your testing and training details?

Thanks a lot."
"Even with --gpu_id 0 only CPU is used when using the windows executable, any workaround for that?"
"Sorry for my noob question, but Is possible train this network with my dataset (images from one class) and get same results???"
"![image](https://user-images.githubusercontent.com/73474866/149773334-5fdf02a2-3f72-4c1e-b698-9a23db654056.png)

how can i use numpy in pytorch?"
"Below is the error message:
""size mismatch for ref_score_subtract.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1, 1]).""

Need to manually adjust the size of 'ref_score_subtract.weight'."
Hi! Will the training code be uploaded any time soon? Am excited to try and train with this metric.
"I execute the following command, but cannot download the weights of a pre-trained model.
```bash
bash scripts/download_PieAPPv0.1_PT_weights.sh
```

The weights seems to be missing from https://web.ece.ucsb.edu/~ekta/projects/PieAPPv0.1/weights/PieAPPv0.1.pth."
"Hi!

I recently re-written your PyTorch implementation and added it to the [PyTorch Image Quality](https://github.com/photosynthesis-team/piq) library.

It's interface is simpler, so if anyone needs PieAPP to inference a batch of images and get their scores, feel free to use it.
https://github.com/photosynthesis-team/piq/pull/184"
"This paper proposes a new dataset and the paper is published in 2018. However, today is around 2020. Is the proposed data set still planned for release? It will be a great contribution to this field and please release the dataset."
"InvalidArgumentError (see above for traceback): Computed output size would be negative: -1 [input_size: 0, effective_filter_size: 64, stride: 27]
	 [[Node: ExtractImagePatches_1 = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 64, 64, 1], padding=""VALID"", rates=[1, 1, 1, 1], strides=[1, 27, 27, 1], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_Placeholder_1_0_1)]]"
"I tried to run the python code and I am getting following errors:

**For Tensorflow variant:**

```
[marian@localhost PerceptualImageError]$ python test_PieAPP_TF.py --ref_path imgs/ref.png --A_path imgs/A.png --sampling_mode sparse
  File ""test_PieAPP_TF.py"", line 82
    image_ref_batch: im_Ref
                  ^
SyntaxError: invalid syntax
```

**For PyTorch variant:**

```
[marian@localhost PerceptualImageError]$ python test_PieAPP_PT.py --ref_path imgs/ref.png --A_path imgs/A.png --sampling_mode sparse
downloading dataset
--2018-10-23 22:10:58--  https://web.ece.ucsb.edu/~ekta/projects/PieAPPv0.1//weights/PieAPPv0.1.pth
Resolving web.ece.ucsb.edu (web.ece.ucsb.edu)... 128.111.56.99
Connecting to web.ece.ucsb.edu (web.ece.ucsb.edu)|128.111.56.99|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 273509598 (261M)
Saving to: ‘PieAPPv0.1.pth’

PieAPPv0.1.pth                    100%[============================================================>] 260.84M  1.62MB/s    in 2m 3s

2018-10-23 22:13:02 (2.13 MB/s) - ‘PieAPPv0.1.pth’ saved [273509598/273509598]

PieAPPv0.1.pth not downloaded
```

After manually moved the file PieAPPv0.1.pth to weights folder I got following:

```
[marian@localhost PerceptualImageError]$ mv PieAPPv0.1.pth weights/
[marian@localhost PerceptualImageError]$ python test_PieAPP_PT.py --ref_path imgs/ref.png --A_path imgs/A.png --sampling_mode sparse
Traceback (most recent call last):
  File ""test_PieAPP_PT.py"", line 61, in <module>
    PieAPP_net.load_state_dict(torch.load('weights/PieAPPv0.1.pth'))
  File ""/home/ok/.local/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 719, in load_state_dict
    self.__class__.__name__, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for PieAPP:
        size mismatch for ref_score_subtract.weight: copying a param of torch.Size([1, 1]) from checkpoint, where the shape is torch.Size([1]) in current model.
```

The windows executable works well."
"Hi,

I have been running most of the models and have been consistently getting low accuracies for questions related to 'when'.

Do you have any insights on this or perhaps reasoning to this? Or could it be attributed to the dataset?"
"Hello nice to meet you!

i'm reading about your paper,code and  i don't understand about your  code that how to visualize image like paper??

In paper visualized have highest weight ,how to extract highest weight??

Is correct using ""att_logits"" with visualize??
"
"hello!

**If i want to optimized with mse, What should I change to mse?**
I thought
""imgcomp-cvpr/code/ae_configs/cvpr/base
distortion_to_minimize=ms_ssim
-> distortion_to_minimize=mse"" to optimize to mse.
Is it right???

Thank you."
"hello, I don't figure out that when testing the model , why to use the hardout for the input of the P network .In my opinions ,the hardout  and the symbols are equal in a sense ."
"Hello @fab-jul, 

the download link seems to no longer work (https://data.vision.ee.ethz.ch/mentzerf/imgcomp-ckpts/ckpts.tar.gz). Is there any other way to get the checkpoints?

Thank you very much."
"Hello
I don't know if it still solves the issue related to the code, but I'll share it with you.

I would like to optimize the code you provided to me in mse.

I thought
""imgcomp-cvpr/code/ae_configs/cvpr/base
distortion_to_minimize=ms_ssim
-> distortion_to_minimize=mse"" to optimize to mse.

However, the above method causes the following error.
For your information, there is nothing fixed except a distortion (ms-ssim optimization did a good job).

<img width=""599"" alt=""error"" src=""https://user-images.githubusercontent.com/75883475/118921939-0e2b5d80-b974-11eb-9207-7d1f23b33a6e.png"">

I tried to solve it by myself, but I'm a beginner, so I don't know how to solve it.
Thank you for sharing such a wonderful code. I'd appreciate it if you could help me with my problem.

Thank you :)"
"Hello!

When I run val.py and I pass --real_bpp, I got an error ""Expected bpp_theory to match loss! "". I can't understand why ? bpp_real and bpp_theory are equal. You can see the results in the pic below.

Thank you.

![Capture](https://user-images.githubusercontent.com/29478246/85124687-9c5e0780-b22a-11ea-8933-6fc67f8ecd92.PNG)
"
"I am sorry for bothering you again, but please allow me to show my issue for the last time. After I prepared all the environment, including the python packages and TFrecords, my training always stopped at the string ""-STARTING TRAINING-------------"", then it won't show any infomation at all ,it just stopped there, and will never finish itself.I don't know why.Here is my training command:"
"After I created the python3.4.5,I used ""pip install -r requirements.txt"" to install the packages,a warning appeared：**imageio requires Python '>=3.5' but the running Python is 3.4.5**.I don't whether I should switch to python3.5"
Sorry to bother you again!but a warming appers when I am train a model using ImageNet :
"In the train process:
**_python train.py ae_configs/cvpr/low pc_configs/cvpr/res_shallow \
            --dataset_train ~/data/train \
            --dataset_test ~/data/val \
            --log_dir_root ~/imgcomp-cvpr-master/code/log_**
I got this error:
**- STARTING TRAINING ------------------------------------------------------------
Traceback (most recent call last):
  File ""train.py"", line 530, in <module>
    main()
  File ""train.py"", line 526, in main
    description=flags.description if not flags.temporary else None)
  File ""train.py"", line 194, in train
    train_flags, logdir, saver, is_restored=restore_manager is not None)
  File ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/sitepackages/fjcommon/tf_helpers.py"", l
ine 39, in start_queues_in_sess    coord.join(threads)
  File ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/sitepackages/tensorflow/python/training/coordinator.py"", line 389, in join    six.reraise(*self._exc_info_to_raise)
  File
 ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/site-packages/six.py"", line 703, in reraise    raise value
  File ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/sitepackages/tensorflow/python/training/queue_runner_impl.py"", line 238, in _run    enqueue_callable()
  File ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/sitepackages/tensorflow/python/client/session.py"", line 1231, in _single_operation_run    target_list_as_strings, status, None)
  File ""/public/home/xqqstu/anaconda3/envs/py35/lib/python3.5/sitepackages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: /public/home/xqqstu/data/train; Is a directory	 
[[Node: input_glob__public_home_xqqstu_data_train/images_decoded/ReaderReadV2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](input_glob__public_home_xqqstu_data_train/images_decoded/WholeFileReaderV2, input_glob__public_home_xqqstu_data_train/images_decoded/input_producer)]]**"
"I don't the meaning of ""log_dir_root"" in the training process"
"I‘m sorry!I did just as your advice said,but the error still exist:
![1](https://user-images.githubusercontent.com/65826012/83653748-becb1080-a5ee-11ea-9f22-17441cfc29a0.png)
here is The directory structure of the code:
![2](https://user-images.githubusercontent.com/65826012/83653881-e91cce00-a5ee-11ea-8284-d378545796aa.png)
I wonder whether the error is related to the directory stucture.Hoping for your reply!"
"I just did all as the README said,but when I run the test code:**val.py** like this: **python val.py ../ckpts 0515_1103 ./Kodak --save_ours**.A error occurs like this:
![image](https://user-images.githubusercontent.com/65826012/83599427-b519bc80-a59e-11ea-806e-b0a93a6fd932.png)
"
"I want to compare the bpp between real value and estimated value, and I put the Kodak dataset named 'kodak' under the folder ""code/ckpts"",  why I use the command ""python val.py ../ckpts 0515_1103 ../ckpts/kodak --save_ours --real_bpp"", I cannot see any results unless ""***All given job_ids validated""?. Is there something wrong?"
"In the quantizer.py file, soft quantization is used. How do I update the sigma parameter during training? Is sigma only 1 during training?  


thank you"
How can i apply to testing your code for monochromatic images. Is it possible? Or what i need to change from your code? Could you please let me know?
In the plot chapter，how can we get the *.csv files
"I noticed that in create_first/other_mask functions in probclass.py, you only set the causality mask on the last one on the C or D channel, which is different to Algo 1 in the supplemental material, where causality are supposed to be set across the C/D.
In other words, why not 
mask[:, K // 2, K // 2:] instead of mask[-1, K // 2, K // 2:] ?

Thank you"
"Hello!
I used your models and my trained models to compress images. Even though the bpp is low, the images reconstructed are large in size, e.g. the original image is 671.48kB, the compressed image is 500kB of 0.388 bpp, however, the image compressed by JPEG is 350kB of 0.6 bpp. I cannot find out why the JPEG-compressed image of a larger bpp is smaller than the model-compressed image of a small bpp. "
"@fab-jul 
hello~The Model-size of ckpts provied by you(0515_1103) is 136530KB = 1KB(checkpoint) +117723KB(.data) + 32KB(.index) + 18687KB(.meta) + 60KB(.pkl).
But the size of the model I trained with the network you provided is 232814 KB = 1KB(checkpoint) + 196160KB(.data) + 47KB(.index) + 36511KB(.meta) + 95KB(.pkl).

I think the model-size trained on the same network should be the same.  So what i am confused about is whether the ckpts you provided are trained by the network you gave here?  What is the difference between the network you trained and the network provided here?
I am looking forward to your reply."
Why does the decompression images have chromatic aberration? @fab-jul 
"@fab-jul Excuse me.The MS-SSIM  of the model  I trained  is almost as high as described in the article, but the psnr and ssim have no performance of JPEG compression. Is this the case for your training model?"
""
"In quantizer.py (line 82), there are two softmax usages over distances. I wonder if there is any special meaning to phi_hard softmax. 

I would say that argmax could have been done directly on distances, but I might be missing something here."
"If I save the quantized volume after the quantizer, the numpy file is larger in size than the original image. How to save the compressed form so that it takes lesser memory than the original image ?"
""
"Hello, there are some questions about the decoding of Bitstream.
1、How can we get the distribution of each location in the adaptive arithmetic decoding phase, by computing the distribution of each location one by one as an auto-regressive model through the context model?
2、 And what kind of information should be encoded into the bitstream?
Looking forward to your reply."
"Hello, the center of the quantized value is selected randomly, why? If it was randomly allocated, how to realize the part of arithmetic coding? Looking for your reply~"
"Hello, when I using the argument of --restore, there is something wrong:
OutOfRangeError (see above for traceback): RandomShuffleQueue '_2_input_glob__data_wency_CLIC_mobile_train__.png/shuffle_batch_join/random_shuffle_queue' is closed and has insufficient elements (requested 30, current size 0)

Can you give me some advice? Thank you~"
"Hello, I am very confused of the part of context model in your code, what's the output of the context model represents? And what's the meaning of  pc_loss = beta * tf.maximum(H_soft - H_target, 0), the reason for setting H_target is for what? Thank you very much and looking forward to your reply~"
"Excuse me. I'm confused with the configurations of training part.
I want to train a lower bit rate model such as bpp 0.2 or lower.
I used the `ae_configs/cvpr/low` directly or modified it by `H_target = 2*0.1`.
But after about 200000 iterations the average bpp of models on Kodak dataset are still about 0.35.
How can I train a lower bit rate model on Kodak dataset?
Thanks!"
"when I used the follow comand to create tf_records of imagenet datasets:
```
    pushd train
    find . -name ""*.JPEG"" | parallel --bar -j64 -N 1250 \
        'OUTPUT_PATH=$(printf ""../records/train/train-%05d.tfrecord"" {#});' \
        'python -m fjcommon tf_records mk_img_rec {} -o $OUTPUT_PATH --feature_key image/encoded'
    popd

    pushd val
    find . -name ""*.JPEG"" | parallel --bar -j16 -N 1250 \
        'OUTPUT_PATH=$(printf ""../records/val/val-%05d.tfrecord"" {#});' \
        'python -m fjcommon tf_records mk_img_rec {} -o $OUTPUT_PATH --feature_key image/encoded'
    popd
```

I got the error log:
```
usage: __main__.py [-h] {mk_img_recs,mk_img_recs_dist,join,extract,check} ...
__main__.py: error: argument mode: invalid choice: 'mk_img_rec' (choose from 'mk_img_recs', 'mk_img_recs_dist', 'join', 'extract', 'check')
```"
"Hi, I cannot fully understand your parameters in val.py, such as job_ids, what kind of parameter should i give? I very appricate that you can give a detailed explation for others parameters. Thanks a lot."
"Hi John,
Thank you for providing the code for a lot of methods. While I was trying to recreate the results for the model dropout_fn_of_xstar, it tries to load the dlupi trained model. If I load the untrained model, the loss diverges and goes to Nan. Could you please look into it. Thank you."
"https://github.com/johnwlambert/dlupi-heteroscedastic-dropout/blob/da71881506a550d19a0879fc625d4ff609ac11e3/cnns/imagenet/create_bbox_dataset.py#L160-L172

I'm trying to re-create the exact dataset. I feel like all the shutil.copy lines here should be uncommented because otherwise, I'm just iterating over the data and not creating anything. "
"Hi John Lambert,
I wonder the ""log"" in the eq.7 is missing in the code.  I checked the code in line 142/144 (dlupi-heteroscedastic-dropout/cnns/models/vgg_dlupi_model.py) and code in line 24(dlupi-heteroscedastic-dropout/cnns/train/modular_loss_fns.py ). I didn't find the ""log"" operation which is inconsistent with eq.7 in the paper.
Do I miss some important thing? Looking for your reply."
"I would like to kindly ask if the pre-trained weights for your work are available.

In the readme.md, it is stated that  they can be downloaded by executing the following script
`bash models/download_CNN_models.sh`

However, the script can not be found. 
Would it be possible to re-upload the specific script and the relevant weight files?


"
"I'll remove this from issue and contact via email, sorry."
"As the title says, does the training time remain in the log?"
"Hello, thanks for your marvelous contribution.I would like to know that the category of red traffic lights is not available on bdd, have you re-labeled it on the bdd dataset?"
"hi hi, thanks for your marvelous contribution. I am very impressed. Now I want to apply this pretrain model（tagging road type and weather） on my own dataset, do you have any codebase for finetuning?

 "
"Hello,

I'm generating semantic segmentation using the following command. 
```
python ./test.py ~/config.py --show-dir ~/Documents/bdd100k-models/data/bdd100k/labels/seg_track_20/val --opacity 1
```
This generates the colormaps for the images, however, the output produced is in .jpg format which results in blur within the labels (as shown below.) How can I update the script so that it generates the labels in png format.  My input images are from the `MOTS 2020 Images` dataset, which are in jpg format. 

![image](https://user-images.githubusercontent.com/19765483/179498684-40dd345d-20f2-458c-8b5f-2a2a0e39a0db.png)

"
"Error when running Sem_seg model inference 
Command Run:
`python ./test.py ./configs/sem_seg/deeplabv3+_r50-d8_512x1024_40k_sem_seg_bdd100k.py  --format-only --format-dir ./outputs`

ERROR:
```2022-06-28 12:51:09,657 - mmseg - INFO - Loaded 1000 images
workers per gpu=2
/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/mmseg/models/losses/cross_entropy_loss.py:235: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.
  warnings.warn(
load checkpoint from http path: https://dl.cv.ethz.ch/bdd100k/sem_seg/models/deeplabv3+_r50-d8_512x1024_40k_sem_seg_bdd100k.pth
'CLASSES' not found in meta, use dataset.CLASSES instead
'PALETTE' not found in meta, use dataset.PALETTE instead
[                                                  ] 0/1000, elapsed: 0s, ETA:ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
ERROR: Unexpected segmentation fault encountered in worker.
Traceback (most recent call last):
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 1011, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/queue.py"", line 179, in get
    self.not_empty.wait(remaining)
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/threading.py"", line 306, in wait
    gotit = waiter.acquire(True, timeout)
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py"", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 15796) is killed by signal: Segmentation fault. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""./test.py"", line 174, in <module>
    main()
  File ""./test.py"", line 150, in main
    outputs = single_gpu_test(
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/mmseg/apis/test.py"", line 89, in single_gpu_test
    for batch_indices, data in zip(loader_indices, data_loader):
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 530, in __next__
    data = self._next_data()
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 1207, in _next_data
    idx, data = self._get_data()
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 1163, in _get_data
    success, data = self._try_get_data()
  File ""/home/lunet/codsn/.conda/envs/bdd100k-mmseg/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 1024, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 15796) exited unexpectedly
```"
"@thomasehuang 

   Do you have a script to do this? I have tried to use image_demo.py from mmsegmentation/demo, but it has an error.

Thanks"
RT
"Hi,

can i use the models under ""bdd100k-models/det/"" to make predictions on other images ?

When i followed the ""Usage""-Section, it seems that the models can only be used to evaluate the Test/Val Images.
"
""
"https://github.com/open-mmlab/mmsegmentation/blob/master/docs/model_zoo.md#common-settings
The above link is not working

I would like to know the settings under which the segmentation models are trained , so that i can replicate the results .
thank you."
"Hello, 

I am trying to convert the bdd100k instance segmentation using this command: 
`python3 -m bdd100k.label.to_coco -m ins_seg --only-mask -i ./bdd100k/labels/ins_seg/bitmasks/val -o ./ins_seg_val_cocofmt_v2.json`

Also, tried this: 
`python3 -m bdd100k.label.to_coco -m ins_seg -i ./bdd100k/labels/ins_seg/polygons/ins_seg_val.json -o ./ins_seg_val_cocofmt_v3.json -mb ./bdd100k/labels/ins_seg/bitmasks/val`

The conversion is successful in both cases and the annotation looks like this 

![Screen Shot 2022-01-07 at 11 46 36 AM](https://user-images.githubusercontent.com/75509659/148577201-4e07622b-9e0f-495d-a2d8-004deabb623e.png)
** that's not how coco annotations are.

Now, if you see the segmentation field above there's string encoding of the masks. Now, I am unsure if that's expected or not. 

Further, assuming it's correct,  I tried to load the annotations using loader from DETR https://github.com/facebookresearch/detr/blob/091a817eca74b8b97e35e4531c1c39f89fbe38eb/datasets/coco.py#L36 

The line I have mentioned above is supposed to do the conversion but I am getting an error from the pycocotools that it's not expecting a string in the mask. 
![Screen Shot 2022-01-07 at 11 53 51 AM](https://user-images.githubusercontent.com/75509659/148578244-bcb1d369-4b04-493d-a68c-0dd82563f771.png)

So, I am unsure where the problem is? Is the conversion correct to coco then the loader should work? 
Note: I tried to convert the detections and they worked fine. 

Thank you for any help you can provide. 
"
""
"Hello! thank you for your work~~but i wonder if i could train these models on my own gpu? i wonder if there are som instructions or usages? plz ,thank u!"
"When I am running `Deeplabv3+` model by using:
`python ./test.py configs/drivable/deeplabv3plus_r50-d8_512x1024_40k_drivable_bdd100k.py --format-only --format-dir output`
It just stuck in around 1490 step
![image](https://user-images.githubusercontent.com/41173132/143347422-a471bb73-5ba2-4acf-9d4f-54546c6ebe12.png)
I have tried several different configs, they all have the same issue.
"
you have done a really fantastic job! I am wondering if it is possible to add models for time of the day?
""
"Hello. Thank you for your paper and code.
Would you release the datasets you used in the experiment through Google drives or other external links?
I think it'll be really helpful to refer to your paper."
"Hi，authors,I cant't find the same 'GeometricTools/WildMagic5/SDK' and 'ann_1.1.2_built'  code base on the Internet, can you share these two code bases?"
"hi, I want to ask a question, can this be used in part-segmentation, such as shapenet mesh segmentation?"
"Hello, authors. I saw in the paper that PFCNN can be extended to point clouds easily, but I don't know how to implement it. Can you give me some advice? Thank you very much."
"Hi, authors, thank you very much to publish you source code, but when I try to compile the pySurfaceHierarchy with cmake, I got this error ""pySurfaceHierarchy/include/DirectionFields.hpp:12:24: fatal error: Wm5Matrix3.h: no such file or directory "", and I cannot found the Wm5Matrix3.h, if you can give me some advice I will be very grateful.

BTW,If someone met the error and fix it, could you pls help me with it, thank you very much. "
"Hi, @haopan ,

I'm interested in your work and would like to study it further. When will the code be released?

THX!"
""
"![scheme](https://user-images.githubusercontent.com/49682515/76166263-fcdaa480-6165-11ea-9755-19f5bec032ef.png)
"
"[scheme.pdf](https://github.com/benyv/uncord/files/4303454/scheme.pdf)
"
"![image](https://user-images.githubusercontent.com/49682515/76166090-84271880-6164-11ea-96ae-697e9c722775.png)
"
"hello, thank you for your excellent work~
I have some problem about accuracy of protonet. when i use Resnet12 as backbone to train protonet from scratch with 30way-1shot, i found the best accuracy is roughly 49%. However, the best acc in your paper can arrive 62%, maybe use pre-train model? in other words, i need pretrain model like traditional CNN in training set and use it in training protonet if i want to get 62% ?
sorry to bother you, I hope I have already clarified the issue thoroughly. :)"
"　　Hello, thank you for your thoughtful paper and open code. I have gained a lot after reading it, but I have some questions to ask you
　　First, about pre training, if I don't load the pre training model, the accuracy of res12 and convnet used by feat in the backbone network is 47% and 59% respectively, which is far lower than 80% of RESNET loaded with the pre training model. I  understand that pre training can speed up the training speed, but why it can greatly improve the accuracy. Then I see that there is a pretrain.py  in the project , so can I use my own data to train the model？
![image](https://user-images.githubusercontent.com/64951554/167252870-234388fd-37a1-4e10-8a49-1822c5b226dd.png)
In the code here, the training set does not follow the small sample k-ways-n-shot method, but sends all data into the training in batches. Just like the ordinary CNN training, why not follow the feat training method？What should I pay attention to when I train with my own data set？
　　Second, as for the feat model, I carefully read the code and found that instead of using transformer completely, simply used multiheadattention to input the proto to generate a new proto. I can simply understand that using self-attention strengthens the correlation between prototypes.and about loss fucntion？
![image](https://user-images.githubusercontent.com/64951554/167253142-fe02f8e2-c299-4835-9ec1-d5024c6b5c51.png)
Here you use the prototype loss function + regularization function. I looked at the regularization function. It uses all the data of the support set and query set to average the prototype, and then calculates the Euclidean distance with these data sets. Is this loss function obviously helpful to improve the accuracy of the model, or may it not be obvious?Thank you for your reply

"
""
"hello，
I would like to ask how to devide tierdimagenet? If it's convenient for you, can you upload the split folder of tieredimagenet?
best wishes."
"Hi,

I am trying to recreate the pretrained resnet12 model for tieredimagenet. Could you please tell me what is the best_val_acc_sim and best_val_acc_dist you got when you pretrained on tiered imagenet?

Thanks and Regards
Mayug"
"Hi, 

Could you please provide the hyperparameters used for pertaining convnet and res12 on the CUB dataset?

Also, what were the best val_dist acc and val_sim acc you got for convnet and res12 on the CUB dataset in the pertaining stage?

Thanks and Regards
Mayug"
"I saw you compared with CTM, which has 64.12 acc. I can't reproduce such a high accuracy rate through his code. I would like to know your parameter settings or can you provide a CTM reproduction code?
Very Thanks"
"My problem is the same as #69，
In the experiment, I found that the pre-training strategy had a significant effect on the final experimental results，
And I want to recreate ResNet12 pre-trained weights for CUB. 
Could you please give some suggestions for the hyperparameters that pretraining ResNet12 on CUB?

like the pretrain ResNet12 on MiniImageNet ""python pretrain.py --batch_size 128 --max_epoch 500 --lr 0.1 --dataset MiniImageNet --backbone_class Res12 --schedule 350 400 440 460 480 --gamma 0.1"""
"Hi authors,

Thank you for the great work! We are preparing a submission and we would really love to use FEAT as a baseline. However, we couldn't reproduce the numbers for FEAT on the tieredImageNet and miniImagenet. We have the same numbers for all other 3 methods: ProtoNet, DeepSet, and GCN.

Could you please let us know if there are some differences in the way you organize the datasets for FEAT, compared to the other 3 methods?  

As the deadline is approaching, we would highly appreciate if you can help us in the next 2 days! 🙏 

Best,
Hieu"
""
"Hi

I noticed that in line 99: proto = support.mean(dim=1)  of feat.py file, you calculate the mean of the first dimension. However, I think the shape of support is (batch_size, way, shot, d). Shouldn't it be averaged over the second dimension?

Thank you"
"Hello, I could not have access to google drive because of regional restrictions.
Can I get all the pretrained weights by other approaches?
Thank you!"
"Hey,
Great work. I have been trying to recreate ResNet12 pre-trained weights for CUB.

Could you please provide the hyperparameters that you used for ResNet12 pretraining on CUB?

Thanks"
"首先和感谢您的工作以及代码的开源，帮助学习到了很多，但是对论文中的transductive FSL 的描述很迷惑，还望帮忙解决下：
论文附录里关于transductive的描述如下：
When classifying test instance xtest in the transdutive scenario, other test instances Xtest from the N categories would also be available. Therefore, we enrich the transformer’s query and key/value sets
Q=K=V=Xtrain∪ Xtest
In this manner, the embedding adaptation procedure would also consider the structure among unlabeled test instances.When the number of shots K >1, we average the embedding of labeled instances in each class first before combining them with the test set embeddings.
针对以上描述，有以下几个困惑：
1、other test instances Xtest from the N categories would also be available中other test instances Xtest是指什么，是来自支持集5类中的其他图片吗，那这不是相当于扩充了训练时候支持集的shot数？
2、other test instances Xtest有标签吗？需要进行预测的是test instance xtest，那没有标签的xtest的信息就未用到，所以怎么体现transductive呢。
3、When the number of shots K >1, we average the embedding of labeled instances in each class first before combining them with the test set embeddings.中，先平均支持集嵌入，然后与test set embeddings结合计算更好的类中心，这样把test set用来计算类中心了不是就已经知道它的标签了，还怎么预测呢。

在实验介绍部分描述如下：
Based on the adapted embedding by the joint set of labeled training instances and unlabeled test instances, we can make predictions with two strategies. First, we still compute the center of the labeled instances, while such adapted embeddings are influenced by the unlabeled instances (we denote this approach asFEAT†,which works the same way as standardFEATexcept the augmented input of the embedding transformation function);Second, we consider to take advantage of the unlabeled instances and use their adapted embeddings to construct a better class prototype as in Semi-ProtoNet (we denote this approach asFEAT‡).

1、Based on the adapted embedding by the joint set of labeled training instances and unlabeled test instances,问题同上面3，这种和unlabeled test instances怎么进行结合，如果是一起用来计算类原型的，那岂不是已经知道它的标签就无需预测了；
2、FEAT、FEAT†和FEAT‡的具体区别是哪里不同？

综上就是，在transductive的设置上具体是怎样进行设置，怎样来进行查询集的预测的？
读了很多遍都没理清其关系，真的很困惑，万分期待您的回复！

"
"Should I use the same lr and gamma for both 1-shot and 5-shot tasks?
"
"Transformer in FEAT model compares query image feature vector with all support set feature vector. I got the idea that query image is Q of the transformer. So what is the key and value, I think support set are key or value. I wanted to understand which component of FSL is key, and what is value for Transformer? Or key and value are same in this case?"
"感谢你们的工作！
我发现文中提到了位置编码，但是每个episode输入的一系列图片好像不像句子中的单词一样有先后位置，而只是来自不同类的个体。所以请问为什么我们还需要位置编码呢？"
I am using your FEAT model for my own dataset which is quite noisy. I found that when I use FEAT model ResNet 12 is better than ConvNet but when I use same prototypical network as in your code I found that ConvNet outperforms ResNet12 by 3%. What could be the reason for that FEAT best accuracy is through deeper network and prototypical network works better with shallow network?
"Great work!
I have some confusion about the code implementation on contrastive loss. 
In the paper, it is suggested that the loss acts on `instances embeddings after adaptation`.
However, in `feat.py`, logits_reg is calculated by aux_center and aux_task, where aux_task seems `embeddings before adaptation`?
Looking forward to your reply!"
"Thanks for your work！ There is a question about the cosine distance.        
`if self.args.use_euclidean:
            query = query.view(-1, emb_dim).unsqueeze(1) # (Nbatch*Nq*Nw, 1, d)
            proto = proto.unsqueeze(1).expand(num_batch, num_query, num_proto, emb_dim).contiguous()
            proto = proto.view(num_batch*num_query, num_proto, emb_dim) # (Nbatch x Nq, Nk, d)

            logits = - torch.sum((proto - query) ** 2, 2) / self.args.temperature
        else:
            proto = F.normalize(proto, dim=-1) # normalize for cosine distance
            query = query.view(num_batch, -1, emb_dim) # (Nbatch,  Nq*Nw, d)

            logits = torch.bmm(query, proto.permute([0,2,1])) / self.args.temperature
            logits = logits.view(-1, num_proto)`
When the argument is false, i.e. when using cosine distance, the query should also be normalized, right?"
请问，paper里有说Pre-Avg  version比Post-Avg version好，但为什么代码里正则时要用Post_Avg?是因为两种loss计算不一样要这样吗？希望得到你们的指点，谢谢！
"Thanks for the excellent work. 

In your paper, I noticed that you pre-trained the backbone to initialize its weights rather than training the backbone from scratch, is that right?
As can be seen from Table 1, the pre-trained ProtoNet has achieved comparable performance to those of SOTA approaches, which makes me believe that the pre-training strategy has a big impact on the performance improvement. If so, will the classification accuracies drop without the pre-training? Are there any evaluation results of the proposed method without using the pre-training strategy from the beginning?


"
"I am going through the code of testing part and I wanted to know which images are incorrectly classified by model. So when I printed the data and labels in each batch I found that labels are always from 0 to 4 (5 way 5 shot classification). So Is this correct? as in every batch different set of image classes is present but its always prints label from 0 to 5.
So if I try to print confusion matrix stats/confusion matrix its give wrong result as labels are always 0 to 4 instead of actual label. Part of code to look into-
`
with torch.no_grad():

            for i, (batch,latch) in tqdm(enumerate(self.test_loader, 1)):
                if torch.cuda.is_available():
                    data, _ = [_.cuda() for _ in batch]
                else:
                    data = batch[0]

                logits = self.model(data)
                loss = F.cross_entropy(logits, label)
                pred = torch.argmax(logits, dim=1)
                print(""data is"", data) # here tensor form of image data is printed
                print('label is:', label) # here its print from 0 to 4 for 5W5S classification, Even if the image label id is more than 4
                #print('pred is:',pred)
                acc = count_acc(logits, label)
                record[i-1, 0] = loss.item()
                record[i-1, 1] = acc`

To plot confusion matrix Do I have to comment out one hot encoding part?"
Can we plot confusion matrix during testing in FEAT model setup?  As we are doing testing based on episodes and in each episode we are using different combination of classes. Any pointer regarding confusion matrix generation.
"感谢你们提供FEAT源码！我尝试用自己的数据集，SEEN(train)有两类，UNSEEN(validate)有4类。 我在SEEN classes上做预训练
`python3 pretrain.py --dataset CustomizedData --lr 0.001 --batch_size 128 --max_epoch 600 --schedule 170 300 400 500 --gamma 0.1 --backbone_class ConvNet`
得到
[dist] val acc = 0.5646
[sim] val acc = 0.5758
然后用此模型做FEAT
`python3 train_fsl.py  --max_epoch 200 --model_class FEAT --use_euclidean --backbone_class ConvNet --dataset CustomizedData --way 2 --eval_way 4 --shot 5 --eval_shot 5 --query 15 --eval_query 15 --balance 0.1 --temperature 32 --temperature2 64 --lr 0.0001 --lr_mul 10 --lr_scheduler step --step_size 20 --gamma 0.5 --gpu 14 --init_weights ./saves/initialization/miniimagenet/con-pre.pth --eval_interval 1`
只能在epoch 7 得到最高acc = 0.6272
请问有什么修改建议可以提高准确率吗？

还有另一个问题，请问如果像我这么正常运行是没有用到test集的吧？"
"你好！当我用自己的数据集和预训练模型时，如下命令：
`python3 train_fsl.py  --max_epoch 200 --model_class FEAT --use_euclidean --backbone_class ConvNet --dataset CustomizedData --way 4 --eval_way 4 --shot 5 --eval_shot 5 --query 15 --eval_query 15 --balance 0.1 --temperature 32 --temperature2 64 --lr 0.0001 --lr_mul 10 --lr_scheduler step --step_size 20 --gamma 0.5 --gpu 14 --init_weights ./saves/initialization/miniimagenet/con-pre.pth --eval_interval 1`
会遇到报错：
Traceback (most recent call last):
  File ""train_fsl.py"", line 19, in <module>
    trainer.train()
  File ""/home/tribta/YiXu/FEAT-original/model/trainer/fsl_trainer.py"", line 76, in train
    logits, reg_logits = self.para_model(data)
  File ""/home/tribta/.local/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/tribta/YiXu/FEAT-original/model/models/base.py"", line 48, in forward
    logits, logits_reg = self._forward(instance_embs, support_idx, query_idx)
  File ""/home/tribta/YiXu/FEAT-original/model/models/feat.py"", line 113, in _forward
    query   = instance_embs[query_idx.contiguous().view(-1)].contiguous().view(  *(query_idx.shape   + (-1,)))
IndexError: index 40 is out of bounds for dimension 0 with size 40

请问应该怎么解决呢"
"Thank you for your great work.
could you give the hyperparameters  for the CUB ? like the 
python train_fsl.py  --max_epoch 200 --model_class FEAT  --backbone_class Res12 --dataset MiniImageNet --way 5 --eval_way 5 --shot 1 --eval_shot 1 --query 15 --eval_query 15 --balance 0.01 --temperature 64 --temperature2 64 --lr 0.0002 --lr_mul 10 --lr_scheduler step --step_size 40 --gamma 0.5 --gpu 1 --init_weights ./saves/initialization/miniimagenet/Res12-pre.pth --eval_interval 1 --use_euclidean
for the 1 shot and 5 shot, thank you very much!
@Han-Jia "
"Hi,
I appreciate your very insightful work as well as providing source code. Slightly related to Issue #24, I have been trying to recreate WRN pre-trained weights for miniImagenet that you had provided [here](https://drive.google.com/file/d/1fGE4s9aCUFBT75hNgEOSMbqfA3kANHiP/view) for the protonet but can't seem to get the 61.4% performance you obtained in those results. Would you be willing to provide the command you used to train that protonet WRN model? 

Thank you"
"Hey,
Great work. I have been trying to recreate convnet pre-trained weights for mini imagenet.

These are the hyperparameters I used. 
python pretrain.py --max_epoch 500 --batch_size 128 --lr 0.01 --schedule ""350, 400, 440, 460, 480"" --gamma 0.1  --backbone_class ConvNet.

I got the following metrics on the 16 way classification on val set.
[Dist] best epoch 498, current best val acc=0.2437                                                                                                                                                                   [Sim] best epoch 362, current best val acc=0.2490

Using the model_best.pth.tar for 5 way one shot training I am only able to get test accuracy of 52.85 and not anywhere close to 55.

But using the provided con-pre.pth I am able to get test accuracy of 55.

Could you please provide the hyperparameters that you used for Convnet pretraining on Mini ImageNet?

Thanks

"
"Hi, 
Dr. Ye, thanks for your nice and valuable work.

I can not find the implementation details about the results of MAML on mini-imageNet with ConvNet in Supplementary Material. Besides, the corresponding code about MAML is also not available in this repository.

These materials are so important for my current work.  So could you share with me the code and the implementation details about the results of MAML on mini-imageNet with ConvNet.

Thank you very much!

Best wishes!

Email: jizhang.jim@gmail.com 
"
"请问一下data sample的过程代码是按episode来取的，每次也是一个episode输入model，为什么模型内部的解决的输入shape为（batch,  (n_way)*(shot+query), embed），而不是（(n_way)*(shot+query), embed）呢？"
"Hi, thanks for your nice work!

I use training scripts given in README.md with pretrained ResNet12 model:

`python train_fsl.py  --max_epoch 200 --model_class FEAT  --backbone_class Res12 --dataset MiniImageNet --way 5 --eval_way 5 --shot 5 --eval_shot 5 --query 15 --eval_query 15 --balance 0.1 --temperature 64 --temperature2 32 --lr 0.0002 --lr_mul 10 --lr_scheduler step --step_size 40 --gamma 0.5 --gpu 1 --init_weights ./saves/initialization/miniimagenet/Res12-pre.pth --eval_interval 1 --use_euclidean`

But I got ""Test acc=0.7821 + 0.0015"" which is far lower from the results ""82.05%"" reported on paper.

![image](https://user-images.githubusercontent.com/23455720/109916888-5cbf4900-7cef-11eb-8058-17c0cbc0ffb5.png)

Did I miss something？
"
"实测在使用WRN作backbone时，模型的效果会大幅下降……大概降到77左右（FEAT 5way5shot miniimagenet 有数据增强）。训练相关的参数我都使用了默认值。预训练模型使用的是您README中提供的那个。
十分感谢！"
请问在预训练时有什么需要注意的地方吗？
"I went through all the issue posts but didn't find the pretraining hyper-parameters for tiered imagenet. I can only find mini-imagenet's. Could you also release them for tiered imagenet?

Thanks!"
"Hi thanks for the great work!

I have questions about the labels used in meta-training stage, I find you didn't use gt_label in your code.
you put gt_label in cuda(data, gt_label = [_.cuda() for _ in batch]), when computing cross_entropy, you use other labels(loss = F.cross_entropy(logits, label)), which is prepared by prepare_label func, this is very weird

"
"Hi, Dr. Ye
Your nice work brings me a lot of inspiratons. Thank you very much!

I have tried to reproduct the results of the ProtoNet with the pre-trained weights of ResNet12 you provided. However, i find the model  achieves the best acc just after 1 epoch using the the following scripts. In addition,  when i look at the curves of  val_acc and val_loss, i found  with the val_loss decrease, the val_acc also decrease. Is it normal?  

Thank you so much!

Scripts used in my experiments:
python train_fsl.py  --max_epoch 200 --model_class ProtoNet--backbone_class Res12 --dataset MiniImageNet --way 5 --eval_way 5 --shot 5 --eval_shot 5 --query 15 --eval_query 15 --balance 0.1 --temperature 64 --temperature2 32 --lr 0.0002 --lr_mul 10 --lr_scheduler step --step_size 40 --gamma 0.5 --gpu 7 --init_weights ./saves/initialization/miniimagenet/Res12-pre.pth --eval_interval 1 --use_euclidean



"
"Hi thanks for the great work!

I read your code very carefully but i am really confused here. In the resnet-12.py file, the forward function is like this:

```
  def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        if self.keep_avg_pool:
            x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        return x
```

and

```
def Res12(keep_prob=1.0, avg_pool=False, **kwargs):
    """"""Constructs a ResNet-12 model.
    """"""
    model = ResNet(BasicBlock, keep_prob=keep_prob, avg_pool=avg_pool, **kwargs)
    return model
```

you can see here the default argument for avg_pool is false. Thus the output feature vector is 5*5*640 dimension instead of 640, which is claimed in your papers.

And in the base fewshot model, i don't see the avg_pool argument is specified as True:

```
class FewShotModel(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        if args.backbone_class == 'ConvNet':
            from model.networks.convnet import ConvNet
            self.encoder = ConvNet()
        elif args.backbone_class == 'Res12':
            hdim = 640
            from model.networks.res12 import ResNet
            self.encoder = ResNet()
        elif args.backbone_class == 'Res18':
            hdim = 512
            from model.networks.res18 import ResNet
            self.encoder = ResNet()
        elif args.backbone_class == 'WRN':
            hdim = 640
            from model.networks.WRN28 import Wide_ResNet
            self.encoder = Wide_ResNet(28, 10, 0.5)  # we set the dropout=0.5 directly here, it may achieve better results by tunning the dropout
        else:
            raise ValueError('')
```

The same as other fewshot models.

Could u please explain this here? thanks!

"
"Hi thanks for the great work!

I went through the github issue and found someone asked the same question. However, seems like at different times, your response is also slightly different, sometimes lr=1e-3 and sometimes be 1e-4. So could you finalize the answer for the setting of init lr, schedule, gamma, max epoch and temperature? thanks

i have tried some combination of these hyper-parameters with the pre-trained model you provided. But for 5way 5shot i cannot reach 80. Also when i look at the curve of training acc/error and validation acc, i found they actually change very little through the whole training process and the best validation epoch is basically the first 5 epochs. Just wondering is this behaviour normal or similar to your end?

Thanks in advance!"
"I see in the prepare_model we have:
```
def prepare_model(args):
    model = eval(args.model_class)(args)

    # load pre-trained model (no FC weights)
    if args.init_weights is not None:
        model_dict = model.state_dict()        
        pretrained_dict = torch.load(args.init_weights)['params']
        if args.backbone_class == 'ConvNet':
            pretrained_dict = {'encoder.'+k: v for k, v in pretrained_dict.items()}
        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}
        print(pretrained_dict.keys())
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    if args.multi_gpu:
        model.encoder = nn.DataParallel(model.encoder, dim=0)
        para_model = model.to(device)
    else:
        para_model = model.to(device)

    return model, para_model
```
later we have 
```
def train(self):
        args = self.args
        self.model.train()
        if self.args.fix_BN:
            self.model.encoder.eval()
        
        # start FSL training
        label, label_aux = self.prepare_label()
        for epoch in range(1, args.max_epoch + 1):
            self.train_epoch += 1
            self.model.train()
            if self.args.fix_BN:
                self.model.encoder.eval()
            
            tl1 = Averager()
            tl2 = Averager()
            ta = Averager()

            start_tm = time.time()
            for batch in self.train_loader:
                self.train_step += 1

                if torch.cuda.is_available():
                    data, gt_label = [_.cuda() for _ in batch]
                else:
                    data, gt_label = batch[0], batch[1]
               
                data_tm = time.time()
                self.dt.add(data_tm - start_tm)

                # get saved centers
                logits, reg_logits = self.para_model(data)
                if reg_logits is not None:
                    loss = F.cross_entropy(logits, label)
                    total_loss = loss + args.balance * F.cross_entropy(reg_logits, label_aux)
                else:
                    loss = F.cross_entropy(logits, label)
                    total_loss = F.cross_entropy(logits, label)
```
what is the difference between `self.para_model` and `self.model`, and why we have  `self.model.train()` at the beginning , but in the   `prepare_model(args)` we have `model = eval(args.model_class)(args)`, why we use `eval` here and then `.train()`. can you please explain it?
Thanks a lot "
I was wondering what does the cash do and if I need to use it?
"Thanks for sharing your code, it is so useful and great.
I'm new to FSL, I think I understand most of the stuff about it and I know what is query and support.
I am not able to understand the concept of the query when it is more than `1`? can you please clear that concept for me? what does it mean to have query `15`?

to be more specific, for `way=5`, `shot=1`, and `query=15`, the data is in the shape of `80x3xwxh`.
I don't understand this. does `instance` define as `query+support`? if so, I guess for each instance (query+suppor) we have (5 support + 1 query), am I correct? and then `15*6` should be `90`? can you please explain what I'm missing here, where the `80` comes from?

In this case (input = `80x3xwxh`), the outputs of model is `logits` and `reg_logits` which are in the shape of `75x5` and `80x5`. would you also explain what those dimensions represent? 
I understand that 5 is the number of ways (classes in this episode), but cannot figure out the rest"
"With the pretrain.py, I can't reproduce the performance of the Released weight. 
Can you give the hyper-parameters for the pre-training?
"
"Thanks for sharing you code!!
I see you provide the scripts to train the model. How I can test the model using the pretrained models?
Let say I want to use your pretrain model to test on cub dataset?
Thanks in advance :)"
"I use the script and the pretrained model provided by the auther (resnet12), but only get 68.9 accuracy on 5-way 1-shot, is there anything I should notice? On the other hand, I can reproduce the results on mini-imagenet."
"Hello, I am a new student in deep learning. I have a simple problem that I cannot solve at present. I would like to ask for your advice。
I download the dataset CUB and miniimagenet, put it in the right palce. When I run train_fsl.py, the program reported an error.

100%|█████████████████| 38400/38400 [00:00<00:00, 314378.16it/s]
100%|███████████████████| 9600/9600 [00:00<00:00, 370283.04it/s]
100%|█████████████████| 12000/12000 [00:00<00:00, 315266.39it/s]
best epoch 0, best val acc=0.0000 + 0.0000
Traceback (most recent call last):
  File ""/Users/sihanwang/Desktop/FEAT/train_fsl.py"", line 19, in <module>
    trainer.train()
  File ""/Users/sihanwang/Desktop/FEAT/model/trainer/fsl_trainer.py"", line 105, in train
    self.try_evaluate(epoch)
  File ""/Users/sihanwang/Desktop/FEAT/model/trainer/base.py"", line 53, in try_evaluate
    vl, va, vap = self.evaluate(self.val_loader)
  File ""/Users/sihanwang/Desktop/FEAT/model/trainer/fsl_trainer.py"", line 136, in evaluate
    logits = self.model(data)
  File ""/Users/sihanwang/.conda/envs/input_bear_data.py/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File ""/Users/sihanwang/Desktop/FEAT/model/models/base.py"", line 51, in forward
    logits = self._forward(instance_embs, support_idx, query_idx)
  File ""/Users/sihanwang/Desktop/FEAT/model/models/semi_protofeat.py"", line 135, in _forward
    proto = self.get_proto(support, query) # we can also use adapted query set here to achieve better results
  File ""/Users/sihanwang/Desktop/FEAT/model/models/semi_protofeat.py"", line 114, in get_proto
    proto = torch.bmm(z.permute([0,2,1]), h)
RuntimeError: Expected tensor to have size 90 at dimension 1, but got size 80 for argument #2 'batch2' (while checking arguments for bmm)

I know this is because the dimension of z and h is wrong and the matrix cannot be multiplied, but I don't know how to fix it. I hope you can tell me if it is convenient, thank you.
P.S. I change the ROOT_PATH from data/CUB/images to FEAT/data/CUB/images, otherwise the program will not be able to find the image data."
"Hello,I am very interested in your paper. However,When I try to undersatnd contrastive learning in your paper and regularization in your code,I get confused. You construct the prototype by using support and query in the train set,but I think we can only use the support to construct the prototype,because we don't know the label of the query. So could you answer my confusion,thanks!"
"Hi  that's really a nice code!  However, i have a question about resnet.  In the resnet paper, resnet's first conv layer's kernel size is 7. But  in your code , I found the first conv kernel size is 3. I also found other codes which use resnet-12 or 18. They also set the kernel size is 3.  
     What's more, I read the TADAM's paper which design the resnet-12(maybe not). It just use blocks, didn't add conv layer before the blocks.
    Could you tell me why you change conv kernel size ?  Thank you !"
"Hi,

I downloaded your code and initial weight, and try to reproduce the performance what you reported in the paper. I use the hyperparameters you right in **Readme**

Here is the result, on the val-set your work performs pretty good but on the test set it works badly.


![image](https://user-images.githubusercontent.com/20614505/85536246-82b32a80-b645-11ea-9672-82fd15e0c8e8.png)


so I want to know whether there are some tricks not show in the code?

Or show me a good hyperparameters"
"Hi,"
How can I test the model after meta-training?
"Hi, thanks for this work.
I have tried to use the pretrained weights directly in a prototypical network on 1-shot miniImageNet. The result is about 59% when using Res12 and 55% when using WRN. I wonder if this is reasonable since WRN is a deeper network compared to Res12."
"Hi there,

Are the default hyperparameters in pretrain.py good or are there variations depending on the architecture or the dataset ?

Thanks"
"Hi! The codes are really neat! Thanks for sharing!
I notice that it requires a Pytorch>=1.4. However because of the limitation of my working environment,  the CUDA requirement of Pytorch v1.4 can't be realized, so I wonder if the codes will work on lower versions of Pytorch that work with CUDA 9.0 (pytorch v1.1.0 for example)?
"
"Hi, thanks for sharing your code.

Could you provide command lines for training the Protonet models ? Or are the hyperparameters exacty the same as for FEAT ?

Thanks !"
"Hi, can you please provide download links to the trained models for tiered imagenet."
"Do you train 1 shot and 5 shot separately? Some papers use large numbers of shots, such as 15 to train a model once and test for 1 shot and 5 shot. [MetaOptNet](https://github.com/kjunelee/MetaOptNet) is the case."
"Hi!

In the supplementary section you mention that you work with three architectures: ConvNet, ResNet and WRN, however, the code contains only two feature extractors which are referred to as ConvNet and ResNet. Are you planning to add the code for your WRN implementation? If I'm not mistaken, the ResNet architecture that you use is normally referred to as WRN-28-10 in other papers (including the Qiao's paper you mention). Could you, please, specify the difference between your ResNet and WRN-28-10 architectures?"
"Thanks for your code! But i have a question, what's the meaning of the use of the temperature? Thank you!"
"I tried to evaluate the feat on CUB dataset using your provided checkpoint: 

`
python eval_feat.py --model_type ConvNet --dataset CUB --model_path ./saves/FEAT-Models/CUB-Conv-1-Shot-5-Way.pth --shot 1 --way 5 --gpu 0
`

But only get Test Acc 0.5310 + 0.0022. Is there anything need to be modified to reproduce the result in the paper (68.87%)?"
""
"hello, where can I find the WRN backbone used in this paper?  Thank you."
"Thanks for sharing the nice paper and implementations.

I have a question for your shared code pretrain.py

It seems that your default argparse setting of pretraining is set to 1e-3 and decay 0.1 in 30, 50, 80 epochs.

I found that it yields lower accuracy of pretrain phase train/valid/test in MiniImageNet or TieredImageNet.

Is there any special reason to set you 1e-3 for initial learning rate or bigger lr (e.g. 1e-1) will works better?"
"It seems like `eval_feat.py` doesn't work currently.

Also I am not able to reproduce evaluation results for feat* with the pretrained models. Can you check whether this currently works?"
"Hi, It is very nice for you to provide the pre-trained model. 

Can you also provide the code of pre-training the backbone?

"
"Hi,
can you tell me what the performance of the pretrained backbones is if their embeddings are not further adapted or trained in a few-shot setting, but only evaluated?

Best regards,
Tim"
"Hello, @Han-Jia , Thanks for your implementation, but when you calculate the KL divergence, why do you formalize the attention labels like that? Why this implementation can reflect the property of the contrastive loss proposed in the paper?
```
    # construct attention label
    att_label_basis = []
    for i in range(args.way):
        temp = torch.eye(args.way + 1)
        temp[i, i] = 0.5
        temp[-1, -1] = 0.5
        temp[i, -1] = 0.5
        temp[-1, i] = 0.5
        att_label_basis.append(temp)
```
 Could you please explain it a little bit more, or is there any reference I can have a look at?

Thanks,"
"I think your provided weights for the initialisation are mixed with your final weights.

The README states:
1) Pre-trained weights are here - https://drive.google.com/open?id=14Jn1t9JxH-CxjfWy4JmVpCxkC9cDqqfE
2) The learned models are here - https://drive.google.com/open?id=1ZjkiEJh_96VYNWCOXUGsPuesLaFzV_z9

For MiniIMageNet-Res-5-Shot-5-Way.pth at the very least these appear to be the wrong way round.

Can you confirm?"
"@Han-Jia , Thanks for your source code! But I have some troubles in reproducing them.
I have achieved only **51.97%** for the 1-shot 5-way of ConvNet case and **70.60%** for 5-shot 5-way of ConvNet case.  The hyper-parameters I used is:

- --lr 0.0001 --temperature 32 --max_epoch 200  step_size (schedulr) = 10, gamma=0.2 for 1-shot 5-way of ConvNet

- --lr 0.0001 --temperature 32 --max_epoch 200  step_size (schedulr) = 10, gamma=0.2 for 5-shot 5-way of ConvNet

Should I make any changes to other default parameters to reproduce the result in Table1 (**52.61%** and **71.33%** test acc)?

any information will be greatly appreciated"
"Thank you for your great work!
I tried to implement the prototypical network with Resnet12, BUT I was failed. After reading your work, I found your code implement it, so I tried to run your train_protonet.py in my server. However, it could not run it to about 76% (5-shot 5-way) presented in your paper without pre-trained  weight. Can you tell me how to set the the hyper-parameters of  train_protonet.py ?
Best Wishes."
"Hi, I think the code in this repo corresponds to the results of FEAT*, because I can only get about 62.57%  when I evaluate for  1-shot -5-way with ResNet, and the model is trained by the script of the repo provided.
So, I wonder how to get the results of FEAT?  Much appreciated!"
"Hello
I have tried to reproduce the model pre-training instead of using your provided parameters. However, I could not get ideal results. I wanna illustrate what I have done to see whether I was doing something wrong.

There are 64 training class with 600 images each. So I add a 640*64 fc layers after  the feature encoder and use this model to train a standard classification network with 64*600 images. 
For validation, I drop the fc layers and use the encoder as the prototypical network to monitor the performance on 16 val classes.
The val performance could at most reach 46 % at pre-training stage. Training loss and acc could easily  overfit to 0 and 1.  When I use this encoder to initialize the FEAT, the val performance could at most reach  50.3. 
So I am wondering if I understand your pre-training stage correctly. Thank you very much.
"
""
"hi, very nice work.
where could I download the MiniImageNet dataset ? or should I download the whole imagenet instead?"
"Very impressed by your work, I notice that better results are reported in the archive version of your paper. Could you release the latest code that gives the better results? Much appreciated!"
"Hi, you have done an amazing work! I really like this work. If possible, could you please send me the  well-trained models (protoNet, matchingNet and FEAT) and optimal hyperparameters in training (like temperature)?"
"Hi Fusheng,

We have released the pre-trained model's weight [here](https://drive.google.com/open?id=14Jn1t9JxH-CxjfWy4JmVpCxkC9cDqqfE). Our pretraining procedure is basically the same as [this](https://arxiv.org/abs/1807.05960), trained on 64 training classes and validated on 16 validation classes. Model selection is based on nearest neighbor classifier's result (accuracy) on 16 validation classes. Please refer to [this](https://github.com/pytorch/examples/tree/master/imagenet) repo for your reference of implementation details.

_Originally posted by @hexiang-hu in https://github.com/Sha-Lab/FEAT/issues/1#issuecomment-450611114_

Hi hexiang,

I am planning to implement the pretraining by myself.
The repo you list seems not applicable for training a wide residual net used in the paper
Hence, I am not sure if I understand the pretraining procedure correctly.
Is it true that I just need to add a 640*64 linear layer after the embedding (model.endoder) and then train the 64-way classifier under the cross-entropy loss?

Any information will be greatly appreciated.
Thanks!"
"Hi~
The  reported result in the paper seems very competitive. But I have some troubles in reproducing them. 
Could you kindly share the settings (e.g. temperature) for achieving ProtoNet results reported in Table 1？

Also, I am a bit confused about the usage of temperature parameter.
In the paper, the temperature scalar is said to be used as
logits=  distances* temperature 
and temperature is chosen from {0.1, 1, 16, 32, 64, 128}.

But in the code, it seems to be used as 
logits =  distances/ temperature 
In this case, should the temperature still be chosen from {0.1, 1, 16, 32, 64, 128}?

Thanks!"
"Hi, I want to check result of trained model on single image. It will be really very helpful if you can provide code to directly predict the output class for input query image."
"Hello, I can only attain 29.44% accuracy when running `train_protonet.py` by the default setting, however, ProtoNet should get about 49.42% accuary according to the paper, do you run `train_protonet.py` by its default setting or should I set some hyperparameters manually? Any idea would be appreciated. "
Pre-training stage is used to initialize backbone neural networks. It may be necessary to achieve the reported performance. Can you provide implementation details of the pre-training stage or the source code. Thanks! 
"Hi,

Thank you for your great job!

I downloaded this for long time.
But the compressed file has error... I can't recognize which file (z01, z02,,,) is wrong

So, is it possible to get part of dataset?
I already see issue #58  but i can find your e-mail addr.
If possible, please let me know your e-mail!

Thanks."
"Hello STF team, 

First of all, thank you for your contribution, this is a amazing dataset. But I have a question, I'm wondering why there is missing detection in front of the vlp32c in many frames, I'm expecting unwanted detections of snowflakes and road reflection. 

![Screenshot from 2022-11-02 10-35-08](https://user-images.githubusercontent.com/20644522/199456252-1872cb0a-b841-406d-80c2-f8624b0f850b.png)

Best regards"
"The equation 1 in the article, what function is δ？"
"Has anyone successfully trained an object detector (yolov3, ssd, etc...) on just lidar_hdl64_last_stereo_left or lidar_hdl64_strong_stereo_left? I'm getting pretty trash results, which I believe is due to the sparsity of the points. Anyone got tips for training? "
"Hi,

to better understand the proposed foggification I had a look at the code but couldn't find where the lost points are actually discarded. The probabilities are computed but then only used for selecting points that are scattered.
To be precise, I'm talking about this method:
https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/094b95f64e1a1ed8a57e556984be16a8f8482cbc/tools/DatasetFoggification/lidar_foggification.py#L35-L112

In the end, `old_points` are returned although only the points with a distance larger than `dmax` were removed. Or am I misunderstanding the algorithm?

I also found this related issue: #21"
"Hi,
First of all, I have to say your work is really fascinating and the data you've provided is so wealthy and useful.
Moreover, I am highly interested in working on a topic relevant to this paper. So, the network architecture of this paper would be extremely helpful for its progress. I saw other comments with the same request and your response about improving it as well. I believe it would be so great if you share the network architecture code even before your intended improvement for many interested researchers.
Thanks!"
"I hava review the supplement documents and  relevant code。but I have some question :in algorithm1 ,just shows the area where the red box is located,may be somecode is lost .
![Snipaste_2020-11-02_10-57-31](https://user-images.githubusercontent.com/33310310/97825702-6792ae80-1cfa-11eb-8519-100b98885ed1.png)
"
"Hello, 

I read your paper and would like to reproduce it. However, the details of the architecture in Figure 4 are not clear. Could you please release the network script? If not, could you please provide a table of detailed network layers? 

Thank you. "
"I want to do research on FIR camera, where can get the groundtruths and calibration of FIR camera?thanks"
"Hi! I want to use your comprehensive dataset in my research, but I found it very difficult to download it because it is too big. Could you help me with a smaller dataset? It will help me so much! Thanks!"
"Dear Mario Bijelic, thanks for your great job! Recently, I am trying to use CyCADA to realize domain adaptation from clear winter captures to adverse weather scenes, as shown on your official website. However,  my CyCADA generates snow with poor results after training on the SeeingThroughFog for 70 epochs. So I would like to ask you for help. Could you tell me about your training steps and training parameter settings？

<img width=""460"" alt=""1662179604106"" src=""https://user-images.githubusercontent.com/76581571/188255953-9cfd7f4a-2eee-4e52-8cd7-890dfb7252ca.png"">

"
"I have registered and got a download link. But when when I input the provided user name(dXXXe) and password (BXXX-XXXX-XXXX-XXXY) ,it reminds me of the error."
"I have downloaded the files 01 to 18, how should I unpack them？

My file directory is as follows：
SeeingThroughFogCompressed.z01  SeeingThroughFogCompressed.z08  SeeingThroughFogCompressed.z15
SeeingThroughFogCompressed.z02  SeeingThroughFogCompressed.z09  SeeingThroughFogCompressed.z16
SeeingThroughFogCompressed.z03  SeeingThroughFogCompressed.z10  SeeingThroughFogCompressed.z17
SeeingThroughFogCompressed.z04  SeeingThroughFogCompressed.z11  SeeingThroughFogCompressed.z18
SeeingThroughFogCompressed.z05  SeeingThroughFogCompressed.z12  SeeingThroughFogCompressed.zip
SeeingThroughFogCompressed.z06  SeeingThroughFogCompressed.z13
SeeingThroughFogCompressed.z07  SeeingThroughFogCompressed.z14"
I'm not sure which images uesd for object detection are from the cam_stereo_left file or the cam_stereo_left_lut file？
"Dear people,

Thank you for the nice project and sharing the dataset.

I am trying to load the Dense dataset for a sensor fusion project. However, 

According to this [website](https://github.com/princeton-computational-imaging/SeeingThroughFog)  The data shall be under 

SeeingThroughFogData
    |-- cam_stereo_left

But I can not find any data.


How can I load the following data:


|-- SeeingThroughFogData
    |-- cam_stereo_left
        |-- *.tiff
        |-- ...
    |-- cam_stereo_left_lut
        |-- *.png
        |-- ...
    |-- lidar_hdl64_last
        |-- *.bin
        |-- ...
    |-- lidar_hdl64_strongest
        |-- *.bin
        |-- ...
    |-- ...


Regards, and thank you

"
"Hi,

Great work!
I'm interested in experimenting on the dataset, and I've got a link to download it.
However, I can't download any file.  It always shows empty files.  (It worked a few months ago but not now.)
Can someone help me with this?  Thanks!"
"Hi, thanks for the great work! Could you please check your Dense Dataset website? It's currently not available."
"Hello :)

Where can we download the Seeing through fog dataset instead of using the official website?

I tried to register but received the following e-mail:

```
Dear Sir or Madam,

Thank you very much for your message. I am currently out of office. 
I will reply as soon as possible. In urgent cases please contact my secretariat
or other known contact persons. 

Best regards

Klaus Dietmayer
```"
"Thanks for the great work!

I want to know how the depth maps are obtained in supplementary 5.1."
"@MarioBijelic 
First of all thank you for the dataset in adverse weather. I am planning to make use of the friction data provided in the dataset, however is there a way to map the friction data with location or GPS data ? 
Also I assume that the friction data corresponds to the friction values under vehicle and time synchronized with the image and velodyne data under the same name."
"Hi, many thanks for your great work!

As for the quatitative detection AP listed in your paper, which dataset is used? The daytime split, night split or both of them?
And in the training stage, both the daytime and the night data are used?

THX."
"Hi, thanks for your impressive work! The dataset is so well-collected with novel preprocessing tools developed.

I would like to use the dataset in my research work, but I registered and it said the download link will be sent to my email. But I did not received any link or email. I have also checked the junk box so it is not missed. Is there any problems? Thanks.

Btw, if the network mentioned in the paper ""Seeing through Fog"" can be released, it would be even more helpful for our to follow your work.

Looking forward to your reply, thanks"
read.py only retrieves up to the 26th element of the labels (i.e 'visibleLidar': kitti_properties[25] ). What is the 27th element supposed to be (kitti_properties[26])? visibleRadar?   
"In the supplementary material, it says that a homography map between gated and RGB was calculated to warp gated into the RGB plane. Is this transformation matrix provided anywhere? 

It also says that RGB was cropped into gated FOV. Was there any resizing done at all prior to cropping or was it just a simple crop? "
"Hello,

I am very impressed by your work in the SeeingThroughFog paper. I am working on training an SSD model using only Camera Stereo Images to learn from your work. I have found 14 different classes in the dataset ('LargeVehicle', 'PassengerCar_is_group', 'RidableVehicle', 'Pedestrian', 'Pedestrian_is_group', 'PassengerCar', 'Vehicle', 'Obstacle', 'RidableVehicle_is_group', 'Vehicle_is_group', 'LargeVehicle_is_group', 'DontCare', 'train', 'person'). I was trying to understand the label classes, but I have few questions. How many classes have you used for training? What is the difference between label class 'PassengerCar' and 'Vehicle'? What does the label 'Vehicle_is_group' exactly mean? 

Also, while generating TFRecord files for Lidar and Radar data, did you gave input as a projected 2D image?  

Thank you very much. Looking forward to hearing from you back!"
"Currently, the dataset is given with no documentation on what each folder contains. Can we add to the README.md a short description of what the data in each folder represents, and if its used in the paper or not? This would allow for students and researchers to use this dataset without trying to match back and forth between the paper/exploratory github code and the dataset (which currently has a bit of educated guesswork involved). 

For example, right now, I don't know what the difference is between the lidar_hdl64_strongest and lidar_hdl64_strongest_stereo_left. Likewise, gated_full_rect and gatedX_rect. This documentation would elucidate these types of questions.   

I think a good method to document this would simply be a table format like this: 

Folder | Used for Training | Used for Testing | Description |  
---|---|---|---|
cam_stereo_left | yes | yes | 8-bit RGB images captured from the left stereo cameras. cam_stereo_left_lut is derived from this...
fir_axis | no | no | 8-bit Thermal infrared images
gated0_rect | yes | yes | gated infrared images. Different from gated1_rect, gated2_rect and gated_full_rect in these ways: ....
... | ... | ...| ...|
FOLDER_X | ?| ?| Description of FOLDER_X |

If any of the author's would be interested in spending 15-20 min to guide me through this dataset, I would be more than happy to submit a PR for this addition myself.   "
"Hi,
If I would like to generate the calib files for each sample like in KITTI,:

`P0: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 0.000000000000e+00 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 0.000000000000e+00`

`P1: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 -3.797842000000e+02 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 0.000000000000e+00`

`P2: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 4.575831000000e+01 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 -3.454157000000e-01 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 4.981016000000e-03`

`P3: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 -3.341081000000e+02 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 2.330660000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 3.201153000000e-03`

`R0_rect: 9.999128000000e-01 1.009263000000e-02 -8.511932000000e-03 -1.012729000000e-02 9.999406000000e-01 -4.037671000000e-03 8.470675000000e-03 4.123522000000e-03 9.999556000000e-01`

`Tr_velo_to_cam: 6.927964000000e-03 -9.999722000000e-01 -2.757829000000e-03 -2.457729000000e-02 -1.162982000000e-03 2.749836000000e-03 -9.999955000000e-01 -6.127237000000e-02 9.999753000000e-01 6.931141000000e-03 -1.143899000000e-03 -3.321029000000e-01`

`Tr_imu_to_velo: 9.999976000000e-01 7.553071000000e-04 -2.035826000000e-03 -8.086759000000e-01 -7.854027000000e-04 9.998898000000e-01 -1.482298000000e-02 3.195559000000e-01 2.024406000000e-03 1.482454000000e-02 9.998881000000e-01 -7.997231000000e-01`


would I have to use the functions in DataViewerV2.py to generate these values?  Does the ` camera_to_velodyne` returned by the `load_calib_data` function correspond to `Tr_imu_to_velo`? 

If I am only using the lidar files from **lidar_hdl4_last_stereo_left** will I be able to iterate over the files and have a corresponding camera calib file and tf tree? 

Also, is there a reason some lidar files are `.npz` and some are `.bin`?


Thank you for the dataset!
"
"Hello, 

I am referring to label files from: cam_left_labels_TMP/

It would be helpful if I get the information about what these labels denote and how to read this file.

![Issue_Labels](https://user-images.githubusercontent.com/62149326/111175924-43d05680-857f-11eb-9be0-2f583dc9d04b.JPG)

"
"I was wondering which specific folders (cam_stereo_left, gated0_raw, etc...) contain the RGB, LIDAR, Gated, and Radar training/validation data in the paper. The overall download size is very large and the download speed is very slow, so I would like to avoid downloading irrelevant data. 

 Is there any README file or documentation describing what I'm asking?

Also, my download speed is roughly 1500 KB/s. Total download time for SeeingThroughFogCompressed.zip is projected to be a couple days. Is this to be expected? I'm downloading from the US west coast. Is there any quicker way to get the data?    "
"Hello, 

I am watching the Cloudstore Space for Dense Data, I am working on Seeing through Fog, and it seems from last week it is not accessible. Could you please confirm!"
"![image](https://user-images.githubusercontent.com/78135656/109511807-c6b8d200-7ae6-11eb-8e61-a976e7af49af.png)

First of all, I was impressed by the great research.
I would like to implement and use these models myself, but I do not know how to do it.

Did you label the fog point separately by making such an image?
I'd like to hear a specific explanation.

Also, what should I do if I want to use those models?
In addition, I wonder if LiDAR-Only SSD used 2D Detector or 3D Detector. If there is a ref code, can you tell me?


I'll wait for your answer. Thank you in advance.

"
"Hello! Thanks for your works.

I tried to download SeeingThroughFogCompressed from chrome or wget.
However, the download was said to be done before downloading about 5% of the dataset.
I have tried to download it a lot of times, but it didn't work.
Is there any way to reliably download the dataset?"
"Hi,
after registration at https://www.uni-ulm.de/en/in/driveu/projects/dense-datasets, I got an download url  in my mailbox. However, I can't download it. It reminded me that the file had been deleted or expired. Maybe you should check it. Thanks!"
"Hi,

since the strongest and last returns are stored in different files with different counts of points per point cloud I'm having trouble matching them together, to find out e.g. what the corresponding last return is for a strongest return yielded by the same laser beam. Is it possible to somehow reconstruct this information? 
I converted the position data to polar coordinates but noticed the points are not evenly distributed, and a series of points might overlap with another one (see below), so matching them by polar angles similarity won't work properly either.

![image](https://user-images.githubusercontent.com/1062119/105069624-80516b80-5a82-11eb-9bc0-c75d15c92f45.png)
"
"Hello, 

I tried the command, conda env create -f environment.yml, it is not working and says to restart the kernel after package installations repetitively. "
"Hello!

I am trying to project Lidar data onto the cam images, I have seen calibration files and your code on reading them, but it is still not that easy to understand on how should it work, the best I could make so far is this, but something is definitely missing :). The ones I got so far is this: 
![image](https://user-images.githubusercontent.com/10817016/102130171-cb0f1080-3e50-11eb-8f37-df168bfcf8b7.png)
![image](https://user-images.githubusercontent.com/10817016/102130196-d5c9a580-3e50-11eb-893e-3ee63c2891f7.png)

the right one seem to look like:
![image](https://user-images.githubusercontent.com/10817016/102130610-525c8400-3e51-11eb-960e-8d382883425f.png)

I am doing:

```python
scan = load_velodyne_scan(scan_path)
velodyne_to_camera, camera_to_velodyne, P, R, vtc, radar_to_camera, zero_to_camera = load_calib_data(calib_root, name_camera_calib, tftree)

ps = project_3d_to_2d(scan[:,:3].transpose(), vtc)

```
the first image I get is when I use `vtc` on ""lidar_hdl64_strongest""
the second is when I use `vtc` on ""lidar_vlp32_strongest"", which seems much better, but still with some shift.


Can you please give some more details on how to make projection with the calibration you have?

Thank you a lot in advance!

"
"Hello, 

I download the DENSE dataset but find its radar data only consists of discrete points. Do you have a plan to release raw radar data as in Fig.3 in your main paper? 

Thank you"
"Hi,
Since 3D BB annotations are in camera coordinate system, could you please let me know how to get the corresponding yaw angle 
for the objects in the lidar frame? I would like to know if there is any alignment difference in the forward looking axis of camera and lidar, ie the constant angle that needs to be added to ''rotz' in annotations. Also, please let me know what 'orient3d' means in annotations?

Best regards,
George"
"Hi! I was checking the attributes in the lidar pcd files. I understood that 5 attributes are being stored for each point. Other than (x,y,z), could you please let me know what the other two attributes are exactly? I noticed that one of the attribute has range [0,255], and the other [0,63]. 

Thanks and regards,
George Sebastian"
"There is an issue with frame_id `2018-02-04_10-02-19_00000` in the _validation_day_ split.

The LiDAR scan misses many points and the ground truth 3D boxes do not contain any points.

![image](https://user-images.githubusercontent.com/14181188/96347594-f7055400-10a2-11eb-920a-b86ced8a3378.png)

I will exclude this frame in my LiDAR only 3D object detection pipeline."
"Some labels apparently don't have information to draw the 3D bounding box in lidar coordinates (height, width, length, posx, posy, poz, and so on)

So instead of looking like this
`PassengerCar 0.00 1 -1 218.00 258.00 313.00 309.00 1.38 1.81 4.16 -11.19 1.31 64.94 -4.676 0.000 0.011 -3.106 1.00 0.0053058781 -0.0000952959 -0.9998246710 0.0179573322 True True True False`
they look like this
`PassengerCar 0.00 -1 -1 388.62 243.29 445.67 290.16 -1.00 -1.00 -1.00 -1000.00 -1000.00 -1000.00 -1.000 -1.000 -1.000 -1.000 1.00 -1000.0000000000 -1000.0000000000 -1000.0000000000 -1000.0000000000 -1 -1 -1 -1`
(it's all -1s and -1000s instead of actual information)

(the lines above are from `/gt_labels/gated_labels_TMP/2018-02-12_15-59-37_00070.txt` as an example)

Does anyone know if this is a bug or if this information is simply unavailable? 

Thanks in advance!"
"Hi! Radar targets have the following information: rcsLog, azimuthAngle_sc, rVelOverGroundOdo_sc, x_sc, y_sc, x_cc, y_cc and rDist_sc. I was wondering what the coordinate frames **sc** and **cc** in there mean. 

I'm particularly not sure whether I should use x/y_cc or x/ y_sc when comparing to **posx** and **posy** from the labels in gt_labels/cam_left_labels (e.g. to count how many radar points are inside a bounding box). 

In tools/DatasetViewer/lib/read.py, the function load_calib_data mentions four coordinate frames: _zero_, _camera_, _velodyne_ and _radar_. Can I assume that all label parameters are given in camera coordinates? And what is this _zero_ reference frame?

Thanks for the dataset and for the support!"
"I am trying to run DatasetViewer.

Even though the window with the images is running. 
I am getting many of these errors in the console:
```
Traceback (most recent call last):
  File ""/Users/kirill/.conda/envs/SeeingThroughFog/lib/python3.6/site-packages/pyqtgraph/opengl/GLViewWidget.py"", line 189, in paintGL
    self.setModelview()
  File ""/Users/kirill/.conda/envs/SeeingThroughFog/lib/python3.6/site-packages/pyqtgraph/opengl/GLViewWidget.py"", line 142, in setModelview
    m = self.viewMatrix()
  File ""/Users/kirill/.conda/envs/SeeingThroughFog/lib/python3.6/site-packages/pyqtgraph/opengl/GLViewWidget.py"", line 152, in viewMatrix
    tr.translate(-center.x(), -center.y(), -center.z())
AttributeError: 'list' object has no attribute 'x'
```

Looks like something works wrong. No idea how to fix it. I have tried a few versions of pyopengl, but it didn't help"
"I suggest removing the following scenes from the split files because they do not contain any labels.
```
2018-02-09_14-51-35,00500
2018-02-05_13-01-37,00300
2018-02-09_09-45-12,00100
2018-02-05_13-01-37,00100
2018-02-05_12-48-21,00200
2018-02-09_14-51-35,00600
2018-02-04_14-31-18,00100
2018-02-06_17-24-50,00000
2018-02-09_18-49-42,00200
2018-02-03_23-02-24,00100
2018-12-10_07-23-55,00500
2018-12-09_10-45-12,00200
2018-10-08_08-10-40,01310
2018-03-15_09-29-41,00050
2018-03-15_09-28-52,00200
2018-10-08_08-10-40,01300
2018-03-15_09-42-29,00100
2018-03-15_09-42-29,00510
2018-10-29_15-09-31,01400
2019-01-09_10-55-25,01400
2018-10-29_16-00-52,02300
2018-10-29_15-37-43,01400
2018-10-08_08-10-40,02550
2018-10-29_16-00-52,00880
2018-10-29_16-00-52,02440
2018-10-29_15-46-53,02600
2018-10-29_15-02-37,00160
2018-12-10_10-51-06,00170
2018-02-04_11-24-11,00000
2019-01-09_14-50-13,00800
2018-12-10_10-51-06,00150
2018-02-04_11-27-05,00200
2018-02-07_12-04-45,00200
2019-01-09_10-55-25,02200
2018-02-06_14-33-00,00600
2019-01-09_11-15-11,03800
2018-12-10_10-51-06,00130
2018-02-07_11-55-33,00500
2019-01-09_13-42-11,00000
2019-01-09_14-54-03,02900
2019-01-09_10-55-25,02000
2018-02-12_11-00-35,00560
2018-02-04_21-47-40,00100
2018-02-04_21-22-55,00300
2018-02-04_21-37-51,00200
2018-02-07_18-20-02,00320
2018-02-07_18-20-02,00310
2018-02-07_18-20-02,00300
2018-02-05_22-27-19,00200
2018-02-12_09-17-17,00600
2018-12-10_08-22-28,00800
2018-12-10_09-39-47,00000
2018-02-05_22-27-19,00300
2018-02-12_08-57-46,00700
2018-02-12_08-57-46,00100
2018-12-17_19-33-05,02800
2018-02-08_17-42-24,00400
2018-02-12_06-46-13,00400
2018-12-17_07-36-29,01240
2018-12-11_14-42-41,00200
2018-12-17_19-27-00,01200
2018-12-10_10-04-35,00500
2018-02-04_11-15-24,00400
2018-10-08_08-10-40,00620
2018-12-10_10-51-06,00140
```"
"Hi, I was wondering if there are labels for the FIR data. I haven't found any in the folders. We've tried overlaying the RGB labels after normalizing them to the FIR image size. However, it seems like there is an offset, perhaps due to the placement of the FIR cameras. 

How did you guys handle this in the paper? "
"In the supplemental material, you guys mentioned height as part of the LIDAR input. Do you guys have a script to calculate this from the LIDAR data? Currently, it seems like it is only depth and pulse intensity. Thanks!"
"Hello, I have a question about your dataset (This is not a real issue)
DENSE dataset is super attractive so that i really want to use this dataset.
BTW, your dataset includes thermal dataset in 8 bit, however, Do you have 14 bit format Thermal data?
14 bit thermal data contains more detailed data, thus, i am really curious about this.
If you have, Could you share the dataset?

-PS. DENSE dataset is really cool I'm trying to do calibrate thermal data with stereo image for my research"
"Those 3 lines should be changed to `bool(kitti_properties[*]),`
otherwise displaying the labels won't work.

https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/5de1c423e0239eef3e84fa402885e7f2a5b0a10e/tools/DatasetViewer/lib/read.py#L48
https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/5de1c423e0239eef3e84fa402885e7f2a5b0a10e/tools/DatasetViewer/lib/read.py#L49
https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/5de1c423e0239eef3e84fa402885e7f2a5b0a10e/tools/DatasetViewer/lib/read.py#L50

Also, I noticed, that there is another `bool` value a.k.a. `kitti_properties[26]` in the annotation files.
Does this last value stand for `visibleRadar` or what does this last value in the annotation files stand for?"
"Where can I find/get`timestamps.json` for `--path_timestamps`? 
https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/5de1c423e0239eef3e84fa402885e7f2a5b0a10e/tools/DatasetViewer/DataViewer_V2.py#L25

**Problem when reading matching file for AdverseWeather2Algolux!**

```
python-BaseException
Traceback (most recent call last):
  File ""/scratch_net/hox/mhahner/apps/pycharm-2020.1.2/plugins/python/helpers/pydev/pydevd.py"", line 1448, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/scratch_net/hox/mhahner/apps/pycharm-2020.1.2/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/scratch_net/hox/mhahner/repositories/SeeingThroughFog/tools/DatasetViewer/DataViewer_V2.py"", line 886, in <module>
    main(args)
  File ""/scratch_net/hox/mhahner/repositories/SeeingThroughFog/tools/DatasetViewer/DataViewer_V2.py"", line 867, in main
    DatasetViewer(root_dir, topics, timedelays, can_speed_topic, can_steering_angle_topic,
UnboundLocalError: local variable 'timedelays' referenced before assignment
```"
"Hi, 
unfortunately, I can not successfully unzip the files within `FogchamberDataset` and `SeeingThroughFogCompressed`.

To make sure that the download was not corrupted on my side, I already downloaded `FogchamberDataset.zip` and `SeeingThroughFogCompressed.zip` multiple times from your server.

While unzipping `FogchamberDataset.zip` and `SeeingThroughFogCompressed.zip` itself works fine, 
unzipping the files within it's extracted subfolders doesn't work.

When trying to unzip those, I get the following message for the archives in `FogchamberDataset`:
```bash
4 archives were successfully processed.
7 archives had fatal errors.
```

and this for the archives in `SeeingThroughFogCompressed`:
```bash
10 archives were successfully processed.
1 archive had warnings but no fatal errors.
32 archives had fatal errors.
```


I think you should be able to reproduce a ""fatal error"" e.g. when you do 
`unzip lidar_hdl64_strongest.zip` in `FogchamberDataset/lidar_hdl64_strongest`.

<img width=""396"" alt=""grafik"" src=""https://user-images.githubusercontent.com/14181188/89998780-4471d900-dc8e-11ea-9801-60246b9533d1.png"">

This is the trace I get:
```bash
Archive:  lidar_hdl64_strongest.zip
warning [lidar_hdl64_strongest.zip]:  zipfile claims to be last disk of a multi-part archive;
  attempting to process anyway, assuming all parts have been concatenated
  together in order.  Expect ""errors"" and warnings...true multi-part support
  doesn't exist yet (coming soon).
file #1:  bad zipfile offset (local header sig):  4
file #2:  bad zipfile offset (local header sig):  84
file #3:  bad zipfile offset (local header sig):  1292455
file #4:  bad zipfile offset (local header sig):  2585819
file #5:  bad zipfile offset (local header sig):  3877631
file #6:  bad zipfile offset (local header sig):  5169771
file #7:  bad zipfile offset (local header sig):  6462841
file #8:  bad zipfile offset (local header sig):  7755507
file #9:  bad zipfile offset (local header sig):  9046452
file #10:  bad zipfile offset (local header sig):  10341663
file #11:  bad zipfile offset (local header sig):  11633918
file #12:  bad zipfile offset (local header sig):  12929252
file #13:  bad zipfile offset (local header sig):  14225869
file #14:  bad zipfile offset (local header sig):  15518351
file #15:  bad zipfile offset (local header sig):  16816588
file #16:  bad zipfile offset (local header sig):  18111883
file #17:  bad zipfile offset (local header sig):  19405226
file #18:  bad zipfile offset (local header sig):  20698401
file #19:  bad zipfile offset (local header sig):  21990109
file #20:  bad zipfile offset (local header sig):  23281287
file #21:  bad zipfile offset (local header sig):  24574933
file #22:  bad zipfile offset (local header sig):  25868771
file #23:  bad zipfile offset (local header sig):  27161983
file #24:  bad zipfile offset (local header sig):  28453010
file #25:  bad zipfile offset (local header sig):  29745940
file #26:  bad zipfile offset (local header sig):  31039964
file #27:  bad zipfile offset (local header sig):  32334045
file #28:  bad zipfile offset (local header sig):  33629028
file #29:  bad zipfile offset (local header sig):  34923028
file #30:  bad zipfile offset (local header sig):  36218481
file #31:  bad zipfile offset (local header sig):  37512234
file #32:  bad zipfile offset (local header sig):  38810796
file #33:  bad zipfile offset (local header sig):  40106979
file #34:  bad zipfile offset (local header sig):  41401521
file #35:  bad zipfile offset (local header sig):  42699155
file #36:  bad zipfile offset (local header sig):  43992782
file #37:  bad zipfile offset (local header sig):  45287246
file #38:  bad zipfile offset (local header sig):  46581336
file #39:  bad zipfile offset (local header sig):  47873381
file #40:  bad zipfile offset (local header sig):  49170729
file #41:  bad zipfile offset (local header sig):  50466394
file #42:  bad zipfile offset (local header sig):  51766117
file #43:  bad zipfile offset (local header sig):  53062799
file #44:  bad zipfile offset (local header sig):  54360313
file #45:  bad zipfile offset (local header sig):  55654613
file #46:  bad zipfile offset (local header sig):  56949106
file #47:  bad zipfile offset (local header sig):  58244705
file #48:  bad zipfile offset (local header sig):  59539629
file #49:  bad zipfile offset (local header sig):  60835290
file #50:  bad zipfile offset (local header sig):  62129307
file #51:  bad zipfile offset (local header sig):  63422834
file #52:  bad zipfile offset (local header sig):  64718391
file #53:  bad zipfile offset (local header sig):  66013988
file #54:  bad zipfile offset (local header sig):  67311345
file #55:  bad zipfile offset (local header sig):  68606532
file #56:  bad zipfile offset (local header sig):  69899595
file #57:  bad zipfile offset (local header sig):  71192095
file #58:  bad zipfile offset (local header sig):  72485408
file #59:  bad zipfile offset (local header sig):  73781890
file #60:  bad zipfile offset (local header sig):  75075232
file #61:  bad zipfile offset (local header sig):  76376460
file #62:  bad zipfile offset (local header sig):  77671315
file #63:  bad zipfile offset (local header sig):  78964356
file #64:  bad zipfile offset (local header sig):  80256813
file #65:  bad zipfile offset (local header sig):  81549048
file #66:  bad zipfile offset (local header sig):  82844999
file #67:  bad zipfile offset (local header sig):  84137137
file #68:  bad zipfile offset (local header sig):  85437263
file #69:  bad zipfile offset (local header sig):  86731938
file #70:  bad zipfile offset (local header sig):  88028050
file #71:  bad zipfile offset (local header sig):  89325220
file #72:  bad zipfile offset (local header sig):  90617851
file #73:  bad zipfile offset (local header sig):  91915628
file #74:  bad zipfile offset (local header sig):  93210948
file #75:  bad zipfile offset (local header sig):  94503290
file #76:  bad zipfile offset (local header sig):  95796339
file #77:  bad zipfile offset (local header sig):  97087997
file #78:  bad zipfile offset (local header sig):  98383722
file #79:  bad zipfile offset (local header sig):  99678481
file #80:  bad zipfile offset (local header sig):  100973162
file #81:  bad zipfile offset (local header sig):  102270798
file #82:  bad zipfile offset (local header sig):  103566676
file #83:  bad zipfile offset (local header sig):  104860820
file #84:  bad zipfile offset (local header sig):  106155307
file #85:  bad zipfile offset (local header sig):  107451449
file #86:  bad zipfile offset (local header sig):  108749420
file #87:  bad zipfile offset (local header sig):  110054951
file #88:  bad zipfile offset (local header sig):  111346346
file #89:  bad zipfile offset (local header sig):  112641992
file #90:  bad zipfile offset (local header sig):  113894838
file #91:  bad zipfile offset (local header sig):  115147796
file #92:  bad zipfile offset (local header sig):  116399960
file #93:  bad zipfile offset (local header sig):  117654099
file #94:  bad zipfile offset (local header sig):  118910916
file #95:  bad zipfile offset (local header sig):  120168276
file #96:  bad zipfile offset (local header sig):  121429930
file #97:  bad zipfile offset (local header sig):  122694434
file #98:  bad zipfile offset (local header sig):  123955740
file #99:  bad zipfile offset (local header sig):  125219836
file #100:  bad zipfile offset (local header sig):  126479615
file #101:  bad zipfile offset (local header sig):  127734190
file #102:  bad zipfile offset (local header sig):  128988789
file #103:  bad zipfile offset (local header sig):  130241864
file #104:  bad zipfile offset (local header sig):  131497036
file #105:  bad zipfile offset (local header sig):  132754659
file #106:  bad zipfile offset (local header sig):  134011773
file #107:  bad zipfile offset (local header sig):  135268425
file #108:  bad zipfile offset (local header sig):  136526702
file #109:  bad zipfile offset (local header sig):  137790606
file #110:  bad zipfile offset (local header sig):  139056533
file #111:  bad zipfile offset (local header sig):  140327818
file #112:  bad zipfile offset (local header sig):  141598340
file #113:  bad zipfile offset (local header sig):  142868462
file #114:  bad zipfile offset (local header sig):  144135196
file #115:  bad zipfile offset (local header sig):  145403230
file #116:  bad zipfile offset (local header sig):  146680238
error: invalid zip file with overlapped components (possible zip bomb)
```

This is my setup:
```bash
$ hostnamectl

  Operating System: Debian GNU/Linux 9 (stretch)
            Kernel: Linux 4.9.0-13-amd64
      Architecture: x86-64

$ unzip --version

UnZip 6.00 of 20 April 2009, by Debian. Original by Info-ZIP.
```"
"Hello, 

I download some HDL64 and VLP32 Lidar data from the DENSE datasets. Each data file consists of 5 columns where the first 3 columns are x/y/z coordinates and the 4th column is intensity. What is the meaning of the 5th column? 

I have a VLP32 Lidar and would like to apply fortification to its data frames. Have you measured the parameters g and n for VLP32 Lidar? If not, is it possible to derive them from the matching data frames of HDL64 and VLP32 Lidar and the known parameters of the HDL64 Lidar?

Thank you and looking forward to your reply. "
"Hi Mario, 
when I want to use your conda environment, I, unfortunately, run in the following error.

```shell
Collecting pyqt5==5.14.1 (from -r /scratch_net/hox/mhahner/repositories/SeeingThroughFog/condaenv.6lohyfv7.requirements.txt (line 1
3))
  Using cached https://files.pythonhosted.org/packages/3a/fb/eb51731f2dc7c22d8e1a63ba88fb702727b324c6352183a32f27f73b8116/PyQt5-5.1
4.1.tar.gz        
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):                                                                                            
      File ""<string>"", line 1, in <module>
      File ""/home/mhahner/scratch/apps/anaconda3/envs/LabelTool/lib/python3.7/tokenize.py"", line 447, in open
        buffer = _builtin_open(filename, 'rb')
    FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pip-install-ptb8ajni/pyqt5/setup.py'
                 
    ----------------------------------------
                                                                           
Pip subprocess error:
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-ptb8ajni/pyqt5/
 

CondaEnvException: Pip failed
```

Removing the following line from the yaml file also did not help.

https://github.com/princeton-computational-imaging/SeeingThroughFog/blob/46acec916243d5db4dd115d59369043733f6e2bd/environment.yml#L58

It should be removed in general I think because it is user-specific, no?

Also, the file itself should be renamed to environment.y**a**ml to be concise with the readme.

I am using the latest conda 4.8.3.

P.S.: Thanks for this contribution to the research community of adverse wheater."
"Hi, thanks for nice research and code upload.
I viewed the repository, but couldn't find the main train & estimation code.
where is it ? I'm sorry If I missed something. "
"Hi thanks for this dataset! 

I am trying to figure out where RGB images and 2D bounding box labels are located in the download folder.
I see the following contents after downloading `SeeingThroughFogCompressed`. 
```
calib_cam_stereo_left.json
calib_gated_bwv.json
calib_tf_tree_full.json
filtered_relevant_can_data.zip
gated_full_acc_rect.zip
gt_labels_cmore_copied_together.zip
labeltool_labels.zip
lidar_hdl64_last_gated.zip
lidar_hdl64_last_stereo_left.zip
lidar_hdl64_strongest_gated.zip
lidar_hdl64_strongest_stereo_left.zip
radar_targets.zip
road_friction.zip
weather_station.zip
```"
"I followed the installation instructions, but kept getting the following message.


`[DEBUG] Loading application... integrity failure`


There was no problem with my CUDA settings, and tensorflow was picking up my GPU just fine, so I tried taking stuff out of try/exceptions and found out that keras was failing on loading the models.


    original_keras_version = f.attrs['keras_version'].decode('utf8')
AttributeError: 'str' object has no attribute 'decode'


Googled it, and found that it works at h5py version 2.10.0.
I think it would be nice if h5py versions were also included in conda settings.

Thank you anyways for your awesome work.
I'll try it out now since my bug is fixed.
"
"  Hello, after reading your paper, I have some questions about the input of FANnet.
  In the paper, one of the inputs to FANnet is ""**_a one-hot encoding v of length 26 of the target character_**"", where 26 represents 26 capital letters. Have you ever tried to change 26 to 62 (uppercase letters + lowercase letters + numbers), because the same font theoretically has some commonalities, so they should also be able to complete the font style transfer. I am interested in this question. Do you have any comments or suggestions?"
"I appreciate your excellent work on text editing. I tried to run FANnet with pretrained model on your datasets.  So I downloaded the pretrained weights from [here](https://drive.google.com/drive/folders/16-mq3MOR1zmOsxNgegRmGDeVRyeyQ0_H) and datasets from [here](https://drive.google.com/drive/folders/1dOl4_yk2x-LTHwgKBykxHQpmqDvqlkab) following README.

To generate results using the valid set as the input, I modified `fannet.py` and  ran the following code
```python
from skimage.metrics import structural_similarity as ssim
for data in valid_datagen.flow():
    [x, onehot], y = data
    out = fannet.predict([x, onehot]) 
    n = x.shape[0]
    for i in range(n):
        _x = x[i].reshape(64, 64)
        _gt = y[i].reshape(64, 64)
        _out = out[i].reshape(64, 64)
        _, _out_bin = cv2.threshold(_out,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
        sv = ssim(_gt, _out_bin, data_range=255, gaussian_weights=True, sigma=1.5, use_sample_covariance=False)
        print(sv)
```
 But the SSIM value `sv` is far from what was claimed in the paper. I also tried to calculate the average SSIM w.r.t different source characters. There is also a large gap. So I am wondering if there exists some mistakes when running the model or just the SSIM calculation. 
"
"Hello, I noticed that in the stefann.py file, you have provided three methods for style transfer, transfer_color_pal, transfer_color_max, and the colornet implementation. I uncommented the lines from 480-487 in the stefann.py file to use the colornet model. When I tested the application on images from 'sample_images' folder using the colornet method with the provided pretrained weights, it did not work very well and produced blurry and inconsistent results. The results of colornet did not match the results given in the 'editing_examples' folder.

Given Result (Left - Original Image, Right - stefann generated image)
![08](https://user-images.githubusercontent.com/35864383/115967328-b275dc00-a54f-11eb-8850-0c14e2e71b0e.jpg)
Output using colornet
![08_colornet](https://user-images.githubusercontent.com/35864383/115967267-514e0880-a54f-11eb-8634-d6ef9ad633b6.jpg)

There are many more cases where the colornet model is not performing as expected. Could you please help me with this? "
"Hi, thanks for your hard work on this project. It's really cool!
I've seen issue #7  but I still have some doubts. I would like to try to replace english text with it's corresponding chinese translation, but how can I do so if characters are stored in jpg file named as ASCII numbers? Chinese it's not included in ASCII.
Another question regarding chinese is, do I also need to generate new images, one for each character, in the colornet directory?
Your help would be much appreciated!"
"Hi,

Thanks for sharing your awesome work! I am wondering if you have a pretrained model to edit numbers? Thanks."
"Now I'm playing around with Stefann a bit. Your [readme](https://github.com/prasunroy/stefann/blob/master/release/readme.html) was really helpful here! But I guess there's more knobs and dials to it than I expected. Was still wondering is there e.g. a way to make the word you're replacing longer? Also, your examples seem way more crips than my quick shot at it. Got any tips?

Original:
![backspace orig](https://user-images.githubusercontent.com/2487783/90815259-b4393100-e32a-11ea-80d6-192b58bd8d9e.jpg)
Edited
![backspace-button_2020-08-20-21-15-52](https://user-images.githubusercontent.com/2487783/90815382-e77bc000-e32a-11ea-8f33-c7301550daf3.jpg)
"
"Hi,

I was trying out your project but I'm stuck installing dependencies. I tried the conda installation method but it keeps failing (tried cpu, gpu and osx yamls). 
`[DEBUG] Loading application... integrity failure`

I also think the yaml file here 
https://github.com/prasunroy/stefann/blob/acac8fec6985e5e651313472738dbf36bbc27199/release/env_cpu.yml#L12
and here is faulty:
https://github.com/prasunroy/stefann/blob/acac8fec6985e5e651313472738dbf36bbc27199/release/env_gpu.yml#L12

I guess it should look more like the OSX version with a double `==` and tensorflow below pip.
https://github.com/prasunroy/stefann/blob/acac8fec6985e5e651313472738dbf36bbc27199/release/env_osx.yml#L10-L15
However even when I edit the env_cpu.yaml that way it seems to fail.

It could be related to this issue: https://github.com/tensorflow/tensorflow/issues/37316
but when I tried `tensorflow>=2.1.0 ` it also failed.

Any idea how to solve this? Or is there an alternative way to install it? A docker container maybe?"
"Thank you very much for your contribution in this field.
I still have some questions about the character replacement part. As you said in the paper, when training fannet and colornet, the input character size is 64*64, but when performing character replacement in a real scene, some characters are much larger in size 64*64 (for example, 128*128), how did you solve this situation? Is it to directly resize the generated characters to 128*128 or some other way?"
"Hello, prasunroy. Thank you for the amazing work. I tried to train fannet by `python stefann/fannet.py` following your codes, but got `ValueError: No gradients provided for any variable` and it stopped training.
Could you give me a solution to fix the bug？

![1618383906(1)](https://user-images.githubusercontent.com/39898699/114668551-76fb3680-9d33-11eb-8066-66958b0667b1.jpg)
"
"Is it necessary to restart the program after processing one image?
How to run it continuously?"
"After I run the command `python stefann.py` and open an image, what are the steps to replace the characters in that image?"
"When I run the code, it is reporting the error `integrity failure`."
"@mauromazzei @prasunroy 
What about languages like Arabic & Hebrew?"
"Hi,

Thanks for sharing your code. I am trying to reproduce your results, how many epochs does it take to converge to results similar to what you shared in your paper ?

Thanks"
""
"Hi,

Thanks for sharing the implementation! I'm wondering that how many epochs you used for training LSUN church?

Thanks"
"Hi, I am new to python and machine learning. I find this idea very useful to calculate the gradient at not just the final layer but at the intermediate layers of GAN too. If I am not wrong it means that you calculate loss between let's say: Gen3 (generator intermediate layer 3) and Dis3 (discriminator intermediate layer 3) and calculate the gradient update for that layer only. In simple words the loss is being calculated at each layer. Please correct me if I am wrong.

The problem is I am unable to find this implementation in the code provided. Could you please help in understaing the confusion? "
"I'm currently working on an own implementation of your great paper during which I came up with the question, if the second generator pass in each training step is actually necessary. What I mean is, that we generate fake samples once to optimize D and then once again to do so for G. But for the second case, the weights of G actually haven't changed so the drawn fakes won't differ either. It would be different, if we used different latent samples for each pass, but this does not seem to be the case.

From my current understanding, we could simply retain the gradients from the first pass and avoid one costly generator call, reducing the computational resources by 25% or something per training iteration. But I have to admit, that I'm more familiar with your PyTorch implementation of the paper, which I took as a reference, to clarify on this question. Nonetheless, I'm asking here, as it seems to be ""official"" repository."
"tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation G_synthesis/noise0/Initializer/random_normal/RandomStandardNormal: Could not satisfy explicit device specification '' because the node {{colocation_node G_synthesis/noise0/Initializer/random_normal/RandomStandardNormal}} was colocated with a group of nodes that required incompatible device '/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_CPU:0]. 
"
"Hi,
    Thanks for creating this repository! I'm implementing the MSG-GAN method for my own project and I'm trying to use WGAN-GP loss, but I'm finding it difficult since there are multiple images. Could you point to the WGAN-GP loss implementation with multiple images in this repo so that I can use it for reference?
Thanks,
Tharun Mohandoss"
"Hi， I am using your MSG-GAN code ， and it very human-readable and good for user
I tried to read the msg-stylegan but， tensorflow version is so difficult to read ，maybe it's efficient？？

If you have proved the effectiveness of the method, I wonder if it is convenient to provide pytorch version of the code for more people to use

Thank you"
"![image](https://user-images.githubusercontent.com/16167043/78984479-e787da00-7b58-11ea-969c-16200d39ba04.png)
"
"![image](https://user-images.githubusercontent.com/16167043/78979231-02088600-7b4e-11ea-82c0-e113f76f694b.png)
![image](https://user-images.githubusercontent.com/16167043/78979269-16e51980-7b4e-11ea-8f39-a735cf1c817c.png)
"
"Dear all,
I would like to kindly ask further explanation about the architectural design of the network, for instance:
- Why are there multiple skip connections and how the whole thing is trained? 
- Do I have to concatenate both the multi-scale layers from the generator and random real images instances while evaluating the discriminator? How do I do that?
- There's a ""combinational layer"" in yellow, what's that?
I have indeed read the original paper but still, there are some things that I cannot understand so I hope you will kindly answer me.

Looking forward to hearing from you soon.

Sincerely"
I want to use the pre-trained model but I am not able to do so using CPU. I don't have a GPU and its raising error.  
"Hello, I tried to resume my training from the previous learning, but cannot make it. Please can you give me a brief guide? Thank you! :)
"
To test the model can i give the input my images which are not in the train dataset will that work?
"Hello, we are currently working on motion prediction problems using nuScenes dataset.

We are interested in the nuScenes dataset because it has rich semantic information of the environment in its map expansion with 11 layers of non-geometric diving data. One thing we are currently looking into is the lane data of the map, especially the lane connectivity. What we found is that nuScenes has the concept of “lane_connector”, which is used to connect lanes in special road areas like intersections. However, this lane_connector has rather less semantic information, and current map_api in nuscenes dev-kit does not have a good support for visualizing the lane_connector.

We made some changes in the map api to visualize the polygon associated with the lane_connector. It seems to us that the shape of the lane connector is rather like a patch box, not a road element. 

So we have three questions as shown below.
1. Do you have an idea that how the lane_connector is generated ?
2. How to generate the center lines of lane_connector as normal lane? (For normal lane, it has get_arcline_path function to get centerlines or baseline paths)
3. Can lane_connector represents turning information, we notice that in the intersection area, one lane can have two lane connectors (Does this mean that these two lane connectors are representing two different turning directions?)

Great Thanks"
"Download link: https://www.nuscenes.org/download, nuScenes-lidarseg, All, Asia

Error accurred when running the first block in the tutorial: [nuscenes-devkit](https://github.com/nutonomy/nuscenes-devkit)/[python-sdk](https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk)/[tutorials](https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/tutorials)/nuscenes_lidarseg_panoptic_tutorial.ipynb:

```
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)
```

says

```
FileNotFoundError: [Errno 2] No such file or directory: './v1.0-mini/attribute.json'
```"
"Hello, thanks to your excellent work.
I can't find the entrance of submit about test set (3D Object Detection) in the nuScenes websit. Can you tell me how to submit to the nuScenes or the detailed entrance link?
Thanks."
"create a Box object results in an error occurring at the following line

https://github.com/nutonomy/nuscenes-devkit/blob/28765b8477dbd3331bacd922fada867c2c4db1d7/python-sdk/nuscenes/utils/data_classes.py#L553

the score is a vector, np.isnan(score) returns a vector. condition on a vector of bools is invalid."
"Hi, 
I examined that some frames have bad (wrong) lidar annotations regarding measurement points, which are invalid (for instance the laser beam didn't come back as for very remote targets or when hitting very reflective surfaces such as car windows.

The NuScene data set is dealing with invalid data (e.g. measurements which did not return) by labelling them with 31 (ego_vehicle).
In fact points belonging to those invalid measurements have a radius close to zero.

Here is an example from ""scene-1073"", sample 4:
- Upper image: plotting the log10 of the radii of lidar measurements using phi-theta coordinates (1084-1087 phi points, 32 theta points),
Black spots correspond to invalid data as radius is below 10^-4 meter 
- Lower image: annotation of ego (1) and vegetation (2) is plotted).

![grafik](https://user-images.githubusercontent.com/67096719/203047999-8265ab8f-d15b-4b73-aad2-f8f63bbdb89f.png)

From the image it is clear, that in the upper part lidar is shooting in the sky black spots. The labels here should not be vegetation but Ego vehicle (or the class corresponding to invalid data).

This is sample 1 of the same scene, providing a correct (consistent) annotation
![grafik](https://user-images.githubusercontent.com/67096719/203050730-c1b107c2-ee2f-4421-8286-10d92f822799.png)
Note that the radii of non-valid points (ego_vehicle) are not scaled down to 10^-5 in this sample

"
"Greetings, The lateral velocity of the ego vehicle is always given as zero, even when we have significant lateral acceleration as shown by the IMU and in the pose messages. Is this an error or I am missing something here?   Thanks in advance.    "
"As in followed link, mmdetection3d uses the negative of yaw when converting to nuscenes-format bbox in camera cooridnate system. Does nuscenes define yaw in camera coordinate system in another fashion?
[https://github.com/open-mmlab/mmdetection3d/blob/962fc830daebad3f93fc7cbf970728057c64977b/mmdet3d/datasets/nuscenes_mono_dataset.py#L713](url)

Considering following scenario, an object in front of a camera driving towards, in mmdetection3d the yaw should be negative 0.5pi. Is it negative 0.5pi in nuscenes?"
"Hi,

Thanks for the great dataset.

It seems the global coordinate corresponds to the each map, like 0,0,0 is the top left of the map.

So, I am wondering how to transform the global coordinate (x, y, z) to the world coordinate, like longitude and latitude.

Also, the z value of the map is 0, I think we will never get the height/altitude info, right?"
""
"        Hi. See also https://forum.nuscenes.org/t/problem-with-conversion-from-nuscenes-into-kitti/548. 
In general the code was not designed to work with the different splits *separately*. They are supposed to all be merged into one folder and likewise there is only one annotation (v1.0-trainval) for all of them. Hence you would need to adjust the code to only operate on the images you are interested in.

_Originally posted by @holger-motional in https://github.com/nutonomy/nuscenes-devkit/issues/583#issuecomment-820201814_
      For e.g in Full dataset (v1.0) : Trainval, if I want to use only the part 1 file blobs of 85 scenes, what changes do I have to make so I dont get this error and where should those changes be? I am trying my best to figure it out, I would really appreciate if anyone could guide me through the process."
"Hello, does the training data of nuImages have intersection with the val and test sets of nuscenes? If I want to do pre-training on nuImages and val/test on nuScenes, can I directly use the official split of nuImages?
"
"hi

other modules can be found. just not bitmap. 

can you help? thanks!

Tao"
"Hi, I recently successfully made tracking result to run the evaluation code.(which is validation set)

I actually want to visualize in a specific time as an image file.(camera)
How can I visualize the result as an image file for both data which is my tracking result(json file) and the data provided?

I'd be grateful if you reply.
Thanks.
"
"Hi, 

I wanted to load the v1.0 trainval for evaluation, but cannot find attribute.json file. I checked the folder and can only find category.json and panoptic.json. 

Can you help? thanks!

Tao"
"Hey there!
We're working on a project at the Technical University of Munich that involves segmentation of driving scenarios images and trying to deal with the problem of bad weather conditions.

Is there any way to select the rainy / snowy images from the nuImages dataset?

Thanks!
"
I observe it is mentioned in the challenge that annotations are not provided for the test split. The `sample_annotation.json` is also an empty list in `v1.0-test`. But sample annotation includes all the surrounding vehicles' information. Does that mean I am expected to train a model that ignore interaction with neighbor vehicles since the neighbor vehicles information won't even be available during the test?
"I checked out [prediction tutorial](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/prediction_tutorial.ipynb), and tried to run
`nuscenes = NuScenes('v1.0-test', dataroot=DATAROOT)`
`get_prediction_challenge_split(""test"", dataroot=DATAROOT)`
and got error. I saw `create_splits_scenes()` used in `get_prediction_challenge_split` can return test split, but when I tried to modify `get_prediction_challenge_split` to accept `split` as `'test'`, the output `list(chain.from_iterable(token_list_for_scenes))` is an empty list. 

I assume we should train on 'v1.0-trainval' and then evaluate on 'v1.0-test'. I did not see anywhere teaching how to load test data from 'v1.0-test' in order to load input and generate submissions for the prediction challenge.
"
"![image](https://user-images.githubusercontent.com/25739094/197140620-3705edd5-bd13-4441-b1a6-c1bfa34bd35e.png)
https://github.com/open-mmlab/mmdetection3d/blob/14c5ded49dd5df7a52368bd54c32a003c2521e1d/tools/data_converter/nuscenes_converter.py#L237
I find here exist annotations for object without lidar or radar points，how these annotations captured? Thank you !"
"Hello, could you share the definition of the 3D box yaw?

I collected a dataset, and don't know how to calculate yaw by a (8, 3) points array."
"Hi I have some issues about evaluating results.

I actually already used evaluation.py file using my tracking result json file. But Is there any ways that I can see results with respect to each scene?"
"I had registered nuscenes account before and downloaded the dataset many times successfully. However, when I try to download the data today, there is no reaction when I click the asia or us button. I  used a new browser, utilized another computer, and even registered a new account but all failed. May you tell me how to solve this problem. Thanks!"
"Hi, I was wondering what is the field of view of individual camera?
Thanks!"
"Hi! Since the frequency of Lidar (20hz) is higher than the cameras (12hz), is it possible to find a well-aligned lidar sweep (which is likely to be non-keyframe as well) for a non-keyframe camera image, such that the lidar points can be projected and used as depth?

Thank you!"
"from the tracking result format I think I've made the same json file format which is described here:

https://www.nuscenes.org/tracking?externalData=all&mapData=all&modalities=Any

But I don't know why key error message happens when I run evaluate file. From results format, I also uploaded empty prediction list to my json file, does this matter?

I've uploaded some screen shots of my json file could anyone help with my issue?

![screen1](https://user-images.githubusercontent.com/61575966/192453207-72e5b451-4fab-4a94-bf6a-a1affe06c2b5.png)
![screen2](https://user-images.githubusercontent.com/61575966/192453202-4350598a-8929-4fa5-bd5a-070d2a9fe87b.png)
![screen3](https://user-images.githubusercontent.com/61575966/192453197-212ac32d-5805-4449-8976-c233adfc0219.png)
![screen4](https://user-images.githubusercontent.com/61575966/192453187-4278c5c5-3793-409a-8927-fbcd5c5946e4.png)


"
"Hi :) Thanks for your all great job!

I want to ask why the ""rotation"" matrix in ego_post.json file is not orthogonal? Or it should not be an orthogonal matrix? "
"I'm having the following error:
AssertionError: Samples in split don't match samples in predicted tracks.

Can anyone please explain why this happens?
What's samples in split? "
"I got some 3d boxes in lidar frame after inference. I tried to visualize them on camera frame, but I failed.
I follow the function `map_pointcloud_to_image` and transform them like 'lidar -> ego -> global -> ego -> camera'. However, it doesn't work.
could you give me some idea? thank you."
"Hello,

I want to use the prediction extension of nuscenes, however I encounter a little misunderstanding of the returned values I get from `get_future_for_sample` and `get_past_for_sample`. To my understanding, the result does not contain the record for timestep 0, e.g. setting `seconds=1` would result with 2 record that correspond to timesteps 0.5 and 1. How can one get the record for timestep 0?"
"The nuScenes paper reports that the HDLE-32 LiDAR sensor emits up to 1.4 million points per second, 700,000 points per scan, and has a range of up to 70 meters. 
![image](https://user-images.githubusercontent.com/88674452/189728009-48644bee-9bc9-4555-8f0d-c6e0f9b6dbd0.png)

However, in practice we see around 650,000 points per second, 34,500 points per LiDAR scan, with a range of 100 meters maximum xyz-distance in the real data. 
![image](https://user-images.githubusercontent.com/88674452/189747516-6b6b5cbb-6cc0-4461-a80f-b6994dc38134.png)


I'm curious which LiDAR sensor spec was actually used? If we are trying to recreate the nuScenes lidar sensor in simulation, how many points should we simulate per second, 34500 * 20 = 695,000 or the nuScenes paper's reported value of 1.4 million points per second? 

https://www.mapix.com/wp-content/uploads/2018/07/97-0038_Rev-M_-HDL-32E_Datasheet_Web.pdf Here is the LiDAR spec that I found for the HDL-32E sensor which has sensor specs for both 1.4 million points per second and 695,000 points per second. 

"
"Hi, is there any code that fuses non-key frames on point clouds?
The point cloud size in the key frame is so small and the point cloud is sparse. I was wondering if we could fuse the point clouds in non-key frames into the key frame for 3D tasks. "
"I would like to check if it is possible to calculate false positives by considering ground truth and predicted bounding boxes that are very close by but are the wrong class.

From what I understand, the current evaluation (https://github.com/nutonomy/nuscenes-devkit/blob/87b88fe37ad503e6e35dc2546ae1bf74f4ef6c01/python-sdk/nuscenes/eval/detection/algo.py) finds the closest bounding box of the same class (line 81) and checks if it is close to the ground truth box. Using the distance threshold, we can tweak the sensitivity of the prediction being a true or false positive.

However, is it possible to consider boxes that aren't the same class to be false positives as well? For example, if a `bus` box is predicted right on top of a `car` ground truth, can this be used as an indication of a false positive too? I would like to build a multi-class confusion matrix along this thought process.

Thank you very much"
"hello, everyone, i am so confused that  when i tried to construct  the instance of nusences, i inited  the nusences' class by calling the __init__ function in nuscenes.py file , however, i found there are so many json files missing on foldersof v1.0-* which i downloaded from website, namely, the lastest version of nuimages-v1.0-all-metadata.tgz,  i checked the foloders on the path, these missing files really don't exist...

i installed the nuscenes-devlit version 1.1.9,  but i noticed the nuscenes.py , it still tried to construct instance of nuscenes' class by __load_table__(""visibilty"") and (""scene:) where couldn't be found on the path
  "
"I got an infence result like 
`   {'sample_token': '3e8750f331d7499e9b5123e9eb70f2e2', 'translation': [637.141, 1636.252, -0.235], 
    'size': [0.621, 0.647, 1.778], 'rotation': [0.3495370229501108, 0.0, 0.0, 0.9369225526088982], 
    'velocity': array([-0.82728047,  0.65143342]), 
    'ego_translation': (37.020786205233094, -11.238776275174132, -0.235), 'num_pts': 1, 
    'detection_name': 'pedestrian', 'detection_score': -1.0, 'attribute_name': 'pedestrian.moving'}`
what is the different between translation and ego_translation?"
"Why are you repeating similar actions with different thresholds?

https://github.com/nutonomy/nuscenes-devkit/blob/28765b8477dbd3331bacd922fada867c2c4db1d7/python-sdk/nuscenes/utils/geometry_utils.py#L73-L75"
"Hi,
Thank you for the wonderful work!
I have a problem:
    
> Traceback (most recent call last):
    File ""nuScenes_data.py"", line 317, in
    main(args)
    File ""nuScenes_data.py"", line 112, in main
    test=args.test)
    File ""/home/d/Desktop/code/PlanningAwareEvaluation/pred_metric/environment/nuScenes_data/nuScenes_env.py"", line 672, in gen_expert_data
    nusc_map = NuScenesMap(dataroot=self.data_root, map_name=self.helper.get_map_name_from_sample_token(scene.name))
    File ""/home/d/anaconda3/envs/gca/lib/python3.6/site-packages/nuscenes/prediction/helper.py"", line 381, in get_map_name_from_sample_token
    sample = self.data.get('sample', sample_token)
    File ""/home/d/anaconda3/envs/gca/lib/python3.6/site-packages/nuscenes/nuscenes.py"", line 207, in get
    return getattr(self, table_name)[self.getind(table_name, token)]
    File ""/home/d/anaconda3/envs/gca/lib/python3.6/site-packages/nuscenes/nuscenes.py"", line 216, in getind
    return self._token2ind[table_name][token]
    KeyError: '61'
    Generating Data: 0%|

Do you have any recommendations what my issue might be

Thanks!
Phil"
"```
# .PCD v0.7 - Point Cloud Data file format
VERSION 0.7
FIELDS x y z dyn_prop id rcs vx vy vx_comp vy_comp is_quality_valid ambig_state x_rms y_rms invalid_state pdh0 vx_rms vy_rms
SIZE 4 4 4 1 2 4 4 4 4 4 1 1 1 1 1 1 1 1
```

What is the field 'z'? Are you using 4D Radar?"
"I got the sample_annotation ""1408af8e897d40fbbd78c33b59f8821f"", then i render the annotation.
![image](https://user-images.githubusercontent.com/63044657/185361000-8be3e007-8b82-4006-93b1-09008fde5847.png)
![image](https://user-images.githubusercontent.com/63044657/185361001-c44a2569-27ce-43a8-b5b1-3caa42186d25.png)
You can see the picture captured by ""CAM_FRONT_LEFT"".
I want to ask you how to get the camera type by code.
"
"Hi
I found that polygon tokens in roadblock all records are all None and the exterior and holes attributes of each record have an extremely large number of node tokens. I found these abnormal records only in roadblock records of hollandvillage and queenstown. Other locations do not have this issue.

  
The code to reproduce the problem is shown below.

    import numpy as np
    from nuscenes.map_expansion.map_api import NuScenesMap
    location = 'singapore-queenstown'
    nuscenes_map = NuScenesMap(dataroot='/mnt/datasets/nuScenes', map_name=location)
    count = 0
    exterior_nodes = []
    for rc in nuscenes_map.road_block:
        if rc['polygon_token'] is None:
            count += 1
        
        exterior_nodes.append(len(rc['exterior_node_tokens']))
    
    print('polygon None num:', count)
    exterior_nodes = np.array(exterior_nodes)
    print('exterior_node statistics: mean({}), max({})'.format(exterior_nodes.mean(), exterior_nodes.max()))


the outputs are:

  
      polygon None num: 676
      exterior_node statistics: mean(7748.0), max(7748)

@whyekit-motional"
"Hi
I found that polygon tokens in roadblock all records are all None and the exterior and holes attributes of each record have an extremely large number of node tokens. I found these abnormal records only in roadblock records of hollandvillage and queenstown. Other locations do not have this issue."
"Hi, thanks for your great dataset! 

I notice there are some labelled objects that are totally occluded and there are no lidar points in them. The visualization could be seen the second picture below. And you also note that ""A small number of 3d bounding boxes is annotated despite the object being temporarily occluded. For this reason we make sure to filter objects without lidar or radar points in the nuScenes benchmarks. See https://github.com/nutonomy/nuscenes-devkit/issues/366."".
![图片](https://user-images.githubusercontent.com/19658242/183855197-1822ee6d-3554-48b3-a6be-f54c1b80de0c.png)
![图片](https://user-images.githubusercontent.com/19658242/183855276-7e054fbf-3628-4a7b-8386-8b80f179c9e9.png)
![图片](https://user-images.githubusercontent.com/19658242/183855245-2217b63f-b50b-488a-81c1-f999e96850ac.png)

There are two questions:
1.  Since the objects are totally occluded, how do you label the occluded 3D objects?
2.  Do you regard the occluded trajectory as one trajectory or two trajectories with different tracking ID in tracking task?

Many thanks!"
"I would expect that using the ground truth labels as predictions would give perfect results on the detection benchmark, but I found that mAP and NDS are less than 1.

Here are the results of doing this on the mini_val set
```
Per-class results:
Object Class	AP	ATE	ASE	AOE	AVE	AAE
car	0.824	0.000	0.000	0.000	0.000	0.000
truck	1.000	0.000	0.000	0.000	0.000	0.000
bus	1.000	0.000	0.000	0.000	0.000	0.000
trailer	0.000	1.000	1.000	1.000	1.000	1.000
construction_vehicle	0.000	1.000	1.000	1.000	1.000	1.000
pedestrian	0.937	0.000	0.000	0.000	0.000	0.000
motorcycle	0.981	0.000	0.000	0.000	0.000	0.000
bicycle	0.660	0.000	0.000	0.000	0.000	0.000
traffic_cone	0.895	0.000	0.000	nan	nan	nan
barrier	0.000	1.000	1.000	1.000	nan	nan
```

The results for categories not represented in the min_val set are not surprising. But `[""car"", ""pedestrian"", ""motorcycle"", ""bicycle"", ""traffic_cone""]` which have AP<1 is surprising.

Code to reproduce:
```import json

from nuscenes import NuScenes
from nuscenes.eval.detection.evaluate import NuScenesEval
from nuscenes.eval.common.config import config_factory
from nuscenes.eval.common.loaders import load_gt
from nuscenes.eval.detection.data_classes import DetectionBox



def main():
    result_path = 'temp/nuscenes_eval.json'
    output_dir = 'temp/nuscenes_eval'
    version = 'v1.0-mini'
    version_set = 'mini_val'
    

    nusc = NuScenes(version=version, dataroot='/mnt/local/NuScenes', verbose=False)

    # make perfect predictions by loading GT
    data=load_gt(nusc, version_set, DetectionBox, verbose=True)
    results = {
        ""meta"": {""use_lidar"": False, ""use_camera"": True, ""use_radar"": False, ""use_map"": False, ""use_external"": False},
        ""results"": {}
    }
    for token in data.sample_tokens:
        results['results'][token] = []
        for ann in data[token]:
            results['results'][token].append({
                ""sample_token"": token,
                ""translation"": ann.translation,
                ""size"": ann.size,
                ""rotation"": ann.rotation,
                ""velocity"": ann.velocity.tolist(),
                ""detection_name"": ann.detection_name,
                ""detection_score"": 1.,
                ""attribute_name"": ann.attribute_name 
            })

    json.dump(results, open(result_path, 'w'))


    # do evaluation
    nusc = NuScenes(version=version, dataroot='/mnt/local/NuScenes', verbose=False)
    eval_detection_config = config_factory('detection_cvpr_2019')
    nusc_eval = NuScenesEval(
        nusc,
        config=eval_detection_config,
        result_path=result_path,
        eval_set=version_set,
        output_dir=output_dir,
        verbose=False)
    nusc_eval.main(render_curves=False)

main()```"
"Hello sir, I'd like to ask you my issue about Result format

Q 1 Rotation Issue

As I saw from the result format, I need four values to run the evaluation code(tracking).
From the result format, It says I need estimated rotation value. 

Does it mean that I need to run kalman flter to estimate the rotation value for the next time step?
Or
Can I just bring the value from the dataset and apply it directly to the evaluation code?

Q 2 Velocity Issue

From the mapillary dataset(camera), there's no velocity value. 
Then do I just have to put zeros for the evaluation code?

Meanwhile, megvii dataset(lidar) has!
In this case, can I just put the same value?

Q3 Size Issue
what does Estimated size means? (size cannot be changed??)"
"Hey motional team, two questions:
-  What is vx_comp and vy_comp's difference from  vx vy for the radar field? In this [line](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/utils/data_classes.py#L329-L331 ) it says that vx_comp is compensated by ego motion velocity but what does that mean? 

- Also, wondering what x_rms and y_rms mean: 
https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/utils/data_classes.py#L315 "
"Thank you so much for the impressive dataset.

We recently wanted to label a dataset and want to learn some strategies from Nuscene.

Here is one question related to the dataset labeling. I noticed the Nuscene dataset also labels the occluded part of the objects. On the Nuscene webpage ""If an object is occluded, then draw the bounding box to include the occluded part of the object according to your best guess.""

The question is why do we label the occluded parts? Because I noticed some datasets only label the visible parts of the objects. Will it cause some difference for the deep learning model if we choose different occlusion labeling methods? Personally, I feel we should label the occluded part if the occluded parts are important for the objective. "
""
"Hi, first thank you for sharing this dataset and its devkit.

I have faced a misalignment issue when I run the official render code.

The code snippets are below:

```python
sample_token = '7c0aa537b98344bea85226ba8148ae98'
nusc.render_pointcloud_in_image(sample_token, dot_size=1, out_path='./out.png', verbose=False)
```

As I know, if I run the code, the point cloud is scattered on the image with colors by depth values.

However, As described in the below image, there are some misaligned points as in the red boxes.

How can I solve this problem?

![image](https://user-images.githubusercontent.com/74757131/181914119-14a82901-35d2-4ef5-b86e-2488af06ac2c.png)
"
"Thanks for contribution!

I've run pip install nuscenes-devkit. But it occurs that:

`ModuleNotFoundError: No module named 'nuscenes.nuscenes'; 'nuscenes' is not a package`

How can I solve this?"
"Thanks for your great contribution!

I have a simple question about nuImages bounding box annotation.

I am aware when that when a person is on a motorcycle, the bounding box covers them both with the label ""vehicle.motorcycle"". I am confused if the same applies to `vehicle.bicycle`. 

So, when a person is on a bicycle does the bounding box cover both the person and bicycle while the label is `vehicle.bicycle`?

Thanks in advance."
"Due to its huge amount data, I'm currently working on to separate json file with respect to scene.
I'm working on for Mapillary and Megvii json file(train, validation, test)

On the way for that, I got an Assertion error as follows:
AssertionError: Database version not found: /Home/Nuscenes/nuscenes-devkit/python-sdk/nuscenes/v1.0-mini

nusc = NuScenes(version='v1.0-mini', dataroot=data_root, verbose=False)
data_root = '/Home/Nuscenes/nuscenes-devkit/python-sdk/nuscenes'

I actually couldn't understand how this file catches the version(should I make a folder?? Cuz in the tutorial there was mini folder  I downloaded all the files through github and there was no v1.0-mini folder in nuscenes folder)

Should set dataroot to the location that has Mapillary and Megvii json file? And how do I set the version?"
"I want to project 3D bounding box in lidar coordinate to image. However, I see there is a code that projects 3D bounding box in camera coordinate to image. Is there any codes that can convert 3D bounding Box in LiDAR coordinate to 3D bounding Box in Camera coordinate?"
"Hi motional team, I'm curious what is the velodyne hdle32 lidar scan pattern serialization format? 

Concretely, do you guys have a datasheet that explains more in depth how these lidar points get serialized out? E.g: similar to [this](https://hexagondownloads.blob.core.windows.net/public/AutonomouStuff/wp-content/uploads/2019/05/HDL-32E_Datasheet_whitelabel.pdf) datasheet from velodyne but a bit more fleshed out on the serialization format. 

Concretely, at every scan we have around 695000 / 20 = 34000 lidar points. 
- point 1 is the first channel ring index at 0 degrees azimuth and 10 degrees vertical elevation angle
- point 2 is the first channel ring index at 0 degrees azimuth and 9.9 degrees vertical elevation angle.
...
- point 34000 is the last channel ring index at 360 degrees azimuth and -30 degrees vertical elevation angle? 
"
"For most LiDAR sensors, a large portion of points (~10% give or take) will fall below the signal to noise ratio/hit air and will be classified as noise. This either results in a) the LiDAR point clouds varying significantly in size and/or b) a lot of points being classified as the same NOISE value e.g. [0, 0, 0] in the LIDAR point cloud. 

For the nuScenes data, it seems that all LiDAR scans are around 35000 points and there are no repeated common NOISE values. I'm curious where these noisy LiDAR point clouds are/were they filtered by the LiDAR sensor itself? "
"`import os
import json
import numpy as np

from tqdm import tqdm
from nuscenes import NuScenes`

I imported nuscenes devkit from this page and I tried to import nuscenes.py using from nuscenes import NuScenes

But I have the following error

Traceback (most recent call last):
  File ""/home/yeop/Nuscenes/nuscenes-devkit/python-sdk/nuscenes/nuscene_processing_ex1.py"", line 6, in <module>
    from nuscenes import NuScenes
  File ""/home/yeop/Nuscenes/nuscenes-devkit/python-sdk/nuscenes/nuscenes.py"", line 46
    version: str = 'v1.0-mini',
           ^
SyntaxError: invalid syntax

Can anyone please tell me what's wrong with this?
I haven't touched any contents of the nuscenes.py"
"Hello,

I tried to install _nuscenes-devkit_ by following [the setup guidelines](https://github.com/nutonomy/nuscenes-devkit#nuscenes-setup), however, when I run the _unittest_ it ends with a series of errors:

```
/home/nuscenes-devkit/python-sdk$ python -m unittest

............................................./home/nuscenes-devkit/python-sdk/nuscenes/map_expansion/map_api.py:1823: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.
  exteriors = [int_coords(poly.exterior.coords) for poly in polygons]
/home/nuscenes-devkit/python-sdk/nuscenes/map_expansion/map_api.py:1824: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.
  interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]
...........................ssss....................................EEE.................................../home/nuscenes-devkit/python-sdk/nuscenes/utils/map_mask.py:110: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  img = img.resize((size_x, size_y), resample=Image.NEAREST)
/home/nuscenes-devkit/python-sdk/nuscenes/utils/map_mask.py:64: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  on_mask = np.ones(px.size, dtype=np.bool)
....
======================================================================
ERROR: test_classes (nuscenes.tests.test_lidarseg.TestNuScenesLidarseg)
Check that the class names match the ones in the colormap, and are in the same order.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/nuscenes-devkit/python-sdk/nuscenes/tests/test_lidarseg.py"", line 31, in test_classes
    for name, idx in self.nusc.lidarseg_name2idx_mapping.items():
AttributeError: 'NuScenes' object has no attribute 'lidarseg_name2idx_mapping'

======================================================================
ERROR: test_num_classes (nuscenes.tests.test_lidarseg.TestNuScenesLidarseg)
Check that the correct number of classes (32 classes) are loaded.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/nuscenes-devkit/python-sdk/nuscenes/tests/test_lidarseg.py"", line 16, in test_num_classes
    self.assertEqual(len(self.nusc.lidarseg_idx2name_mapping), 32)
AttributeError: 'NuScenes' object has no attribute 'lidarseg_idx2name_mapping'

======================================================================
ERROR: test_num_colors (nuscenes.tests.test_lidarseg.TestNuScenesLidarseg)
Check that the number of colors in the colormap matches the number of classes.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/nuscenes-devkit/python-sdk/nuscenes/tests/test_lidarseg.py"", line 22, in test_num_colors
    num_classes = len(self.nusc.lidarseg_idx2name_mapping)
AttributeError: 'NuScenes' object has no attribute 'lidarseg_idx2name_mapping'

----------------------------------------------------------------------
Ran 181 tests in 32.328s

FAILED (errors=3, skipped=6)
```

Is there anything that I forget to install? Could it be a wrong version? 

Thank you!
Hamid"
"Is there any calibration matrix that maps 3D bounding box in lidar coordinate to image???
I tried map_pointcloud_to_image function that maps pcd to image, however it did not work."
"Hello, is there any way to convert KITTI dataset into nuscenes dataset?"
"Hi, thanks for the dataset.

I have trouble when unzipping the v1.0-trainvalxx_blobs.tgz files. I have tried to unzip on Ubuntu with ""tar -xf"" and unzip on Windows with WinRAR, but they all say the file is not complete. I have checked the md5 of all the blobs. They are different from the mentioned ones but the size of the files are the same. I have tried to download the data with wget for 3 times. The results are all the same. Do I do something wrong?
 
![截图](https://user-images.githubusercontent.com/17980676/180377490-7d57e092-2a88-4e86-977e-b6b68ba9187d.PNG)
![截图2](https://user-images.githubusercontent.com/17980676/180377512-80314635-604b-4dfe-9a76-c30b81f7bdbd.PNG)
 "
""
"Did the velodyne 32e model use a return mode of strongest or last? 

For reference:
https://velodynelidar.com/wp-content/uploads/2019/09/63-9277-Rev-D-HDL-32E-Application-Note-Packet-Structure-Timing-Definition.pdf "
"I tried to submit the test results of lidarseg and also got an error with no information.
I passed the local file validation using _nuscenes.eval.lidarseg.validate_submission_
How should I fix it?

```
Starting Evaluation...
Submission related metadata:
Unpacking dataset...
Unpacked /tmp/tmpw9zx4i6m/compute/challenge_data/challenge_720/phase_data/phase_1549/bb9bb38a-ea59-4266-8a50-7f2fdc109dac.zip to /tmp/tmp2f23q79t/dataset.
Unpacking user submission...
Unpacked /tmp/tmpw9zx4i6m/compute/submission_files/submission_213616/fea0a3f6-18ad-481a-9d77-9b0e4aa4c95a.zip to /tmp/tmp2f23q79t/submission.
Evaluating for test phase
======
Loading NuScenes tables for version v1.0-test...
Loading nuScenes-lidarseg...
32 category,
8 attribute,
4 visibility,
11997 instance,
12 sensor,
1800 calibrated_sensor,
462901 ego_pose,
15 log,
150 scene,
6008 sample,
462901 sample_data,
201130 sample_annotation,
4 map,
6008 lidarseg,
Done loading in 9.579 seconds.
======
Reverse indexing ...
Done reverse indexing in 2.1 seconds.
======
```
"
"I only need the map and trajectory information, which I believe is included in the meta data. However, when I tried to use v1.0-trainval with only meta and map extension, the program got killed when creating the NuScenes object. Do I really need to download all the camera/lidar /radar files? "
"Hi, thx for your nice works! :) 
i have questions about **merging** nuScenes + nuScenes_lidaraseg. 

First of all, I'm using 9 b-box parameters made by below. 
```
            nusc = NuScenes(version=self.dataset_cfg.VERSION, dataroot=str(self.root_path), verbose=True)
            annotations = [nusc.get('sample_annotation', token) for token in sample['anns']]

            # the filtering gives 0.5~1 map improvement
            num_lidar_pts = np.array([anno['num_lidar_pts'] for anno in annotations])
            num_radar_pts = np.array([anno['num_radar_pts'] for anno in annotations])
            mask = (num_lidar_pts + num_radar_pts > 0)

            locs = np.array([b.center for b in ref_boxes]).reshape(-1, 3)     # box center! 
            dims = np.array([b.wlh for b in ref_boxes]).reshape(-1, 3)[:, [1, 0, 2]]  # wlh == > dxdydz (lwh)
            velocity = np.array([b.velocity for b in ref_boxes]).reshape(-1, 3)
            rots = np.array([quaternion_yaw(b.orientation) for b in ref_boxes]).reshape(-1, 1)

            names = np.array([b.name for b in ref_boxes])
            tokens = np.array([b.token for b in ref_boxes])

            gt_boxes = np.concatenate([locs, dims, rots, velocity[:, :2]], axis=1)
```

And i think that the meaning of these 9 box parameters are as follows.
`gt_boxes = center point(x, y, z) + length + width + height + rotations + velocity`

Then, I want to know **the lidar segmentation value** corresponding to the (x, y, z) center point using nuScenes_lidarseg dataset in particular scene. 
In other words, I want to know **what segmentation label this 3D b-box's center point** corresponds to.

Can you help me to solve this work?
Best, "
"Unable to run docker file provided in the setup folder, Do we have any working dockerfile ?"
"What is the value of this variable when our results are evaluated in line 54 of [https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/panoptic/evaluate.py](url)?

It will be helpful if we get this value to assess our method on val set."
"Hi, i am working on a project . i want some datasets with its ground truth file and calibration file to do some experiments and check the results of my code. please anyone here to help me."
"Appears when running 'python export_kitti.py render_kitti --nusc_kitti_dir ~/nusc_kitti --render_2d False' command in 'export_kitti.py'
(
File ""export_kitti.py"", line 267, in render_kitti 
         kitti.render_sample_data(token, sensor_modality=sensor, out_path=out_path, render_2d=render_2d)
 File ""D:\conda\lib\site-packages\nuscenes\utils\kitti.py"", line 501, in render_sample_data
         color = np.array(color_func(box.name)) / 255
TypeError: get_color() missing 1 required positional argument: 'category_name'
)
Is there any solution please?"
"the range used during evaluation is 0~40 or 0~50.
but how far will nuscenes mark the groundtruth.
whether the obstacle out of the range of groundtruth marked will be considerder as fp?"
"I have results from running a tracking algorithm CenterPoint on Nuscenes data.
How do I perform matching then between the results and ground truth (most importantly re-assign the instance_token to each result to take place of the existing tracking_id in the results file)"
"In nuscences dataset, there are 5 radars used, so we are having radar_top, radar_front_left, radar_front_right, radar_back_left, radar_back_right, is there any common timestamp for those 5 radar to fuse the data? 
otherwise how to perform the early fusing using these 5 radar data's?
"
"In nuscences dataset, inside sweeps the radar pointcloud data is in RadarObjects datatype format, how to convert them to PointCloud2 datatype format?"
"Hey motional team, wondering what vehicle model you all used to collect the nuScenes dataset? Thanks!"
"Hello, I was trying to use your dataset for my college project and I would like to get sample images without annotations. How to get those images because in the render_sample function I don't see a way to get rid of those annotations whereas in render_image function there is an option for it, but I cannot use token from nuscenes in nuimages(gives a KeyError). Can you please help me?
"
"first of all, thanks for the data!

You mentioned that the exposure of a camera is triggered when the top LIDAR sweeps across the center of the camera’s FOV.
If then, I think the capture cycle of the camera should be the same as the that of lidar at 20Hz. It seems contradictory. 
Can you tell me what I'm misunderstanding?"
""
"Hi, I'm a new user of nuscenes. I heard that nuscenes contains labels for semantic segmentation. Does one pointcloud has both labels for semantic segmentation and 3d detection? Or it likes KITTI that different pointcloud and labels for different tasks? @whyekit-motional "
Is there a way to parse and auto populate incoming camera and radar data into nuscenes tables?
Can nuscenes be done using nosql database as a document having all the relevant details rather having multiple tables and joining them?
"Is nuscenes annotation can be converted to and from CVAT, COCO, VOC or any other annotated data formats?"
""
"I just download 10 file dataset .tgz ( v1.0-trainval01_blobs.tgz , v1.0-trainval02_blobs.tgz ,.. ) .
How do I unzip and merge them automatically
├── data
│   ├── nuscenes
│   │   │── v1.0-trainval
│   │   │   │── samples
│   │   │   │── sweeps
│   │   │   │── maps
│   │   │   │── v1.0-trainval  "
"hi, 

can I get the NDS  without velocity result?

thanks!"
""
thanks!
"Based on the Issue #748 
> @joeking11829 do take a look at the following: https://forum.nuscenes.org/t/radar-vx-vy-and-vx-comp-vy-comp/283
<br>

Hi @whyekit-motional ,

I read the article, but I could not get any conclusion.

In the disscussion, 
the vector compose of ""vx"" and ""vy"" must point from radar origin to radar point
[ reference from ""isrohutama"" ]
![image](https://user-images.githubusercontent.com/5969040/168737033-02609c8f-331f-428d-b9ea-c863acb68084.png)


But it seems to me that nuScenes's radar data ( vx and vy ) does not behave that way.
And it make me confused that how the compensated velocities are compensated by ego motion,
it make the vector compose of ""vx_comp"" and ""vy_comp"" point from radar origin to radar point, but the vector compose of ""vx"" and ""vy"" does not.

Any ideas ?

Thanks !!"
"Hi,

Thank you for the great work! I have downloaded nuScenes-lidarseg from [`this link`](https://www.nuscenes.org/download) and tried to follow this [`instruction`](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/nuscenes_lidarseg_panoptic_tutorial.ipynb) to read point cloud. However, it seems that the dataset is supposed to include `attribute.json` file in `v1.0-*` folders, while I only get `category.json` and `lidarseg.json` in them. Here is the folder structure on my computer:

├── nuScenes-lidarseg-all-v1.0
│   ├── lidarseg
│   │   ├── v1.0-mini
│   │   ├── v1.0-test
│   │   └── v1.0-trainval
│   ├── v1.0-mini
│   │   ├── category.json
│   │   └── lidarseg.json
│   ├── v1.0-test
│   │   ├── category.json
│   │   └── lidarseg.json
│   └── v1.0-trainval
│       ├── category.json
│       └── lidarseg.json
├── nuScenes-lidarseg-all-v1.0.tar.bz2
└── nuscenes_lidarseg_panoptic_tutorial.ipynb


Could you help find whether I have downloaded the correct dataset, or do I need to download other datasets (up to now I only download nuScenes-lidarseg)?


"
"When I write the following function according to the official example to project the 3D detection result (CenterPoint) to the pixel plane
```
def project_3d_to_2d(nusc, det, yaw, sample_data, cam):
    # 得到对应相机的data
    box = Box(det[""translation""],
              det[""size""],
              yaw,
              score=det[""detection_score""],
              name=det[""detection_name""])
    sd_rec = nusc.get('sample_data', sample_data[cam])
    cs_rec = nusc.get('calibrated_sensor', sd_rec['calibrated_sensor_token'])
    pose_rec = nusc.get('ego_pose', sd_rec['ego_pose_token'])
    camera_intrinsic = np.array(cs_rec['camera_intrinsic'])
    # 从世界坐标系->车身坐标系
    box.translate(-np.array(pose_rec['translation']))
    box.rotate(Quaternion(pose_rec['rotation']).inverse)

    # 从车身坐标系->相机坐标系
    box.translate(-np.array(cs_rec['translation']))
    box.rotate(Quaternion(cs_rec['rotation']).inverse)

    # 过滤掉相机光心之后的点(即z < 0)
    corners_3d = box.corners()
    in_front = np.argwhere(corners_3d[2, :] > 0).flatten()
    corners_3d = corners_3d[:, in_front]

    # 将三维点投影到image plane
    corner_coords = view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()

    # 过滤掉超过(1600, 900)范围的像素点
    final_coords = export_2d.post_process_coords(corner_coords)

    return final_coords
```
on the validation set, the following error will be reported for the sample whose sample_token is ""50c7c32fa4e446119d53d1deb289aa7f""
```
Traceback (most recent call last):
  File ""detections_3d.py"", line 525, in <module>
    test(nusc, full_results_json)
  File ""detections_3d.py"", line 470, in test
    sample_token='50c7c32fa4e446119d53d1deb289aa7f')
  File ""detections_3d.py"", line 219, in token_to_data
    bbox2d = project_3d_to_2d(nusc, det, tmp_yaw, sample_data, cam_name)
  File ""detections_3d.py"", line 507, in project_3d_to_2d
    final_coords = export_2d.post_process_coords(corner_coords)
  File ""/home/lixiaoyu/anaconda3/envs/mm/lib/python3.6/site-packages/nuscenes/scripts/export_2d_annotations_as_json.py"", line 42, in post_process_coords
    intersection_coords = np.array([coord for coord in img_intersection.exterior.coords])
AttributeError: 'LineString' object has no attribute 'exterior'
```
this error is not reported in other samples of the validation set, I think that export_2d_annotations_as_json.py may need additional checks for such cases? I'm not sure why this kind of error is reported, hope to get your answer, it's very helpful!!!
"
""
"Hello, team

I want to get the points indices of a specified object from lidarseg dataset such as a car as shown in image. However, the labels of a frame are point-wise annotation without object-level information. So how can we get the point indices of the target object of interest?

![2022-05-05_16-28](https://user-images.githubusercontent.com/24294571/166887799-99ac844b-c4c5-47f5-ad3a-90bbad98fdf3.png)

"
cannot access sweeps/LIDAR_TOP/n008-2018-08-30-15-31-50-0400__LIDAR_TOP__1535657812199943.pcd.bin: No such file or directory
"Hello,

I am trying to convert point clouds and boxes to ego frame using the built in methods

`sample_data_lidar_top_token = sample_obj[""data""][""LIDAR_TOP""]

                calibrated_lidar_token = nusc.get(""sample_data"", sample_data_lidar_top_token)[""calibrated_sensor_token""]  # get calibraton params using the lidar token for this sample
                lidar_calibs_rot_quat = pyquaternion.Quaternion(nusc.get(""calibrated_sensor"", calibrated_lidar_token)[""rotation""])  # rotation as quaternion
                lidar_calibs_trans = nusc.get(""calibrated_sensor"", calibrated_lidar_token)[""translation""]
                trans_mat_rotate_z = gutils.transform_matrix(rotation=lidar_calibs_rot_quat, translation=lidar_calibs_trans, inverse=False)  # 19/04/22 -> full transform to ego frame

                # a sensor's token is used to get the actual data path and annotations, boxes converted to vehicle frame
                pcl_path, box_obj_list, cam_intrins = nusc.get_sample_data(sample_data_lidar_top_token, use_flat_vehicle_coordinates=True)`

Now when I plot the point cloud with boxes, some boxes do not seem to be aligned the way that the orientation.degrees angle returns.

![image](https://user-images.githubusercontent.com/69435296/165825776-c3085af0-ad5e-4f87-bf18-24ecf951d5ae.png)

![image](https://user-images.githubusercontent.com/69435296/165825963-e508d3f4-7542-4caa-b886-a5cb4bdf477b.png)

As we can see, the degrees reports 27 degrees approx whereas visually the box looks like almost 90 degrees.

So what am I missing?

Concretely, 

1) What is the axis convention when representing box orientation in ego frame? (I assume x (0 degrees) is pointing right, counter clockwise is +ve and clockwise is -ve)

2) What is the correct way to project point clouds and boxes to vehicle frame and read the rotation about z axes for the boxes?

Please help.

Best Regards
Sambit
"
"Hi, thanks for your code.

What is the data format of 2D detection for nuScence tracking？

for example:

![image](https://user-images.githubusercontent.com/71493146/165306582-758f7aa9-c1ec-4e8e-b9a1-6eded1c069be.png)


""7e351605549547f8af19432917f23b73""（blue box）: that means frame?

""df3af96318c945c3b23080ced7720aa7""（red box）: I don't know what this is...

“0”：that means pedestrian？and ""2"" means car?

Thank you for your reply."
"Hi, Thank you for your open source dataset.

My current research is multi-object tracking based on the fusion of camera and lidar. Before, it was tested on Kitti, and our method has achieved good results[KITTI]( http://www.cvlibs.net/datasets/kitti/eval_tracking_detail.php?result=5a93530d8296e670c99ff5fa751efa3d235ead5d). And our work is [open source](https://github.com/wangxiyang2022/DeepFusionMOT). 

At present, we want to test on the large dataset nuscence, but we find a problem. 

Because the previous debugging is for the format of Kitti dataset, and the format of nuscence dataset is different from that of Kitti dataset, especially when fusing the detection data of lidar and camera, I want to ask whether there is a corresponding script to convert the detection results of nuscence into the format of Kitti dataset, Then it is converted to the format of nuscence to submit the evaluation."
"Hi, 

I have a question about the generation of  3d box corners.
https://github.com/nutonomy/nuscenes-devkit/blob/85b918050644964731bdfc8d8f1d53012a34b96a/python-sdk/nuscenes/utils/data_classes.py#L612-L619

 According to the code, the xyz direction of the initial corners are front, left and up no matter what the sensor frame. The orientation of box has transform to the local sensor frame, why can we use `corners = np.dot(self.orientation.rotation_matrix, corners)` to get the correct  result? Should we define the corners in the sensor frame and use the yaw of the box to get the final result? 

I'm also confused by the orientation of the box. Please see the following code. I use a car as a example. 

![nuscene_screenshot_23 04 2022](https://user-images.githubusercontent.com/24294571/164883986-6664d25b-25da-4e96-a7a5-c0477fb49fef.png)



```
cam_front_token = 'a61bfbde6877413386a772591617f38d'
sample_annotations_token = 'fb5169ad9daa4d1086ec59d6cf5ed4b0'
_, box_camera, _ = nusc.get_sample_data(cam_front_token, selected_anntokens=[sample_annotations_token])
angles_box_camera = np.array(box_camera[0].orientation.yaw_pitch_roll) / np.pi * 180
print('box orientation in camera frame: ', angles_box_camera)
```
the output is `box orientation in camera frame:  [120.46844792   1.10142931  91.32124014]`

The yaw of the car in camera frame should be negative, the roll and pitch should be close to zero. However, the output isn't satisfied but I still get the correct plot using the `corners()` method. I must be misunderstanding something, please let me know the reason."
"Hi,

I'm trying to render the map data with different layers. The problem is that, some of the rotated maps can not align properly. 

My question is: are there different offsets in `ego_pose['rotation']` for different maps? Could you give me some advice? 

My code for map generation: 
```
nusc_map = NuScenesMap(dataroot='data/nuScenes', map_name=location)
sample_data_record = nusc.get('sample_data', sample_data_token)
pose_record = nusc.get('ego_pose', sample_data_record['ego_pose_token'])
ego_center_x, ego_center_y, ego_center_z = pose_record['translation']

# the rotation angle
ypr_rad = Quaternion(pose_record['rotation']).yaw_pitch_roll
patch_angle = math.degrees(ypr_rad[0])

layer_names = ['drivable_area', 'walkway', 'carpark_area']

x_min, y_min, x_max, y_max = ego_center_x - 40, ego_center_y - 40, ego_center_x + 40, ego_center_y + 40
patch_box = [x_min, y_min, x_max, y_max]

fig, ax = nusc_map.render_map_patch(patch_box, layer_names,
                                  render_egoposes_range=False, render_legend=False)

image = fig2data(fig)
image = rotate_bound_white_bg(image, patch_angle)

def rotate_bound_white_bg(image, angle):
    # grab the dimensions of the image and then determine the
    # center
    (h, w) = image.shape[:2]
    (cX, cY) = (w // 2, h // 2)

    # grab the rotation matrix (applying the negative of the
    # angle to rotate clockwise), then grab the sine and cosine
    # (i.e., the rotation components of the matrix)
    M = cv2.getRotationMatrix2D((cX, cY), -angle, 0.75)
    cos = np.abs(M[0, 0])
    sin = np.abs(M[0, 1])

    # compute the new bounding dimensions of the image
    nW = int((h * sin) + (w * cos))
    nH = int((h * cos) + (w * sin))

    # adjust the rotation matrix to take into account translation
    M[0, 2] += (nW / 2) - cX
    M[1, 2] += (nH / 2) - cY

    # perform the actual rotation and return the image
    return cv2.
```
The output: 
![0af1568c817a44048cfc67879f893f35__HDMAP](https://user-images.githubusercontent.com/82573861/164232401-561e6014-add0-4159-92e4-63f70ab23136.png)

Run the render_sample_data:
```
nusc.render_sample_data(sample_data_token, with_anns=False, nsweeps=10, out_path=None, underlay_map=True)
```
The output:
![0af1568c817a44048cfc67879f893f35__HDMAP](https://user-images.githubusercontent.com/82573861/164232836-b0137ded-0efe-4f9b-97dc-a7f9b4ee3435.png)

As shown above, there is a certain angle deviation between the two map images. 

"
"Hi, 

I want to ask, if it is possible to render the centric HD map according to the sample data token? 

Similar with `render_ego_centric_map`, but i need the HD map layer information (using the map extension packet)."
Is there an introduction to sensor and vehicle coordinate calibration
"Hi nuScenes teams,

Radar have two velocity column

vx, vy: in m/s. Radial velocity measured for this detection
vr_comp, vy_comp in m/s: Radial velocity for this detection but compensated for the ego-motion
Because I am not familiar with Radar, I have not idea why Radial Velocity need to compensate with ego-motion.

I use the **""vx, xy""** to plot radial velocity, the direction of radial velocity for each radar point **is not point to radar** itself.
when I use the **""vx_comp, vy_comp""** to plot radial velocity, and the direction of radial velocity for each radar point **is point to radar** itself


Could you explain that ?
Thanks !!"
"Earlier today I was trying to plot the map mask of drivable area with `map_api.render_map_mask()` for the whole boston-seaport, but then I found that part of it is missing. However, it doesn't happen if I use `map_api.render_layers()` instead (see attached).
- `map_api.render_map_mask()`
![Unknown](https://user-images.githubusercontent.com/85070296/162879325-e2c4526f-c3b7-47fd-8184-b56621b2ddc7.png)
- `map_api.render_layers()`
![Unknown-2](https://user-images.githubusercontent.com/85070296/162879430-0ebcfb90-b45b-4021-aba4-0eb2b14ba64a.png)

Hence, I would like to know how can I ensure that `map_api.get_map_mask()` actually returns the correct sets of points for the whole map?"
I want to know where non key-frame image poses get from. Are they interpolated poses between key-frames? I found that there are some mistakes when I use non key-frame image poses. Thanks in advance :)
"Thanks for you great work! I want to konw how could I get access to 12HZ source camera images.
![image](https://user-images.githubusercontent.com/27892562/161967437-dbd20c2e-f237-42b6-aa9a-9fda5d366189.png)
"
"Hi, I am confused about the meaning of 'ego_pose' in different sensors.
For  a specific sample, I can get ego_pose respect to the CAM_FRONT by:  
`cam_front_pose= self.nusc.get('ego_pose', self.nusc.get('sample_data', record['data']['CAM_FRONT'])['ego_pose_token']))`
At the same time, I can get ego_pose respect to the LIDAR_TOP by
`lidar_pose = self.nusc.get('ego_pose', self.nusc.get('sample_data', record['data']['LIDAR_TOP'])['ego_pose_token'])`
And I can get ego_pose w.r.t CAM_BACK, CAM_FRONT_LEFT in the same way.
Here comes the question. 
1. What's the difference of these ego_poses? What's the meaning of these ego_poses of the same sample? They look quite similar with little differences. 
2. I observe that in many cases where coordinate system transformation is need, points coordinates are first transformed to global and then transformed back to ego coordinates system, finally transform to specific sensor.  E.g. lidar -> global -> ego -> cam. Why not transform by ego coordinates system since all the sensors are calibrated respected to ego coordinate system, like lidar - > ego -> cam? This is related to the first problem because the ego_pose w.r.t. different sensors are slightly different.


"
"Maybe I didn't clear the problem，The radar data provided by the Nuscenes is sparse point clouds that has been handled by a CFAR (Constant False Alarm Rate) algorithm. I want to guide whether you can provide the original data that has not been processed with CFAR.
In addition, can we get dense data after transitioning of your method?
Thank you for your answer!"
Hello! Does the nsweeps value mean that other non-key frame data is added to the key frame?
"Hi,

The boxes we get from the function 'get_sample_data' is wrt sensor frame of reference, right?

Also, I want to change the data of the box to a different frame of reference without using Box(as given in the repo code)
Please suggest what mistake I am making as it doesn't give correct result.

```
# Transforming box data from sensor to ego vehicle frame 
    veh_with_ego = np.dot(sens_rotation_m,(veh_translation)) + sens_translation
# Transforming data from ego to global frame
    veh_ = np.dot(ego_rotation_m, (veh_with_ego)) + ego_translation
# Transforming global data to ref_ego frame
    veh_with_ref_ego = np.dot(np.linalg.inv(ref_ego_rotation_m),((veh_ - ref_ego_translation)))
# Transforming data from ref_ego frame to ref_sensor frame
    veh_with_ref_sens = np.dot(np.linalg.inv(ref_sens_rotation_m), ((veh_with_ref_ego - ref_sens_translation)))
```
Note: veh_translation is an array of location of vehicle and I am using rotation matrix from quaternions. 

Please suggest if the above method is correct?
If not, what needs to be changes as I don't want to use Box features.

Thanks"
"Hello，

How to find  ‘token’ by ‘filename’.
For example, 'filename' :samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151605047769.

Thanks!"
"Hello,
I'm currently working on a object detection method based on radar, and I need radar data in range-Doppler -Azimuth Tensor format, instead of sparse point cloud format.  I wonder if you can provide point cloud data in this format?  
Looking forward to your reply!  "
"Hi,

I am recently trying to couple the tracking data with the HD-Map. However, I cannot find any API that shows the city of a tracking scene. Therefore, I am wondering (1) if it is possible to link the tracking data and HD-Map; (2) if so, what functions/API may I use?

Please point out if I am missing anything. Thank you for your help!"
"Hello,
    Does the camera extrinsic matrix equals the inverse matrix from calibrated_sensor?
    The extrinsic of the camera should be the matrix from the points in the vehicle coordinate system to the camera coordinate system.
     But  The definition of calibrated_sensor seems to be the rigid body transformation matrix from camera to vehicle coordinate system."
"Cases that we only need `nuscenes`, it may not be necessary to have `pycocotools` as requirements.

We can have it as something like `pip install nuscenes-devkit` for nuscenes only  and `pip install nuscenes-devkit[nuimages]` for `pycocotools`.

If OK, I will submit a PR for this."
"Hello，
I implemented my network in [OpenPCDet](https://github.com/open-mmlab/OpenPCDet) framework. Now I want to visualize the object detection results.  Specifically，I want save the results, and select some good pictures for my paper. 
I have two ways： 
1.  Use open3d or mayavi for visualization, but I don't know how to save 3D results as pictures on a large scale.
2. Project the result of  detection onto the camera image, but I don't know how to coordinate the six cameras and how to project.

I want to know if there is a tool similar to [kitti-viewer](https://github.com/hailanyi/3D-Detection-Tracking-Viewer) for nuscenes dataset to directly complete visualization
Is there a visualization tool in nuscenes-devkit？"
"How do we do this in nuImages dataset?

_Originally posted by @wannapg in https://github.com/nutonomy/nuscenes-devkit/issues/698#issuecomment-1073511079_"
"Can I render sample data without bounding boxes?
I used: `nusc.render_sample_data(my_sample['data']['CAM_FRONT'])` but this includes bboxes."
I have downloaded all archives.I am wondering if I need to put samples to the samples folder by my self or just create a empty samples/sweeps folder and put the files in v1.0* folder?
"I noticed, that `nuScenes/maps/prediction/prediction_scenes.json` is missing many objects. E.g. for scene-0003, it only contains the instance tokens of four vehicles.

> unique_instance_tokens_of_scene_0003_are = ['35a767667a984516bf1feb228c03de03', '5f51a3503ea140769371b9824533a607', '88081a9cb6c74d5190af6d6ea845a6de', 'a5f12393fdf042a4a1c25cbe29f3f4ea']

But a single sample of this scene contains a magnitude more of instance tokens.
I noticed, that e.g. all parked cars aren't stored in this json.
Is somewhere noted which criteria were used to add instances to `prediction_scenes.json `
Maybe all vehicles that have a moving attribute?
"
"The PCD file contains all the information about the target detected by the radar, such as speed, azimuth, etc. What tools are needed to extract all this data.tks for your help:)"
"Hi, in MTP [model definition](https://github.com/nutonomy/nuscenes-devkit/blob/f94dcd313feb8adc91e7d01312fb7d27cc77098e/python-sdk/nuscenes/prediction/models/mtp.py#L51-L54) or its [`forward()`](https://github.com/nutonomy/nuscenes-devkit/blob/f94dcd313feb8adc91e7d01312fb7d27cc77098e/python-sdk/nuscenes/prediction/models/mtp.py#L72) method it appears that a nonlinearity is missing between the linear layers. Or you might not need it, in which case it seems that the two linear layers can be combined into:
```python
self.lin = nn.Linear(backbone_feature_dim + ASV_DIM, int(num_modes * predictions_per_mode + num_modes))
```
Please let me know if I'm missing something."
" In tracking task, is it possible to draw a series of frames of the same video continuously on the same picture in BEV? The original function can only display each frame of the picture frame by frame. I have inquired about fig, plt and other functions, but it seems that I have not been able to solve this problem。looking forward to your reply."
"resolved, thanks."
"Hello,

AFAIK, the time difference between keyframes is approximately 500 ms. However, the lidar is being sampled within a 50 ms period. I want to run validation for not only keyframes but also for the sweeps in the middle. For example, if S-1 and S-11 are two consecutive keyframes, I want to include the sweeps in the middle with a 100 ms period, so the validation processes the samples [S-1, S-3, S-5, S-7, S-9] instead of processing S-1 only. I think, only the keyframes have annotations, but the devkit has the capability to interpolate the annotations for the sweeps between keyframes. Please let me know how can I do this.

Best regards,"
"Hi nuScenes Team,
nuScenes uses data from [all six cameras](https://github.com/nutonomy/nuscenes-devkit/issues/538#issuecomment-955907398) for evaluation. However, I want to run the evaluation of nuScenes for the front camera. 

I use the following command for evaluation with the oracle labels on val split
```bash
python /home/abhinav/project/nuscenes-devkit/python-sdk/nuscenes/eval/detection/evaluate.py \ 
--version v1.0-trainval \ 
--eval_set val \
--plot_examples 0 \
--render_curves 0 \
--result_path ~/project/output/run_1100/oracle_label/submission.json 
```
Here are the results:
```
mAP: 0.1700                                                                                                                                                                                                  
mATE: 0.0039
mASE: 0.0159
mAOE: 0.0025
mAVE: 1.5243
mAAE: 1.0000
NDS: 0.3828
Eval time: 17.3s
Per-class results:
Object Class	AP	ATE	ASE	AOE	AVE	AAE
         car	0.178	0.004	0.016	0.003	2.775	1.000
       truck	0.200	0.004	0.009	0.003	1.827	1.000
         bus	0.322	0.004	0.009	0.003	2.953	1.000
     trailer	0.211	0.004	0.007	0.003	0.531	1.000
construction	0.122	0.004	0.011	0.002	0.105	1.000
  pedestrian	0.111	0.004	0.020	0.003	0.860	1.000
  motorcycle	0.233	0.004	0.017	0.003	2.283	1.000
     bicycle	0.089	0.004	0.026	0.003	0.861	1.000
traffic_cone	0.089	0.004	0.026	nan	nan	nan
     barrier	0.144	0.004	0.016	0.003	nan	nan
```

Thus, the above command reports an AP of one-sixth while the ideal AP should have been 1 with the oracle labels of six cameras. 
Therefore, I need to filter out the boxes which do not show up in the front camera, and keep the boxes which show up in the front camera.

I digged into the devkit and found that the `evaluate` [function](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/common/loaders.py#L109) uses data from all camera images:
```python
sample = nusc.get('sample', sample_token)
sample_annotation_tokens = sample['anns']
```

I did not know how should we filter sample_annotation_tokens by front camera. So, I looked into the [tutorial](https://www.nuscenes.org/tutorials/nuscenes_tutorial.html) as well, but the following code from the tutorial does not give me annotations
```python
sensor = 'CAM_FRONT'
cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])
cam_front_data
```

Therefore, it would be awesome if you could help me filter/get ground truths for the front camera and thereby, help me run the evaluation only on the front camera images in nuScenes.
"
"I have got the json file of the tracking task and got the metrics like AMOTA by evalute.py, if I want to display the tracking result as 3Dbbox on the image in a video as well as display the BEV tracking result,how can I do it? looking forward to your reply"
"Hello,
It seems that some of the lanes are missing in the map's lane list, while other information such as `arcline_path` is available.
Here is an example:

```Python
>>> from nuscenes.map_expansion.map_api import NuScenesMap
>>> city_map = NuScenesMap(dataroot='datasets/nuScenes-prediction/v1.0-full', map_name='singapore-onenorth')
>>> lane = city_map.lane[504]['token']
>>> lane
'8e1a801d-5c47-430e-b9f1-58a13b656927'
>>> outgoings = city_map.get_outgoing_lane_ids(lane)
>>> outgoings
['e4d2789f-a3e9-42aa-a3aa-8cf5b0f88661']
>>> city_map.get_arcline_path(outgoings[0])    # This works
[{'start_pose': [222.2254118951833, 915.391419147894, 3.0714875295916646], 'end_pose': [191.21890533615024, 917.7923193982866, 3.0696631071597644], 'shape': 'RSL', 'radius': 999.999, 'segment_length': [9.660415496296451, 13.603044909701394, 7.835994888818709]}]
>>> outgoings[0] in {l['token'] for l in city_map.lane}      # check if the outgoing token is in the lane list
False
```

In fact, I need to access the `polygon_token` of `outgoings[0]` but I don't know any way other than using `city_map.lane`. I would appreciate it if you could help me with this issue.

Thanks
"
"Hi,

I'm trying to accumulate point clouds over whole scenes to recover the scene geometry.
However as noted in the documentation tz is always zero and results noticeable surface errors in the z direction.
Is there any workaround for this issue? Has anyone tried this before and got good results?
What is the reason to set tz to zero?

Thanks for your help! 
Bjarne"
"Hello,

I am trying to implement a prediction mechanism for stationary objects. The idea is as follows:
1- Get the timestamp of the keyframe X and keyframe X-1 (two consecutive frames having 0.5 seconds time difference). Keyframe X is the current frame.
2- Get the ego-pose of both frames using the timestamp. The ego-poses are already given in the dataset in terms of translation + rotation(quaternion). I convert quaternions to rotation matrices, merge with translations to create the 4x4 transformation matrix for both ego-poses, where the last row of them is always [0,0,0,1]. I use the formula in [this link](http://www.songho.ca/opengl/gl_quaternion.html#:~:text=Multiplying%20Quaternions%20implies%20a%20rotation,cheaper%20than%20the%20matrix%20multiplication.) to calculate the matrices. 
3- Get the relative positions of detected objects at keyframe X-1 with respect to the ego-vehicle's coordinate system at keyframe X-1. These relative positions are basically the output of the neural network.
4- Now I want to calculate the relative positions of detected objects with respect to the ego-vehicle's coordinate system at keyframe X. In other words, I am trying to calculate what would be the output of the network for keyframe X. I calculate the transformation matrix from the coordinate system at X-1 to X follows:

transformation = torch.matmul(torch.inverse(past_egopose), cur_egopose)
I also tried:
transformation = torch.matmul(torch.inverse(cur_egopose), past_egopose)

5- And then, I apply this transformation to the previous detection results:

predicted_object_poses = torch.matmul(transformation, object_poses)

However, I am unable to do correct predictions which I verified through visualization. The results are 1-2 meters away from what they should be. Do you see anything wrong with my method?

Best regards,

UPDATE: It seems I am missing the transformation from ego vehicle to lidar. I will try to put that and try again."
"I encounter the same problem mentioned in #700. The sanity check works fine on `numpy==1.18.1`, but fails in `numpy==1.21.5`.

In `numpy==1.18.1`, the function `np.unique` recognize different `nan`s as different unique values
```python3
>>> import numpy as np
>>> np.__version__
'1.18.1'
>>> np.unique([0.5, float('nan'), float('nan')])
array([0.5, nan, nan])
```

However, In `numpy==1.21.5`, only one `nan` is returned.
```python3
>>> import numpy as np
>>> np.__version__
'1.21.5'
>>> np.unique([0.5, float('nan'), float('nan')])
array([0.5, nan])
```"
"Hi I need a function that can map all the pixels in an image to point cloud using `hfov_image`, `vfov_image`, `heading`, `phi`, `theta` and `(x,y,z)` of camera / car"
"Hi,

Thank you for this powerful dataset. I have seen that you have a leader board for tracking (https://www.nuscenes.org/tracking?externalData=all&mapData=all&modalities=Any), however when I looked through your annotation jsons and the data annotation scheme you provide, I haven't found any tracking IDs for the annotated objects. Could you please tell me how to find them or how to generate them ?

Thank you for your help!"
"Hello, thank you for the great work.
I have a question about `prediction_scenes.json`. As far as I investigate it, it contains 833 scenes, and 17 scenes are missing.
Here is the missing 17 scenes in the file.
```
[scene-0012, scene-0034, scene-0062, scene-0349, scene-0417, scene-0418, scene-0440, scene-0441, scene-0442, scene-0449, scene-0568, scene-0630, scene-0632, scene-0919, scene-0924, scene-1087, scene-1105]
```
Is there any reason for that?"
"I am trying to evaluate my tracker results using the nuscenes 'evaluate.py' script. I am using the v1.0-trainval version but I am only running my tracker on the first scene (of the pointPillars-val.json). As a result, I am getting an error (AssertionError: Samples in split don't match samples in predicted tracks.) when running the evaluate script. I believe this is because the script is trying to evaluate over the entire data set while I am only inputting tracking results for the first scene. How can I modify the 'evaluate.py' script to evaluate over a select number of scenes ONLY? If I am mistaken, what might the issue be? Thanks in advance!"
"Do I need to guarantee the order of the test data when testing?
I have the data sorted by time.
![image](https://user-images.githubusercontent.com/53482524/151283193-e27f9804-c7fb-4cfe-9c4c-234b89c20057.png)
"
"Hi, how can I download the dataset by ```wget``` on Linux Server? It will be ```ERROR 403 : Forbidden``` when I use ```wget link```. "
"I am trying to figure out how to load multiple radar sweeps with the correct velocities.
In viewing multi radar sweeps:
https://github.com/nutonomy/nuscenes-devkit/blob/05d05b3c994fb3c17b6643016d9f622a001c7275/python-sdk/nuscenes/nuscenes.py#L1299-L1310
The code here is just transforming all the velocity from the radar coordinate to lidar. But shouldn't each previous sweep's velocity be first converted to the current frame using their own previous ego_pose? Or it doesn't make a difference?

So do I need to convert my previous sweep velocity to the current velocity, by rotating from previous_sensor to previous_ref to world and then to current?

Thank you for your help
"
" Please take a look at this paper, specifically Figure 5. https://arxiv.org/pdf/2111.13672.pdf


In this scene, from frame 98 to frame 114, there is a single track, yet the ground truth only registered information sporadically in that duration. In my opinion, the ground truth might be wrong here? since if the object remain in the FOV, it should be part of the ground truth?


Such a ground truth might lead to inaccurate tracker evaluation? Curious about your thoughts on this issue.

"
"Hello
Thanks for your open-source code. It is greatly useful.
I have two problems with data. First, each lane is a list of ArcLinePath objects. Each ArcLinePath has a ""shape"" attribute which is a 3-letter string. What exactly is this ""shape"" indicating? (I have visualized the lanes and didn't get their meaning.) Why isn't this attribute the same for all ArcLinePaths of a specific lane?
Second, is there any way to distinguish left/right neighbor lanes of a lane (if there is)? I understand that we can use trajectories for this task, but I'm asking if there is a more reliable solution.

Thank you very much!"
"Hi,
I want to knoe the distribution of 3d bounding box size(distribution) distribution, how can i get it？Can you give me some visualization results or code？

I read the original paper of nuscenes and found that there is only a size distribution for cars, not a bounding box distribution for all categories.

Thanks!
:)"
"Is ego coordinate system the same as IMU coordinate system?
Whether the world coordinate system and IMU coordinate system have the same direction and different positioning points?
"
"Hi,
Thanks your repo, nice work.
I have some questions:
- Is the point cloud data positive？ or will there be negative numbers？
- What's the max and min value of 3D Bounding box size [w,l,h]?
- Bounding box location center correspond to points on the point cloud？or do they not necessarily correspond to points on the point cloud？"
"Hi all,

Thank you for the wonderful dataset. I followed the tutorials to better understand it. I am kind of new to point clouds and trying to understand how to use LiDAR pointcloud dataset with graph neural networks (GNNs). What should be the right workflow for such an application? (e.g. loading all the point clouds, storing them in an array, forming a graph, training the model etc.). As there are lots of modules and methods in the repo, I am kind of lost. I don't know which function to use for what. If you can provide me some starting points, guidelines or refer some tutorials/github repos that would be great. Thanks in advance!

Best,
Emre"
"Hi, thanks for attention! I'm new in using Lidar data to machine learning,  so my problem may be trivial to you, or even some don't make sense.

I want to do object detection mission, and I am trying giving 3D bounding box to the raw Lidar data. And I use  `v1.0-mini`. My questions are:

1. Do every point clouds in  `.pcd.bin` file have labels? For example, the exact point cloud is a vehicle. Or they only have the ground truth boxes data like:
```
{
""token"": ""70aecbe9b64f4722ab3c230391a3beb8"",
""sample_token"": ""cd21dbfc3bd749c7b10a5c42562e0c42"",
""instance_token"": ""6dd2cbf4c24b4caeb625035869bca7b5"",
""visibility_token"": ""4"",
""attribute_tokens"": [
""4d8821270b4a47e3a8a300cbec48188e""
],
""translation"": [
373.214,
1130.48,
1.25
],
""size"": [
0.621,
0.669,
1.642
],
""rotation"": [
0.9831098797903927,
0.0,
0.0,
-0.18301629506281616
],
""prev"": ""a1721876c0944cdd92ebc3c75d55d693"",
""next"": ""1e8e35d365a441a18dd5503a0ee1c208"",
""num_lidar_pts"": 5,
""num_radar_pts"": 0
}
```

2. How to use the bounding box to train the model?   
      I want give bounding boxes to point clouds, just like what we do in CV object detection mission. But the difference between point clouds and pictures is that point clouds stored in `.pcd.bin` file are unordered. So my question is: how to train a model that can give bounding box to raw point clouds, so that I can let the computer know that this point clouds cluster forms a vehicle, and the others forms a motor?   Is there any code sample?

3. Is it possible for me to make a dataset that every point cloud is labeled? 
"
"Hi, I tried to render the video using `render_scene_channel` function, but it would also render all the bounding boxes of objects in the scene. I'm wondering is there a way I can render the video without bboxes, i.e., just the original video? Or can you please add an option to the `render_scene_channel` function to let user decide whether to render bboxes or not? Thank you!"
"I downloaded the full dataset and divided it into two parts, when I tried to load the data, it prompted as following:
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/tmp/ipykernel_156861/93392779.py in <module>
      3 data_root = '/public/MARS/datasets/nuScenes/'
      4 
----> 5 nusc = NuScenes(version='v1.0-trainval-val', dataroot=data_root, verbose=True)
      6 
      7 sensor_list = [

~/miniconda3/lib/python3.9/site-packages/nuscenes/nuscenes.py in __init__(self, version, dataroot, verbose, map_resolution)
    122 
    123         # Make reverse indexes for common lookups.
--> 124         self.__make_reverse_index__(verbose)
    125 
    126         # Initialize NuScenesExplorer class.

~/miniconda3/lib/python3.9/site-packages/nuscenes/nuscenes.py in __make_reverse_index__(self, verbose)
    185         for record in self.sample_data:
    186             if record['is_key_frame']:
--> 187                 sample_record = self.get('sample', record['sample_token'])
    188                 sample_record['data'][record['channel']] = record['token']
    189 

~/miniconda3/lib/python3.9/site-packages/nuscenes/nuscenes.py in get(self, table_name, token)
    214         assert table_name in self.table_names, ""Table {} not found"".format(table_name)
    215 
--> 216         return getattr(self, table_name)[self.getind(table_name, token)]
    217 
    218     def getind(self, table_name: str, token: str) -> int:

~/miniconda3/lib/python3.9/site-packages/nuscenes/nuscenes.py in getind(self, table_name, token)
    223         :return: The index of the record in table, table is an array.
    224         """"""
--> 225         return self._token2ind[table_name][token]
    226 
    227     def field2token(self, table_name: str, field: str, query) -> List[str]:

KeyError: 'e93e98b63d3b40209056d129dc53ceee'
```
the code is extreamly simple:
```
from nuscenes.nuscenes import NuScenes

data_root = '/public/MARS/datasets/nuScenes/'
nusc = NuScenes(version='v1.0-trainval-val', dataroot=data_root, verbose=True)
```
PTAL, I have no idea where go wrong..."
"Hi, I met a problem when using the tracking evaluation, with this assertion triggered: 
https://github.com/nutonomy/nuscenes-devkit/blob/05d05b3c994fb3c17b6643016d9f622a001c7275/python-sdk/nuscenes/eval/tracking/algo.py#L158-L161
The assertion was triggered by the case of multiple NaNs occurring in the threshold list, e.g. `thresholds=[nan, nan, nan, nan, nan, nan, nan, nan, 0.14626915256182352, 0.17571373816047395, 0.1982034333050251, 0.22260631062090397, 0.24383812966558294, 0.2603936386294663, 0.2848243469541723, 0.3308456484003034, 0.3668973971960257, 0.3824963077902794, 0.39220086903106877, 0.41123709362000227, 0.4171593593699591, 0.42581389248371126, 0.4534411536795752, 0.4758617915213108, 0.5016027579412741, 0.5305963274505403, 0.5490754961967468, 0.5555436402559281, 0.5697322227060795, 0.5863827850956184, 0.5895171731710434, 0.5930080786347389, 0.5991135852776776, 0.6053177920034692, 0.6182821939388911, 0.6370792790101125,0.6611657304068407, 0.6718776701018214, 0.6763528749346733, 0.7061878126114607]` and `num_thresholds=40`.
Not sure about the purpose of this sanity check. Is multiple NaNs occurence an invalid situation here?
P.s. Temporally I fixed this by changing L160 to:
```python
duplicate_thresholds = len(thresholds) - len(np.unique(thresholds)) - (0 if unachieved_thresholds <= 1 else unachieved_thresholds - 1)
```"
"hi, i have some questions about the relationship between the coordinate systems.
1. Is the calibrated_sensor.json depicts the extrinsic parameter between the sensor coordinate and the ego coordiante ? if the origin the ego coordinate is the center of the rear axle projected onto the ground, so where does the axes point to ? is it fixed orientation ?
2. Is the orientation of imu coordinate fixed ? does the x. y. z axes always point to east. north. sky direction ?
3. Does the 'rotation' and 'translation' in the ego_pose.json file depicts the relationship between world coordinate and ego coordinate ?
or between world coordinate and imu coordinate ?
"
Thanks for attention.
"Hi, when i submit the json file for the tracking challenge as specified by the webisite, i got the following error prompt. Yet i checked the result, there are at most 58 bounding boxes per sample, so not sure why it says i have over 500 bboxes per sample?







Traceback (most recent call last):
  File ""/code/scripts/workers/submission_worker.py"", line 491, in run_submission
    submission_metadata=submission_serializer.data,
  File ""/tmp/tmppdezx0d_/compute/challenge_data/challenge_476/main.py"", line 98, in evaluate
    verbose=verbose)
  File ""/usr/local/lib/python3.7/site-packages/nuscenes/eval/tracking/evaluate.py"", line 84, in __init__
    verbose=verbose)
  File ""/usr/local/lib/python3.7/site-packages/nuscenes/eval/common/loaders.py"", line 48, in load_prediction
    ""Error: Only <= %d boxes per sample allowed!"" % max_boxes_per_sample
AssertionError: Error: Only <= 500 boxes per sample allowed!"
"This issue is about `arcline_path_utils.py`

So far as I know, each `arcline_path` is divided into 3 segment. To get the coordinates of  a pose at `s` meters along the arcline path, first find out which of the 3 segment this pose is located, then calculate the arcline path distance from the starting point of the segment found.
Let's denote the radius of the curvature of the segment as `r` and assume the shape of the segment is `L` (left). 

Then function `get_transformation_at_step()`  will output the `xy` distance from the starting point of the segment after **Rotation Transformation**：(`sin\theta` * `r`,   `(1-cos\theta)` * `r`, `theta` )

Finally, function `apply_affine_transformation()` will output the final coordinates.
My confusion is indeed about the `apply_affine_transformation()` function:
My understanding is `pose[2]` is the **yaw** of  the starting pose，why does this function rotate the distance (`sin\theta` * `r`,   `(1-cos\theta)` * `r`)  by  `pose[2]` radians counterclockwise? why rotate the distance? why rotate `pose[2]` radians?
https://github.com/nutonomy/nuscenes-devkit/blob/05d05b3c994fb3c17b6643016d9f622a001c7275/python-sdk/nuscenes/map_expansion/arcline_path_utils.py#L79-L92

Is there any tutorial or illustration of this process?

"
"Scene descriptions (weather/night/...) are provided for nuScenes, but I didn't find similar schema for nuImages. Is it possible to get such descriptions for images in nuImages?"
"Hi

When I call the `_get_layer_polygon` function in `NuScenesMapExplorer`, I have found a polygon is invalid.
the polygon token is `43e22c72-d1bf-4f95-a1cf-81ea98b3e778` and the layer name is `road_segement` in `boston-seaport`.

However I haven't found any reason for why this polygon is invalid by visualization. "
"Hello,

Thanks a lot for the code. I'm a little confused on how to get the total number of true positives, false positives, true negatives and false negatives per class. In addition to that, I would to print the confusion matrix. Do you happen to have a function to perform that? 

PS I've found TP and FP in here [(https://github.com/nutonomy/nuscenes-devkit/blob/05d05b3c994fb3c17b6643016d9f622a001c7275/python-sdk/nuscenes/eval/detection/algo.py#L127 ] but the shape doesn't correspond to what I'm expecting.

Thank you for your help :)"
"Hi, Dear 
    Are the nuImages sample data tokens related to the data in nuScenes? Like can I use nuImages data find corresponding annotations in nuScenes?

Regards,"
"Hello,

where could I find the height of the camera in meters for NuScenes videos?

Thanks a lot"
"Hi, how can I retrieve the lane on the left/right of the target lane? In map_api the connectivity only tells incoming and outgoing lanes, but I somehow need to access neighbouring lanes as well - any idea how I can achieve that?

Update: the way I approach this is by retrieving all lanes in a patch, then store the node tokens of all of their lane divider segments and look for matches in another loop (e.g. find if any lane `lane_a` has a right_lane_divider_segments overlapping with some other lane `lane_b`'s left_lane_divider_segments, if yes then I call `lane_b` the right neighbour of `lane_a`). However, I'm not sure if I'd miss any pairs."
"Hi!

I'm trying to match bounding boxes from a pertained YoLo network to the 2D projections of bounding boxes in NuScenes. I'm getting the pixel corners of the image using the viewpoints and post_process_coords functions:

```  
    %Project 3d box to 2d.
    corner_coords = view_points(corners_3d, camera_intrinsic, True).T[:, :2].tolist()
    corner_coords_list.append(corner_coords)
    box_2D = post_process_coords(corner_coords)
    box_pixel_nuscene.append(box_2D)
```

These are the ground truth annotation boxes (in 3D) for the detection challenge
![3d_image](https://user-images.githubusercontent.com/13437374/142960932-cf602167-8f85-4c3c-bbfc-185b3f3db537.png)
:


However, when I try to render the 2D projections onto the image frame, I do not get the right results:

<img width=""535"" alt=""Screen Shot 2021-11-22 at 6 15 11 PM"" src=""https://user-images.githubusercontent.com/13437374/142962029-3b14c694-27ee-459b-8008-94574706fc08.png"">

Specifically, I render the 2D boxes as follows:

1. Load the image:
`img = cv2.imread(data_path_f) # Successfully reading data`

2. Function to render 2D boxes (in pixel coordinates) on the image frame. Note that box argument in the following functions represents in pixel dimensions: (x,y, wx, wy), where (x,y) are the pixel coords of the lower-left corner, and wx is the width of the 2D bounding box in the x-dimension and wy is the width of the bounding box along the y-dimension. On the image, x-axis points to the right and y-axis points upward.

```
def plot_pixel_boxes(box, img, a, plot_labels=True, color = None):
    colors = torch.FloatTensor([[1,0,1],[0,0,1],[0,1,1],[0,1,0],[1,1,0],[1,0,0]])
    
    def get_color(c, x, max_val):
        ratio = float(x) / max_val * 5
        i = int(np.floor(ratio))
        j = int(np.ceil(ratio))
        
        ratio = ratio - i
        r = (1 - ratio) * colors[i][c] + ratio * colors[j][c]
        return int(r * 255)
    
    width = img.shape[1]
    height = img.shape[0]
    
    a.imshow(img)
    
    % Set the default rgb value to red
    rgb = (1, 0, 0)
    x1 = box[0]
    y2 = box[1]
    
    % Use the same color to plot the bounding boxes of the same object class
    if len(box) >= 7 and class_names:
        cls_conf = box[5]
        cls_id = box[6]
        classes = len(class_names)
        offset = cls_id * 123457 % classes
        red   = get_color(2, offset, classes) / 255
        green = get_color(1, offset, classes) / 255
        blue  = get_color(0, offset, classes) / 255

        % If a color is given then set rgb to the given color instead
        if color is None:
            rgb = (red, green, blue)
        else:
            rgb = color

    % Calculate the width and height of the bounding box relative to the size of the image.
    width_x = box[2]
    width_y = box[3]
        
    rect = patches.Rectangle((x1, y2),
                                     width_x, width_y,
                                     linewidth = 2,
                                     edgecolor = rgb,
                                     facecolor = 'none')

    % Draw the bounding box on top of the image
    a.add_patch(rect)
    plt.show()
```

3. We use the same plot_boxes function to plot the YoLo  (pertained; not trained on nuscenes) detection boxes and they find the objects quite well:
<img width=""556"" alt=""Screen Shot 2021-11-22 at 6 15 30 PM"" src=""https://user-images.githubusercontent.com/13437374/142962009-4df45560-aeb6-41bd-a116-6d048b49c8c4.png"">

I'm using the same plot_pixel_boxes function to get the third image. 

So, my questions are:
1. How do I properly render the 2D bounding boxes on the image? 
   A) Do I use the render_cv2 function? However, this seems to deal with radar or lidar point clouds and not just image data. Still, would render_cv2 give me the 2d bounding box given by the pixel coordinates?
   B) I'm perhaps not downloading the image properly; it seems like the pixel coordinates are very off from the second figure. 

**My goal is to match the bounding boxes from the perception algorithm (ex: yoLo) to the 2d pixel bounding boxes from the nuscenes dataset to count whether or not that object has been properly detected.**

Thanks!"
"I am attempting to utilize the PointPillars Detections (https://www.nuscenes.org/tracking?externalData=all&mapData=all&modalities=Any, Detections download. pointpillars-val.json) to analyze the performance of my deterministic tracking system, and am running into a case where there are hundreds of detections, but only a single annotated object in the sample sample frame. 

I am utilizing ijson to parse the raw json file provided, splitting up the detected objects in a given sample frame and writing it to a ROS bagfile which I can then utilize for evaluating my tracking system. 

The problem I am having is that per sample token I am seeing upwards of 400 detected objects with detection_name 'car' or 'pedestrian' in a sample with only one object.

For example sample token: b9c60cfaf1814c8bb363037dec7abd35

with the following camera views:
![image](https://user-images.githubusercontent.com/21249830/142909372-7730722f-bbf7-4104-81f8-fe55bd49ea1e.png)
![image](https://user-images.githubusercontent.com/21249830/142909473-9d704230-71a0-4d37-9cf6-def3b58ad6ae.png)
![image](https://user-images.githubusercontent.com/21249830/142909508-f64a9b44-027d-4af5-8126-c06106e263e6.png)
![image](https://user-images.githubusercontent.com/21249830/142909544-651498e4-1e9a-4b25-811c-7b9b6f66ba64.png)
![image](https://user-images.githubusercontent.com/21249830/142909576-9c4062e2-b1d5-4ca7-af70-7e01614b8632.png)
![image](https://user-images.githubusercontent.com/21249830/142909609-110e96b5-1fc9-466f-8c13-906601abc842.png)

results 396 'cars' or 'pedestrians' in the sample frame. Which I am then visualizing in rviz displays the following:
![image](https://user-images.githubusercontent.com/21249830/142909784-1e828d07-5773-47f3-909a-513e5dd13610.png)
Where each blue cube is a detection of the size, orientation and translation provided from the json. This large number of detections can also be verified by simply running:

 grep -nr 'b9c60cfaf1814c8bb363037dec7abd35*' .

In the directory where the files are located. Since each detection begins with the sample_token it will show all the scene results frame and all detections from that frame. Which also displays this high number of results.

I would highly appreciate some help with this, as I am assuming I am making some incorrect assumption about the data provided.

Thanks,

Calvin"
"Thanks!
I have two questions about coordinates transform.
1. Labels are in the global coordinate system. Which is the global coordinate system？

![image](https://user-images.githubusercontent.com/59494593/141674057-9a070dd4-cc3c-4719-b3ee-52d195e30470.png)

2. I want to get labels in lidar coordinates, I did the following steps：
```
  ref_lidar_path, ref_boxes, _ = get_sample_data(nusc, ref_sd_token) 
  locs = np.array([b.center for b in ref_boxes]).reshape(-1, 3)
  dims = np.array([b.wlh for b in ref_boxes]).reshape(-1, 3)
  rots = np.array([quaternion_yaw(b.orientation) for b in ref_boxes]).reshape(-1, 1)
  gt_boxes = np.concatenate([locs, dims,  -rots - np.pi / 2], axis=1) ###
  # gt_boxes = np.concatenate([locs, dims, rots], axis=1)                 ###
```
I use `get_sample_data`. Has it converted the yaw angle to the lidar coordinate system? 
Maybe `-rots - np.pi / 2` is no use?I just need to use `gt_boxes = np.concatenate([locs, dims, rots], axis=1)`
```
def get_sample_data(
    nusc, sample_data_token: str, selected_anntokens: List[str] = None
) -> Tuple[str, List[Box], np.array]:
    """"""
    Returns the data path as well as all annotations related to that sample_data.
    Note that the boxes are transformed into the current sensor's coordinate frame.
    :param sample_data_token: Sample_data token.
    :param selected_anntokens: If provided only return the selected annotation.
    :return: (data_path, boxes, camera_intrinsic <np.array: 3, 3>)
    """"""

    # Retrieve sensor & pose records
    sd_record = nusc.get(""sample_data"", sample_data_token)
    cs_record = nusc.get(""calibrated_sensor"", sd_record[""calibrated_sensor_token""])
    sensor_record = nusc.get(""sensor"", cs_record[""sensor_token""])
    pose_record = nusc.get(""ego_pose"", sd_record[""ego_pose_token""])

    data_path = nusc.get_sample_data_path(sample_data_token)

    if sensor_record[""modality""] == ""camera"":
        cam_intrinsic = np.array(cs_record[""camera_intrinsic""])
        imsize = (sd_record[""width""], sd_record[""height""])
    else:
        cam_intrinsic = None
        imsize = None

    # Retrieve all sample annotations and map to sensor coordinate system.
    if selected_anntokens is not None:
        boxes = list(map(nusc.get_box, selected_anntokens))
    else:
        boxes = nusc.get_boxes(sample_data_token)

    # Make list of Box objects including coord system transforms.
    box_list = []
    for box in boxes:

        # Move box to ego vehicle coord system
        box.translate(-np.array(pose_record[""translation""]))
        box.rotate(Quaternion(pose_record[""rotation""]).inverse)

        #  Move box to sensor coord system
        box.translate(-np.array(cs_record[""translation""]))
        box.rotate(Quaternion(cs_record[""rotation""]).inverse)

        box_list.append(box)

    return data_path, box_list, cam_intrinsic
```
"
"Hi, thanks for sharing the great database and the toolkit. Several days ago I submited a result with the wrong metadata for the nuScenes tracking benchmark. My method actually uses both LiDAR and camera but I only set use_lidar to True. Is it possible to change the metadata or I have to resubmit a correct .zip file?  

Besides, I would like to know when will the leaderboard (for both tracking and detection) be updated on the main website? Thanks in advance.

Best,
Xuyang."
"Both discretize_centerlines and discretize_lanes use arcline_path_3. According to 

https://forum.nuscenes.org/t/lane-path-not-matching-lane-polygon-length/377

arcline_path_3 gives the path of an AV assuming no obstacles instead of true center lines. How were these paths created? Are they based on a path planning algorithm, actual AV motion, or something else?


"
"The computation for distance_along_lane is:
    distance_along_lane = closest_pose_index * 0.5
I believe the 0.5 should be resolution_meters. The default value for resolution_meters is 0.5, but the distance will be incorrect if the input is change."
"I have ploted camera poses of scene-655 and I found that 6 camera poses were not symmetric which could be seen below. 
![image](https://user-images.githubusercontent.com/27892562/140722171-4fc06cbb-0fc3-40b5-87b7-f919ea2239a0.png)
How could I fix this? Thanks in advance."
"I noticed that the link to the annotator instructions on this (https://nuscenes.org/nuscenes#data-format) page is broken. 

It should probably be fixed and link to this (https://github.com/nutonomy/nuscenes-devkit/blob/master/docs/instructions_nuimages.md) page"
"I used export_kitti.py to convert NuScenes data to Kitti format,
I used the following to get yaw, pitch and roll of the camera (which is the same for all of the frames):

```
            cs_record_cam = self.nusc.get('calibrated_sensor', sd_record_cam['calibrated_sensor_token'])
            angles_camera = np.array(Quaternion(cs_record_cam['rotation']).yaw_pitch_roll) / np.pi * 180

```

How do I get yaw, pitch and roll per frame?

Thanks"
"I believe there's a memory leak in [render.py](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/detection/render.py#L23). My computer crashes when I run this script for 8000 lidar samples (profiled by tracking htop memory usage). I resolved it by changing this [line](https://github.com/nutonomy/nuscenes-devkit/blob/864d0a207539e5383cd3eb26ebb1d7a44622f09d/python-sdk/nuscenes/eval/detection/render.py#L68) from 
`_, ax = plt.subplots(1, 1, figsize=(9, 9))` to `fig, ax = plt.subplots(1, 1, figsize=(9, 9))` and then closing the figure created."
"Hi. Thank you for all the great works.

I have found out that the layers and the bitmap image are not quite well aligned.
Below is the code that I used.

```
import matplotlib.pyplot as plt

from nuscenes.map_expansion.map_api import NuScenesMap
from nuscenes.map_expansion.bitmap import BitMap

nusc_map = NuScenesMap(dataroot='/home/sr/dev/hdmap_parser', map_name='boston-seaport')
bitmap = BitMap(nusc_map.dataroot, nusc_map.map_name, 'basemap')
fig, ax = nusc_map.render_layers(['drivable_area'], figsize=1, bitmap=bitmap)
plt.show()
```
And I see that the bmp image and the layers are not aligned well, as it can be seen from the snipshot below.
![image](https://user-images.githubusercontent.com/84299390/139797909-15500c1b-23c8-400f-b34d-1663e61107e9.png)"
"Hi,

I don't actively work in computer vision, so the following question might be a trivial one. 
I am trying to apply YoLo for object detection to the nuscenes dataset on the vision data only. YoLo returns pixel coordinates of the objects detected in the image. **How do I convert this pixel output into the camera coordinate frame consistent with nuscenes?** After that, I need to go from the camera coordinate frame to the global coordinate frame, and I was hoping to modify box_to_sensor to do that. https://github.com/nutonomy/nuscenes-devkit/blob/864d0a207539e5383cd3eb26ebb1d7a44622f09d/python-sdk/nuscenes/eval/common/utils.py#L130"
"Hi, I think L1323 to L1325 in `python-sdk/nuscenes/nuscenes.py` might use the wrong `rotation_vehicle_flat_from_vehicle`. If my understanding is correct, `rotation_vehicle_flat_from_vehicle` is the transformation (without translation) from the ego body frame `E` to an ego body frame `E'` with a zero yaw such that the z-axis is parallel to the world frame `W`. 

The current code is

```python
rotation_vehicle_flat_from_vehicle = np.dot(
                    Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).rotation_matrix,
                    Quaternion(pose_record['rotation']).inverse.rotation_matrix)
```

I think L1323 to L1325 should be

```python
rotation_vehicle_flat_from_vehicle = np.dot(
                    Quaternion(scalar=np.cos(ego_yaw / 2), vector=[0, 0, np.sin(ego_yaw / 2)]).inverse.rotation_matrix,
                    Quaternion(pose_record['rotation']).rotation_matrix)
```

instead of your version,

to firstly map the points in `E` to `W`, then `W` to `E'`.

If I am right, this mistake is not obvious to catch because the roll and pitch are very close to zeros such that `rotation_vehicle_flat_from_vehicle` is almost an identity matrix. But you can still observe the small difference in the visualization.

Thanks!"
"Hi, Thanks for your great datasets. I am a new for this research area.
My network is designed for object detection.  And I want to load nuScenes datasets to my network to replace the Kitti datasets. 
I'm confused about how to load the data and evaluate the model. I need to load pointcloud (keyframes and sweeps), label, calibration files. Can you give me some suggestions, such as whether there are specific examples.
Thanks!"
"Hi,

I am trying to get the camera geometry of all the cameras and lidar (plot their centers and X, Y, Z-axis in a 3D plane). Also trying out Birds Eye View surrounding from the camera parameters. Can anyone please help me out?"
"Hello, thanks for the awesome dataset! Following the [nuScenes prediction tutorial](https://www.nuscenes.org/tutorials/prediction_tutorial.html), I'm able to get the map (attached below) from the perspective of the agent in the sample which is specified by the `instance_token_img`. Could you clarify how can I get the same map but from the perspective of the SDV?

Currently, my solution is to change [this line](https://github.com/nutonomy/nuscenes-devkit/blob/9b209638ef3dee6d0cdc5ac700c493747f5b35fe/python-sdk/nuscenes/prediction/input_representation/agents.py#L266) to the rotation of the ego vehicle (instead of the rotation of the vehicle specified by the `sample_token_img`) and do the same for the translation and, finally, repeat the same for the `StaticLayerRasterizer`. I thought this is too cumbersome and was wandering is there a more elegant way to get it done?


```python
from nuscenes import NuScenes
from nuscenes.prediction import PredictHelper

nuscenes = NuScenes('v1.0-mini', dataroot='/data/sets/nuscenes')
helper = PredictHelper(nuscenes)
```

```python
import matplotlib.pyplot as plt
%matplotlib inline

from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.combinators import Rasterizer

static_layer_rasterizer = StaticLayerRasterizer(helper)
agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)
mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())

instance_token_img, sample_token_img = 'bc38961ca0ac4b14ab90e547ba79fbb6', '7626dde27d604ac28a0240bdd54eba7a'
anns = [ann for ann in nuscenes.sample_annotation if ann['instance_token'] == instance_token_img]
img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)

plt.imshow(img)
```
![download](https://user-images.githubusercontent.com/57019863/138572247-0af2e714-6e33-455a-9718-9e7df8ecc79a.png)"
"Hi! 

I hope everything is going well with the nuScenes dataset!

I have a few (simple?) questions regarding the data for the nuScenes prediction challenge. As introduced in the prediction tutorial (https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/prediction_tutorial.ipynb), the get_prediction_challenge_split() function returns a list of strings of the instancetoken+sampletoken. In my understanding, these are the vehicle object instances that need to be predicted for evaluation. 

However, I realized that these vehicle instances are only a very small subset of all vehicle instances in the nuScenes dataset. When I just pull out of all vehicles including cars, buses, trucks, emergency and construction vehicles from the train, val, and test split, it looks like there are far more vehicles existing, and the vehicle instances returned by get_prediction_challenge_split() are only about 10% of all vehicle instances. Is there a standard about how and why those vehicle instances are filtered out, e.g., near the end of frames, static without movement?

Also, is there a reason not to include trailer, motorcycle, and bicycle in the prediction data which are vehicles as well, as defined in the nuScenes data annotation page (https://www.nuscenes.org/nuscenes#data-annotation)?

Thanks for your clarification in advance!!

"
"I have seen lidar data rendering in tutorial, but any suggestions on how i can use that code for cropping lidar files projected only for front camera. i want to use only fron camera and same field of view of lidar scans."
"I am trying to convert xyz points into spherical coordinates with LiDAR as the origin, in which x=r * cos(phi) * cos(theta), y=r * cos(phi) * sin(theta), z=r * sin(theta). And want to visulize point clouds that they are arranged as LiDAR beams.

the transforming and visulizing codes as follows:

    radius = torch.sqrt(pts[:, 0].pow(2) + pts[:, 1].pow(2) + pts[:, 2].pow(2))
    sine_theta = pts[:, 2] / radius
    # theta [-pi/2, pi/2]
    theta = torch.asin(sine_theta)
    # phi [-pi, pi]
    phi = torch.atan2(pts[:, 1], pts[:, 0])
    im = torch.zeros(2001, 2001, 3)
    y_img = (phi + math.pi) * 1000 / math.pi
    y_img = y_img.round()
    x_img = (sine_theta + 1) * 1000
    im[x_img, y_img] = 1
    im = im.permute(2, 0, 1)
    vutils.save_image(im, 'rangeview/' + 'sample' + '.jpg')

the results are confusing: 
some range views are exactly arranged well as 32 LiDAR beams
![image](https://user-images.githubusercontent.com/73167393/136490533-5eaa19e4-daee-49eb-8dea-b36329cdcd7a.png)

but many range views have artifacsts that top right corner beams are upwards inclined
![5026501a2f1a5c77864b42cc755b474](https://user-images.githubusercontent.com/73167393/136490488-5a787ee0-9db2-4d90-a132-2c4b0b613a48.png)

Is the inherent noise in nuscenes datasets or are there some wrong in my code or my understanding?"
""
"I’m first to use the nuSenes-Data, it’s feel good. I can train ‘v1.0-mini’, but when training the ‘v1.0-trainval01_blobs_lidar’, it shows that ’sets/nuscenes/samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603547590.pcd.bin’ in val_dataset_loader. I check the samples datas and the sweeps data, which miss the ‘n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603547590.pcd.bin’.
How to use the  ‘v1.0-trainval01_blobs_lidar’ to train? whether I miss some file or I should modify the codes? 
Thank you!"
"Hi Author,
It seems that the API don't support it friendly, Can I have some ideas to make it ?"
"I'm currently working with the NuScenes dataset for trajectory forecasting. There is just one thing I don't fully understand.

I'm using the get_prediction_challenge_split to get the <instance_token>_<sample_token> to be able to identify the agents, I've seen your code and it seems like those strings are saved in a .json file and the function retrieves them according to the name of the split ( in my case I am using the mini-train)

But there are only 742 observations of <instance_token>_<sample_token> with 68 differente instances.  

In the other hand, if I traverse the dataset with the nuscenes.sample_annotation and I filter those results by all the agents that are vehicles and all the agents that belong to the mini-train split scenes, I get 6604 unique observations of the form <instance_token>_<sample_token> with 338 different instances. 

That's a huge difference, is there any reason  why the data retrieved by the get_prediction_challenge_split retrieves only those 742 observations saved in the .json file and ignores the other ones? 

Thanks in advance. "
"I know the data is collected from Boston Seaport, Singapore’s One North, Queenstown and Holland Village districts. But how to split it for each city?  (e.g. scenes-0001~scenes-1111 are from Boston Seaport), "
"I want to know whether sceces will overlap somewhere. Because of #381 , I could not get gps data to verify."
"Thank you for your wonderful work!

I'm trying to reuse the training code for SemanticKITTI for Panoptic-PolarNet to train the network on NuScenes (https://github.com/edwardzhou130/Panoptic-PolarNet/blob/main/train.py)

To do so, I need to be able to get a cloud representing particular instance on a certain scene.
I see that I can get instances tokens via `nusc.sample_annotation` but it's still not quite clear for me how can I get the list of all instances on a certain scene with its pointcloud and instance label.

Thank you!"
"Hi,

I would like to ask if the longitude and latitude units of the autonomous vehicle are defined in the nuscenes dataset?

Thanks."
"For the ground truths, the rotation angle starts counting from the negative y axis (for TOP LiDAR coordinate system), when the object rotate clockwisely, the angle becomes positive, otherwise it becomes negative. The range of the angle is [-pi, pi], am I right? Thank you."
"Hey,
I see there is only 11 points (i assume that they are x, y) in each mode, are they of interval 1 second? it's really large intervals to be considered as trajectory output if they are. And what about the other states like velocity? 
thanks!"
"Thanks for your great work! I want to know how to filt out moving objects like vehicle in point cloud. I could only find lidar-seg tutorials to render labels I have chosen. But I could not get the **point cloud coordinates.** I follow the code from [lift-spalt-shoot](https://github.com/nv-tlabs/lift-splat-shoot/blob/07b77905c04b590fd17f3ae2b9e312089854504b/src/tools.py#L23)
![image](https://user-images.githubusercontent.com/27892562/131058496-74255307-6677-413a-bb67-7d119d74fc2c.png)
So could someone tell me how to filt the points that I do not need?"
"In the README file of can-bus, it is shown that the range of **Zoe Sensors/brake-sensor** is [0.375,0.411].
![image](https://user-images.githubusercontent.com/32608622/130411163-10a54275-7228-4895-9858-407c0458c01a.png)
But I find out that the brake sensor is not in that range [0.375,0.411].
![image](https://user-images.githubusercontent.com/32608622/130410864-968fc345-e326-4381-b40d-1d8d5d826dad.png)

Steering-sensor and throttle-sensor have the same question that do not has the same range stated in README.
I am confused about this point and want to know the exact range of these three sensors."
"Hi developers,

I'm new to nuscenes and tried to make some sense out of the dataset. I downloaded the `mini` version of the `Full dataset` from [nuscenes](https://www.nuscenes.org/download) and followed the tutorial. I did the following code:

```
nusc = NuScenes(version='v1.0-mini', dataroot='/home/user/data/sets/nuscenes/v1.0-mini/', verbose=True)
cfg = config_factory('detection_cvpr_2019')
nusc_eval = DetectionEval(nusc, config=cfg, result_path='./example_submission.json', 
                     eval_set='mini_train', output_dir='./res', verbose=True)

s = nums_eval.sample_tokens[0]  # first sample
print(s)
print(nums_eval.gt_boxes.boxes[s], nums_eval.pred_boxes.boxes[s])
```

And I get as the printed value:
```
ca9a282c9e77460f8360f564131a8af5
33 1
```

About the above results, I have several questions:
1. Is `nusc_eval.sample_tokens` equivalent to timestamp? If so, is it an ordered list(by ascending time)?
2. Is `nums_eval.gt_boxes.boxes[s]` a dictionary with `{timestamp: [obstacle_1_box, obstacle_2_box...]}` structure?
3. Why is  the length of `nums_eval.pred_boxes.boxes[s]` always 1? Are the length not supposed to be the number of obstacles detected? (E.g. the length of `nums_eval.gt_boxes.boxes[s]` always differ)
"
"Hi, this is a good job!

Can you provide some parameters of radar, such as range resolution, the number of chirps in one sequence, the number of samples on each chirp, and so on?

Thanks!"
"Has anyone seen this mistake？
----------------------------------------------------------   
metrics = nusc_eval.main(render_curves=False)
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/detection/evaluate.py"", line 204, in main
    metrics, metric_data_list = self.evaluate()
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/detection/evaluate.py"", line 116, in evaluate
    md = accumulate(self.gt_boxes, self.pred_boxes, class_name, self.cfg.dist_fcn_callable, dist_th)
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/detection/algo.py"", line 103, in accumulate
    match_data['scale_err'].append(1 - scale_iou(gt_box_match, pred_box))
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/common/utils.py"", line 99, in scale_iou
    assert all(sr_size > 0), 'Error: sample_result sizes must be >0.'
"
"Hi thanks for sharing this excellent dataset and toolbox. I am trying to download the test set, but I find although I get the .tar file with exactly the same size, the md5 values are different. And the zipping process exits with the following error, after which I find only 1200 .bin in the `LIDAR_TOP` folder instead of 6008. I have tried downloading the file again but still met such a problem. Any idea on this problem? 
![image](https://user-images.githubusercontent.com/15166089/128794992-dfab61b6-85d8-4cb6-bd96-24e188c6da5e.png)
"
"The `category.json` of the `nuScenes-v1.0-mini` has 116 lines.
The `category.json` of the `nuScenes-lidarseg-v1.0-mini` and  `nuScenes-panoptic-v1.0-mini` has 193 lines.

Why are they different? 
When I unzip the downloaded files, one `category.json` will cover another. "
"Hi, I want to get the mask of ""drivable_area"" in Bird-Eye-View. 
For road, I know I should fill the poly of ""lane"" and ""road_segment"", but for drivable_area?"
"Hi, Does the current covernet from nuscenes api support dynamic or hybrid trajectory sets for inference?

Thank you,
Srikanth"
"Hi, this is a good work!

How can I obtain range azimuth map from radar point clouds?

Thanks!"
How i can know which scenes are available in different downloadable blobs. I want to train in parts with full dataset.
"It's a short edit in the documentation of two methods, adding a bit of information missing but nonetheless relevant."
"Hello:

I want to read the bin file of Lidar in Matlab, and i am wondering which kind of type is it.

Float or int? or sth else?

Thanks"
"Hi!

I went through the installation following the tutorial: https://github.com/nutonomy/nuscenes-devkit#nuscenes-lidarseg
Unfortunately, I'm still getting the error in this notebook: https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/nuscenes_lidarseg_panoptic_tutorial.ipynb

I made sure that my /datasets/NuScenes/ folder is in the correct format. Could you please guide me which library is missing and how to install it?


# nuscenes-lidarseg
nusc.list_lidarseg_categories(sort_by='count')
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-0aa335bb282b> in <module>
      1 # nuscenes-lidarseg
----> 2 nusc.list_lidarseg_categories(sort_by='count')

~/anaconda3/lib/python3.8/site-packages/nuscenes/nuscenes.py in list_lidarseg_categories(self, sort_by)
    483 
    484     def list_lidarseg_categories(self, sort_by: str = 'count') -> None:
--> 485         self.explorer.list_lidarseg_categories(sort_by=sort_by)
    486 
    487     def list_attributes(self) -> None:

~/anaconda3/lib/python3.8/site-packages/nuscenes/nuscenes.py in list_lidarseg_categories(self, sort_by)
    645                         class index.
    646         """"""
--> 647         assert hasattr(self.nusc, 'lidarseg'), 'Error: nuScenes-lidarseg not installed!'
    648         assert sort_by in ['count', 'name', 'index'], 'Error: sort_by can only be one of the following: ' \
    649                                                       'count / name / index.'

AssertionError: Error: nuScenes-lidarseg not installed!


"
"In function  'render_map_in_image', the interior points of polygon are ignored and only the exterior points are processed.
https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/map_expansion/map_api.py"
"Hi, thanks for your great work in building and sharing the dataset. I want to ask how to get the dense segmentation in BEV instead of the sparse result. Is there any function to realize it? Or do you have any suggestions about it?
Thanks a lot~"
"Hello, I'm trying to transform nuscenes 3D point cloud to 2D range image referencing semantic-kitti [https://github.com/PRBonn/semantic-kitti-api/blob/c2d7712964a9541ed31900c925bf5971be2107c2/auxiliary/laserscan.py](url). I set [H=32, W=1024, fov_up=10.0, fov_down=-30.0](url). But it seems not right. Is there are something different with semantic-kitti ?

Thanks, in advance.

"
"The Detection eval script prints along the lines the following statement:
```
=> After LIDAR points based filtering: X
```
This is very misleading because it sounds like only LIDAR points are counted. When e.g. manually filtering ground truths via LIDAR points and then feeding this back to the eval routine results in a non-perfect score. This is very irritating, and a simple code inspection does not easily reveal that actually RADAR points are taken also into account, as it is hidden behind the variable name ""num_pts"".

I would suggest to simply update the printed statement to
```
=> After (LI+RA)DAR points based filtering: X
=> After LIDAR+RADAR points based filtering: X
```

https://github.com/nutonomy/nuscenes-devkit/blob/bfb57a4e5b044a9f448a3412b23966821f695aaf/python-sdk/nuscenes/eval/common/loaders.py#L258"
"Hello, I'm trying to evaluate a model trained and tested on nuScenes on only the blobs of part 06. However, I'm uncertain from which scene (Scene number) does part 06 start and at which scene does it end? Can you please inform me? Thanks, in advance."
"Hello, the staff of nutonomy!
    I am working on the lidar-seg dataset in my project. And I want to get the range view image of current frame by projecting point clouds into front-view. As range image is the native form of rotated lidar data, I want to know whether there is a official transformation code about the process. Thanks a lot !"
"Hi @holger-motional ,
Is there a way to export annotations as text files?
I have seen the export to kitti  file. But velocity of each box is not included in the output.
I would like to have text files with velocity labels also.
Thanks."
"Hi, thanks for the great devkit. I have two questions:

1. I cannot find the exact mounting position of the LiDAR sensor in the documents. Specifically, I am interested in the height above ground. I see that this was somehow mentioned [here](https://github.com/nutonomy/nuscenes-devkit/issues/56#issuecomment-558555994) already, however it only states the height of the ego-vehicle roof and not the LiDAR mounting position.

2. Maybe I missed it, but I wasn't able to see a link on the website that points to the technical data sheet of the exact LiDAR sensor that was used to capture the dataset. Since it is a Velodyne VLP-32, I was wondering if [this site](https://velodynelidar.com/products/ultra-puck/) contains the information about the same sensor as used for nuScenes.

Thanks for your help."
"When I map lidar data to the camera_front plane, a sparse depth map is generated.
But it seems that the generated depth map is so sparse that it is invalid for me to use the same depth completion method as Kitti.
How can I convert sparse depth map to dense depth map？
What is the range of the depth map？"
"Is there a method for 2D object detection on nuscense dataset?
Or is there a comparison of 2D object detection on nuimage?
I found that the 2D annotation converted from 3D annotation in nuscenes dataset can't fit the object well. Can I use the converted data for 2D detector training?"
"How can I read 2D annotation information of front camera image?
From nuscenes to 3D annotation conversion or 2D annotation in nuimage"
"Does the image in nuscenes correspond to the image in nuimage?
How can I generate depth map of image plane from LIDAR point cloud image?"
"Hi, @holger-motional 
Thanks for your help. How can I get the transform matrix? Am I right?
```
# rotation is quaternion, [q0, q1, q2, q3]
T = Quaternion(rotation).transform_matrix
T[:3, 3] = translation[0, :]
```

_Originally posted by @zhangchbin in https://github.com/nutonomy/nuscenes-devkit/issues/612#issuecomment-870437753_"
"Hi, I am sorry to disturb you. I am in trouble that the pose matrix between the two frames I get is incorrect.
My code is:
```
def get_translation_matrix(translation_vector):
    T = torch.zeros(translation_vector.shape[0], 4, 4)
    t = translation_vector.contiguous().view(-1, 3, 1)
    T[:, 0, 0] = 1
    T[:, 1, 1] = 1
    T[:, 2, 2] = 1
    T[:, 3, 3] = 1
    T[:, :3, 3, None] = t
    return T

def rot_from_qual(a):
    # a: [batchsize, 1, 4]
    w, x, y, z = a[:, 0, 0], a[:, 0, 1], a[:, 0, 2], a[:, 0, 3]
    rot_matrix = torch.zeros((a.shape[0], 3, 3))
    rot_matrix[:, 0, 0] = 1 - 2 * y**2 - 2 * z**2
    rot_matrix[:, 0, 1] = 2 * x * y - 2 * z * w
    rot_matrix[:, 0, 2] = 2 * x * z + 2 * y * w
    rot_matrix[:, 1, 0] = 2 *x * y + 2 * z * w
    rot_matrix[:, 1, 1] = 1 - 2 * x **2 - 2* z**2
    rot_matrix[:, 1, 2] = 2 * y * z - 2 * x * w
    rot_matrix[:, 2, 0] = 2 * x * z - 2 * y * w
    rot_matrix[:, 2, 1] = 2 * y * z + 2 * x * w
    rot_matrix[:, 2, 2] = 1 - 2 * x**2 - 2* y**2
    return rot_matrix

def transformation_from_parameters(axisangle, translation):
    t = translation.clone()
    R_tmp = rot_from_qual(axisangle)
    R = torch.zeros((R_tmp.shape[0], 4, 4))
    R[:, :3, :3] = R_tmp[:, :, :]
    R[:, 3, :3] = 0
    R[:, :3, 3] = t.squeeze(1)[:, :]
    R[:, 3, 3] = 1
    
    T = get_translation_matrix(t)
    M = torch.matmul(T, R)
    return M

if __name__ == '__main__':
    l_axisangle, l_translation, r_axisangle, r_translation = data       # axisangle: [batchsize, 1, 1, 4], translation: [batchsize, 1, 1, 4]
    T_0_n_1 = transformation_from_parameters( l_axisangle[:, 0], l_translation[:, 0])
    T_0_n = transformation_from_parameters(r_axisangle[:, 0], r_translation[:, 0])
    T = torch.bmm(T_0_n, torch.inverse(T_0_n_1))  # get the pose matrix from l -> r
```
"
"Hi, I do not find the API to get the depth map, given a token of a sampe."
"Hello,
    I noticed that the translation and rotation matrix is in global Coordinate system. Is there any scripts to get the pose between adjacent frames?"
"Hello nuscenes-devkit team.

I am trying to test my KITTI-trained model on v1.0-mini nuscenes dataset. And I have some question listed as follow:

1.  I used your export_kitti.py code to convert the nuscenes data into KITTI format. As I checked, there are only 10 nuscenes data is converted to KITTI format. Whether the small number of data is related to the v1.0-mini version of nuscenes?
2. Then, I used the converted data to test my model. The detector could get AP>80% when evaluated on KITTI dataset. Whereas, the detector tested on nuScenes the results weird, which is copied as below. Do you have any ideas why the results show like this? 
```
INFO  Car AP@0.70, 0.70, 0.70:
bbox AP:1.8182, 3.0303, 3.0303
bev  AP:1.8182, 1.2987, 1.2987
3d   AP:1.8182, 1.2987, 1.2987
aos  AP:1.41, 1.64, 1.64
Car AP_R40@0.70, 0.70, 0.70:
bbox AP:0.0000, 0.7143, 0.7143
bev  AP:0.1163, 0.0943, 0.0943
3d   AP:0.0000, 0.0000, 0.0000
aos  AP:0.00, 0.45, 0.45
Car AP@0.70, 0.50, 0.50:
bbox AP:1.8182, 3.0303, 3.0303
bev  AP:2.5974, 2.0202, 2.0202
3d   AP:1.8182, 1.2987, 1.2987
aos  AP:1.41, 1.64, 1.64
Car AP_R40@0.70, 0.50, 0.50:
bbox AP:0.0000, 0.7143, 0.7143
bev  AP:0.8887, 0.6971, 0.6971
3d   AP:0.1163, 0.0943, 0.0943
aos  AP:0.00, 0.45, 0.45
Pedestrian AP@0.50, 0.50, 0.50:
bbox AP:0.2884, 0.2822, 0.2822
bev  AP:0.2506, 0.2711, 0.2711
3d   AP:0.0377, 0.0365, 0.0365
aos  AP:0.11, 0.11, 0.11
Pedestrian AP_R40@0.50, 0.50, 0.50:
bbox AP:0.2379, 0.1940, 0.1940
bev  AP:0.1705, 0.1861, 0.1861
3d   AP:0.0065, 0.0064, 0.0064
aos  AP:0.09, 0.08, 0.08
Pedestrian AP@0.50, 0.25, 0.25:
bbox AP:0.2884, 0.2822, 0.2822
bev  AP:1.4559, 1.7526, 1.7526
3d   AP:0.3108, 0.2856, 0.2856
aos  AP:0.11, 0.11, 0.11
Pedestrian AP_R40@0.50, 0.25, 0.25:
bbox AP:0.2379, 0.1940, 0.1940
bev  AP:1.3953, 1.6581, 1.6581
3d   AP:0.2533, 0.1959, 0.1959
aos  AP:0.09, 0.08, 0.08
Cyclist AP@0.50, 0.50, 0.50:
bbox AP:0.0000, 0.0000, 0.0000
bev  AP:0.0000, 0.0000, 0.0000
3d   AP:0.0000, 0.0000, 0.0000
aos  AP:0.00, 0.00, 0.00
Cyclist AP_R40@0.50, 0.50, 0.50:
bbox AP:0.0000, 0.0000, 0.0000
bev  AP:0.0000, 0.0000, 0.0000
3d   AP:0.0000, 0.0000, 0.0000
aos  AP:0.00, 0.00, 0.00
Cyclist AP@0.50, 0.25, 0.25:
bbox AP:0.0000, 0.0000, 0.0000
bev  AP:0.0000, 0.0000, 0.0000
3d   AP:0.0000, 0.0000, 0.0000
aos  AP:0.00, 0.00, 0.00
Cyclist AP_R40@0.50, 0.25, 0.25:
bbox AP:0.0000, 0.0000, 0.0000
bev  AP:0.0000, 0.0000, 0.0000
3d   AP:0.0000, 0.0000, 0.0000
aos  AP:0.00, 0.00, 0.00
```
When I further see the visualized result, it shows many objects are incorrectly detected, which leads to the low AP.
![image](https://user-images.githubusercontent.com/37301722/123510594-bf609a00-d67c-11eb-8c93-267f24aa876f.png)
 
3. As you illustrated in the export_kitti.py, the function kitti_res_to_nuscenes() could be used to convert kitti results. I am a little bit confused about what do you mean by the 'kitti results'. Do you mean the .pth model trained on kitti dataset? What kind of kitti result does this code need?

Thanks in advance."
"Hello, could you tell me how you define the yaw angle of traffic cones?

Thank you!

"
"Hello!

Do I understand correctly that I can only train MTP and CoverNet models on my own to get some results?
It looks like the code from the Prediction tutorial can produce an only random and meaningless result when I do inference
Seems like the weights of the trained model are don't loaded from anywhere

Thanks! "
"I have a question about the method for computing recall.
The recall is calculated as `recall = num_matches / num_objects` in function `motar` in module `nuscenes/eval/tracking/metrics.py` and the scores used for computing recall levels are also selected only by `matches = events[events.Type == 'MATCH']` in function `accumulate_threshold` in module `nuscenes/eval/tracking/algo.py`. 
However, in py-motmetrics, the recall is calculated as `num_detections / num_objects` in function `recall` in module `motmetrics/metrics.py` with `num_detections = num_matches + num_switches`, and this is the same as the method proposed in paper `Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics`. 
So, why not to compute recall using the same method as py-motmetrics?  Thanks.
"
"The evaluation is really time-consuming, so is there any method to fasten the process?"
"Hello,
As part of my Bachelor's thesis, I'm currently working with nuScenes dataset and trying to convert the dataset into KITTI format. Indeed, I converted the dataset into KITTI format. However the 3D Boxes in the 2D images don't seem to be enclosing the object of interest correctly. Is this normal and expected? I know, it has been a while since you wrote the code but I would be delighted to have any suggestions from you, so that I can tweak the script a little bit. Below you can see my exemplary  results. Thanks in advance!

**In this image, green boxes are ground truth boxes and blue boxes are my prediction resuts**
The GT-box for the left-most car seems not to be correct. (Scene: 0593, Fifth sample in the scene _(sorry that I can not provide you with the sample token since I converted the dataset into tracking format of KITTI )_)
![000004](https://user-images.githubusercontent.com/38523756/120758399-1528a300-c512-11eb-93a8-0899096e28da.png)"
"Hi,

I would like to ask how the dataset of map expansion was generated (both png file and json file. ) In map_api.py stated that the WGS 84 Web Mercator (EPSG:3857) projection as Google Maps/Earth was used. However, by using google earth I can only export map in kml data format instead of png format. Moreover, how could we find the detailed information of the map (e.g. non geometric layer, geometric layer, ....) and turn it into json file?

Hope that someone can help me with this problem.

Thank you."
"Hi! I am recently trying to access the point clouds that are not in the key frames, and I am confused by the question below. Thank you for your help!

The paths to the LiDAR data are different when using different interfaces. When accessing the point cloud in a frame with `is_key_frame=False`, the Approach 1 refers to the point cloud in the folder `samples`, and the Approach 2 refers to the point cloud in the folder`sweeps`. Therefore, I am wondering **which approach is right and what are the differences**?

1. The first approach is by the following code. Specifically, 
``` python
    data = nusc.get(‘sample_data’, token)
    lidar_path = data['filename']
``` 

2. The second approach is by the function `get_sample_data_path`. Specifically,
``` python
    lidar_path = nusc.get_sample_data_path(token)
``` "
"Hi, I download dataset here: https://www.nuscenes.org/data/v1.0-mini.tgz. I want to project the 3d box and point cloud onto the image to verify the accuracy of the label.
code is :
```
from nuscenes.nuscenes import NuScenes

nusc = NuScenes(version='v1.0-mini', dataroot='D:/code/dataset/opendata/nuscenes/v1.0-mini/v1.0-mini', verbose=True)
nusc.list_scenes()
for i in range(10):
    my_scene = nusc.scene[i]

    first_sample_token = my_scene['first_sample_token']
    nusc.render_sample(first_sample_token, out_path=""work_dirs/"" + str(i))
    lidar = nusc.sample[i]
    nusc.render_pointcloud_in_image(lidar['token'], pointsensor_channel='LIDAR_TOP',
                                    out_path=""work_dirs/lidar_"" + str(i))
    nusc.render_sample_data(lidar['data']['LIDAR_TOP'], nsweeps=5, underlay_map=True,
                            out_path=""work_dirs/lidar_dense_"" + str(i))
```
The results seems not good.
1.  For example, the 3dbox does not fit the edge of the target well.
![image](https://user-images.githubusercontent.com/46736075/120134613-1780b980-c201-11eb-87ee-9d94d9a3f616.png)
2. The point cloud is also not well aligned, and there is a lot of overlap.
![image](https://user-images.githubusercontent.com/46736075/120134801-7cd4aa80-c201-11eb-96cb-f7888c7109fa.png)
Am i missing something? Does this mean that the accuracy of this dataset is not very high? Thank you."
"Hi,

I encountered the following error while running the map expansion tutorial. 
Also, I used the lastest version Map expansion pack (v1.3) 

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-1-d72434e1bc2d> in <module>
      7 from nuscenes.map_expansion.bitmap import BitMap
      8 
----> 9 nusc_map = NuScenesMap(dataroot='/home/yihong/data/sets/nuscenes/', map_name='singapore-onenorth')

~/git_workspace/nuscenes-devkit/python-sdk/nuscenes/map_expansion/map_api.py in __init__(self, dataroot, map_name)
     99 
    100         if self.version < '1.3':
--> 101             raise Exception('Error: You are using an outdated map version (%s)! '
    102                             'Please go to https://www.nuscenes.org/download to download the latest map!')
    103 

Exception: Error: You are using an outdated map version (%s)! Please go to https://www.nuscenes.org/download to download the latest map!

------------------------------------------------------------------------------

Can someone help me with this problem? 
Thank you.

"
"Hey,
I'm trying to understand the relation between the prediction challenge and the cameras on the vehicle

In the prediction challenge, we're trying to predict the trajectories of some agent in the scene. 
Is this agent, the same agent that is taking pictures? or is it some other vehicle?
If they are not the same one, is the agent on which we predict trajectories present in the images? can we use them for the prediction challenge?"
"Hi! 

Thank you for your help on this dataset! It is very helpful to all the people! However, I have some questions that may need your help, and I indeed haven't found them in the tutorials.

I am wondering how to access the point clouds that are not in the key frames, i.e., samples. The original LiDAR scans the surrounding environment at a frequency of 10Hz, but the samples only have a frequency of 2Hz. Specifically, I am curious about the following two questions:

1. **How to access the data related to LIDAR_TOP but not in the key frames (samples), such as point clouds, ego poses, time stamps?**
2. **May I believe that ""for every two key frames, there are four not-key frames,"" and no exceptions exist in the dataset?**

Thank you again for your time and help!"
"Hello,

I am trying spherical projection on nuScenes dataset that project lidar data into five channels (xyz coordinate, remission, range). I want to know if this devkit provides the statistics for normalization (mean and std of each channel. )

Thanks in  advance. "
"Hello! I found the following information from nuScenes website (detection, evaluation metrics, preprocessing section):
''All boxes (GT) without lidar or radar points in them are removed. The reason is that we can not guarantee that they are actually visible in the frame. We do not filter the predicted boxes based on number of points.'' and I have 2 questions related to this:

1. For ""All boxes (GT) without lidar or radar points in them are removed."", I am not very clear about this. For example, if a box has lidar points = 2, but radar points = 0, will it be removed? 
2.For ""We do not filter the predicted boxes based on number of points."", For example, if a GT box is removed, but we submit predicted box corresponding to that removed GT box, do we get punishment for a false positive, or this predicted box is ignored during evaluation?

Thank you."
"Hello, I am trying to config the nuScenes-lidasreg. I have downloaded the whole dataset and the all-meta-data (nuimages-v1.0-all-metadata.tgz). 
But I found some many .json files are missing, e.g. visibility.json.  where can I  get them?

![image](https://user-images.githubusercontent.com/12868455/117278375-0f548900-aea4-11eb-8497-32d753f0e6fa.png)

![image](https://user-images.githubusercontent.com/12868455/117278516-2eebb180-aea4-11eb-84c0-cb6c5f6d1679.png)

Thanks. "
"I'm trying to train the MTP model for the prediction challenge, and I managed to make it converge.
I then try to calculate some metrics over the validation set. When using [`compute_metrics`](https://github.com/nutonomy/nuscenes-devkit/blob/57889ff20678577025326cfc24e57424a829be0a/python-sdk/nuscenes/eval/prediction/compute_metrics.py#L17) I suddenly got abnormally high results, with MinADE_5 of about 2000. I then checked at it seems the function calculates the metrics over the global frame coordinate system.

 I've been having trouble finding a simple way to transfer my prediction (given a sample and instance token) to the global system.

Would love some help!"
"Hi, thanks nutonomy provide a wonderful dataset. 
I am trying to visualize ""road segment"" in Singapore Queenstown's map, but get a broken results. 
![singapore-queenstown0](https://user-images.githubusercontent.com/40080544/115852804-0e126d80-a45b-11eb-95d5-36731512c193.png)
I check the Singapore Queenstown map's json file. In my opinion, it seems lack of data.
I also visualize ""road segment"" in Boston Seaport's map and get a nice results.
![boston-seaport](https://user-images.githubusercontent.com/40080544/115855160-87ab5b00-a45d-11eb-8690-0edf4adc9ed7.png)

Here is my code
```
from nuscenes.nuscenes import NuScenes
from nuscenes.map_expansion.map_api import NuScenesMap
from nuscenes.map_expansion.bitmap import BitMap
from nuscenes.map_expansion import arcline_path_utils

import matplotlib.pyplot as plt

nusc_map = NuScenesMap(dataroot='/data/sets/nuscenes', map_name='singapore-queenstown')
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)
bitmap = BitMap(nusc_map.dataroot, nusc_map.map_name, 'basemap')
fig, ax = nusc_map.render_layers(['road_segment'], figsize=1, bitmap=bitmap)
plt.show()
```
Do I neglect anything or it just has incomplete data?

"
"I've tried to upload a result file to the evaluation server, however it seems there is a mismatch between my prediction's tokens and those of the val set. I create sequences of 16 frames (4 history, 12 ground truth) directly from the scenes in the val_set, filtering objects and parked vehicles.  I've noticed in prediction_scenes.json that you filter lot of agents in each scene, also moving vehicles, as well as random samples, not all the samples in the scene are contained in the json, also some required samples don't have 3 previous samples. Could you please explain how do you filter the samples and agents, so that I can process it in the same way?
Thanks a lot.
"
"radar.points.dtype==np.float64,but in source code you assert dtype==np.float32"
"Hi all:
I want to know how to use the rotation and transformation to achieve the coordinate system conversion.
`poserecord = self.nusc.get('ego_pose', cam['ego_pose_token'])
pc.translate(-np.array(poserecord['translation']))
pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)`
`box.translate(-np.array(pose_record['translation']))
box.rotate(Quaternion(pose_record['rotation']).inverse)`
Whether the matrix is orthogonal. Because I think box coordinate is the global coordinate,  the second code should use the transpose too.
Can you show me the formulas for the conversion of various coordinate systems？"
"Hey @holger-motional 

Hope you are doing good in this tough time. I need to convert nuscenes data into kitti format. I downloaded train and val 1,2,3,4,5,6. and also I convert trainval1 upto 2462 into kitti format(after that I got error FilenotFounderror in image). 

I used this python3 export_kitti.py nuscenes_gt_to_kitti --nusc_kitti_dir ~/Downloads/nuscences/nucsences/nu5/ --image_count 100000000   command to run the export_kitti.py file. 

I changed the data_dir as well. But I got only one v1.0-trainval folder for all trainval1,2,3,4,5,6 folders. 

These are the changes I am making for different trainval folders:

In nuscenes.py
```
  def __init__(self,
                 version: str = 'v1.0-trainval',
                 dataroot: str = '/home/surendra/Downloads/nuscences/nucsences/nu5',
                 verbose: bool = True,
                 map_resolution: float = 0.1):
```
Assume each trainval has its own folders like nu1,nu2,nu3,nu4,nu5, But annotation folder is only one v1.0-trainval. So I copied the v1.0-trainval to nu1,nu2,nu3,nu4,nu5 folders.

in export_kitti.py

```
  def __init__(self,
                 nusc_kitti_dir: str = '~/nusc_kitti',
                 cam_name: str = 'CAM_FRONT',
                 lidar_name: str = 'LIDAR_TOP',
                 image_count: int = 10,
                 nusc_version: str = 'v1.0-trainval',
                 split: str = 'train'):
```
Could you please help me how to convert all files ? am I doing anything wrong here?

Thanks in advance"
"Hey,

I am trying to convert nuscenes lidar data into kitti format. I downloaded the data from [here](https://www.nuscenes.org/download) extracted the data also, but I did not see any annotation or calib or images files. There are annotation files for mini data contains data of all cameras, lidar and radar. But annotations are trainVal1. Do I need to download whole blob to see the annotations?

Could someone help me with this?"
"I am going to convert nuscenes dataset to kitti data format and conduct a simple test, but the given code alone has some problems. It produces only ~30000 points. So I want to use multisweep to do this. 

I changed your code like below, but the code is not working well.

`            pcl, dum = LidarPointCloud.from_file_multisweep(self.nusc,sample,""LIDAR_TOP"",""LIDAR_TOP"",nsweeps=10)
            pcl.rotate(kitti_to_nu_lidar_inv.rotation_matrix)  # In KITTI lidar frame.`

But this can not produce accurate point cloud bin file.

Please tell me the correct way to use Multisweep in nuscenes_gt_to_kitti."
"Hello,

I am training CoverNet, and finding that the training loss is oscillating. I am using the following parameters. Could you confirm if these match with the original paper implementation?

learning_rate: 1e-4
optimizer: SGD
momentum: 0.9
batch_size: 16 (due to memory limitations - I know the original paper used 64)
epochs: 60 (the original paper used 20-30?)

Thanks!
"
"The schema does not show any link between the sample_data table and sample_annotation table. How do I establish a relationship between annotation and the corresponding cameras that it is visible in. 

example:

{'token': 'ca9a282c9e77460f8360f564131a8af5',
 'timestamp': 1532402927647951,
 'prev': '',
 'next': '39586f9d59004284a7114a68825e8eec',
 'scene_token': 'cc8c0bf57f984915a77078b10eb33198',
 'data': {'RADAR_FRONT': '37091c75b9704e0daa829ba56dfa0906',
  'RADAR_FRONT_LEFT': '11946c1461d14016a322916157da3c7d',
  'RADAR_FRONT_RIGHT': '491209956ee3435a9ec173dad3aaf58b',
  'RADAR_BACK_LEFT': '312aa38d0e3e4f01b3124c523e6f9776',
  'RADAR_BACK_RIGHT': '07b30d5eb6104e79be58eadf94382bc1',
  'LIDAR_TOP': '9d9bf11fb0e144c8b446d54a8a00184f',
  'CAM_FRONT': 'e3d495d4ac534d54b321f50006683844',
  'CAM_FRONT_RIGHT': 'aac7867ebf4f446395d29fbd60b63b3b',
  'CAM_BACK_RIGHT': '79dbb4460a6b40f49f9c150cb118247e',
  'CAM_BACK': '03bea5763f0f4722933508d5999c5fd8',
  'CAM_BACK_LEFT': '43893a033f9c46d4a51b5e08a67a1eb7',
  'CAM_FRONT_LEFT': 'fe5422747a7d4268a4b07fc396707b23'},
 'anns': ['ef63a697930c4b20a6b9791f423351da',
  '6b89da9bf1f84fd6a5fbe1c3b236f809',
  '924ee6ac1fed440a9d9e3720aac635a0',
  '91e3608f55174a319246f361690906ba',
  'cd051723ed9c40f692b9266359f547af',
  '36d52dfedd764b27863375543c965376',
  '70af124fceeb433ea73a79537e4bea9e',
  '63b89fe17f3e41ecbe28337e0e35db8e',
  'e4a3582721c34f528e3367f0bda9485d',
  'fcb2332977ed4203aa4b7e04a538e309',
  'a0cac1c12246451684116067ae2611f6',
  '02248ff567e3497c957c369dc9a1bd5c',
  '9db977e264964c2887db1e37113cddaa',
  'ca9c5dd6cf374aa980fdd81022f016fd',
  '179b8b54ee74425893387ebc09ee133d',
  '5b990ac640bf498ca7fd55eaf85d3e12',
  '16140fbf143d4e26a4a7613cbd3aa0e8',
  '54939f11a73d4398b14aeef500bf0c23',
  '83d881a6b3d94ef3a3bc3b585cc514f8',
  '74986f1604f047b6925d409915265bf7',
  'e86330c5538c4858b8d3ffe874556cc5',
  'a7bd5bb89e27455bbb3dba89a576b6a1',
  'fbd9d8c939b24f0eb6496243a41e8c41',
  '198023a1fb5343a5b6fad033ab8b7057',
  'ffeafb90ecd5429cba23d0be9a5b54ee',
  'cc636a58e27e446cbdd030c14f3718fd',
  '076a7e3ec6244d3b84e7df5ebcbac637',
  '0603fbaef1234c6c86424b163d2e3141',
  'd76bd5dcc62f4c57b9cece1c7bcfabc5',
  '5acb6c71bcd64aa188804411b28c4c8f',
  '49b74a5f193c4759b203123b58ca176d',
  '77519174b48f4853a895f58bb8f98661',
  'c5e9455e98bb42c0af7d1990db1df0c9',
  'fcc5b4b5c4724179ab24962a39ca6d65',
  '791d1ca7e228433fa50b01778c32449a',
  '316d20eb238c43ef9ee195642dd6e3fe',
  'cda0a9085607438c9b1ea87f4360dd64',
  'e865152aaa194f22b97ad0078c012b21',
  '7962506dbc24423aa540a5e4c7083dad',
  '29cca6a580924b72a90b9dd6e7710d3e',
  'a6f7d4bb60374f868144c5ba4431bf4c',
  'f1ae3f713ba946069fa084a6b8626fbf',
  'd7af8ede316546f68d4ab4f3dbf03f88',
  '91cb8f15ed4444e99470d43515e50c1d',
  'bc638d33e89848f58c0b3ccf3900c8bb',
  '26fb370c13f844de9d1830f6176ebab6',
  '7e66fdf908d84237943c833e6c1b317a',
  '67c5dbb3ddcc4aff8ec5140930723c37',
  'eaf2532c820740ae905bb7ed78fb1037',
  '3e2d17fa9aa5484d9cabc1dfca532193',
  'de6bd5ffbed24aa59c8891f8d9c32c44',
  '9d51d699f635478fbbcd82a70396dd62',
  'b7cbc6d0e80e4dfda7164871ece6cb71',
  '563a3f547bd64a2f9969278c5ef447fd',
  'df8917888b81424f8c0670939e61d885',
  'bb3ef5ced8854640910132b11b597348',
  'a522ce1d7f6545d7955779f25d01783b',
  '1fafb2468af5481ca9967407af219c32',
  '05de82bdb8484623906bb9d97ae87542',
  'bfedb0d85e164b7697d1e72dd971fb72',
  'ca0f85b4f0d44beb9b7ff87b1ab37ff5',
  'bca4bbfdef3d4de980842f28be80b3ca',
  'a834fb0389a8453c810c3330e3503e16',
  '6c804cb7d78943b195045082c5c2d7fa',
  'adf1594def9e4722b952fea33b307937',
  '49f76277d07541c5a584aa14c9d28754',
  '15a3b4d60b514db5a3468e2aef72a90c',
  '18cc2837f2b9457c80af0761a0b83ccc',
  '2bfcc693ae9946daba1d9f2724478fd4']}


Now in this sample how do i know which annotation belongs to which camera? There may be some annotations which are not visible in certain cameras. "
"Is there any simple method to find out the scenes present in v1.0-trainval01_blobs.tgz?

I am aware of the splits.py file which indicates the samples present each of the train/val/test splits, but due to hardware constraints I am not able to download all the 850 scenes.

Would the v1.0-trainval01_blobs contain the scenes from 0001-0102 since scenes 0077-0091 are present in the test set?
![image](https://user-images.githubusercontent.com/46735044/111808646-945ef100-88aa-11eb-8c28-e49fbf729830.png)

"
"Dear authors,

thank you very much for this dataset. I have read the accompanying paper where you mention a baseline model based on OFT model and others based on MonoDis and SSD+3D. I tried to search for it here but I haven't found it.  Do you provide it? If not, would it be possible to provide it?

I also noticed that in https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/nuscenes/eval/tracking#baselines , you provide a baseline detections from Mapillary but it contains only the actual detections and not the model or the code. Would it please be possible to get these?

Thank you very much in advance.

Best regards,
Antonin."
"Hi,

Thanks for this great dataset. I would like to render a pointcloud using Pyrender but I'm not sure how to set the camera parameters. Pyrender uses the Opengl format (+x points right, +y points up and +z points backward). Looking at the cameras in your [dataset](https://www.nuscenes.org/nuscenes#overview), +x points right, +y points down and +z points forward. Here are a few things I would like to understand.

1. To convert the camera translation to opengl format, I multiply the y and z values by -1. Is that right?
2. Given that the camera parameters are given with respect to egoframe, that means the lidar points I would like to project need to be in the egoframe before I can project using the given camera parameters right?

Some context:
I extracted the pointcloud in the egoframe (after the third stage in [map_pointcloud_to_image](https://github.com/nutonomy/nuscenes-devkit/blob/60ce73af095fa84f1a4943e352ea9ae2714a3149/python-sdk/nuscenes/nuscenes.py#L740)), cropped it using the 3d bbox to obtain the pointcloud for a single person and then turned it to a mesh using trimesh's convex_hull function. I then set the camera parameters(translation, rotation(converted to rotation matrix using Quaternion(rotation).rotation_matrix), camera intrinsics(fx,fy,cx,cy) in pyrender's [Intrinsic Camera](https://github.com/mmatl/pyrender/blob/dd6dbd895aada77f33975cedaad039ac58811ea4/pyrender/camera.py#L313) but the mesh is not in the camera's view in the scene.

3. Should the rotation matrix be transposed or not when projecting in pyrender? Your projection code uses the transposed rotation matrix.  
4. Is there anything I'm missing or anything else I should consider when projecting in a different system like Pyrender?

I will really appreciate your help with this.

Thank you."
Is there a version that supports Python3.5?
"I am able to tell whether agents are moving in opposite directions, however I am not able to determine how to get in which direction a given agent is headed. 

In the image below, I would like to determine the agent that is ""in-front-of"" the ego-agent (in red). However, that is relative to the direction in which the car is headed (does y coordinate increase as agent moves forward or decrease). The same would apply for left and right agents. 

P.S: This is not an issue, but a discussion. I don't see a ""Discussion"" tab in this repo - may be a good idea. 

![direction-of-travel](https://user-images.githubusercontent.com/1392807/111239975-b0c00c80-85d0-11eb-8824-95e611ba7d05.png)


"
"Hi, I am wondering is it possible to get the corresponding 6 images of the same sample in nuImages dataset?"
"When get_future_for_agent is called with seconds=3, it returns only 5 coordinates instead of 6 for some tokens. You can reproduce the error with the below script. Is it intended behavior?

**reproduce.py**
```
import nuscenes
from nuscenes.prediction import PredictHelper

scene = nuscenes.NuScenes('v1.0-trainval', dataroot='/data/sets/nuscenes')
helper = PredictHelper(scene)
future = helper.get_future_for_agent(
    instance_token='3a0b59f10aee4a6792b99576d166d526',
    sample_token='2fd27781707445d7a2a2d8e708c72301',
    seconds=3,
    in_agent_frame=True)
assert future.shape[0] == 6, f'Expect future.shape[0] to be 6, but got {future.shape[0]}'
```

**Output**
```
$ python reproduce.py
======
Loading NuScenes tables for version v1.0-trainval...
Loading nuScenes-lidarseg...
32 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
34149 lidarseg,
Done loading in 20.600 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.1 seconds.
======
Traceback (most recent call last):
  File ""reproduce.py"", line 11, in <module>
    assert future.shape[0] == 6, f'Expect future.shape[0] to be 6, but got {future.shape[0]}'
AssertionError: Expect future.shape[0] to be 6, but got 5
```"
"I was wondering if the Map API can be used to get speed limits of the various lanes.  Or are there GPS coordinates provided that could be used with a Google maps / OpenStreetMap API to get the speed limits?  The use case I have in mind is to better constrain physics-based motion prediction with speed and curvature context.

Thanks!"
"I want to overlap two consecutive  point clouds.  I need to write the translation and rotation matrix  for that. Can you please suggest me fast way to do that?
it is enough to provide 2d overlap (not considering the z axiz or projecting all the points to xy plane)"
"I converted a certain number of samples into KITTI format using the `nuscenes_gt_to_kitti` function in **export_kitti.py** and generated predictions. I converted those predictions to Nuscenes format using the `kitti_res_to_nuscenes` function in **export_kitti.py** and saved it into a submissions json file. 

When I attempt to evaluate this file suing **evaluate.py**, I get the error 

`AssertionError: Samples in split doesn't match samples in predictions.
`

I understand it is because I am missing all the files in the `train` `val` or `train_detect` splits. Is there any way to evaluate on only the samples I have?"
"For missing in lidar points but appearing in radar,  how to get the 3D-annotations by radar & project to CAM_FRONT 2D-image?"
"I am having difficulties trying to overfit the MTPModel for the prediction challenge. After training for 10,000 epochs, the best results I have gotten so far is listed below. Possible issues:

1. The generated ground_truth in the dataset is in the global frame: `ground_truth = self.helper.get_future_for_agent(instance_token, sample_token, 6, in_agent_frame=False)`, since to my understanding the predictions need to be performed in the global frame. Should I do the training based on `in_agent_frame=True` (local frame) and then when calculating the predictions convert the output of MTP model to global frame?
2. My construction of the Prediction challenge is wrong: 
```
 pred_obj = Prediction(
                instance_token[i], 
                sample_token[i], 
                np.array(out[i, :48].reshape(num_modes, 12, 2)),
                probabilities=np.array([out[i, 48], out[i, 49]])
            )
```

Results in prediction challenge: 

```
{
  ""MinFDEK"": {
    ""RowMean"": [
      86.68397575502848,
      86.68397575502848,
      86.68397575502848
    ]
  },
  ""MinADEK"": {
    ""RowMean"": [
      81.69712783228461,
      81.69712783228461,
      81.69712783228461
    ]
  },
  ""MissRateTopK_2"": {
    ""RowMean"": [
      1.0,
      1.0,
      1.0
    ]
  },
  ""OffRoadRate"": {
    ""RowMean"": [
      1.0
    ]
  }
}
``` 

# 1. Dataset

The Dataset class being used by the Dataloader: 

```python
import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from typing import List
from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.combinators import Rasterizer
from nuscenes.prediction.models.backbone import ResNetBackbone
from nuscenes.prediction.models.mtp import MTP
from nuscenes.prediction.models.covernet import CoverNet
from nuscenes.prediction import PredictHelper
import numpy as np

class NuscenesDataset(Dataset):
    val_nan_state_vector_cnt = 0
    train_nan_state_vector_cnt = 0
    
    def __init__(self, tokens: List[str], helper: PredictHelper, split: str = 'val'):
        self.tokens = tokens
        self.static_layer_rasterizer = StaticLayerRasterizer(helper)
        self.agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)
        self.mtp_input_representation = InputRepresentation(self.static_layer_rasterizer, self.agent_rasterizer, Rasterizer())

        self.split = split  
        self.helper = helper      
        
    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, index: int):
        
        token = self.tokens[index]
        instance_token, sample_token = token.split(""_"")

        # Wrong: only passed static image without vehicles!
        # image = self.static_layer_representation.
        # make_representation(instance_token, sample_token)
        
        image = self.mtp_input_representation.make_input_representation(instance_token, sample_token)
        
        # Why the permute?
        # Same permute to the tensor done for Covernet as well
        image = torch.Tensor(image).permute(2, 0, 1)
        
        agent_state_vector = torch.Tensor([self.helper.get_velocity_for_agent(instance_token, sample_token),
                                        self.helper.get_acceleration_for_agent(instance_token, sample_token),
                                        self.helper.get_heading_change_rate_for_agent(instance_token, sample_token)])
        
        # agent_state_vector can have nan values
        # 643 / 9041 for val
        # 2297 / 32186 for train
        agent_state_vector = torch.nan_to_num(agent_state_vector)
            
        ground_truth = self.helper.get_future_for_agent(instance_token, sample_token, 6, in_agent_frame=False)
        ground_truth = np.expand_dims(ground_truth, 0)

        return image, agent_state_vector, ground_truth, instance_token, sample_token
```

# 2. MTP Training

```python
from datetime import datetime
import json
import os
Gpu.freegpu()
import torchvision

from nuscenes.prediction.models.mtp import MTP, MTPLoss
NUM_MODES = 2
backbone = ResNetBackbone('resnet50')
model = MTP(backbone, NUM_MODES)
# 
# Gpu.IDS = [1]
device = torch.device('cuda:' + str(Gpu.IDS[0]) if torch.cuda.is_available() else 'cpu')
print('running on ', device)
model = model.to(device)

if torch.cuda.device_count() > 1:
        print(""Let's use multiple GPUs:"", len(Gpu.IDS), ""out of "", torch.cuda.device_count(), ""GPUs!"")      
        model = nn.DataParallel(model, device_ids=Gpu.IDS)

tokens = train_non_nan[:4]

dataset = NuscenesDataset(tokens, helper)
dataloader = DataLoader(dataset, batch_size=16*len(Gpu.IDS), num_workers=2 * len(Gpu.IDS))

n_iter = 0

minimum_loss = 0

loss_function = MTPLoss(NUM_MODES, 1, 5)
current_loss = 10000
model_dir = make_model_dir()
learning_rates = [0.1]


for lr in learning_rates:
    # optimizer = optim.SGD(model.parameters(), lr=lr, momentum = 0.9)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    print('--------- LEARNING RATE ', lr, '--------------')
    for epoch in range(1, 10001): 
        # print below - only for large dataset
        # print('-> epoch:', epoch)
        for img, agent_state_vector, ground_truth, _, _ in dataloader:            
    #         imshow(torchvision.utils.make_grid(img)) 
            img = img.to(device)
            agent_state_vector = agent_state_vector.to(device)    
            ground_truth = ground_truth.to(device)
            ground_truth = ground_truth.to(torch.float32)
            optimizer.zero_grad()

            prediction = model(img, agent_state_vector)            
    #         print('prediction.shape', prediction.shape, 'ground_truth.shape', ground_truth.shape)
            loss = loss_function(prediction, ground_truth)
            loss.backward()
            optimizer.step()

            current_loss = loss.cpu().detach().numpy()        
            n_iter += 1   
            
            # print(f""Current loss is {current_loss:.4f}, Iter {n_iter}"")
            if np.allclose(current_loss, minimum_loss, atol=1e-4):
                print(f""Achieved near-zero loss after {n_iter} iterations."")
                print('saving model at epoch:', epoch)
                state = {
                    'epoch': epoch,
                    'state_dict': model.module.state_dict(),
                    'optimizer': optimizer.state_dict(),
                }
                model_name = 'overfit' + str(epoch)
                model_path = os.path.join(model_dir, model_name + '.t7')
                torch.save(state, model_path)

        if epoch % 200 == 0:
            print('saving model at epoch:', epoch)
            state = {
                'epoch': epoch,
                'state_dict': model.module.state_dict(),
                'optimizer': optimizer.state_dict(),
            }
            model_name = 'overfit' + str(epoch)
            model_path = os.path.join(model_dir, model_name + '.t7')
            torch.save(state, model_path)

        # # if epoch % 50  == 0:         
        # print(f""Current loss is {current_loss:.4f}, Iter {n_iter}, epoch {epoch}"")
        # if np.allclose(current_loss, minimum_loss, atol=1e-4):
        #     print(f""Achieved near-zero loss after {n_iter} iterations."")

        n_iter = 0
        print('=> completed epoch ', epoch, 'current_loss', current_loss, ' <=')
```

# 3. Predictions Calculator

```python
    mtp_preds = []
    for img, agent_state_vector, ground_truth, instance_token, sample_token in dataloader:
        print('Iteration', n_iter, 'img.shape', img.shape, 'agent_state_vector.shape', agent_state_vector.shape)
        img = img.to(device)
        agent_state_vector = agent_state_vector.to(device)    
        prediction = model(img, agent_state_vector)

        out = prediction.detach().clone()
        out = out.cpu()  
        print(out.shape, 'out.shape') 


        for i in range(out.shape[0]):
            pred_obj = Prediction(
                instance_token[i], 
                sample_token[i], 
                np.array(out[i, :48].reshape(num_modes, 12, 2)),
                probabilities=np.array([out[i, 48], out[i, 49]])
            )
            mtp_preds.append(pred_obj.serialize())
        
        n_iter += 1
```"
"There seems to be a small bug in `nuimages.py`, for which non-keyframe images cannot be rendered:

https://github.com/nutonomy/nuscenes-devkit/blob/5325d1b400950f777cd701bdd5e30a9d57d2eaa8/python-sdk/nuimages/nuimages.py#L642-L643

The condition within the `assert` statement should be negated as follows:

```python
if not sample_data['is_key_frame']: 
     assert annotation_type == 'none', 'Error: Cannot render annotations for non keyframes!' 
 ```

I hope it will be fixed in a future release:+1:"
"Hi. I'm confused about the meanings of some lines.

Specifically,
1. In [line 24-27](https://github.com/nutonomy/nuscenes-devkit/blob/7222ced569e75c68e79dab8a31d05190bf813119/python-sdk/nuscenes/eval/prediction/splits.py#L24-L27), why do we use 'train' split when we want 'train_val' split? As far as I can understand, we should merge the 'train' and the 'val' split for the 'train_val' split.
2. In [line 34-37](https://github.com/nutonomy/nuscenes-devkit/blob/7222ced569e75c68e79dab8a31d05190bf813119/python-sdk/nuscenes/eval/prediction/splits.py#L34-L37), what is the meaning of NUM_IN_TRAIN_VAL? For the 'train' split, why do we use the first 200 scenes and exclude the last 150 scenes? Why 'train_val' split does not contain the 'train' split?

Thanks in advance :)"
"Hi, is there a way for the input_representation class to create raster images for the ego? thanks "
"The method ""field2token()"" gives me the convenience to retrieve the records according to some criterion. For example:
```
instance = nusc.instance[0]:
ann_tokens = nusc.field2token(""sample_annotation"", ""instance_token"", instance[""token""]
```
This will give me a list of annotation tokens for me to retrieve the annotation data. However, is this list ordered? What I mean by ""ordered"" is that the first element in the list refers to the first annotation of the instance, and the second refers to the second annotation, and so on... 

If it is not ordered, is there any way I can get the sequence of annotations of a particular instance that is temporally ordered?"
"Hi all,

I am trying to get a sequence of connected lanes, but I see get_outgoing_lane_ids() also returns lane_connectors. Lane_connectors are also polygons it seems, but I don't see any visualization in the tutorial notebook. Could you explain why we have lane_connectors in the dataset, and how I can visualize them, preferably in a Bird's Eye view image?"
"By using the depths, with  render_intensity=False, and show_lidarseg=False, in 'map_pointcloud_to_image' as per 
https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/nuscenes.py#L805; works good, except for the Y values. for example, the Y values are in the range of 198 to 898. 

![image](https://user-images.githubusercontent.com/70491128/107247706-8f7f6580-69ff-11eb-9c15-457ede55f777.png)

But with an image dimension of 1600x 900 and using 'map_pointcloud_to_image'; as per my understanding the Y values should in the range of 0 to 899.  Please correct me if I am wrong. or how to map the 198 to 898 Y-values in lidar point to 0 to 899 in image plane?
Note: X values are correctly mapped with the range of 1 to 1595, for lidar with same image size.
"
"I am trying to map lidar pointclouds to image using NuScenesExplorer.map_pointcloud_to_image, but all the values of ""lidar_points[2,:] ""=1. 
An example is shared below
![image](https://user-images.githubusercontent.com/70491128/107178955-47325a00-69a3-11eb-9abf-df7cb0e0f698.png)


By using 'map_pointcloud_to_image' my understanding is that the point cloud data is aligned to image, for example image is 1600x900xRGB, the lidar 'mapped' or 'projected to 2D' points will be 1600x900x Intensity (after rounding to int). Please correct me if I am wrong. Please inform the correct method to get the  'mapped' or 'projected 2D'  data for Lidar with respect to the FOV of the front camera. 



"
"I am wondering why we can't render line layers in the first view image?
https://github.com/nutonomy/nuscenes-devkit/blob/5325d1b400950f777cd701bdd5e30a9d57d2eaa8/python-sdk/nuscenes/map_expansion/map_api.py#L1083-L1085"
"I converted the Nuscenes annotations to the KITTI label format by 'export_kitti.py'. To test the [KITTI eval kit](https://github.com/prclibo/kitti_eval) and the 'new KITTI labels', when I evaluated the ground truth with the ground truth by KITTI eval kit, the following result was returned.  Do you have any ideas?

```
car_detection AP: 0.027176 0.025802 0.025802
pedestrian_detection AP: 0.022784 0.022150 0.022150
bicycle_detection AP: 0.000000 0.000000 0.000000
car_detection_ground AP: 0.024155 0.022471 0.022471
pedestrian_detection_ground AP: 0.000000 0.000000 0.000000
bicycle_detection_ground AP: 0.000000 0.000000 0.000000
Eval 3D bounding boxes
car_detection_3d AP: 0.006540 0.005825 0.005825
pedestrian_detection_3d AP: 0.000000 0.000000 0.000000
bicycle_detection_3d AP: 0.000000 0.000000 0.000000
```
"
"Hello, I was wondering if there were any LiDAR sweeps available for the nuImages frames, and if there were any correspondences between nuImages frames and any frames annotated with 3D boxes in nuScenes?

Thank you."
"Hi,

Thanks for this great dataset. I have an object in a cartesian coordinate system which I would like to align to that of a point cloud in the global frame. After converting the pointcloud to global coordinates(1st and 2nd steps in map_pointcloud_to_image), I found that it's orientation is different from that of my object and I'm not sure how to get them to align in the global frame. Can you help me understand how to go about doing this?

Thank you!

Micael"
"First, thanks a lot for providing the dataset and the tools.

There may a bit inconsistency in the body frame convention between the dataset and the prediction tools.
Based on the calibration files, the data are recorded with the x-axis of the body frame aligned with the heading of the car.
In the prediction tools, y-axis of the body frame is aligned with the heading of the car (such as `get_future_for_agent`)
https://github.com/nutonomy/nuscenes-devkit/blob/5325d1b400950f777cd701bdd5e30a9d57d2eaa8/python-sdk/nuscenes/prediction/helper.py#L214
Although it's not a big issue, it can easily lead to confusions and possibly errors. 
Just want to point this out. It would be great if it can be fixed later."
"Hi there, 

I reproduced the setup of nuscenes dev kit and after setting up the initial algorithm, I encountered the following errors:

`======
Loading NuScenes tables for version v1.0-mini...
Loading nuScenes-lidarseg...

---------------------------------------------------------------------------

AssertionError                            Traceback (most recent call last)

<ipython-input-7-bba4e6dc39b3> in <module>()
      3 from nuscenes import NuScenes
      4 
----> 5 nusc = NuScenes(version='v1.0-mini', dataroot='/content/data/sets/nuscenes', verbose=True)

/usr/local/lib/python3.6/dist-packages/nuscenes/nuscenes.py in __init__(self, version, dataroot, verbose, map_resolution)
    100                 # Check that the category records contain both the keys 'name' and 'index'.
    101                 assert 'index' in lidarseg_category.keys(), \
--> 102                     'Please use the category.json that comes with nuScenes-lidarseg, and not the old category.json.'
    103 
    104                 self.lidarseg_idx2name_mapping[lidarseg_category['index']] = lidarseg_category['name']

AssertionError: Please use the category.json that comes with nuScenes-lidarseg, and not the old category.json.

`

Any pointers as to why this is happening?"
"Hello,
I've been trying to evaluate my tracking result of the Test Set on eval.ai website. Despite the offline evaluation (against the validation set) executes fine, my submission for the Test Set generated using the same code can't be evaluated. The evaluation stops with the following Stdout and no Stderr:
```
Starting Evaluation...
Submission related metadata:
Unpacking dataset...
Unpacking user submission...
Evaluating for test phase
======
Loading NuScenes tables for version v1.0-private-test...
23 category,
8 attribute,
4 visibility,
11997 instance,
12 sensor,
1800 calibrated_sensor,
462901 ego_pose,
15 log,
150 scene,
6008 sample,
462901 sample_data,
201130 sample_annotation,
4 map,
Done loading in 10.3 seconds.
======
Reverse indexing ...
Done reverse indexing in 2.2 seconds.
======
Initializing nuScenes tracking evaluation
Loaded results from /tmp/tmpnlpt04pc/submission/nuscenes_test_megvii.json. Found detections for 6008 samples.
Loading annotations for test split from nuScenes version: v1.0-private-test
Loaded ground truth annotations for 6008 samples.
Filtering tracks
```
To generate my submission, I use the baseline MEGVII detection result and the US split of the test set.
What should I do to get my submission evaluated?"
"I am wondering how to get the orientation of the car using the rotation of the ego pose. Specifically, I want to get the map mask of the car using API  `nusc_map.render_map_mask` but I don't know how to set patch_angle so the map is in the same direction as the car (from the first one to the second one).

![image](https://user-images.githubusercontent.com/48043983/105331228-2a113380-5c0e-11eb-9e38-750eda44a907.png)


![image](https://user-images.githubusercontent.com/48043983/105331174-1d8cdb00-5c0e-11eb-917b-15e8378d4a15.png)
"
"Hi,

I'm currently working with the University of Michigan's Autonomous Boat team (UM::Autonomy) and we are planning to use the nuscenes-devkit to work with our data (with some modifications to support our environment). I was wondering if there were any scripts available for creating the JSON data files (either from ROS bags or any other format). If not, I already started implementing some and would be happy to contribute them to the nuscenes-devkit once they are finished.

Thanks so much,
Eric"
"Hi, thanks for releasing the code for CoverNet.
As the training code is not available, I'm trying to implement it myself. The learning rate and its decay schedule are written in the paper, but the other details are missing. Can you provide some additional information such as how many epochs it takes to converge, the batch size, the optimizer, etc.?"
"I use MTP loss to train MTP model, and set the mode to 3 as the paper recommended.  However, although my training data set has diverse directions of trajectories, the model learns 3 fixed-angle trajectories. **And the feature of the map is somehow lost**. I'm fused how the model can learn the different trajectory direction in one mode(e.g. left) when the loss contains _cross-entropy_ part, which makes _regression_ part less impactful. Is there any notices during training(such as improve epochs?)? Or is anyone have trained a good model? "
"Hello. Thank you for releasing the codes of various prediction models including CoverNet.

I saw that you also released the fixed trajectory sets for epsilon=2,4,8 in [the previous issue](https://github.com/nutonomy/nuscenes-devkit/issues/346). But I'd like to use the dynamic and hybrid trajectory sets and also the fixed sets for other epsilon values (the best-reported epsilon was 3 for the case of the fixed trajectory set, but only those for 2, 4, and 8 are released). Can you release those trajectory sets? I would more appreciate it even more if you release the code for generating the sets. Thanks in advance!"
"Hello, nuScenes contributors. Thank you for providing the nuscenes-devkit; it's been a great help for my research. However, there's something I need to ask about the 3D Detection Evaluation code.
### What I've done:

- I converted the datasets to KITTI format using the `nuscenes_gt_to_kitti` presents in the **export_kitti.py**
- I run the detection and dump the result into JSON file using `kitti_res_to_nuscenes`
- I run the evaluation using the **detection/evaluate.py** script
- I tested the evaluation using **mapillary_val.json**

### The problem:

- I found that all my boxes are filtered out due to the high **ego_dist** value (larger than the default max_dist)
- I went deeper to the code and found that the ego_dist values are calculated from the _box translation_ (x, y, z); correct me if I'm wrong
- I compared my JSON file with the one from mapillary and found that the translation difference is significant. My x and y values are 100 times smaller than the one I observed in mapillary's. I don't know how mapillary can get such values

### Note:

- I re-run the `kitti_res_to_nuscenes` using the validation groundtruth (nuScenes -> KITTI -> JSON), and re-run the detection/evaluate.py.
- I got the similar ego_dist like mine and everything is filtered out; logically, it shouldn't be like that, right?

I'm sorry for the long question, and I hope you can provide some answer and solution. Thank you in advance."
"Hello, the (optional) detections (_Megvii_, _PointPillars_, _Mapillary_) that you offer for the tracking challenge only seem to cover samples/keyframes. I'm aware these were provided by third parties, but do you happen to also have detections for the non-keyframes and could make them available? Using only keyframe detections seems like an unneccesary handicap for tracking :D. But thank you for offering detections in the first place!
"
"Hello!

I got this error when I tried to use the function get_2d_boxes.
I think 'nusc' is only defined in main() but not elsewhere and it has not been accessible to any other part of the code. So, the error is coming.
Is that it or am I missing something?"
"Just letting you know. 
I'm installing your package from source for now. 
Thank you. "
"Hi,

I have a question regarding the size of the dataset. On the webpage it says:

> The full dataset includes approximately 1.4M camera images, 390k LIDAR sweeps, 1.4M RADAR sweeps and 1.4M object bounding boxes in 40k keyframes.

However, when I use the dataloader provided, I can only see 34149 samples in 850 scenes of train+val. Am I looking at the wrong thing?

My problem is that I would like to have the access to those 1.4M images with RADAR sweeps and 390k LIDAR sweeps. Based on these numbers, I suppose that there is a RADAR sweep for every image but the same does not hold for LIDAR sweeps that are sparser in time. I would like to access each of these images and get RADAR sweep + LIDAR sweep (if available). I don't need to have any annotations and the calibration does not need to be perfect (as long as it is not considerably wrong).

Can you please help me?

Thank you very much for your help!"
"According to [Radar cross-section](https://en.wikipedia.org/wiki/Radar_cross-section), RCS has units of area (meters squared). But why are some RCS values of radar points negative?
Here is an example on how to get a negative rcs value of a radar point. I did it by following the official tutorial:
```
from nuscenes import NuScenes
from nuscenes.utils.data_classes import RadarPointCloud
import os.path as osp

dataset = 'D:/Data/aDataset/nuScenes/data_mini'
nusc = NuScenes(version='v1.0-mini', dataroot=dataset, verbose=True)

my_scene = nusc.scene[0]

first_sample_token = my_scene['first_sample_token']
my_sample = nusc.get('sample', first_sample_token)

sensor = 'RADAR_FRONT'
my_radar_data = nusc.get('sample_data', my_sample['data'][sensor])

pcl_path = osp.join(dataset, my_radar_data['filename'])
pc = RadarPointCloud.from_file(pcl_path)
points = pc.points
```
![rcs_field](https://user-images.githubusercontent.com/73425467/103438015-1c2b4c80-4c26-11eb-91ef-703787aa337e.png)
![negative_rcs](https://user-images.githubusercontent.com/73425467/103438016-1c2b4c80-4c26-11eb-8042-6a5e5272fca3.png)

 "
"Hi, nuScenes provides a function to get the information of agent like below :
`print(f""Velocity: {helper.get_velocity_for_agent(instance_token_2, sample_token_2)}\n"")`

But how can I get the instance_token of ego agent? Or is there any other function I can use to get the information like velocity and acceleration of the ego agent?

Thank you very much!"
"In the section 4, Input representation, in the [tutorial](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/prediction_tutorial.ipynb) . The code below seems not complete, right? How is the maps loaded?  And why `anns = [ann for ann in nuscenes.sample_annotation if ann['instance_token'] == instance_token_img]`  is here but seems not be used.

```
import matplotlib.pyplot as plt
%matplotlib inline

from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.combinators import Rasterizer

from nuscenes import NuScenes

# This is the path where you stored your copy of the nuScenes dataset.
DATAROOT = '/data/sets/nuscenes'
nuscenes = NuScenes('v1.0-mini', dataroot=DATAROOT)

from nuscenes.prediction import PredictHelper
helper = PredictHelper(nuscenes)

static_layer_rasterizer = StaticLayerRasterizer(helper)
agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)
mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())

instance_token_img, sample_token_img = 'bc38961ca0ac4b14ab90e547ba79fbb6', '7626dde27d604ac28a0240bdd54eba7a'
anns = [ann for ann in nuscenes.sample_annotation if ann['instance_token'] == instance_token_img]
img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)

plt.imshow(img)
```

An error has occurred as below:
`
Exception has occurred: KeyError
'singapore-onenorth'`"
"Hello,

I was trying out the prediction tutorial notebook with the following choice of layers for the static layer rasterizer:
static_layer_rasterizer = StaticLayerRasterizer(helper, layer_names=['ped_crossing', 'traffic_light'])

I looked at every 1000th instance in the training dataset and never saw any of the traffic lights, only the pedestrian crossings.

Is this because traffic lights are relatively rare in the dataset and only appear at a limited number of intersections, or is the traffic light layer not really annotated for the semantic map?  If the former, is there a good sample/instance token I could try out?

Also, am I correct in understanding that only the locations of the traffic lights are available, not their dynamic light state?

Thanks for the help!"
"Hello
I have implemented the tracking algorithm using only lidar data. At the output, I get a bounding box in lidar coordinates. And the sample annotation data, as I understand it, is given in some other coordinates, then to run your code by counting (https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/tracking/evaluate.py). How do I transform the coordinates of the bounding box so that we can start calculating metrics later?"
"Hello everyone, I will be very glad if you can help me solve this problem
When i run metrics count the following error appears

ValueError: Length of names must match number of levels in MultiIndex.


Traceback (most recent call last):
  File ""python-sdk/nuscenes/eval/tracking/tests/my_test.py"", line 114, in <module>
    calculate()
  File ""/python-sdk/nuscenes/eval/tracking/tests/my_test.py"", line 109, in calculate
    metrics = nusc_eval.main(render_curves=True)
  File ""/python-sdk/nuscenes/eval/tracking/evaluate.py"", line 205, in main
    metrics, metric_data_list = self.evaluate()
  File ""/python-sdk/nuscenes/eval/tracking/evaluate.py"", line 135, in evaluate
    accumulate_class(class_name)
  File ""python-sdk/nuscenes/eval/tracking/evaluate.py"", line 131, in accumulate_class
    curr_md = curr_ev.accumulate()
  File ""python-sdk/nuscenes/eval/tracking/algo.py"", line 123, in accumulate
    thresholds, recalls = self.compute_thresholds(gt_box_count)
  File ""python-sdk/nuscenes/eval/tracking/algo.py"", line 303, in compute_thresholds
    _, scores = self.accumulate_threshold(threshold=None)
  File ""/python-sdk/nuscenes/eval/tracking/algo.py"", line 273, in accumulate_threshold
    events = acc.events.loc[frame_id]
  File ""/python-sdk/nuscenes/eval/tracking/mot.py"", line 64, in events
    self.cached_events_df = MOTAccumulatorCustom.new_event_dataframe_with_data(self._indices, self._events)
  File ""python-sdk/nuscenes/eval/tracking/mot.py"", line 39, in new_event_dataframe_with_data
    idx = pd.MultiIndex.from_tuples(indices, names=['FrameId', 'Event'])
  File ""/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/multi.py"", line 507, in from_tuples
    return MultiIndex.from_arrays(arrays, sortorder=sortorder, names=names)
  File ""/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/multi.py"", line 443, in from_arrays
    return MultiIndex(
  File ""/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/multi.py"", line 284, in __new__
    result._set_names(names)
  File ""/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/multi.py"", line 1349, in _set_names
    raise ValueError(
ValueError: Length of names must match number of levels in MultiIndex."
"Hi, I get the detection 3D box result in lidar coordinate system, I want to visualize the result in image, but have no code to refer to. How can I get the visualization result in image? Thanks."
"Hi, sir. I have got 3D bbox in nuscenes lidar coords and transform it to kitti lidar coords by [kitti_to_nu_lidar_inv](https://github.com/nutonomy/nuscenes-devkit/blob/568762bdb507bf3fc3d7c028e1a6074208aa7675/python-sdk/nuscenes/utils/kitti.py#L139), and get **box_lidar_kitti**:
![微信图片_20201218163231](https://user-images.githubusercontent.com/40256410/102592688-c8613380-414e-11eb-9c0d-09c8b3d627e4.png)
and I want to use nuscenes data in kitti lidar coordinate system to train some 3D detection model which don't support nuscenes dataset, but I have no idea about converting quaternion to yaw; In [box_to_string](https://github.com/nutonomy/nuscenes-devkit/blob/60ce73af095fa84f1a4943e352ea9ae2714a3149/python-sdk/nuscenes/utils/kitti.py#L399), it converts quaternion to yaw in kitti image frame, and how to convert quaternion to yaw in kitti lidar frame? Can I use [quaternion_yaw](https://github.com/nutonomy/nuscenes-devkit/blob/60ce73af095fa84f1a4943e352ea9ae2714a3149/python-sdk/nuscenes/eval/common/utils.py#L112)? Appreciate for your reply!"
"I'm trying to render a lane on the Boston NuScenes map. But I'm getting the following error. Could you please help me understand why? 

```
from nuscenes.map_expansion.map_api import NuScenesMap
nusc_map = NuScenesMap(dataroot=DATAROOT, map_name='boston-seaport')

x_test, y_test = 654.6341469166431, 1605.5431136242184
closest_lane = nusc_map.get_closest_lane(x_test, y_test, radius=100) # returns: '101a5b41-3d93-442f-9229-31b97c410669'
nusc_map.render_record(""lane"", closest_lane);
```

Error: 
```
KeyError: '101a5b41-3d93-442f-9229-31b97c410669'
```

The rendering works for `x_test, y_test = 673.9613133067386, 1620.9530475461518` though, which is a nearby point.
 "
"  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/evaluate.py"", line 205, in main
    metrics, metric_data_list = self.evaluate()
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/evaluate.py"", line 135, in evaluate
    accumulate_class(class_name)
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/evaluate.py"", line 131, in accumulate_class
    curr_md = curr_ev.accumulate()
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/algo.py"", line 118, in accumulate
    thresholds, recalls = self.compute_thresholds(gt_box_count)
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/algo.py"", line 298, in compute_thresholds
    _, scores = self.accumulate_threshold(threshold=None)
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/algo.py"", line 268, in accumulate_threshold
    events = acc.events.loc[frame_id]
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/mot.py"", line 62, in events
    self.cached_events_df = MOTAccumulatorCustom.new_event_dataframe_with_data(self._indices, self._events)
  File ""/usr/local/lib/python3.6/dist-packages/nuscenes/eval/tracking/mot.py"", line 37, in new_event_dataframe_with_data
    idx = pd.MultiIndex.from_tuples(indices, names=['FrameId', 'Event'])
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/multi.py"", line 507, in from_tuples
    return MultiIndex.from_arrays(arrays, sortorder=sortorder, names=names)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/multi.py"", line 448, in from_arrays
    verify_integrity=False,
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/multi.py"", line 284, in __new__
    result._set_names(names)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/multi.py"", line 1350, in _set_names
    ""Length of names must match number of levels in MultiIndex.""
ValueError: Length of names must match number of levels in MultiIndex."
"Training using MTP yields nan MTPLoss - do you have a training example where you circumvent this problem? Do you provide the MTP model (and Covernet) `state_dict` with pretrained weights? Any suggestions are welcome. 

Details below: 


```
import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from typing import List
from nuscenes.prediction.input_representation.static_layers import StaticLayerRasterizer
from nuscenes.prediction.input_representation.agents import AgentBoxesWithFadedHistory
from nuscenes.prediction.input_representation.interface import InputRepresentation
from nuscenes.prediction.input_representation.combinators import Rasterizer
from nuscenes.prediction.models.backbone import ResNetBackbone
from nuscenes.prediction.models.mtp import MTP
from nuscenes.prediction.models.covernet import CoverNet
from nuscenes.prediction import PredictHelper
import numpy as np

helper = PredictHelper(nuscenes)

static_layer_rasterizer = StaticLayerRasterizer(helper)
agent_rasterizer = AgentBoxesWithFadedHistory(helper, seconds_of_history=1)
mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())

# torch.set_default_tensor_type('torch.FloatTensor') # default tensor is cuda


class MTPDataset(Dataset):
    def __init__(self, tokens: List[str], helper: PredictHelper):
        self.tokens = tokens
        self.static_layer_representation = StaticLayerRasterizer(helper)
        
    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, index: int):

        token = self.tokens[index]
        instance_token, sample_token = token.split(""_"")

        image = self.static_layer_representation.make_representation(instance_token, sample_token)
        image = torch.Tensor(image).permute(2, 0, 1)
        
        agent_state_vector = torch.Tensor([helper.get_velocity_for_agent(instance_token, sample_token),
                                        helper.get_acceleration_for_agent(instance_token, sample_token),
                                        helper.get_heading_change_rate_for_agent(instance_token, sample_token)])
        
        ground_truth = helper.get_future_for_agent(instance_token, sample_token, 6, in_agent_frame=False)
        ground_truth = np.expand_dims(ground_truth, 0)

        return image, agent_state_vector, ground_truth
```

```
import torchvision

from nuscenes.prediction.models.mtp import MTP, MTPLoss
NUM_MODES = 2
backbone = ResNetBackbone('resnet50')
model = MTP(backbone, NUM_MODES)

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print('running on ', device)
model = model.to(device)

if torch.cuda.device_count() > 1:
  print(""Let's use"", torch.cuda.device_count(), ""GPUs!"")
  model = nn.DataParallel(model)

tokens = full_train[:100]

dataset = MTPDataset(tokens, helper)
dataloader = DataLoader(dataset, batch_size=16*torch.cuda.device_count(), num_workers=4 * torch.cuda.device_count())

n_iter = 0

minimum_loss = 0

loss_function = MTPLoss(NUM_MODES, 1, 5)

current_loss = 10000

optimizer = optim.SGD(model.parameters(), lr=0.1)


for img, agent_state_vector, ground_truth in dataloader:
    print('Iteration', n_iter, 'img.shape', img.shape, 'agent_state_vector.shape', agent_state_vector.shape)
    imshow(torchvision.utils.make_grid(img)) 
    img = img.to(device)
    agent_state_vector = agent_state_vector.to(device)    
    ground_truth = ground_truth.to(device)
    ground_truth = ground_truth.to(torch.float32)

    optimizer.zero_grad()
    
    
    
    prediction = model(img, agent_state_vector)    

    
    print('prediction.shape', prediction.shape, 'ground_truth.shape', ground_truth.shape)
    loss = loss_function(prediction, ground_truth)
    loss.backward()
    optimizer.step()

    current_loss = loss.cpu().detach().numpy()
    
       
    n_iter += 1
    
    print(f""Current loss is {current_loss:.4f}"")
    if np.allclose(current_loss, minimum_loss, atol=1e-4):
        print(f""Achieved near-zero loss after {n_iter} iterations."")
        break

        n_iter += 1
        
    
print('completed')
```

yields

```
Current loss is nan
```"
"Hi, sir. I visualize the point cloud and 3d bboxes using the method in tutorial, and there is a question about the 3D bboxes,
![Figure_1](https://user-images.githubusercontent.com/40256410/102300360-c604ab00-3f8f-11eb-8b2d-04d34c8779b1.png)
![test](https://user-images.githubusercontent.com/40256410/102300373-c9983200-3f8f-11eb-9c0f-2413205073a7.jpg)
on the left of the pic, there should not exist point cloud behind the black car because of the occlusion , why there are some boxes behind the black car, it confused me a lot. Vert appreciate for your reply!

"
"Hello nuScenes Team,

Thanks for the excellent work.  I am trying to generate depth map for evaluation using offered _map_pointcloud_to_image_ function with Lidar measurement. However, the result seems very sparse using only keyframe measurement. And I think apply multi-frame scans, like kitti, may be a advisable way to increase the depth density. But I don't know how to perform it. Can you provide me with some advices? Thank you."
"A scene object has attributes `nbr_samples`, `first_sample_token` and `last_sample_token`. To access e.g. sample 15 (< `nbr_samples`), currently I iterate over the `next` sample tokens (starting at `first_sample_token`) 15 times, which requires linear time, assuming access is constant. Is there a way to see all (ordered) sample tokens linked to a scene? So I can just access the 15th one directly. Similar to how each sample has an attribute `anns` which is a list of `sample_annotation_token`s to see all annotations linked to that specific sample."
"I read some the .pcb.bin files and find that the 4th col (intensity)  maximum is 255.0, the minimum is -94.988, compared to kitti that intensity is range from (0,1), does the intensity matter? Very appreciate for your reply!"
I created a script that generates videos for instances of objects across a scene. I created it to generate data for a project on detecting brake and turn signals from vehicles. Here's the Github [repo](https://github.com/EricWiener/nuscenes-instance-videos). Is there any use in merging this into the scripts folder?
"Hi, My question is that does 3D Radar information available in nuscenes?
In the tutorials, only 2D Radar map is available, is there any way we can obtain a 3D point cloud from Radar?
![image](https://user-images.githubusercontent.com/12952034/101162904-e9655700-366d-11eb-9066-ec8a676fe622.png)

Any help will be appreciated!"
"Why do you filter radar points with ""stopped"" (7) as dynprop?"
"run  
nusc.get('calibrated_sensor',cam_front_data['calibrated_sensor_token'])

{'camera_intrinsic': [[1266.417203046554, 0.0, 816.2670197447984],
  [0.0, 1266.417203046554, 491.50706579294757],
  [0.0, 0.0, 1.0]],
 'rotation': [0.4998015430569128,
  -0.5030316162024876,
  0.4997798114386805,
  -0.49737083824542755],
 'sensor_token': '725903f5b62f56118f4094b46a4470d8',
 'token': '1d31c729b073425e8e0202c5c6e66ee1',
 'translation': [1.70079118954, 0.0159456324149, 1.51095763913]}


Is 'rotation' and 'translation'  are Camera extrinsics?
I know that
R 3X3 C 3x1 matrix? how can i get them?"
"File ""tools/nuscenes-devkit/python-sdk/nuscenes/eval/detection/evaluate.py"", line 45
 nusc: NuScenes,
        ^
SyntaxError: invalid syntax

How to fix this problem"
"Hi! I am about to doing velocity estimation research on nuscenes dataset, it is very appreciated that nuscenes has the labels of velocity. But can I trust the velocity labels totally? Thanks in advance :)"
"Hi, thank you for your hard work.

I want to design some visual odometry experiments by using the lidar data provided by the nuScenes dataset. But I don't know how to use your toolkit to abtain the ground truth poses. And I want these poses to be in the same format as the KITTI odometry dataset. Please help me, thank you very much.

In the KITTI odometry dataset, the ground  truth pose is represented in the file as a single row:
`r11 r12 r13 tx r21 r22 r23 ty r31 r32 r33 tz`"
"Hi, sir. I am a newbie in 3D detection. I have download the detection result from [https://www.nuscenes.org/data/detection-pointpillars.zip](url) and I read the **pointpillars-test.json** and found that the detection result is like this:
`{'sample_token': '557161e4e2374f42bac36c36b93a97fd', 'translation': [708.97, 1821.07, 1.491], 'size': [1.884, 4.223, 1.628], 'rotation': [-0.914, -0.016, -0.009, 0.405], 'velocity': [2.681, -3.124], 'detection_name': 'car', 'detection_score': 0.9408, 'attribute_name': 'vehicle.parked'}`
I can not figure it out, there is not 3D xyz and 2D bbox info,rotation_y, how can I visualize it? Very appreciate for your reply!!"
"Hi, thanks for the nice work.

(1) I'd like to draw the PR curve of a specific scene instead of all the spilt set.

<img width=""613"" alt=""Screen Shot 2020-11-16 at 9 40 33 AM"" src=""https://user-images.githubusercontent.com/3997703/99209277-c594bb00-27fd-11eb-8ab1-dd2b41e9c219.png"">

I have an idea to restrict the sample token in [load_prediction](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/common/loaders.py#L47) and [load_gt](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/eval/common/loaders.py#L97), but not sure if it's the best approach.

(2) What should I do if I also want to run the detection/evaluate.py on the tracking_result to draw the PR curve? I think first  is to change the tracking_name to detection_name in the json file.

(3) Is it possible to evaluate the result only with metadata? Or it's necessary to download all the pointcloud binary and images.

Best,"
"Hi, sir. I am using nuscenes dataset, and I can only use pytorch 1.1.0 at most, but in the requirements.txt, it requires torch>=1.3.1, does nuscenes-devkit support torch 1.1.0? Very appreciate for your reply."
"Hi, 
I came across the lane with token 8f23d3ed-1089-4fcf-a178-a174abc69938, which is part of the lane dictionary in singapore-hollandvillage, but when I try to do : lane_record = nusc_map.get_lane(way['token']), it returns a ""not a valid lane"" error. I manually checked that this lane is infact present in the map files, under lane, but not in arcline_path_3. This is the cause of this error message."
"hi, Thank you for your efforts. how can i get  the distance of object from the datasets,thanks!"
"Hi,
I am interested in extracting the lane markings(which divide 2 lanes) from the map. I looked at the tutorial and the map files, and could not figure out a good way to get predecessor/successor for a lane divider or a road divider. Is there some way to extract this from the map files?"
"hello, I'm not familiar with coordinates calibration.
I see two parameters,  sensor2ego, ego2global,   I want to know what is the origin of global coordinates？
 I see the tranlation between global to lidar is about  400，500,...   is it measured in  meters?
 how to calibrate between ego and global ? 
"
"I am currently use the following code from `export_kitti.py` to get the lidar pose with respect to the world:
```
lid_to_ego = transform_matrix(cs_record_lid[""translation""], Quaternion(cs_record_lid[""rotation""]), inverse=False)
lid_ego_to_world = transform_matrix(ego_record_lid[""translation""], Quaternion(ego_record_lid[""rotation""]), inverse=False)
lid_to_world = np.dot(lid_ego_to_world, lid_to_ego)
```
and save the KITTI style lidar using the code:
```
kitti_to_nu_lidar = Quaternion(axis=(0, 0, 1), angle=np.pi / 2)
kitti_to_nu_lidar_inv = kitti_to_nu_lidar.inverse
pcl = LidarPointCloud.from_file(src_lid_path)
pcl.rotate(kitti_to_nu_lidar_inv.rotation_matrix)  # In KITTI lidar frame.
with open(dst_lid_path, ""w"") as lid_file:
        pcl.points.T.tofile(lid_file)
```
However, I cannot merge the lidar point clouds using the pose to a local map. But if I delete `pcl.rotate(kitti_to_nu_lidar_inv.rotation_matrix) `, the merge is perfect. I know there must need some processing on the `lid_to_world `. Could anyone tell me how to do?

I have tried the follow solutions, but all of them do not work.
`lid_to_world = np.dot(lid_ego_to_world, np.dot(kitti_to_nu_lidar.transformation_matrix, lid_to_ego))
`
`lid_to_world = np.dot(lid_ego_to_world, np.dot(kitti_to_nu_lidar_inv.transformation_matrix, lid_to_ego))
`
`lid_to_world = np.dot(lid_ego_to_world, np.dot(lid_to_ego,kitti_to_nu_lidar_inv.transformation_matrix))
`
I also tried the following, they also do not work
`lid_to_world = np.dot(lid_to_world, kitti_to_nu_lidar_inv.transformation_matrix)
`
`lid_to_world = np.dot(lid_to_world, kitti_to_nu_lidar.transformation_matrix)
`

Could anyone help me？"
"I am currently use the following code from `export_kitti.py` to get the lidar pose with respect to the world:
```
lid_to_ego = transform_matrix(cs_record_lid[""translation""], Quaternion(cs_record_lid[""rotation""]), inverse=False)
lid_ego_to_world = transform_matrix(ego_record_lid[""translation""], Quaternion(ego_record_lid[""rotation""]), inverse=False)
lid_to_world = np.dot(lid_ego_to_world, lid_to_ego)
```
and save the KITTI style lidar using the code:
```
pcl = LidarPointCloud.from_file(src_lid_path)
pcl.rotate(kitti_to_nu_lidar_inv.rotation_matrix)  # In KITTI lidar frame.
with open(dst_lid_path, ""w"") as lid_file:
        pcl.points.T.tofile(lid_file)
```
However, I cannot merge the lidar point clouds using the pose to a local map. But if I delete `pcl.rotate(kitti_to_nu_lidar_inv.rotation_matrix) `, the merge is perfect. I know there must need some processing on the `lid_to_world `. Could anyone tell me how to do?
"
"Hello is there a easy way to get social features a given agent without requiring rasterizing the scene ?

For example, lets say i get a given instance and sample token for a scene
""045cd82a77a1472499e8c15100cb5ff3_6bfd42cf0aba4f1a94ec11fa43e2dd92""
I know this is an specific sample for a vehicle instance.

How can I get all the other vehicles/objects that are in this same instant that is represented
by the sample token 6bfd42cf0aba4f1a94ec11fa43e2dd92 ? Do I need to search ? That would be
pretty inefficient and might require restructuring the data.
"
"Hi Team, 

I installed nuscenes-devikit by the command `pip install nuscenes-devikit` and found that it changed the original torch version. So I intended to reinstall the old version of torch and uninstall the nuscenes-devikit.

However, when I tried to uninstall it by `pip uninstall nuscenes-devikit` the warning `WARNING: Skipping nuscenes-devikit as it is not installed.` popped out and the uninstallation was skipped.

How can it be fixed, thanks.
"
"To process the IMU data, we need the IMU calibration parameters. Where can I find it?

"
"Hi,
I have downloaded the nuscenes_lidarseg dataset by following the instructions in link
https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/nuscenes_lidarseg_tutorial.ipynb

The nuscene python dev-kit is installed by following the instructions as in link
https://github.com/nutonomy/nuscenes-devkit/blob/master/docs/installation.md#install-python

Upon verifying the nuscenes-devkit installation in virtal environment  by using the command  ""python -m unittest"", the error has come as attached in the sceenshot.How this error can be solved?

![InstallatationVergicationError](https://user-images.githubusercontent.com/72782377/95819792-69450400-0d44-11eb-8575-773259496dbd.PNG)

======================================================================
ERROR: test_boston (nuscenes.eval.prediction.tests.test_metrics.TestOffRoadRate)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 299, in test_boston
    self._do_test('boston-seaport', predictions, 2/3)
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 287, in _do_test
    np.testing.assert_allclose(off_road_rate(np.array([]), prediction), np.array([answer]))
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/metrics.py"", line 382, in __call__
    drivable_area = self.drivable_area_polygons[map_name]
KeyError: 'boston-seaport'

======================================================================
ERROR: test_hollandvillage (nuscenes.eval.prediction.tests.test_metrics.TestOffRoadRate)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 331, in test_hollandvillage
    self._do_test('singapore-hollandvillage', predictions, 0)
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 287, in _do_test
    np.testing.assert_allclose(off_road_rate(np.array([]), prediction), np.array([answer]))
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/metrics.py"", line 382, in __call__
    drivable_area = self.drivable_area_polygons[map_name]
KeyError: 'singapore-hollandvillage'

======================================================================
ERROR: test_one_north (nuscenes.eval.prediction.tests.test_metrics.TestOffRoadRate)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 312, in test_one_north
    self._do_test('singapore-onenorth', predictions, 1/3)
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 287, in _do_test
    np.testing.assert_allclose(off_road_rate(np.array([]), prediction), np.array([answer]))
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/metrics.py"", line 382, in __call__
    drivable_area = self.drivable_area_polygons[map_name]
KeyError: 'singapore-onenorth'

======================================================================
ERROR: test_queenstown (nuscenes.eval.prediction.tests.test_metrics.TestOffRoadRate)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 320, in test_queenstown
    self._do_test('singapore-queenstown', predictions, 1/2)
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/tests/test_metrics.py"", line 287, in _do_test
    np.testing.assert_allclose(off_road_rate(np.array([]), prediction), np.array([answer]))
  File ""/home/teroot/nuscenes-devkit/python-sdk/nuscenes/eval/prediction/metrics.py"", line 382, in __call__
    drivable_area = self.drivable_area_polygons[map_name]
KeyError: 'singapore-queenstown'

----------------------------------------------------------------------
Ran 200 tests in 17.671s

FAILED (errors=4)


"
"For lane token='8f23d3ed-1089-4fcf-a178-a174abc69938' with map='singapore-hollandvillage', if we discretize this lane, then we will have empty lane even when the resolution_meters=0.1. But we can still see this lane if we run `nusc_map.render_record(""lane"", lane_token)`. 
I use the following code to discretize the lane: 
`discretized_lane = nusc_map.discretize_lanes(['8f23d3ed-1089-4fcf-a178-a174abc69938'], 0.1)['8f23d3ed-1089-4fcf-a178-a174abc69938']`
And it will return an empty list. "
"Hi developers,

Thanks for your great work on both nuScenes & nuImages datasets.
To catch up with the release of nuImages, [MMDetection3D](https://github.com/open-mmlab/mmdetection3d/) just supported nuImages dataset and released Mask R-CNN/Cascade Mask R-CNN pre-trained models and their corresponding results [here](https://github.com/open-mmlab/mmdetection3d/tree/master/configs/nuimages).

So firstly I wish you and the communities who are interested in nuScenes/nuImages know this news and try out the pre-trained models. They could not only serve as good pre-trained models but also references of the models' performance. 

The second thing is a question about the implementation, especially about the class definition. The current implementation in MMDetection3D uses the same class mapping as it is in nuScenes. Thus, it maps the original about 30 categories to 10 classes and perform instance segmentation/object detection. I am not sure whether this is a suitable practice so I am seeking your opinions. So maybe this issue is also created for discussion about the implementation on nuImages dataset.

BTW, direct PRs, suggestions, and collaborations for MMDet3D are also welcomed.

Thanks."
"Thanks for the great work making all of this freely available! 

While playing around with the dataset and the devkit, I noticed that the helper methods `get_past_for_agent()` and `get_past_for_sample()` return sometimes longer trajectories than expected. Here is an example of reproducing the problem with `get_past_for_agent()`:

```python
import numpy as np
from nuscenes.eval.prediction.splits  import get_prediction_challenge_split
from nuscenes.nuscenes import NuScenes
from nuscenes.prediction import PredictHelper

dataroot = ""/YOUR/DATA/ROOT""
nusc = NuScenes(version=""v1.0-trainval"", verbose=False, dataroot=dataroot)
helper = PredictHelper(nusc)
data_split = get_prediction_challenge_split(""train"", dataroot=dataroot)

past = list()
for sample in data_split:
    instance_token, sample_token = sample.split(""_"")
    past.append(helper.get_past_for_agent(
            instance_token, sample_token, seconds=3, 
            in_agent_frame=True))
num_past_timesteps = np.array([traj.shape[0] for traj in past])
np.sum(num_past_timesteps > 6)
```
I would expect each trajectory to have 6 or fewer timesteps (because of looking at most 3 sec into the past). However, for me, the output of the code above is 10. A similar example can be constructed for `get_past_for_sample()`. I use version 1.1.0 of the nuscenes-devkit package with Python 3.7.0 and numpy 1.19.1. I wonder whether I accidentally corrupted my data or am doing otherwise something wrong."
"Hi developers,

Thanks for the great work on nuImages. I have a question about the calibration distortion parameters.

In your documentation [in the official page](https://www.nuscenes.org/nuimages#data-format), you says that 
> We use the 5 parameter camera convention of the CalTech camera calibration toolbox, that is also used in OpenCV. Only for fish-eye lenses in CAM_BACK do we use the 6th parameter.

I am a little bit confused about their physical meanings, i.e., the exact mappings from the 5/6 parameters to the coefficients k1, k2, k3, k4, k5, k6, p1, p2 in the [OpenCV documentation](https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?). As the correct order in OpenCV is `(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6]])`, should I simply treat them as from k_1 to k_5/k_6 or treat them as the first 5 or 6 parameters in the OpenCV order?

For now, I tried to undistort the image as below:
```python
    calibrated_sensor = nuim.get(
        'calibrated_sensor',
        sample_data['calibrated_sensor_token'])
    cam_intrinsic = calibrated_sensor['camera_intrinsic']
    cam_distortion = calibrated_sensor['camera_distortion']
    cam_intrinsic = np.array(cam_intrinsic, dtype=np.float32)
    cam_distortion = np.array(cam_distortion, dtype=np.float32)
    print(f'cam_intrinsic: {cam_intrinsic}')
    print(f'cam_distortion: {cam_distortion}')
    newcameramtx, roi = cv2.getOptimalNewCameraMatrix(
        cam_intrinsic,
        cam_distortion,
        (width, height), 1, (width, height))
```
The above code works in some situations. However, when it prints `cam_distortion: [-3.01740e-01  1.38439e-01  3.31000e-04 -8.00000e-06 -4.73840e-02 7.45000e-03]`, the OpenCV throws an error as below:
```
cv2.error: OpenCV(4.2.0) /io/opencv/modules/calib3d/src/undistort.dispatch.cpp:297: 
error: (-215:Assertion failed) CV_IS_MAT(_distCoeffs) && 
(_distCoeffs->rows == 1 || _distCoeffs->cols == 1) && 
(_distCoeffs->rows*_distCoeffs->cols == 4 || _distCoeffs->rows*_distCoeffs->cols == 5 || _distCoeffs->rows*_distCoeffs->cols == 8 || _distCoeffs->rows*_distCoeffs->cols == 12 || _distCoeffs->rows*_distCoeffs->cols == 14) 
in function 'cvUndistortPointsInternal'
```

So I am confused about the correct implementation, could you help me to undistort the images correctly?"
"The function [_get_past_or_future_for_agent](https://github.com/nutonomy/nuscenes-devkit/blob/d578e7f294ba4a333b079c4221ac3c9856d58d01/python-sdk/nuscenes/prediction/helper.py#L180) will only return a sequence that does not contain current annotation, which is shown in [__iterate](https://github.com/nutonomy/nuscenes-devkit/blob/d578e7f294ba4a333b079c4221ac3c9856d58d01/python-sdk/nuscenes/prediction/helper.py#L116) function. In other words, the future or past trajectories will not contain the position at the current timestamp. Why would it be in this way? Thank you!"
"Hi,How did you do the image ROI cropped?1600x1200 to 1600x900"
"Dear author,

Appreciate your great dataset! 

Since I only trained my method on two cameras (front and back). May I know how to evaluate the results on the specific camera, e.g., CAM_FRONT&CAM_BACK? Is there any argument that we can manipulate in order to do that?

Thanks!"
"Hello,

I just found that there are missing CAN bus expansion data for some scenes (for instance, scene-0161 to scene-0168). I would like to know if you missed them when you upload the data, or you do not have these data? 

Cheers,
Yi "
"I was folllowing tutorial of nuscene lidarseg and got error below:

AttributeError: 'NuScenes' object has no attribute 'get_sample_lidarseg_stats'

Iam using nuscenes-devkit==1.0.9

Kindly hellp"
""
"Hello, 

I tried running the tests to verify the environment and received the error below.
I'm running on Ubuntu 20.04 LTS (please let me know if there is any other system information you need)
I followed the [installation instructions](https://github.com/nutonomy/nuscenes-devkit/blob/master/docs/installation.md#setup-matplotlib-backend) and everything ran smoothly until there.

I can also run the [nuscenes tutorial notebook](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/nuscenes_tutorial.ipynb) without error. I'm pretty sure I've got the data in the right place.

Any help much appreciated... 

```
(nuscenes) martin@bigmachine:~/nuscenes-devkit/python-sdk$ python -m unittest
Traceback (most recent call last):
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/__main__.py"", line 18, in <module>
    main(module=None)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/main.py"", line 100, in __init__
    self.parseArgs(argv)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/main.py"", line 130, in parseArgs
    self._do_discovery([])
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/main.py"", line 244, in _do_discovery
    self.createTests(from_discovery=True, Loader=Loader)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/main.py"", line 154, in createTests
    self.test = loader.discover(self.start, self.pattern, self.top)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 349, in discover
    tests = list(self._find_tests(start_dir, pattern))
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 414, in _find_tests
    yield from self._find_tests(full_path, pattern, namespace)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 414, in _find_tests
    yield from self._find_tests(full_path, pattern, namespace)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 406, in _find_tests
    full_path, pattern, namespace)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 460, in _find_test_path
    return self.loadTestsFromModule(module, pattern=pattern), False
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 124, in loadTestsFromModule
    tests.append(self.loadTestsFromTestCase(obj))
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/loader.py"", line 93, in loadTestsFromTestCase
    loaded_suite = self.suiteClass(map(testCaseClass, testCaseNames))
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/suite.py"", line 24, in __init__
    self.addTests(tests)
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/unittest/suite.py"", line 57, in addTests
    for test in tests:
  File ""/home/martin/nuscenes-devkit/python-sdk/nuimages/tests/test_attributes.py"", line 24, in __init__
    self.dataroot = os.environ['NUIMAGES']
  File ""/home/martin/anaconda3/envs/nuscenes/lib/python3.7/os.py"", line 681, in __getitem__
    raise KeyError(key) from None
KeyError: 'NUIMAGES'
```"
"I could neither today nor yesterday download the following file from your servers:
![image](https://user-images.githubusercontent.com/14181188/92093355-8d233a80-edd3-11ea-8bf5-3cd7c58afcce.png)
When I click on the respective link, a wheel is shortly spinning, but then nothing is downloading.
I tried both servers, US and Asia, and tried it on two different browsers (Firefox and Chrome). 
The other downloads are working fine."
"Hi, I used the _""export_human_2D_annotation_as_json""_ script to project all the 3D bounding boxes (visibilities = '', '1', '2', '3', '4') into 2D, and the ouptut file contains **255254** bounding boxes. But in the official documentation I found that the annotations for humans (adult, child,  construction_worker, etc.) are **222164**. So why did I get 33.090 additional bounding boxes? Shouldn't  bounding boxes be 222164 too?

Thank you."
"I would like to analyze the impact of image format (jpg, png, bmp etc) on NuScene dataset. For this, is there any way to get png format for the image.

Moreover, **I would like to understand, why  we have use jpg format for nuscene?** Is there any technical reason behind it or just to compress size? I am reading paper, where they studiede change in Object Detection Accuracy based on image format.

@holger-nutonomy @kokseang-aptiv @whyekit-aptiv "
"In CoverNet's paper, it mentions

> We select a mode uniformly at random when there are no modes with an angle below the threshold.

I am wondering what is the threshold used for CoverNet or this only applies to the implementation of MTP? Also, does that mean for the loss function, we first filter out all the trajectories in the trajectory sets that have angle difference beyond the threshold and afterward calculate the best match with least L2 distance in the remaining trajectories?
Also I am wondering for the regression loss of CoverNet, does it use L1 smooth loss for positions or angles?"
why is `alpha=-10` in all car  
"I want to converts all camera image to KITTI format, but when I try `CAM_BACK` images,error:
```
======
Reverse indexing ...
Done reverse indexing in 0.1 seconds.
======
> /data/datasets/nuScences/nuscenes-devkit/python-sdk/nuscenes/scripts/export_kitti.py(153)nuscenes_gt_to_kitti()
-> assert (velo_to_cam_rot.round(0) == np.array([[0, -1, 0], [0, 0, -1], [1, 0, 0]])).all()
(Pdb) velo_to_cam_rot
array([[ 0.94808283,  0.31705002,  0.02486433],
       [ 0.03285883, -0.0198925 , -0.99926202],
       [-0.31632143,  0.94820017, -0.02927763]])
(Pdb) velo_to_cam_rot.round(0)
array([[ 1.,  0.,  0.],
       [ 0., -0., -1.],
       [-0.,  1., -0.]])
(Pdb) n
AssertionError
> /data/datasets/nuScences/nuscenes-devkit/python-sdk/nuscenes/scripts/export_kitti.py(153)nuscenes_gt_to_kitti()
-> assert (velo_to_cam_rot.round(0) == np.array([[0, -1, 0], [0, 0, -1], [1, 0, 0]])).all()
```"
"As mentioned in #400  and #405, the velocity or acceleration can be NaN due to the first observation. I am wondering how would CoverNet deal with this issue. Did CoverNet set all NaN to 0 or ignore all data with NaN?"
"I want to display the image which is related to a lidar data file. Knowing the bin file how to get the image name or the token?
Is there a way to convert Kitti dataset to this format?"
"I wanted to know how to get the lane marking on both sides of a lane, given that I know the token for this lane. Its not clear to me from the map expansion tutorial, what might be a way to do this. 
I am also interested to find out the type of lane divider present on the road(given I know a lane on the road).
Thanks for your time - I hope you respond to my query."
"Hi,
Does the dataset have traffic light (2D/3D) bounding box and state of the traffic light?

Best Regards,
Srikanth"
"Hello,
recently I try to render a lane I got as following
`nusc_map = NuScenesMap(dataroot='/path/to/v1.02-train', map_name='singapore-onenorth')`
`sample_lane_record = nusc_map.lane[500]`
`incoming_lane = nusc_map.get_incoming_lane_ids(sample_lane_record['token'])`
`fig, ax = nusc_map.render_record('lane', incoming_lane[0], other_layers=[])`
I got a key error from these commands KeyError: 'b7967676-b5c8-422d-a8ec-ab27bac1e511'
I notice that I can find the record of this token by 
`nusc_map.get_lane('b7967676-b5c8-422d-a8ec-ab27bac1e511')`
but it cannot be found by
`nusc_map.get('lane', 'b7967676-b5c8-422d-a8ec-ab27bac1e511')`
I checked the source code of these 2 functions, and noticed that 'b7967676-b5c8-422d-a8ec-ab27bac1e511' can be found in the nusc_map.json_obj['arcline_path_3'] but not nusc_map.json_obj['lane']. 
I'm a little bit confused since this token should also be a lane. Can anybody tell me what is the difference between 'arcline_path_3' and 'lane', and what is the correct way to render 'b7967676-b5c8-422d-a8ec-ab27bac1e511'? Thank you!"
"Hi! I find some annotations are missed, for example in ""4cf5d6c3f6ab42aab23f67b5a9782d1a"". 
The radar detects the black car in the direct front, but there is no annotation for this object. Maybe more similar cases exist, however, I haven't gone through the dataset.  I personally guess there are two possibilities leads this occasion: 
* the object does not appear in the adjacent key-frames so that the interpolation cannot perform
* the annotation is actually missed 


![1592464346(1)](https://user-images.githubusercontent.com/42286907/84989542-6a8b6900-b176-11ea-822e-01afa057a66c.png)
![1592464363(1)](https://user-images.githubusercontent.com/42286907/84989545-6c552c80-b176-11ea-88b0-438c761ed1ee.png)

"
"Hi,

I receive this error while running any of the tutorial as well for any new jupyter book for procesing after installing and initializing the necessary setup requirements as mentioned in the requirements.txt file.
Added the path in the environment variables in Windows 10 also done, still I get the following error:
![ENvironemnt_Variable](https://user-images.githubusercontent.com/38406867/84902048-eaa7c500-b0c9-11ea-8efd-5c7e912c6caf.PNG)

---------------------------------------------------------------------------
ModuleNotFoundError: No module named 'nuscenes.utils'; 'nuscenes' is not a package
"
"Hi,

Is the record below have the expected behavior or is it an error?

1. The ""right_lane_divider_segments"" and ""right_lane_divider_segments_nodes"" has 16 entries of which only 9 are unique.
2. The ""left_lane_divider_segments"" and ""left_lane_divider_segments_nodes"" has 0 entries.
3. The ""exterior_node_tokens"" has 14 entries.

```python
nusc_map = NuScenesMap(dataroot='/data/sets/nuscenes', map_name='boston-seaport')
lane = nusc_map.lane[105]
lane

{'token': '18ea75d6-4f75-44cf-976b-8e75c7623725',
 'polygon_token': 'fe65462a-4d09-4623-a779-6126ca1c1264',
 'lane_type': 'CAR',
 'from_edge_line_token': 'e478f443-5f11-4449-9699-42d911158db9',
 'to_edge_line_token': '86f982c9-a87e-44fc-a4bb-cc0ca2821060',
 'left_lane_divider_segments': [],
 'right_lane_divider_segments': [{'node_token': 'ce39442f-247f-4b2f-aadf-ceb4b350ec53',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': 'b475ba47-dfc4-46d1-89a6-ce996e130046',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '81c9f293-b036-4d18-8bac-4cc16f99372b',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': 'ce39442f-247f-4b2f-aadf-ceb4b350ec53',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '506f3269-ba62-4f78-97f6-5da04e6c60ab',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '81c9f293-b036-4d18-8bac-4cc16f99372b',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '50339ca2-274e-4ffb-822a-3319e85c2885',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '506f3269-ba62-4f78-97f6-5da04e6c60ab',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': 'ff7150cc-58d4-4704-ad95-c12c79e06fcb',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '50339ca2-274e-4ffb-822a-3319e85c2885',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '9b7a6485-974c-46e1-ad62-6201fb3de348',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': 'a8a001dd-e715-43f8-a1c2-979151ceb485',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': 'a8a001dd-e715-43f8-a1c2-979151ceb485',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '6971b0b6-f45a-4766-b215-7a72833d5804',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': '6971b0b6-f45a-4766-b215-7a72833d5804',
   'segment_type': 'SINGLE_SOLID_WHITE'},
  {'node_token': 'ff7150cc-58d4-4704-ad95-c12c79e06fcb',
   'segment_type': 'SINGLE_SOLID_WHITE'}],
 'exterior_node_tokens': ['b475ba47-dfc4-46d1-89a6-ce996e130046',
  'ce39442f-247f-4b2f-aadf-ceb4b350ec53',
  '81c9f293-b036-4d18-8bac-4cc16f99372b',
  '506f3269-ba62-4f78-97f6-5da04e6c60ab',
  '50339ca2-274e-4ffb-822a-3319e85c2885',
  'ff7150cc-58d4-4704-ad95-c12c79e06fcb',
  '6971b0b6-f45a-4766-b215-7a72833d5804',
  'a8a001dd-e715-43f8-a1c2-979151ceb485',
  '9b7a6485-974c-46e1-ad62-6201fb3de348',
  '17e794b0-df78-4ee0-978b-046905234512',
  'a13e1d3d-b7f2-4e5a-bbb3-fd0b5494edcf',
  '226274f7-05af-4c67-a046-818aea71ef3b',
  '97ebbc73-63b5-4a07-a4fc-106773ec0ffb',
  'cc9df3fb-7ae1-46e2-8af1-a8c5777cdc85'],
 'holes': [],
 'left_lane_divider_segment_nodes': [],
 'right_lane_divider_segment_nodes': [{'token': 'ce39442f-247f-4b2f-aadf-ceb4b350ec53',
   'x': 620.0937544727044,
   'y': 1643.4395491815048},
  {'token': 'b475ba47-dfc4-46d1-89a6-ce996e130046',
   'x': 626.1456150616126,
   'y': 1637.486664008812},
  {'token': '81c9f293-b036-4d18-8bac-4cc16f99372b',
   'x': 616.309196163898,
   'y': 1645.778102578997},
  {'token': 'ce39442f-247f-4b2f-aadf-ceb4b350ec53',
   'x': 620.0937544727044,
   'y': 1643.4395491815048},
  {'token': '506f3269-ba62-4f78-97f6-5da04e6c60ab',
   'x': 607.1105949816074,
   'y': 1650.6485805513064},
  {'token': '81c9f293-b036-4d18-8bac-4cc16f99372b',
   'x': 616.309196163898,
   'y': 1645.778102578997},
  {'token': '50339ca2-274e-4ffb-822a-3319e85c2885',
   'x': 590.2696032319186,
   'y': 1660.0173720370012},
  {'token': '506f3269-ba62-4f78-97f6-5da04e6c60ab',
   'x': 607.1105949816074,
   'y': 1650.6485805513064},
  {'token': 'ff7150cc-58d4-4704-ad95-c12c79e06fcb',
   'x': 569.7026931272442,
   'y': 1671.2256941329929},
  {'token': '50339ca2-274e-4ffb-822a-3319e85c2885',
   'x': 590.2696032319186,
   'y': 1660.0173720370012},
  {'token': '9b7a6485-974c-46e1-ad62-6201fb3de348',
   'x': 476.9290033947601,
   'y': 1721.605743282405},
  {'token': 'a8a001dd-e715-43f8-a1c2-979151ceb485',
   'x': 505.7441535941218,
   'y': 1706.1089513089275},
  {'token': 'a8a001dd-e715-43f8-a1c2-979151ceb485',
   'x': 505.7441535941218,
   'y': 1706.1089513089275},
  {'token': '6971b0b6-f45a-4766-b215-7a72833d5804',
   'x': 527.8849590921542,
   'y': 1694.086425514736},
  {'token': '6971b0b6-f45a-4766-b215-7a72833d5804',
   'x': 527.8849590921542,
   'y': 1694.086425514736},
  {'token': 'ff7150cc-58d4-4704-ad95-c12c79e06fcb',
   'x': 569.7026931272442,
   'y': 1671.2256941329929}]}
```"
"Hi, 
Is there any standard form for prediction with zero boxes? For example, one of the forms I have tried is shown below. It seems that it does not work well (the detection_name None can not pass the test in the evaluation).
token:{""sample_token"": det[""metadata""][""token""], ""translation"": [0, 0, 0], ""size"": [0, 0, 0], ""rotation"": [0, 0, 0, 0], ""velocity"": [0, 0], ""detection_name"": None, ""detection_score"": -1, ""attribute_name"": None}"
"Dear all,

Recently I found a lane token by get_outgoing_ids **ee16b887-bbad-49cc-a786-aa276a4b3265** in Singapore-onenorth, but when I use the map_api, it raised an error
 _/nuscenes/map_expansion/map_api.py"", line 525, in get_lane
    raise ValueError(f'Lane token ee16b887-bbad-49cc-a786-aa276a4b3265 is not a valid lane.')_
I wonder if it is something wrong about the map, or I get the wrong version maybe since I have met several similar cases in the full dataset.

I searched this token in singapore-onenorth.json, I found it only had a token( showing that it is the outcoming of some lanes), but no record of this lane token.

Thanks"
"I have downloaded the mini dataset and trying to run the basic tutorial notebook. 
The following file is missing:

```
======
Loading NuScenes tables for version v1.0-mini...
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-aa13d07038d3> in <module>
      2 from nuscenes.nuscenes import NuScenes
      3 
----> 4 nusc = NuScenes(version='v1.0-mini', dataroot='/home/m/share/data/nuscene', verbose=True)

/usr/local/lib/python3.6/dist-packages/nuscenes/nuscenes.py in __init__(self, version, dataroot, verbose, map_resolution)
     79         # Initialize map mask for each map record.
     80         for map_record in self.map:
---> 81             map_record['mask'] = MapMask(osp.join(self.dataroot, map_record['filename']), resolution=map_resolution)
     82 
     83         if verbose:

/usr/local/lib/python3.6/dist-packages/nuscenes/utils/map_mask.py in __init__(self, img_file, resolution)
     21         :param resolution: Map resolution in meters.
     22         """"""
---> 23         assert osp.exists(img_file), 'map mask {} does not exist'.format(img_file)
     24         assert resolution >= 0.1, ""Only supports down to 0.1 meter resolution.""
     25         self.img_file = img_file

AssertionError: map mask /home/m/share/data/nuscene/maps/53992ee3023e5494b90c316c183be829.png does not exist

```"
"Is the torch >=1.3.1 strict? I think multiple 3D object detection frameworks (det3d, spconv) only works well with torch == 1.1.0. 

Moreover, installing from the pip will install PyTorch 1.5 directly which from my experiences doesn't work well with spconv. and will also mess up other existing packages that depend on the exact torch package (apex, spconv, deformable convolution, det3d, etc..) So that you need to reinstall all other packages. "
"I would like to stitch the images captured by 6 cameras to form a panorama. Do you plan to support this function in the future?
Traditional panorama stitching pipeline relies on homograph estimation and warping. 
But control points between image pairs are not easy to find out.  
Since the camera intrinsic and poses (translation+orientation) are all known, is it possible to take advantage this information?
One issue is that cameras are not rotating around the common centre. Any tips?
"
"Hi Team

I am wondering if there some information about the lane width, and if I have a token, how can I know it is lane or lane connector? 

Thanks."
"Hi Team 

Can we use the current version of nuscenes-devkit to parse the Lyft dataset? Is it still compatible with each other?

https://github.com/lyft/nuscenes-devkit"
"Hi guys,

I'm trying to run the prediction challenge tutorial and I got this error. It seems to be a missing file. 

Is this file available somewhere else in the repository or did I miss something?

Thanks a lot.

![image](https://user-images.githubusercontent.com/23075615/83251892-9a180880-a1aa-11ea-8668-6570034ec252.png)
"
"Hi,
I am looking at the velocity annotations, and I see that some of them are nan, when are objects annotated this way?
example
scene_token: 'fd8420396768425eabec9bdddf7e64b6'
sample_token: 'b214d0d42c404d4f925aa56ee9cac607'"
"Hi Team 

I have questions about the  function [`rank_metric_over_top_k_modes`](https://github.com/nutonomy/nuscenes-devkit/blob/0fddd0cf7378f76574493d402ad431325d3223d4/python-sdk/nuscenes/eval/prediction/metrics.py#L84). 

From my understanding of the current implementation, it supports calculating the minFDE and minADE for the models that output trajectories with probabilities but if the model doesn’t produce the probabilities the current implementation will fail to calculate the minADE or minFDE.  If we see the [Argoverse implementation](https://github.com/argoai/argoverse-api/blob/2f93a9e18321ec6199c3f6345c533581678382b7/argoverse/evaluation/eval_forecasting.py#L102), it is handling both cases.

What do you think? Am I missing something here? 😅"
"For some agents in the predication tasks, calling helper.get_acceleration_for_agent() returns nan. An example is 

instance_token: 100047e87c594de396c1e110674a85ad
sample_token: 1c7ffbf13c214f45b62d5ad58d939650

Help would be appreciated."
"hello, thanks for your amazing work, can you release the label tool of nuscenes"
"Hi, I am curious when the tracking leaderboard will be updated on the main website. The reason is that I want to compare with other methods in a paper but the evalAI page truncates the AMOTA value to two significant digits so that the top few are the same. "
"I have the following error.  
```
OSError                                 Traceback (most recent call last)
<ipython-input-25-e4f3109b3fa4> in <module>
----> 1 import torch
      2 from nuscenes.prediction.models.backbone import ResNetBackbone
      3 from nuscenes.prediction.models.mtp import MTP
      4 from nuscenes.prediction.models.covernet import CoverNet

~\Anaconda3\lib\site-packages\torch\__init__.py in <module>
     79     dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))
     80     for dll in dlls:
---> 81         ctypes.CDLL(dll)
     82 
     83 

~\Anaconda3\lib\ctypes\__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error)
    346 
    347         if handle is None:
--> 348             self._handle = _dlopen(self._name, mode)
    349         else:
    350             self._handle = handle

OSError: [WinError 126] The specified module could not be found

```
My conda list shows that I have installed torch ==1.5.0,  torchvision ==0.6.0. So I am wondering if one of you occurred this problem. Thank you. "
"I wonder why I get 0 pedestrians in both train and test instances, from get_prediction_challenge_split() (in both mini dataset and main dataset)?
Does it mean the challenge is designed just for the prediction of vehicles?"
When lidar semantic segmentation will get released? Now its approaching end of Q2
"Hi!

I have a question about how to evaluate our 3D object detector's performance locally according to nuscenes evaluation rule.
In python-sdk/nuscenes/eval/detection/README.md I didn't find direct API to get the evaluation result. It seems like the only way to evaluate the detection results locally is to download and modify the source code in /detection folder.
But I am not sure whether I missed some other tutorials about detection evaluation, is there any API to directly evaluate the detection performance given my detection results?

Thank you!"
"Hi,

Could you please share the IMU datasheet? 
There are some sensor’s information at DataCollection page, but I couldn’t find about IMU specs.

Best Regards"
"Hi there!
I would like to use the provided calibration data, in the ""calibrated_sensor.json"" file. I noticed that it contains 850 calibration data for each sensor. If I'm right, each sensor has an own data for each scene. But how can I determine, which one to use? 

I would like to do the following:
- read a lidar file (from sweeps folder)
- read an image file (from sweeps folder)
- find the corresponding rotation and translation matrices for them

Suppose I want to know the calibration data for the data in the ""sweeps"" folder in part 4 of the trainval package. A scene_token is specified in the ""sample.json"" file for the sample data, but no such file is specified for the sweeps data.

How can I find out which of the 850 data should be used for a given timestamp?

Thank you very much for help!"
"Hi there!

Thanks for your amazing work! I used the given trajectory sets (eps=2m, 4m, 8m) and have successfully reproduced your results in your covernet paper. I am wondering whether you can further provide the dynamic and hybrid trajectory sets since they may lead to better results on nuscenes as indicated by your paper. This will be very useful for my benchmarking. Thanks a lot!"
"It seems like NuScene.render_sample() creates a new matplotlib figure and user needs to use `plt.show()` to display it. Jupyter notebook handles this automatically, but python script does not. If this is a design choice, it is not documented in the `render_sample()`."
"1. Add dataroot to several dataloader so that user can customize their data path 
```
# line 41
mini_train = get_prediction_challenge_split(""mini_train"", dataroot=DATAROOT)

# line 129
from nuscenes.map_expansion.map_api import NuScenesMap
nusc_map = NuScenesMap(map_name='singapore-onenorth', dataroot=DATAROOT)
```

2. Typo in line 12
```
1. Data splis for the challenge  -> ... splits ...
```"
"Full sensor suite (1x LIDAR, 5x RADAR, 6x camera, IMU, GPS), as per the https://www.nuscenes.org/

I was able to get all data except GPS. 

Please share the steps to be taken, to get the GPS data. it would be also helpful if an example is shared.
"
"Request for support, error while trying prediction_tutorial with v1.0-mini, missing file ""prediction_scenes.json"". The details are shown below. 
Note the data folder has the correct path (DATAROOT = '/data/sets/nuscenes/') and the first step of loading the data is successful.

Code lines. 
`from nuscenes.eval.prediction.splits import get_prediction_challenge_split
mini_train = get_prediction_challenge_split(""mini_train"")
mini_train[:5]`

Error details 
`---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-2-bfe7e7d26809> in <module>
      1 from nuscenes.eval.prediction.splits import get_prediction_challenge_split
----> 2 mini_train = get_prediction_challenge_split(""mini_train"")
      3 mini_train[:5]

~/nus_data/lib/python3.7/site-packages/nuscenes/eval/prediction/splits.py in get_prediction_challenge_split(split, dataroot)
     28 
     29     path_to_file = os.path.join(dataroot, ""maps"", ""prediction_scenes.json"")
---> 30     prediction_scenes = json.load(open(path_to_file, ""r""))
     31     scenes = create_splits_scenes()
     32     scenes_for_split = scenes[split_name]

FileNotFoundError: [Errno 2] No such file or directory: '/data/sets/nuscenes/maps/prediction_scenes.json'`"
"Hi, will there be a leader board for the prediction tasks on the website soon? Thanks."
"The newly released dataset contains the IMU data with following field:

- Linear Acc
-Rotation
-d

It does not mention that the linear Acc is with or without gravitational.
Therefore I have the following questions:

1. How to find linear accel. with/without gravitational?
2. method to get IMU Angle
3. Angular Accel with vehicle frame coordinate
4. Angular Rate in vehicle frame coordinates"
""
"Is there some way to directly download the dataset to a server?
"
"Hi Team
I have two questions related prediction task:

**1. How to reproduce the numbers in CoverNet paper and MTP using the same [Evaluation metrics](https://www.nuscenes.org/prediction/#evaluation-metrics) in the challenge?** 
So the released code (as far as I now) supports only inference, but I cannot find the evaluation code. For example, If I want to calculate minADE_k or minFDE_k for CoverNet to reproduce the same results reported on the paper, how should I do this? Is there an open-source implementation for **Evaluation metrics** that will be used in the competition and how can we use it on CoverNet or MTP 

**2. What is the possibility to open another submission in the challenge for validation submissions?**
So some competitions have two leaderboards, one for validation dataset and one for testing. The validation leaderboard is open to unlimited submission times and testing is limited to a specific number as the current situation. This will help the community to find better results given that the testing and validation data are I.I.D.
"
"According to the readme, positive value of steer angle feedback indicates right turn, but in the can bus tutorial (https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorials/can_bus_tutorial.ipynb), it has the following description. As shown in the image below, the initial value is positive (around 3), so should the description be "" It seems like the scene starts with a strong **right** turn and then continues more or less straight"" if the readme description is correct? 

![image](https://user-images.githubusercontent.com/39305807/80335124-5c2d6a80-8821-11ea-9100-234038884306.png)
"
"Hi, below are the first few ego poses from scene-0061 where the first vector is the translation and the last number is the orientation in degrees (calculated from the quaternion provided) all in the global frame. 

[411.3039349319818, 1180.8903791765097, 0.0],  -69.78323712700228
[409.7431520488096, 1176.6769733781416, 0.0],  -70.11635890781453
[408.101879146712, 1172.1515777892457, 0.0], -70.47484456994222
[406.7260055966978, 1168.2112865775816, 0.0],  -71.136324633

However, when I try to calculate the orientation using atan2(y2-y1, x2-x1) from the translations the results do not match that from the data shown. 

Also, if [0,0,0] is at the top left corner of the map, positive x is to the right and positive y is downwards, does that mean a left hand coordinate frame is used? 

Thanks"
"Traceback (most recent call last):
  File ""/home/cyrus/workspace2/nuScenes/python-sdk/nuscenes/eval/prediction/baseline_model_inference.py"", line 11, in <module>
    from nuscenes.eval.prediction.config import load_prediction_config
  File ""/home/cyrus/workspace2/nuScenes/python-sdk/nuscenes/eval/prediction/config.py"", line 7, in <module>
    from nuscenes.eval.prediction.metrics import Metric, deserialize_metric
  File ""/home/cyrus/workspace2/nuScenes/python-sdk/nuscenes/eval/prediction/metrics.py"", line 337
    maps: Dict[str, NuScenesMap] = load_all_maps(helper)
        ^
SyntaxError: invalid syntax"
"Hi, I was wondering if there is support to render top down view video to visualize trajectory prediction results for example. Thank you. "
"I'm wondering what's the reasoning behind requiring `torch>=1.3.1`? What in this library conflicts with earlier versions of pytorch?

Thank you!"
"It seems like there are some issues with ground truth annotations in some frames. I do not have an exhaustive list, but I've mainly been experimenting with the val set, and here are some sample tokens with annotation issues. The main issue is some objects (I'm only experimenting with Cars) are not annotated in the ground truth.

0d9c4c2de24b49758901191f623d426b
0ed1a404c1fb4f7a87a30d3ee45f7f97
139bce92199440ea8929d1a1bd10dbda
224d34c137b64e4f8012c7280b4b9089
3abf81a7c3894000a4c508e6ced0caca
4b5202b4625f48799812af5d212e68a4
4e56a7a63b984597844eb55df9a2ba21
74109c3e72b24fb48e2262dc869ba868
8d265c91cc944ba790c09e74d2811d08
9827d52b3aa2484c8901f67f89742e15
f868542113014aeab862aa47e088b1ec
f91ec82037fb47ccbac160cb5de453bf
f9438d42bb944364b5a75d6c5d0bc758
fbbad6309f1543f78634e49c50dfb779

Is there something I'm missing?
Here are some sample images, with the ground truth annotations (**Only Cars**) visualized : 

False positive annotations
![000309](https://user-images.githubusercontent.com/11703754/80007713-a6ff5900-8494-11ea-8788-4d9972291621.png)

Missing Car annotations
![000329](https://user-images.githubusercontent.com/11703754/80007760-b9799280-8494-11ea-9aa9-547756a81f15.png)
![000811](https://user-images.githubusercontent.com/11703754/80007848-d7df8e00-8494-11ea-91a7-6d95c133f968.png)
![001721](https://user-images.githubusercontent.com/11703754/80007856-dca44200-8494-11ea-995d-e8942a186578.png)
![005835](https://user-images.githubusercontent.com/11703754/80007884-e8900400-8494-11ea-93ae-8551e42de33b.png)
"
"I checked velocity which exceeds 35 and -35 in half second and there were four 'car' data which is larger than 35(70m/s) as below.

![image](https://user-images.githubusercontent.com/34183439/79982183-e18ee300-84e0-11ea-9961-bd9aaa62b714.png)

To check the velocity(residual_vel in the link) I ran this code.
https://github.com/eddyhkchiu/mahalanobis_3d_multi_object_tracking/blob/master/get_nuscenes_stats.py#L82

To get not only velocity but also matching dt, box.tracking_name, box.tracking_id, scene_token I wrote debugging code as below. Changed from this line.(https://github.com/eddyhkchiu/mahalanobis_3d_multi_object_tracking/blob/master/get_nuscenes_stats.py#L72)

        if box.tracking_id not in gt_trajectory_map[box.tracking_name][scene_token]:
            gt_trajectory_map[box.tracking_name][scene_token][box.tracking_id] = {t_idx: box_data}
            if box.tracking_id in time_memory:
                time_memory[box.tracking_id][t_idx] = t
            elif len(time_memory) == 0:
                time_memory = {box.tracking_id:{t_idx:t}}
            else:
                time_memory[box.tracking_id] = {t_idx:t}
        else: 
            gt_trajectory_map[box.tracking_name][scene_token][box.tracking_id][t_idx] = box_data
            time_memory[box.tracking_id][t_idx] = t
        
        # if we can find the same object in the previous frame, get the velocity
        if box.tracking_id in gt_trajectory_map[box.tracking_name][scene_token] and t_idx-1 in gt_trajectory_map[box.tracking_name][scene_token][box.tracking_id]:
          residual_vel = box_data[3:7] - gt_trajectory_map[box.tracking_name][scene_token][box.tracking_id][t_idx-1][3:7]
          if (residual_vel[0] > 35) or (residual_vel[0] < -35):
              dt = time_memory[box.tracking_id][t_idx] - time_memory[box.tracking_id][t_idx-1]
              vel_memory.append((dt, residual_vel, box.tracking_name, box.tracking_id, scene_token))"
Thanks!
"In Nuscene-devkit, we are using the calibration matrix of lidar data to render point cloud on the image.
 
Any direction to understand the calibration of nuscene and how we derived these values would be helpful. 
I read the paper as well as checked the dev-kit.

1. What is the use of rotation and translation in the camera's calibration? For lidar, we require a calibration matrix to render over the image? However, I do not understand the use of camera's rotation matrix.
2. Why the rotation matrix of lidar different from the camera? 
3. Is the rotation/translation matrix of a particular sensor the same for a scene? 

Looking for understanding the calibration work"
""
"I calculated object's velocity by (current pose of object)-(previous timestep pose of object) and found  out 'car' object's x_velocity is 40.5 and y_velocity is -22.4.
I understood one timestep is around 0.5second since  39~41 key frames are taken in 20 Seconds.
Meaning, x_velocity becomes 81m/s and y_velocity becomes -44.8m/s w.r.t global coordinates.

Am I missing something?

![image](https://user-images.githubusercontent.com/34183439/79577389-9e5afb80-80ff-11ea-879f-d0d95fa0efe2.png)
"
"I was exploring the code in:
https://github.com/nutonomy/nuscenes-devkit/blob/3758022d577e7c4715eafb26e1a08c1c2ac0f76a/python-sdk/nuscenes/map_expansion/map_api.py#L1074-L1081

Grabbing the depths vector just for the shape is unnecessary since we will already have points vector from 1077th line with the same shape:

1077: `inside = np.ones(points.shape[1], dtype=bool)`

It is a small issue but I am asking because I am curious if I am missing something, isn't it unnecessary?"
"When converting the dataset to KITTI format, the script converts images of one camera angle (ex. FRONT). Are the objects which do not fall in the selected camera's field of view also included in the ground truth file for the corresponding frame? And the same for the Lidar points, are the points not falling in the FOV of the camera not included in the corresponding point cloud file as well?"
"Hello,firstly,thank you for your work.Some labels are empty when converted to kitti data set format. What is the reason？"
When code for CoverNet: Multimodal Behavior Prediction using Trajectory Sets likely to get released?
"Does the trajectory sets been updated in the tutorial 

cv_model = ConstantVelocityHeading(sec_from_now=6, helper=helper)
physics_oracle = PhysicsOracle(sec_from_now=6, helper=helper)

am trying to save the output as list and then to Json but getting through many error help me "
It seems that detection leaderboard is closed since new submission won't appear in the leaderboard. I'm wondering when will the leaderboard be open again
"It would be nice to be able to convert the Waymo dataset to the nuscenes format.

Nuscences shared a lot of good work on the Lidar + camera. It would be interesting to check how will it work on the Waymo dataset."
"FileNotFoundError: [Errno 2] No such file or directory: '/data/sets/nuscenes/maps/prediction_scenes.json'


i had unzipped the file using tar and the file prediction_scenes.json is missing in the map folder"
"Hi the developers,

This devkit is a great work for users to easily access the nuScenes dataset.
One question of mine is that does these packages are all necessary to install the nuscenes-devkit?
As shown in the pictures below, there seem to be many packages to be installed, which may cause difficulty for the python environment control.
![image](https://user-images.githubusercontent.com/40779233/78423368-7b106500-7698-11ea-8392-bdf9358a6996.png)

Therefore, is there anyway to release the burdun to install too many packages? Or if the users do not need all the functionality, is it possible to install a more light-weight version?"
Hello. Can you explain how I can get trajectory using  CoverNet output layer? For example MPT returns points for each mode. I want to get the same from CoverNet. Thank you  for answer !
"Hello,

I have tried to upload my results on the tracking benchmark on EvalAI however I get an error in stdout and no stderr:
“Length of names must match number of levels in MultiIndex.”
On the other hand, it works on the validation set on my machine.

```
Starting Evaluation...
Submission related metadata:
Unpacking dataset...
Unpacking user submission...
Evaluating for test phase
======
Loading NuScenes tables for version v1.0-private-test...
23 category,
8 attribute,
4 visibility,
11997 instance,
12 sensor,
1800 calibrated_sensor,
462901 ego_pose,
15 log,
150 scene,
6008 sample,
462901 sample_data,
201130 sample_annotation,
4 map,
Done loading in 7.6 seconds.
======
Reverse indexing ...
Done reverse indexing in 1.9 seconds.
======
Initializing nuScenes tracking evaluation
Loaded results from /tmp/tmpaeaobejj/submission/data.json. Found detections for 6008 samples.
Loading annotations for test split from nuScenes version: v1.0-private-test
Loaded ground truth annotations for 6008 samples.
Filtering tracks
=> Original number of boxes: 342283
=> After distance based filtering: 332718
=> After LIDAR points based filtering: 332718
=> After bike rack filtering: 332701
Filtering ground truth tracks
=> Original number of boxes: 159753
=> After distance based filtering: 120995
=> After LIDAR points based filtering: 108973
=> After bike rack filtering: 108973
Accumulating metric data...
Computing metrics for class bicycle...

Length of names must match number of levels in MultiIndex.
```

I am not really sure if it’s a problem with my data and however it sounds related to #299.

Thank you very much.
"
"Hi. I don't think it is practical to load everything into memory at once (not everyone has access to hardware with more than 8 GB RAM). Do you guys know how to split your dataset into smaller chunks to make it possible to load that into memory instead? 

Furthermore, once we split the dataset like that, is it possible to store that split in a separate folder with its own JSON files and folders? This way one could work with that smaller split dataset as one would with the original larger dataset. This would be particularly useful if one has to convert that smaller chunk of the dataset to the kitti format."
"I run this line and get the above memory error:

```
from nuscenes.nuscenes import NuScenes

nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)
```
I have Ubuntu 18.04, GTX 1050Ti, 8 GB RAM, 256 GB SSD and 1 TB HDD. Please tell me how to fix this. Thank you."
"First of all, thank you for this repo and the tutorials. 

- I downloaded the v1.0-mini dataset and place it at: `~/Documents`
- I finished the [tutorial notebook](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/tutorial.ipynb). 
- I downloaded the Map Expansion data: **Map expansion pack (v1.1)  US** and placed it at: `~/Documents/v1.0-mini/maps` _(there are 4 different maps from map expansion and 4 different map masks from the mini dataset I downloaded earlier)_
- Then I switched to the [map_demo notebook](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/map_expansion/map_demo.ipynb) for better understanding the map format and render functionality:
   - When loading the the map I got an error that says there is no 'arcline_path_3' key in boston-seaport.json:

```python
import matplotlib.pyplot as plt
from nuscenes.map_expansion.map_api import NuScenesMap

nusc_map = NuScenesMap(dataroot='/home/adastec-oyucedag/Documents/v1.0-mini/', map_name='boston-seaport')

```
```python
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-5-51bca8c15fbf> in <module>
      2 from nuscenes.map_expansion.map_api import NuScenesMap
      3 
----> 4 nusc_map = NuScenesMap(dataroot='/home/adastec-oyucedag/Documents/v1.0-mini/', map_name='boston-seaport')

~/nuscenes-devkit/python-sdk/nuscenes/map_expansion/map_api.py in __init__(self, dataroot, map_name)
     90 
     91         self.canvas_edge = self.json_obj['canvas_edge']
---> 92         self._load_layers()
     93         self._make_token2ind()
     94         self._make_shortcuts()

~/nuscenes-devkit/python-sdk/nuscenes/map_expansion/map_api.py in _load_layers(self)
    141         self.lane_divider = self._load_layer('lane_divider')
    142         self.traffic_light = self._load_layer('traffic_light')
--> 143         self.arcline_path_3 = self._load_arcline_path()
    144         self.lane_connector = self._load_layer('lane_connector')
    145         self.connectivity = self._load_lane_connectivity()

~/nuscenes-devkit/python-sdk/nuscenes/map_expansion/map_api.py in _load_arcline_path(self)
    118         :return: Dictionary Mapping token to arcline_path_3.
    119         """"""
--> 120         return self.json_obj['arcline_path_3']
    121 
    122     def _load_lane_connectivity(self) -> Dict[str, Dict[str, List[str]]]:

KeyError: 'arcline_path_3'
```
I don’t see any key of `arcline_path_3` or `connectivity` in any of the json map files. Maybe I downloaded the wrong version or files. 

What can I do to fix it? Is there any configuration or files that I missed to read/download?

Have a great day.


"
"If firetruck is included, there are 24 labels total, rather than 23. Otherwise, which label does the firetruck belong to? truck?

![image](https://user-images.githubusercontent.com/39305807/76710448-fb692900-66dd-11ea-9244-b14019104f7b.png)
"
"Hi,
I am just exploring your nuScenes data and I use function `nusc.render_sample_data` to visualize the label, however I find some results are obviously inaccurate, for example:
`nusc = NuScenes(version='v1.0-trainval', dataroot='/path/', verbose=True)`
`plot_sample = nusc.get('sample', '00cc708ba96a4a47b8a35118d8019bee')`
`nusc.render_sample_data(plot_sample['data']['CAM_FRONT'])`
then I would get the visualization result like this:
![image](https://user-images.githubusercontent.com/17951622/76642419-25abd100-658e-11ea-85a1-fbbcf121d034.png)
is this expected or do you have any suggestions?

Thanks a lot!"
"Hi, 

The paper mentions that lidar data is captured at 10Hz with 32 spinning beams generating nearly 1.3 Millions points per second. 

How does sampling rate transforms to points per second ? What is the duration of data like 10 min data or an hour data ?"
""
""
""
"I'm working on a vehicle-only motion forecasting neural network using NuScenes, so I have outputs of X and Y coordinates taken from sample annotations, as well as the ego pose X and Y coordinates for one sample. 
I'm building a visualizer tool for these outputs where I'd plot these coordinates onto a LIDAR graph. To do this, I found all the vehicle Boxes using flat coordinates for one sample.
![Image of data](https://i.imgur.com/t9JpmRb.png)
The picture above shows the first box for the first vehicle in that scene, and the first row in the preds array shows the actual coordinates for that same vehicle taken from its sample annotation translation (these will be predicted coordinates eventually, I'm using the real coordinates to test if the plotting works). I plan to make copies of these boxes and replace their box.center coordinates with the coordinates I need to plot. 
I assumed I could just edit find the displacement of the annotation coordinates from the ego pose coordinates but this seems to not give the same coordinates as that of box.center. How would I go about converting these coordinates from sample annotation translation to that of box.center's LIDAR coordinates? Does this have anything to do with the misalignment referenced in #197? 
Thank you in advance."
"Hi!,
I'd like to know why you multiply by 100 to calculate the FAF metric.

Thanks!

https://github.com/nutonomy/nuscenes-devkit/blob/41605aa8858ce4f650ffd332b199ee72bbfec71a/python-sdk/nuscenes/eval/tracking/metrics.py#L178"
"The trainval set contains 850 scenes, but how can we split it into train and val sets separately? I want to run some experiments on just the val set, anyway I can get just the val set? "
"Hello! 

Is there a way to convert specific scenes in Nuscenes to Kitti Format through the Nuscenes Devkit? 

Forexample, I want to convert only the night scenes. Is it possible? 

Thanks and kind regards.  "
"Hi!
According to _https://github.com/nutonomy/nuscenes-devkit/blob/master/instructions.md_
In one annotation box there should be at least one lidar or one radar point.
However, in v1.0_mini dataset, there are 3466 empty annotation boxes, occupying 18.7% of total annotation number.
If I expand the bounding box size by 1.2x times, there are still 2925 empty annotation boxes, occupying 15.8%, if I expand the bounding box size by 1.5x times, there are still 2400 empty annotation boxes, occupying 12.9%.
These are significant amount of numbers and has conflict with the official statement.
Why this is happening?
PS. I check empty boxes both by calling API and simply searching in *.json file. Both have empty boxes."
"Hi, 

It looks like there are more observations in both the train set and the validation set in [prediction_scenes.json](https://github.com/nutonomy/nuscenes-devkit/blob/nuscenes-predict-challenge/python-sdk/nuscenes/eval/predict/prediction_scenes.json) than what is mentioned in the [CoverNet](https://arxiv.org/pdf/1911.10298.pdf) paper. 

Quoting the paper:
> This leaves us with 37,714 observations in the train set, 8,064 observations in the validation set, and 7,790 observations in the test set. 

In `prediction_scenes.json`, 
```
>>> len(splits.get_prediction_challenge_split(""train""))
109794
>>> len(splits.get_prediction_challenge_split(""val""))
21630
```

I am guessing stationary observations have not been filtered out of `prediction_scenes.json`?

Thanks,
Peiyun"
"According to this [README](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/can_bus/README.md), pose data is identical to ego_pose but sampled at a higher frequency. However, pose orientations from CAN bus data range from [0, pi], while the ones from ego_pose (at least for LIDAR_TOP) range from [-pi, pi]. 

To check, one can run the following code
```
import numpy as np
import os.path as osp
from pyquaternion import Quaternion
from nuscenes.can_bus.can_bus_api import NuScenesCanBus

nusc_root = ""/data/nuscenes""
nusc_can = NuScenesCanBus(dataroot=nusc_root)

angles = []
for i in range(1000):
    scene_name = f""scene-{i:04d}""
    pose_path = f""{nusc_root}/can_bus/{scene_name}_pose.json""
    if not osp.exists(pose_path) or i in nusc_can.route_blacklist:
        continue
    poses = nusc_can.get_messages(scene_name, ""pose"")
    angles += [Quaternion(x[""orientation""]).angle for x in poses]
angles = np.array(angles)
print(""min angle:"", angles.min(), ""max angle"", angles.max())
```

The output I have is: 
```
min angle: 6.059748280939914e-07 max angle 3.1415911355187074
```"
"Hallo, I am training PointPillars on Nuscenes, but with my configs, it can not reproduce the results in the leaderboard, is it possible that someone can share a config, which can reproduce a close results?
thanks"
"I just cloned the repo and followed the instructions in the Installation.md readme file, the last of which is running the unit tests.

8 tests in the unittest suite fail with the same message.

For example:

> ======================================================================
> ERROR: test_drop_gt (nuscenes.eval.tracking.tests.test_algo.TestAlgo)
> Drop one box from the GT.
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/Users/fiodar/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/tests/test_algo.py"", line 217, in test_drop_gt
>     md = ev.accumulate()
>   File ""/Users/fiodar/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/algo.py"", line 118, in accumulate
>     thresholds, recalls = self.compute_thresholds(gt_box_count)
>   File ""/Users/fiodar/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/algo.py"", line 298, in compute_thresholds
>     _, scores = self.accumulate_threshold(threshold=None)
>   File ""/Users/fiodar/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/algo.py"", line 268, in accumulate_threshold
>     events = acc.events.loc[frame_id]
>   File ""/Users/fiodar/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/mot.py"", line 62, in events
>     self.cached_events_df = MOTAccumulatorCustom.new_event_dataframe_with_data(self._indices, self._events)
>   File ""/Users/fiodar/nuscenes-devkit/python-sdk/nuscenes/eval/tracking/mot.py"", line 37, in new_event_dataframe_with_data
>     idx = pd.MultiIndex.from_tuples(indices, names=['FrameId', 'Event'])
>   File ""/opt/miniconda3/envs/nuscenes/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 495, in from_tuples
>     return MultiIndex.from_arrays(arrays, sortorder=sortorder, names=names)
>   File ""/opt/miniconda3/envs/nuscenes/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 436, in from_arrays
>     verify_integrity=False,
>   File ""/opt/miniconda3/envs/nuscenes/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 272, in __new__
>     result._set_names(names)
>   File ""/opt/miniconda3/envs/nuscenes/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 1217, in _set_names
>     ""Length of names must match number of levels in MultiIndex.""
> ValueError: Length of names must match number of levels in MultiIndex.


The other 7 failed tests are:

test_drop_gt_interpolate
test_drop_prediction
test_drop_prediction_multiple
test_empty_submission
test_gt_submission
test_identity_switch
test_scenarios

Any clues as to why this might be happening?"
"Hi

Can you please guide on as to how to project 3d boxes to camera left and right from lidar 3d annotation. I could do it properly for camera front and back."
Could anyone tell me the rotation direction of the Lidar in NuScenes? it rotates from the positive direction of y axis to positive direction of x axis in lidar coordinate or reverse? Thanks
"Hi, I was wondering if the global coordinates or other coordinates are provided for the land-divider? if so, how can I find it? thank you"
"Should we call `remove_close` before `transform`? Because `remove_close` seems to assume LiDAR origin sits at (0,0,0). 

https://github.com/nutonomy/nuscenes-devkit/blob/02e9200218977193a1058dd7234f935834378319/python-sdk/nuscenes/utils/data_classes.py#L111-L116"
"Thank you for building this fantastic library! I found a small but important bug in line 1360 of `_is_line_record_in_patch`

https://github.com/nutonomy/nuscenes-devkit/blob/db2df52aca557a9f81f25235a0618f4f98faa162/python-sdk/nuscenes/map_expansion/map_api.py#L1360

should be

`cond_y = np.logical_and(node_coords[:, 1] < y_max, node_coords[:, 1] > y_min)`

(change the 0 to a 1)"
"Hi, thanks so much for this devkit.

I noticed that [`render_map_mask`](https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/map_expansion/map_api.py#L514) returns (somewhat arbitrarily) the most recently created axis in the gridspec. Maybe I misunderstood but the docstring seems to suggest the function is meant to return a list of all the axes (plural). Maybe the final line could be changed from `return fig, ax` to `return fig, fig.axes`."
"Hello, thank you for the nice dataset and this devkit. I have a question regarding the rotate method in the Box class.
I want to rotate a point cloud scene with a random angle `globalRot` w.r.t Z axis, which is the vertical axis.
I first rotate the points with
`pc.rotate(Quaternion(axis=[0, 0, 1], angle=globalRot).rotation_matrix)`
and then, rotate the boxes with
`box.rotate(Quaternion(axis=[0, 0, 1], angle=globalRot))`

However, while I get the correct box coordinate, I get somehow a wrong box rotation. 
Since I'm new at quaternion, I,m not sure if it makes sense what I say but suppose we have quartenion Q and R. If we want to firstly rotate a box with Q and then rotate it with R, the corresponding quartenion is RQ, which is a product of R and Q.

In the rotate method of the Box class, the definition of the box orientation is
`self.orientation =  quaternion * self.orientation`.

However, if the box first has to be rotated w.r.t `quartenion` and then w.r.t `self.orientation`, the definition should be 
`self.orientation =  self.orientation * quaternion`.


Thank you!"
"Hi, I have installed multiple nuscenes-devkit versions and im now confused which version is being used. Is there any scripts that tells the version of the devkit? Thanks you!"
"Hi nuscene team,

Thanks for this great dataset.

According to paper ""nuScenes: A multimodal dataset for autonomous driving"", top lidar is 32-beam . So I'm guessing that's a velodyne lidar. Is that correct?

As far as I know, there are two types of 32-beam velodyne lidar, HDL-32E and 32C, where 32C have more dense beams in the middle and sparser on the boundary, while 32E have even vertical resolution. Could you please tell me the exact type of lidar that nuscene used? Thanks a lot!"
"I just wrote a simple script to explore the data and to save several render pictures in each scene so that I can visualize it later. Basically, I call `nusc.render_sample_data` and other `render` functions several times in my script. And I ran into this warning:

```bash
~/nuscenes-devkit/python-sdk/nuscenes/nuscenes.py:671: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, axes = plt.subplots(int(np.ceil(n/cols)), cols, figsize=(16, 24))
~/nuscenes-devkit/python-sdk/nuscenes/nuscenes.py:878: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  _, ax = plt.subplots(1, 1, figsize=(9, 16))
```

This is not a bug but I was thinking maybe we could probably fix this (?).
- https://github.com/nutonomy/nuscenes-devkit/blob/9fa456a17103dc8f0dd8df4653cb18535b92ce65/python-sdk/nuscenes/nuscenes.py#L671
- https://github.com/nutonomy/nuscenes-devkit/blob/9fa456a17103dc8f0dd8df4653cb18535b92ce65/python-sdk/nuscenes/nuscenes.py#L878

Edit: @holger-nutonomy Thanks! I totally forget that it is a user's responsibility to close the figure!"
"Hi,
My question is regarding the way thresholds are calculated in the tracking evaluation.
in the function accumulate_threshold(), the recall is calculated per threshold.
the line
matches = events[events.Type == 'MATCH']
implies that only  'MATCH' event counts as true-positive event for recall calculation.
Why don't you count 'SWITCH' event as well? 
I would have expect something like
matches = events[(events.Type == 'MATCH') | (events.Type == 'SWITCH')]
As a switch is also a TP event.
We can see for example, that for precision calculation, motmetrics/metrics.py uses:

def num_detections(df, num_matches, num_switches):
    """"""Total number of detected objects including matches and switches.""""""
    return num_matches + num_switches

Thanks a lot!"
"Thanks for your great work. 
I'm working on the detection task from the 16-beam data. I wonder how to downsample your 32-beam lidar points into 16-beam. "
"Hi,

Question regarding the use of Map api, I'm at the point where I have to retrieve a lot of small map patches using `map.render_map_patch()` which I then transform into numpy array and use for my experiments. The problem that I'm currently facing is that it takes hell of a lot time to retrieve these small patches in real time so my thought was that I could initially load the whole map of given city e.g. boston-seaport, transform into numpy array (lets call it `np_whole_map`) and retrieve the part of the patch that I need directly from that array which cuts the time dramatically. However, at this point I'm struggling to find a way to accurately retrieve the same part of the map that `map.render_map_patch()` would give me by accessing the `np_whole_map` e.g:

by defining the following:

`start_x = 500` 
`start_y = 600` 

`patch = (start_x, start_y, start_x+25, start_y+25`

the `map.render_map_patch()`  will return defined small map patch, question is, how would one retrieve that very same patch directly from `np_whole_map`?

Any suggestions how to work around it would be appreciated. 

Just in addition the way I'm currently working with `np_whole_map` is that I get it using `map.render_map_patch()` where the patch defines the whole map and I set the figure size large enough so when I transform it to numpy it's roughly 10,000 x 10,000, and this is the struggle point where I cannot find an easy way to accurately retrieve the small patch defined above.

Regards,
"
"Hi,

Just to clarify the question, we are having trouble interpreting the camera rotation from the sensor_calibration.json file, we know your data is correct but we'd appreciate having some advice from you on how to do it correctly.*

The description of our issue is the following one:

1. theoretical interpretation:
our interpretation of the situation is that in the sensor_calibration.json, all the cameras rotation and translation is given with respect to the car for every single scene, so if we plot the camera scheme for a given scene we should obtain the six cameras pointing in a different direction

2. 1st implementation:
to verify the point 1, we were using the python 'transformations' module (https://pypi.org/project/transformations/) to extract the yaw from every single quaternion from the 6 cameras, the result we obtained is that all the yaw values are very close to each other.

3. how we created the plot:
we used the 6 cameras rotation and translation from the scene 'cc8c0bf57f984915a77078b10eb33198' contained in sensor_calibration.json (just to make a toy implementation), then we employed the Matlab function plotCamera() and converted each quaternion to a rotation matrix using the function quat2rotm() provided by matlab, see code** and result in the attached image.


*. why are we doing this?
just to give you a context of our project.. we are trying to project 2d detections in the image canvas to the world coordinates by having a depth aproximation based on the average height of the pedestrians (we know this is not accurate, but we want to know how far we can arrive without using the lidar), but the preliminary results showed that all the pedestrians are projected to the front of the car, and during the debugging process we identified the error in our interpretation of the camera rotations.


**matlab code:

close all
clear all

scene= 'cc8c0bf57f984915a77078b10eb33198';
camera_id = '725903f5b62f56118f4094b46a4470d8';
translation = [1.70079119 0.01594563 1.51095764
               1.55084775 -0.4934048   1.49574801
               1.0148781  -0.48056822  1.56239545
               0.02832603 0.00345137 1.57910346
               1.035691   0.48479503 1.59097015
               1.52387798 0.49463134 1.50932822];
rotation = [ 0.49980154 -0.50303162  0.49977981 -0.49737084
           0.2060348  -0.20269406  0.68245078 -0.67136109
           0.1228098  -0.13240084 -0.70043058  0.69049603
           0.50378727 -0.4974025  -0.49418502  0.50454961
           0.69241856 -0.70316194 -0.11648343  0.11203318
           0.6757265  -0.67362665  0.21214015 -0.21122827];


for i=1:6
   cam = plotCamera('Location',translation(i,:),'Orientation',quat2rotm(rotation(i,:)),'Opacity',0.2);
   hold on
end
grid on
axis equal

![image](https://user-images.githubusercontent.com/56935798/70975752-d3e8cd00-20aa-11ea-824b-c19be77cc429.png)

_Originally posted by @Marel-88 in https://github.com/nutonomy/nuscenes-devkit/issues/265#issuecomment-566426352_"
"Hi, can you help me understand how can I retrieve the global coordinates starting from the output of my detection? I know the approximate depth so I can consider the distance of the object with respect to the camera, but I have trouble understanding the camera parameters about rotation, because I have seen, from a plot, that all cameras are facing the same direction unlike the nuScenes scheme presented on the web site. How should I consider rotations and translations? Aren't they given with respect to the car? This is giving me issues with my final projection. Thank you."
"Hi,

Is there is any script which we can evaluate mAP with respect to distance  like over has shown their results in pixie, Henry, internet papers. Kindly let us know"
"For training and val dataset , evaluation toolkit works but when i start evaluating on v1.0 mini dataset gives

""""Samples in split doesn't match samples in predictions.""""

Kindly help

"
"Does nuscenes-devkit have the API to apply ""HD-map"" filter? Eg. filter the objects or pointcloud inside the drivable area. 

If not, does the map possibly suppot the functionality in nature? Thanks!"
"Hi,

Question in regards to the use of both nuscene and the map api. I'm looking to incorporate data from the map api into my model, what I have noticed is that rendering ego pose with `render_egoposes_on_fancy_map` requires the scene token, however, if the given scene is not associated with the currently loaded map object e.g. 'boston-seaport' then it will obviously throw an exception. My question is, is there a way to identify to which map a selected scene belongs to? So that I can loop through all the scenes and pass them to the appropriate map object to retrieve the data.

Regards,"
"Hi,

export_2d_annotations_as_json.py line: 142

This line attempts to only grab corners in front of camera sensor. However, it pulls in Z-axis coordinates, which should be describing the height. To truly get BBox coordinates 'in front' of the sensor, one would have to get the X-axis coordinates for comparison."
"I am downloading one subset of the trainval dataset and try to convert it to kitti dataset but I got the error below.

```
python3 export_kitti.py nuscenes_gt_to_kitti --nusc_kitti_dir ~/nucesc_kitti_trainval/ --split train
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 84.7 seconds.
======
Reverse indexing ...
Done reverse indexing in 25.8 seconds.
======
Traceback (most recent call last):
  File ""export_kitti.py"", line 368, in <module>
    fire.Fire(KittiConverter)
  File ""/home/yujiaq3/venv_python3/lib/python3.7/site-packages/fire/core.py"", line 138, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/yujiaq3/venv_python3/lib/python3.7/site-packages/fire/core.py"", line 471, in _Fire
    target=component.__name__)
  File ""/home/yujiaq3/venv_python3/lib/python3.7/site-packages/fire/core.py"", line 675, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""export_kitti.py"", line 157, in nuscenes_gt_to_kitti
    assert (velo_to_cam_rot.round(0) == np.array([[0, -1, 0], [0, 0, -1], [1, 0, 0]])).all()
AssertionError
```"
"I had issues installing openCV via pip so i was forced to install it via the alternative (by building my own), when i do a `pip install -r setup/requirements.txt` i get the error below: 

```
ERROR: Could not find a version that satisfies the requirement opencv-python (from versions: none)
ERROR: No matching distribution found for opencv-python
```

I was forced to removed it from my requirements file and everything else installed fine. However when i do `pip install nuscenes-devkit` the same error above shows up.

How can I bypass this? "
"Hi, I need to go back to the global coordinates after projecting the coordinates into the image thanks to the code in the Nuscenes devkit. Any help or suggestion? How did you manage that?"
"I am not sure if I am addressing this issue correctly or whether it's even an issue, but upon trying to merge all my data sets into one I have observed duplicate files between the Mini and the Trainval part 1 dataset. I am not sure if this is supposed to be a problem but I would like to hear from you with regards to this, because I had overwritten the duplicate files. Have i done it correctly? Will this have some sort of implications on my project?"
"Hi,
While extracting the zip files, I realized that these archives contain files with same name and one is being overwritten by the other.  Can you please publish number of files in each directory after extraction is complete. And what is meant by ""Please unpack the archives to the /data/sets/nuscenes folder *without* overwriting folders that occur in multiple archives."" ?"
"Hi, I'm doing with some action prediction work on this dataset, but I found you guys haven't publish the raw imu and gps data. Do you have a plan or something?
Thanks!!"
"Hi, there,

I noticed the code of transformation between the camera coordinate and the lidar coordinate is below. 

```
lid_to_ego = transform_matrix(cs_record_lid['translation'], Quaternion(cs_record_lid['rotation']),
                                          inverse=False)
ego_to_cam = transform_matrix(cs_record_cam['translation'], Quaternion(cs_record_cam['rotation']),
                                          inverse=True)
velo_to_cam = np.dot(ego_to_cam, lid_to_ego)
```

Why not `velo_to_cam = np.dot(lid_to_ego, ego_to_cam)`? It makes more sense. I mean, lidar to ego, then ego to the camera."
"Hi,

I wanted to ask if your annotation tool is available

Best regards,
Amine
 "
How to convert the nuscenes dataset to kitti tracking fromat?
"Hi, 
I've been wondering why don't we use the box.orientation.yaw_pitch_roll[0] as the r_y in Kiiti's label?
Cause, as far as I know, the r_y denotes the radians that rotate around the z-axis in Nucense format and y_axis in Kitti, however, that's the same axis if we view them in a global coordinate.
![image](https://user-images.githubusercontent.com/38548370/66637251-3fdf2d00-ec45-11e9-97fa-02d870583f2b.png)
Thanks in advance!"
"Hi nuscenes-devkit team,
Currently, I am trying to extract video sequence dataset from nuScenes (like kitti odometry dataset). Specifically, I would like to render image sequences where each image data has its corresponding LiDAR and radar data. My method is similar to the suggestions in #160 and #180. But I encountered some problem:

The matched LiDAR data has some inconsistent measurements on the boundary of the car. For example:
![image](https://user-images.githubusercontent.com/13144700/66434092-0b3f5b80-ea22-11e9-9c63-28d77531527f.png)
![image](https://user-images.githubusercontent.com/13144700/66434732-7178ae00-ea23-11e9-8222-b085b77d8db0.png)
The discontinuities on LiDAR points do not match with the edge of the car. I know this might be resulted from other reasons (even bugs in my code). But I would like to double-check one possible issue:

According to the description in #160, the timestamp ""of the LIDAR scan is the time when the full rotation of the current LIDAR frame is achieved. Let delta_lidar be the time difference between two consecutive LiDAR sweeps (delta_lidar roughly equals to 1/20 s). 
Then, for example, if the LiDAR scans clockwise starting from the left side and the timestamp of the LiDAR scan triggering the camera is T, the timestamp of the ""front"" camera will be around T - (3/4) * delta_lidar (because the timestamp T is the finishing time of the scan). But if we just look for the LiDAR scan with the closest timestamp to the ""front"" camera, we will actually use (T - delta_lidar) one instead of the T one (Correct me if I am wrong or the explanation is not clear). 
How can I solve this problem? Can I know more details on which direction the LiDAR starts (and ends) its scanning? 

Or do you have any suggestion on generating video (or say sequence) dataset from the nuScenes dataset?

Thank you in advance for any help!

Greetings,
Juan-Ting"
"Hi, 
I find strange that the cameras orientation have large pitches and rolls. How is that possible?
I am trying to get the global yaw from only the yaw in camera coordinate (and not the pitch and roll). Do you have any idea how to do that?

Thanks for your inputs"
"I'm wondering whether there is a convenient function that could help us to quickly determine whether the points are in the 3d_boxes or to return all the points' indexes within the boxes, Thanks!"
"Hello, Thank you for the great work! I'm relatively new here and I'm wondering whether we could split the dataset with official SDK into smaller ones, still we'would be okay to load them? Cause the dataset is really big and too heavy for us to debug and test our new thoughts. Thanks in advance!"
"I get this error when I try to render the data for `lidar_token =  c20de558317f4c00bb6e4c0e01f4386a` via `nusc.render_sample_data(lidar_token)`
```
/usr/local/lib/python3.6/dist-packages/nuscenes/utils/data_classes.py in from_file(cls, file_name)
    253 
    254         scan = np.fromfile(file_name, dtype=np.float32)
--> 255         points = scan.reshape((-1, 5))[:, :cls.nbr_dims()]
    256         return cls(points.T)
    257 

ValueError: cannot reshape array of size 5504 into shape (5)
```

Seems the affected file is `samples/LIDAR_TOP/n015-2018-07-18-11-18-34+0800__LIDAR_TOP__1531884539448719.pcd.bin`.

I ran the `nuscenes/scripts/assert_download.py` script without any errors so I assume there were no problems with the download."
"Hello,

I want to install this devkit from source, I cloned the repo, ran `python setup.py install` as the setup.py is configured to install 1.0.2 version, it doesn't install the latest source code. Could you please help me in installing the newest version from source code? I want to use eval functions which weren't available in 1.0.2"
"Hello!  It's possible to delete boxes from annotation which overlap with other like on this image?   For example  I need to delete red and blue boxes here. 
![test35](https://user-images.githubusercontent.com/50096725/65491249-5bada980-deb7-11e9-88c6-5ee7a98177ca.jpg)
Thanks for help! 
"
"Using the latest version to convert to KITTI dataset formate is invaild, the box is simply out of the image and become <0 for example.

I notice this when I transform all the data and try to train PointRCNN net
But the past version hadn't occurred this problem ever."
"Thanks for this awesome dataset. 

Is it possible to transform the 6 camera images in a single panoramic view + having the projection of point cloud and 3d boxes in the  panoramic view with the nuscenes-devkit?"
"like the title, thanks a lot!"
"Hello,

If I am not wrong, the visibility level of vehicle is set depending upon:
 - Views in all the cameras
 - Views across time

So a vehicle not visible at time T but fully visible at T+1 (in any of the cameras) would be binned as visibility level 4.

If this is correct so far, is there any way to determine which FRAMES the vehicle is fully visible in? for example, given a vehicle instance, I want to get all the camera frames where the vehicle is fully visible (across all time steps). My goal is to get 2D bounding boxes at these frames where I can see the entire vehicle.

Thank you."
"Hi, 

How can I find the ego pose with respect to a sample? It seems like ego pose is available with respect to sensor readings such as when the lidar sweep was obtained, or camera image was obtained, etc. 

Thanks!"
"What I'm planning to do is run `LidarPointCloud.from_file_multisweep(nusc, sample, 'LIDAR_TOP', 'LIDAR_TOP', 1)` to get the 1-sweep pcd (currently getting an array of all zeroes but must be doing something wrong), then use `get_sample_data` for the annotation-specific box and run `points_in_box` to apply the resulting mask to the pcd. Please let me know if I'm on the right track and not reinventing something that's already there.

Edit: figured out that the points are in the `.points` property, zeros were the timestamps returned as the second object."
"Hi nuscenes-devkit team,

I am currently working on sensor fusion between RGB images and radar points. When I projected radar points in nuscenes dataset to images, I found some obvious inconsistent measurements on objects. Here are some examples: (All of the tokens are from the mini split)

1. sample token=b2e2ffeabf91451cb31a79649bfd95df
This plot is generated by the following code (function from the devkit), so should be bug-free:
```python
# Here nuscene_dataset is my own dataset object, but this won't affect the resulting plot given the same sample token.
sample_obj = nuscene_dataset.samples[150] 
sample_token = sample_obj[""token""]
nuscene_dataset.explorer.render_pointcloud_in_image(sample_token,
                                                   dot_size=30,
                                                   pointsensor_channel=""RADAR_FRONT"",
                                                   camera_channel=""CAM_FRONT"")
```
![image](https://user-images.githubusercontent.com/13144700/63552303-9d090d00-c537-11e9-9ecc-c72de90c873f.png)
For better visualization, I further plot with colorbar indicating the depth values:
![image](https://user-images.githubusercontent.com/13144700/63552836-10f7e500-c539-11e9-8107-1eeeb66daa4b.png)

Here are more examples:
2. sample token=d0ecf474e0e64950aa265cd44b9c9d75
![image](https://user-images.githubusercontent.com/13144700/63553160-e3f80200-c539-11e9-93b5-773412d5da00.png)

3. sample token=b8766f4291d5449d95bab459ca724ad8
![image](https://user-images.githubusercontent.com/13144700/63553466-c11a1d80-c53a-11e9-8316-210326f4aa59.png)
There are more similar examples that are not listed here.

In #173, I posted a similar issue on side views However, in these cases (front view), the inconsistent points just too many to be ignored, and they are mainly on foreground objects. I would like to know whether you have more analysis or interpretation on these cases for example:
1. Are they caused by reflections/diffractions from the buildings or other far away objects behind the car or just noisy measurements? (We might be able to obtain useful info from the former but not the latter)
2. Are these points included in the radar point count label for the object?
3. Is there any possible solution to get rid of these points? (I have tried the filter setting, but no promising results were found currently.)

Thank you!

"
"ERROR: Command errored out with exit status 1:
     command: 'c:\users\sarim\appdata\local\programs\python\python37\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\sarim\\AppData\\Local\\Temp\\pip-install-b2hrt6bw\\Shapely\\setup.py'""'""'; __file__='""'""'C:\\Users\\sarim\\AppData\\Local\\Temp\\pip-install-b2hrt6bw\\Shapely\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' egg_info --egg-base pip-egg-info
         cwd: C:\Users\sarim\AppData\Local\Temp\pip-install-b2hrt6bw\Shapely\
    Complete output (9 lines):
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""C:\Users\sarim\AppData\Local\Temp\pip-install-b2hrt6bw\Shapely\setup.py"", line 80, in <module>
        from shapely._buildcfg import geos_version_string, geos_version, \
      File ""C:\Users\sarim\AppData\Local\Temp\pip-install-b2hrt6bw\Shapely\shapely\_buildcfg.py"", line 200, in <module>
        lgeos = CDLL(""geos_c.dll"")
      File ""c:\users\sarim\appdata\local\programs\python\python37\lib\ctypes\__init__.py"", line 364, in __init__
        self._handle = _dlopen(self._name, mode)
    OSError: [WinError 126] The specified module could not be found
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output."
"The devkit is tested for Python 3.6 and Python 3.7. To install Python, please check here.

This 'here' link does not work.

Thanks."
"`render_sample_data` works for me inside the Jupyter notebook, but not in a plain Python script or interpreter. Is this the intended behavior? I'm using Python 3.6, if it matters."
"Hi, I have got the ""image_annotations.json"" file by ""export_2d_annotations_as_json.py"" and put it ""v1.0-mini"" directory.  When I run `nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)`, it could load the load the image annotations table successfully (self.image_annotations had values).

I want to get the 2d annotations for a single image by `nusc.get`:
```
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)
my_scene = nusc.scene[0]
first_sample_token = my_scene['first_sample_token']
my_sample = nusc.get('sample', first_sample_token)
my_annotation_token = my_sample['anns'][18]
my_annotation_metadata = nusc.get('image_annotations', my_annotation_token)
```
it returned errors:
```
  File ""/home/vincent/Code/mobilenet-detection/test_code/test_nuscene.py"", line 9, in <module>
    my_annotation_metadata = nusc.get('image_annotations', my_annotation_token)
  File ""/home/vincent/Code/mobilenet-detection/nuscenes/nuscenes.py"", line 171, in get
    assert table_name in self.table_names, ""Table {} not found"".format(table_name)
AssertionError: Table image_annotations not found
```

Could you please tell me how can I get the 2d annotations for a single image and render it? Thanks!

"
"Hello,

I am trying to get all key frames from the front camera. This is what I am doing:

`sample_data_camera_tokens = [s['token'] for s in dataset.sample_data if (s['sensor_modality'] == 'camera') and s['is_key_frame'] and s['channel'] == 'CAM_FRONT']`

`for sample_data_token in tqdm(sample_data_camera_tokens):
      sample_data_record = nusc.get('sample_data', sample_data_token)
      print(sample_data_record['filename']`

      
The problem is that when I print each sample_data_record['filename'] some of the files are not in chronological order (in terms of time). Any idea about this?"
"Hi Nuscenes Team,

I am currently working on a camera-radar fusion technique using 'CAM_FRONT' and 'RADAR_FRONT' from sample folder of mini dataset. While mapping the radar pointclouds on image using ""map_pointcloud_to_image"" method and then filtering it with my algorithm, I came across few possible discrepancies and therefore wanted to bring it to your notice. 
I have formulated few related questions as well,

![image](https://user-images.githubusercontent.com/53562117/62293338-b05d1700-b468-11e9-9ac3-12f1eb89db50.png)

![image](https://user-images.githubusercontent.com/53562117/62294924-552d2380-b46c-11e9-82df-7c5267fc38c3.png)

**-** The figure is from **scene1 frame 0** of sample folder. The gray coloured car has four radar pointclouds on it (represented by yellow dots) and since it is a parked car, the velocity components should be close to 0. However, if you refer **table 1**, the radar point (represented by a blue dot) at pixel **x = 832.3** and **y = 500.1** with a **radar_id = 85** (obtained from .pcd file) shows a high velocity values v.i.z. **vx_comp = 21.267 m/s** and **vy_comp = 4.5218 m/s**, which could possibly be from the black car that is approaching the ego vehicle.

**Question 1:** Is it a known problem, could it be possible that the image and mapped radar points are not perfectly overlapping and therefore the pixels don't exactly represent radar pcs on image?

![image](https://user-images.githubusercontent.com/53562117/62295035-84439500-b46c-11e9-8ba5-1d8b4a75c2d6.png)

![image](https://user-images.githubusercontent.com/53562117/62295013-7726a600-b46c-11e9-9c80-f0f5ae8218d0.png)

**-** According to the radar calibration, **x is front** (x coordinates in forward direction are positive) and **y is left** (y coordinates on the left side are positive) in radar coordinate system. So, taking this into consideration the y coordinates of radar point clouds lying on the left side of the ego vehicle (assuming the position of radar sensor as 0,0) should be positive and those on the right should have negative values. Likewise, after transforming the radar pointclouds to image coordinate system, **z becomes front** (positive in front) and **x becomes right** (positive on the right side). But, if you refer **table 2**, in which I have considered two points in the left half of the image plane (represented with yellow and white coloured stars). The point cloud with **radar_id = 23** is with an opposite sign convention in both the coordinate system.
Similarly, if you refer table 3, there is a similar discrepancy in radar point cloud with **radar_id = 51** in the right half of image plane (represented with cyan coloured star).

**Question 2:** Is my assumption related to the sign convention correct? If so, what could be the reason for this change in sign convention?

![image](https://user-images.githubusercontent.com/53562117/62296866-506a6e80-b470-11e9-9ac4-3a678a195713.png)

**-** This is related to velocity components, before (vx, vy) and after compensation (vx_comp, vy_comp). 

**Question 3:** Is there a positive/negative direction defined for vx, vy and likewise for vx_comp and vy_comp mentioned in table 4)? Could you please explain a bit how the compensated velocities were calculated?

Best Regards,
Ajay K


"
"I realised that none of the implemented metrics in the evaluation code is sensitive to the provided height prediction in the submitted results. I actually tested this and moved all predictions 100m under the ground, and the metrics return the same values, as only the xy-position is relevant for AP and ATE and all other errors do not include position at all.

Is this intended behaviour, and when yes, with what reasoning?

If not, might I suggest including this into the ASE by aligning the prediction with the ground truth in xy-position and orientation, but not aligning in z-position, and then computing the IOU. This was actually the behaviour I expected to see, when I read the metric description, as xy-position and orientation were the only things talked about, so I assumed those were the features used for alignment."
"Thanks for the great dataset. I wanted to play around a little bit with the evaluation code so I cloned the github repo into my project and wanted to install it in editable mode using `pip install -e packages/nuscenes-devkit` as I sometimes do with other packages as well. However, in this case it did not work directly, as the `setup.py` lies in the subdir `setup`. So I change the pip-command to `pip install -e packages/nuscenes-devkit/setup`. However, on the first run, this creates a directory `python-sdk/nuscenes-devkit.egg-info` and fails, and with subsequent calls it provides a slightly different error because now this is regarded by the `setup.py` also as python package (see [here](https://github.com/nutonomy/nuscenes-devkit/blob/master/setup/setup.py#L17)). This I fixed and the original error returns. I am not super familiar with `setuptools` and organizing Pypi packages, so I just wanted to ask, how is the intended way to get this working? I assume there is an intended way, because it works nicely from installing it directly from the Pypi repo.

Any help is appreciated."
"Hello,

How can I get the position of the pedestrians on the semantic map? Also, I want to get the semantic labels in an area around each pedestrian."
"I'm trying to render ego_pose on the map and I got the following result using the nuscenes_mini, as you can see the ego vehicle track (orange) ends up outside the map, is there a reason that area is not mapped as derivable layer in the map?: 
![Figure_1](https://user-images.githubusercontent.com/24555279/62185903-2f9bff00-b332-11e9-84cd-5e62e7cbf7d1.png)

Thanks"
"Hi, 

Quick question, is there a way to get the bounding box of an ego's vehicle? Or if not, is there a way to estimate its velocity? Noticed that when I retrieve the bounding box of an annotation I can get all the bb data (width, height, yaw etc.) and estimate the velocity, but how would one do it for the ego vehicle?

Regards"
"Hi, 
I wonder if you could provide some code example of how to include map information in the bird view 

![image](https://user-images.githubusercontent.com/24555279/61804820-984f1d00-ae02-11e9-8a25-15bf35596060.png)

"
"Hi,

I would like to project the Lidar_Top pointcloud into the camera_front image plane, ie like in the Kitti Dataset using projection matrix to get each lidar point pixel coordinates. Is there a way to do it with NuScenes? "
"Dear All,

I have downloaded the sample photos and dataset v1.0-mini and I have a question regarding the v1.0-mini dataset, more specifically in the translation and rotation of the Calibrated_Sensor, Ego_Pose and Sample_Annotation e.g. the location of the car, sensor and the bounding box.

I can get: the translation and rotation from the ego pose (for the car)
                the translation, rotation and camera intrinsic from the calibrated sensor (for the sensor)
                the translation, rotation and size from the sample annotation (for the bounding box)

I would like to draw a bounding box on the object, but unfortunately, the shared data have **no units** and they are **extremely different from each other** e.g. translation **(sample_annotation = 632.956 , 1639.65 , 1.131)** and translation **(calibrated_sensor = 1.72200568478 , 0.00475453292289 , 1.49491291905)**

According to that, I would like to ask a couple of questions:
1- Can you please let me know the units of the given coordinates?
2- How can I draw a bounding box? (it should be between the calibrated_sensor and the sample_annotation) as far as I understand, correct me if I am mistaken.
3- what is the difference between the translation and rotation and what is the camera intrinsic coordinates.

Thanks in advance,
Regards
Hazem


"
"Hi ,

I'm using this package [(https://github.com/traveller59/second.pytorch)]. nuscenes-devkit is installed as per the instructions.

When i execute this: 
 python3 create_data.py nuscenes_data_prep --root-path=/home/ubuntu/nuscene_trainval --version=""v1.0-trainval"" --dataset_name=""nuscene""--max_sweeps=10

I'm getting the following error,

assert name in REGISTERED_DATASET_CLASSES, f""available class: {REGISTERED_DATASET_CLASSES}""
AssertionError: available class: {'KittiDataset': <class 'second.data.kitti_dataset.KittiDataset'>, 'NuScenesDataset': <class 'second.data.nuscenes_dataset.NuScenesDataset'>, 'NuScenesDatasetD8': <class 'second.data.nuscenes_dataset.NuScenesDatasetD8'>, 'NuScenesDatasetD8Velo': <class 'second.data.nuscenes_dataset.NuScenesDatasetD8Velo'>, 'NuScenesDatasetVelo': <class 'second.data.nuscenes_dataset.NuScenesDatasetVelo'>, 'NuScenesDatasetD7': <class 'second.data.nuscenes_dataset.NuScenesDatasetD7'>, 'NuScenesDatasetD6': <class 'second.data.nuscenes_dataset.NuScenesDatasetD6'>, 'NuScenesDatasetD5': <class 'second.data.nuscenes_dataset.NuScenesDatasetD5'>, 'NuScenesDatasetD4': <class 'second.data.nuscenes_dataset.NuScenesDatasetD4'>, 'NuScenesDatasetD3': <class 'second.data.nuscenes_dataset.NuScenesDatasetD3'>, 'NuScenesDatasetD2': <class 'second.data.nuscenes_dataset.NuScenesDatasetD2'>, 'NuScenesDatasetD2Velo': <class 'second.data.nuscenes_dataset.NuScenesDatasetD2Velo'>}

Any idea/help will be appreciated.
thanks.
"
"Thanks very much for the map expansion pack and [Jupyter notebook](https://www.google.com/url?q=https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/map_expansion/map_demo.ipynb&sa=D&source=hangouts&ust=1563470906254000&usg=AFQjCNH4mCIcDBuzGLvaZCvQsN6oBKtfSA). I think this will provide great value to our community.

I'm having a bit of trouble understanding the many overlapping layers, and figure they would be easier to understand when projected into image space.

We provided image space projections in our Jupyter notebooks when we created Argoverse. Would you consider adding such functionality to your Jupyter notebooks?  Would it also be possible to provided rendered versions of the notebooks to prevent the need for a full download of all of the data?

For an example of projecting the map into image space, please see this Argoverse notebook (scroll to the last two figures): https://github.com/argoai/argoverse-api/blob/master/demo_usage/argoverse_map_tutorial.ipynb"
" Hi, i have two question about radar point cloud. The first one is that what's cluster of radar point cloud and how to obtain point cloud cluster. The second one is that What does 'false alarm probability of cluster' of radar point cloud mean and how to calculate the false alarm probability.

Thank you!



"
""
"@holger-nutonomy @aloukkal 
I would like to calculate the distance of the object from the camera thus Focal length is required in mm. Kindly let me know whats is the camera Focal Length in mm. 
As you have used ""1/1.8'' CMOS sensor of 1600x1200 resolution"" the following website shows: 
[https://www.vision-doctor.com/en/camera-technology-basics/sensor-and-pixel-sizes.html](url)
7.2x5.4mm. 
is it right???
"
"I was wondering if there was a method to access the sweep data, both lidar and camera and their associated timestamps through the database tokens? As far as I can see the DB tokens map only to sample images, timestamps and data.
Thanks!"
"When installing on win10, and using pip install -r requirements.txt,  a issue will be triggered.
This is caused by version of  Shapely.
---------------------
lgeos = CDLL(""geos_c.dll"")
..................
OSError: [WinError 126] The specified module could not be found
--------------------
According to  [https://github.com/Toblerity/Shapely/issues/738](url), I manully degraded the version to **1.4.1** , because the pip will use the newest version of Shapely (**1.6.1**).
This helps me install the toolkit successfully, but I am not sure if the version will cause any other problem. 

Does the repo has a strict requirements on **Shapely** version?
"
can you support convert nuScenes scenes to rosbag files?
"At first: Thanks a lot for the data and the corresponding devkit!
Please correct me if the following is wrong:

Annotations are given on a sample-level, so their position in the global coordinate system belongs to the sample-timestamp X (which seems to be equal to the Lidar-sample_data-timestamp?!).
Say you use render_sample_data() for CAM_FRONT, then boxes made from annotations with timestamp X are transformed to the sensor coordinate system by using the ego_pose of the specific sample_data and therefore timestamp Y (because CAM_FRONT data has a different timestamp as the sample).

So here we make a little mistake by assuming the ego_pose didn't change too much between timestamp X and timestamp Y?
"
"Reopening the following [issue](https://github.com/nutonomy/nuscenes-devkit/issues/171)

sample_token: f15297033ecb46b49866929ac71889f8, filename: CAM_FRONT/n008-2018-09-18-14-43-59-0400__CAM_FRONT__1537296428112404.jpg
For the above sample token, there are 3D annotations. However, I am not getting any corresponding 2D annotations for the same. This is one of the tokens out of 2.4 million tokens
"
"Hi,
first of all, thanks for releasing the dataset. I have problems in the projection of LiDAR points on the CAM_FRONT image. I want to render the lidar points on CAM_FRONT also for sweeps, so I'm using the following code:

```                
# In images, lidars there is an ordered list of associated CAM_FRONT samples + sweeps and pointclouds

for image, lidar in zip(images, lidars):
    # nexp is a nuscenes explorer
    lidar_points, lidar_coloring, lidar_image = nexp.map_pointcloud_to_image(pointsensor_token=lidar['token'], camera_token=image['token'])

    savename_viz = os.path.join(DIR_VIZ, os.path.basename(image['filename'].replace('.jpg', '_lidar.png')))

    plt.figure(figsize=(9, 16))
    plt.imshow(lidar_image)
    plt.scatter(lidar_points[0, :], lidar_points[1, :], c=lidar_coloring, s=5)
    plt.axis('off')
    plt.savefig(savename_viz)

```

What I get is some good projections, and some really noisy ones. Do you have any idea of where is the problem?

Wrong projection
![wrong](https://user-images.githubusercontent.com/10325202/60165235-4618dd00-97ff-11e9-805c-39ad31597e46.png)

Right projection
![right](https://user-images.githubusercontent.com/10325202/60165236-46b17380-97ff-11e9-9636-f56fa6559d5e.png)

"
"Hi nuscenes-devkit team,

When using nuscenes dataset, I found some serious inconsistencies between the rendered RADAR and LiDAR points. Here are some examples:

In sample d0ecf474e0e64950aa265cd44b9c9d75, if we plot the 'LIDAR_TOP' on 'CAM_BACK_RIGHT' using the given method render_pointcloud_in_image(), we will get the following depth map:
![gg](https://user-images.githubusercontent.com/13144700/60102992-0c909500-975f-11e9-90fe-c74c38b99b5f.png)
which looks reasonable.

But if we render the radar 'RADAR_BACK_RIGHT' on 'CAM_BACK_RIGHT', we will get the following depth map:
![gg](https://user-images.githubusercontent.com/13144700/60103173-6e50ff00-975f-11e9-8222-1655bcd2f2e1.png)
We can see that the yellow points are not consistent with the LiDAR scans.

Similar for the case between 'LIDAR_TOP' and 'RADAR_FRONT_LEFT' (both map to 'CAM_BACK_LEFT') in the same sample.

Is it resulted from errors of radar measurements?"
""
"![image](https://user-images.githubusercontent.com/21303438/59910109-d04df380-9443-11e9-8589-5d24c0631e17.png)


I token dim, location from annotations, as  well a intrinsic, the box drawing above not accurate. I am trying to project 3D on image by using intrinsic but not using SDK, It seems something were miss.

Except for this intrinsic, should we got a rector matrix like KITTI?

I got calib data in KITTI is 3x4 like:

```
[[7.215377e+02 0.000000e+00 6.095593e+02 4.485728e+01]
 [0.000000e+00 7.215377e+02 1.728540e+02 2.163791e-01]
 [0.000000e+00 0.000000e+00 1.000000e+00 2.745884e-03]]
```

but intrinsic from nuScenes is 3x3 like:

```
[[1.26641720e+03 0.00000000e+00 8.16267020e+02]
 [0.00000000e+00 1.26641720e+03 4.91507066e+02]
 [0.00000000e+00 0.00000000e+00 1.00000000e+00]]
```

How can I got the same 3x4 matrix from nuScenes? 

thanks in advance"
"Does there any example for this operation?

I am currently do this:

```
root_dir = '../v1.0-trainval01'
nusc = NuScenes(version='v1.0-trainval', dataroot=root_dir, verbose=False)
all_category_db = list(NameMapping.keys())
all_category = list(NameMapping.values())

my_scene = nusc.scene[15]
sample_token = my_scene['first_sample_token']
end_token = my_scene['last_sample_token']
my_sample = nusc.get('sample', sample_token)

i = 0
while sample_token != end_token:
    my_sample = nusc.get('sample', my_sample['next'])
 
    img = get_image(my_sample, root_dir)
    sample_data = nusc.get_sample_data(my_sample['data']['CAM_FRONT'])
    labels = sample_data[1]
    # this label is according to camera, what we have is lidar
    # converts top lidar 
```

The labels by default is according to that sensor. What if I have points on TOP_LIDAR, how to project that points on this CAM_FRONT?"
"**Hello,**

**I always receive an error when I run the following code:**

get_ipython().run_line_magic('matplotlib', 'inline')
from nuscenes.nuscenes import NuScenes
nusc = NuScenes(version='v1.0-mini', dataroot='/data/sets/nuscenes', verbose=True)

**I guess the problem is in saving the datasets (v1.0-mini file), I know that I should save it under the specified dataroot but unfortunately I can not find it. can anyone help me please?**

**Here is the error I receive:**

Traceback (most recent call last):

  File ""<ipython-input-18-22a1f015afd3>"", line 3, in <module>
    from nuscenes.nuscenes import NuScenes

  File ""C:\Users\haal1\tensorflow\nuscenes-devkit\python-sdk\nuscenes\nuscenes.py"", line 21, in <module>
    from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box

ModuleNotFoundError: No module named 'nuscenes.utils'; 'nuscenes' is not a package"
"When converting the 3D boxes into 2D boxes, we obtain a new file (named by default `image_annotations.json`). Currently, however, this data is not taken into account in the nuScenes dataset created with the command `NuScenes(version='v1.0-mini', dataroot=data_root, verbose=True)`.
I believe that automatically adding all available tables, if they meet the formatting requirements, would be a _nice to have_ .

Thanks for your work :)"
"Hi.
I used python 3.7, and install dev-kit in local

To validation  I run assert_download, the following issues arise.

![Screenshot from 2019-06-17 18-37-55](https://user-images.githubusercontent.com/36911945/59594608-1d258780-912f-11e9-9371-6e58d1623f31.png)

Can I ignore it?
"
"Hi,
I am looking to get vehicle data. I found closed issue here https://github.com/nutonomy/nuscenes-devkit/issues/23, but don't have any update on that.
Greatly appreciate if we get speed, acceleration and steering angle."
"Hi 

I wonder if there are any annotations available for road marking, lanes, borders ? Or is there a way to incorporate them easily into nuscenes ? If not is there any other LIDAR dataset I could use that has lane/road markings annotated ? 

Thanks"
"Hello nuScenes, question for you -- from 16-18 seconds of this video, it looks like there is a  spurious cuboid https://youtu.be/Sipud0IonvQ?t=18. Was this intentional?

<img width=""539"" alt=""Screen Shot 2019-05-28 at 8 39 28 PM"" src=""https://user-images.githubusercontent.com/16724970/58527775-e213e200-8188-11e9-9f41-b0cd4a619e89.png"">
"
"May be you should generate some list files of samle names like ""trainval.txt"" in kitti.
It's more convenient, I think."
Can you provide the height of the lidar installation location? I need this value.
"Hi,
I would like to know if it is possible to render Lidar points to an image which is in a sweep but not in a sample. I know that the Lidar and camera are aligned for samples only. Nevertheless would it be possible to render the Lidar pointcloud with the nearest timestamp to the given image.
Anyhow is it possible to loop over images in a sweep by a token or do they have a token that can be used?
In the data colection description it is said that not every Lidar Scan has a corresponding camera frame but does every camera frame has a Lidar scan?
I hope you can help me as quick and helpfull as the other time I asked a question.
Thank you in advance for any help.

Greetings
Cathrina"
"The current  ""nuscenes format to KITTI format"" tools have no 2d bounding box output, all of them are -1.0.  For producing 2d annotation, I use the ""project_kitti_box_to_image"" function that you provide in kitti.py , the following error happens:

 File ""/home/jjldr/nuscenes-devkit/python-sdk/nuscenes/utils/geometry_utils.py"", line 40, in view_points
    assert points.shape[0] == 3
AssertionError

Please help me !!!
"
"It would be nice to have a conda distribution of this package.

While it is possible to pip install into a conda environment, this package depends on two packages that have different names in public conda channels (namely pyquaternion/quaternion and opencv-python/opencv) which could lead to dependency problems.

Ideally a feedstock should be added to conda-forge.
"
"I noticed that there're still frequently commits, so is the nuscenes_kitti branch ready to be used now?"
"I quickly checked and the first 64 bits/8bytes of the token are sufficient to discriminate individual elements.
This is fortunate since your devkit already uses python dict [where the key hashing function returns 64 bits values](https://docs.python.org/3/reference/datamodel.html#object.__hash__) (actually 61 due to the randomization added for security) ;-)"
"Hi @nutonomy,
Could you please answer the following questions.

1. Where can I find LiDAR data format? No metadata unlike RADAR.
2. Normally RADAR data comes from different CAN message IDs , Is the Nuscenes RADAR data extracted by bitwise operations from the CAN frames ?
3. Are all the 5 RADARs connected to single CAN bus, while acquiring data?
4. How to print out the radar data in numbers in jupyter notebook? After printing metadata, actual data is printed in encoded format. 

Many Thanks.

"
"Hi
I am trying around with the devkit you provided. The code is really easy to understand. Nevertheless I do have a question or two. 
As I understood it, it is possible to retrieve the pose of the ego vehicle  from the dataset and even render it in the map for a certain location and scene. My question is if it is possible to retrieve the ego pose per sample for a given scene. 
I am working with the data taken in Boston. Is it possible for you to tell me the GPS coordinates of the xyz origin? From this I can calculate alle other GPS positions. 
Thank you in advance.
Cheers Cathrina"
Is it possible to make it ?  
"Hi 
  I have annotated 2d bounding box using your script ""export_2d_annotations_as_json.py"". The result gives a lot of extra boxes for a single object.
Example:
![n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151188012404](https://user-images.githubusercontent.com/20641562/57276648-d704ef00-70bf-11e9-9328-5220ad67da23.jpg)
![n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151250612404](https://user-images.githubusercontent.com/20641562/57276650-d79d8580-70bf-11e9-9649-762fc115f764.jpg)
![n008-2018-08-01-15-52-19-0400__CAM_FRONT__1533153285362404](https://user-images.githubusercontent.com/20641562/57276651-d79d8580-70bf-11e9-8103-f1370a9ad1bd.jpg)
If you see the images, there are multiple bounding boxes
Could you tell me if this is some sort of bug ? If it is what is the fix ?."
"In the nuScenes paper, Table 4
AP:
(0.759+0.637+0.447+0.428+0.329+0.319+0.235+0.228+0.063+0.021)/10 * 100% = 34.66%
so the NDS should be
( (0.3466 * 5) + (1 - 0.54) + (1-0.29) + (1-0.45) + (1-0.29) + (1-0.41))/10 * 100%  = 47.53%

I emailed nuScenes, and get a response as follows:
 Table 4 prints the AP for only the 2m threshold.

"
"Hi, 
I m getting the following error: 
______________________________________________________________________
Traceback (most recent call last):
  File ""/home/jai/nuscenes-devkit/python-sdk/nuscenes/tests/test_nuscenes.py"", line 20, in test_load
    nusc = NuScenes(version='v1.0-mini', dataroot=os.environ['NUSCENES'], verbose=False)
  File ""/home/jai/nuscenes-devkit/python-sdk/nuscenes/nuscenes.py"", line 56, in __init__
    assert osp.exists(self.table_root), 'Database version not found: {}'.format(self.table_root)
AssertionError: Database version not found: /data/sets/nuscenes/v1.0-mini
_________________________________________________________________________
Data is at ""/home/jai/data/sets/nuscenes""
Is data located right? How can this be sorted? 
Thank you.

 
Thank

"
"Hi, I found there are some mistakes about point cloud axis.

1. In `data_classes.py` line 249, the return of  function `from_file` should be (x, z, y, intensity). The order of y and z is wrong. 
I found this problem by ploting a 3D scatter plot. Also, if you watch the range of `pc.points`, you will find the range of `pc.points[0, :]` and `pc.points[2, :]` is similar and the range of `pc.points[1, :]` is very different from them. For these reasons, I think `pc.points[1, :]` is z axis, which means height.

2. In `nuscenes.py` line 550, it should be `y axis` instead of `z axis` in th comment. 
(line 551 is right, because `pc.points[2, :]` is y axis which is also means depth.)

3. In `nuscenes.py` line 554, it should be `coloring = pc.points[1, :]`, because the depth is `pc.points[1, :]`, not `pc.points[2, :]`.
(It is apparent that there is a contradiction between line 551 and line 554, because depth is y axis and height is z axis. It not should be both use `pc.points[2, :]`)

4. It you modify codes as I said and run `render_pointcloud_in_image` again, you will find the output image with point clouds is more reasonable.

5. If my finds are right, I'm afraid that they will have an influence of other functions like `view_points` or the radar data may have the same mistakes.

Thanks!"
How many beams of lidar? 16 or 32? It seems using 16 beams but there were no docs indicates sensor details
"Hi!

I'm going to flatten the ""front view"" of a LIDAR sensor to a 2D image ([method liks this](https://github.com/Karthikeya108/AdvancedObjectDetection)), but the horizontal resolution,  vertical resolution and y fudge factor of the LIDAR sensor are not provided. 

1. Could you please provide the LIDAR specs so that we can get more details about the senor.
2. For each LIDAR data(*.pcd.bin), there are about 35000 points. In order to plot LIDAR data on the camera image, it decrease to about 3000 points after using the mask (in function `map_pointcloud_to_image`). Is that the all points of a frame?  Is there any way can I get more points for a front view?

Thanks!"
"I try to project the transformation and hwl on image, here is what I got:

![image](https://user-images.githubusercontent.com/21303438/56717889-40a40580-6770-11e9-812b-2c7779e0b1c1.png)

The projection is right ( the centers are not offset), but the rotation was wrong when rotate 3d box.

What comfusing is: **which direction is xyz in interms of car whl?**, in kitti, the top axis which is the height of vehicle is -y.

I assume nuScenes is also too. But the result was totally wrong in rotate"
""
"It is not clear of which unit using for translation:

```
{'token': '91cb8f15ed4444e99470d43515e50c1d', 'sample_token': 'ca9a282c9e77460f8360f564131a8af5', 'instance_token': 'e3c5b72c12c34c85aac247734ad83bef', 'visibility_token': '1', 'attribute_tokens': ['c3246a1e22a14fcb878aa61e69ae3329'], 'translation': [398.565, 1110.043, 1.788], 'size': [3.016, 3.992, 2.916], 'rotation': [0.2020779797531598, 0.0, 0.0, 0.9793694349421375], 'prev': '', 'next': '787ce4c9d8e840d399c390e259fa62d4', 'num_lidar_pts': 4, 'num_radar_pts': 0, 'category_name': 'vehicle.construction'}

```

I have no idea what does 398.56 means. If it in meters, then it is too long isn't it?
Maybe in centimeters?
From doc I can not tell it"
"Hi,
I have 3 questions.
1. How to import .pcd RADAR data and read values from a text file?
2. What is the difference between samples and sweeps in RADAR data?
3. Which RADAR is used for data collection just to refer to their documentation. Is it ARS 408?   
Thanks.

"
"From this method:

```
def get_anno(anno_token):
    return nusc.get('sample_annotation', anno_token)
```

I can only got this annotations information:

```
{'token': '49f76277d07541c5a584aa14c9d28754', 'sample_token': 'ca9a282c9e77460f8360f564131a8af5', 'instance_token': 'c1958768d48640948f6053d04cffd35b', 'visibility_token': '4', 'attribute_tokens': ['cb5118da1ab342aa947717dc53544259'], 'translation': [399.863, 1143.574, 0.738], 'size': [1.907, 4.727, 1.957], 'rotation': [-0.576881477903537, 0.0, 0.0, 0.816827864639687], 'prev': '', 'next': 'ae589a5a2bdb4813ae8021cd51b7e6eb', 'num_lidar_pts': 15, 'num_radar_pts': 1, 'category_name': 'vehicle.car'}

```

How to the the box on image? both 2d or 3d? maybe we need calibration params first?"
"I am using previous teaser data which somewhat labels as v0.1, but can not parse now:

```
File ""/usr/local/lib/python3.6/dist-packages/nuscenes/nuscenes.py"", line 49, in __init__
    raise ValueError('Invalid DB version: {}'.format(version))
ValueError: Invalid DB version: v0.1

```

It better enum avaiable datasets now"
"Do you have an overview of 1000 scenes, such as which scenes are night scenes and which scenes are rain scenes?
Also,Are there any papers introducing this data set?"
"In the data set, I want to know if the camera and lidar has the same time system or these sensors has there own time system.how do you ensure the precision of time when car is running.

"
"I would like to represent points in a coordinate system with a z-axis parallel to the direction of gravity. From the diagram [here](https://www.nuscenes.org/data-collection), all coordinate frames attached to the ego vehicle have z-axes parallel to the local ground plane (the legend says ""downward from ground"" and ""upward from ground"", which leads me to believe that each axis is normal to the local ground plane that contains the points where the four wheels touch the ground). How can I rotate these frames to be parallel with the direction of gravity? Relatedly, how is the global coordinate frame constructed? Are there any guarantees for its axes, possibly related to the direction of gravity?"
"Since I want to load a mini version of the full dataset, however, the 'version='v1.0-mini'' setting in the tutorial does not work."
"Hello,

Is there a place where we can get the distortion parameters for the cameras? (k1, k2, p1, p2)? I found the calibration parameters in the calibrated_sensor table, but the others don't seem to be there. Thanks!"
I was trying to convert the timestamp to a readable format in python using datetime or ctime without success. Any suggestions?
"Hi,

Is there an overview of the scenes (beyond nusc.list_scenes) that would enable the user to know which scenes correspond to the same route, just at a different time or under different weather conditions? I see the ""singapore-onenorth"" field for example, but there are multiple routes that the car took in scenes under this description.

(Based on your familiarity with the dataset, if you have a couple of scenes you already know correspond to the same route under different weather, that would be great!)"
"Hi,
I am trying to apply a spherical projection on the data, but I am not quite sure how invalid Lidar points are handled in your dataset? By invalid I mean rays that never returned to the sensor due to absorption, diffusion, ..
Have they just been left out of the point clouds or is it still possible to detect those points?

Thanks in advance!
"
"Does the dataset use any terrain models? It seems for each sample, ground always has zero elevation, because after transform LiDAR points to global frame, the ones on the ground have a height of 0. If I place an object on the ground in the global frame, how can I get the right elevation so that it can be correctly projected onto images? "
"How can I get object annotations only in samples/CAM_FRONT image subset. 
Thanks. "
"Hi, I want to download the full nuScenes dataset, but the download speed is too slow.
After building download missions by chrome (on Windows), it show 0 KB/s (never larger than 50 KB/s).

It's strange so I want to ask if there are some solutions for it?
Is Chinese Wall blocking it ?

Thanks."
"I can't see how sidewalk is annotated differently from driveable area. Are they lumped together into the ""semantic prior"", i.e. they are not differentiated?

I just see a single foreground and single background intensity value here: https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/utils/map_mask.py#L28"
"Hello,

I'd like to play around with some of the scenes of the newly released dataset, but I see on the devkit readme: ""For the devkit to work you will need to download all archives""; does that mean we have to download the whole 250GB of data for the provided notebook to work?

I've seen the rendered notebook online, which is great, but I'd like to run some things on my laptop. Is there a way to just work with the mini version?"
Could a method for downloading the data from command line be provided ? I need to download the data to a cluster.
"Hello,

When I untar the files **v1.0-trainval\*_blobs.tgz** on a Ubuntu 18.04.1 LTS, it gives me the following error:
```
sweeps/RADAR_FRONT/n015-2018-07-27-11-36-48+0800__RADAR_FRONT__1532663153790733.pcd

gzip: stdin: invalid compressed data--crc error
sweeps/RADAR_FRONT/n008-2018-07-26-12-13-50-0400__RADAR_FRONT__1532622141160534.pcd
sweeps/RADAR_FRONT/n015-2018-07-27-11-36-48+0800__RADAR_FRONT__1532663021491223.pcd
.v1.0-trainval02_blobs.txt
tar: Child returned status 1
tar: Error is not recoverable: exiting now
```

The command I run is `tar xvzf <filename>`

Thank you very much"
"1.It is written that the translation and the rotation parameters are given with respect to the ego vehicle body frame, what is the [0,0,0] location? I thought that it is the center of the 3D bounding box (the same as the reference point to the translation values for the ego_pose and other objects) but the numbers doesn't add up.
2.The translation of the sensors are written as [l,w,h] ?
3. The rotation of the ego_pose and other objects rotation is relative to which axis? are they all written at the same coordinate system?"
"What is the easiest way to get the distance between the ego-vehicle and other vehicles in the scenes.
Is it just the xyz field of the object box that we get from nusc.get_sample_data?
"
"Is numpy==1.14.5 necessary? Some other dependencies of mine requires numpy>=1.16.1. Would be convenient to be able to install both in the same environment without conflict. How about for example using numpy==1.16 in NuScenes?

One hack that seems to work right now is:
1. pip install nuscenes-devkit
2. pip install --upgrade numpy
3. pip install other-package"
"The [Download](https://www.nuscenes.org/download) page says the trainval split consists of: ""850 scenes, 700 train, 150 val."" I am assuming this distinction is intended to standardize the training and validation sets. How is each scene identified as belonging to either train or val? I can't find these annotations or descriptions. Thanks!"
"Hi, organizers,

As shown from [here](https://www.nuscenes.org/download), the full nuScenes dataset with 1000 scenes will be released by 25/3. However, the corresponding download link still not appears...
When will it be released for download?

THX!"
"If the nuScenes-devkit is the first virtual environment a user creates, the procedure needs a small tweak. Before the step of ""Create the virtual environment"", $PATH needs to be updated to include the path of [VIRTUAL_ENV_LOCATION]. I guess ""source ~/.profile"" will work - if yes, then add a line ""source ~/.profile"" to bashrc in the previous step ""Install virtualenvwrapper"" will do. But I used a dumb reboot to resolve the ""error: virtualenvwrapper could not find virtualenv in your path"" I encountered by following the original steps."
"Hi,

There are a few repeated samples. The images and annotations are the same (occasionally additional annotations). 

e.g sample 343 and sample 369  (the camera keys point to the same filenames)

This issue extends from samples 343 -357 which are the same as these samples: 369-383

Was this intentional? If so, why?

Thank you.



"
"Hello, 

I want to compute the homography matrix from the ground plane to the camera plane. To do so, I use the following matrix:
H = K * T * R * A 
where K is the intrinsic matrix, T the translation matrix, R the rotation matrix and A the 3d to 2d projection matrix. 
I got wrong results until I tweaked the values setting the focal length to a third of its value and setting the translation to values that do not correspond to the ones given. I know I am doing something wrong because your projection works perfectly fine when projecting the Lidar in the camera plane. 

Can you give me an idea of what could be possibly wrong in my approach. 
Many thanks. "
"Hi,
Need some help on how to set up nu-scenes in windows (I am stuck for several weeks ):
I have installed python 3.7 through anaconda prompt 
I have installed the packages from the requirements.txt
Completed the data download (meta, point_samples, point_sweep, image_samples, image_sweep, dev-kit)

From this point how to proceed further ?
Have tried working the solution by going through the existing reported issues but no luck so far.
will appreciate a lot if somebody can help.
I am a beginner in python."
"as I am working on collision avoidance paper I would like to get the true data ie,  x,y,z co-ordinates for all the scenes available could you help me out with the same."
"Hello, 
Do you provide a 2D bounding box of the detected objects (xy coordinates in pixels)?
 I saw that _view points_ in _geometry utils.py_ allows to project the corners of the 3d bounding box into the image plane. However, projecting those corners (e.g all the 8 corners or only the frontal ones) does not match exactly a 2D bounding box, depending on the position of the instance in the image. 

Thank you"
"Hi, 

Could you please share the following specification:
1- Radars Vertical and horizontal resolution and FOV
2- Cameras FOV

Best Regards,
"
"The documentation doesn't discuss how samples are created. The `nusc` object contains 3,977 samples. Assuming they are taken at 2Hz (the same frequency as the labels, as mentioned in the dataset overview), the total number of seconds in the sample records, as computed in a modified `list_scenes` function, should be equivalent to 3,977 / 2. However, I got `total length: 1931.2375695705414` -> `3862 != 3977`. Here is the modified `list_scenes()` function I used to compute the length. The 13 second overlap as mentioned in #8 is not enough to account for the discrepancy. How are the extra samples created?
 
```python
    def list_scenes(self) -> None:
        """""" Lists all scenes with some meta data. """"""

        def ann_count(record):
            count = 0
            sample = self.nusc.get('sample', record['first_sample_token'])
            while not sample['next'] == """":
                count += len(sample['anns'])
                sample = self.nusc.get('sample', sample['next'])
            return count

        recs = [(self.nusc.get('sample', record['first_sample_token'])['timestamp'], record) for record in
                self.nusc.scene]

        total_length = 0
        for start_time, record in sorted(recs):
            start_time = self.nusc.get('sample', record['first_sample_token'])['timestamp'] / 1000000
            length_time = self.nusc.get('sample', record['last_sample_token'])['timestamp'] / 1000000 - start_time
            location = self.nusc.get('log', record['log_token'])['location']
            total_length += length_time
            desc = record['name'] + ', ' + record['description']
            if len(desc) > 55:
                desc = desc[:51] + ""...""

            print('{:16} [{}] {:4.0f}s, {}, #anns:{}'.format(
                desc, datetime.utcfromtimestamp(start_time).strftime('%y-%m-%d %H:%M:%S'),
                length_time, location, ann_count(record)))
        print(""total length: {}"".format(total_length))
```
"
"Bug on the following line?
https://github.com/nutonomy/nuscenes-devkit/blob/f529e4f188c8f86c3d542563436b3d03af8440f7/python-sdk/nuscenes/eval/nuscenes_eval.py#L128"
"`eval_utils.attr_acc()` will return `np.nan` for classes without attributes (e.g. _barrier_).

This will eventually lead to `assert np.nanmin(metric_vals) >= 0` failing when `tp_metrics()` is run for the `barrier` class.

Thus, the `nuscenes_eval` script will always crash, I believe. Haven't checked the code thoroughly but I think that is the case.."
"I want to try the `nuscenes_eval.py` script with fake results. I.e. there might be 0 TPs in the results file. 

In this case the `assert np.nanmin(metric_vals) >= 0` fails, since `metric_vals` contains only `np.nan`. Took me a while to understand why.

How about adding an error message to the `assert` statement?
Or should the case of 0 TPs be handled in another way?"
"what is the parameter for setting the threshold value for the bounding boxes. By default it is set to BoxVisibility.ANY, how can i change this to make bounding boxes when visibility is greater the 70%?

Thanks in advance."
"
```
Traceback (most recent call last):
3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)]
  File ""play.py"", line 1, in <module>
    from nuscenes.nuscenes import NuScenes
  File ""A:\Sync\work\nuscenes-devkit\python-sdk\nuscenes\nuscenes.py"", line 30, in <module>
    from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box
  File ""A:\Sync\work\nuscenes-devkit\python-sdk\nuscenes\utils\data_classes.py"", line 20, in <module>
    class PointCloud(ABC):
  File ""A:\Sync\work\nuscenes-devkit\python-sdk\nuscenes\utils\data_classes.py"", line 60, in PointCloud
    min_distance: float=1.0) -> Tuple[self, np.ndarray]:
NameError: name 'Dict' is not defined
```

Typo? should be `dict`?"
"Using python 3.7.2

```
3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 22:20:52) [MSC v.1916 32 bit (Intel)]
Traceback (most recent call last):
  File ""play.py"", line 1, in <module>
    from nuscenes.nuscenes import NuScenes
  File ""A:\Sync\work\nuscenes-devkit\python-sdk\nuscenes\nuscenes.py"", line 30, in <module>
    from nuscenes.utils.data_classes import LidarPointCloud, RadarPointCloud, Box
  File ""A:\Sync\work\nuscenes-devkit\python-sdk\nuscenes\utils\data_classes.py"", line 20, in <module>
    class PointCloud(ABC):
  File ""A:\Sync\work\nuscenes-devkit\python-sdk\nuscenes\utils\data_classes.py"", line 47, in PointCloud
    def from_file(cls, file_name: str) -> PointCloud:
NameError: name 'PointCloud' is not defined
```"
I was just playing around with the notebook and was just wondering how can we get the actual 3D bounding box point printed? Is there any way of getting these points?
"I am trying to run the tutorial.ipynb notebook, I am able to setup the virtualenvwrapper and virtualenv easily, and the notebook is opening but the issue is that there is an error whenever I try to run the first cell. It gives an error: AssertionError: Database version not found: /data/nuscenes/v0.1

Please help me out. "
"I strongly suggest you using anaconda instead of virtualenv, because virtualenv is giving me lots of errors between python3.7 and the rest of the system (Ubuntu14.04)."
"when I use your tutorial code for test,  raise the following error:

---> 47     def from_file(cls, file_name: str) -> PointCloud:
     48         """"""
     49         Loads point cloud from disk.

NameError: name 'PointCloud' is not defined


please help me !!!"
"The dataset on the aws expires less than 30 minutes.
We have to re-download using the updated links so many times, since the files are large."
Do we have access to the IMU measurements such as the velocity and acceleration of the ego vehicle? Or do we obtain them by deriving the vehicle's translation with respect to time? 
"Hi, 
Is there semantic segmentation for lidar points in the sample data and is there an example of how to use it?
Thank you"
" Hello,

For every annotation, the object Box returns the orientation of the bounding box in degrees:

**filename, boxes, kk = nusc.get_sample_data(token_sd, box_vis_level=0)
for box in boxes:
    angle= box.orientation.degrees**

Can you please clarify what the angle refers to? I expected the angle to be the orientation in the XZ plane of the bounding box with respect to the camera. However, people facing the camera sometimes have positive angles, sometimes negative angles. 

In general, how can I obtain the orientation of the bounding box with respect to the camera?

Thank you very much
Lorenzo"
"the description of the function ""render_pointcloud_in_image"" says the RADAR or LIDAR channel is supported, but it can not render the radar point yet"
"Can not run it under python3, this syntax is really not pythonic:

```
    def __init__(self, version: str='v0.1', dataroot: str='/data/nuscenes', verbose: bool=True):

```"
"Hello,

I run into an error when I run
python examples/export_pointclouds_as_obj.py with the default options.

FileNotFoundError: [Errno 2] No such file or directory: 'nuScenes/sweeps/LIDAR_TOP/n015-2018-07-24-11-22-45+0800__LIDAR_TOP__1532402927747489.pcd.bin'

The path is correct, but when I check the downloaded dataset, I cannot find the file n015-2018-07-24-11-22-45+0800__LIDAR_TOP__1532402927747489.pcd.bin. The closest one I could find is n015-2018-07-24-11-22-45+0800__LIDAR_TOP__1532402927797806.pcd.bin. Are there any files missing? Just want to be sure before I delve further into the code to figure this out."
"Hi,
I know this might be a noob question, but I am trying to read the point cloud of a pcd.bin file of the Lidar using PCL library in C++ but as the extension is pcd.bin loadPCDFile() does not work. I have alredy tried PCDReader but same error occurs

> [pcl::PCDReader::readHeader] No points to read

**How can I read the binary pointclouds using pcl in c++?**
"
"The definition that works:

def render_cv2(self, im: np.ndarray, view: np.ndarray=np.eye(3), normalize: bool=False,
                   colors: Tuple=((0, 0, 255), (255, 0, 0), (155, 155, 155)), linewidth: int=2) -> None:"
After a fresh clone needed to install scipy manually.
"https://github.com/nutonomy/nuscenes-devkit/blob/56487d3ea2914380d9655d142b1fd28977eb9bc5/python-sdk/nuscenes_utils/data_classes.py#L218

Proposed change:

        ax.set_ylim(top=y_lim[0], bottom=y_lim[1])

"
"After making an account to download the data I keep getting an error. On a Linux machine I get the error this xml file does not appear to have any style information associated with it on ubuntu 16.04 and the file will not download. On a windows machine I can download the data, but when I extract it using WinRAR I get just the name of the tar file with no file type extension. Reading your documentation, I should expect it to extract into a folder, but that doesn’t seem to be the case. Let me know what I need to do to fix the issue. Thank you."
"I have downloaded theNuScenes dataset to visualize the Radar cloud.
I am trying to execute the code given in github:

```
import os
import os.path as osp
import argparse
from typing import Tuple

import numpy as np
from PIL import Image
from pyquaternion import Quaternion
from tqdm import tqdm

from nuscenes_utils.data_classes import PointCloud
from nuscenes_utils.geometry_utils import view_points
from nuscenes_utils.nuscenes import NuScenes, NuScenesExplorer


def export_scene_pointcloud(explorer: NuScenesExplorer, out_path: str, scene_token: str, channel: str='LIDAR_TOP',
                            min_dist: float=3.0, max_dist: float=30.0, verbose: bool=True) -> None:
    """"""
    Export fused point clouds of a scene to a Wavefront OBJ file.
    This point-cloud can be viewed in your favorite 3D rendering tool, e.g. Meshlab or Maya.
    :param explorer: NuScenesExplorer instance.
    :param out_path: Output path to write the point-cloud to.
    :param scene_token: Unique identifier of scene to render.
    :param channel: Channel to render.
    :param min_dist: Minimum distance to ego vehicle below which points are dropped.
    :param max_dist: Maximum distance to ego vehicle above which points are dropped.
    :param verbose: Whether to print messages to stdout.
    :return: <None>
    """"""

    # Check inputs.
    valid_channels = ['LIDAR_TOP', 'RADAR_FRONT', 'RADAR_FRONT_RIGHT', 'RADAR_FRONT_LEFT', 'RADAR_BACK_LEFT',
                      'RADAR_BACK_RIGHT']
    camera_channels = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']
    assert channel in valid_channels, 'Input channel {} not valid.'.format(channel)

    # Get records from DB.
    scene_rec = explorer.nusc.get('scene', scene_token)
    start_sample_rec = explorer.nusc.get('sample', scene_rec['first_sample_token'])
    sd_rec = explorer.nusc.get('sample_data', start_sample_rec['data'][channel])

    # Make list of frames
    cur_sd_rec = sd_rec
    sd_tokens = []
    while cur_sd_rec['next'] != '':
        cur_sd_rec = explorer.nusc.get('sample_data', cur_sd_rec['next'])
        sd_tokens.append(cur_sd_rec['token'])

    # Write point-cloud.
    with open(out_path, 'w') as f:
        f.write(""OBJ File:\n"")

        for sd_token in tqdm(sd_tokens):
            if verbose:
                print('Processing {}'.format(sd_rec['filename']))
            sc_rec = explorer.nusc.get('sample_data', sd_token)
            sample_rec = explorer.nusc.get('sample', sc_rec['sample_token'])
            lidar_token = sd_rec['token']
            lidar_rec = explorer.nusc.get('sample_data', lidar_token)
            pc = PointCloud.from_file(osp.join(explorer.nusc.dataroot, lidar_rec['filename']))

            # Get point cloud colors.
            coloring = np.ones((3, pc.points.shape[1])) * -1
            for channel in camera_channels:
                camera_token = sample_rec['data'][channel]
                cam_coloring, cam_mask = pointcloud_color_from_image(nusc, lidar_token, camera_token)
                coloring[:, cam_mask] = cam_coloring

            # Points live in their own reference frame. So they need to be transformed via global to the image plane.
            # First step: transform the point cloud to the ego vehicle frame for the timestamp of the sweep.
            cs_record = explorer.nusc.get('calibrated_sensor', lidar_rec['calibrated_sensor_token'])
            pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)
            pc.translate(np.array(cs_record['translation']))

            # Optional Filter by distance to remove the ego vehicle.
            dists_origin = np.sqrt(np.sum(pc.points[:3, :] ** 2, axis=0))
            keep = np.logical_and(min_dist <= dists_origin, dists_origin <= max_dist)
            pc.points = pc.points[:, keep]
            coloring = coloring[:, keep]
            if verbose:
                print('Distance filter: Keeping %d of %d points...' % (keep.sum(), len(keep)))

            # Second step: transform to the global frame.
            poserecord = explorer.nusc.get('ego_pose', lidar_rec['ego_pose_token'])
            pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)
            pc.translate(np.array(poserecord['translation']))

            # Write points to file
            for (v, c) in zip(pc.points.transpose(), coloring.transpose()):
                if (c == -1).any():
                    # Ignore points without a color.
                    pass
                else:
                    f.write(""v {v[0]:.8f} {v[1]:.8f} {v[2]:.8f} {c[0]:.4f} {c[1]:.4f} {c[2]:.4f}\n"".format(v=v, c=c/255.0))

            if not sd_rec['next'] == """":
                sd_rec = explorer.nusc.get('sample_data', sd_rec['next'])


def pointcloud_color_from_image(nusc, pointsensor_token: str, camera_token: str) -> Tuple[np.array, np.array]:
    """"""
    Given a point sensor (lidar/radar) token and camera sample_data token, load point-cloud and map it to the image
    plane, then retrieve the colors of the closest image pixels.
    :param pointsensor_token: Lidar/radar sample_data token.
    :param camera_token: Camera sample data token.
    :return (coloring <np.float: 3, n>, mask <np.bool: m>). Returns the colors for n points that reproject into the
        image out of m total points. The mask indicates which points are selected.
    """"""

    cam = nusc.get('sample_data', camera_token)
    pointsensor = nusc.get('sample_data', pointsensor_token)

    pc = PointCloud.from_file(osp.join(nusc.dataroot, pointsensor['filename']))
    im = Image.open(osp.join(nusc.dataroot, cam['filename']))

    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.
    # First step: transform the point-cloud to the ego vehicle frame for the timestamp of the sweep.
    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])
    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)
    pc.translate(np.array(cs_record['translation']))

    # Second step: transform to the global frame.
    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])
    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)
    pc.translate(np.array(poserecord['translation']))

    # Third step: transform into the ego vehicle frame for the timestamp of the image.
    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])
    pc.translate(-np.array(poserecord['translation']))
    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)

    # Fourth step: transform into the camera.
    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])
    pc.translate(-np.array(cs_record['translation']))
    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)

    # Fifth step: actually take a ""picture"" of the point cloud.
    # Grab the depths (camera frame z axis points away from the camera).
    depths = pc.points[2, :]

    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).
    points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)

    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.
    mask = np.ones(depths.shape[0], dtype=bool)
    mask = np.logical_and(mask, depths > 0)
    mask = np.logical_and(mask, points[0, :] > 1)
    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)
    mask = np.logical_and(mask, points[1, :] > 1)
    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)
    points = points[:, mask]

    # Pick the colors of the points
    im_data = np.array(im)
    coloring = np.zeros(points.shape)
    for i, p in enumerate(points.transpose()):
        point = p[:2].round().astype(np.int32)
        coloring[:, i] = im_data[point[1], point[0], :]

    return coloring, mask


if __name__ == '__main__':
    # Read input parameters
    parser = argparse.ArgumentParser(description='Export a scene in Wavefront point cloud format.',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--scene', default='scene-0061', type=str, help='Name of a scene, e.g. scene-0061')
    parser.add_argument('--out_dir', default='', type=str, help='Output folder')
    parser.add_argument('--verbose', default=0, type=int, help='Whether to print outputs to stdout')
    args = parser.parse_args()
    out_dir = args.out_dir
    scene_name = args.scene
    verbose = bool(args.verbose)

    out_path = osp.join(out_dir, '%s.obj' % scene_name)
    if osp.exists(out_path):
        print('=> File {} already exists. Aborting.'.format(out_path))
        exit()
    else:
        print('=> Extracting scene {} to {}'.format(scene_name, out_path))

    # Create output folder
    if not out_dir == '' and not osp.isdir(out_dir):
        os.makedirs(out_dir)

    # Extract point-cloud for the specified scene
    nusc = NuScenes()
    scene_tokens = [s['token'] for s in nusc.scene if s['name'] == scene_name]
    assert len(scene_tokens) == 1, 'Error: Invalid scene %s' % scene_name

    export_scene_pointcloud(nusc.explorer, out_path, scene_tokens[0], channel='LIDAR_TOP', verbose=verbose)
```



But when I execute it an error comes up:

>   File ""export_pointclouds_as_obj.py"", line 16
>     def export_scene_pointcloud(explorer: NuScenesExplorer, out_path: str, scene_token: str, channel: str='RADAR_FRONT',
>                                         ^
> SyntaxError: invalid syntax

**Why does this error appear?? Thank you**"
"Hi, I just run your tutorial.ipynb, and I notice that : 
len(nusc.sample) = 3977
while
actually in nuscenes/samples/LIDAR_TOP, we just see 3962 .bin files

and why is that?"
"As Cross Subject setting, the accuracy is 89%, but the result in the paper is 86.6%.
And paper use two GCN layers, but you use three... "
"Hi, thank you for your excellent work, but when I run seq_transformation.py, I wonder where I can get the files like setup.txt, camera.txt, performer.txt, replication.txt, and label.txt？
Looking forward to your reply."
看代码是的one-hot就是一个对角矩阵，直接和特征拼接能对应上吗？
想问一下怎么削减内存开销，加载数据集过程中内存开销太大了
""
"I have problem in training skeleton-400 dataset , could you please send the related code for me ? My e-mail is qinyangz@163.com"
"Does anyone use this model to run the WorkoutSU-10 dataset?
Why can't the loss of val drop during training?"
"Have you operated experiments on  Kinect 400 or 600 dataset? Could you please release the preprocess code for  Kinect 400 or Kinect 600 dataset?
Thanks a lot !"
"Hi,
I'd like to use this code with pre-trained models to do inference on my own data. 
In other words, I'd like to give a certain video in input to the framework and have the predicted action back as output. 
Is it possible? And how can I dot it?
Thanks"
"Could you please release the preprocess code for SYSU dataset? 
NTU60 dataset is kind of large to train due to the computational resources limitation. 
It would be very kind of you to provide code for small dataset like SYSU.
Thanks a lot !"
"Excuseme,in zhe 267th row of main.py file, **pred = pred.log_softmax(dim=self.dim)**  has a error,pred is a linear(FC),did you have such error?"
"如何在自己的数据集上复现该代码？
期待您的回复
谢谢"
"Thanks for your great job!
In your paper, we konw that on the SYSU 3D Human-Object Interaction Dataset (SYSU), each subject perform each action one time.  For the Same Subject (SS) setting, half of the samples of each activity are used for training and the rest for testing. Its means for the same subject, half of the frames of each action are used for training and the rest for testing, right? 
Looking forward to your reply!
作者，您好。请问SS setting的意思是针对同一个对象所做的同一个动作视频的前一半帧用于训练，后一半帧用于测试吗？期待您的回复，非常感谢。"
"Thanks for your great job!
When reading your paper, I notice the table as bellow
![image](https://user-images.githubusercontent.com/55725190/102226669-9fa92600-3f23-11eb-8e7d-3faf07e8363f.png)
I want to know how to get the result of  SGN*!

Looking forward to your reply!
Thank you very much!"
Thank you for your great work! Can you tell me how to calculate the number of parameters for different network?
"`OSError: Unable to open file (unable to open file: name = './data/ntu/NTU_CS.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)`
there is no h5 file in ./data/ntu"
貌似你的代码里面没有任何与NTU120有关的内容
Hello，how to implement skeleton detection of people in pictures or videos?
